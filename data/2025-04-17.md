<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.CV](#cs.CV) [Total: 92]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 46]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 10]
- [stat.ML](#stat.ML) [Total: 7]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)
*Hardy Chen,Haoqin Tu,Fali Wang,Hui Liu,Xianfeng Tang,Xinya Du,Yuyin Zhou,Cihang Xie*

Main category: cs.CL

TL;DR: SFT in LVLMs can hinder RL by inducing pseudo reasoning paths. VLAA-Thinking dataset and GRPO-based RL improve reasoning, achieving top performance on Open LMM Reasoning Leaderboard.


<details>
  <summary>Details</summary>
Motivation: To address how SFT undermines RL in LVLMs by creating pseudo reasoning paths and to improve reasoning capabilities.

Method: Introduce VLAA-Thinking dataset, compare SFT and RL, and use GRPO with a mixed reward module for adaptive reasoning.

Result: VLAA-Thinker (Qwen2.5VL 3B) achieves top-1 performance on Open LMM Reasoning Leaderboard, surpassing SOTA by 1.8%.

Conclusion: SFT locks models into rigid reasoning, while RL with GRPO fosters genuine reasoning. Insights can guide future LVLM research.

Abstract: This work revisits the dominant supervised fine-tuning (SFT) then
reinforcement learning (RL) paradigm for training Large Vision-Language Models
(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent
RL by inducing ``pseudo reasoning paths'' imitated from expert models. While
these paths may resemble the native reasoning paths of RL models, they often
involve prolonged, hesitant, less informative steps, and incorrect reasoning.
To systematically study this effect, we introduce VLAA-Thinking, a new
multimodal dataset designed to support reasoning in LVLMs. Constructed via a
six-step pipeline involving captioning, reasoning distillation, answer rewrite
and verification, VLAA-Thinking comprises high-quality, step-by-step visual
reasoning traces for SFT, along with a more challenging RL split from the same
data source. Using this dataset, we conduct extensive experiments comparing
SFT, RL and their combinations. Results show that while SFT helps models learn
reasoning formats, it often locks aligned models into imitative, rigid
reasoning modes that impede further learning. In contrast, building on the
Group Relative Policy Optimization (GRPO) with a novel mixed reward module
integrating both perception and cognition signals, our RL approach fosters more
genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on
Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard
(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)
among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope
our findings provide valuable insights in developing reasoning-capable LVLMs
and can inform future research in this area.

</details>


### [2] [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
*Jiazhan Feng,Shijue Huang,Xingwei Qu,Ge Zhang,Yujia Qin,Baoquan Zhong,Chengquan Jiang,Jinxin Chi,Wanjun Zhong*

Main category: cs.CL

TL;DR: ReTool enhances reasoning models by integrating real-time code execution and automated RL, outperforming text-based baselines in complex tasks like mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning models struggle with structured problem-solving (e.g., geometric reasoning), while computational tools like code interpreters excel. ReTool aims to bridge this gap.

Method: ReTool combines dynamic interleaving of code execution with natural language reasoning and an automated RL paradigm for tool invocation. Training involves synthetic data generation and RL refinement.

Result: ReTool-32B achieves 67% accuracy on AIME (vs. 40% for baseline) and 72.5% in extended settings, surpassing OpenAI's o1-preview by 27.9%. Emergent behaviors like code self-correction are observed.

Conclusion: ReTool demonstrates the potential of outcome-driven tool integration for advancing complex reasoning and hybrid neuro-symbolic systems.

Abstract: While reasoning models (e.g., DeepSeek R1) trained with reinforcement
learning (RL), excel in textual reasoning, they struggle in scenarios requiring
structured problem-solving, such as geometric reasoning, concise computation,
or complex equation solving-areas where computational tools like code
interpreters (CI) demonstrate distinct advantages. To bridge this gap, we
propose ReTool, which enhances long-form reasoning with tool-integrated
learning, including two key features: (1) dynamic interleaving of real-time
code execution within natural language reasoning processes, and (2) an
automated RL paradigm that allows policy rollouts with multi-turn real-time
code execution and teaches the model in learning when and how to invoke tools
based on outcome feedback. ReTool employs a systematic training framework,
beginning with synthetic cold-start data generation to produce code-augmented
long-form reasoning traces for fine-tuning base models. Subsequent RL training
leverages task outcomes as rewards to iteratively refine the model's tool use
strategy, enabling autonomous discovery of optimal tool invocation patterns
without human priors. Experiments on the challenging MATH Olympiad benchmark
AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with
400 training steps, outperforming text-based RL baseline (40% accuracy, 1080
steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%
accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further
analysis reveals emergent behaviors such as code self-correction, signaling an
''aha moment'' in which the model autonomously masters adaptive tool use. These
findings highlight the promise of outcome-driven tool integration for advancing
complex mathematical reasoning and offer new insights into hybrid
neuro-symbolic systems.

</details>


### [3] [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
*Dayeon Ki,Kevin Duh,Marine Carpuat*

Main category: cs.CL

TL;DR: AskQE is a framework for detecting critical MT errors via question generation and answering, aiding monolingual users in evaluating translations without target language knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing MT error detection and QE techniques fail to address the practical scenario of monolingual users assessing translation quality.

Method: AskQE uses LLaMA-3 70B and entailed facts for question generation, tested on synthetic (ContraTICO) and real (BioMQM) MT error datasets.

Result: AskQE outperforms other QE metrics in correlation (Kendall's Tau) and decision accuracy with human ratings.

Conclusion: AskQE effectively helps monolingual users evaluate MT outputs, offering actionable feedback for decision-making.

Abstract: How can a monolingual English speaker determine whether an automatic
translation in French is good enough to be shared? Existing MT error detection
and quality estimation (QE) techniques do not address this practical scenario.
We introduce AskQE, a question generation and answering framework designed to
detect critical MT errors and provide actionable feedback, helping users decide
whether to accept or reject MT outputs even without the knowledge of the target
language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the
COVID-19 domain, we explore design choices for AskQE and develop an optimized
version relying on LLaMA-3 70B and entailed facts to guide question generation.
We evaluate the resulting system on the BioMQM dataset of naturally occurring
MT errors, where AskQE has higher Kendall's Tau correlation and decision
accuracy with human ratings compared to other QE metrics.

</details>


### [4] [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
*Ozan İrsoy,Pengxiang Cheng,Jennifer L. Chen,Daniel Preoţiuc-Pietro,Shiyue Zhang,Duccio Pappadopulo*

Main category: cs.CL

TL;DR: Reducing instruction-tuning strength improves few-shot learning but weakens instruction-following ability, revealing a trade-off.


<details>
  <summary>Details</summary>
Motivation: To investigate the trade-off between instruction-following ability and in-context few-shot learning performance in instruct models.

Method: Partial adaptation method to scale down instruction-tuning strength across various model families and sizes.

Result: Reduced instruction-tuning improves few-shot learning performance but diminishes instruction-following ability.

Conclusion: A trade-off exists between in-context learning and instruction-following, requiring practical consideration.

Abstract: Instruct models, obtained from various instruction tuning or post-training
steps, are commonly deemed superior and more usable than their base
counterpart. While the model gains instruction following ability, instruction
tuning may lead to forgetting the knowledge from pre-training or it may
encourage the model being overly conversational or verbose. This, in turn, can
lead to degradation of in-context few-shot learning performance. In this work,
we study the performance trajectory between base and instruct models by scaling
down the strength of instruction-tuning via the partial adaption method. We
show that, across several model families and model sizes, reducing the strength
of instruction-tuning results in material improvement on a few-shot in-context
learning benchmark covering a variety of classic natural language tasks. This
comes at the cost of losing some degree of instruction following ability as
measured by AlpacaEval. Our study shines light on the potential trade-off
between in-context learning and instruction following abilities that is worth
considering in practice.

</details>


### [5] [Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
*Minwoo Kang,Suhong Moon,Seung Hyeong Lee,Ayush Raj,Joseph Suh,David M. Chan*

Main category: cs.CL

TL;DR: The paper introduces a method to enhance LLMs' ability to simulate human behavior by creating detailed virtual personas with synthetic backstories, improving their use in political science studies.


<details>
  <summary>Details</summary>
Motivation: To address the need for LLMs to better approximate higher-order human behaviors, like inter-group perceptions, for applications in political science.

Method: Proposes a novel methodology using multi-turn interview transcripts to generate rich, consistent backstories for virtual personas.

Result: Virtual personas with these backstories closely replicate human responses (87% improvement in Wasserstein Distance) and match original study effect sizes.

Conclusion: Extends LLMs' applicability beyond individual opinions, enabling broader use in human studies.

Abstract: Large language models (LLMs) are increasingly capable of simulating human
behavior, offering cost-effective ways to estimate user responses during the
early phases of survey design. While previous studies have examined whether
models can reflect individual opinions or attitudes, we argue that a
\emph{higher-order} binding of virtual personas requires successfully
approximating not only the opinions of a user as an identified member of a
group, but also the nuanced ways in which that user perceives and evaluates
those outside the group. In particular, faithfully simulating how humans
perceive different social groups is critical for applying LLMs to various
political science studies, including timely topics on polarization dynamics,
inter-group conflict, and democratic backsliding. To this end, we propose a
novel methodology for constructing virtual personas with synthetic user
``backstories" generated as extended, multi-turn interview transcripts. Our
generated backstories are longer, rich in detail, and consistent in
authentically describing a singular individual, compared to previous methods.
We show that virtual personas conditioned on our backstories closely replicate
human response distributions (up to an 87\% improvement as measured by
Wasserstein Distance) and produce effect sizes that closely match those
observed in the original studies. Altogether, our work extends the
applicability of LLMs beyond estimating individual self-opinions, enabling
their use in a broader range of human studies.

</details>


### [6] [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
*Takashi Morita,Timothy J. O'Donnell*

Main category: cs.CL

TL;DR: The study shows that the Germanic-Latinate distinction in English words is learnable from phonotactic information, using unsupervised clustering to align with etymological classes and uncover new linguistic features.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learnability in etymology-based generalizations, as historical origins are inaccessible to learners.

Method: Unsupervised clustering on corpus-extracted words to analyze phonotactic information.

Result: Clusters aligned with etymological distinctions and recovered known linguistic generalizations, while uncovering new features.

Conclusion: The findings suggest phonotactic information can reveal etymological distinctions, offering new hypotheses for future research.

Abstract: Cross-linguistically, native words and loanwords follow different
phonological rules. In English, for example, words of Germanic and Latinate
origin exhibit different stress patterns, and a certain syntactic structure is
exclusive to Germanic verbs. When seeing them as a cognitive model, however,
such etymology-based generalizations face challenges in terms of learnability,
since the historical origins of words are presumably inaccessible information
for general language learners. In this study, we present computational evidence
indicating that the Germanic-Latinate distinction in the English lexicon is
learnable from the phonotactic information of individual words. Specifically,
we performed an unsupervised clustering on corpus-extracted words, and the
resulting word clusters largely aligned with the etymological distinction. The
model-discovered clusters also recovered various linguistic generalizations
documented in the previous literature regarding the corresponding etymological
classes. Moreover, our findings also uncovered previously unrecognized features
of the quasi-etymological clusters, offering novel hypotheses for future
experimental studies.

</details>


### [7] [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
*Zhisong Zhang,Tianqing Fang,Kaixin Ma,Wenhao Yu,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: The paper introduces a rollback mechanism for web agents to improve navigation in dynamic web environments, showing effectiveness in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current web agents struggle with complex, dynamic environments due to limited planning and search abilities, especially in recovering from errors.

Method: The proposed method enhances web agents with an explicit rollback mechanism, allowing them to revert to previous states for better control over navigation.

Result: Experiments on live web navigation benchmarks demonstrate the approach's effectiveness in both zero-shot and fine-tuning settings.

Conclusion: The rollback mechanism significantly improves web agent performance by enabling flexible and efficient navigation.

Abstract: With recent advancements in large language models, web agents have been
greatly improved. However, dealing with complex and dynamic web environments
requires more advanced planning and search abilities. Previous studies usually
adopt a greedy one-way search strategy, which may struggle to recover from
erroneous states. In this work, we enhance web agents with an explicit rollback
mechanism, enabling the agent to revert back to a previous state in its
navigation trajectory. This mechanism gives the model the flexibility to
directly control the search process, leading to an effective and efficient web
navigation method. We conduct experiments on two live web navigation benchmarks
with zero-shot and fine-tuning settings. The results demonstrate the
effectiveness of our proposed approach.

</details>


### [8] [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
*Yue Li,Lihong Zhang*

Main category: cs.CL

TL;DR: SAFL reduces FL communication overhead and enhances privacy in LLMs by dynamically fine-tuning attention-critical transformer layers.


<details>
  <summary>Details</summary>
Motivation: Address challenges of communication overhead and model privacy in FL for LLMs, especially in healthcare.

Method: Introduces Selective Attention Federated Learning (SAFL), which dynamically fine-tunes attention-critical transformer layers using attention patterns.

Result: SAFL achieves competitive performance with centralized models on clinical NLP benchmarks while improving communication efficiency and privacy.

Conclusion: SAFL is effective for FL in LLMs, balancing performance, efficiency, and privacy.

Abstract: Federated Learning (FL) faces major challenges regarding communication
overhead and model privacy when training large language models (LLMs),
especially in healthcare applications. To address these, we introduce Selective
Attention Federated Learning (SAFL), a novel approach that dynamically
fine-tunes only those transformer layers identified as attention-critical. By
employing attention patterns to determine layer importance, SAFL significantly
reduces communication bandwidth and enhances differential privacy resilience.
Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and
MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive
performance with centralized models while substantially improving communication
efficiency and privacy preservation.

</details>


### [9] [Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture](https://arxiv.org/abs/2504.11809)
*Biao Fu,Donglei Yu,Minpeng Liao,Chengxi Li,Yidong Chen,Kai Fan,Xiaodong Shi*

Main category: cs.CL

TL;DR: EASiST introduces an efficient and adaptive approach for simultaneous speech translation (SimulST) using a fully unidirectional architecture, dynamic read/write policy, and multi-stage training, outperforming baselines in latency-quality trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based SimulST methods face computational inefficiency or rigid policies, limiting performance. EASiST aims to address these challenges.

Method: EASiST employs a unidirectional architecture, multi-latency data curation, explicit read/write tokens, a lightweight policy head, and multi-stage training.

Result: EASiST achieves superior latency-quality trade-offs on MuST-C En→De and En→Es datasets.

Conclusion: EASiST provides an efficient and adaptive solution for SimulST, improving performance and flexibility over existing methods.

Abstract: Simultaneous speech translation (SimulST) produces translations incrementally
while processing partial speech input. Although large language models (LLMs)
have showcased strong capabilities in offline translation tasks, applying them
to SimulST poses notable challenges. Existing LLM-based SimulST approaches
either incur significant computational overhead due to repeated encoding of
bidirectional speech encoder, or they depend on a fixed read/write policy,
limiting the efficiency and performance. In this work, we introduce Efficient
and Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional
architecture, including both speech encoder and LLM. EASiST includes a
multi-latency data curation strategy to generate semantically aligned SimulST
training samples and redefines SimulST as an interleaved generation task with
explicit read/write tokens. To facilitate adaptive inference, we incorporate a
lightweight policy head that dynamically predicts read/write actions.
Additionally, we employ a multi-stage training strategy to align speech-text
modalities and optimize both translation and policy behavior. Experiments on
the MuST-C En$\rightarrow$De and En$\rightarrow$Es datasets demonstrate that
EASiST offers superior latency-quality trade-offs compared to several strong
baselines.

</details>


### [10] [ARWI: Arabic Write and Improve](https://arxiv.org/abs/2504.11814)
*Kirill Chirkunov,Bashar Alhafni,Chatrine Qwaider,Nizar Habash,Ted Briscoe*

Main category: cs.CL

TL;DR: ARWI is a new Arabic writing assistant with grammar correction, essay scoring, and a prompt database for learners at different proficiency levels.


<details>
  <summary>Details</summary>
Motivation: Advanced Arabic writing tools are limited despite the language's widespread use.

Method: ARWI includes a prompt database, text editor, grammatical error detection/correction, and automated essay scoring.

Result: A preliminary study shows ARWI helps learners identify gaps and improve proficiency.

Conclusion: ARWI fills a gap in Arabic writing assistance and supports further research.

Abstract: Although Arabic is spoken by over 400 million people, advanced Arabic writing
assistance tools remain limited. To address this gap, we present ARWI, a new
writing assistant that helps learners improve essay writing in Modern Standard
Arabic. ARWI is the first publicly available Arabic writing assistant to
include a prompt database for different proficiency levels, an Arabic text
editor, state-of-the-art grammatical error detection and correction, and
automated essay scoring aligned with the Common European Framework of Reference
standards for language attainment. Moreover, ARWI can be used to gather a
growing auto-annotated corpus, facilitating further research on Arabic grammar
correction and essay scoring, as well as profiling patterns of errors made by
native speakers and non-native learners. A preliminary user study shows that
ARWI provides actionable feedback, helping learners identify grammatical gaps,
assess language proficiency, and guide improvement.

</details>


### [11] [Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
*Julia Kreutzer,Eleftheria Briakou,Sweta Agrawal,Marzieh Fadaee,Kocmi Tom*

Main category: cs.CL

TL;DR: The paper highlights gaps in evaluating multilingual large language models (mLLMs) and proposes adopting best practices from machine translation (MT) evaluation to improve rigor and transparency.


<details>
  <summary>Details</summary>
Motivation: Current evaluation practices for mLLMs lack comprehensiveness and consistency, hindering their development.

Method: The study draws parallels with MT evaluation, conducts experiments, and identifies components for robust meta-evaluation.

Result: Demonstrates how MT evaluation practices can enhance understanding of mLLM quality differences and provides a checklist for better evaluation.

Conclusion: Adopting MT-inspired evaluation practices can improve mLLM development and ensure rigorous assessment of evaluation methods.

Abstract: Generation capabilities and language coverage of multilingual large language
models (mLLMs) are advancing rapidly. However, evaluation practices for
generative abilities of mLLMs are still lacking comprehensiveness, scientific
rigor, and consistent adoption across research labs, which undermines their
potential to meaningfully guide mLLM development. We draw parallels with
machine translation (MT) evaluation, a field that faced similar challenges and
has, over decades, developed transparent reporting standards and reliable
evaluations for multilingual generative models. Through targeted experiments
across key stages of the generative evaluation pipeline, we demonstrate how
best practices from MT evaluation can deepen the understanding of quality
differences between models. Additionally, we identify essential components for
robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are
rigorously assessed. We distill these insights into a checklist of actionable
recommendations for mLLM research and development.

</details>


### [12] [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833)
*Changjiang Gao,Xu Huang,Wenhao Zhu,Shujian Huang,Lei Li,Fei Yuan*

Main category: cs.CL

TL;DR: Multilingual reasoning in LLMs can outperform English-only tasks by nearly 10 Acc@$k$ points, offering robustness and higher upper bounds, though current methods fall short due to biases and limitations.


<details>
  <summary>Details</summary>
Motivation: To explore the under-examined phenomenon where non-English languages sometimes outperform English in reasoning tasks and to determine the upper bounds of multilingual reasoning.

Method: Analyze the performance of multilingual reasoning tasks, compare it to English-only reasoning, and investigate the reasons for the upper bound and challenges in achieving it.

Result: Multilingual reasoning shows a nearly 10 Acc@$k$ points improvement over English-only tasks, with robustness to translation and language variations, but current answer selection methods fail to reach this potential.

Conclusion: Multilingual reasoning in LLMs holds significant promise, but new methods are needed to overcome biases and limitations to fully harness its potential.

Abstract: Previous work indicates that large language models exhibit a significant
"English bias", i.e. they often perform better when tasks are presented in
English. Interestingly, we have observed that using certain other languages in
reasoning tasks can yield better performance than English. However, this
phenomenon remains under-explored. In this paper, we explore the upper bound of
harnessing multilingualism in reasoning tasks, suggesting that multilingual
reasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly
(tolerance for variations in translation quality and language choice) higher
upper bounds than English-only reasoning. Besides analyzing the reason behind
the upper bound and challenges in reaching it, we also find that common answer
selection methods cannot achieve this upper bound, due to their limitations and
biases. These insights could pave the way for future research aimed at fully
harnessing the potential of multilingual reasoning in LLMs.

</details>


### [13] [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
*Yue Zhao,Qingqing Gu,Xiaoyu Wang,Teng Chen,Zhonglin Jiang,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: FiSMiness, a framework using Finite State Machine (FSM) on LLMs, improves emotional support conversations by enabling self-reasoning of emotions and strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based ESC studies lack state model perspective, leading to suboptimal long-term satisfaction.

Method: Leverages FSM on LLMs for self-reasoning of seeker's emotion, support strategy, and response per turn.

Result: FiSMiness outperforms baselines like direct inference, self-refine, and chain of thought, even with fewer parameters.

Conclusion: FiSMiness provides a superior solution for ESC by integrating state model reasoning into LLMs.

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Finite State Machine (FSM) on LLMs, and propose a framework called
FiSMiness. Our framework allows a single LLM to bootstrap the planning during
ESC, and self-reason the seeker's emotion, support strategy and the final
response upon each conversational turn. Substantial experiments on ESC datasets
suggest that FiSMiness outperforms many baselines, including direct inference,
self-refine, chain of thought, finetuning, and external-assisted methods, even
those with many more parameters.

</details>


### [14] [Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection](https://arxiv.org/abs/2504.11900)
*Kabir Ahuja,Melanie Sclar,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: The paper introduces FlawedFictions, a benchmark for evaluating LLMs' ability to detect plot holes in stories, revealing their limitations in narrative consistency and reasoning.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess LLMs' deeper language understanding and reasoning skills, particularly in narrative consistency, which existing benchmarks overlook.

Method: Developed FlawedFictionsMaker, an algorithm to synthesize plot holes in stories, and constructed the FlawedFictions benchmark with human filtering for quality.

Result: State-of-the-art LLMs perform poorly in detecting plot holes, especially in longer stories, and LLM-based story tasks introduce more plot holes than human-written originals.

Conclusion: LLMs struggle with nuanced narrative reasoning, highlighting the need for improved benchmarks and models for deeper language understanding.

Abstract: Stories are a fundamental aspect of human experience. Engaging deeply with
stories and spotting plot holes -- inconsistencies in a storyline that break
the internal logic or rules of a story's world -- requires nuanced reasoning
skills, including tracking entities and events and their interplay, abstract
thinking, pragmatic narrative understanding, commonsense and social reasoning,
and theory of mind. As Large Language Models (LLMs) increasingly generate,
interpret, and modify text, rigorously assessing their narrative consistency
and deeper language understanding becomes critical. However, existing
benchmarks focus mainly on surface-level comprehension. In this work, we
propose plot hole detection in stories as a proxy to evaluate language
understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel
algorithm to controllably and carefully synthesize plot holes in human-written
stories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot
hole detection abilities in stories -- FlawedFictions -- , which is robust to
contamination, with human filtering ensuring high quality. We find that
state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless
of the reasoning effort allowed, with performance significantly degrading as
story length increases. Finally, we show that LLM-based story summarization and
story generation are prone to introducing plot holes, with more than 50% and
100% increases in plot hole detection rates with respect to human-written
originals.

</details>


### [15] [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) to evaluate gender-neutral translation (GNT), addressing limitations of current monolingual classifiers. Two prompting methods are tested, with phrase-level annotations improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current GNT evaluation methods are limited to monolingual classifiers, which ignore the source text and require extensive data for new languages. LLMs offer a scalable alternative.

Method: Two prompting approaches are investigated: one for sentence-level assessments and another with phrase-level annotations (chain-of-thought) before sentence-level judgments. Experiments involve five LLMs across multiple languages.

Result: LLMs effectively evaluate GNT, with phrase-level prompting consistently improving accuracy across all models.

Conclusion: LLMs provide a scalable and accurate solution for GNT evaluation, outperforming current monolingual methods.

Abstract: Gender-neutral translation (GNT) aims to avoid expressing the gender of human
referents when the source text lacks explicit cues about the gender of those
referents. Evaluating GNT automatically is particularly challenging, with
current solutions being limited to monolingual classifiers. Such solutions are
not ideal because they do not factor in the source sentence and require
dedicated data and fine-tuning to scale to new languages. In this work, we
address such limitations by investigating the use of large language models
(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:
one in which LLMs generate sentence-level assessments only, and another, akin
to a chain-of-thought approach, where they first produce detailed phrase-level
annotations before a sentence-level judgment. Through extensive experiments on
multiple languages with five models, both open and proprietary, we show that
LLMs can serve as evaluators of GNT. Moreover, we find that prompting for
phrase-level annotations before sentence-level assessments consistently
improves the accuracy of all models, providing a better and more scalable
alternative to current solutions.

</details>


### [16] [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
*Ram Mohan Rao Kadiyala,Siddartha Pullakhandam,Kanwal Mehreen,Drishti Sharma,Siddhant Gupta,Jebish Purbey,Ashay Srivastava,Subhasya TippaReddy,Arvind Reddy Bobbili,Suraj Telugara Chandrashekhar,Modabbir Adeeb,Srinadh Vura,Hamza Farooq*

Main category: cs.CL

TL;DR: The paper introduces token classification models for detecting AI-generated content, especially in human-LLM co-authored texts, and presents a new dataset of 2.4M texts across 23 languages.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle with short and co-authored texts, prompting the need for a robust detection method adaptable to various generators and domains.

Method: Token classification models trained on a large dataset of human-LLM co-authored texts, tested on unseen domains, generators, and adversarial inputs.

Result: Models performed well across diverse scenarios, including non-native speakers and adversarial inputs, with detailed performance analysis by domain and generator.

Conclusion: The proposed models and dataset advance detection of AI-generated content, particularly in co-authored and adversarial settings.

Abstract: An ideal detection system for machine generated content is supposed to work
well on any generator as many more advanced LLMs come into existence day by
day. Existing systems often struggle with accurately identifying AI-generated
content over shorter texts. Further, not all texts might be entirely authored
by a human or LLM, hence we focused more over partial cases i.e human-LLM
co-authored texts. Our paper introduces a set of models built for the task of
token classification which are trained on an extensive collection of
human-machine co-authored texts, which performed well over texts of unseen
domains, unseen generators, texts by non-native speakers and those with
adversarial inputs. We also introduce a new dataset of over 2.4M such texts
mostly co-authored by several popular proprietary LLMs over 23 languages. We
also present findings of our models' performance over each texts of each domain
and generator. Additional findings include comparison of performance against
each adversarial method, length of input texts and characteristics of generated
texts compared to the original human authored texts.

</details>


### [17] [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
*Xanh Ho,Jiahao Huang,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: LLM-as-a-judge outperforms traditional EM/F1 metrics in evaluating QA models, showing higher correlation with human judgments (0.85 vs. 0.17/0.36).


<details>
  <summary>Details</summary>
Motivation: Traditional metrics (EM/F1) inadequately capture QA model performance, prompting exploration of LLM-as-a-judge for better evaluation.

Method: Reassessed QA model performance using LLM-as-a-judge across four datasets, comparing LLM families and answer types.

Result: LLM-as-a-judge correlates strongly with human judgments (0.85), outperforming EM/F1, though struggles with complex answer types like 'job'.

Conclusion: LLM-as-a-judge is a superior alternative to EM/F1, despite minor limitations, and avoids bias issues like self-preference.

Abstract: Extractive reading comprehension question answering (QA) datasets are
typically evaluated using Exact Match (EM) and F1-score, but these metrics
often fail to fully capture model performance. With the success of large
language models (LLMs), they have been employed in various tasks, including
serving as judges (LLM-as-a-judge). In this paper, we reassess the performance
of QA models using LLM-as-a-judge across four reading comprehension QA
datasets. We examine different families of LLMs and various answer types to
evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show
that LLM-as-a-judge is highly correlated with human judgments and can replace
traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human
judgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.
These findings confirm that EM and F1 metrics underestimate the true
performance of the QA models. While LLM-as-a-judge is not perfect for more
difficult answer types (e.g., job), it still outperforms EM/F1, and we observe
no bias issues, such as self-preference, when the same model is used for both
the QA and judgment tasks.

</details>


### [18] [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
*Raúl Vázquez,Timothee Mickus,Elaine Zosa,Teemu Vahtola,Jörg Tiedemann,Aman Sinha,Vincent Segonne,Fernando Sánchez-Vega,Alessandro Raganato,Jindřich Libovický,Jussi Karlgren,Shaoxiong Ji,Jindřich Helcl,Liane Guillou,Ona de Gibert,Jaione Bengoetxea,Joseph Attieh,Marianna Apidianaki*

Main category: cs.CL

TL;DR: The Mu-SHROOM shared task focuses on detecting hallucinations in LLM outputs across 14 languages, using span-labeling. It attracted 2,618 submissions from 43 teams, highlighting community interest. Results and key performance factors are analyzed, with challenges like language variability and annotator disagreement noted.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations and overgeneration in instruction-tuned LLMs, and to foster community engagement in developing solutions.

Method: Framed as a span-labeling task, the task involved detecting hallucinations in LLM outputs across 14 languages, with submissions from diverse methodologies.

Result: 2,618 submissions from 43 teams were received, with empirical analysis identifying key performance factors. Challenges included language variability and annotator disagreement.

Conclusion: The task successfully engaged the community in hallucination detection, revealing performance insights and ongoing challenges like language differences and labeling consistency.

Abstract: We present the Mu-SHROOM shared task which is focused on detecting
hallucinations and other overgeneration mistakes in the output of
instruction-tuned large language models (LLMs). Mu-SHROOM addresses
general-purpose LLMs in 14 languages, and frames the hallucination detection
problem as a span-labeling task. We received 2,618 submissions from 43
participating teams employing diverse methodologies. The large number of
submissions underscores the interest of the community in hallucination
detection. We present the results of the participating systems and conduct an
empirical analysis to identify key factors contributing to strong performance
in this task. We also emphasize relevant current challenges, notably the
varying degree of hallucinations across languages and the high annotator
disagreement when labeling hallucination spans.

</details>


### [19] [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
*Jose Manuel Guevara-Vela*

Main category: cs.CL

TL;DR: The paper draws an analogy between LLMs and quasicrystals, emphasizing their ability to produce coherent linguistic patterns without strict repetition, suggesting new evaluation and design approaches.


<details>
  <summary>Details</summary>
Motivation: To reframe the understanding of LLMs by comparing them to quasicrystals, highlighting their unique structural coherence rather than just predictive accuracy or factuality.

Method: Proposes viewing LLMs as generators of quasi-structured language, focusing on constraint propagation and coherence over token-level accuracy.

Result: LLMs exhibit emergent patterning, characterized by constraint, resonance, and structural depth, rather than randomness or strict rules.

Conclusion: Evaluating LLMs through this structural lens opens new paths for design and analysis, emphasizing patterns of coherence and constraint.

Abstract: This essay proposes an analogy between large language models (LLMs) and
quasicrystals: systems that exhibit global coherence without periodic
repetition and that are generated through local constraints. While LLMs are
often evaluated in terms of predictive accuracy, factuality, or alignment, this
structural perspective suggests that their most characteristic behavior is the
production of internally resonant linguistic patterns. Just as quasicrystals
forced a redefinition of order in physical systems, viewing LLMs as generators
of quasi-structured language opens new paths for evaluation and design:
privileging propagation of constraint over token-level accuracy, and coherence
of form over fixed meaning. LLM outputs should be read not only for what they
say, but for the patterns of constraint and coherence that organize them. This
shift reframes generative language as a space of emergent patterning: LLMs are
neither fully random nor strictly rule-based, but defined by a logic of
constraint, resonance, and structural depth.

</details>


### [20] [Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS](https://arxiv.org/abs/2504.12052)
*François Haguinet,Jeffery L Painter,Gregory E Powell,Andrea Callegaro,Andrew Bate*

Main category: cs.CL

TL;DR: The paper introduces a Bayesian dynamic borrowing (BDB) approach, IC SSM, to improve adverse event identification in spontaneous reporting systems by leveraging semantic similarity measures and Bayesian hierarchical modeling.


<details>
  <summary>Details</summary>
Motivation: Current disproportionality analysis (DPA) methods rely on rigid hierarchical grouping, limiting their effectiveness. The study aims to enhance AE identification by incorporating continuous similarity-based borrowing.

Method: The IC SSM approach integrates meta-analytic predictive priors and semantic similarity measures within a Bayesian hierarchical model, evaluated against traditional IC and HLGT-based methods using FAERS data (2015-2019).

Result: IC SSM outperformed traditional methods in sensitivity and early signal detection, identifying more true positives and detecting signals sooner, despite minor trade-offs in F1 scores and Youden's index.

Conclusion: The study advocates for SSM-informed Bayesian borrowing as a scalable improvement to DPA, suggesting future validation across datasets and exploration of additional similarity metrics.

Abstract: We present a Bayesian dynamic borrowing (BDB) approach to enhance the
quantitative identification of adverse events (AEs) in spontaneous reporting
systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior
within a Bayesian hierarchical model and incorporates semantic similarity
measures (SSMs) to enable weighted information sharing from MedDRA Preferred
Terms (PTs) that are clinical similar to the target PT. This continuous
similarity-based borrowing addresses limitation of rigid hierarchical grouping
in current disproportionality analysis (DPA).
  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015
and 2019, we evalute this approach - termed IC SSM - against standard
Information Component (IC) analysis and IC with borrowing at the MedDRA
high-level group term (HLGT) level. A novel references set (PVLens), derived
from FDA product label updates, enabled prospective evaluation of method
performance in identifying AEs prior to official labeling.
  The IC SSM approach demonstrated improved sensitivity compared to both
traditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and
Youden's index. IC SSM consistently identified more true positives and detected
signals over 5 months sooner than traditional IC. Despite a marginally lower
aggregate Youden's index, IC SSM showed higher performance in the early
post-marketing period, providing more stable and relevant estimates than
HLGT-based borrowing and traditional IC.
  These findings support the use of SSM-informed Bayesian borrowing as a
scalable and context-aware enhancement to traditional DPA methods. Future
research should validate this approach across other datasets and explore
additional similarity metrics and Bayesian inference strategies using
case-level data.

</details>


### [21] [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
*Yumin Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: Proposes a novel method for detecting implicit hate speech using in-context learning, improving precision and reducing biases without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Implicit hate speech detection is challenging due to context, cultural subtleties, and model biases, leading to inconsistent results and misclassifications.

Method: Utilizes in-context learning with adaptive retrieval of demonstrations focusing on similar groups or high similarity scores.

Result: Outperforms current state-of-the-art techniques in experimental evaluations.

Conclusion: The method enhances contextual comprehension and robustness in hate speech detection, addressing key challenges in the field.

Abstract: Hate speech detection is a crucial area of research in natural language
processing, essential for ensuring online community safety. However, detecting
implicit hate speech, where harmful intent is conveyed in subtle or indirect
ways, remains a major challenge. Unlike explicit hate speech, implicit
expressions often depend on context, cultural subtleties, and hidden biases,
making them more challenging to identify consistently. Additionally, the
interpretation of such speech is influenced by external knowledge and
demographic biases, resulting in varied detection results across different
language models. Furthermore, Large Language Models often show heightened
sensitivity to toxic language and references to vulnerable groups, which can
lead to misclassifications. This over-sensitivity results in false positives
(incorrectly identifying harmless statements as hateful) and false negatives
(failing to detect genuinely harmful content). Addressing these issues requires
methods that not only improve detection precision but also reduce model biases
and enhance robustness. To address these challenges, we propose a novel method,
which utilizes in-context learning without requiring model fine-tuning. By
adaptively retrieving demonstrations that focus on similar groups or those with
the highest similarity scores, our approach enhances contextual comprehension.
Experimental results show that our method outperforms current state-of-the-art
techniques. Implementation details and code are available at TBD.

</details>


### [22] [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
*Adil Bahaj,Hamed Rahimi,Mohamed Chetouani,Mounir Ghogho*

Main category: cs.CL

TL;DR: The paper investigates overconfidence in LLMs, focusing on overprecision, and introduces a three-phase framework (generation, refinement, evaluation) to study it. Findings include LLMs' poor calibration for numerical tasks and lack of correlation between interval length and confidence level.


<details>
  <summary>Details</summary>
Motivation: To address biases in existing methods for measuring LLM confidence by studying overprecision, inspired by cognitive science, to better understand and quantify LLM trustworthiness.

Method: A three-phase framework: 1) Generate answers to numerical questions as intervals with imposed confidence levels, 2) Refine answers, and 3) Evaluate and analyze LLM behavior.

Result: LLMs are uncalibrated for numerical tasks, show no correlation between interval length and confidence level, and refinement rarely improves precision.

Conclusion: The study provides new insights into LLM overconfidence and establishes a baseline for future research on overprecision in LLMs.

Abstract: Recently, overconfidence in large language models (LLMs) has garnered
considerable attention due to its fundamental importance in quantifying the
trustworthiness of LLM generation. However, existing approaches prompt the
\textit{black box LLMs} to produce their confidence (\textit{verbalized
confidence}), which can be subject to many biases and hallucinations. Inspired
by a different aspect of overconfidence in cognitive science called
\textit{overprecision}, we designed a framework for its study in black box
LLMs. This framework contains three main phases: 1) generation, 2) refinement
and 3) evaluation. In the generation phase we prompt the LLM to generate
answers to numerical questions in the form of intervals with a certain level of
confidence. This confidence level is imposed in the prompt and not required for
the LLM to generate as in previous approaches. We use various prompting
techniques and use the same prompt multiple times to gauge the effects of
randomness in the generation process. In the refinement phase, answers from the
previous phase are refined to generate better answers. The LLM answers are
evaluated and studied in the evaluation phase to understand its internal
workings. This study allowed us to gain various insights into LLM
overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)
{\color{blue}there is no correlation between the length of the interval and the
imposed confidence level, which can be symptomatic of a a) lack of
understanding of the concept of confidence or b) inability to adjust
self-confidence by following instructions}, {\color{blue}3)} LLM numerical
precision differs depending on the task, scale of answer and prompting
technique {\color{blue}4) Refinement of answers doesn't improve precision in
most cases}. We believe this study offers new perspectives on LLM
overconfidence and serves as a strong baseline for overprecision in LLMs.

</details>


### [23] [Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation](https://arxiv.org/abs/2504.12108)
*Shizhan Cai,Liang Ding,Dacheng Tao*

Main category: cs.CL

TL;DR: A novel watermarking scheme for LLMs improves detectability and text quality using a cumulative watermark entropy threshold, outperforming existing methods by over 80% on datasets like MATH and GSM8K.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about content traceability and misuse in LLMs, while balancing text quality and robust detection against attacks.

Method: Introduces a cumulative watermark entropy threshold, compatible with existing sampling functions, to enhance adaptability.

Result: Significant improvements (over 80%) on datasets like MATH and GSM8K, with high detection accuracy.

Conclusion: The proposed scheme effectively balances detectability and text quality, outperforming current methods.

Abstract: The rapid development of Large Language Models (LLMs) has intensified
concerns about content traceability and potential misuse. Existing watermarking
schemes for sampled text often face trade-offs between maintaining text quality
and ensuring robust detection against various attacks. To address these issues,
we propose a novel watermarking scheme that improves both detectability and
text quality by introducing a cumulative watermark entropy threshold. Our
approach is compatible with and generalizes existing sampling functions,
enhancing adaptability. Experimental results across multiple LLMs show that our
scheme significantly outperforms existing methods, achieving over 80\%
improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining
high detection accuracy.

</details>


### [24] [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
*Miguel Moura Ramos,Patrick Fernandes,Sweta Agrawal,André F. T. Martins*

Main category: cs.CL

TL;DR: The paper proposes a method to enhance LLM-based document-level translation by fine-tuning on curated DocBlocks, improving cross-sentence dependencies and translation quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scaling LLMs to document-level translation, particularly in handling long-range dependencies and discourse phenomena.

Method: Targeted fine-tuning on high-quality document-level data (DocBlocks), supporting multiple translation paradigms (direct document-to-document and chunk-level) with contextual instructions.

Result: Improved document-level translation quality and inference speed compared to prompting and agent-based methods.

Conclusion: The proposed method effectively enhances LLM performance in document-level translation by leveraging multiple paradigms and curated data.

Abstract: Large language models (LLMs) have demonstrated strong performance in
sentence-level machine translation, but scaling to document-level translation
remains challenging, particularly in modeling long-range dependencies and
discourse phenomena across sentences and paragraphs. In this work, we propose a
method to improve LLM-based long-document translation through targeted
fine-tuning on high-quality document-level data, which we curate and introduce
as DocBlocks. Our approach supports multiple translation paradigms, including
direct document-to-document and chunk-level translation, by integrating
instructions both with and without surrounding context. This enables models to
better capture cross-sentence dependencies while maintaining strong
sentence-level translation performance. Experimental results show that
incorporating multiple translation paradigms improves document-level
translation quality and inference speed compared to prompting and agent-based
methods.

</details>


### [25] [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
*Maged S. Al-Shaibani,Zaid Alyafeai,Irfan Ahmad*

Main category: cs.CL

TL;DR: A framework for automatic identification of Arabic poetry meters in recited poems, integrating high-resource systems to address low-resource challenges, with a published benchmark for future research.


<details>
  <summary>Details</summary>
Motivation: Arabic poetry's rhythmic structure (meter) is complex and requires technical knowledge. Automating meter identification for recited poetry is challenging due to limited labeled data.

Method: Proposes a state-of-the-art framework integrating two high-resource systems to tackle the low-resource task of meter identification in recited Arabic poetry.

Result: Developed a framework for meter identification and published a benchmark to support future research.

Conclusion: The study advances automatic meter identification in Arabic poetry and provides a foundation for further research with its benchmark.

Abstract: Arabic poetry is an essential and integral part of Arabic language and
culture. It has been used by the Arabs to spot lights on their major events
such as depicting brutal battles and conflicts. They also used it, as in many
other languages, for various purposes such as romance, pride, lamentation, etc.
Arabic poetry has received major attention from linguistics over the decades.
One of the main characteristics of Arabic poetry is its special rhythmic
structure as opposed to prose. This structure is referred to as a meter.
Meters, along with other poetic characteristics, are intensively studied in an
Arabic linguistic field called "\textit{Aroud}". Identifying these meters for a
verse is a lengthy and complicated process. It also requires technical
knowledge in \textit{Aruod}. For recited poetry, it adds an extra layer of
processing. Developing systems for automatic identification of poem meters for
recited poems need large amounts of labelled data. In this study, we propose a
state-of-the-art framework to identify the poem meters of recited Arabic
poetry, where we integrate two separate high-resource systems to perform the
low-resource task. To ensure generalization of our proposed architecture, we
publish a benchmark for this task for future research.

</details>


### [26] [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
*Victor Manuel Hernandez Lopez,Jaime E. Cuellar*

Main category: cs.CL

TL;DR: The paper analyzes Spanish-language YouTube comments on the Hamas-Israel conflict using NLP and BERT, revealing a shift in public opinion influenced by media coverage.


<details>
  <summary>Details</summary>
Motivation: To understand public opinion dynamics and media influence on the Hamas-Israel conflict through computational and social science methods.

Method: Combines STS and NLP (BERT) to classify 253,925 YouTube comments into seven categories, applying agenda-setting theory.

Result: Pro-Palestinian comments were most common, but pro-Israeli and anti-Palestinian ones got more likes. Media coverage shifted public opinion.

Conclusion: Highlights the value of integrating computational tools with social theories to analyze public opinion and media narratives.

Abstract: This article analyzes the Hamas-Israel controversy through 253,925
Spanish-language YouTube comments posted between October 2023 and January 2024,
following the October 7 attack that escalated the conflict. Adopting an
interdisciplinary approach, the study combines the analysis of controversies
from Science and Technology Studies (STS) with advanced computational
methodologies, specifically Natural Language Processing (NLP) using the BERT
(Bidirectional Encoder Representations from Transformers) model. Using this
approach, the comments were automatically classified into seven categories,
reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli
positions, among others. The results show a predominance of pro- Palestinian
comments, although pro-Israeli and anti-Palestinian comments received more
"likes." This study also applies the agenda-setting theory to demonstrate how
media coverage significantly influences public perception, observing a notable
shift in public opinion, transitioning from a pro- Palestinian stance to a more
critical position towards Israel. This work highlights the importance of
combining social science perspectives with technological tools in the analysis
of controversies, presenting a methodological innovation by integrating
computational analysis with critical social theories to address complex public
opinion phenomena and media narratives.

</details>


### [27] [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
*Jaime E. Cuellar,Oscar Moreno-Martinez,Paula Sofia Torres-Rodriguez,Jaime Andres Pavlich-Mariscal,Andres Felipe Mican-Castiblanco,Juan Guillermo Torres-Hurtado*

Main category: cs.CL

TL;DR: The study examines how minor prompt variations affect GPT-4o mini's sentiment classification, revealing inconsistencies and vulnerabilities in the model's robustness.


<details>
  <summary>Details</summary>
Motivation: To assess trust in complex predictive models like ChatGPT by testing their sensitivity to prompt changes in sentiment analysis.

Method: Used 100,000 Spanish comments on Latin American presidents, classified 10 times with varied prompts, followed by exploratory and confirmatory analyses.

Result: Minor prompt changes led to inconsistent classifications, including mixed categories and hallucinations, with statistical significance in most cases.

Conclusion: Large Language Models' trustworthiness is questioned due to sensitivity to prompts, emphasizing the need for structured grammar and social-institutional trust.

Abstract: One fundamental question for the social sciences today is: how much can we
trust highly complex predictive models like ChatGPT? This study tests the
hypothesis that subtle changes in the structure of prompts do not produce
significant variations in the classification results of sentiment polarity
analysis generated by the Large Language Model GPT-4o mini. Using a dataset of
100.000 comments in Spanish on four Latin American presidents, the model
classified the comments as positive, negative, or neutral on 10 occasions,
varying the prompts slightly each time. The experimental methodology included
exploratory and confirmatory analyses to identify significant discrepancies
among classifications.
  The results reveal that even minor modifications to prompts such as lexical,
syntactic, or modal changes, or even their lack of structure impact the
classifications. In certain cases, the model produced inconsistent responses,
such as mixing categories, providing unsolicited explanations, or using
languages other than Spanish. Statistical analysis using Chi-square tests
confirmed significant differences in most comparisons between prompts, except
in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models
for classification tasks, highlighting their vulnerability to variations in
instructions. Moreover, it was evident that the lack of structured grammar in
prompts increases the frequency of hallucinations. The discussion underscores
that trust in Large Language Models is based not only on technical performance
but also on the social and institutional relationships underpinning their use.

</details>


### [28] [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
*Suyoung Bae,Hyojun Kim,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CL

TL;DR: SALAD improves NLP model robustness by generating structure-aware and counterfactual data for contrastive learning, reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning PLMs often leads to spurious correlations, harming performance on out-of-distribution data.

Method: SALAD uses tagging for structure-aware positives and LLMs for counterfactual negatives, applying contrastive learning.

Result: SALAD boosts robustness and performance in Sentiment Classification, Sexism Detection, and NLI, including cross-domain scenarios.

Conclusion: SALAD effectively mitigates spurious correlations, enhancing generalization and robustness in NLP tasks.

Abstract: In various natural language processing (NLP) tasks, fine-tuning Pre-trained
Language Models (PLMs) often leads to the issue of spurious correlations, which
negatively impacts performance, particularly when dealing with
out-of-distribution data. To address this problem, we propose SALAD}(Structure
Aware and LLM-driven Augmented Data), a novel approach designed to enhance
model robustness and generalization by generating structure-aware and
counterfactually augmented data for contrastive learning. Our method leverages
a tagging-based approach to generate structure-aware positive samples and
utilizes large language models (LLMs) to generate counterfactual negative
samples with diverse sentence patterns. By applying contrastive learning, SALAD
enables the model to focus on learning the structural relationships between key
sentence components while minimizing reliance on spurious correlations. We
validate our approach through experiments on three tasks: Sentiment
Classification, Sexism Detection, and Natural Language Inference. The results
demonstrate that SALAD not only improves model robustness and performance
across different environments but also enhances generalization to
out-of-distribution datasets and cross-domain scenarios.

</details>


### [29] [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
*Céline Budding*

Main category: cs.CL

TL;DR: The paper argues that LLMs can acquire tacit knowledge, contrary to Davies' view, by meeting semantic, syntactic, and causal constraints.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that LLMs inherently 'know' language or facts, and to explore whether they can possess tacit knowledge.

Method: Analyzes LLM architectures against Davies' criteria for tacit knowledge (semantic description, syntactic structure, causal systematicity).

Result: Demonstrates that LLMs satisfy the constraints for tacit knowledge, suggesting they can acquire it.

Conclusion: Tacit knowledge provides a useful framework for understanding and intervening in LLM behavior.

Abstract: It is sometimes assumed that Large Language Models (LLMs) know language, or
for example that they know that Paris is the capital of France. But what -- if
anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire
tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself
denies that neural networks can acquire tacit knowledge, I demonstrate that
certain architectural features of LLMs satisfy the constraints of semantic
description, syntactic structure, and causal systematicity. Thus, tacit
knowledge may serve as a conceptual framework for describing, explaining, and
intervening on LLMs and their behavior.

</details>


### [30] [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
*Siyan Zhao,Devaansh Gupta,Qinqing Zheng,Aditya Grover*

Main category: cs.CL

TL;DR: The paper introduces d1, a framework to adapt pre-trained masked diffusion-based LLMs (dLLMs) for reasoning tasks using supervised finetuning (SFT) and RL, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether diffusion-based LLMs (dLLMs) can leverage advances in reasoning like autoregressive LLMs, given their competitive language modeling performance.

Method: Proposes d1, combining masked SFT for knowledge distillation and a novel critic-free RL algorithm (diffu-GRPO) to enhance reasoning in dLLMs.

Result: d1 significantly improves reasoning performance of a state-of-the-art dLLM on mathematical and logical benchmarks.

Conclusion: The framework successfully adapts dLLMs for reasoning, demonstrating their potential beyond traditional autoregressive paradigms.

Abstract: Recent large language models (LLMs) have demonstrated strong reasoning
capabilities that benefits from online reinforcement learning (RL). These
capabilities have primarily been demonstrated within the left-to-right
autoregressive (AR) generation paradigm. In contrast, non-autoregressive
paradigms based on diffusion generate text in a coarse-to-fine manner. Although
recent diffusion-based large language models (dLLMs) have achieved competitive
language modeling performance compared to their AR counterparts, it remains
unclear if dLLMs can also leverage recent advances in LLM reasoning. To this
end, we propose d1, a framework to adapt pre-trained masked dLLMs into
reasoning models via a combination of supervised finetuning (SFT) and RL.
Specifically, we develop and extend techniques to improve reasoning in
pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge
and instill self-improvement behavior directly from existing datasets, and (b)
we introduce a novel critic-free, policy-gradient based RL algorithm called
diffu-GRPO. Through empirical studies, we investigate the performance of
different post-training recipes on multiple mathematical and logical reasoning
benchmarks. We find that d1 yields the best performance and significantly
improves performance of a state-of-the-art dLLM.

</details>


### [31] [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
*Shuming Ma,Hongyu Wang,Shaohan Huang,Xingxing Zhang,Ying Hu,Ting Song,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: BitNet b1.58 2B4T is the first open-source 1-bit LLM with 2B parameters, trained on 4T tokens, matching full-precision models in performance while being more efficient.


<details>
  <summary>Details</summary>
Motivation: To create a highly efficient LLM with reduced computational costs without sacrificing performance.

Method: Training a 1-bit LLM (BitNet b1.58 2B4T) on 4 trillion tokens and evaluating it across multiple benchmarks.

Result: Achieves performance comparable to full-precision LLMs with significant efficiency gains in memory, energy, and latency.

Conclusion: BitNet b1.58 2B4T is a viable, efficient alternative to traditional LLMs, with open-source availability for broader adoption.

Abstract: We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large
Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4
trillion tokens, the model has been rigorously evaluated across benchmarks
covering language understanding, mathematical reasoning, coding proficiency,
and conversational ability. Our results demonstrate that BitNet b1.58 2B4T
achieves performance on par with leading open-weight, full-precision LLMs of
similar size, while offering significant advantages in computational
efficiency, including substantially reduced memory footprint, energy
consumption, and decoding latency. To facilitate further research and adoption,
the model weights are released via Hugging Face along with open-source
inference implementations for both GPU and CPU architectures.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
*Tobias Hallmen,Robin-Nico Kampa,Fabian Deuser,Norbert Oswald,Elisabeth André*

Main category: cs.CV

TL;DR: A multimodal approach using Wav2Vec 2.0, BERT-like encoder, ViT, and LSTM improves performance in BAH and EMI challenges by integrating audio, text, and visual modalities.


<details>
  <summary>Details</summary>
Motivation: To enhance recognition of behavioral ambivalence/hesitancy and emotional mimicry intensity by leveraging multimodal data (audio, text, vision) for richer contextual understanding.

Method: Combines Wav2Vec 2.0 for audio features, a BERT-like encoder for text, ViT for vision, and LSTM for temporal modeling, integrating all modalities.

Result: Significant performance improvements over baseline methods in both tasks.

Conclusion: Multimodal integration (audio, text, vision) is effective for behavioral and emotional analysis, outperforming unimodal approaches.

Abstract: In this study, we present our methodology for two tasks: the Behavioural
Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry
Intensity (EMI) Estimation Challenge, both conducted as part of the 8th
Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building
on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast
dataset to extract various audio features, capturing both linguistic and
paralinguistic information. Our approach incorporates a
valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like
encoder, and a vision transformer (ViT) with predictions subsequently processed
through a long short-term memory (LSTM) architecture for temporal modeling. In
this iteration, we integrate the textual and visual modality into our analysis,
recognizing that semantic content provides valuable contextual cues and
underscoring that the meaning of speech often conveys more critical insights
than its acoustic counterpart alone. Fusing in the vision modality helps in
some cases to interpret the textual modality more precisely. This combined
approach yields significant performance improvements over baseline methods.

</details>


### [33] [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
*Kangsheng Wang,Chengwei Ye,Huanzhen Zhang,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: A multimodal framework combining visual, audio, and text features for personality prediction in short videos, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Automating personality trait prediction is challenging; this paper addresses it by leveraging multimodal data for richer analysis.

Method: Uses facial graphs, Geo-based two-stream networks (GCN, CNN), ResNet18, VGGFace, BiGRU with temporal attention, VGGish for audio, XLM-Roberta for text, and MLP for regression.

Result: The framework outperforms state-of-the-art methods in personality trait prediction.

Conclusion: The proposed multimodal approach effectively integrates diverse features for accurate personality analysis.

Abstract: Predicting personality traits automatically has become a challenging problem
in computer vision. This paper introduces an innovative multimodal feature
learning framework for personality analysis in short video clips. For visual
processing, we construct a facial graph and design a Geo-based two-stream
network incorporating an attention mechanism, leveraging both Graph
Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture
static facial expressions. Additionally, ResNet18 and VGGFace networks are
employed to extract global scene and facial appearance features at the frame
level. To capture dynamic temporal information, we integrate a BiGRU with a
temporal attention module for extracting salient frame representations. To
enhance the model's robustness, we incorporate the VGGish CNN for audio-based
features and XLM-Roberta for text-based features. Finally, a multimodal channel
attention mechanism is introduced to integrate different modalities, and a
Multi-Layer Perceptron (MLP) regression model is used to predict personality
traits. Experimental results confirm that our proposed framework surpasses
existing state-of-the-art approaches in performance.

</details>


### [34] [MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition](https://arxiv.org/abs/2504.11467)
*Qianxue Zhang,Eiman Kanjo*

Main category: cs.CV

TL;DR: A novel TinyML-based livestock monitoring system combines accelerometer and vision data for efficient, real-time animal activity recognition, achieving significant model size reduction and low latency.


<details>
  <summary>Details</summary>
Motivation: To enhance farming efficiency and productivity by transitioning from labor-intensive practices to automated, AI-powered livestock monitoring, especially in remote areas with poor connectivity.

Method: Leverages TinyML, wireless communication, and microcontrollers to fuse accelerometer and vision data for multi-modal tasks (image classification, object detection, behavior recognition).

Result: Demonstrates 270× model size reduction, <80ms latency, and performance comparable to existing methods, enabling real-time inference on microcontrollers.

Conclusion: The system provides a robust, scalable IoT-edge solution adaptable to diverse farming needs, with potential for future extensions.

Abstract: The advancement of technology has revolutionised the agricultural industry,
transitioning it from labour-intensive farming practices to automated,
AI-powered management systems. In recent years, more intelligent livestock
monitoring solutions have been proposed to enhance farming efficiency and
productivity. This work presents a novel approach to animal activity
recognition and movement tracking, leveraging tiny machine learning (TinyML)
techniques, wireless communication framework, and microcontroller platforms to
develop an efficient, cost-effective livestock sensing system. It collects and
fuses accelerometer data and vision inputs to build a multi-modal network for
three tasks: image classification, object detection, and behaviour recognition.
The system is deployed and evaluated on commercial microcontrollers for
real-time inference using embedded applications, demonstrating up to
270$\times$ model size reduction, less than 80ms response latency, and on-par
performance comparable to existing methods. The incorporation of the TinyML
technique allows for seamless data transmission between devices, benefiting use
cases in remote locations with poor Internet connectivity. This work delivers a
robust, scalable IoT-edge livestock monitoring solution adaptable to diverse
farming needs, offering flexibility for future extensions.

</details>


### [35] [Interpreting the Linear Structure of Vision-language Model Embedding Spaces](https://arxiv.org/abs/2504.11695)
*Isabel Papadimitriou,Huangyuan Su,Thomas Fel,Naomi Saphra,Sham Kakade,Stephanie Gil*

Main category: cs.CV

TL;DR: The paper investigates the organization of language and images in vision-language models' joint space using sparse autoencoders (SAEs). It finds SAEs reconstruct embeddings better and retain sparsity, with stable key concepts across runs. Unimodal concepts often encode cross-modal semantics, and the Bridge Score metric identifies concept pairs supporting cross-modal integration.


<details>
  <summary>Details</summary>
Motivation: To understand how vision-language models encode meaning and modality in their joint space and explore the structure of their embeddings.

Method: Train and analyze sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, AIMv2). Introduce the Bridge Score to quantify cross-modal concept alignment.

Result: SAEs outperform other linear feature learning methods in reconstruction and sparsity. Key concepts are stable across runs, while rare ones vary. Unimodal concepts often encode cross-modal semantics, and the Bridge Score reveals latent bridges for cross-modal integration.

Conclusion: The study reveals a sparse linear structure in VLM embedding spaces shaped by modality but connected through latent bridges, offering insights into multimodal meaning construction.

Abstract: Vision-language models encode images and text in a joint space, minimizing
the distance between corresponding image and text pairs. How are language and
images organized in this joint space, and how do the models encode meaning and
modality? To investigate this, we train and release sparse autoencoders (SAEs)
on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,
and AIMv2). SAEs approximate model embeddings as sparse linear combinations of
learned directions, or "concepts". We find that, compared to other methods of
linear feature learning, SAEs are better at reconstructing the real embeddings,
while also able to retain the most sparsity. Retraining SAEs with different
seeds or different data diet leads to two findings: the rare, specific concepts
captured by the SAEs are liable to change drastically, but we also show that
the key commonly-activating concepts extracted by SAEs are remarkably stable
across runs. Interestingly, while most concepts are strongly unimodal in
activation, we find they are not merely encoding modality per se. Many lie
close to - but not entirely within - the subspace defining modality, suggesting
that they encode cross-modal semantics despite their unimodal usage. To
quantify this bridging behavior, we introduce the Bridge Score, a metric that
identifies concept pairs which are both co-activated across aligned image-text
inputs and geometrically aligned in the shared space. This reveals that even
unimodal concepts can collaborate to support cross-modal integration. We
release interactive demos of the SAEs for all models, allowing researchers to
explore the organization of the concept spaces. Overall, our findings uncover a
sparse linear structure within VLM embedding spaces that is shaped by modality,
yet stitched together through latent bridges-offering new insight into how
multimodal meaning is constructed.

</details>


### [36] [SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection](https://arxiv.org/abs/2504.11470)
*Huaxiang Zhang,Hao Zhang,Aoran Mei,Zhongxue Gan,Guo-Niu Zhu*

Main category: cs.CV

TL;DR: SO-DETR improves small object detection by integrating a dual-domain hybrid encoder, enhanced query selection, and knowledge distillation, outperforming existing methods on VisDrone-2019-DET and UAVVaste datasets.


<details>
  <summary>Details</summary>
Motivation: Existing encoders and query selection strategies struggle with small object detection, prompting the need for an efficient solution.

Method: SO-DETR uses a dual-domain hybrid encoder, enhanced query selection, and knowledge distillation to improve small object detection.

Result: SO-DETR outperforms existing methods on VisDrone-2019-DET and UAVVaste datasets with similar computational demands.

Conclusion: SO-DETR effectively addresses small object detection challenges, offering a computationally efficient solution.

Abstract: Detection Transformer-based methods have achieved significant advancements in
general object detection. However, challenges remain in effectively detecting
small objects. One key difficulty is that existing encoders struggle to
efficiently fuse low-level features. Additionally, the query selection
strategies are not effectively tailored for small objects. To address these
challenges, this paper proposes an efficient model, Small Object Detection
Transformer (SO-DETR). The model comprises three key components: a dual-domain
hybrid encoder, an enhanced query selection mechanism, and a knowledge
distillation strategy. The dual-domain hybrid encoder integrates spatial and
frequency domains to fuse multi-scale features effectively. This approach
enhances the representation of high-resolution features while maintaining
relatively low computational overhead. The enhanced query selection mechanism
optimizes query initialization by dynamically selecting high-scoring anchor
boxes using expanded IoU, thereby improving the allocation of query resources.
Furthermore, by incorporating a lightweight backbone network and implementing a
knowledge distillation strategy, we develop an efficient detector for small
objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets
demonstrate that SO-DETR outperforms existing methods with similar
computational demands. The project page is available at
https://github.com/ValiantDiligent/SO_DETR.

</details>


### [37] [Towards Realistic Low-Light Image Enhancement via ISP Driven Data Modeling](https://arxiv.org/abs/2504.12204)
*Zhihua Wang,Yu Long,Qinghua Lin,Kai Zhang,Yazhu Zhang,Yuming Fang,Li Liu,Xiaochun Cao*

Main category: cs.CV

TL;DR: A novel ISP-driven data synthesis pipeline generates diverse low-light training data, improving DNN performance for low-light image enhancement.


<details>
  <summary>Details</summary>
Motivation: Current DNNs for low-light image enhancement suffer from issues like noise amplification and unnatural enhancements due to limited training data diversity.

Method: Proposes a data synthesis pipeline: reverse ISP to RAW, RAW-domain degradations, and varied ISP processing stages. Trains a vanilla UNet with this data.

Result: The UNet trained with synthetic data outperforms SOTA methods in fidelity and visual quality.

Conclusion: The synthetic pipeline effectively addresses data diversity challenges, enhancing DNN performance for low-light image enhancement.

Abstract: Deep neural networks (DNNs) have recently become the leading method for
low-light image enhancement (LLIE). However, despite significant progress,
their outputs may still exhibit issues such as amplified noise, incorrect white
balance, or unnatural enhancements when deployed in real world applications. A
key challenge is the lack of diverse, large scale training data that captures
the complexities of low-light conditions and imaging pipelines. In this paper,
we propose a novel image signal processing (ISP) driven data synthesis pipeline
that addresses these challenges by generating unlimited paired training data.
Specifically, our pipeline begins with easily collected high-quality
normal-light images, which are first unprocessed into the RAW format using a
reverse ISP. We then synthesize low-light degradations directly in the RAW
domain. The resulting data is subsequently processed through a series of ISP
stages, including white balance adjustment, color space conversion, tone
mapping, and gamma correction, with controlled variations introduced at each
stage. This broadens the degradation space and enhances the diversity of the
training data, enabling the generated data to capture a wide range of
degradations and the complexities inherent in the ISP pipeline. To demonstrate
the effectiveness of our synthetic pipeline, we conduct extensive experiments
using a vanilla UNet model consisting solely of convolutional layers, group
normalization, GeLU activation, and convolutional block attention modules
(CBAMs). Extensive testing across multiple datasets reveals that the vanilla
UNet model trained with our data synthesis pipeline delivers high fidelity,
visually appealing enhancement results, surpassing state-of-the-art (SOTA)
methods both quantitatively and qualitatively.

</details>


### [38] [High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving](https://arxiv.org/abs/2504.11472)
*Kebin Contreras,Brayan Monroy,Jorge Bacca*

Main category: cs.CV

TL;DR: The paper proposes using modulo sensors for robust object detection in autonomous driving, addressing lighting variations and saturation issues. It achieves HDR-like quality with faster processing than traditional HDR methods.


<details>
  <summary>Details</summary>
Motivation: Real-world lighting variations cause image saturation, losing crucial details for object detection in autonomous driving. Traditional HDR methods are inefficient for real-time use.

Method: Modulo sensors reset upon saturation, capturing irradiance encoding images. Unwrapping algorithms reconstruct HDR-like images, preserving details under extreme lighting.

Result: Modulo-processed images match HDR performance in object detection (tested with YOLOv10) and outperform saturated images. Processing time is shorter than conventional HDR.

Conclusion: Modulo sensors offer a practical solution for real-time HDR-like imaging in autonomous driving, improving detection accuracy under challenging lighting.

Abstract: Object detection precision is crucial for ensuring the safety and efficacy of
autonomous driving systems. The quality of acquired images directly influences
the ability of autonomous driving systems to correctly recognize and respond to
other vehicles, pedestrians, and obstacles in real-time. However, real
environments present extreme variations in lighting, causing saturation
problems and resulting in the loss of crucial details for detection.
Traditionally, High Dynamic Range (HDR) images have been preferred for their
ability to capture a broad spectrum of light intensities, but the need for
multiple captures to construct HDR images is inefficient for real-time
applications in autonomous vehicles. To address these issues, this work
introduces the use of modulo sensors for robust object detection. The modulo
sensor allows pixels to `reset/wrap' upon reaching saturation level by
acquiring an irradiance encoding image which can then be recovered using
unwrapping algorithms. The applied reconstruction techniques enable HDR
recovery of color intensity and image details, ensuring better visual quality
even under extreme lighting conditions at the cost of extra time. Experiments
with the YOLOv10 model demonstrate that images processed using modulo images
achieve performance comparable to HDR images and significantly surpass
saturated images in terms of object detection accuracy. Moreover, the proposed
modulo imaging step combined with HDR image reconstruction is shorter than the
time required for conventional HDR image acquisition.

</details>


### [39] [Visual moral inference and communication](https://arxiv.org/abs/2504.11473)
*Warren Zhu,Aida Ramezani,Yang Xu*

Main category: cs.CV

TL;DR: A framework for moral inference from images, outperforming text-only models, reveals biases in news media.


<details>
  <summary>Details</summary>
Motivation: Morality is conveyed beyond text, but AI lacks multimodal moral inference.

Method: A computational framework for moral inference from images, tested on human judgment and news data.

Result: Language-vision fusion models outperform text-only models in visual moral inference; biases found in news.

Conclusion: Enables automated visual moral inference and reveals patterns in public media.

Abstract: Humans can make moral inferences from multiple sources of input. In contrast,
automated moral inference in artificial intelligence typically relies on
language models with textual input. However, morality is conveyed through
modalities beyond language. We present a computational framework that supports
moral inference from natural images, demonstrated in two related tasks: 1)
inferring human moral judgment toward visual images and 2) analyzing patterns
in moral content communicated via images from public news. We find that models
based on text alone cannot capture the fine-grained human moral judgment toward
visual stimuli, but language-vision fusion models offer better precision in
visual moral inference. Furthermore, applications of our framework to news data
reveal implicit biases in news categories and geopolitical discussions. Our
work creates avenues for automating visual moral inference and discovering
patterns of visual moral communication in public media.

</details>


### [40] [SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification](https://arxiv.org/abs/2504.11477)
*Yunkai Zhang,Shiyin Wei,Yong Huang,Yawu Su,Shanshan Lu,Hui Li*

Main category: cs.CV

TL;DR: SDIGLM, a large multi-modal model (LMM), improves structural damage identification by combining visual and textual data, achieving 95.24% accuracy and detailed damage descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing CV-based models lack comprehensive damage recognition and linguistic capabilities, limiting their practical use in civil engineering.

Method: SDIGLM integrates a U-Net-based semantic segmentation module for visual Chain of Thought (CoT) and uses a multi-round dialogue dataset for fine-tuning.

Result: The model achieves 95.24% accuracy in damage identification and effectively describes damage characteristics.

Conclusion: SDIGLM demonstrates superior performance in structural damage analysis, offering a practical solution for civil engineering applications.

Abstract: Existing computer vision(CV)-based structural damage identification models
demonstrate notable accuracy in categorizing and localizing damage. However,
these models present several critical limitations that hinder their practical
application in civil engineering(CE). Primarily, their ability to recognize
damage types remains constrained, preventing comprehensive analysis of the
highly varied and complex conditions encountered in real-world CE structures.
Second, these models lack linguistic capabilities, rendering them unable to
articulate structural damage characteristics through natural language
descriptions. With the continuous advancement of artificial intelligence(AI),
large multi-modal models(LMMs) have emerged as a transformative solution,
enabling the unified encoding and alignment of textual and visual data. These
models can autonomously generate detailed descriptive narratives of structural
damage while demonstrating robust generalization across diverse scenarios and
tasks. This study introduces SDIGLM, an innovative LMM for structural damage
identification, developed based on the open-source VisualGLM-6B architecture.
To address the challenge of adapting LMMs to the intricate and varied operating
conditions in CE, this work integrates a U-Net-based semantic segmentation
module to generate defect segmentation maps as visual Chain of Thought(CoT).
Additionally, a multi-round dialogue fine-tuning dataset is constructed to
enhance logical reasoning, complemented by a language CoT formed through prompt
engineering. By leveraging this multi-modal CoT, SDIGLM surpasses
general-purpose LMMs in structural damage identification, achieving an accuracy
of 95.24% across various infrastructure types. Moreover, the model effectively
describes damage characteristics such as hole size, crack direction, and
corrosion severity.

</details>


### [41] [Flux Already Knows - Activating Subject-Driven Image Generation without Training](https://arxiv.org/abs/2504.11478)
*Hao Kang,Stathi Fotiadis,Liming Jiang,Qing Yan,Yumin Jia,Zichuan Liu,Min Jin Chong,Xin Lu*

Main category: cs.CV

TL;DR: A zero-shot framework using a vanilla Flux model for subject-driven image generation, achieving high fidelity without extra training or data.


<details>
  <summary>Details</summary>
Motivation: To enable resource-efficient, high-quality subject-driven image generation without additional training or fine-tuning.

Method: Grid-based image completion with mosaic layout, cascade attention design, and meta prompting.

Result: Outperforms baselines in benchmarks and human studies, supports diverse edits like logo insertion and virtual try-on.

Conclusion: Pre-trained models can enable lightweight, high-quality customization for downstream applications.

Abstract: We propose a simple yet effective zero-shot framework for subject-driven
image generation using a vanilla Flux model. By framing the task as grid-based
image completion and simply replicating the subject image(s) in a mosaic
layout, we activate strong identity-preserving capabilities without any
additional data, training, or inference-time fine-tuning. This "free lunch"
approach is further strengthened by a novel cascade attention design and meta
prompting technique, boosting fidelity and versatility. Experimental results
show that our method outperforms baselines across multiple key metrics in
benchmarks and human preference studies, with trade-offs in certain aspects.
Additionally, it supports diverse edits, including logo insertion, virtual
try-on, and subject replacement or insertion. These results demonstrate that a
pre-trained foundational text-to-image model can enable high-quality,
resource-efficient subject-driven generation, opening new possibilities for
lightweight customization in downstream applications.

</details>


### [42] [snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing](https://arxiv.org/abs/2504.11482)
*Vidya Sudevan,Fakhreddine Zayer,Rizwana Kausar,Sajid Javed,Hamad Karki,Giulia De Masi,Jorge Dias*

Main category: cs.CV

TL;DR: snnTrans-DHZ is a lightweight SNN for underwater image dehazing, achieving high efficiency and performance with low power consumption.


<details>
  <summary>Details</summary>
Motivation: Underwater image dehazing is essential for marine operations due to visibility issues caused by light scattering and absorption.

Method: Uses a Spiking Neural Network (SNN) to process time-dependent image sequences in LAB color space, with modules for feature extraction, background light estimation, and image reconstruction. Trained via surrogate gradient-based BPTT and a novel loss function.

Result: Achieves PSNR of 21.68 dB (UIEB) and 23.46 dB (EUVP), with SSIM of 0.8795 and 0.8439, respectively. Low energy (0.0151 J) and computational cost (7.42 GSOPs).

Conclusion: snnTrans-DHZ is efficient and effective, making it ideal for underwater robotics and marine applications.

Abstract: Underwater image dehazing is critical for vision-based marine operations
because light scattering and absorption can severely reduce visibility. This
paper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)
specifically designed for underwater dehazing. By leveraging the temporal
dynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image
sequences while maintaining low power consumption. Static underwater images are
first converted into time-dependent sequences by repeatedly inputting the same
image over user-defined timesteps. These RGB sequences are then transformed
into LAB color space representations and processed concurrently. The
architecture features three key modules: (i) a K estimator that extracts
features from multiple color space representations; (ii) a Background Light
Estimator that jointly infers the background light component from the RGB-LAB
images; and (iii) a soft image reconstruction module that produces haze-free,
visibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a
surrogate gradient-based backpropagation through time (BPTT) strategy alongside
a novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ
achieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it
yields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million
network parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the
algorithm significantly outperforms existing state-of-the-art methods in terms
of efficiency. These features make snnTrans-DHZ highly suitable for deployment
in underwater robotics, marine exploration, and environmental monitoring.

</details>


### [43] [Uncovering Branch specialization in InceptionV1 using k sparse autoencoders](https://arxiv.org/abs/2504.11489)
*Matthew Bozoukov*

Main category: cs.CV

TL;DR: SAEs reveal interpretable features in neural networks, with branch specialization in InceptionV1's later layers showing consistent patterns across convolution sizes.


<details>
  <summary>Details</summary>
Motivation: To understand and demonstrate branch specialization in later layers of InceptionV1, which remains poorly understood despite SAE improvements.

Method: Analyzed branch specialization in mixed4a-4e, 5x5, and 1x1 branches of InceptionV1 using SAEs.

Result: Found consistent branch specialization patterns across layers, with similar features localized in same-sized convolution branches.

Conclusion: Branch specialization in InceptionV1 is layer-consistent, suggesting a structured feature organization in later layers.

Abstract: Sparse Autoencoders (SAEs) have shown to find interpretable features in
neural networks from polysemantic neurons caused by superposition. Previous
work has shown SAEs are an effective tool to extract interpretable features
from the early layers of InceptionV1. Since then, there have been many
improvements to SAEs but branch specialization is still an enigma in the later
layers of InceptionV1. We show various examples of branch specialization
occuring in each layer of the mixed4a-4e branch, in the 5x5 branch and in one
1x1 branch. We also provide evidence to claim that branch specialization seems
to be consistent across layers, similar features across the model will be
localized in the same convolution size branches in their respective layer.

</details>


### [44] [TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification](https://arxiv.org/abs/2504.11500)
*Kaicong Huang,Talha Azfar,Jack Reilly,Ruimin Ke*

Main category: cs.CV

TL;DR: TransitReID is a novel framework for collecting transit OD data using onboard cameras and visual person re-identification, addressing occlusion and efficiency challenges with innovative algorithms and edge-device optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional transit OD data collection methods are costly, inefficient, or limited in coverage. Onboard cameras offer an opportunity, but occlusion and viewpoint variations hinder accuracy.

Method: TransitReID combines an occlusion-robust ReID algorithm with a Hierarchical Storage and Dynamic Matching mechanism, optimized for edge devices and privacy protection.

Result: The framework achieves ~90% accuracy in bus route simulations, outperforming existing methods.

Conclusion: TransitReID provides an efficient, accurate, and privacy-preserving solution for transit OD data collection, overcoming key challenges in the field.

Abstract: Transit Origin-Destination (OD) data are essential for transit planning,
particularly in route optimization and demand-responsive paratransit systems.
Traditional methods, such as manual surveys, are costly and inefficient, while
Bluetooth and WiFi-based approaches require passengers to carry specific
devices, limiting data coverage. On the other hand, most transit vehicles are
equipped with onboard cameras for surveillance, offering an opportunity to
repurpose them for edge-based OD data collection through visual person
re-identification (ReID). However, such approaches face significant challenges,
including severe occlusion and viewpoint variations in transit environments,
which greatly reduce matching accuracy and hinder their adoption. Moreover,
designing effective algorithms that can operate efficiently on edge devices
remains an open challenge. To address these challenges, we propose TransitReID,
a novel framework for individual-level transit OD data collection. TransitReID
consists of two key components: (1) An occlusion-robust ReID algorithm
featuring a variational autoencoder guided region-attention mechanism that
adaptively focuses on visible body regions through reconstruction
loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic
Matching (HSDM) mechanism specifically designed for efficient and robust
transit OD matching which balances storage, speed, and accuracy. Additionally,
a multi-threaded design supports near real-time operation on edge devices,
which also ensuring privacy protection. We also introduce a ReID dataset
tailored for complex bus environments to address the lack of relevant training
data. Experimental results demonstrate that TransitReID achieves
state-of-the-art performance in ReID tasks, with an accuracy of approximately
90\% in bus route simulations.

</details>


### [45] [ConvShareViT: Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators](https://arxiv.org/abs/2504.11517)
*Riad Ibadulla,Thomas M. Chen,Constantino Carlos Reyes-Aldasoro*

Main category: cs.CV

TL;DR: ConvShareViT adapts Vision Transformers for optical systems by replacing linear layers with shared depthwise convolutions, achieving comparable attention scores and faster inference.


<details>
  <summary>Details</summary>
Motivation: To adapt Vision Transformers for 4f optical systems, leveraging their parallelism and high-resolution capabilities while simplifying the architecture.

Method: Replaces MHSA and MLP linear layers with shared depthwise convolutions, analyzing their effectiveness in learning attention.

Result: Valid-padded shared convolutions achieve comparable attention scores to standard ViTs, while same-padded ones behave like CNNs. Theoretical inference is 3.04x faster than GPUs.

Conclusion: ConvShareViT is promising for optical deep learning, balancing performance and complexity by using only convolutions.

Abstract: This paper introduces ConvShareViT, a novel deep learning architecture that
adapts Vision Transformers (ViTs) to the 4f free-space optical system.
ConvShareViT replaces linear layers in multi-head self-attention (MHSA) and
Multilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared
weights across input channels. Through the development of ConvShareViT, the
behaviour of convolutions within MHSA and their effectiveness in learning the
attention mechanism were analysed systematically. Experimental results
demonstrate that certain configurations, particularly those using valid-padded
shared convolutions, can successfully learn attention, achieving comparable
attention scores to those obtained with standard ViTs. However, other
configurations, such as those using same-padded convolutions, show limitations
in attention learning and operate like regular CNNs rather than transformer
models. ConvShareViT architectures are specifically optimised for the 4f
optical system, which takes advantage of the parallelism and high-resolution
capabilities of optical systems. Results demonstrate that ConvShareViT can
theoretically achieve up to 3.04 times faster inference than GPU-based systems.
This potential acceleration makes ConvShareViT an attractive candidate for
future optical deep learning applications and proves that our ViT
(ConvShareViT) can be employed using only the convolution operation, via the
necessary optimisation of the ViT to balance performance and complexity.

</details>


### [46] [Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey](https://arxiv.org/abs/2504.11588)
*Siteng Ma,Honghui Du,Yu An,Jing Wang,Qinqin Wang,Haochang Wu,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: A survey on deep learning in medical imaging under limited, inexact, or missing labels, reviewing 600+ studies since 2018, covering tasks like classification, segmentation, and detection.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining large, well-annotated medical datasets drive interest in learning paradigms like incomplete, inexact, and absent supervision.

Method: Categorizes and reviews research, establishes relationships among studies, and provides formal definitions and summaries of learning mechanisms.

Result: Comprehensive analysis of learning strategies and their applications in medical imaging tasks.

Conclusion: Identifies future research challenges and aids understanding of the current landscape in limited-label learning for medical imaging.

Abstract: Deep learning has achieved significant breakthroughs in medical imaging, but
these advancements are often dependent on large, well-annotated datasets.
However, obtaining such datasets poses a significant challenge, as it requires
time-consuming and labor-intensive annotations from medical experts.
Consequently, there is growing interest in learning paradigms such as
incomplete, inexact, and absent supervision, which are designed to operate
under limited, inexact, or missing labels. This survey categorizes and reviews
the evolving research in these areas, analyzing around 600 notable
contributions since 2018. It covers tasks such as image classification,
segmentation, and detection across various medical application areas, including
but not limited to brain, chest, and cardiac imaging. We attempt to establish
the relationships among existing research studies in related areas. We provide
formal definitions of different learning paradigms and offer a comprehensive
summary and interpretation of various learning mechanisms and strategies,
aiding readers in better understanding the current research landscape and
ideas. We also discuss potential future research challenges.

</details>


### [47] [Exploring Video-Based Driver Activity Recognition under Noisy Labels](https://arxiv.org/abs/2504.11966)
*Linjuan Fan,Di Wen,Kunyu Peng,Kailun Yang,Jiaming Zhang,Ruiping Liu,Yufan Chen,Junwei Zheng,Jiamin Wu,Xudong Han,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces a label noise learning approach for driver activity recognition, combining clustering, co-refinement, and a flexible sample selection strategy to improve model reliability.


<details>
  <summary>Details</summary>
Motivation: Real-world video data often contains mislabeled samples, which degrade model performance, but label noise learning is underexplored in driver activity recognition.

Method: The method involves learning clustering-friendly representations, co-refining classifier outputs within clusters, and using a hyperparameter-free sample selection strategy with a self-adaptive parameter for class balancing.

Result: The approach outperforms other label-denoising methods on the Drive&Act dataset across all granularity levels.

Conclusion: The proposed method effectively addresses label noise in driver activity recognition, offering superior performance and flexibility.

Abstract: As an open research topic in the field of deep learning, learning with noisy
labels has attracted much attention and grown rapidly over the past ten years.
Learning with label noise is crucial for driver distraction behavior
recognition, as real-world video data often contains mislabeled samples,
impacting model reliability and performance. However, label noise learning is
barely explored in the driver activity recognition field. In this paper, we
propose the first label noise learning approach for the driver activity
recognition task. Based on the cluster assumption, we initially enable the
model to learn clustering-friendly low-dimensional representations from given
videos and assign the resultant embeddings into clusters. We subsequently
perform co-refinement within each cluster to smooth the classifier outputs.
Furthermore, we propose a flexible sample selection strategy that combines two
selection criteria without relying on any hyperparameters to filter clean
samples from the training dataset. We also incorporate a self-adaptive
parameter into the sample selection process to enforce balancing across
classes. A comprehensive variety of experiments on the public Drive&Act dataset
for all granularity levels demonstrates the superior performance of our method
in comparison with other label-denoising methods derived from the image
classification field. The source code is available at
https://github.com/ilonafan/DAR-noisy-labels.

</details>


### [48] [DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization](https://arxiv.org/abs/2504.11637)
*Yiming Xiao,Ali Mostafavi*

Main category: cs.CV

TL;DR: DamageCAT introduces a typology-based framework for building damage assessment, outperforming traditional severity-based methods with a hierarchical U-Net-based transformer model and a new dataset (BD-TypoSAT).


<details>
  <summary>Details</summary>
Motivation: Current damage assessment methods use binary or ordinal severity ratings, which lack practical utility for disaster response. Typology-based descriptions are needed for better decision-making.

Method: The paper presents DamageCAT, using a hierarchical U-Net-based transformer to analyze pre-post disaster satellite images (BD-TypoSAT dataset) and categorize damage into four types.

Result: The model achieved 0.7921 IoU and 0.8835 F1 scores, excelling in recognizing rare damage types despite class imbalances.

Conclusion: DamageCAT provides actionable, typological damage information, improving disaster response and resource allocation over traditional methods.

Abstract: Natural disasters increasingly threaten communities worldwide, creating an
urgent need for rapid, reliable building damage assessment to guide emergency
response and recovery efforts. Current methods typically classify damage in
binary (damaged/undamaged) or ordinal severity terms, limiting their practical
utility. In fact, the determination of damage typology is crucial for response
and recovery efforts. To address this important gap, this paper introduces
DamageCAT, a novel framework that provides typology-based categorical damage
descriptions rather than simple severity ratings. Accordingly, this study
presents two key contributions: (1) the BD-TypoSAT dataset containing satellite
image triplets (pre-disaster, post-disaster, and damage masks) from Hurricane
Ida with four damage categories (partial roof damage, total roof damage,
partial structural collapse, and total structural collapse), and (2) a
hierarchical U-Net-based transformer architecture that effectively processes
pre-post disaster image pairs to identify and categorize building damage.
Despite significant class imbalances in the training data, our model achieved
robust performance with overall metrics of 0.7921 Intersection over Union (IoU)
and 0.8835 F1 scores across all categories. The model's capability to recognize
intricate damage typology in less common categories is especially remarkable.
The DamageCAT framework advances automated damage assessment by providing
actionable, typological information that better supports disaster response
decision-making and resource allocation compared to traditional severity-based
approaches.

</details>


### [49] [A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction](https://arxiv.org/abs/2504.12112)
*Zhenyu Yu,Mohd Yamani Inda Idris,Pei Wang*

Main category: cs.CV

TL;DR: SatelliteMaker is a diffusion-based method for reconstructing missing remote sensing data, ensuring consistency across bands and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Data loss in remote sensing imagery due to cloud cover, sensor failures, or incomplete acquisition limits its effectiveness, especially in high-resolution tasks.

Method: Proposes SatelliteMaker, a diffusion-based method with DEM conditioning and tailored prompts, and introduces a VGG-Adapter module for style consistency.

Result: Achieves state-of-the-art performance in reconstructing missing data while maintaining spatial, spectral, and temporal consistency.

Conclusion: SatelliteMaker effectively addresses data loss challenges in remote sensing, offering a robust solution for quantitative tasks.

Abstract: Remote sensing imagery is essential for environmental monitoring,
agricultural management, and disaster response. However, data loss due to cloud
cover, sensor failures, or incomplete acquisition-especially in high-resolution
and high-frequency tasks-severely limits satellite imagery's effectiveness.
Traditional interpolation methods struggle with large missing areas and complex
structures. Remote sensing imagery consists of multiple bands, each with
distinct meanings, and ensuring consistency across bands is critical to avoid
anomalies in the combined images. This paper proposes SatelliteMaker, a
diffusion-based method that reconstructs missing data across varying levels of
data loss while maintaining spatial, spectral, and temporal consistency. We
also propose Digital Elevation Model (DEM) as a conditioning input and use
tailored prompts to generate realistic images, making diffusion models
applicable to quantitative remote sensing tasks. Additionally, we propose a
VGG-Adapter module based on Distribution Loss, which reduces distribution
discrepancy and ensures style consistency. Extensive experiments show that
SatelliteMaker achieves state-of-the-art performance across multiple tasks.

</details>


### [50] [Real-time Object and Event Detection Service through Computer Vision and Edge Computing](https://arxiv.org/abs/2504.11662)
*Marcos Mendes,Gonçalo Perna,Pedro Rito,Duarte Raposo,Susana Sargento*

Main category: cs.CV

TL;DR: The paper proposes a computer vision and edge computing-based system for road monitoring in smart cities to reduce accidents involving vulnerable road users, achieving real-time detection and collision prediction.


<details>
  <summary>Details</summary>
Motivation: Road traffic crashes cost 3% of global GDP annually, with urban fatal accidents often involving vulnerable road users. Smart cities can leverage technology to address this issue.

Method: The system uses computer vision and edge computing, implemented with vision algorithms and tracking via surveillance cameras in the Aveiro Tech City Living Lab.

Result: The algorithm accurately detects and tracks cars, pedestrians, and bicycles, predicts road state and distances, and infers collision events in near real-time.

Conclusion: The proposed system shows promise for enhancing road safety in smart cities by preventing collisions through real-time monitoring.

Abstract: The World Health Organization suggests that road traffic crashes cost
approximately 518 billion dollars globally each year, which accounts for 3% of
the gross domestic product for most countries. Most fatal road accidents in
urban areas involve Vulnerable Road Users (VRUs). Smart cities environments
present innovative approaches to combat accidents involving cutting-edge
technologies, that include advanced sensors, extensive datasets, Machine
Learning (ML) models, communication systems, and edge computing. This paper
proposes a strategy and an implementation of a system for road monitoring and
safety for smart cities, based on Computer Vision (CV) and edge computing.
Promising results were obtained by implementing vision algorithms and tracking
using surveillance cameras, that are part of a Smart City testbed, the Aveiro
Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars,
pedestrians, and bicycles, while predicting the road state, the distance
between moving objects, and inferring on collision events to prevent
collisions, in near real-time.

</details>


### [51] [Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video Pipeline](https://arxiv.org/abs/2504.12169)
*Joanne Lin,Crispian Morris,Ruirui Lin,Fan Zhang,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: A new Degradation Estimation Network (DEN) generates realistic sRGB noise for low-light images/videos without camera metadata, improving synthetic data quality for tasks like noise replication, video enhancement, and object detection.


<details>
  <summary>Details</summary>
Motivation: Low-light conditions hinder annotation and research, with existing methods relying on unrealistic noise models or limited synthetic data.

Method: DEN estimates physics-informed noise parameters in a self-supervised, zero-shot manner to create diverse, realistic synthetic noise.

Result: Improvements of up to 24% KLD, 21% LPIPS, and 62% AP$_{50-95}$ in noise replication, video enhancement, and object detection tasks.

Conclusion: DEN advances low-light machine understanding by generating realistic synthetic noise without metadata, outperforming existing methods.

Abstract: Low-light conditions pose significant challenges for both human and machine
annotation. This in turn has led to a lack of research into machine
understanding for low-light images and (in particular) videos. A common
approach is to apply annotations obtained from high quality datasets to
synthetically created low light versions. In addition, these approaches are
often limited through the use of unrealistic noise models. In this paper, we
propose a new Degradation Estimation Network (DEN), which synthetically
generates realistic standard RGB (sRGB) noise without the requirement for
camera metadata. This is achieved by estimating the parameters of
physics-informed noise distributions, trained in a self-supervised manner. This
zero-shot approach allows our method to generate synthetic noisy content with a
diverse range of realistic noise characteristics, unlike other methods which
focus on recreating the noise characteristics of the training data. We evaluate
our proposed synthetic pipeline using various methods trained on its synthetic
data for typical low-light tasks including synthetic noise replication, video
enhancement, and object detection, showing improvements of up to 24\% KLD, 21\%
LPIPS, and 62\% AP$_{50-95}$, respectively.

</details>


### [52] [Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation](https://arxiv.org/abs/2504.11669)
*Amirhossein Dadashzadeh,Parsa Esmati,Majid Mirmehdi*

Main category: cs.CV

TL;DR: Co-STAR integrates curriculum learning and collaborative self-training between a teacher model and CLIP to improve pseudo-label quality in SFUVDA, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing noisy pseudo-labels and over-confident predictions in SFUVDA by leveraging vision-language models.

Method: Combines curriculum learning with a reliability-based weight function and Adaptive Curriculum Regularization to balance confident and uncertain predictions.

Result: Co-STAR consistently outperforms existing SFUVDA methods in benchmarks.

Conclusion: The proposed framework effectively mitigates noise and overconfidence, enhancing domain adaptation performance.

Abstract: Recent advances in Source-Free Unsupervised Video Domain Adaptation (SFUVDA)
leverage vision-language models to enhance pseudo-label generation. However,
challenges such as noisy pseudo-labels and over-confident predictions limit
their effectiveness in adapting well across domains. We propose Co-STAR, a
novel framework that integrates curriculum learning with collaborative
self-training between a source-trained teacher and a contrastive
vision-language model (CLIP). Our curriculum learning approach employs a
reliability-based weight function that measures bidirectional prediction
alignment between the teacher and CLIP, balancing between confident and
uncertain predictions. This function preserves uncertainty for difficult
samples, while prioritizing reliable pseudo-labels when the predictions from
both models closely align. To further improve adaptation, we propose Adaptive
Curriculum Regularization, which modifies the learning priority of samples in a
probabilistic, adaptive manner based on their confidence scores and prediction
stability, mitigating overfitting to noisy and over-confident samples.
Extensive experiments across multiple video domain adaptation benchmarks
demonstrate that Co-STAR consistently outperforms state-of-the-art SFUVDA
methods. Code is available at: https://github.com/Plrbear/Co-Star

</details>


### [53] [SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction](https://arxiv.org/abs/2504.12245)
*Xia Wang,Haiyang Sun,Tiantian Cao,Yueying Sun,Min Feng*

Main category: cs.CV

TL;DR: SIDME introduces a self-supervised model for demoiréing by leveraging masked encoder-decoder reconstruction and specialized loss functions for color channels, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional demoiréing methods treat images uniformly, ignoring color channel differences and struggling with real-world moiré variability.

Method: SIDME uses a masked encoder-decoder with self-supervised learning, a random masked reconstructor, and a specialized loss for the green channel.

Result: SIDME excels in real-world moiré pattern processing, showing superior generalization and robustness.

Conclusion: SIDME's innovative approach effectively addresses moiré pattern challenges, offering high-quality image reconstruction.

Abstract: Moir\'e patterns, resulting from aliasing between object light signals and
camera sampling frequencies, often degrade image quality during capture.
Traditional demoir\'eing methods have generally treated images as a whole for
processing and training, neglecting the unique signal characteristics of
different color channels. Moreover, the randomness and variability of moir\'e
pattern generation pose challenges to the robustness of existing methods when
applied to real-world data. To address these issues, this paper presents SIDME
(Self-supervised Image Demoir\'eing via Masked Encoder-Decoder Reconstruction),
a novel model designed to generate high-quality visual images by effectively
processing moir\'e patterns. SIDME combines a masked encoder-decoder
architecture with self-supervised learning, allowing the model to reconstruct
images using the inherent properties of camera sampling frequencies. A key
innovation is the random masked image reconstructor, which utilizes an
encoder-decoder structure to handle the reconstruction task. Furthermore, since
the green channel in camera sampling has a higher sampling frequency compared
to red and blue channels, a specialized self-supervised loss function is
designed to improve the training efficiency and effectiveness. To ensure the
generalization ability of the model, a self-supervised moir\'e image generation
method has been developed to produce a dataset that closely mimics real-world
conditions. Extensive experiments demonstrate that SIDME outperforms existing
methods in processing real moir\'e pattern data, showing its superior
generalization performance and robustness.

</details>


### [54] [Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics](https://arxiv.org/abs/2504.11686)
*Yiran He,Yun Cao,Bowen Yang,Zeyu Zhang*

Main category: cs.CV

TL;DR: The paper explores using multimodal LLMs for detecting AI-generated content, proposing a framework for image authenticity evaluation, localization, and tracing. It achieves competitive accuracy with state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Generative AI makes content manipulation harder to detect, and existing LLMs lack tailored capabilities for forgery analysis.

Method: A framework leveraging prompt engineering and few-shot learning to unlock LLMs' potential in forgery detection.

Result: GPT4V achieves 92.1% accuracy in Autosplice and 86.3% in LaMa, competitive with top AIGC detection methods.

Conclusion: Multimodal LLMs show promise in forgery detection but have limitations; improvements are suggested.

Abstract: The rapid development of generative AI facilitates content creation and makes
image manipulation easier and more difficult to detect. While multimodal Large
Language Models (LLMs) have encoded rich world knowledge, they are not
inherently tailored for combating AI-generated Content (AIGC) and struggle to
comprehend local forgery details. In this work, we investigate the application
of multimodal LLMs in forgery detection. We propose a framework capable of
evaluating image authenticity, localizing tampered regions, providing evidence,
and tracing generation methods based on semantic tampering clues. Our method
demonstrates that the potential of LLMs in forgery analysis can be effectively
unlocked through meticulous prompt engineering and the application of few-shot
learning techniques. We conduct qualitative and quantitative experiments and
show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in
LaMa, which is competitive with state-of-the-art AIGC detection methods. We
further discuss the limitations of multimodal LLMs in such tasks and propose
potential improvements.

</details>


### [55] [Human Aligned Compression for Robust Models](https://arxiv.org/abs/2504.12255)
*Samuel Räber,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.CV

TL;DR: Learned compression methods (HiFiC, ELIC) outperform JPEG in defending against adversarial attacks on image models, especially for Vision Transformers, by preserving semantic content and removing noise. Sequential compression further enhances defense efficacy.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks introduce imperceptible perturbations to images, leading to incorrect predictions. The study aims to evaluate human-aligned learned compression as a defense mechanism.

Method: Compare learned compression models (HiFiC, ELIC) with JPEG across quality levels. Test on ImageNet subsets, including white-box settings, and evaluate sequential compression.

Result: Learned methods outperform JPEG, preserving semantic content and removing adversarial noise. Sequential compression boosts defense efficacy without harming classification.

Conclusion: Human-aligned compression is an effective, efficient defense against adversarial attacks, protecting features relevant to human and machine understanding.

Abstract: Adversarial attacks on image models threaten system robustness by introducing
imperceptible perturbations that cause incorrect predictions. We investigate
human-aligned learned lossy compression as a defense mechanism, comparing two
learned models (HiFiC and ELIC) against traditional JPEG across various quality
levels. Our experiments on ImageNet subsets demonstrate that learned
compression methods outperform JPEG, particularly for Vision Transformer
architectures, by preserving semantically meaningful content while removing
adversarial noise. Even in white-box settings where attackers can access the
defense, these methods maintain substantial effectiveness. We also show that
sequential compression--applying rounds of
compression/decompression--significantly enhances defense efficacy while
maintaining classification performance. Our findings reveal that human-aligned
compression provides an effective, computationally efficient defense that
protects the image features most relevant to human and machine understanding.
It offers a practical approach to improving model robustness against
adversarial threats.

</details>


### [56] [Non-uniform Point Cloud Upsampling via Local Manifold Distribution](https://arxiv.org/abs/2504.11701)
*Yaohui Fang,Xingce Wang*

Main category: cs.CV

TL;DR: A novel point cloud upsampling method using Gaussian functions to model manifold distributions, improving quality and uniformity in sparse, non-uniform inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore intrinsic data distribution, leading to suboptimal results for sparse and non-uniform point clouds.

Method: Uses a network to iteratively optimize Gaussian components and weights, leveraging their probabilistic properties to impose distribution constraints.

Result: Produces higher-quality, more uniformly distributed dense point clouds, outperforming state-of-the-art techniques.

Conclusion: The approach effectively addresses limitations of current methods by incorporating manifold distribution constraints.

Abstract: Existing learning-based point cloud upsampling methods often overlook the
intrinsic data distribution charac?teristics of point clouds, leading to
suboptimal results when handling sparse and non-uniform point clouds. We
propose a novel approach to point cloud upsampling by imposing constraints from
the perspective of manifold distributions. Leveraging the strong fitting
capability of Gaussian functions, our method employs a network to iteratively
optimize Gaussian components and their weights, accurately representing local
manifolds. By utilizing the probabilistic distribution properties of Gaussian
functions, we construct a unified statistical manifold to impose distribution
constraints on the point cloud. Experimental results on multiple datasets
demonstrate that our method generates higher-quality and more uniformly
distributed dense point clouds when processing sparse and non-uniform inputs,
outperforming state-of-the-art point cloud upsampling techniques.

</details>


### [57] [Learning What NOT to Count](https://arxiv.org/abs/2504.11705)
*Adriano D'Alessandro,Ali Mahdavi-Amiri,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: Proposes an annotation-free method for fine-grained object counting using synthetic data and attention prediction, improving few/zero-shot models.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of distinguishing fine-grained categories in few/zero-shot object counting, reducing reliance on manual annotations.

Method: Leverages latent generative models to synthesize category-specific crowded scenes and introduces an attention prediction network trained on synthetic data.

Result: Enhances pre-trained models on fine-grained counting tasks using only synthetic data, validated with the FGTC dataset.

Conclusion: The method effectively integrates new fine-grained categories into counting models without manual labeling, improving performance.

Abstract: Few/zero-shot object counting methods reduce the need for extensive
annotations but often struggle to distinguish between fine-grained categories,
especially when multiple similar objects appear in the same scene. To address
this limitation, we propose an annotation-free approach that enables the
seamless integration of new fine-grained categories into existing few/zero-shot
counting models. By leveraging latent generative models, we synthesize
high-quality, category-specific crowded scenes, providing a rich training
source for adapting to new categories without manual labeling. Our approach
introduces an attention prediction network that identifies fine-grained
category boundaries trained using only synthetic pseudo-annotated data. At
inference, these fine-grained attention estimates refine the output of existing
few/zero-shot counting networks. To benchmark our method, we further introduce
the FGTC dataset, a taxonomy-specific fine-grained object counting dataset for
natural images. Our method substantially enhances pre-trained state-of-the-art
models on fine-grained taxon counting tasks, while using only synthetic data.
Code and data to be released upon acceptance.

</details>


### [58] [Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset](https://arxiv.org/abs/2504.11707)
*Muhammad Shahid Muneer,Simon S. Woo*

Main category: cs.CV

TL;DR: The paper addresses misuse of Text-to-Image (T2I) models, focusing on adversarial attacks bypassing NSFW filters. It introduces a million-scale dataset and a robust multimodal defense method.


<details>
  <summary>Details</summary>
Motivation: To combat the growing concern of adversarial attacks on T2I models and the lack of a robust multimodal NSFW dataset.

Method: Proposes a dataset of prompt-image pairs and adversarial examples, then develops a multimodal defense system.

Result: The model outperforms existing NSFW detection methods, reducing Attack Success Rate (ASR) significantly.

Conclusion: The work provides a scalable solution to enhance safety in T2I models against adversarial threats.

Abstract: In the past years, we have witnessed the remarkable success of Text-to-Image
(T2I) models and their widespread use on the web. Extensive research in making
T2I models produce hyper-realistic images has led to new concerns, such as
generating Not-Safe-For-Work (NSFW) web content and polluting the web society.
To help prevent misuse of T2I models and create a safer web environment for
users features like NSFW filters and post-hoc security checks are used in these
models. However, recent work unveiled how these methods can easily fail to
prevent misuse. In particular, adversarial attacks on text and image modalities
can easily outplay defensive measures. %Exploiting such leads to the growing
concern of preventing adversarial attacks on text and image modalities.
Moreover, there is currently no robust multimodal NSFW dataset that includes
both prompt and image pairs and adversarial examples. This work proposes a
million-scale prompt and image dataset generated using open-source diffusion
models. Second, we develop a multimodal defense to distinguish safe and NSFW
text and images, which is robust against adversarial attacks and directly
alleviates current challenges. Our extensive experiments show that our model
performs well against existing SOTA NSFW detection methods in terms of accuracy
and recall, drastically reducing the Attack Success Rate (ASR) in multimodal
adversarial attack scenarios. Code:
https://github.com/shahidmuneer/multimodal-nsfw-defense.

</details>


### [59] [EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos](https://arxiv.org/abs/2504.11732)
*Jilan Xu,Yifei Huang,Baoqi Pei,Junlin Hou,Qingqiu Li,Guo Chen,Yuejie Zhang,Rui Feng,Weidi Xie*

Main category: cs.CV

TL;DR: EgoExo-Gen is a two-stage model for cross-view video prediction, using hand-object interaction (HOI) masks to enhance ego-centric video generation from exo-centric inputs and textual instructions.


<details>
  <summary>Details</summary>
Motivation: First-person video generation has applications in augmented reality and embodied intelligence. The paper focuses on leveraging HOI dynamics to improve cross-view video prediction.

Method: EgoExo-Gen first predicts HOI masks for future ego-frames using spatio-temporal correspondence, then uses a video diffusion model with HOI masks as structural guidance. An automated pipeline generates pseudo HOI masks for training.

Result: EgoExo-Gen outperforms previous models on Ego-Exo4D and H2O datasets, with HOI masks improving hand and object generation in ego-centric videos.

Conclusion: Explicit modeling of HOI dynamics enhances cross-view video prediction, demonstrating the effectiveness of EgoExo-Gen.

Abstract: Generating videos in the first-person perspective has broad application
prospects in the field of augmented reality and embodied intelligence. In this
work, we explore the cross-view video prediction task, where given an
exo-centric video, the first frame of the corresponding ego-centric video, and
textual instructions, the goal is to generate futur frames of the ego-centric
video. Inspired by the notion that hand-object interactions (HOI) in
ego-centric videos represent the primary intentions and actions of the current
actor, we present EgoExo-Gen that explicitly models the hand-object dynamics
for cross-view video prediction. EgoExo-Gen consists of two stages. First, we
design a cross-view HOI mask prediction model that anticipates the HOI masks in
future ego-frames by modeling the spatio-temporal ego-exo correspondence. Next,
we employ a video diffusion model to predict future ego-frames using the first
ego-frame and textual instructions, while incorporating the HOI masks as
structural guidance to enhance prediction quality. To facilitate training, we
develop an automated pipeline to generate pseudo HOI masks for both ego- and
exo-videos by exploiting vision foundation models. Extensive experiments
demonstrate that our proposed EgoExo-Gen achieves better prediction performance
compared to previous video prediction models on the Ego-Exo4D and H2O benchmark
datasets, with the HOI masks significantly improving the generation of hands
and interactive objects in the ego-centric videos.

</details>


### [60] [DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment](https://arxiv.org/abs/2504.11733)
*Li Yu,Situo Wang,Wei Zhou,Moncef Gabbouj*

Main category: cs.CV

TL;DR: The paper proposes DVLTA-VQA, a method integrating decoupled CLIP components into NR-VQA to address limitations in capturing temporal/motion info and adaptive feature fusion.


<details>
  <summary>Details</summary>
Motivation: Inspired by the dual-stream HVS theory and CLIP's semantic capabilities, the paper aims to enhance VQA by addressing CLIP's lack of temporal/motion understanding and rigid feature fusion in NR-VQA.

Method: Decouples CLIP's visual and textual components, integrating them into NR-VQA stages for adaptive feature fusion and improved semantic understanding.

Result: DVLTA-VQA enhances video quality assessment by leveraging CLIP's strengths while addressing its limitations for temporal and motion analysis.

Conclusion: The proposed method effectively bridges gaps in dual-stream VQA by adapting CLIP for video-specific needs, improving NR-VQA performance.

Abstract: Inspired by the dual-stream theory of the human visual system (HVS) - where
the ventral stream is responsible for object recognition and detail analysis,
while the dorsal stream focuses on spatial relationships and motion perception
- an increasing number of video quality assessment (VQA) works built upon this
framework are proposed. Recent advancements in large multi-modal models,
notably Contrastive Language-Image Pretraining (CLIP), have motivated
researchers to incorporate CLIP into dual-stream-based VQA methods. This
integration aims to harness the model's superior semantic understanding
capabilities to replicate the object recognition and detail analysis in ventral
stream, as well as spatial relationship analysis in dorsal stream. However,
CLIP is originally designed for images and lacks the ability to capture
temporal and motion information inherent in videos. %Furthermore, existing
feature fusion strategies in no-reference video quality assessment (NR-VQA)
often rely on fixed weighting schemes, which fail to adaptively adjust feature
importance. To address the limitation, this paper propose a Decoupled
Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality
Assessment (DVLTA-VQA), which decouples CLIP's visual and textual components,
and integrates them into different stages of the NR-VQA pipeline.

</details>


### [61] [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
*Bingjie Gao,Xinyu Gao,Xiaoxue Wu,Yujie Zhou,Yu Qiao,Li Niu,Xinyuan Chen,Yaohui Wang*

Main category: cs.CV

TL;DR: RAPO is a retrieval-augmented prompt optimization framework designed to improve Text-to-Video (T2V) generation by refining user prompts through dual optimization branches.


<details>
  <summary>Details</summary>
Motivation: Current T2V models are sensitive to input prompts, and prior methods lack tailored guidance for prompt vocabulary and structure. RAPO addresses this gap.

Method: RAPO uses two branches: one augments prompts with modifiers from a relational graph and aligns them via a fine-tuned LLM, while the other rewrites prompts using a pre-trained LLM.

Result: RAPO enhances both static and dynamic aspects of generated videos, proving the importance of prompt optimization.

Conclusion: RAPO effectively improves T2V generation by optimizing user prompts, demonstrating its value in the field.

Abstract: The evolution of Text-to-video (T2V) generative models, trained on
large-scale datasets, has been marked by significant progress. However, the
sensitivity of T2V generative models to input prompts highlights the critical
role of prompt design in influencing generative outcomes. Prior research has
predominantly relied on Large Language Models (LLMs) to align user-provided
prompts with the distribution of training prompts, albeit without tailored
guidance encompassing prompt vocabulary and sentence structure nuances. To this
end, we introduce \textbf{RAPO}, a novel \textbf{R}etrieval-\textbf{A}ugmented
\textbf{P}rompt \textbf{O}ptimization framework. In order to address potential
inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO
refines the naive prompts through dual optimization branches, selecting the
superior prompt for T2V generation. The first branch augments user prompts with
diverse modifiers extracted from a learned relational graph, refining them to
align with the format of training prompts via a fine-tuned LLM. Conversely, the
second branch rewrites the naive prompt using a pre-trained LLM following a
well-defined instruction set. Extensive experiments demonstrate that RAPO can
effectively enhance both the static and dynamic dimensions of generated videos,
demonstrating the significance of prompt optimization for user-provided
prompts. Project website:
\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.

</details>


### [62] [SkeletonX: Data-Efficient Skeleton-based Action Recognition via Cross-sample Feature Aggregation](https://arxiv.org/abs/2504.11749)
*Zongye Zhang,Wenrui Cai,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: SkeletonX is a lightweight training pipeline for skeleton action recognition, improving performance in low-data and one-shot settings by leveraging performer variability and action commonality.


<details>
  <summary>Details</summary>
Motivation: Current skeleton action recognition models struggle with new action categories, diverse performers, and varied skeleton layouts, especially with limited data. Existing methods underutilize mutual information between labeled samples.

Method: SkeletonX introduces a sample pair construction strategy and a feature aggregation module to enhance labeled data utility. It integrates with GCN-based recognizers.

Result: Experiments on NTU RGB+D, NTU RGB+D 120, and PKU-MMD show improved performance with limited data and surpasses state-of-the-art in one-shot settings with fewer parameters and FLOPs.

Conclusion: SkeletonX effectively addresses low-data challenges in skeleton action recognition, offering a lightweight and efficient solution.

Abstract: While current skeleton action recognition models demonstrate impressive
performance on large-scale datasets, their adaptation to new application
scenarios remains challenging. These challenges are particularly pronounced
when facing new action categories, diverse performers, and varied skeleton
layouts, leading to significant performance degeneration. Additionally, the
high cost and difficulty of collecting skeleton data make large-scale data
collection impractical. This paper studies one-shot and limited-scale learning
settings to enable efficient adaptation with minimal data. Existing approaches
often overlook the rich mutual information between labeled samples, resulting
in sub-optimal performance in low-data scenarios. To boost the utility of
labeled data, we identify the variability among performers and the commonality
within each action as two key attributes. We present SkeletonX, a lightweight
training pipeline that integrates seamlessly with existing GCN-based skeleton
action recognizers, promoting effective training under limited labeled data.
First, we propose a tailored sample pair construction strategy on two key
attributes to form and aggregate sample pairs. Next, we develop a concise and
effective feature aggregation module to process these pairs. Extensive
experiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various
GCN backbones, demonstrating that the pipeline effectively improves performance
when trained from scratch with limited data. Moreover, it surpasses previous
state-of-the-art methods in the one-shot setting, with only 1/10 of the
parameters and much fewer FLOPs. The code and data are available at:
https://github.com/zzysteve/SkeletonX

</details>


### [63] [GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision](https://arxiv.org/abs/2504.11754)
*Zihui Zhang,Yafei Yang,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: GrabS, a two-stage pipeline, learns object-centric priors and uses an embodied agent to discover objects, outperforming unsupervised methods in 3D segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised 3D object segmentation methods rely on 2D features or external signals, limiting performance on complex objects.

Method: GrabS first learns generative and discriminative priors from object datasets, then uses an embodied agent to query these priors for object discovery.

Result: Evaluated on real-world and synthetic datasets, GrabS surpasses existing unsupervised methods in segmentation performance.

Conclusion: GrabS advances unsupervised 3D segmentation by leveraging object-centric priors and active discovery.

Abstract: We study the hard problem of 3D object segmentation in complex point clouds
without requiring human labels of 3D scenes for supervision. By relying on the
similarity of pretrained 2D features or external signals such as motion to
group 3D points as objects, existing unsupervised methods are usually limited
to identifying simple objects like cars or their segmented objects are often
inferior due to the lack of objectness in pretrained features. In this paper,
we propose a new two-stage pipeline called GrabS. The core concept of our
method is to learn generative and discriminative object-centric priors as a
foundation from object datasets in the first stage, and then design an embodied
agent to learn to discover multiple objects by querying against the pretrained
generative priors in the second stage. We extensively evaluate our method on
two real-world datasets and a newly created synthetic dataset, demonstrating
remarkable segmentation performance, clearly surpassing all existing
unsupervised methods.

</details>


### [64] [Extended Short- and Long-Range Mesh Learning for Fast and Generalized Garment Simulation](https://arxiv.org/abs/2504.11763)
*Aoran Liu,Kun Hu,Clinton Mo,Changyang Li,Zhiyong Wang*

Main category: cs.CV

TL;DR: A novel GNN-based framework with LSDMP and GSA modules improves 3D garment simulation by extending message-passing range efficiently.


<details>
  <summary>Details</summary>
Motivation: Overcoming computational inefficiency in GNNs for high-resolution garment simulation.

Method: Introduces Laplacian-Smoothed Dual Message-Passing (LSDMP) and Geodesic Self-Attention (GSA) modules for efficient short- and long-range mesh modeling.

Result: Achieves state-of-the-art performance with fewer layers and lower inference latency.

Conclusion: The proposed framework effectively enhances garment simulation efficiency and accuracy.

Abstract: 3D garment simulation is a critical component for producing cloth-based
graphics. Recent advancements in graph neural networks (GNNs) offer a promising
approach for efficient garment simulation. However, GNNs require extensive
message-passing to propagate information such as physical forces and maintain
contact awareness across the entire garment mesh, which becomes computationally
inefficient at higher resolutions. To address this, we devise a novel GNN-based
mesh learning framework with two key components to extend the message-passing
range with minimal overhead, namely the Laplacian-Smoothed Dual Message-Passing
(LSDMP) and the Geodesic Self-Attention (GSA) modules. LSDMP enhances
message-passing with a Laplacian features smoothing process, which efficiently
propagates the impact of each vertex to nearby vertices. Concurrently, GSA
introduces geodesic distance embeddings to represent the spatial relationship
between vertices and utilises attention mechanisms to capture global mesh
information. The two modules operate in parallel to ensure both short- and
long-range mesh modelling. Extensive experiments demonstrate the
state-of-the-art performance of our method, requiring fewer layers and lower
inference latency.

</details>


### [65] [TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion](https://arxiv.org/abs/2504.11773)
*Yiran Wang,Jiaqi Li,Chaoyi Hong,Ruibo Li,Liusheng Sun,Xiao Song,Zhe Wang,Zhiguo Cao,Guosheng Lin*

Main category: cs.CV

TL;DR: TacoDepth is a one-stage Radar-Camera fusion model for efficient and accurate depth estimation, improving accuracy by 12.8% and speed by 91.8%.


<details>
  <summary>Details</summary>
Motivation: Existing methods are slow and not robust due to multi-stage frameworks and sparse Radar data.

Method: Uses a graph-based Radar structure extractor and pyramid-based fusion module for efficient one-stage fusion.

Result: Outperforms state-of-the-art with 12.8% better accuracy and 91.8% faster processing.

Conclusion: TacoDepth offers a robust, efficient solution for Radar-Camera depth estimation, balancing speed and accuracy.

Abstract: Radar-Camera depth estimation aims to predict dense and accurate metric depth
by fusing input images and Radar data. Model efficiency is crucial for this
task in pursuit of real-time processing on autonomous vehicles and robotic
platforms. However, due to the sparsity of Radar returns, the prevailing
methods adopt multi-stage frameworks with intermediate quasi-dense depth, which
are time-consuming and not robust. To address these challenges, we propose
TacoDepth, an efficient and accurate Radar-Camera depth estimation model with
one-stage fusion. Specifically, the graph-based Radar structure extractor and
the pyramid-based Radar fusion module are designed to capture and integrate the
graph structures of Radar point clouds, delivering superior model efficiency
and robustness without relying on the intermediate depth results. Moreover,
TacoDepth can be flexible for different inference modes, providing a better
balance of speed and accuracy. Extensive experiments are conducted to
demonstrate the efficacy of our method. Compared with the previous
state-of-the-art approach, TacoDepth improves depth accuracy and processing
speed by 12.8% and 91.8%. Our work provides a new perspective on efficient
Radar-Camera depth estimation.

</details>


### [66] [Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets](https://arxiv.org/abs/2504.11777)
*Yongpei Ma,Pengyu Wang,Adam Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TL;DR: The paper proposes SEQA, a framework using LLMs to generate diverse but semantically equivalent questions for MVQA, improving model consistency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Linguistic variability in question phrasing undermines MVQA system consistency.

Method: SEQA framework leverages LLMs for question augmentation and introduces TAR-SC and other diversity metrics.

Result: Enhanced datasets show significant improvements (e.g., ANQI +86.1). Fine-tuned models achieve 19.35% higher accuracy.

Conclusion: SEQA improves MVQA model consistency and accuracy, validated by new metrics.

Abstract: Medical Visual Question Answering (MVQA) systems can interpret medical images
in response to natural language queries. However, linguistic variability in
question phrasing often undermines the consistency of these systems. To address
this challenge, we propose a Semantically Equivalent Question Augmentation
(SEQA) framework, which leverages large language models (LLMs) to generate
diverse yet semantically equivalent rephrasings of questions. Specifically,
this approach enriches linguistic diversity while preserving semantic meaning.
We further introduce an evaluation metric, Total Agreement Rate with
Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a
model's capability to generate consistent and correct responses to semantically
equivalent linguistic variations. In addition, we also propose three other
diversity metrics - average number of QA items per image (ANQI), average number
of questions per image with the same answer (ANQA), and average number of
open-ended questions per image with the same semantics (ANQS). Using the SEQA
framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,
and PathVQA. As a result, all three datasets achieved significant improvements
by incorporating more semantically equivalent questions: ANQI increased by an
average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate
three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and
fine-tuning settings on the enhanced datasets. Experimental results in MVQA
datasets show that fine-tuned models achieve an average accuracy improvement of
19.35%, while our proposed TAR-SC metric shows an average improvement of 11.
61%, indicating a substantial enhancement in model consistency.

</details>


### [67] [Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection](https://arxiv.org/abs/2504.11779)
*Qishun Wang,Zhengzheng Tu,Chenglong Li,Bo Jiang*

Main category: cs.CV

TL;DR: The paper proposes MSGNet, a graph-based method for alignment-free RGB-Thermal Video Object Detection (RGBT VOD), addressing limitations of manual alignment in multimodal tasks. It introduces adaptive partitioning and spatio-temporal graph learning for robust performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RGBT VOD relies on manually aligned multimodal pairs, limiting practicality. The paper aims to overcome this by developing an alignment-free solution using graph representation learning.

Method: MSGNet includes an Adaptive Partitioning Layer (APL) for inexact alignment, Spatial Sparse Graph Learning Module (S-SGLM) for cross-modal interaction, and Hybrid Structured Temporal Modeling (HSTM) with T-SGLM and TSB for temporal cues.

Result: Experiments on VT-VOD50 and UVT-VOD2024 datasets show the method's effectiveness and superiority over existing approaches.

Conclusion: MSGNet offers a robust, alignment-free solution for RGBT VOD, leveraging graph learning and temporal modeling for improved performance in challenging conditions.

Abstract: RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of
traditional RGB-based VOD in challenging lighting conditions, making it more
practical and effective in many applications.
  However, similar to most RGBT fusion tasks, it still mainly relies on
manually aligned multimodal image pairs.
  In this paper, we propose a novel Multimodal Spatio-temporal Graph learning
Network (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust
graph representation learning model.
  Specifically, we first design an Adaptive Partitioning Layer (APL) to
estimate the corresponding regions of the Thermal image within the RGB image
(high-resolution), achieving a preliminary inexact alignment.
  Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which
employs a sparse information passing mechanism on the estimated inexact
alignment to achieve reliable information interaction between different
modalities.
  Moreover, to fully exploit the temporal cues for RGBT VOD problem, we
introduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal
Sparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM
aims to filter out some redundant information between adjacent frames by
employing the sparse aggregation mechanism on the temporal graph. Meanwhile,
TSB is dedicated to achieving the complementary learning of local spatial
relationships.
  Extensive comparative experiments conducted on both the aligned dataset
VT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness
and superiority of our proposed method. Our project will be made available on
our website for free public access.

</details>


### [68] [ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model](https://arxiv.org/abs/2504.11781)
*Guanchun Wang,Xiangrong Zhang,Yifei Zhang,Zelin Peng,Tianyang Zhang,Xu Tang,Licheng Jiao*

Main category: cs.CV

TL;DR: ACMamba reduces computational costs for unsupervised anomaly detection in hyperspectral images (HSI) by using region-level instances and a Mamba-based module, achieving faster and more accurate results.


<details>
  <summary>Details</summary>
Motivation: Current HSI anomaly detection methods are computationally expensive due to high-dimensional data and dense sampling. The paper aims to reduce costs without sacrificing accuracy.

Method: Proposes ACMamba, an asymmetrical anomaly detection paradigm using region-level instances and a Mamba-based module for global context. Includes consensus learning for background reconstruction and anomaly compression.

Result: ACMamba outperforms state-of-the-art methods in speed and performance across eight benchmarks.

Conclusion: ACMamba offers a cost-effective and efficient solution for HSI anomaly detection, balancing accuracy and computational efficiency.

Abstract: Unsupervised anomaly detection in hyperspectral images (HSI), aiming to
detect unknown targets from backgrounds, is challenging for earth surface
monitoring. However, current studies are hindered by steep computational costs
due to the high-dimensional property of HSI and dense sampling-based training
paradigm, constraining their rapid deployment. Our key observation is that,
during training, not all samples within the same homogeneous area are
indispensable, whereas ingenious sampling can provide a powerful substitute for
reducing costs. Motivated by this, we propose an Asymmetrical Consensus State
Space Model (ACMamba) to significantly reduce computational costs without
compromising accuracy. Specifically, we design an asymmetrical anomaly
detection paradigm that utilizes region-level instances as an efficient
alternative to dense pixel-level samples. In this paradigm, a low-cost
Mamba-based module is introduced to discover global contextual attributes of
regions that are essential for HSI reconstruction. Additionally, we develop a
consensus learning strategy from the optimization perspective to simultaneously
facilitate background reconstruction and anomaly compression, further
alleviating the negative impact of anomaly reconstruction. Theoretical analysis
and extensive experiments across eight benchmarks verify the superiority of
ACMamba, demonstrating a faster speed and stronger performance over the
state-of-the-art.

</details>


### [69] [DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation](https://arxiv.org/abs/2504.11786)
*Sang-Jun Park,Keun-Soo Heo,Dong-Hee Shin,Young-Han Son,Ji-Hye Oh,Tae-Eui Kam*

Main category: cs.CV

TL;DR: The paper introduces DART, a framework for radiology report generation that improves accuracy by aligning reports with disease-relevant findings in X-ray images and refining them through self-correction.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and trustworthiness of radiology reports by ensuring they closely match disease-relevant findings in X-ray images.

Method: Uses a two-stage approach: (1) image-to-text retrieval with disease-matching via contrastive learning, and (2) a self-correction module to refine reports.

Result: Achieves state-of-the-art performance on benchmarks, outperforming previous methods in report generation and clinical efficacy.

Conclusion: DART significantly improves radiology report accuracy and trustworthiness, demonstrating its potential for clinical applications.

Abstract: The automatic generation of radiology reports has emerged as a promising
solution to reduce a time-consuming task and accurately capture critical
disease-relevant findings in X-ray images. Previous approaches for radiology
report generation have shown impressive performance. However, there remains
significant potential to improve accuracy by ensuring that retrieved reports
contain disease-relevant findings similar to those in the X-ray images and by
refining generated reports. In this study, we propose a Disease-aware
image-text Alignment and self-correcting Re-alignment for Trustworthy radiology
report generation (DART) framework. In the first stage, we generate initial
reports based on image-to-text retrieval with disease-matching, embedding both
images and texts in a shared embedding space through contrastive learning. This
approach ensures the retrieval of reports with similar disease-relevant
findings that closely align with the input X-ray images. In the second stage,
we further enhance the initial reports by introducing a self-correction module
that re-aligns them with the X-ray images. Our proposed framework achieves
state-of-the-art results on two widely used benchmarks, surpassing previous
approaches in both report generation and clinical efficacy metrics, thereby
enhancing the trustworthiness of radiology reports.

</details>


### [70] [Neighbor-Based Feature and Index Enhancement for Person Re-Identification](https://arxiv.org/abs/2504.11798)
*Chao Yuan,Tianyi Zhang,Guanglin Niu*

Main category: cs.CV

TL;DR: The paper proposes DMON-ARO, a model for person re-identification that enhances feature representation and retrieval performance by leveraging multi-order neighborhood information and optimizing query-to-gallery relationships.


<details>
  <summary>Details</summary>
Motivation: Existing Re-ID methods often overlook contextual information, limiting feature representation effectiveness. Multi-order neighborhood information can enrich features but is underexplored.

Method: DMON-ARO combines Dynamic Multi-Order Neighbor Modeling (DMON) for adaptive neighborhood aggregation and Asymmetric Relationship Optimization (ARO) for refining distance matrices.

Result: Experiments on benchmark datasets show improved Rank-1 accuracy and mAP compared to baselines.

Conclusion: DMON-ARO effectively enhances Re-ID performance and can be extended to other re-identification tasks.

Abstract: Person re-identification (Re-ID) aims to match the same pedestrian in a large
gallery with different cameras and views. Enhancing the robustness of the
extracted feature representations is a main challenge in Re-ID. Existing
methods usually improve feature representation by improving model architecture,
but most methods ignore the potential contextual information, which limits the
effectiveness of feature representation and retrieval performance. Neighborhood
information, especially the potential information of multi-order neighborhoods,
can effectively enrich feature expression and improve retrieval accuracy, but
this has not been fully explored in existing research. Therefore, we propose a
novel model DMON-ARO that leverages latent neighborhood information to enhance
both feature representation and index performance. Our approach is built on two
complementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and
Asymmetric Relationship Optimization (ARO). The DMON module dynamically
aggregates multi-order neighbor relationships, allowing it to capture richer
contextual information and enhance feature representation through adaptive
neighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing
query-to-gallery relationships, improving the index accuracy. Extensive
experiments on three benchmark datasets demonstrate that our approach achieves
performance improvements against baseline models, which illustrate the
effectiveness of our model. Specifically, our model demonstrates improvements
in Rank-1 accuracy and mAP. Moreover, this method can also be directly extended
to other re-identification tasks.

</details>


### [71] [Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting](https://arxiv.org/abs/2504.11820)
*Delong Suzhang,Meng Yang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of recovering high-quality depth from low-quality raw depth maps in real-world RGB-D datasets, proposing a method to improve generalization by tackling structure misalignment in both input and output stages.


<details>
  <summary>Details</summary>
Motivation: Real-world depth recovery is hindered by the lack of paired raw-GT data and the diversity of structure misalignment in raw depth maps, leading to poor generalization. Existing methods fail to address these issues adequately.

Method: The proposed method includes a new raw depth generation pipeline to diversify structure misalignment in input, a structure uncertainty module to identify misaligned structures, and a robust feature alignment module for output to align with accurate RGB structures.

Result: Extensive experiments show the method achieves competitive accuracy and generalization across challenging raw depth maps.

Conclusion: The approach effectively improves generalization in real-world depth recovery by addressing structure misalignment in both input and output stages.

Abstract: The low-quality structure in raw depth maps is prevalent in real-world RGB-D
datasets, which makes real-world depth recovery a critical task in recent
years. However, the lack of paired raw-ground truth (raw-GT) data in the real
world poses challenges for generalized depth recovery. Existing methods
insufficiently consider the diversity of structure misalignment in raw depth
maps, which leads to poor generalization in real-world depth recovery. Notably,
random structure misalignments are not limited to raw depth data but also
affect GT depth in real-world datasets. In the proposed method, we tackle the
generalization problem from both input and output perspectives. For input, we
enrich the diversity of structure misalignment in raw depth maps by designing a
new raw depth generation pipeline, which helps the network avoid overfitting to
a specific condition. Furthermore, a structure uncertainty module is designed
to explicitly identify the misaligned structure for input raw depth maps to
better generalize in unseen scenarios. Notably the well-trained depth
foundation model (DFM) can help the structure uncertainty module estimate the
structure uncertainty better. For output, a robust feature alignment module is
designed to precisely align with the accurate structure of RGB images avoiding
the interference of inaccurate GT depth. Extensive experiments on multiple
datasets demonstrate the proposed method achieves competitive accuracy and
generalization capabilities across various challenging raw depth maps.

</details>


### [72] [A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification](https://arxiv.org/abs/2504.11838)
*Bianca Lamm,Janis Keuper*

Main category: cs.CV

TL;DR: The paper introduces a Visual RAG pipeline combining Retrieval Augmented Generation (RAG) and Vision Language Models (VLMs) for few-shot Fine-Grained Classification (FGC), achieving 86.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: Fine-Grained Classification remains challenging in retail for tasks like automated price-monitoring and product recommendation due to fast-changing, visually similar products.

Method: A novel Visual RAG pipeline integrates RAG and VLMs to extract and predict product details from advertisement leaflets without re-training for new products.

Result: The approach achieves 86.8% accuracy on a diverse dataset, outperforming previous methods by enabling prediction of novel products with minimal updates.

Conclusion: The Visual RAG pipeline is effective for FGC in retail, offering flexibility and high accuracy without requiring frequent re-training.

Abstract: Despite the rapid evolution of learning and computer vision algorithms,
Fine-Grained Classification (FGC) still poses an open problem in many
practically relevant applications. In the retail domain, for example, the
identification of fast changing and visually highly similar products and their
properties are key to automated price-monitoring and product recommendation.
This paper presents a novel Visual RAG pipeline that combines the Retrieval
Augmented Generation (RAG) approach and Vision Language Models (VLMs) for
few-shot FGC. This Visual RAG pipeline extracts product and promotion data in
advertisement leaflets from various retailers and simultaneously predicts
fine-grained product ids along with price and discount information. Compared to
previous approaches, the key characteristic of the Visual RAG pipeline is that
it allows the prediction of novel products without re-training, simply by
adding a few class samples to the RAG database. Comparing several VLM back-ends
like GPT-4o [23], GPT-4o-mini [24], and Gemini 2.0 Flash [10], our approach
achieves 86.8% accuracy on a diverse dataset.

</details>


### [73] [Boosting Multi-View Stereo with Depth Foundation Model in the Absence of Real-World Labels](https://arxiv.org/abs/2504.11845)
*Jie Zhu,Bo Peng,Zhe Zhang,Bingzheng Liu,Jianjun Lei*

Main category: cs.CV

TL;DR: DFM-MVS leverages a depth foundation model to generate depth priors for training MVS networks without real-world labels, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Training MVS networks without real-world labels is challenging, and existing methods lack effective supervision.

Method: DFM-MVS uses depth priors for pseudo-supervised training and error correction in a coarse-to-fine network structure.

Result: Outperforms existing MVS methods on DTU and Tanks & Temples datasets without real-world labels.

Conclusion: DFM-MVS effectively addresses the label-free training challenge in MVS by leveraging depth priors.

Abstract: Learning-based Multi-View Stereo (MVS) methods have made remarkable progress
in recent years. However, how to effectively train the network without using
real-world labels remains a challenging problem. In this paper, driven by the
recent advancements of vision foundation models, a novel method termed DFM-MVS,
is proposed to leverage the depth foundation model to generate the effective
depth prior, so as to boost MVS in the absence of real-world labels.
Specifically, a depth prior-based pseudo-supervised training mechanism is
developed to simulate realistic stereo correspondences using the generated
depth prior, thereby constructing effective supervision for the MVS network.
Besides, a depth prior-guided error correction strategy is presented to
leverage the depth prior as guidance to mitigate the error propagation problem
inherent in the widely-used coarse-to-fine network structure. Experimental
results on DTU and Tanks & Temples datasets demonstrate that the proposed
DFM-MVS significantly outperforms existing MVS methods without using real-world
labels.

</details>


### [74] [ACE: Attentional Concept Erasure in Diffusion Models](https://arxiv.org/abs/2504.11850)
*Finn Carter*

Main category: cs.CV

TL;DR: ACE is a novel method for erasing harmful or unwanted concepts from diffusion models while preserving other content, using attention manipulation and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the issue of harmful or undesirable content generation in diffusion models by enabling selective concept removal.

Method: Attentional Concept Erasure (ACE) combines closed-form attention manipulation with lightweight fine-tuning, aligning the model's conditional distribution on the target concept with a neutral distribution.

Result: ACE achieves state-of-the-art concept removal efficacy and robustness across various benchmarks, balancing generality and specificity efficiently.

Conclusion: ACE offers a scalable and efficient solution for safer deployment of diffusion models by enabling precise concept erasure.

Abstract: Large text-to-image diffusion models have demonstrated remarkable image
synthesis capabilities, but their indiscriminate training on Internet-scale
data has led to learned concepts that enable harmful, copyrighted, or otherwise
undesirable content generation. We address the task of concept erasure in
diffusion models, i.e., removing a specified concept from a pre-trained model
such that prompting the concept (or related synonyms) no longer yields its
depiction, while preserving the model's ability to generate other content. We
propose a novel method, Attentional Concept Erasure (ACE), that integrates a
closed-form attention manipulation with lightweight fine-tuning. Theoretically,
we formulate concept erasure as aligning the model's conditional distribution
on the target concept with a neutral distribution. Our approach identifies and
nullifies concept-specific latent directions in the cross-attention modules via
a gated low-rank adaptation, followed by adversarially augmented fine-tuning to
ensure thorough erasure of the concept and its synonyms. Empirically, we
demonstrate on multiple benchmarks, including object classes, celebrity faces,
explicit content, and artistic styles, that ACE achieves state-of-the-art
concept removal efficacy and robustness. Compared to prior methods, ACE better
balances generality (erasing concept and related terms) and specificity
(preserving unrelated content), scales to dozens of concepts, and is efficient,
requiring only a few seconds of adaptation per concept. We will release our
code to facilitate safer deployment of diffusion models.

</details>


### [75] [Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation](https://arxiv.org/abs/2504.11856)
*Zhenhuan Zhou,Yuchen Zhang,Along He,Peng Wang,Xueshuo Xie,Tao Li*

Main category: cs.CV

TL;DR: The paper introduces a semi-supervised learning network (CFC-Net) for root canal segmentation, addressing the lack of public datasets with a new dataset (FMRC-2025) and leveraging unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Root canal treatment lacks objective diagnostic tools due to limited datasets and reliance on clinician experience. Deep learning can improve accuracy but requires labeled data.

Method: Proposes CFC-Net with two components: CFC-MT for multi-frequency training and UCF-Mix for high-confidence pseudo-label generation.

Result: CFC-Net outperforms state-of-the-art methods on FMRC-2025 and other dental datasets, demonstrating strong generalizability.

Conclusion: CFC-Net effectively addresses dataset scarcity and improves root canal segmentation, with potential for broader dental applications.

Abstract: Root canal (RC) treatment is a highly delicate and technically complex
procedure in clinical practice, heavily influenced by the clinicians'
experience and subjective judgment. Deep learning has made significant
advancements in the field of computer-aided diagnosis (CAD) because it can
provide more objective and accurate diagnostic results. However, its
application in RC treatment is still relatively rare, mainly due to the lack of
public datasets in this field. To address this issue, in this paper, we
established a First Molar Root Canal segmentation dataset called FMRC-2025.
Additionally, to alleviate the workload of manual annotation for dentists and
fully leverage the unlabeled data, we designed a Cross-Frequency Collaborative
training semi-supervised learning (SSL) Network called CFC-Net. It consists of
two components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which
introduces two specialized students (SS) and one comprehensive teacher (CT) for
collaborative multi-frequency training. The CT and SS are trained on different
frequency components while fully integrating multi-frequency knowledge through
cross and full frequency consistency supervisions. (2) Uncertainty-guided
Cross-Frequency Mix (UCF-Mix) mechanism enables the network to generate
high-confidence pseudo-labels while learning to integrate multi-frequency
information and maintaining the structural integrity of the targets. Extensive
experiments on FMRC-2025 and three public dental datasets demonstrate that
CFC-MT is effective for RC segmentation and can also exhibit strong
generalizability on other dental segmentation tasks, outperforming
state-of-the-art SSL medical image segmentation methods. Codes and dataset will
be released.

</details>


### [76] [Synthetic Data for Blood Vessel Network Extraction](https://arxiv.org/abs/2504.11858)
*Joël Mathys,Andreas Plesner,Jorel Elmiger,Roger Wattenhofer*

Main category: cs.CV

TL;DR: The paper proposes a method combining synthetic data generation and deep learning to extract detailed vessel networks from microscopy data, addressing data scarcity and improving accuracy for stroke research.


<details>
  <summary>Details</summary>
Motivation: Understanding brain vessel topology is critical for stroke research, but extracting detailed networks from microscopy data is challenging due to limited labeled data and the need for high accuracy.

Method: A three-stage synthetic data generation pipeline creates realistic vessel networks, followed by a two-stage deep learning approach (3D U-Net models) for node detection and edge prediction, fine-tuned on real data.

Result: Fine-tuning on just 5 labeled samples improved edge prediction F1 scores from 0.496 to 0.626, demonstrating practical feasibility.

Conclusion: Automated vessel network extraction is now feasible, enabling large-scale vascular analysis in stroke research.

Abstract: Blood vessel networks in the brain play a crucial role in stroke research,
where understanding their topology is essential for analyzing blood flow
dynamics. However, extracting detailed topological vessel network information
from microscopy data remains a significant challenge, mainly due to the
scarcity of labeled training data and the need for high topological accuracy.
This work combines synthetic data generation with deep learning to
automatically extract vessel networks as graphs from volumetric microscopy
data. To combat data scarcity, we introduce a comprehensive pipeline for
generating large-scale synthetic datasets that mirror the characteristics of
real vessel networks. Our three-stage approach progresses from abstract graph
generation through vessel mask creation to realistic medical image synthesis,
incorporating biological constraints and imaging artifacts at each stage. Using
this synthetic data, we develop a two-stage deep learning pipeline of 3D
U-Net-based models for node detection and edge prediction. Fine-tuning on real
microscopy data shows promising adaptation, improving edge prediction F1 scores
from 0.496 to 0.626 by training on merely 5 manually labeled samples. These
results suggest that automated vessel network extraction is becoming
practically feasible, opening new possibilities for large-scale vascular
analysis in stroke research.

</details>


### [77] [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
*Laura Fieback,Nishilkumar Balar,Jakob Spiegelberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: The paper introduces Efficient Contrastive Decoding (ECD) to reduce hallucinations in Large Vision Language Models (LVLMs) by adjusting output distributions without additional training.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate hallucinatory responses misaligned with visual inputs, necessitating a method to improve accuracy.

Method: ECD uses probabilistic hallucination detection to contrast token probabilities and hallucination scores, refining output distributions.

Result: ECD outperforms state-of-the-art methods in reducing hallucinations and computational efficiency on benchmark datasets.

Conclusion: ECD is a versatile, training-free solution for mitigating hallucinations in LVLMs, enhancing performance and efficiency.

Abstract: Despite recent advances in Large Vision Language Models (LVLMs), these models
still suffer from generating hallucinatory responses that do not align with the
visual input provided. To mitigate such hallucinations, we introduce Efficient
Contrastive Decoding (ECD), a simple method that leverages probabilistic
hallucination detection to shift the output distribution towards contextually
accurate answers at inference time. By contrasting token probabilities and
hallucination scores, ECD subtracts hallucinated concepts from the original
distribution, effectively suppressing hallucinations. Notably, our proposed
method can be applied to any open-source LVLM and does not require additional
LVLM training. We evaluate our method on several benchmark datasets and across
different LVLMs. Our experiments show that ECD effectively mitigates
hallucinations, outperforming state-of-the-art methods with respect to
performance on LVLM benchmarks and computation time.

</details>


### [78] [A Category-Fragment Segmentation Framework for Pelvic Fracture Segmentation in X-ray Images](https://arxiv.org/abs/2504.11872)
*Daiqi Liu,Fuxin Fan,Andreas Maier*

Main category: cs.CV

TL;DR: A deep learning framework (CFS) is proposed for automatic segmentation of pelvic bone fragments in 2D X-rays, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Pelvic fractures require precise surgical planning, and current imaging methods need automation for better efficiency.

Method: The CFS framework uses three steps: category segmentation, fragment segmentation, and post-processing.

Result: The model achieves an IoU of 0.91 for anatomical structures and 0.78 for fracture segmentation.

Conclusion: The CFS framework is effective and accurate for pelvic fracture segmentation in 2D X-rays.

Abstract: Pelvic fractures, often caused by high-impact trauma, frequently require
surgical intervention. Imaging techniques such as CT and 2D X-ray imaging are
used to transfer the surgical plan to the operating room through image
registration, enabling quick intraoperative adjustments. Specifically,
segmenting pelvic fractures from 2D X-ray imaging can assist in accurately
positioning bone fragments and guiding the placement of screws or metal plates.
In this study, we propose a novel deep learning-based category and fragment
segmentation (CFS) framework for the automatic segmentation of pelvic bone
fragments in 2D X-ray images. The framework consists of three consecutive
steps: category segmentation, fragment segmentation, and post-processing. Our
best model achieves an IoU of 0.91 for anatomical structures and 0.78 for
fracture segmentation. Results demonstrate that the CFS framework is effective
and accurate.

</details>


### [79] [Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval](https://arxiv.org/abs/2504.11879)
*Yushuai Sun,Zikun Zhou,Dongmei Jiang,Yaowei Wang,Jun Yu,Guangming Lu,Wenjie Pei*

Main category: cs.CV

TL;DR: The paper introduces a Prunable Network with self-compatibility for flexible multi-platform deployment in asymmetric retrieval systems, eliminating the need for additional training when introducing new platforms.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack flexibility for multi-platform deployment, requiring additional training for new platforms. The paper aims to address this by enabling post-training pruning for compatible subnetworks.

Method: The authors propose a Prunable Network optimized for architecture and weight of subnetworks at varying capacities. They introduce a conflict-aware gradient integration scheme to manage gradient conflicts during compatible learning.

Result: Experiments on diverse benchmarks and visual backbones validate the method's effectiveness in generating compatible subnetworks without extra training.

Conclusion: The Prunable Network offers a scalable solution for asymmetric retrieval systems, simplifying deployment across platforms with varying resource configurations.

Abstract: Asymmetric retrieval is a typical scenario in real-world retrieval systems,
where compatible models of varying capacities are deployed on platforms with
different resource configurations. Existing methods generally train pre-defined
networks or subnetworks with capacities specifically designed for
pre-determined platforms, using compatible learning. Nevertheless, these
methods suffer from limited flexibility for multi-platform deployment. For
example, when introducing a new platform into the retrieval systems, developers
have to train an additional model at an appropriate capacity that is compatible
with existing models via backward-compatible learning. In this paper, we
propose a Prunable Network with self-compatibility, which allows developers to
generate compatible subnetworks at any desired capacity through post-training
pruning. Thus it allows the creation of a sparse subnetwork matching the
resources of the new platform without additional training. Specifically, we
optimize both the architecture and weight of subnetworks at different
capacities within a dense network in compatible learning. We also design a
conflict-aware gradient integration scheme to handle the gradient conflicts
between the dense network and subnetworks during compatible learning. Extensive
experiments on diverse benchmarks and visual backbones demonstrate the
effectiveness of our method. Our code and model are available at
https://github.com/Bunny-Black/PrunNet.

</details>


### [80] [CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting](https://arxiv.org/abs/2504.11893)
*Wei Sun,Yanzhao Zhou,Jianbin Jiao,Yuan Li*

Main category: cs.CV

TL;DR: CAGS improves 3D scene understanding by integrating spatial context into 3D Gaussian Splatting, addressing cross-view granularity inconsistency and fragmentation errors.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary 3D scene understanding is essential for robotics and AR, but current methods suffer from inconsistent object segmentations and fragmented representations due to isolated feature learning.

Method: CAGS incorporates spatial context via local graphs, mask-centric contrastive learning, and a precomputation strategy for efficient training.

Result: CAGS enhances 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet.

Conclusion: CAGS enables robust language-guided 3D scene understanding by addressing key challenges in cross-view granularity and spatial context integration.

Abstract: Open-vocabulary 3D scene understanding is crucial for applications requiring
natural language-driven spatial interpretation, such as robotics and augmented
reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation
for scene reconstruction, integrating it with open-vocabulary frameworks
reveals a key challenge: cross-view granularity inconsistency. This issue,
stemming from 2D segmentation methods like SAM, results in inconsistent object
segmentations across views (e.g., a "coffee set" segmented as a single entity
in one view but as "cup + coffee + spoon" in another). Existing 3DGS-based
methods often rely on isolated per-Gaussian feature learning, neglecting the
spatial context needed for cohesive object reasoning, leading to fragmented
representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel
framework that addresses this challenge by incorporating spatial context into
3DGS. CAGS constructs local graphs to propagate contextual features across
Gaussians, reducing noise from inconsistent granularity, employs mask-centric
contrastive learning to smooth SAM-derived features across views, and leverages
a precomputation strategy to reduce computational cost by precomputing
neighborhood relationships, enabling efficient training in large-scale scenes.
By integrating spatial context, CAGS significantly improves 3D instance
segmentation and reduces fragmentation errors on datasets like LERF-OVS and
ScanNet, enabling robust language-guided 3D scene understanding.

</details>


### [81] [Search is All You Need for Few-shot Anomaly Detection](https://arxiv.org/abs/2504.11895)
*Qishan Wang,Jia Guo,Shuyong Gao,Haofen Wang,Li Xiong,Junjie Hu,Hanqi Guo,Wenqiang Zhang*

Main category: cs.CV

TL;DR: VisionAD, a simple nearest-neighbor framework, outperforms state-of-the-art FSAD methods with minimal training and superior few-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of few-shot anomaly detection in industrial inspection, where existing methods require complex prompt engineering and tuning.

Method: VisionAD uses scalable vision models, dual augmentation, multi-layer feature integration, and a class-aware memory bank for efficient anomaly detection.

Result: Achieves AUROC scores of 97.4%, 94.8%, and 70.8% on MVTec-AD, VisA, and Real-IAD benchmarks, outperforming competitors by +1.6%, +3.2%, and +1.4%.

Conclusion: VisionAD's training-free approach and high performance make it ideal for real-world applications with scarce data.

Abstract: Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging
task in industrial inspection, where normal distribution modeling must be
accomplished with only a few normal images. While existing approaches typically
employ multi-modal foundation models combining language and vision modalities
for prompt-guided anomaly detection, these methods often demand sophisticated
prompt engineering and extensive manual tuning. In this paper, we demonstrate
that a straightforward nearest-neighbor search framework can surpass
state-of-the-art performance in both single-class and multi-class FSAD
scenarios. Our proposed method, VisionAD, consists of four simple yet essential
components: (1) scalable vision foundation models that extract universal and
discriminative features; (2) dual augmentation strategies - support
augmentation to enhance feature matching adaptability and query augmentation to
address the oversights of single-view prediction; (3) multi-layer feature
integration that captures both low-frequency global context and high-frequency
local details with minimal computational overhead; and (4) a class-aware visual
memory bank enabling efficient one-for-all multi-class detection. Extensive
evaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate
VisionAD's exceptional performance. Using only 1 normal images as support, our
method achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%
respectively, outperforming current state-of-the-art approaches by significant
margins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior
few-shot capabilities of VisionAD make it particularly appealing for real-world
applications where samples are scarce or expensive to obtain. Code is available
at https://github.com/Qiqigeww/VisionAD.

</details>


### [82] [Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement](https://arxiv.org/abs/2504.11896)
*Xingxing Yang,Jie Chen,Zaifeng Yang*

Main category: cs.CV

TL;DR: A novel low-light image enhancement method, PiCat, uses physics-informed priors and a Color-aware Transform (CAT) to handle lighting variations, combined with a Content-Noise Decomposition Network (CNDN) for noise reduction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with inconsistent color predictions and sensitivity to lighting variations, leading to unstable performance in diverse conditions.

Method: Proposes PiCat, combining CAT for illumination-invariant descriptors and CNDN for noise reduction, leveraging physics-informed priors for robust enhancement.

Result: PiCat outperforms state-of-the-art methods on five benchmark datasets.

Conclusion: The PiCat framework effectively addresses challenges in low-light image enhancement by integrating physics-informed priors and noise reduction, achieving superior performance.

Abstract: Image decomposition offers deep insights into the imaging factors of visual
data and significantly enhances various advanced computer vision tasks. In this
work, we introduce a novel approach to low-light image enhancement based on
decomposed physics-informed priors. Existing methods that directly map
low-light to normal-light images in the sRGB color space suffer from
inconsistent color predictions and high sensitivity to spectral power
distribution (SPD) variations, resulting in unstable performance under diverse
lighting conditions. To address these challenges, we introduce a
Physics-informed Color-aware Transform (PiCat), a learning-based framework that
converts low-light images from the sRGB color space into deep
illumination-invariant descriptors via our proposed Color-aware Transform
(CAT). This transformation enables robust handling of complex lighting and SPD
variations. Complementing this, we propose the Content-Noise Decomposition
Network (CNDN), which refines the descriptor distributions to better align with
well-lit conditions by mitigating noise and other distortions, thereby
effectively restoring content representations to low-light images. The CAT and
the CNDN collectively act as a physical prior, guiding the transformation
process from low-light to normal-light domains. Our proposed PiCat framework
demonstrates superior performance compared to state-of-the-art methods across
five benchmark datasets.

</details>


### [83] [AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection](https://arxiv.org/abs/2504.11914)
*Yuhao Chao,Jie Liu,Jie Tang,Gangshan Wu*

Main category: cs.CV

TL;DR: AnomalyR1 leverages VLM-R1 and GRPO with ROAM for end-to-end industrial anomaly detection, outperforming existing methods with limited defective data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scarce defective samples in IAD, traditional methods' limitations necessitate a paradigm shift.

Method: Integrates VLM-R1 (MLLM) with GRPO and ROAM for autonomous anomaly localization and masking.

Result: Achieves state-of-the-art performance on multimodal IAD benchmarks with a compact 3B-parameter model.

Conclusion: Demonstrates the transformative potential of MLLM-based IAD, positioning AnomalyR1 as a cornerstone for future systems.

Abstract: Industrial Anomaly Detection (IAD) poses a formidable challenge due to the
scarcity of defective samples, making it imperative to deploy models capable of
robust generalization to detect unseen anomalies effectively. Traditional
approaches, often constrained by hand-crafted features or domain-specific
expert models, struggle to address this limitation, underscoring the need for a
paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages
VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional
generalization and interpretability, to revolutionize IAD. By integrating MLLM
with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned
Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution
that autonomously processes inputs of image and domain knowledge, reasons
through analysis, and generates precise anomaly localizations and masks. Based
on the latest multimodal IAD benchmark, our compact 3-billion-parameter model
outperforms existing methods, establishing state-of-the-art results. As MLLM
capabilities continue to advance, this study is the first to deliver an
end-to-end VLM-based IAD solution that demonstrates the transformative
potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking
cornerstone for next-generation intelligent anomaly detection systems in
industrial applications with limited defective data.

</details>


### [84] [Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach](https://arxiv.org/abs/2504.11922)
*Lvpan Cai,Haowei Wang,Jiayi Ji,YanShu ZhouMen,Yiwei Ma,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces BR-Gen, a dataset for detecting localized AI-generated image forgeries, and NFA-ViT, a transformer-based method to improve detection robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in datasets and methods for detecting scene-level AI-generated forgeries, which are often overlooked in favor of object-level forgeries.

Method: BR-Gen is created via an automated pipeline for semantic coherence. NFA-ViT uses noise fingerprints and attention mechanisms to detect forgeries.

Result: BR-Gen covers new scenarios, and NFA-ViT outperforms existing methods on it and other benchmarks.

Conclusion: The work advances localized forgery detection with a novel dataset and method, demonstrating superior performance and generalization.

Abstract: The rise of AI-generated image editing tools has made localized forgeries
increasingly realistic, posing challenges for visual content integrity.
Although recent efforts have explored localized AIGC detection, existing
datasets predominantly focus on object-level forgeries while overlooking
broader scene edits in regions such as sky or ground. To address these
limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000
locally forged images with diverse scene-aware annotations, which are based on
semantic calibration to ensure high-quality samples. BR-Gen is constructed
through a fully automated Perception-Creation-Evaluation pipeline to ensure
semantic coherence and visual realism. In addition, we further propose
\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that
enhances the detection of localized forgeries by amplifying forgery-related
features across the entire image. NFA-ViT mines heterogeneous regions in
images, \emph{i.e.}, potential edited areas, by noise fingerprints.
Subsequently, attention mechanism is introduced to compel the interaction
between normal and abnormal features, thereby propagating the generalization
traces throughout the entire image, allowing subtle forgeries to influence a
broader context and improving overall detection robustness. Extensive
experiments demonstrate that BR-Gen constructs entirely new scenarios that are
not covered by existing methods. Take a step further, NFA-ViT outperforms
existing methods on BR-Gen and generalizes well across current benchmarks. All
data and codes are available at https://github.com/clpbc/BR-Gen.

</details>


### [85] [Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning](https://arxiv.org/abs/2504.11930)
*Hairui Ren,Fan Tang,He Zhao,Zixuan Wang,Dandan Guo,Yi Chang*

Main category: cs.CV

TL;DR: AiR introduces a method to improve unsupervised prompt learning in VLMs by using high-fidelity synthetic samples for pseudo-labeling and enhancing semantic-visual alignment.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality pseudo-labeled data and mismatches between semantic and visual information hinder unsupervised prompt learning in VLMs.

Method: AiR leverages diffusion-based synthetic samples to create an auxiliary classifier, enriching visual variation and improving classification. It also enhances prompt learning by exploiting synthetic sample diversity.

Result: AiR outperforms state-of-the-art methods on five benchmarks, including RESISC45 and Flowers102, across three learning paradigms.

Conclusion: AiR effectively addresses pseudo-labeling challenges and improves unsupervised prompt learning performance in VLMs.

Abstract: Fine-tuning vision-language models (VLMs) with large amounts of unlabeled
data has recently garnered significant interest. However, a key challenge
remains the lack of high-quality pseudo-labeled data. Current pseudo-labeling
strategies often struggle with mismatches between semantic and visual
information, leading to sub-optimal performance of unsupervised prompt learning
(UPL) methods. In this paper, we introduce a simple yet effective approach
called \textbf{A}ugmenting D\textbf{i}scriminative \textbf{R}ichness via
Diffusions (AiR), toward learning a richer discriminating way to represent the
class comprehensively and thus facilitate classification. Specifically, our
approach includes a pseudo-label generation module that leverages high-fidelity
synthetic samples to create an auxiliary classifier, which captures richer
visual variation, bridging text-image-pair classification to a more robust
image-image-pair classification. Additionally, we exploit the diversity of
diffusion-based synthetic samples to enhance prompt learning, providing greater
information for semantic-visual alignment. Extensive experiments on five public
benchmarks, including RESISC45 and Flowers102, and across three learning
paradigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and
consistent performance improvements over state-of-the-art unsupervised prompt
learning methods.

</details>


### [86] [R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors](https://arxiv.org/abs/2504.11946)
*Haoyang Wang,Liming Liu,Peiheng Wang,Junlin Hao,Jiangkai Wu,Xinggong Zhang*

Main category: cs.CV

TL;DR: A novel framework enhances sparse-view mesh reconstruction using diffusion models, addressing instability with a Consensus Diffusion Module and adaptive viewpoint selection via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Performance of mesh reconstruction degrades under sparse-view conditions, especially in unseen regions, and diffusion models often produce inconsistent outputs.

Method: Proposes a Consensus Diffusion Module for reliable pseudo-supervision and an online reinforcement learning strategy for adaptive viewpoint selection, combined with NeRF-based supervision.

Result: Significant improvements in geometric and rendering quality are demonstrated through extensive experiments.

Conclusion: The framework effectively leverages diffusion models for robust and consistent sparse-view mesh reconstruction.

Abstract: Mesh reconstruction from multi-view images is a fundamental problem in
computer vision, but its performance degrades significantly under sparse-view
conditions, especially in unseen regions where no ground-truth observations are
available. While recent advances in diffusion models have demonstrated strong
capabilities in synthesizing novel views from limited inputs, their outputs
often suffer from visual artifacts and lack 3D consistency, posing challenges
for reliable mesh optimization. In this paper, we propose a novel framework
that leverages diffusion models to enhance sparse-view mesh reconstruction in a
principled and reliable manner. To address the instability of diffusion
outputs, we propose a Consensus Diffusion Module that filters unreliable
generations via interquartile range (IQR) analysis and performs variance-aware
image fusion to produce robust pseudo-supervision. Building on this, we design
an online reinforcement learning strategy based on the Upper Confidence Bound
(UCB) to adaptively select the most informative viewpoints for enhancement,
guided by diffusion loss. Finally, the fused images are used to jointly
supervise a NeRF-based model alongside sparse-view ground truth, ensuring
consistency across both geometry and appearance. Extensive experiments
demonstrate that our method achieves significant improvements in both geometric
quality and rendering quality.

</details>


### [87] [Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation](https://arxiv.org/abs/2504.11949)
*Jie Wang,Chen Ye Gan,Caoqi Wei,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: Flow Intelligence introduces a motion-based approach for video feature matching, eliminating the need for spatial features or pretraining, and excels in robustness across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional spatial feature matching fails with noisy, misaligned, or cross-modal data, and deep learning methods are limited by data and computational demands.

Method: Extracts temporal motion signatures from pixel blocks across frames, focusing on motion patterns instead of spatial features.

Result: Achieves invariance to translation, rotation, and scale, works cross-modally without pretraining, and outperforms traditional methods.

Conclusion: Flow Intelligence enables robust, real-time video feature matching by leveraging motion, surpassing spatial-feature-based approaches.

Abstract: Feature matching across video streams remains a cornerstone challenge in
computer vision. Increasingly, robust multimodal matching has garnered interest
in robotics, surveillance, remote sensing, and medical imaging. While
traditional rely on detecting and matching spatial features, they break down
when faced with noisy, misaligned, or cross-modal data. Recent deep learning
methods have improved robustness through learned representations, but remain
constrained by their dependence on extensive training data and computational
demands. We present Flow Intelligence, a paradigm-shifting approach that moves
beyond spatial features by focusing on temporal motion patterns exclusively.
Instead of detecting traditional keypoints, our method extracts motion
signatures from pixel blocks across consecutive frames and extract temporal
motion signatures between videos. These motion-based descriptors achieve
natural invariance to translation, rotation, and scale variations while
remaining robust across different imaging modalities. This novel approach also
requires no pretraining data, eliminates the need for spatial feature
detection, enables cross-modal matching using only temporal motion, and it
outperforms existing methods in challenging scenarios where traditional
approaches fail. By leveraging motion rather than appearance, Flow Intelligence
enables robust, real-time video feature matching in diverse environments.

</details>


### [88] [Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions](https://arxiv.org/abs/2504.11967)
*Yifei Dong,Fengyi Wu,Sanjian Zhang,Guangyu Chen,Yuzhi Hu,Masumi Yano,Jingdong Sun,Siyu Huang,Feng Liu,Qi Dai,Zhi-Qi Cheng*

Main category: cs.CV

TL;DR: This survey examines anti-UAV technologies, focusing on classification, detection, and tracking, and evaluates state-of-the-art methods and benchmarks. It identifies gaps in real-time performance and swarm detection, urging innovation for next-gen defense systems.


<details>
  <summary>Details</summary>
Motivation: UAVs are widely used but pose security risks, necessitating advanced anti-UAV solutions.

Method: The survey reviews methodologies like diffusion-based synthesis, multi-modal fusion, and self-supervised learning, evaluating single and multi-sensor pipelines.

Result: Gaps in real-time performance, stealth detection, and swarm scenarios are identified.

Conclusion: The paper calls for robust, adaptive anti-UAV systems and highlights open research directions to guide future defense strategies.

Abstract: Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure
inspection, surveillance, and related tasks, yet they also introduce critical
security challenges. This survey provides a wide-ranging examination of the
anti-UAV domain, centering on three core objectives-classification, detection,
and tracking-while detailing emerging methodologies such as diffusion-based
data synthesis, multi-modal fusion, vision-language modeling, self-supervised
learning, and reinforcement learning. We systematically evaluate
state-of-the-art solutions across both single-modality and multi-sensor
pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss
large-scale as well as adversarially oriented benchmarks. Our analysis reveals
persistent gaps in real-time performance, stealth detection, and swarm-based
scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.
By highlighting open research directions, we aim to foster innovation and guide
the development of next-generation defense strategies in an era marked by the
extensive use of UAVs.

</details>


### [89] [A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions](https://arxiv.org/abs/2504.11995)
*Rahima Khanam,Muhammad Hussain*

Main category: cs.CV

TL;DR: YOLOv12 integrates attention mechanisms efficiently, improving real-time object detection with innovations like Area Attention and Residual Efficient Layer Aggregation Networks, while maintaining speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance YOLO's performance by incorporating attention mechanisms without compromising real-time capabilities, addressing computational overhead challenges.

Method: Introduces Area Attention for efficient self-attention, Residual Efficient Layer Aggregation Networks for better feature aggregation, and FlashAttention for optimized memory access.

Result: YOLOv12 outperforms prior YOLO versions and competitors in accuracy, inference speed, and computational efficiency.

Conclusion: YOLOv12 advances real-time object detection by optimizing the latency-accuracy trade-off and computational resource usage.

Abstract: The YOLO (You Only Look Once) series has been a leading framework in
real-time object detection, consistently improving the balance between speed
and accuracy. However, integrating attention mechanisms into YOLO has been
challenging due to their high computational overhead. YOLOv12 introduces a
novel approach that successfully incorporates attention-based enhancements
while preserving real-time performance. This paper provides a comprehensive
review of YOLOv12's architectural innovations, including Area Attention for
computationally efficient self-attention, Residual Efficient Layer Aggregation
Networks for improved feature aggregation, and FlashAttention for optimized
memory access. Additionally, we benchmark YOLOv12 against prior YOLO versions
and competing object detectors, analyzing its improvements in accuracy,
inference speed, and computational efficiency. Through this analysis, we
demonstrate how YOLOv12 advances real-time object detection by refining the
latency-accuracy trade-off and optimizing computational resources.

</details>


### [90] [A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning](https://arxiv.org/abs/2504.11999)
*Mengyu Wang,Hanbo Bi,Yingchao Feng,Linlin Xin,Shuo Gong,Tianqi Wang,Zhiyuan Yan,Peijin Wang,Wenhui Diao,Xian Sun*

Main category: cs.CV

TL;DR: A foundation model for SAR image interpretation is proposed, using complex-valued data and polarimetric decomposition for pre-training, achieving state-of-the-art results on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of insufficient information utilization and poor interpretability in SAR image interpretation by leveraging polarimetric decomposition.

Method: Constructs scattering queries to represent scattering bases, uses polarimetric decomposition loss and power self-supervision loss for pre-training.

Result: Achieves state-of-the-art performance on six downstream tasks, with strong generalization even in data-scarce conditions.

Conclusion: The foundation model provides interpretable and stable feature representations, demonstrating superior generalization for SAR image tasks.

Abstract: Vision foundation models in remote sensing have been extensively studied due
to their superior generalization on various downstream tasks. Synthetic
Aperture Radar (SAR) offers all-day, all-weather imaging capabilities,
providing significant advantages for Earth observation. However, establishing a
foundation model for SAR image interpretation inevitably encounters the
challenges of insufficient information utilization and poor interpretability.
In this paper, we propose a remote sensing foundation model based on
complex-valued SAR data, which simulates the polarimetric decomposition process
for pre-training, i.e., characterizing pixel scattering intensity as a weighted
combination of scattering bases and scattering coefficients, thereby endowing
the foundation model with physical interpretability. Specifically, we construct
a series of scattering queries, each representing an independent and meaningful
scattering basis, which interact with SAR features in the scattering query
decoder and output the corresponding scattering coefficient. To guide the
pre-training process, polarimetric decomposition loss and power
self-supervision loss are constructed. The former aligns the predicted
coefficients with Yamaguchi coefficients, while the latter reconstructs power
from the predicted coefficients and compares it to the input image's power. The
performance of our foundation model is validated on six typical downstream
tasks, achieving state-of-the-art results. Notably, the foundation model can
extract stable feature representations and exhibits strong generalization, even
in data-scarce conditions.

</details>


### [91] [Instruction-augmented Multimodal Alignment for Image-Text and Element Matching](https://arxiv.org/abs/2504.12018)
*Xinli Yue,JianHui Sun,Junda Lu,Liangchao Yao,Fan Xia,Tianyi Wang,Fengyun Rao,Jing Lyu,Yuetang Deng*

Main category: cs.CV

TL;DR: The paper introduces iMatch, an improved method for evaluating semantic alignment between text and generated images, using multimodal large language models and innovative augmentation strategies. It outperforms existing methods and won a CVPR competition.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing image-text alignment, like VQA-based approaches, lack fine-grained precision. The need for a more accurate and robust evaluation method drives this research.

Method: iMatch fine-tunes multimodal large language models with four augmentation strategies: QAlign for score conversion, validation set augmentation for data expansion, element augmentation for better matching, and image augmentation for robustness. Additional strategies like prompt type augmentation and score perturbation are also used.

Result: iMatch significantly outperforms existing methods and won first place in the CVPR NTIRE 2025 competition for image-text alignment assessment.

Conclusion: iMatch is an effective and practical solution for evaluating semantic alignment in text-to-image generation, validated by superior performance and competition success.

Abstract: With the rapid advancement of text-to-image (T2I) generation models,
assessing the semantic alignment between generated images and text descriptions
has become a significant research challenge. Current methods, including those
based on Visual Question Answering (VQA), still struggle with fine-grained
assessments and precise quantification of image-text alignment. This paper
presents an improved evaluation method named Instruction-augmented Multimodal
Alignment for Image-Text and Element Matching (iMatch), which evaluates
image-text semantic alignment by fine-tuning multimodal large language models.
We introduce four innovative augmentation strategies: First, the QAlign
strategy creates a precise probabilistic mapping to convert discrete scores
from multimodal large language models into continuous matching scores. Second,
a validation set augmentation strategy uses pseudo-labels from model
predictions to expand training data, boosting the model's generalization
performance. Third, an element augmentation strategy integrates element
category labels to refine the model's understanding of image-text matching.
Fourth, an image augmentation strategy employs techniques like random lighting
to increase the model's robustness. Additionally, we propose prompt type
augmentation and score perturbation strategies to further enhance the accuracy
of element assessments. Our experimental results show that the iMatch method
significantly surpasses existing methods, confirming its effectiveness and
practical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025
Text to Image Generation Model Quality Assessment - Track 1 Image-Text
Alignment.

</details>


### [92] [MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes](https://arxiv.org/abs/2504.12020)
*Shiwei Gan,Yafeng Yin,Zhiwei Jiang,Hongkai Wen,Lei Xie,Sanglu Lu*

Main category: cs.CV

TL;DR: MixSignGraph, a graph-based method, improves sign language feature extraction by focusing on spatial, temporal, and hierarchical features, outperforming SOTA models without gloss annotations.


<details>
  <summary>Details</summary>
Motivation: CNN-based backbones struggle with capturing sign-related features, necessitating a method to focus on region-specific and collaborative features in sign language.

Method: MixSignGraph uses Local Sign Graph (LSG), Temporal Sign Graph (TSG), and Hierarchical Sign Graph (HSG) modules to extract spatial, temporal, and hierarchical features. A Text-driven CTC Pre-training (TCP) method generates pseudo gloss labels for pre-training.

Result: The model outperforms SOTA models on multiple sign language tasks across five datasets without additional cues.

Conclusion: MixSignGraph effectively captures sign-related features and improves performance, demonstrating its superiority in sign language tasks.

Abstract: Recent advances in sign language research have benefited from CNN-based
backbones, which are primarily transferred from traditional computer vision
tasks (\eg object identification, image recognition). However, these CNN-based
backbones usually excel at extracting features like contours and texture, but
may struggle with capturing sign-related features. In fact, sign language tasks
require focusing on sign-related regions, including the collaboration between
different regions (\eg left hand region and right hand region) and the
effective content in a single region. To capture such region-related features,
we introduce MixSignGraph, which represents sign sequences as a group of mixed
graphs and designs the following three graph modules for feature extraction,
\ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and
Hierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the
correlation of intra-frame cross-region features within one frame, \ie focusing
on spatial features. The TSG module tracks the interaction of inter-frame
cross-region features among adjacent frames, \ie focusing on temporal features.
The HSG module aggregates the same-region features from different-granularity
feature maps of a frame, \ie focusing on hierarchical features. In addition, to
further improve the performance of sign language tasks without gloss
annotations, we propose a simple yet counter-intuitive Text-driven CTC
Pre-training (TCP) method, which generates pseudo gloss labels from text labels
for model pre-training. Extensive experiments conducted on current five public
sign language datasets demonstrate the superior performance of the proposed
model. Notably, our model surpasses the SOTA models on multiple sign language
tasks across several datasets, without relying on any additional cues.

</details>


### [93] [Action Anticipation from SoccerNet Football Video Broadcasts](https://arxiv.org/abs/2504.12021)
*Mohamad Dalal,Artur Xarles,Anthony Cioppa,Silvio Giancola,Marc Van Droogenbroeck,Bernard Ghanem,Albert Clapés,Sergio Escalera,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: The paper introduces action anticipation in football videos, proposing a new dataset (SoccerNet Ball Action Anticipation) and a baseline method (FAANTRA) to predict ball-related actions. It evaluates performance with new metrics and highlights applications in sports analytics.


<details>
  <summary>Details</summary>
Motivation: Little attention has been given to anticipating game actions before they occur, despite AI advancements in sports video analysis. This work aims to address this gap by predicting future actions in football broadcasts.

Method: The paper proposes FAANTRA, a baseline method adapting FUTR for ball-related action anticipation. It introduces new metrics (mAP@δ and mAP@∞) and conducts ablation studies on task settings, inputs, and architectures.

Result: Experimental results show feasibility and challenges in action anticipation, providing insights for predictive sports analytics models.

Conclusion: The work enables applications in automated broadcasting, tactical analysis, and player decision-making by forecasting actions before they unfold. The dataset and code are publicly available.

Abstract: Artificial intelligence has revolutionized the way we analyze sports videos,
whether to understand the actions of games in long untrimmed videos or to
anticipate the player's motion in future frames. Despite these efforts, little
attention has been given to anticipating game actions before they occur. In
this work, we introduce the task of action anticipation for football broadcast
videos, which consists in predicting future actions in unobserved future
frames, within a five- or ten-second anticipation window. To benchmark this
task, we release a new dataset, namely the SoccerNet Ball Action Anticipation
dataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a
Football Action ANticipation TRAnsformer (FAANTRA), a baseline method that
adapts FUTR, a state-of-the-art action anticipation model, to predict
ball-related actions. To evaluate action anticipation, we introduce new
metrics, including mAP@$\delta$, which evaluates the temporal precision of
predicted future actions, as well as mAP@$\infty$, which evaluates their
occurrence within the anticipation window. We also conduct extensive ablation
studies to examine the impact of various task settings, input configurations,
and model architectures. Experimental results highlight both the feasibility
and challenges of action anticipation in football videos, providing valuable
insights into the design of predictive models for sports analytics. By
forecasting actions before they unfold, our work will enable applications in
automated broadcasting, tactical analysis, and player decision-making. Our
dataset and code are publicly available at
https://github.com/MohamadDalal/FAANTRA.

</details>


### [94] [Understanding Attention Mechanism in Video Diffusion Models](https://arxiv.org/abs/2504.12027)
*Bingyan Liu,Chengyu Wang,Tongtong Su,Huan Ten,Jun Huang,Kailing Guo,Kui Jia*

Main category: cs.CV

TL;DR: The paper analyzes spatial and temporal attention blocks in diffusion-based T2V models, revealing their impact on video quality and structure. It proposes lightweight methods to enhance quality and enable text-guided editing.


<details>
  <summary>Details</summary>
Motivation: Understanding how attention mechanisms in T2V models influence video synthesis, particularly quality and temporal consistency, is unclear.

Method: An in-depth perturbation analysis of attention blocks using an information-theoretic approach.

Result: Temporal and spatial attention maps affect video timing, layout, complexity, and aesthetics. High-entropy maps correlate with superior quality, while low-entropy maps relate to intra-frame structure.

Conclusion: Proposed lightweight methods for attention matrix manipulation improve video quality and enable text-guided editing, validated across datasets.

Abstract: Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered
significant attention due to their ability to generate high-quality videos from
a text prompt. In diffusion-based T2V models, the attention mechanism is a
critical component. However, it remains unclear what intermediate features are
learned and how attention blocks in T2V models affect various aspects of video
synthesis, such as image quality and temporal consistency. In this paper, we
conduct an in-depth perturbation analysis of the spatial and temporal attention
blocks of T2V models using an information-theoretic approach. Our results
indicate that temporal and spatial attention maps affect not only the timing
and layout of the videos but also the complexity of spatiotemporal elements and
the aesthetic quality of the synthesized videos. Notably, high-entropy
attention maps are often key elements linked to superior video quality, whereas
low-entropy attention maps are associated with the video's intra-frame
structure. Based on our findings, we propose two novel methods to enhance video
quality and enable text-guided video editing. These methods rely entirely on
lightweight manipulation of the attention matrices in T2V models. The efficacy
and effectiveness of our methods are further validated through experimental
evaluation across multiple datasets.

</details>


### [95] [Object Placement for Anything](https://arxiv.org/abs/2504.12029)
*Bingjie Gao,Bo Zhang,Li Niu*

Main category: cs.CV

TL;DR: A semi-supervised framework is proposed to improve object placement models by leveraging large-scale unlabeled data and transferring knowledge of rationality variation.


<details>
  <summary>Details</summary>
Motivation: Previous works are limited by small-scale labeled datasets, hindering real-world application of object placement.

Method: A semi-supervised framework exploits unlabeled data and transfers knowledge of rationality variation from labeled to unlabeled data.

Result: The framework effectively enhances the generalization ability of discriminative object placement models.

Conclusion: The proposed method successfully addresses limitations of small datasets and improves model performance.

Abstract: Object placement aims to determine the appropriate placement (\emph{e.g.},
location and size) of a foreground object when placing it on the background
image. Most previous works are limited by small-scale labeled dataset, which
hinders the real-world application of object placement. In this work, we devise
a semi-supervised framework which can exploit large-scale unlabeled dataset to
promote the generalization ability of discriminative object placement models.
The discriminative models predict the rationality label for each foreground
placement given a foreground-background pair. To better leverage the labeled
data, under the semi-supervised framework, we further propose to transfer the
knowledge of rationality variation, \emph{i.e.}, whether the change of
foreground placement would result in the change of rationality label, from
labeled data to unlabeled data. Extensive experiments demonstrate that our
framework can effectively enhance the generalization ability of discriminative
object placement models.

</details>


### [96] [RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model](https://arxiv.org/abs/2504.12039)
*Yizhuo Wu,Francesco Fioranelli,Chang Gao*

Main category: cs.CV

TL;DR: RadMamba, a lightweight Mamba SSM for radar-based HAR, achieves high accuracy with minimal parameters, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Address the computational demands of current radar-based HAR methods while maintaining accuracy.

Method: Introduces RadMamba, a parameter-efficient Mamba SSM tailored for radar micro-Doppler signals.

Result: Achieves 99.8% accuracy on Dataset DIAT with 1/400 parameters and 92.0% on Dataset CI4R with 1/10 parameters. Outperforms others by 3% on Dataset UoG2020 with only 6.7k parameters.

Conclusion: RadMamba offers a lightweight, accurate solution for radar-based HAR, suitable for resource-constrained scenarios.

Abstract: Radar-based HAR has emerged as a promising alternative to conventional
monitoring approaches, such as wearable devices and camera-based systems, due
to its unique privacy preservation and robustness advantages. However, existing
solutions based on convolutional and recurrent neural networks, although
effective, are computationally demanding during deployment. This limits their
applicability in scenarios with constrained resources or those requiring
multiple sensors. Advanced architectures, such as ViT and SSM architectures,
offer improved modeling capabilities and have made efforts toward lightweight
designs. However, their computational complexity remains relatively high. To
leverage the strengths of transformer architectures while simultaneously
enhancing accuracy and reducing computational complexity, this paper introduces
RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM
specifically tailored for radar-based HAR. Across three diverse datasets,
RadMamba matches the top-performing previous model's 99.8% classification
accuracy on Dataset DIAT with only 1/400 of its parameters and equals the
leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their
parameters. In scenarios with continuous sequences of actions evaluated on
Dataset UoG2020, RadMamba surpasses other models with significantly higher
parameter counts by at least 3%, achieving this with only 6.7k parameters. Our
code is available at: https://github.com/lab-emi/AIRHAR.

</details>


### [97] [pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild](https://arxiv.org/abs/2504.12045)
*Jonas Myhre Schiøtt,Viktor Sebastian Petersen,Dimitrios P. Papadopoulos*

Main category: cs.CV

TL;DR: The paper introduces pix2pockets, an RL-assisted pool coach system for detecting pool table objects and suggesting optimal shots, achieving high accuracy in detection and baseline performance in RL-based shot suggestions.


<details>
  <summary>Details</summary>
Motivation: Leverage advances in computer vision and reinforcement learning to develop a system for assisting in the game of 8-ball pool, addressing object detection and optimal shot suggestions.

Method: Created a dataset of 195 annotated images for object segmentation, developed an RL environment for benchmarking, and tested standard RL algorithms for shot suggestions.

Result: Object detection achieved 91.2 AP50, ball location error was 0.4 cm. RL baselines failed to pocket all balls without fouls, but a simple baseline achieved 94.7% per-shot success and 30% full-game clearance.

Conclusion: The system shows promise in object detection and shot suggestion, though RL algorithms need improvement for flawless gameplay.

Abstract: Computer vision models have seen increased usage in sports, and reinforcement
learning (RL) is famous for beating humans in strategic games such as Chess and
Go. In this paper, we are interested in building upon these advances and
examining the game of classic 8-ball pool. We introduce pix2pockets, a
foundation for an RL-assisted pool coach. Given a single image of a pool table,
we first aim to detect the table and the balls and then propose the optimal
shot suggestion. For the first task, we build a dataset with 195 diverse images
where we manually annotate all balls and table dots, leading to 5748 object
segmentation masks. For the second task, we build a standardized RL environment
that allows easy development and benchmarking of any RL algorithm. Our object
detection model yields an AP50 of 91.2 while our ball location pipeline obtains
an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set
a baseline for the shot suggestion task and we show that all of them fail to
pocket all balls without making a foul move. We also present a simple baseline
that achieves a per-shot success rate of 94.7% and clears a full game in a
single turn 30% of the time.

</details>


### [98] [Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM](https://arxiv.org/abs/2504.12048)
*Zirui Pan,Xin Wang,Yipeng Zhang,Hong Chen,Kwan Man Cheng,Yaofei Wu,Wenwu Zhu*

Main category: cs.CV

TL;DR: The paper introduces Modular-Cam, a method for text-to-video generation that handles complex prompts by decomposing them into scenes and controlling camera movements.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex prompts involving dynamic scenes and camera-view changes, failing to decompose or transition smoothly.

Method: Modular-Cam uses a large language model to analyze prompts, a temporal transformer for scene continuity, CamOperator for camera control, and AdaControlNet for consistency.

Result: Experiments show Modular-Cam effectively generates multi-scene videos with fine-grained camera control.

Conclusion: Modular-Cam addresses limitations of current methods, offering improved performance in complex text-to-video generation.

Abstract: Text-to-Video generation, which utilizes the provided text prompt to generate
high-quality videos, has drawn increasing attention and achieved great success
due to the development of diffusion models recently. Existing methods mainly
rely on a pre-trained text encoder to capture the semantic information and
perform cross attention with the encoded text prompt to guide the generation of
video. However, when it comes to complex prompts that contain dynamic scenes
and multiple camera-view transformations, these methods can not decompose the
overall information into separate scenes, as well as fail to smoothly change
scenes based on the corresponding camera-views. To solve these problems, we
propose a novel method, i.e., Modular-Cam. Specifically, to better understand a
given complex prompt, we utilize a large language model to analyze user
instructions and decouple them into multiple scenes together with transition
actions. To generate a video containing dynamic scenes that match the given
camera-views, we incorporate the widely-used temporal transformer into the
diffusion model to ensure continuity within a single scene and propose
CamOperator, a modular network based module that well controls the camera
movements. Moreover, we propose AdaControlNet, which utilizes ControlNet to
ensure consistency across scenes and adaptively adjusts the color tone of the
generated video. Extensive qualitative and quantitative experiments prove our
proposed Modular-Cam's strong capability of generating multi-scene videos
together with its ability to achieve fine-grained control of camera movements.
Generated results are available at https://modular-cam.github.io.

</details>


### [99] [Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects](https://arxiv.org/abs/2504.12078)
*Trina De,Adrian Urbanski,Artur Yakimovich*

Main category: cs.CV

TL;DR: The paper proposes two architectures, HydraStarDist (HSD) and HSD-WBR, for nested instance segmentation in biomedical images by leveraging spatial correlations and semantic relations between objects.


<details>
  <summary>Details</summary>
Motivation: Biomedical images often contain spatially correlated or nested objects, but existing methods treat detection tasks independently, missing opportunities for more efficient and meaningful representations.

Method: HSD and HSD-WBR extend StarDist (SD) to incorporate spatial correlation priors. HSD uses a joint encoder, while HSD-WBR adds a regularization layer with a Within Boundary Regularisation Penalty (WBR).

Result: Both architectures outperform baseline methods (SD and Cellpose) in nested instance segmentation, as measured by IoU_R, AP, and Joint TP rate (JTPR).

Conclusion: The approach enables single-shot, computationally efficient segmentation and can be adapted for multi-object interactions in microscopy or digital imaging.

Abstract: Biomedical images often contain objects known to be spatially correlated or
nested due to their inherent properties, leading to semantic relations.
Examples include cell nuclei being nested within eukaryotic cells and colonies
growing exclusively within their culture dishes. While these semantic relations
bear key importance, detection tasks are often formulated independently,
requiring multi-shot analysis pipelines. Importantly, spatial correlation could
constitute a fundamental prior facilitating learning of more meaningful
representations for tasks like instance segmentation. This knowledge has, thus
far, not been utilised by the biomedical computer vision community. We argue
that the instance segmentation of two or more categories of objects can be
achieved in parallel. We achieve this via two architectures HydraStarDist (HSD)
and the novel (HSD-WBR) based on the widely-used StarDist (SD), to take
advantage of the star-convexity of our target objects. HSD and HSD-WBR are
constructed to be capable of incorporating their interactions as constraints
into account. HSD implicitly incorporates spatial correlation priors based on
object interaction through a joint encoder. HSD-WBR further enforces the prior
in a regularisation layer with the penalty we proposed named Within Boundary
Regularisation Penalty (WBR). Both architectures achieve nested instance
segmentation in a single shot. We demonstrate their competitiveness based on
$IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate
(JTPR) compared to their baseline SD and Cellpose. Our approach can be further
modified to capture partial-inclusion/-exclusion in multi-object interactions
in fluorescent or brightfield microscopy or digital imaging. Finally, our
strategy suggests gains by making this learning single-shot and computationally
efficient.

</details>


### [100] [DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency](https://arxiv.org/abs/2504.12080)
*Mengshi Qi,Pengfei Zhu,Xiangtai Li,Xiaoyang Bi,Lu Qi,Huadong Ma,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DC-SAM adapts SAM and SAM2 for in-context segmentation using prompt-tuning, achieving state-of-the-art results on COCO-20i, PASCAL-5i, and a new IC-VOS benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the gap in applying Segment Anything Models (SAM) to in-context segmentation for images and videos.

Method: Proposes Dual Consistency SAM (DC-SAM) with prompt-tuning, feature fusion, cycle-consistent cross-attention, and a dual-branch design. Introduces a mask-tube training strategy and the IC-VOS benchmark for videos.

Result: Achieves 55.5 mIoU on COCO-20i, 73.0 mIoU on PASCAL-5i, and 71.52 J&F on IC-VOS.

Conclusion: DC-SAM effectively adapts SAM for in-context segmentation, with potential for video applications.

Abstract: Given a single labeled example, in-context segmentation aims to segment
corresponding objects. This setting, known as one-shot segmentation in few-shot
learning, explores the segmentation model's generalization ability and has been
applied to various vision tasks, including scene understanding and image/video
editing. While recent Segment Anything Models have achieved state-of-the-art
results in interactive segmentation, these approaches are not directly
applicable to in-context segmentation. In this work, we propose the Dual
Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2
for in-context segmentation of both images and videos. Our key insights are to
enhance the features of the SAM's prompt encoder in segmentation by providing
high-quality visual prompts. When generating a mask prior, we fuse the SAM
features to better align the prompt encoder. Then, we design a cycle-consistent
cross-attention on fused features and initial visual prompts. Next, a
dual-branch design is provided by using the discriminative positive and
negative prompts in the prompt encoder. Furthermore, we design a simple
mask-tube training strategy to adopt our proposed dual consistency method into
the mask tube. Although the proposed DC-SAM is primarily designed for images,
it can be seamlessly extended to the video domain with the support of SAM2.
Given the absence of in-context segmentation in the video domain, we manually
curate and construct the first benchmark from existing video segmentation
datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess
the in-context capability of the model. Extensive experiments demonstrate that
our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on
PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our
source code and benchmark are available at https://github.com/zaplm/DC-SAM.

</details>


### [101] [Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization](https://arxiv.org/abs/2504.12083)
*Pritam Sarkar,Ali Etemad*

Main category: cs.CV

TL;DR: The paper proposes a self-alignment framework for Large Video Language Models (LVLMs) to improve their fine-grained temporal understanding and reduce errors, introducing a novel method called Refined Regularized Preference Optimization (RRPO).


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with fine-grained temporal understanding, hallucination, and simple mistakes, limiting their real-world reliability.

Method: The framework uses self-alignment with preferred/non-preferred response pairs and introduces RRPO for precise alignment and stable training.

Result: RRPO outperforms Direct Preference Optimization (DPO), showing effectiveness in tasks like video hallucination and temporal reasoning.

Conclusion: The self-alignment framework and RRPO enhance LVLMs' performance, addressing key challenges for reliable deployment.

Abstract: Despite recent advances in Large Video Language Models (LVLMs), they still
struggle with fine-grained temporal understanding, hallucinate, and often make
simple mistakes on even simple video question-answering tasks, all of which
pose significant challenges to their safe and reliable deployment in real-world
applications. To address these limitations, we propose a self-alignment
framework that enables LVLMs to learn from their own errors. Our proposed
framework first obtains a training set of preferred and non-preferred response
pairs, where non-preferred responses are generated by incorporating common
error patterns that often occur due to inadequate spatio-temporal
understanding, spurious correlations between co-occurring concepts, and
over-reliance on linguistic cues while neglecting the vision modality, among
others. To facilitate self-alignment of LVLMs with the constructed preferred
and non-preferred response pairs, we introduce Refined Regularized Preference
Optimization (RRPO), a novel preference optimization method that utilizes
sub-sequence-level refined rewards and token-wise KL regularization to address
the limitations of Direct Preference Optimization (DPO). We demonstrate that
RRPO achieves more precise alignment and more stable training compared to DPO.
Our experiments and analysis validate the effectiveness of our approach across
diverse video tasks, including video hallucination, short- and long-video
understanding, and fine-grained temporal reasoning.

</details>


### [102] [AttentionDrop: A Novel Regularization Method for Transformer Models](https://arxiv.org/abs/2504.12088)
*Mirza Samad Ahmed Baig,Syeda Anshrah Gillani,Abdul Akbar Khan,Shahid Munir Shah*

Main category: cs.CV

TL;DR: AttentionDrop is a stochastic regularization method for Transformers, introducing three variants to mitigate overfitting by modifying self-attention distributions.


<details>
  <summary>Details</summary>
Motivation: Transformers often overfit with limited or noisy data. AttentionDrop addresses this by regularizing self-attention.

Method: Three variants: Hard Attention Masking, Blurred Attention Smoothing, and Consistency-Regularized AttentionDrop.

Result: Improved regularization for Transformers, reducing overfitting.

Conclusion: AttentionDrop effectively enhances Transformer robustness by diversifying attention patterns.

Abstract: Transformer-based architectures achieve state-of-the-art performance across a
wide range of tasks in natural language processing, computer vision, and
speech. However, their immense capacity often leads to overfitting, especially
when training data is limited or noisy. We propose AttentionDrop, a unified
family of stochastic regularization techniques that operate directly on the
self-attention distributions. We introduces three variants: 1. Hard Attention
Masking: randomly zeroes out top-k attention logits per query to encourage
diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic
Gaussian convolution over attention logits to diffuse overly peaked
distributions. 3. Consistency-Regularized AttentionDrop: enforces output
stability under multiple independent AttentionDrop perturbations via a KL-based
consistency loss.

</details>


### [103] [Generalized Visual Relation Detection with Diffusion Models](https://arxiv.org/abs/2504.12100)
*Kaifeng Gao,Siqi Chen,Hanwang Zhang,Jun Xiao,Yueting Zhuang,Qianru Sun*

Main category: cs.CV

TL;DR: Diff-VRD proposes a diffusion-based model for generalized visual relation detection (VRD), addressing semantic ambiguity by generating continuous relation embeddings beyond pre-defined categories.


<details>
  <summary>Details</summary>
Motivation: Current VRD models are limited to pre-defined relation categories and fail to handle semantic ambiguity in visual relations.

Method: Diff-VRD uses diffusion models to generate relation embeddings in a latent space, conditioned on visual and text embeddings of subject-object pairs, followed by a matching stage.

Result: The model outperforms benchmarks in human-object interaction (HOI) detection and scene graph generation (SGG), demonstrating its effectiveness.

Conclusion: Diff-VRD advances VRD by enabling generalized relation detection and introduces new evaluation metrics for this task.

Abstract: Visual relation detection (VRD) aims to identify relationships (or
interactions) between object pairs in an image. Although recent VRD models have
achieved impressive performance, they are all restricted to pre-defined
relation categories, while failing to consider the semantic ambiguity
characteristic of visual relations. Unlike objects, the appearance of visual
relations is always subtle and can be described by multiple predicate words
from different perspectives, e.g., ``ride'' can be depicted as ``race'' and
``sit on'', from the sports and spatial position views, respectively. To this
end, we propose to model visual relations as continuous embeddings, and design
diffusion models to achieve generalized VRD in a conditional generative manner,
termed Diff-VRD. We model the diffusion process in a latent space and generate
all possible relations in the image as an embedding sequence. During the
generation, the visual and text embeddings of subject-object pairs serve as
conditional signals and are injected via cross-attention. After the generation,
we design a subsequent matching stage to assign the relation words to
subject-object pairs by considering their semantic similarities. Benefiting
from the diffusion-based generative process, our Diff-VRD is able to generate
visual relations beyond the pre-defined category labels of datasets. To
properly evaluate this generalized VRD task, we introduce two evaluation
metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image
captioning. Extensive experiments in both human-object interaction (HOI)
detection and scene graph generation (SGG) benchmarks attest to the superiority
and effectiveness of Diff-VRD.

</details>


### [104] [Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image](https://arxiv.org/abs/2504.12103)
*Tao Wen,Jiepeng Wang,Yabo Chen,Shugong Xu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Metric-Solver introduces a sliding anchor-based method for metric depth estimation, dynamically adapting to varying scene scales for accurate and generalizable depth prediction.


<details>
  <summary>Details</summary>
Motivation: Diverse depth scales in indoor and outdoor environments make accurate metric depth estimation challenging.

Method: Uses an anchor-based representation to normalize scene depth into near-field and far-field components, dynamically adjusting the anchor for varying scales.

Result: Outperforms existing methods in accuracy and cross-dataset generalization.

Conclusion: Metric-Solver provides a unified and adaptive solution for metric depth estimation across diverse environments.

Abstract: Accurate and generalizable metric depth estimation is crucial for various
computer vision applications but remains challenging due to the diverse depth
scales encountered in indoor and outdoor environments. In this paper, we
introduce Metric-Solver, a novel sliding anchor-based metric depth estimation
method that dynamically adapts to varying scene scales. Our approach leverages
an anchor-based representation, where a reference depth serves as an anchor to
separate and normalize the scene depth into two components: scaled near-field
depth and tapered far-field depth. The anchor acts as a normalization factor,
enabling the near-field depth to be normalized within a consistent range while
mapping far-field depth smoothly toward zero. Through this approach, any depth
from zero to infinity in the scene can be represented within a unified
representation, effectively eliminating the need to manually account for scene
scale variations. More importantly, for the same scene, the anchor can slide
along the depth axis, dynamically adjusting to different depth scales. A
smaller anchor provides higher resolution in the near-field, improving depth
precision for closer objects while a larger anchor improves depth estimation in
far regions. This adaptability enables the model to handle depth predictions at
varying distances and ensure strong generalization across datasets. Our design
enables a unified and adaptive depth representation across diverse
environments. Extensive experiments demonstrate that Metric-Solver outperforms
existing methods in both accuracy and cross-dataset generalization.

</details>


### [105] [Logits DeConfusion with CLIP for Few-Shot Learning](https://arxiv.org/abs/2504.12104)
*Shuo Li,Fang Liu,Zehua Hao,Xinyi Wang,Lingling Li,Xu Liu,Puhua Chen,Wenping Ma*

Main category: cs.CV

TL;DR: The paper introduces Logits DeConfusion (LDC) to address CLIP's inter-class confusion in logits, improving classification accuracy via Multi-level Adapter Fusion (MAF) and Inter-Class Deconfusion (ICD) modules.


<details>
  <summary>Details</summary>
Motivation: CLIP's logits exhibit inter-class confusion in downstream tasks, harming accuracy.

Method: Proposes LDC with MAF for multi-level feature fusion and ICD for learnable logit deconfusion.

Result: Significantly improves classification performance and reduces inter-class confusion.

Conclusion: LDC effectively enhances CLIP's accuracy by addressing logit confusion.

Abstract: With its powerful visual-language alignment capability, CLIP performs well in
zero-shot and few-shot learning tasks. However, we found in experiments that
CLIP's logits suffer from serious inter-class confusion problems in downstream
tasks, and the ambiguity between categories seriously affects the accuracy. To
address this challenge, we propose a novel method called Logits DeConfusion,
which effectively learns and eliminates inter-class confusion in logits by
combining our Multi-level Adapter Fusion (MAF) module with our Inter-Class
Deconfusion (ICD) module. Our MAF extracts features from different levels and
fuses them uniformly to enhance feature representation. Our ICD learnably
eliminates inter-class confusion in logits with a residual structure.
Experimental results show that our method can significantly improve the
classification performance and alleviate the inter-class confusion problem. The
code is available at https://github.com/LiShuo1001/LDC.

</details>


### [106] [Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals](https://arxiv.org/abs/2504.12121)
*Jose Francisco Diez-Pastor,Francisco Javier Gonzalez-Moya,Pedro Latorre-Carmona,Francisco Javier Perez-Barbería,Ludmila I. Kuncheva,Antonio Canepa-Oneto,Alvar Arnaiz-González,Cesar Garcia-Osorio*

Main category: cs.CV

TL;DR: The paper evaluates machine learning algorithms to detect grazing trails formed by large herbivores, identifying UNet with MambaOut as the best method for conservation and monitoring.


<details>
  <summary>Details</summary>
Motivation: To detect areas of intense herbivore activity for ecosystem conservation and monitoring, focusing on grazing trails as indicators.

Method: Applied five semantic segmentation methods combined with fourteen encoders on aerial images to map grazing trails.

Result: UNet with MambaOut encoder performed best, successfully mapping trails, though some underestimation occurred.

Conclusion: The approach can aid in mapping and monitoring trails for conservation, marking a first in competitive image segmentation for herbivore trail detection.

Abstract: Detection of spatial areas where biodiversity is at risk is of paramount
importance for the conservation and monitoring of ecosystems. Large terrestrial
mammalian herbivores are keystone species as their activity not only has deep
effects on soils, plants, and animals, but also shapes landscapes, as large
herbivores act as allogenic ecosystem engineers. One key landscape feature that
indicates intense herbivore activity and potentially impacts biodiversity is
the formation of grazing trails. Grazing trails are formed by the continuous
trampling activity of large herbivores that can produce complex networks of
tracks of bare soil. Here, we evaluated different algorithms based on machine
learning techniques to identify grazing trails. Our goal is to automatically
detect potential areas with intense herbivory activity, which might be
beneficial for conservation and management plans.
  We have applied five semantic segmentation methods combined with fourteen
encoders aimed at mapping grazing trails on aerial images. Our results indicate
that in most cases the chosen methodology successfully mapped the trails,
although there were a few instances where the actual trail structure was
underestimated. The UNet architecture with the MambaOut encoder was the best
architecture for mapping trails. The proposed approach could be applied to
develop tools for mapping and monitoring temporal changes in these landscape
structures to support habitat conservation and land management programs. This
is the first time, to the best of our knowledge, that competitive image
segmentation results are obtained for the detection and delineation of trails
of large herbivorous mammals.

</details>


### [107] [Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis](https://arxiv.org/abs/2504.12129)
*Songping Wang,Yueming Lyu,Shiqi Liu,Ning Li,Tong Tong,Hao Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: The paper proposes a Hierarchical Anti-Aesthetic (HAA) framework to degrade the quality of maliciously customized diffusion models, protecting facial identity by disrupting aesthetics globally and locally.


<details>
  <summary>Details</summary>
Motivation: Customized diffusion models pose risks to privacy and copyright. The paper addresses this by leveraging aesthetic degradation to hinder malicious use.

Method: The HAA framework includes global and local anti-aesthetic branches, using reward mechanisms and losses to degrade image quality and disrupt facial identity.

Result: HAA outperforms state-of-the-art methods in identity removal, effectively protecting facial privacy and copyright.

Conclusion: The HAA framework provides a novel aesthetic-based solution to mitigate misuse of customized diffusion models, enhancing privacy and copyright protection.

Abstract: The rise of customized diffusion models has spurred a boom in personalized
visual content creation, but also poses risks of malicious misuse, severely
threatening personal privacy and copyright protection. Some studies show that
the aesthetic properties of images are highly positively correlated with human
perception of image quality. Inspired by this, we approach the problem from a
novel and intriguing aesthetic perspective to degrade the generation quality of
maliciously customized models, thereby achieving better protection of facial
identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)
framework to fully explore aesthetic cues, which consists of two key branches:
1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward
mechanism and a global anti-aesthetic loss, it can degrade the overall
aesthetics of the generated content; 2) Local Anti-Aesthetics: A local
anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to
guide adversarial perturbations to disrupt local facial identity. By seamlessly
integrating both branches, our HAA effectively achieves the goal of
anti-aesthetics from a global to a local level during customized generation.
Extensive experiments show that HAA outperforms existing SOTA methods largely
in identity removal, providing a powerful tool for protecting facial privacy
and copyright.

</details>


### [108] [Weakly Semi-supervised Whole Slide Image Classification by Two-level Cross Consistency Supervision](https://arxiv.org/abs/2504.12132)
*Linhao Qu,Shiman Li,Xiaoyuan Luo,Shaolei Liu,Qinhao Guo,Manning Wang,Zhijian Song*

Main category: cs.CV

TL;DR: The paper introduces a weakly semi-supervised WSI classification (WSWC) problem and proposes CroCo, a framework using cross-consistency supervision to address it.


<details>
  <summary>Details</summary>
Motivation: Existing WSI classification methods are ineffective due to the high cost of labeling large datasets, especially for new tasks. WSWC aligns better with clinical practice by using limited labeled data.

Method: CroCo employs two heterogeneous classifier branches with cross-consistency supervision at bag and instance levels.

Result: CroCo outperforms other methods in bag and instance classification on four datasets with limited labeled data.

Conclusion: The paper successfully defines and solves the WSWC problem, offering a practical solution for clinical WSI classification.

Abstract: Computer-aided Whole Slide Image (WSI) classification has the potential to
enhance the accuracy and efficiency of clinical pathological diagnosis. It is
commonly formulated as a Multiple Instance Learning (MIL) problem, where each
WSI is treated as a bag and the small patches extracted from the WSI are
considered instances within that bag. However, obtaining labels for a large
number of bags is a costly and time-consuming process, particularly when
utilizing existing WSIs for new classification tasks. This limitation renders
most existing WSI classification methods ineffective. To address this issue, we
propose a novel WSI classification problem setting, more aligned with clinical
practice, termed Weakly Semi-supervised Whole slide image Classification
(WSWC). In WSWC, a small number of bags are labeled, while a significant number
of bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the
absence of patch labels, distinguishes it from typical semi-supervised image
classification problems, making existing algorithms for natural images
unsuitable for directly solving the WSWC problem. In this paper, we present a
concise and efficient framework, named CroCo, to tackle the WSWC problem
through two-level Cross Consistency supervision. CroCo comprises two
heterogeneous classifier branches capable of performing both instance
classification and bag classification. The fundamental idea is to establish
cross-consistency supervision at both the bag-level and instance-level between
the two branches during training. Extensive experiments conducted on four
datasets demonstrate that CroCo achieves superior bag classification and
instance classification performance compared to other comparative methods when
limited WSIs with bag labels are available. To the best of our knowledge, this
paper presents for the first time the WSWC problem and gives a successful
resolution.

</details>


### [109] [FocusedAD: Character-centric Movie Audio Description](https://arxiv.org/abs/2504.12157)
*Xiaojun Ye,Chun Wang,Yiren Song,Sheng Zhou,Liangcheng Li,Jiajun Bu*

Main category: cs.CV

TL;DR: FocusedAD is a novel framework for generating character-centric movie audio descriptions, addressing challenges like character identification and plot relevance. It outperforms benchmarks and includes tools for automated character query banks.


<details>
  <summary>Details</summary>
Motivation: Movie Audio Description (AD) must narrate visual content for blind and visually impaired audiences, requiring plot-relevant narration with character names, which is challenging.

Method: FocusedAD includes a Character Perception Module (CPM) for tracking characters, a Dynamic Prior Module (DPM) for contextual cues, and a Focused Caption Module (FCM) for enriched narrations. It also introduces an automated pipeline for character query banks.

Result: FocusedAD achieves state-of-the-art performance on benchmarks like MAD-eval-Named and Cinepile-AD, including strong zero-shot results.

Conclusion: FocusedAD effectively addresses the unique challenges of movie AD, offering a robust solution with released code and data.

Abstract: Movie Audio Description (AD) aims to narrate visual content during
dialogue-free segments, particularly benefiting blind and visually impaired
(BVI) audiences. Compared with general video captioning, AD demands
plot-relevant narration with explicit character name references, posing unique
challenges in movie understanding.To identify active main characters and focus
on storyline-relevant regions, we propose FocusedAD, a novel framework that
delivers character-centric movie audio descriptions. It includes: (i) a
Character Perception Module(CPM) for tracking character regions and linking
them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues
from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused
Caption Module(FCM) that generates narrations enriched with plot-relevant
details and named characters. To overcome limitations in character
identification, we also introduce an automated pipeline for building character
query banks. FocusedAD achieves state-of-the-art performance on multiple
benchmarks, including strong zero-shot results on MAD-eval-Named and our newly
proposed Cinepile-AD dataset. Code and data will be released at
https://github.com/Thorin215/FocusedAD .

</details>


### [110] [CodingHomo: Bootstrapping Deep Homography With Video Coding](https://arxiv.org/abs/2504.12165)
*Yike Liu,Haipeng Li,Shuaicheng Liu,Bing Zeng*

Main category: cs.CV

TL;DR: CodingHomo is an unsupervised framework for homography estimation using video coding and motion vectors, outperforming existing methods with improved robustness and generalizability.


<details>
  <summary>Details</summary>
Motivation: Accurate homography estimation in complex motions remains challenging despite advances in deep learning.

Method: Leverages video coding and motion vectors (MVs), featuring Mask-Guided Fusion (MGF) and Mask-Guided Homography Estimation (MGHE) modules for enhanced accuracy.

Result: Outperforms state-of-the-art unsupervised methods, offering good robustness and generalizability.

Conclusion: CodingHomo provides a novel, effective approach for homography estimation, with publicly available code and dataset.

Abstract: Homography estimation is a fundamental task in computer vision with
applications in diverse fields. Recent advances in deep learning have improved
homography estimation, particularly with unsupervised learning approaches,
offering increased robustness and generalizability. However, accurately
predicting homography, especially in complex motions, remains a challenge. In
response, this work introduces a novel method leveraging video coding,
particularly by harnessing inherent motion vectors (MVs) present in videos. We
present CodingHomo, an unsupervised framework for homography estimation. Our
framework features a Mask-Guided Fusion (MGF) module that identifies and
utilizes beneficial features among the MVs, thereby enhancing the accuracy of
homography prediction. Additionally, the Mask-Guided Homography Estimation
(MGHE) module is presented for eliminating undesired features in the
coarse-to-fine homography refinement process. CodingHomo outperforms existing
state-of-the-art unsupervised methods, delivering good robustness and
generalizability. The code and dataset are available at:
\href{github}{https://github.com/liuyike422/CodingHomo

</details>


### [111] [RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning](https://arxiv.org/abs/2504.12167)
*Yuan Luo,Rudolf Hoffmann,Yan Xia,Olaf Wysocki,Benedikt Schwab,Thomas H. Kolbe,Daniel Cremers*

Main category: cs.CV

TL;DR: The paper introduces RadarCity, a dataset with 54K radar-image pairs and 3D city models, and proposes RADLER, a neural network using contrastive SSL and 3D models to improve radar object detection.


<details>
  <summary>Details</summary>
Motivation: To explore the underutilized potential of semantic 3D city models in mitigating noise for radar object detection.

Method: Uses contrastive SSL for robust radar features and fuses semantic-depth features from 3D models to enhance detection.

Result: RADLER improves mAP by 5.46% and mAR by 3.51% over prior methods.

Conclusion: The work advances semantic-guided radar object detection and encourages further research in map-supported detection.

Abstract: Semantic 3D city models are worldwide easy-accessible, providing accurate,
object-oriented, and semantic-rich 3D priors. To date, their potential to
mitigate the noise impact on radar object detection remains under-explored. In
this paper, we first introduce a unique dataset, RadarCity, comprising 54K
synchronized radar-image pairs and semantic 3D city models. Moreover, we
propose a novel neural network, RADLER, leveraging the effectiveness of
contrastive self-supervised learning (SSL) and semantic 3D city models to
enhance radar object detection of pedestrians, cyclists, and cars.
Specifically, we first obtain the robust radar features via a SSL network in
the radar-image pretext task. We then use a simple yet effective feature fusion
strategy to incorporate semantic-depth features from semantic 3D city models.
Having prior 3D information as guidance, RADLER obtains more fine-grained
details to enhance radar object detection. We extensively evaluate RADLER on
the collected RadarCity dataset and demonstrate average improvements of 5.46%
in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over
previous radar object detection methods. We believe this work will foster
further research on semantic-guided and map-supported radar object detection.
Our project page is publicly available
athttps://gpp-communication.github.io/RADLER .

</details>


### [112] [CoMotion: Concurrent Multi-person 3D Motion](https://arxiv.org/abs/2504.12186)
*Alejandro Newell,Peiyun Hu,Lahav Lipson,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: A method for detecting and tracking 3D poses of multiple people from a monocular camera, handling occlusions and crowded scenes, with online tracking and state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in tracking detailed 3D poses in crowded, occluded scenes using a single camera.

Method: Combines per-frame detection with a learned pose update for temporal coherence, avoiding detection matching across frames.

Result: Achieves state-of-the-art accuracy in 3D pose estimation, with faster and more accurate multi-person tracking.

Conclusion: The system is effective for real-time 3D pose tracking in complex scenarios, with publicly available code and weights.

Abstract: We introduce an approach for detecting and tracking detailed 3D poses of
multiple people from a single monocular camera stream. Our system maintains
temporally coherent predictions in crowded scenes filled with difficult poses
and occlusions. Our model performs both strong per-frame detection and a
learned pose update to track people from frame to frame. Rather than match
detections across time, poses are updated directly from a new input image,
which enables online tracking through occlusion. We train on numerous image and
video datasets leveraging pseudo-labeled annotations to produce a model that
matches state-of-the-art systems in 3D pose estimation accuracy while being
faster and more accurate in tracking multiple people through time. Code and
weights are provided at https://github.com/apple/ml-comotion

</details>


### [113] [Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI](https://arxiv.org/abs/2504.12197)
*Mahdi Alehdaghi,Rajarshi Bhattacharya,Pourya Shamsolmoali,Rafael M. O. Cruz,Maguelonne Heritier,Eric Granger*

Main category: cs.CV

TL;DR: PCMNet introduces a dynamic prototype-based method for interpretable deep learning, outperforming existing approaches in interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of conceptual clarity and robustness in current explainability methods like GradCAM and prototype-based approaches.

Method: PCMNet dynamically learns interpretable prototypes from meaningful regions, clustering them into concept groups without extra annotations.

Result: PCMNet achieves high interpretability, stability, and robustness in clean and occluded scenarios, outperforming state-of-the-art methods.

Conclusion: PCMNet offers a promising solution for interpretable deep learning by combining dynamic prototype learning and semantic grounding.

Abstract: Deep learning has provided considerable advancements for multimedia systems,
yet the interpretability of deep models remains a challenge. State-of-the-art
post-hoc explainability methods, such as GradCAM, provide visual interpretation
based on heatmaps but lack conceptual clarity. Prototype-based approaches, like
ProtoPNet and PIPNet, offer a more structured explanation but rely on fixed
patches, limiting their robustness and semantic consistency.
  To address these limitations, a part-prototypical concept mining network
(PCMNet) is proposed that dynamically learns interpretable prototypes from
meaningful regions. PCMNet clusters prototypes into concept groups, creating
semantically grounded explanations without requiring additional annotations.
Through a joint process of unsupervised part discovery and concept activation
vector extraction, PCMNet effectively captures discriminative concepts and
makes interpretable classification decisions.
  Our extensive experiments comparing PCMNet against state-of-the-art methods
on multiple datasets show that it can provide a high level of interpretability,
stability, and robustness under clean and occluded scenarios.

</details>


### [114] [Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing](https://arxiv.org/abs/2504.12215)
*Ilkin Sevgi Isler,David Mohaisen,Curtis Lisle,Damla Turgut,Ulas Bagci*

Main category: cs.CV

TL;DR: A two-stage, uncertainty-guided framework improves tumor segmentation in thoracic CT by combining coarse localization with refined ROI segmentation and anatomical post-processing.


<details>
  <summary>Details</summary>
Motivation: Challenges like boundary ambiguity, class imbalance, and anatomical variability make reliable tumor segmentation in thoracic CT difficult.

Method: A coarse-to-fine approach: first-stage model localizes tumors, followed by anatomically informed filtering and second-stage segmentation with uncertainty-aware loss.

Result: Improved Dice and Hausdorff scores, fewer false positives, and better spatial interpretability on private and public datasets (e.g., Dice improved from 0.4690 to 0.6447 on Orlando dataset).

Conclusion: Combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines enhances robustness and clinical relevance of tumor delineation.

Abstract: Reliable tumor segmentation in thoracic computed tomography (CT) remains
challenging due to boundary ambiguity, class imbalance, and anatomical
variability. We propose an uncertainty-guided, coarse-to-fine segmentation
framework that combines full-volume tumor localization with refined
region-of-interest (ROI) segmentation, enhanced by anatomically aware
post-processing. The first-stage model generates a coarse prediction, followed
by anatomically informed filtering based on lung overlap, proximity to lung
surfaces, and component size. The resulting ROIs are segmented by a
second-stage model trained with uncertainty-aware loss functions to improve
accuracy and boundary calibration in ambiguous regions. Experiments on private
and public datasets demonstrate improvements in Dice and Hausdorff scores, with
fewer false positives and enhanced spatial interpretability. These results
highlight the value of combining uncertainty modeling and anatomical priors in
cascaded segmentation pipelines for robust and clinically meaningful tumor
delineation. On the Orlando dataset, our framework improved Swin UNETR Dice
from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated
with segmentation gains, underscoring the value of anatomically informed
post-processing.

</details>


### [115] [Coding-Prior Guided Diffusion Network for Video Deblurring](https://arxiv.org/abs/2504.12222)
*Yike Liu,Jianhui Zhang,Haipeng Li,Shuaicheng Liu,Bing Zeng*

Main category: cs.CV

TL;DR: CPGDNet leverages coding priors (motion vectors, residuals) and diffusion generative models for superior video deblurring, achieving 30% better IQA metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore valuable coding priors and pre-trained diffusion models, limiting deblurring quality.

Method: Two-stage framework: CPFP for alignment/attention using coding priors, and CPC to guide diffusion models for detail synthesis.

Result: State-of-the-art perceptual quality with 30% improvement in IQA metrics.

Conclusion: CPGDNet effectively combines coding and diffusion priors, setting a new benchmark in video deblurring.

Abstract: While recent video deblurring methods have advanced significantly, they often
overlook two valuable prior information: (1) motion vectors (MVs) and coding
residuals (CRs) from video codecs, which provide efficient inter-frame
alignment cues, and (2) the rich real-world knowledge embedded in pre-trained
diffusion generative models. We present CPGDNet, a novel two-stage framework
that effectively leverages both coding priors and generative diffusion priors
for high-quality deblurring. First, our coding-prior feature propagation (CPFP)
module utilizes MVs for efficient frame alignment and CRs to generate attention
masks, addressing motion inaccuracies and texture variations. Second, a
coding-prior controlled generation (CPC) module network integrates coding
priors into a pretrained diffusion model, guiding it to enhance critical
regions and synthesize realistic details. Experiments demonstrate our method
achieves state-of-the-art perceptual quality with up to 30% improvement in IQA
metrics. Both the code and the codingprior-augmented dataset will be
open-sourced.

</details>


### [116] [FLIP Reasoning Challenge](https://arxiv.org/abs/2504.12256)
*Andreas Plesner,Turlan Kuzhagaliyev,Roger Wattenhofer*

Main category: cs.CV

TL;DR: The paper introduces the FLIP dataset to evaluate AI reasoning, showing current models lag behind human performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of AI reasoning by creating a benchmark (FLIP) based on human verification tasks.

Method: Uses the FLIP dataset with image-ordering tasks, testing VLMs and LLMs in zero-shot settings and ensembles.

Result: Best models achieve 75.5%-77.9% accuracy vs. human 95.3%; ensembles improve to 85.2%.

Conclusion: FLIP highlights AI reasoning gaps and the need for robust multimodal benchmarks.

Abstract: Over the past years, advances in artificial intelligence (AI) have
demonstrated how AI can solve many perception and generation tasks, such as
image classification and text writing, yet reasoning remains a challenge. This
paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning
capabilities based on human verification tasks on the Idena blockchain. FLIP
challenges present users with two orderings of 4 images, requiring them to
identify the logically coherent one. By emphasizing sequential reasoning,
visual storytelling, and common sense, FLIP provides a unique testbed for
multimodal AI systems. Our experiments evaluate state-of-the-art models,
leveraging both vision-language models (VLMs) and large language models (LLMs).
Results reveal that even the best open-sourced and closed-sourced models
achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot
settings, compared to human performance of 95.3%. Captioning models aid
reasoning models by providing text descriptions of images, yielding better
results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5
Pro. Combining the predictions from 15 models in an ensemble increases the
accuracy to 85.2%. These findings highlight the limitations of existing
reasoning models and the need for robust multimodal benchmarks like FLIP. The
full codebase and dataset will be available at
https://github.com/aplesner/FLIP-Reasoning-Challenge.

</details>


### [117] [Cobra: Efficient Line Art COlorization with BRoAder References](https://arxiv.org/abs/2504.12240)
*Junhao Zhuang,Lingen Li,Xuan Ju,Zhaoyang Zhang,Chun Yuan,Ying Shan*

Main category: cs.CV

TL;DR: Cobra is an efficient method for line art colorization in comics, using a Causal Sparse DiT architecture to handle extensive references and ensure consistency, improving speed and control.


<details>
  <summary>Details</summary>
Motivation: The comic industry needs accurate, efficient, and flexible line art colorization, but existing diffusion models struggle with reference handling and latency.

Method: Cobra employs a Causal Sparse DiT architecture with positional encodings, causal sparse attention, and Key-Value Cache to manage long-context references.

Result: Cobra achieves high-quality colorization with over 200 references, enhancing speed and interactivity.

Conclusion: Cobra meets industrial demands for efficient and controlled line art colorization, with released codes and models.

Abstract: The comic production industry requires reference-based line art colorization
with high accuracy, efficiency, contextual consistency, and flexible control. A
comic page often involves diverse characters, objects, and backgrounds, which
complicates the coloring process. Despite advancements in diffusion models for
image generation, their application in line art colorization remains limited,
facing challenges related to handling extensive reference images,
time-consuming inference, and flexible control. We investigate the necessity of
extensive contextual image guidance on the quality of line art colorization. To
address these challenges, we introduce Cobra, an efficient and versatile method
that supports color hints and utilizes over 200 reference images while
maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,
which leverages specially designed positional encodings, causal sparse
attention, and Key-Value Cache to effectively manage long-context references
and ensure color identity consistency. Results demonstrate that Cobra achieves
accurate line art colorization through extensive contextual reference,
significantly enhancing inference speed and interactivity, thereby meeting
critical industrial demands. We release our codes and models on our project
page: https://zhuang2002.github.io/Cobra/.

</details>


### [118] [How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions](https://arxiv.org/abs/2504.12284)
*Aditya Prakash,Benjamin Lundell,Dmitry Andreychuk,David Forsyth,Saurabh Gupta,Harpreet Sawhney*

Main category: cs.CV

TL;DR: Predicting 3D hand motion and contact maps from RGB, text, and 3D contact points using a VQVAE and transformer-decoder model.


<details>
  <summary>Details</summary>
Motivation: To address the novel problem of predicting interaction trajectories (hand motion and contact maps) from limited inputs (RGB, text, 3D contact point).

Method: Uses (1) Interaction Codebook (VQVAE) to tokenize interaction trajectories, and (2) Interaction Predictor (transformer-decoder) to predict trajectories. Trained on HoloAssist dataset.

Result: Outperforms transformer & diffusion baselines on a large, diverse benchmark, showing generalization across object/action categories, tasks, and scenes.

Conclusion: The approach effectively predicts interaction trajectories, demonstrating robustness and scalability.

Abstract: We tackle the novel problem of predicting 3D hand motion and contact maps (or
Interaction Trajectories) given a single RGB view, action text, and a 3D
contact point on the object as input. Our approach consists of (1) Interaction
Codebook: a VQVAE model to learn a latent codebook of hand poses and contact
points, effectively tokenizing interaction trajectories, (2) Interaction
Predictor: a transformer-decoder module to predict the interaction trajectory
from test time inputs by using an indexer module to retrieve a latent
affordance from the learned codebook. To train our model, we develop a data
engine that extracts 3D hand poses and contact trajectories from the diverse
HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger
than existing works, in terms of diversity of objects and interactions
observed, and test for generalization of the model across object categories,
action categories, tasks, and scenes. Experimental results show the
effectiveness of our approach over transformer & diffusion baselines across all
settings.

</details>


### [119] [VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate](https://arxiv.org/abs/2504.12259)
*Zhihang Yuan,Rui Xie,Yuzhang Shang,Hanling Zhang,Siyuan Wang,Shengen Yan,Guohao Dai,Yu Wang*

Main category: cs.CV

TL;DR: VGDFR introduces a training-free method for Diffusion-based Video Generation with Dynamic Latent Frame Rate, improving efficiency by adapting frame rates based on motion frequency.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiency of DiT-based video generation by leveraging temporal non-uniformity in videos.

Method: Proposes a dynamic frame rate scheduler, latent-space frame merging, and optimized RoPE strategy.

Result: Achieves up to 3x speedup in video generation with minimal quality loss.

Conclusion: VGDFR effectively balances efficiency and quality in video generation by exploiting motion dynamics.

Abstract: Diffusion Transformer(DiT)-based generation models have achieved remarkable
success in video generation. However, their inherent computational demands pose
significant efficiency challenges. In this paper, we exploit the inherent
temporal non-uniformity of real-world videos and observe that videos exhibit
dynamic information density, with high-motion segments demanding greater detail
preservation than static scenes. Inspired by this temporal non-uniformity, we
propose VGDFR, a training-free approach for Diffusion-based Video Generation
with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements
in latent space based on the motion frequency of the latent space content,
using fewer tokens for low-frequency segments while preserving detail in
high-frequency segments. Specifically, our key contributions are: (1) A dynamic
frame rate scheduler for DiT video generation that adaptively assigns frame
rates for video segments. (2) A novel latent-space frame merging method to
align latent representations with their denoised counterparts before merging
those redundant in low-resolution space. (3) A preference analysis of Rotary
Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE
strategy optimized for semantic and local information capture. Experiments show
that VGDFR can achieve a speedup up to 3x for video generation with minimal
quality degradation.

</details>


### [120] [SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians](https://arxiv.org/abs/2504.12292)
*Liam Schoneveld,Zhe Chen,Davide Davoli,Jiapeng Tang,Saimon Terazawa,Ko Nishino,Matthias Nießner*

Main category: cs.CV

TL;DR: SHeaP improves 3D head reconstruction from monocular images using self-supervised learning with 2D Gaussians, outperforming existing methods in geometry and expression accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D head reconstruction from 2D data is challenging due to limited ground truth. Self-supervised learning from 2D videos is explored, but differentiable mesh rendering has limitations.

Method: Predicts a 3DMM mesh and rigged Gaussians from an image, reanimates the avatar to match target frames, and backpropagates photometric losses to improve predictions.

Result: Outperforms self-supervised methods on NoW benchmark for neutral faces and a new benchmark for expressions. Also excels in emotion classification.

Conclusion: SHeaP's use of Gaussians enhances self-supervised 3D head reconstruction, achieving superior geometry and expression accuracy.

Abstract: Accurate, real-time 3D reconstruction of human heads from monocular images
and videos underlies numerous visual applications. As 3D ground truth data is
hard to come by at scale, previous methods have sought to learn from abundant
2D videos in a self-supervised manner. Typically, this involves the use of
differentiable mesh rendering, which is effective but faces limitations. To
improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor
Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a
set of Gaussians that are rigged to this mesh. We then reanimate this rigged
head avatar to match a target frame, and backpropagate photometric losses to
both the 3DMM and Gaussian prediction networks. We find that using Gaussians
for rendering substantially improves the effectiveness of this self-supervised
approach. Training solely on 2D data, our method surpasses existing
self-supervised approaches in geometric evaluations on the NoW benchmark for
neutral faces and a new benchmark for non-neutral expressions. Our method also
produces highly expressive meshes, outperforming state-of-the-art in emotion
classification.

</details>


### [121] [Towards Learning to Complete Anything in Lidar](https://arxiv.org/abs/2504.12264)
*Ayca Takmaz,Cristiano Saltori,Neehar Peri,Tim Meinhardt,Riccardo de Lutio,Laura Leal-Taixé,Aljoša Ošep*

Main category: cs.CV

TL;DR: CAL (Complete Anything in Lidar) is a zero-shot approach for Lidar-based shape completion, leveraging temporal context from multi-modal sensor sequences to mine object shapes and semantic features, enabling recognition beyond fixed class vocabularies.


<details>
  <summary>Details</summary>
Motivation: Existing Lidar-based methods are limited to closed vocabularies, restricting their ability to complete and recognize objects in-the-wild. CAL aims to overcome this by using temporal context to mine object shapes and semantics.

Method: The approach mines partial shape completions and semantic features from multi-modal sensor sequences, distilling them into a Lidar-only model for instance-level completion and recognition.

Result: The distilled model infers full object shapes from partial observations and performs well on standard benchmarks for Semantic and Panoptic Scene Completion, localizing objects as 3D bounding boxes and recognizing objects beyond fixed vocabularies.

Conclusion: CAL demonstrates the feasibility of zero-shot Lidar-based shape completion and recognition, extending capabilities beyond traditional closed-vocabulary methods.

Abstract: We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion
in-the-wild. This is closely related to Lidar-based semantic/panoptic scene
completion. However, contemporary methods can only complete and recognize
objects from a closed vocabulary labeled in existing Lidar datasets. Different
to that, our zero-shot approach leverages the temporal context from multi-modal
sensor sequences to mine object shapes and semantic features of observed
objects. These are then distilled into a Lidar-only instance-level completion
and recognition model. Although we only mine partial shape completions, we find
that our distilled model learns to infer full object shapes from multiple such
partial observations across the dataset. We show that our model can be prompted
on standard benchmarks for Semantic and Panoptic Scene Completion, localize
objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class
vocabularies. Our project page is
https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar

</details>


### [122] [Beyond Reconstruction: A Physics Based Neural Deferred Shader for Photo-realistic Rendering](https://arxiv.org/abs/2504.12273)
*Zhuo He,Paul Henderson,Nicolas Pugeault*

Main category: cs.CV

TL;DR: A novel physics-based neural deferred shading pipeline is introduced to decompose illumination and material parameters, enabling photo-realistic shading and relighting with improved performance.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of decomposing illumination and material parameters in deep learning-based rendering, which restricts scene reconstruction control.

Method: Proposes a physics-based neural deferred shading pipeline and a shadow estimator for efficient shadowing effects.

Result: Achieves better performance than classical and state-of-the-art neural shading models, enabling generalizable photo-realistic shading from arbitrary illumination.

Conclusion: The pipeline successfully decomposes rendering processes, offering control over shading and relighting with high realism.

Abstract: Deep learning based rendering has demonstrated major improvements for
photo-realistic image synthesis, applicable to various applications including
visual effects in movies and photo-realistic scene building in video games.
However, a significant limitation is the difficulty of decomposing the
illumination and material parameters, which limits such methods to reconstruct
an input scene, without any possibility to control these parameters. This paper
introduces a novel physics based neural deferred shading pipeline to decompose
the data-driven rendering process, learn a generalizable shading function to
produce photo-realistic results for shading and relighting tasks, we also
provide a shadow estimator to efficiently mimic shadowing effect. Our model
achieves improved performance compared to classical models and a state-of-art
neural shading model, and enables generalizable photo-realistic shading from
arbitrary illumination input.

</details>


### [123] [The Tenth NTIRE 2025 Image Denoising Challenge Report](https://arxiv.org/abs/2504.12276)
*Lei Sun,Hang Guo,Bin Ren,Luc Van Gool,Radu Timofte,Yawei Li,Xiangyu Kong,Hyunhee Park,Xiaoxuan Yu,Suejin Han,Hakjae Jeon,Jia Li,Hyung-Ju Chun,Donghun Ryou,Inju Ha,Bohyung Han,Jingyu Ma,Zhijuan Huang,Huiyuan Fu,Hongyuan Yu,Boqi Zhang,Jiawei Shi,Heng Zhang,Huadong Ma,Deepak Kumar Tyagi,Aman Kukretti,Gajender Sharma,Sriharsha Koundinya,Asim Manna,Jun Cheng,Shan Tan,Jun Liu,Jiangwei Hao,Jianping Luo,Jie Lu,Satya Narayan Tazi,Arnim Gautam,Aditi Pawar,Aishwarya Joshi,Akshay Dudhane,Praful Hambadre,Sachin Chaudhary,Santosh Kumar Vipparthi,Subrahmanyam Murala,Jiachen Tu,Nikhil Akalwadi,Vijayalaxmi Ashok Aralikatti,Dheeraj Damodar Hegde,G Gyaneshwar Rao,Jatin Kalal,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Zhenyuan Lin,Yubo Dong,Weikun Li,Anqi Li,Ang Gao,Weijun Yuan,Zhan Li,Ruting Deng,Yihang Chen,Yifan Deng,Zhanglu Chen,Boyang Yao,Shuling Zheng,Feng Zhang,Zhiheng Fu,Anas M. Ali,Bilel Benjdira,Wadii Boulila,Jan Seny,Pei Zhou,Jianhua Hu,K. L. Eddie Law,Jaeho Lee,M. J. Aashik Rasool,Abdur Rehman,SMA Sharif,Seongwan Kim,Alexandru Brateanu,Raul Balmez,Ciprian Orhei,Cosmin Ancuti,Zeyu Xiao,Zhuoyuan Li,Ziqi Wang,Yanyan Wei,Fei Wang,Kun Li,Shengeng Tang,Yunkai Zhang,Weirun Zhou,Haoxuan Lu*

Main category: cs.CV

TL;DR: Overview of NTIRE 2025 Image Denoising Challenge (σ=50), focusing on methodologies, results, and state-of-the-art insights.


<details>
  <summary>Details</summary>
Motivation: Develop a high-quality denoising network architecture evaluated by PSNR, without computational or size constraints.

Method: Participants proposed solutions for denoising AWGN (σ=50), with 20 teams submitting valid results.

Result: 290 participants registered; 20 teams provided insights into current state-of-the-art denoising.

Conclusion: The challenge showcased advancements in image denoising, highlighting top-performing methodologies.

Abstract: This paper presents an overview of the NTIRE 2025 Image Denoising Challenge
({\sigma} = 50), highlighting the proposed methodologies and corresponding
results. The primary objective is to develop a network architecture capable of
achieving high-quality denoising performance, quantitatively evaluated using
PSNR, without constraints on computational complexity or model size. The task
assumes independent additive white Gaussian noise (AWGN) with a fixed noise
level of 50. A total of 290 participants registered for the challenge, with 20
teams successfully submitting valid results, providing insights into the
current state-of-the-art in image denoising.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [124] [Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder](https://arxiv.org/abs/2504.12005)
*Soobin Suh,Dabi Ahn,Heewoong Park,Jonghun Park*

Main category: cs.SD

TL;DR: Proposes a CVAE-based voice conversion method to generate diverse intonations, improving sound quality and variety.


<details>
  <summary>Details</summary>
Motivation: Conventional voice conversion models produce only one result per input, lacking intonation diversity.

Method: Uses conditional variational autoencoder (CVAE) and inverse autoregressive flow (IAF) to model complex latent spaces.

Result: Achieves diverse intonations and better sound quality compared to non-CVAE models.

Conclusion: The CVAE-based approach successfully enhances voice conversion with varied intonations and improved quality.

Abstract: Voice conversion is a task of synthesizing an utterance with target speaker's
voice while maintaining linguistic information of the source utterance. While a
speaker can produce varying utterances from a single script with different
intonations, conventional voice conversion models were limited to producing
only one result per source input. To overcome this limitation, we propose a
novel approach for voice conversion with diverse intonations using conditional
variational autoencoder (CVAE). Experiments have shown that the speaker's style
feature can be mapped into a latent space with Gaussian distribution. We have
also been able to convert voices with more diverse intonation by making the
posterior of the latent space more complex with inverse autoregressive flow
(IAF). As a result, the converted voice not only has a diversity of
intonations, but also has better sound quality than the model without CVAE.

</details>


### [125] [Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML](https://arxiv.org/abs/2504.12272)
*Kong Ka Hing,Mehran Behjati*

Main category: cs.SD

TL;DR: The paper explores TinyML for real-time hornbill call classification in Malaysia, using audio data and edge devices to aid conservation.


<details>
  <summary>Details</summary>
Motivation: Hornbills face threats like habitat loss and poaching, requiring efficient monitoring. Traditional methods are resource-intensive, prompting the use of TinyML for real-time, edge-based solutions.

Method: Audio data from Xeno-canto is pre-processed, features extracted using MFE, and a model is deployed on Arduino Nano 33 BLE. Training and validation are done via Edge Impulse.

Result: The model achieves high accuracy in identifying hornbill species, demonstrating TinyML's effectiveness for wildlife monitoring.

Conclusion: TinyML shows promise for ecological conservation, offering scalable, efficient solutions for real-time wildlife monitoring.

Abstract: Hornbills, an iconic species of Malaysia's biodiversity, face threats from
habi-tat loss, poaching, and environmental changes, necessitating accurate and
real-time population monitoring that is traditionally challenging and re-source
intensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to
transform wildlife monitoring by enabling efficient, real-time da-ta analysis
directly on edge devices. Addressing the challenge of wildlife conservation,
this research paper explores the pivotal role of machine learn-ing,
specifically TinyML, in the classification and monitoring of hornbill calls in
Malaysia. Leveraging audio data from the Xeno-canto database, the study aims to
develop a speech recognition system capable of identifying and classifying
hornbill vocalizations. The proposed methodology involves pre-processing the
audio data, extracting features using Mel-Frequency Energy (MFE), and deploying
the model on an Arduino Nano 33 BLE, which is adept at edge computing. The
research encompasses foundational work, in-cluding a comprehensive
introduction, literature review, and methodology. The model is trained using
Edge Impulse and validated through real-world tests, achieving high accuracy in
hornbill species identification. The project underscores the potential of
TinyML for environmental monitoring and its broader application in ecological
conservation efforts, contributing to both the field of TinyML and wildlife
conservation.

</details>


### [126] [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
*Mikhail Osipov*

Main category: cs.SD

TL;DR: A geometry-driven method normalizes dysarthric speech using Lie group transformations on spectrograms, improving ASR performance without pathological data.


<details>
  <summary>Details</summary>
Motivation: To address distortions in dysarthric speech for better ASR performance without relying on pathological data.

Method: Uses local Lie group transformations (time, frequency, amplitude) parameterized by scalar fields, inferred by a neural network trained on synthetic distortions.

Result: Achieves up to 16% WER reduction on TORGO samples, with no degradation on clean speech.

Conclusion: Introduces a principled, interpretable approach for robust ASR under motor speech disorders.

Abstract: We present a geometry-driven method for normalizing dysarthric speech using
local Lie group transformations of spectrograms. Time, frequency, and amplitude
distortions are modeled as smooth, invertible deformations, parameterized by
scalar fields and applied via exponential maps. A neural network is trained to
infer these fields from synthetic distortions of typical speech-without using
any pathological data. At test time, the model applies an approximate inverse
to real dysarthric inputs. Despite zero-shot generalization, we observe
substantial ASR gains, including up to 16 percentage points WER reduction on
challenging TORGO samples, with no degradation on clean speech. This work
introduces a principled, interpretable approach for robust speech recognition
under motor speech disorders

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [127] [CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines](https://arxiv.org/abs/2504.11476)
*Ritik Mishra,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: Proposes a class-informed weighted function to enhance Restricted Kernel Machines (RKMs), improving robustness against noise and outliers.


<details>
  <summary>Details</summary>
Motivation: RKMs degrade in noisy or outlier-rich environments, limiting their effectiveness.

Method: Integrates a class-informed weighting mechanism and Schur complement theorem to create CI-RKM.

Result: CI-RKM outperforms baselines in classification accuracy and robustness.

Conclusion: CI-RKM advances kernel-based learning by addressing noise and outlier challenges.

Abstract: Restricted kernel machines (RKMs) represent a versatile and powerful
framework within the kernel machine family, leveraging conjugate feature
duality to address a wide range of machine learning tasks, including
classification, regression, and feature learning. However, their performance
can degrade significantly in the presence of noise and outliers, which
compromises robustness and predictive accuracy. In this paper, we propose a
novel enhancement to the RKM framework by integrating a class-informed weighted
function. This weighting mechanism dynamically adjusts the contribution of
individual training points based on their proximity to class centers and
class-specific characteristics, thereby mitigating the adverse effects of noisy
and outlier data. By incorporating weighted conjugate feature duality and
leveraging the Schur complement theorem, we introduce the class-informed
restricted kernel machine (CI-RKM), a robust extension of the RKM designed to
improve generalization and resilience to data imperfections. Experimental
evaluations on benchmark datasets demonstrate that the proposed CI-RKM
consistently outperforms existing baselines, achieving superior classification
accuracy and enhanced robustness against noise and outliers. Our proposed
method establishes a significant advancement in the development of kernel-based
learning models, addressing a core challenge in the field.

</details>


### [128] [LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2504.11497)
*Chang Liu,Emmanuel A. Olowe,Danial Chitnis*

Main category: cs.LG

TL;DR: An LLM-based AI agent is proposed to automate transistor sizing in AMS circuit design, achieving up to 60% success in meeting performance targets.


<details>
  <summary>Details</summary>
Motivation: Manual transistor sizing in AMS ICs is labor-intensive, and existing ML techniques in EDA face challenges like iterations and lack of design knowledge. LLMs show promise in automating this process.

Method: The AI agent integrates LLMs with circuit simulation tools and data analysis, using prompt engineering to optimize circuits. Different LLMs were evaluated, with Claude 3.5 Sonnet selected for further testing on an operational amplifier.

Result: The agent achieved a 60% success rate in meeting target performance metrics across three requirement groups for the operational amplifier.

Conclusion: LLMs have significant potential to enhance AMS circuit design by automating and optimizing the transistor sizing process.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose an LLM-based
AI agent for AMS circuit design to assist in the sizing process. By integrating
LLMs with external circuit simulation tools and data analysis functions and
employing prompt engineering strategies, the agent successfully optimized
multiple circuits to achieve target performance metrics. We evaluated the
performance of different LLMs to assess their applicability and optimization
effectiveness across seven basic circuits, and selected the best-performing
model Claude 3.5 Sonnet for further exploration on an operational amplifier,
with complementary input stage and class AB output stage. This circuit was
evaluated against nine performance metrics, and we conducted experiments under
three distinct performance requirement groups. A success rate of up to 60% was
achieved for reaching the target requirements. Overall, this work demonstrates
the potential of LLMs to improve AMS circuit design.

</details>


### [129] [Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning](https://arxiv.org/abs/2504.11506)
*Hongliang Lu,Shuqi Shen,Junjie Yang,Chao Lu,Xinhu Zheng,Hai Yang*

Main category: cs.LG

TL;DR: The paper proposes a data-light inverse reinforcement learning scheme for cross-cultural deployment of autonomous vehicles (AVs), reducing local data dependence by up to 98.67%.


<details>
  <summary>Details</summary>
Motivation: Driving culture varies globally, posing challenges for AV deployment in data-scarce regions. The study aims to enable culture-compatible AVs without requiring extensive local data.

Method: The scheme involves comparative analysis of driving datasets from Germany, China, and the USA, followed by testing cross-cultural deployment with minimal local data.

Result: The method reduces local data dependence by 98.67%, facilitating AV deployment in underdeveloped regions.

Conclusion: The study promotes a fairer global AV market by addressing data scarcity in culturally diverse regions.

Abstract: More than the adherence to specific traffic regulations, driving culture
touches upon a more implicit part - an informal, conventional, collective
behavioral pattern followed by drivers - that varies across countries, regions,
and even cities. Such cultural divergence has become one of the biggest
challenges in deploying autonomous vehicles (AVs) across diverse regions today.
The current emergence of data-driven methods has shown a potential solution to
enable culture-compatible driving through learning from data, but what if some
underdeveloped regions cannot provide sufficient local data to inform driving
culture? This issue is particularly significant for a broader global AV market.
Here, we propose a cross-cultural deployment scheme for AVs, called data-light
inverse reinforcement learning, designed to re-calibrate culture-specific AVs
and assimilate them into other cultures. First, we report the divergence in
driving cultures through a comprehensive comparative analysis of naturalistic
driving datasets on highways from three countries: Germany, China, and the USA.
Then, we demonstrate the effectiveness of our scheme by testing the expeditious
cross-cultural deployment across these three countries, with cumulative testing
mileage of over 56084 km. The performance is particularly advantageous when
cross-cultural deployment is carried out without affluent local data. Results
show that we can reduce the dependence on local data by a margin of 98.67% at
best. This study is expected to bring a broader, fairer AV global market,
particularly in those regions that lack enough local data to develop
culture-compatible AVs.

</details>


### [130] [Reward Distance Comparisons Under Transition Sparsity](https://arxiv.org/abs/2504.11508)
*Clement Nyanhongo,Bruno Miranda Henrique,Eugene Santos*

Main category: cs.LG

TL;DR: The paper introduces SRRD, a pseudometric for comparing rewards without requiring high transition coverage, addressing sparsity issues in conventional methods.


<details>
  <summary>Details</summary>
Motivation: Existing direct reward comparison methods fail under transition sparsity due to high coverage requirements, leading to errors.

Method: Proposes the Sparsity Resilient Reward Distance (SRRD) pseudometric, which accommodates diverse sample distributions without needing high transition coverage.

Result: Theoretical and experimental validation shows SRRD's robustness and efficacy across domains.

Conclusion: SRRD provides a reliable solution for reward comparison under sparse transition conditions, outperforming existing methods.

Abstract: Reward comparisons are vital for evaluating differences in agent behaviors
induced by a set of reward functions. Most conventional techniques utilize the
input reward functions to learn optimized policies, which are then used to
compare agent behaviors. However, learning these policies can be
computationally expensive and can also raise safety concerns. Direct reward
comparison techniques obviate policy learning but suffer from transition
sparsity, where only a small subset of transitions are sampled due to data
collection challenges and feasibility constraints. Existing state-of-the-art
direct reward comparison methods are ill-suited for these sparse conditions
since they require high transition coverage, where the majority of transitions
from a given coverage distribution are sampled. When this requirement is not
satisfied, a distribution mismatch between sampled and expected transitions can
occur, leading to significant errors. This paper introduces the Sparsity
Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need
for high transition coverage by accommodating diverse sample distributions,
which are common under transition sparsity. We provide theoretical
justification for SRRD's robustness and conduct experiments to demonstrate its
practical efficacy across multiple domains.

</details>


### [131] [Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs](https://arxiv.org/abs/2504.11511)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.LG

TL;DR: The paper advocates for a new privacy paradigm in reinforcement learning (RL) to address shortcomings of traditional frameworks, proposing four core principles and calling for new theoretical and practical solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional privacy frameworks are inadequate for RL systems due to their sequential, interactive, and context-dependent nature, especially in modern paradigms like federated RL and RL with human feedback.

Method: The paper proposes a new privacy paradigm based on four principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation.

Result: The principles highlight tensions between privacy, utility, and interpretability in RL systems, emphasizing the need for new frameworks and evaluation methods.

Conclusion: The paper calls for developing new theoretical and practical approaches to ensure effective privacy protection in RL systems, particularly in high-stakes domains.

Abstract: The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.

</details>


### [132] [Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor](https://arxiv.org/abs/2504.11513)
*Wonjun Yi,Yong-Hwa Park*

Main category: cs.LG

TL;DR: A multi-output classification (MOC) framework for fault diagnosis under partially labeled target domains and compound faults, using MK-MMD and EM losses, with FLN for frequency domain preservation, outperforming baselines in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in fault diagnosis under partially labeled target domains and compound fault conditions, where traditional methods treat fault combinations as single classes, limiting interpretability and accuracy.

Method: Proposes MOC to independently estimate fault severity, incorporating MK-MMD and EM losses for domain adaptation and FLN to preserve frequency domain properties.

Result: MOC outperforms baselines in macro F1 score and achieves better classification for individual faults; FLN shows superior adaptability.

Conclusion: The MOC framework enhances interpretability and accuracy in fault diagnosis, with FLN proving effective for domain adaptation in rotating machinery.

Abstract: This work introduces a multi-output classification (MOC) framework designed
for domain adaptation in fault diagnosis, particularly under partially labeled
(PL) target domain scenarios and compound fault conditions in rotating
machinery. Unlike traditional multi-class classification (MCC) methods that
treat each fault combination as a distinct class, the proposed approach
independently estimates the severity of each fault type, improving both
interpretability and diagnostic accuracy. The model incorporates multi-kernel
maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to
facilitate feature transfer from the source to the target domain. In addition,
frequency layer normalization (FLN) is applied to preserve structural
properties in the frequency domain, which are strongly influenced by system
dynamics and are often stationary with respect to changes in rpm. Evaluations
across six domain adaptation cases with PL data demonstrate that MOC
outperforms baseline models in macro F1 score. Moreover, MOC consistently
achieves better classification performance for individual fault types, and FLN
shows superior adaptability compared to other normalization techniques.

</details>


### [133] [LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation](https://arxiv.org/abs/2504.11521)
*Wei-Jer Chang,Wei Zhan,Masayoshi Tomizuka,Manmohan Chandraker,Francesco Pittaluga*

Main category: cs.LG

TL;DR: LangTraj, a language-conditioned scene-diffusion model, enhances autonomous vehicle testing by simulating realistic traffic scenarios with flexible language control.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and safety of autonomous vehicle testing by enabling scalable, intuitive, and nuanced control over traffic simulations.

Method: Introduces LangTraj, a diffusion model conditioned on natural language, and a closed-loop training strategy for stability. Uses the Inter-Drive dataset for diverse interaction labels.

Result: LangTraj achieves strong performance in realism, language controllability, and safety-critical simulation on the Waymo Motion Dataset.

Conclusion: LangTraj sets a new standard for flexible and scalable autonomous vehicle testing through language-conditioned simulation.

Abstract: Evaluating autonomous vehicles with controllability enables scalable testing
in counterfactual or structured settings, enhancing both efficiency and safety.
We introduce LangTraj, a language-conditioned scene-diffusion model that
simulates the joint behavior of all agents in traffic scenarios. By
conditioning on natural language inputs, LangTraj provides flexible and
intuitive control over interactive behaviors, generating nuanced and realistic
scenarios. Unlike prior approaches that depend on domain-specific guidance
functions, LangTraj incorporates language conditioning during training,
facilitating more intuitive traffic simulation control. We propose a novel
closed-loop training strategy for diffusion models, explicitly tailored to
enhance stability and realism during closed-loop simulation. To support
language-conditioned simulation, we develop Inter-Drive, a large-scale dataset
with diverse and interactive labels for training language-conditioned diffusion
models. Our dataset is built upon a scalable pipeline for annotating
agent-agent interactions and single-agent behaviors, ensuring rich and varied
supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates
strong performance in realism, language controllability, and
language-conditioned safety-critical simulation, establishing a new paradigm
for flexible and scalable autonomous vehicle testing.

</details>


### [134] [Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism](https://arxiv.org/abs/2504.11558)
*Mete Erdogan,Cengiz Pehlevan,Alper T. Erdogan*

Main category: cs.LG

TL;DR: EBD is a new algorithm for neural networks that broadcasts output errors to layers, using stochastic orthogonality to avoid weight transport, achieving competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addresses the credit assignment problem in neural networks by providing a biologically plausible method for error broadcasting.

Method: Uses stochastic orthogonality of MMSE estimators to define layerwise loss functions, penalizing correlations between activations and errors.

Result: Achieves performance comparable or better than existing error-broadcast methods on benchmark datasets.

Conclusion: EBD offers a biologically plausible, efficient alternative for neural network training, with potential for future advancements in learning paradigms.

Abstract: We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel
learning framework that addresses the credit assignment problem in neural
networks by directly broadcasting output error to individual layers. Leveraging
the stochastic orthogonality property of the optimal minimum mean square error
(MMSE) estimator, EBD defines layerwise loss functions to penalize correlations
between layer activations and output errors, offering a principled approach to
error broadcasting without the need for weight transport. The optimization
framework naturally leads to the experimentally observed three-factor learning
rule and integrates with biologically plausible frameworks to enhance
performance and plausibility. Numerical experiments demonstrate that EBD
achieves performance comparable to or better than known error-broadcast methods
on benchmark datasets. While the scalability of EBD to very large or complex
datasets remains to be further explored, our findings suggest it provides a
biologically plausible, efficient, and adaptable alternative for neural network
training. This approach could inform future advancements in artificial and
natural learning paradigms.

</details>


### [135] [Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring](https://arxiv.org/abs/2504.11581)
*Mert Sehri,Igor Varejão,Zehui Hua,Vitor Bonella,Adriano Santos,Francisco de Assis Boldt,Patrick Dumond,Flavio Miguel Varejão*

Main category: cs.LG

TL;DR: A framework for a large-scale, annotated vibration dataset is proposed to advance vibration analysis, inspired by ImageNet's success in transfer learning. Initial experiments show improved model performance when pre-trained on bearing vibration data.


<details>
  <summary>Details</summary>
Motivation: The lack of a large-scale, annotated vibration dataset hinders progress in predictive maintenance and fault diagnosis, unlike the success seen with ImageNet in visual computing.

Method: The framework collects bearing vibration signals from public datasets, uses deep learning for pre-training, and fine-tunes on domain-specific data. Future work includes expanding to more machinery types and spectrogram-based representations.

Result: Experiments demonstrate improved model performance when pre-trained on bearing vibration data, validating the framework's potential.

Conclusion: The proposed dataset framework can standardize vibration analysis methodologies, accelerate research, and mirror ImageNet's impact in industrial applications.

Abstract: ImageNet has become a reputable resource for transfer learning, allowing the
development of efficient ML models with reduced training time and data
requirements. However, vibration analysis in predictive maintenance, structural
health monitoring, and fault diagnosis, lacks a comparable large-scale,
annotated dataset to facilitate similar advancements. To address this, a
dataset framework is proposed that begins with bearing vibration data as an
initial step towards creating a universal dataset for vibration-based
spectrogram analysis for all machinery. The initial framework includes a
collection of bearing vibration signals from various publicly available
datasets. To demonstrate the advantages of this framework, experiments were
conducted using a deep learning architecture, showing improvements in model
performance when pre-trained on bearing vibration data and fine-tuned on a
smaller, domain-specific dataset. These findings highlight the potential to
parallel the success of ImageNet in visual computing but for vibration
analysis. For future work, this research will include a broader range of
vibration signals from multiple types of machinery, emphasizing
spectrogram-based representations of the data. Each sample will be labeled
according to machinery type, operational status, and the presence or type of
faults, ensuring its utility for supervised and unsupervised learning tasks.
Additionally, a framework for data preprocessing, feature extraction, and model
training specific to vibration data will be developed. This framework will
standardize methodologies across the research community, allowing for
collaboration and accelerating progress in predictive maintenance, structural
health monitoring, and related fields. By mirroring the success of ImageNet in
visual computing, this dataset has the potential to improve the development of
intelligent systems in industrial applications.

</details>


### [136] [Dueling Deep Reinforcement Learning for Financial Time Series](https://arxiv.org/abs/2504.11601)
*Bruno Giorgio*

Main category: cs.LG

TL;DR: The paper explores using Double DQN and Dueling Networks for financial trading with SP500 data, focusing on cost-sensitive environments. RL agents outperformed random strategies despite data complexity.


<details>
  <summary>Details</summary>
Motivation: To apply RL in financial trading, optimizing strategies under practical constraints like transaction costs.

Method: Used Double DQN and Dueling Network Architectures on historical SP500 data, evaluating performance with and without commissions.

Result: RL agents learned meaningful policies and outperformed random strategies, though data complexity caused sub-optimal policies.

Conclusion: RL shows promise in trading but faces challenges due to data complexity and computational limits.

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for solving
decision-making problems in dynamic environments. In this research, we explore
the application of Double DQN (DDQN) and Dueling Network Architectures, to
financial trading tasks using historical SP500 index data. Our focus is
training agents capable of optimizing trading strategies while accounting for
practical constraints such as transaction costs. The study evaluates the model
performance across scenarios with and without commissions, highlighting the
impact of cost-sensitive environments on reward dynamics. Despite computational
limitations and the inherent complexity of financial time series data, the
agent successfully learned meaningful trading policies. The findings confirm
that RL agents, even when trained on limited datasets, can outperform random
strategies by leveraging advanced architectures such as DDQN and Dueling
Networks. However, significant challenges persist, particularly with a
sub-optimal policy due to the complexity of data source.

</details>


### [137] [Possibility for Proactive Anomaly Detection](https://arxiv.org/abs/2504.11623)
*Jinsung Jeon,Jaehyeon Park,Sewon Park,Jeongwhan Choi,Minjung Kim,Noseong Park*

Main category: cs.LG

TL;DR: A proactive approach for time-series anomaly detection using forecasting and data-driven models, evaluated on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing models rely on error between output and ground truth, making them impractical. This work aims to reduce potential damages by detecting anomalies proactively.

Method: Combines a time-series forecasting model with a data-driven anomaly detection model to establish thresholds from training data.

Result: Evaluated on four benchmarks, analyzing predictable and unpredictable anomalies. Source code provided.

Conclusion: Proactive approach improves practicality and effectiveness in detecting anomalies.

Abstract: Time-series anomaly detection, which detects errors and failures in a
workflow, is one of the most important topics in real-world applications. The
purpose of time-series anomaly detection is to reduce potential damages or
losses. However, existing anomaly detection models detect anomalies through the
error between the model output and the ground truth (observed) value, which
makes them impractical. In this work, we present a \textit{proactive} approach
for time-series anomaly detection based on a time-series forecasting model
specialized for anomaly detection and a data-driven anomaly detection model.
Our proactive approach establishes an anomaly threshold from training data with
a data-driven anomaly detection model, and anomalies are subsequently detected
by identifying predicted values that exceed the anomaly threshold. In addition,
we extensively evaluated the model using four anomaly detection benchmarks and
analyzed both predictable and unpredictable anomalies. We attached the source
code as supplementary material.

</details>


### [138] [Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling](https://arxiv.org/abs/2504.11645)
*Feng Zhu,Aritra Mitra,Robert W. Heath*

Main category: cs.LG

TL;DR: The paper introduces FedHSA, a novel federated stochastic approximation algorithm for collaborative RL with heterogeneous agents, ensuring convergence and sample-complexity benefits.


<details>
  <summary>Details</summary>
Motivation: The study addresses gaps in federated RL by accounting for Markovian data and heterogeneous agent-specific operators, overcoming limitations of prior work.

Method: Develops FedHSA, a projection-free algorithm, handling Markovian sampling and local operator heterogeneity, with intermittent agent-server communication.

Result: Proves FedHSA converges to the correct point with an M-fold linear speedup in sample-complexity, a first in finite-time results for such settings.

Conclusion: FedHSA advances federated RL by enabling efficient collaboration among heterogeneous agents, with implications for policy evaluation and control.

Abstract: Motivated by collaborative reinforcement learning (RL) and optimization with
time-correlated data, we study a generic federated stochastic approximation
problem involving $M$ agents, where each agent is characterized by an
agent-specific (potentially nonlinear) local operator. The goal is for the
agents to communicate intermittently via a server to find the root of the
average of the agents' local operators. The generality of our setting stems
from allowing for (i) Markovian data at each agent and (ii) heterogeneity in
the roots of the agents' local operators. The limited recent work that has
accounted for both these features in a federated setting fails to guarantee
convergence to the desired point or to show any benefit of collaboration;
furthermore, they rely on projection steps in their algorithms to guarantee
bounded iterates. Our work overcomes each of these limitations. We develop a
novel algorithm titled \texttt{FedHSA}, and prove that it guarantees
convergence to the correct point, while enjoying an $M$-fold linear speedup in
sample-complexity due to collaboration. To our knowledge, \emph{this is the
first finite-time result of its kind}, and establishing it (without relying on
a projection step) entails a fairly intricate argument that accounts for the
interplay between complex temporal correlations due to Markovian sampling,
multiple local steps to save communication, and the drift-effects induced by
heterogeneous local operators. Our results have implications for a broad class
of heterogeneous federated RL problems (e.g., policy evaluation and control)
with function approximation, where the agents' Markov decision processes can
differ in their probability transition kernels and reward functions.

</details>


### [139] [70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float](https://arxiv.org/abs/2504.11651)
*Tianyi Zhang,Yang Sui,Shaochen Zhong,Vipin Chaudhary,Xia Hu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: DFloat11 is a lossless compression framework for LLMs, reducing model size by 30% while maintaining bit-for-bit identical outputs. It uses entropy coding and custom GPU kernels for efficient decompression, achieving higher throughput and longer context lengths.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLM sizes poses deployment challenges on resource-constrained hardware. Existing storage formats are inefficient due to low entropy in BFloat16 weight representations.

Method: DFloat11 applies entropy coding for dynamic-length encodings, decomposes LUTs for GPU SRAM, and uses a two-phase kernel for efficient decompression.

Result: Experiments show 30% model size reduction, 1.9-38.8x higher throughput, and 5.3-13.17x longer context lengths compared to uncompressed models.

Conclusion: DFloat11 enables efficient, lossless deployment of large LLMs on constrained hardware, with significant performance improvements.

Abstract: Large Language Models (LLMs) have grown rapidly in size, creating significant
challenges for efficient deployment on resource-constrained hardware. In this
paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression
framework that reduces LLM size by 30% while preserving outputs that are
bit-for-bit identical to the original model. DFloat11 is motivated by the low
entropy in the BFloat16 weight representation of LLMs, which reveals
significant inefficiency in existing storage format. By applying entropy
coding, DFloat11 assigns dynamic-length encodings to weights based on
frequency, achieving near information-optimal compression without any loss of
precision. To facilitate efficient inference with dynamic-length encodings, we
develop a custom GPU kernel for fast online decompression. Our design
incorporates the following: (i) decomposition of memory-intensive lookup tables
(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for
coordinating thread read/write positions using lightweight auxiliary variables,
and (iii) transformer-block-level decompression to minimize latency.
Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,
validates our hypothesis that DFloat11 achieves around 30% model size reduction
while preserving bit-for-bit exact outputs. Compared to a potential alternative
of offloading parts of an uncompressed model to the CPU to meet memory
constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.
With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context
lengths than uncompressed models. Notably, our method enables lossless
inference of Llama-3.1-405B, an 810GB model, on a single node equipped with
8x80GB GPUs. Our code and models are available at
https://github.com/LeanModels/DFloat11.

</details>


### [140] [H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning](https://arxiv.org/abs/2504.11699)
*Rui Xue,Tianfu Wu*

Main category: cs.LG

TL;DR: H$^3$GNNs is a self-supervised framework balancing heterophily and homophily in GNNs using joint structural encoding and dynamic masking strategies, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing heterophily and homophily in GNNs, especially in self-supervised settings.

Method: Proposes H$^3$GNNs with joint structural node encoding (linear/non-linear projections + WGCN) and teacher-student predictive architectures with dynamic masking.

Result: Achieves state-of-the-art on heterophily datasets and matches performance on homophily datasets.

Conclusion: H$^3$GNNs effectively harmonizes heterophily and homophily, demonstrating superior performance across diverse graph types.

Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in
representation learning, a challenge further amplified in self-supervised
settings. We propose H$^3$GNNs, an end-to-end self-supervised learning
framework that harmonizes both structural properties through two key
innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified
space combining linear and non-linear feature projections with K-hop structural
representations via a Weighted Graph Convolution Network(WGCN). A
cross-attention mechanism enhances awareness and adaptability to heterophily
and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive
Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a
teacher-student model, the student sees the masked input graph and predicts
node features inferred by the teacher that sees the full input graph in the
joint encoding space. To enhance learning difficulty, we introduce two novel
node-predictive-difficulty-based masking strategies. Experiments on seven
benchmarks (four heterophily datasets and three homophily datasets) confirm the
effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our
H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily
datasets, while retaining on-par performance to previous state-of-the-art
methods on the three homophily datasets.

</details>


### [141] [Clustering and analysis of user behaviour in blockchain: A case study of Planet IX](https://arxiv.org/abs/2504.11702)
*Dorottya Zelenyanszki,Zhe Hou,Kamanashis Biswas,Vallipuram Muthukkumarasamy*

Main category: cs.LG

TL;DR: The paper proposes a pipeline to analyze user behavior in blockchain-based games, revealing privacy risks through transaction data and clustering algorithms.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in decentralized applications (dApps) by demonstrating how user behavior can be extracted and analyzed from public blockchain data.

Method: A pipeline involving data collection from a blockchain game (Planet IX), user flow analysis, GNN-based graph embeddings, and clustering algorithms to identify behavioral clusters.

Result: Behavioral clusters were identified and visualized, revealing potential privacy threats from malicious exploitation of user data.

Conclusion: The study highlights privacy risks in dApps and proposes a threat model to address potential vulnerabilities.

Abstract: Decentralised applications (dApps) that run on public blockchains have the
benefit of trustworthiness and transparency as every activity that happens on
the blockchain can be publicly traced through the transaction data. However,
this introduces a potential privacy problem as this data can be tracked and
analysed, which can reveal user-behaviour information. A user behaviour
analysis pipeline was proposed to present how this type of information can be
extracted and analysed to identify separate behavioural clusters that can
describe how users behave in the game. The pipeline starts with the collection
of transaction data, involving smart contracts, that is collected from a
blockchain-based game called Planet IX. Both the raw transaction information
and the transaction events are considered in the data collection. From this
data, separate game actions can be formed and those are leveraged to present
how and when the users conducted their in-game activities in the form of user
flows. An extended version of these user flows also presents how the
Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter
is given as input for a Graph Neural Network (GNN) model to provide graph
embeddings for these flows which then can be leveraged by clustering algorithms
to cluster user behaviours into separate behavioural clusters. We benchmark and
compare well-known clustering algorithms as a part of the proposed method. The
user behaviour clusters were analysed and visualised in a graph format. It was
found that behavioural information can be extracted regarding the users that
belong to these clusters. Such information can be exploited by malicious users
to their advantage. To demonstrate this, a privacy threat model was also
presented based on the results that correspond to multiple potentially affected
areas.

</details>


### [142] [Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching](https://arxiv.org/abs/2504.11713)
*Aaron Havens,Benjamin Kurt Miller,Bing Yan,Carles Domingo-Enrich,Anuroop Sriram,Brandon Wood,Daniel Levine,Bin Hu,Brandon Amos,Brian Karrer,Xiang Fu,Guan-Horng Liu,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Adjoint Sampling is a scalable, efficient algorithm for learning diffusion processes to sample from unnormalized densities, outperforming previous methods in gradient updates and scalability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in scalability and efficiency of existing methods for sampling from unnormalized densities, particularly in large problem settings.

Method: The algorithm leverages stochastic optimal control, allowing more gradient updates than energy evaluations, and incorporates symmetries and periodic boundary conditions for molecular modeling.

Result: Demonstrated effectiveness on classical energy functions and neural network-based models, enabling amortized conformer generation across molecular systems.

Conclusion: The approach is theoretically grounded, scalable, and practical, with plans to open-source benchmarks to advance computational chemistry research.

Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for
learning diffusion processes that sample from unnormalized densities, or energy
functions. It is the first on-policy approach that allows significantly more
gradient updates than the number of energy evaluations and model samples,
allowing us to scale to much larger problem settings than previously explored
by similar methods. Our framework is theoretically grounded in stochastic
optimal control and shares the same theoretical guarantees as Adjoint Matching,
being able to train without the need for corrective measures that push samples
towards the target distribution. We show how to incorporate key symmetries, as
well as periodic boundary conditions, for modeling molecules in both cartesian
and torsional coordinates. We demonstrate the effectiveness of our approach
through extensive experiments on classical energy functions, and further scale
up to neural network-based energy models where we perform amortized conformer
generation across many molecular systems. To encourage further research in
developing highly scalable sampling methods, we plan to open source these
challenging benchmarks, where successful methods can directly impact progress
in computational chemistry.

</details>


### [143] [Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception](https://arxiv.org/abs/2504.11726)
*Yunzhe Li,Facheng Hu,Hongzi Zhu,Shifan Zhang,Liang Zhang,Shan Chang,Minyi Guo*

Main category: cs.LG

TL;DR: Saga is a fine-grained user perception approach using minimal labeled IMU data, leveraging pre-training and Bayesian Optimization for high accuracy.


<details>
  <summary>Details</summary>
Motivation: Labeling IMU data is challenging due to raw data complexity and lack of ground truth, requiring a solution for efficient model training with minimal labels.

Method: Pre-train a feature extraction model using unlabeled IMU data, then use Bayesian Optimization to optimize weights for downstream tasks.

Result: Achieves over 90% accuracy with only 100 labeled samples per class, matching performance of models trained on thousands of samples.

Conclusion: Saga demonstrates efficient and accurate user perception with minimal labeled data, reducing reliance on large labeled datasets.

Abstract: Inertial measurement units (IMUs), have been prevalently used in a wide range
of mobile perception applications such as activity recognition and user
authentication, where a large amount of labelled data are normally required to
train a satisfactory model. However, it is difficult to label micro-activities
in massive IMU data due to the hardness of understanding raw IMU data and the
lack of ground truth. In this paper, we propose a novel fine-grained user
perception approach, called Saga, which only needs a small amount of labelled
IMU data to achieve stunning user perception accuracy. The core idea of Saga is
to first pre-train a backbone feature extraction model, utilizing the rich
semantic information of different levels embedded in the massive unlabelled IMU
data. Meanwhile, for a specific downstream user perception application,
Bayesian Optimization is employed to determine the optimal weights for
pre-training tasks involving different semantic levels. We implement Saga on
five typical mobile phones and evaluate Saga on three typical tasks on three
IMU datasets. Results show that when only using about 100 training samples per
class, Saga can achieve over 90% accuracy of the full-fledged model trained on
over ten thousands training samples with no additional system overhead.

</details>


### [144] [Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective](https://arxiv.org/abs/2504.11757)
*Pradeep Singh,Ashutosh Kumar,Sutirtha Ghosh,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: Reservoir computing (RC) leverages high-dimensional state spaces for efficient temporal data processing, decoupling reservoir training from readout layer optimization. This paper explores RC's foundational properties, stability, and applications, while addressing theoretical challenges.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze RC's foundational properties, stability, and expressive power, and to explore its computational trade-offs and applications in signal processing, prediction, and control systems.

Method: The study formalizes RC's properties (echo state, fading memory, capacity) using dynamical systems theory, examines input-signal interplay, and evaluates robustness and scalability.

Result: Demonstrates conditions for reservoir stability and expressive power, provides insights into optimization and training, and identifies open challenges for RC's theoretical advancement.

Conclusion: RC offers powerful computational capabilities for temporal data, but further theoretical work is needed to address scalability and optimization challenges.

Abstract: Reservoir computing (RC) represents a class of state-space models (SSMs)
characterized by a fixed state transition mechanism (the reservoir) and a
flexible readout layer that maps from the state space. It is a paradigm of
computational dynamical systems that harnesses the transient dynamics of
high-dimensional state spaces for efficient processing of temporal data. Rooted
in concepts from recurrent neural networks, RC achieves exceptional
computational power by decoupling the training of the dynamic reservoir from
the linear readout layer, thereby circumventing the complexities of
gradient-based optimization. This work presents a systematic exploration of RC,
addressing its foundational properties such as the echo state property, fading
memory, and reservoir capacity through the lens of dynamical systems theory. We
formalize the interplay between input signals and reservoir states,
demonstrating the conditions under which reservoirs exhibit stability and
expressive power. Further, we delve into the computational trade-offs and
robustness characteristics of RC architectures, extending the discussion to
their applications in signal processing, time-series prediction, and control
systems. The analysis is complemented by theoretical insights into
optimization, training methodologies, and scalability, highlighting open
challenges and potential directions for advancing the theoretical underpinnings
of RC.

</details>


### [145] [Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs](https://arxiv.org/abs/2504.11808)
*Kishan Gurumurthy,Himanshu Pal,Charu Sharma*

Main category: cs.LG

TL;DR: A novel federated learning method for GNNs using spectral GNNs and neural ODEs, addressing privacy and non-IID data challenges, with promising results on homophilic and heterophilic graphs.


<details>
  <summary>Details</summary>
Motivation: Centralizing graph data for GNN training is impractical due to privacy and regulatory issues. Federated learning offers a solution but remains underexplored for GNNs.

Method: Spectral GNNs equipped with neural ODEs for better information capture, designed for non-IID data and optimized for privacy and bandwidth.

Result: Effective handling of non-IID data, performance comparable to IID methods, and improvements on heterophilic and homophilic graphs.

Conclusion: The method demonstrates federated learning's potential in diverse graph settings, with applications in social networks, recommendations, and fraud detection.

Abstract: Graph Neural Network (GNN) research is rapidly advancing due to GNNs'
capacity to learn distributed representations from graph-structured data.
However, centralizing large volumes of real-world graph data for GNN training
is often impractical due to privacy concerns, regulatory restrictions, and
commercial competition. Federated learning (FL), a distributed learning
paradigm, offers a solution by preserving data privacy with collaborative model
training. Despite progress in training huge vision and language models,
federated learning for GNNs remains underexplored. To address this challenge,
we present a novel method for federated learning on GNNs based on spectral GNNs
equipped with neural ordinary differential equations (ODE) for better
information capture, showing promising results across both homophilic and
heterophilic graphs. Our approach effectively handles non-Independent and
Identically Distributed (non-IID) data, while also achieving performance
comparable to existing methods that only operate on IID data. It is designed to
be privacy-preserving and bandwidth-optimized, making it suitable for
real-world applications such as social network analysis, recommendation
systems, and fraud detection, which often involve complex, non-IID, and
heterophilic graph structures. Our results in the area of federated learning on
non-IID heterophilic graphs demonstrate significant improvements, while also
achieving better performance on homophilic graphs. This work highlights the
potential of federated learning in diverse and challenging graph settings.
Open-source code available on GitHub
(https://github.com/SpringWiz11/Fed-GNODEFormer).

</details>


### [146] [Manifold meta-learning for reduced-complexity neural system identification](https://arxiv.org/abs/2504.11811)
*Marco Forgione,Ankush Chakrabarty,Dario Piga,Matteo Rufolo,Alberto Bemporad*

Main category: cs.LG

TL;DR: A meta-learning framework is proposed to reduce the computational demands of deep learning for system identification by learning a low-dimensional manifold in the parameter space, validated on Bouc-Wen oscillators.


<details>
  <summary>Details</summary>
Motivation: Deep learning for system identification is resource-intensive; the goal is to enable efficient training and inference with small datasets.

Method: Uses an auxiliary neural network to map datasets to a learned low-dimensional manifold, avoiding costly second-order gradients and reducing first-order updates.

Result: Accurate models are learned even with small datasets, as demonstrated on Bouc-Wen oscillators.

Conclusion: The framework efficiently balances computational cost and model accuracy for nonlinear system identification.

Abstract: System identification has greatly benefited from deep learning techniques,
particularly for modeling complex, nonlinear dynamical systems with partially
unknown physics where traditional approaches may not be feasible. However, deep
learning models often require large datasets and significant computational
resources at training and inference due to their high-dimensional
parameterizations. To address this challenge, we propose a meta-learning
framework that discovers a low-dimensional manifold within the parameter space
of an over-parameterized neural network architecture. This manifold is learned
from a meta-dataset of input-output sequences generated by a class of related
dynamical systems, enabling efficient model training while preserving the
network's expressive power for the considered system class. Unlike bilevel
meta-learning approaches, our method employs an auxiliary neural network to map
datasets directly onto the learned manifold, eliminating the need for costly
second-order gradient computations during meta-training and reducing the number
of first-order updates required in inference, which could be expensive for
large models. We validate our approach on a family of Bouc-Wen oscillators,
which is a well-studied nonlinear system identification benchmark. We
demonstrate that we are able to learn accurate models even in small-data
scenarios.

</details>


### [147] [Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading](https://arxiv.org/abs/2504.11816)
*Kihyun Kim,Jinwoo Kim,Hyunsun Chung,Myung-Hoon Cha,Hong-Yeon Kim,Youngjae Kim*

Main category: cs.LG

TL;DR: InferSave is a cost-efficient VM selection framework for cloud-based LLM inference, optimizing KV cache offloading and GPU memory needs, improving cost efficiency by up to 73.7%.


<details>
  <summary>Details</summary>
Motivation: High GPU instance costs from CSPs like AWS burden LLM inference applications.

Method: Proposes InferSave, which optimizes KV cache offloading, estimates GPU memory needs, and uses CTCF for accurate VM selection.

Result: Lower-cost instances improve efficiency by 73.7% for online workloads; KV cache offloading saves 20.19% for offline workloads.

Conclusion: InferSave effectively reduces costs for LLM inference in cloud environments.

Abstract: LLM inference is essential for applications like text summarization,
translation, and data analysis, but the high cost of GPU instances from Cloud
Service Providers (CSPs) like AWS is a major burden. This paper proposes
InferSave, a cost-efficient VM selection framework for cloud based LLM
inference. InferSave optimizes KV cache offloading based on Service Level
Objectives (SLOs) and workload charac teristics, estimating GPU memory needs,
and recommending cost-effective VM instances. Additionally, the Compute Time
Calibration Function (CTCF) improves instance selection accuracy by adjusting
for discrepancies between theoretical and actual GPU performance. Experiments
on AWS GPU instances show that selecting lower-cost instances without KV cache
offloading improves cost efficiency by up to 73.7% for online workloads, while
KV cache offloading saves up to 20.19% for offline workloads.

</details>


### [148] [Emergence of Computational Structure in a Neural Network Physics Simulator](https://arxiv.org/abs/2504.11830)
*Rohan Hitchcock,Gary W. Delaney,Jonathan H. Manton,Richard Scalzo,Jingge Zhu*

Main category: cs.LG

TL;DR: The paper explores how computational structures emerge in neural networks, focusing on a transformer-like model simulating particle physics. It identifies collision-detecting structures in attention heads, links their emergence to degenerate loss landscape geometry, and observes power-law dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind the emergence of interpretable computational structures in neural networks and how to detect them.

Method: A transformer-like model is trained to simulate particle physics, using attention mechanisms to transfer information between particles. The study analyzes attention heads for emergent structures.

Result: Structures detecting particle collisions emerge in attention heads, associated with degenerate loss landscape geometry and following power-law dynamics.

Conclusion: Computational structures in networks are governed by a degenerate "effective potential," detectable by studying component dynamics, with implications for convergence time.

Abstract: Neural networks often have identifiable computational structures - components
of the network which perform an interpretable algorithm or task - but the
mechanisms by which these emerge and the best methods for detecting these
structures are not well understood. In this paper we investigate the emergence
of computational structure in a transformer-like model trained to simulate the
physics of a particle system, where the transformer's attention mechanism is
used to transfer information between particles. We show that (a) structures
emerge in the attention heads of the transformer which learn to detect particle
collisions, (b) the emergence of these structures is associated to degenerate
geometry in the loss landscape, and (c) the dynamics of this emergence follows
a power law. This suggests that these components are governed by a degenerate
"effective potential". These results have implications for the convergence time
of computational structure within neural networks and suggest that the
emergence of computational structure can be detected by studying the dynamics
of network components.

</details>


### [149] [Support is All You Need for Certified VAE Training](https://arxiv.org/abs/2504.11831)
*Changming Xu,Debangshu Banerjee,Deepak Vasisht,Gagandeep Singh*

Main category: cs.LG

TL;DR: CIVET is a novel method for certified training of VAEs, providing robustness guarantees against adversarial attacks by bounding errors on latent support sets.


<details>
  <summary>Details</summary>
Motivation: VAEs are used in safety-critical applications, necessitating certified performance guarantees under adversarial attacks.

Method: CIVET bounds worst-case VAE error by focusing on latent layer support sets, with a novel training algorithm.

Result: Outperforms SOTA methods in standard performance and robustness across datasets, architectures, and perturbations.

Conclusion: CIVET offers a reliable approach for certified VAE training with strong adversarial robustness.

Abstract: Variational Autoencoders (VAEs) have become increasingly popular and deployed
in safety-critical applications. In such applications, we want to give
certified probabilistic guarantees on performance under adversarial attacks. We
propose a novel method, CIVET, for certified training of VAEs. CIVET depends on
the key insight that we can bound worst-case VAE error by bounding the error on
carefully chosen support sets at the latent layer. We show this point
mathematically and present a novel training algorithm utilizing this insight.
We show in an extensive evaluation across different datasets (in both the
wireless and vision application areas), architectures, and perturbation
magnitudes that our method outperforms SOTA methods achieving good standard
performance with strong robustness guarantees.

</details>


### [150] [On the Problem of Best Arm Retention](https://arxiv.org/abs/2504.11866)
*Houshuang Chen,Yuchen He,Chihao Zhang*

Main category: cs.LG

TL;DR: The paper studies Best Arm Retention (BAR) in multi-armed bandits, focusing on pure exploration and regret minimization, with tight bounds and sample complexity for variants like $r$-BAR.


<details>
  <summary>Details</summary>
Motivation: To address the BAR problem in streaming algorithms for multi-armed bandits, ensuring retention of top arms efficiently.

Method: Revisits lower bounds for PAC algorithms, adapts KL-divergence arguments, and explores variants like $r$-BAR with tight sample complexity proofs.

Result: Derives optimal bounds for PAC algorithms, proves tight sample complexity for $r$-BAR, and develops regret-minimizing algorithms.

Conclusion: The work advances BAR understanding with theoretical guarantees and leaves a conjecture on optimal regret for future exploration.

Abstract: This paper presents a comprehensive study on the problem of Best Arm
Retention (BAR), which has recently found applications in streaming algorithms
for multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms
with the best arm included from $n$ after some trials, in stochastic
multi-armed bandit settings. We first investigate pure exploration for the BAR
problem under different criteria, and then minimize the regret with specific
constraints, in the context of further exploration in streaming algorithms.
  - We begin by revisiting the lower bound for the $(\varepsilon,\delta)$-PAC
algorithm for Best Arm Identification (BAI) and adapt the classical
KL-divergence argument to derive optimal bounds for $(\varepsilon,\delta)$-PAC
algorithms for BAR.
  - We further study another variant of the problem, called $r$-BAR, which
requires the expected gap between the best arm and the optimal arm retained is
less than $r$. We prove tight sample complexity for the problem.
  - We explore the regret minimization problem for $r$-BAR and develop
algorithm beyond pure exploration. We conclude with a conjecture on the optimal
regret in this setting.

</details>


### [151] [Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption](https://arxiv.org/abs/2504.11873)
*Weiqiang Jiao,Suzhi Bi,Xian Li,Cheng Guo,Hao Chen,Zhi Quan*

Main category: cs.LG

TL;DR: Proposes DASEIN, a domain adaptation method for semantic edge inference systems, enabling high accuracy in new environments without labeled data.


<details>
  <summary>Details</summary>
Motivation: High costs in annotating and re-training models for new environments due to varying data and channel distributions.

Method: Uses unsupervised domain adaptation and knowledge distillation to align data distributions and adapt to channel variations.

Result: Outperforms benchmarks by 7.09% and 21.33% in accuracy under varying conditions.

Conclusion: DASEIN effectively adapts to new environments without labeled data, proving practical for transfer deployments.

Abstract: This paper investigates deploying semantic edge inference systems for
performing a common image clarification task. In particular, each system
consists of multiple Internet of Things (IoT) devices that first locally encode
the sensing data into semantic features and then transmit them to an edge
server for subsequent data fusion and task inference. The inference accuracy is
determined by efficient training of the feature encoder/decoder using labeled
data samples. Due to the difference in sensing data and communication channel
distributions, deploying the system in a new environment may induce high costs
in annotating data labels and re-training the encoder/decoder models. To
achieve cost-effective transferable system deployment, we propose an efficient
Domain Adaptation method for Semantic Edge INference systems (DASEIN) that can
maintain high inference accuracy in a new environment without the need for
labeled samples. Specifically, DASEIN exploits the task-relevant data
correlation between different deployment scenarios by leveraging the techniques
of unsupervised domain adaptation and knowledge distillation. It devises an
efficient two-step adaptation procedure that sequentially aligns the data
distributions and adapts to the channel variations. Numerical results show
that, under a substantial change in sensing data distributions, the proposed
DASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in
inference accuracy when the new environment has similar or 25 dB lower channel
signal to noise power ratios (SNRs), respectively. This verifies the
effectiveness of the proposed method in adapting both data and channel
distributions in practical transfer deployment applications.

</details>


### [152] [Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization](https://arxiv.org/abs/2504.11874)
*Ruoyu Sun,Angelos Stefanidis,Zhengyong Jiang,Jionglong Su*

Main category: cs.LG

TL;DR: The paper proposes a reward factor matrix and a multi-critic learning system (Factor-MCLS) to address limitations in DRL for portfolio optimization, enabling better understanding of return/risk factors and investor intervention based on risk aversion.


<details>
  <summary>Details</summary>
Motivation: Existing DRL agents struggle with investor intervention and lack thorough understanding of portfolio return/risk factors due to reliance on reward function outputs.

Method: Introduces a reward factor matrix and Factor-MCLS, a multi-critic framework, to learn these factors and incorporates a risk constraint term for investor customization.

Result: The system effectively learns return/risk factors and allows investor intervention via risk constraints.

Conclusion: The proposed approach enhances DRL-based portfolio optimization by improving factor understanding and enabling investor-specific risk adjustments.

Abstract: Typical deep reinforcement learning (DRL) agents for dynamic portfolio
optimization learn the factors influencing portfolio return and risk by
analyzing the output values of the reward function while adjusting portfolio
weights within the training environment. However, it faces a major limitation
where it is difficult for investors to intervene in the training based on
different levels of risk aversion towards each portfolio asset. This difficulty
arises from another limitation: existing DRL agents may not develop a thorough
understanding of the factors responsible for the portfolio return and risk by
only learning from the output of the reward function. As a result, the strategy
for determining the target portfolio weights is entirely dependent on the DRL
agents themselves. To address these limitations, we propose a reward factor
matrix for elucidating the return and risk of each asset in the portfolio.
Additionally, we propose a novel learning system named Factor-MCLS using a
multi-critic framework that facilitates learning of the reward factor matrix.
In this way, our DRL-based learning system can effectively learn the factors
influencing portfolio return and risk. Moreover, based on the critic networks
within the multi-critic framework, we develop a risk constraint term in the
training objective function of the policy function. This risk constraint term
allows investors to intervene in the training of the DRL agent according to
their individual levels of risk aversion towards the portfolio assets.

</details>


### [153] [Benchmarking Mutual Information-based Loss Functions in Federated Learning](https://arxiv.org/abs/2504.11877)
*Sarang S,Harsh D. Chothani,Qilei Li,Ahmed M. Abdelmoniem,Arnab K. Paul*

Main category: cs.LG

TL;DR: The paper explores using Mutual Information (MI)-based loss functions to address fairness and performance issues in Federated Learning (FL), aiming to reduce biases and disparities among clients.


<details>
  <summary>Details</summary>
Motivation: Growing privacy concerns and regulations like GDPR highlight the need for privacy-preserving and fair machine learning, but FL faces challenges like biases and uneven performance among clients.

Method: The study leverages MI-based loss functions to measure dependencies between variables, extract essential features, and minimize biases in FL.

Result: Benchmarking shows MI-based losses reduce disparities among clients and enhance FL performance.

Conclusion: MI-based loss functions improve fairness and effectiveness in FL, addressing key challenges in decentralized model training.

Abstract: Federated Learning (FL) has attracted considerable interest due to growing
privacy concerns and regulations like the General Data Protection Regulation
(GDPR), which stresses the importance of privacy-preserving and fair machine
learning approaches. In FL, model training takes place on decentralized data,
so as to allow clients to upload a locally trained model and receive a globally
aggregated model without exposing sensitive information. However, challenges
related to fairness-such as biases, uneven performance among clients, and the
"free rider" issue complicates its adoption. In this paper, we examine the use
of Mutual Information (MI)-based loss functions to address these concerns. MI
has proven to be a powerful method for measuring dependencies between variables
and optimizing deep learning models. By leveraging MI to extract essential
features and minimize biases, we aim to improve both the fairness and
effectiveness of FL systems. Through extensive benchmarking, we assess the
impact of MI-based losses in reducing disparities among clients while enhancing
the overall performance of FL.

</details>


### [154] [HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems](https://arxiv.org/abs/2504.11885)
*Qiyue Chen,Shaolin Tan,Suixiang Gao,Jinhu Lü*

Main category: cs.LG

TL;DR: HyperSAT, a novel GNN-based method, improves Weighted MaxSAT problem-solving by using hypergraph representation and cross-attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods for Weighted MaxSAT are underdeveloped due to non-linear dependencies and sensitive objectives from non-uniform clause weights.

Method: HyperSAT employs an unsupervised hypergraph neural network with cross-attention and shared representation constraint loss to model literal-clause interactions.

Result: HyperSAT outperforms state-of-the-art methods on various Weighted MaxSAT datasets.

Conclusion: HyperSAT effectively addresses Weighted MaxSAT challenges, demonstrating superior performance through innovative hypergraph modeling.

Abstract: Graph neural networks (GNNs) have shown promising performance in solving both
Boolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due
to their ability to efficiently model and capture the structural dependencies
between literals and clauses. However, GNN methods for solving Weighted MaxSAT
problems remain underdeveloped. The challenges arise from the non-linear
dependency and sensitive objective function, which are caused by the
non-uniform distribution of weights across clauses. In this paper, we present
HyperSAT, a novel neural approach that employs an unsupervised hypergraph
neural network model to solve Weighted MaxSAT problems. We propose a hypergraph
representation for Weighted MaxSAT instances and design a cross-attention
mechanism along with a shared representation constraint loss function to
capture the logical interactions between positive and negative literal nodes in
the hypergraph. Extensive experiments on various Weighted MaxSAT datasets
demonstrate that HyperSAT achieves better performance than state-of-the-art
competitors.

</details>


### [155] [FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data](https://arxiv.org/abs/2504.11903)
*Yuan Zhou,Jiachen Zhong,Xinli Shi,Guanghui Wen,Xinghuo Yu*

Main category: cs.LG

TL;DR: FedCanon is a novel composite federated learning algorithm that reduces proximal computation costs and mitigates data heterogeneity effects, achieving strong convergence rates and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing composite federated learning methods often require multiple proximal operations and struggle with data heterogeneity, limiting their efficiency and performance.

Method: FedCanon decouples proximal mappings from local updates, requiring only one server-side proximal evaluation per iteration, and uses control variables to incorporate global gradient information.

Result: Theoretical analysis shows sublinear and linear convergence rates under non-convex and Polyak-Łojasiewicz conditions, respectively. Experiments confirm superior accuracy and efficiency, especially with heterogeneous data.

Conclusion: FedCanon addresses key limitations of existing methods, offering improved computational efficiency and robustness to data heterogeneity.

Abstract: Composite federated learning offers a general framework for solving machine
learning problems with additional regularization terms. However, many existing
methods require clients to perform multiple proximal operations to handle
non-smooth terms and their performance are often susceptible to data
heterogeneity. To overcome these limitations, we propose a novel composite
federated learning algorithm called \textbf{FedCanon}, designed to solve the
optimization problems comprising a possibly non-convex loss function and a
weakly convex, potentially non-smooth regularization term. By decoupling
proximal mappings from local updates, FedCanon requires only a single proximal
evaluation on the server per iteration, thereby reducing the overall proximal
computation cost. It also introduces control variables that incorporate global
gradient information into client updates, which helps mitigate the effects of
data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves
sublinear convergence rates under general non-convex settings and linear
convergence under the Polyak-{\L}ojasiewicz condition, without relying on
bounded heterogeneity assumptions. Experiments demonstrate that FedCanon
outperforms the state-of-the-art methods in terms of both accuracy and
computational efficiency, particularly under heterogeneous data distributions.

</details>


### [156] [SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models](https://arxiv.org/abs/2504.11923)
*Zeyu Dai,Shengcai Liu,Rui He,Jiahao Wu,Ning Lu,Wenqi Fan,Qing Li,Ke Tang*

Main category: cs.LG

TL;DR: SemDiff is a novel unrestricted adversarial attack using diffusion models' semantic latent space to create natural and imperceptible adversarial examples, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing unrestricted adversarial examples (UAEs) lack naturalness and imperceptibility due to suboptimal optimization in latent noise.

Method: SemDiff explores semantic latent space for meaningful attributes and uses multi-attribute optimization to balance attack success and naturalness.

Result: SemDiff achieves higher attack success rates and better imperceptibility on high-resolution datasets (CelebA-HQ, AFHQ, ImageNet) and evades defenses effectively.

Conclusion: SemDiff is a highly effective and threatening adversarial attack method, producing natural and semantically meaningful UAEs.

Abstract: Unrestricted adversarial examples (UAEs), allow the attacker to create
non-constrained adversarial examples without given clean samples, posing a
severe threat to the safety of deep learning models. Recent works utilize
diffusion models to generate UAEs. However, these UAEs often lack naturalness
and imperceptibility due to simply optimizing in intermediate latent noises. In
light of this, we propose SemDiff, a novel unrestricted adversarial attack that
explores the semantic latent space of diffusion models for meaningful
attributes, and devises a multi-attributes optimization approach to ensure
attack success while maintaining the naturalness and imperceptibility of
generated UAEs. We perform extensive experiments on four tasks on three
high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results
demonstrate that SemDiff outperforms state-of-the-art methods in terms of
attack success rate and imperceptibility. The generated UAEs are natural and
exhibit semantically meaningful changes, in accord with the attributes'
weights. In addition, SemDiff is found capable of evading different defenses,
which further validates its effectiveness and threatening.

</details>


### [157] [VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning](https://arxiv.org/abs/2504.11944)
*Xuyang Chen,Guojian Wang,Keyu Yan,Lin Zhao*

Main category: cs.LG

TL;DR: VIPO is a novel model-based offline RL algorithm that uses self-supervised feedback from value estimation to improve model training, outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Offline RL is practical but suffers from unreliable conservatism due to model errors. VIPO aims to enhance model accuracy systematically.

Method: VIPO minimizes inconsistency between value estimates from offline data and the model, improving training via self-supervised feedback.

Result: VIPO achieves state-of-the-art performance on D4RL and NeoRL benchmarks, demonstrating superior model accuracy and efficiency.

Conclusion: VIPO provides a general, effective framework for enhancing model-based offline RL, setting new benchmarks in performance.

Abstract: Offline reinforcement learning (RL) learns effective policies from
pre-collected datasets, offering a practical solution for applications where
online interactions are risky or costly. Model-based approaches are
particularly advantageous for offline RL, owing to their data efficiency and
generalizability. However, due to inherent model errors, model-based methods
often artificially introduce conservatism guided by heuristic uncertainty
estimation, which can be unreliable. In this paper, we introduce VIPO, a novel
model-based offline RL algorithm that incorporates self-supervised feedback
from value estimation to enhance model training. Specifically, the model is
learned by additionally minimizing the inconsistency between the value learned
directly from the offline data and the one estimated from the model. We perform
comprehensive evaluations from multiple perspectives to show that VIPO can
learn a highly accurate model efficiently and consistently outperform existing
methods. It offers a general framework that can be readily integrated into
existing model-based offline RL algorithms to systematically enhance model
accuracy. As a result, VIPO achieves state-of-the-art performance on almost all
tasks in both D4RL and NeoRL benchmarks.

</details>


### [158] [Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification](https://arxiv.org/abs/2504.11981)
*Sosei Ikeda,Hiromitsu Awano,Takashi Sato*

Main category: cs.LG

TL;DR: Proposes a dot-product-based reservoir representation (DPRR) and a hardware-friendly delayed-feedback reservoir (DFR) for efficient time-series classification in edge computing.


<details>
  <summary>Details</summary>
Motivation: Existing methods for converting features in reservoir computing (RC) involve costly matrix inversion, increasing hardware complexity. A simpler, hardware-efficient solution is needed.

Method: Introduces DPRR for constant-length intermediate representation and a fully digital DFR model, avoiding analog circuits.

Result: The proposed DFR successfully classified difficult multivariate time-series data with superior accuracy and smaller circuit size compared to existing methods.

Conclusion: The DPRR and digital DFR offer a practical, efficient solution for hardware implementation in RC, outperforming conventional methods.

Abstract: Reservoir computing (RC) is attracting attention as a machine-learning
technique for edge computing. In time-series classification tasks, the number
of features obtained using a reservoir depends on the length of the input
series. Therefore, the features must be converted to a constant-length
intermediate representation (IR), such that they can be processed by an output
layer. Existing conversion methods involve computationally expensive matrix
inversion that significantly increases the circuit size and requires processing
power when implemented in hardware. In this article, we propose a simple but
effective IR, namely, dot-product-based reservoir representation (DPRR), for RC
based on the dot product of data features. Additionally, we propose a
hardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear
element and delayed feedback loop with DPRR. The proposed DFR successfully
classified multivariate time series data that has been considered particularly
difficult to implement efficiently in hardware. In contrast to conventional DFR
models that require analog circuits, the proposed model can be implemented in a
fully digital manner suitable for high-level syntheses. A comparison with
existing machine-learning methods via field-programmable gate array
implementation using 12 multivariate time-series classification tasks confirmed
the superior accuracy and small circuit size of the proposed method.

</details>


### [159] [Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets](https://arxiv.org/abs/2504.11990)
*Yechao Zhang,Yuxuan Zhou,Tianyu Li,Minghui Li,Shengshan Hu,Wei Luo,Leo Yu Zhang*

Main category: cs.LG

TL;DR: The paper addresses backdoor risks in transfer learning, proposing the Trusted Core (T-Core) Bootstrapping framework to enhance security by identifying trustworthy data and neurons.


<details>
  <summary>Details</summary>
Motivation: Pre-trained encoders in transfer learning expose models to backdoor attacks, with existing defenses being reactive and ineffective under resource constraints.

Method: The authors analyze existing defenses and introduce T-Core, a proactive framework focusing on clean elements (data and neurons) for security.

Result: T-Core outperforms baselines, tested against 12 poisoning attacks and 14 defenses across five datasets, handling three backdoor threats.

Conclusion: T-Core provides a robust solution for mitigating backdoor risks in resource-constrained transfer learning, emphasizing proactive security.

Abstract: Transfer learning from pre-trained encoders has become essential in modern
machine learning, enabling efficient model adaptation across diverse tasks.
However, this combination of pre-training and downstream adaptation creates an
expanded attack surface, exposing models to sophisticated backdoor embeddings
at both the encoder and dataset levels--an area often overlooked in prior
research. Additionally, the limited computational resources typically available
to users of pre-trained encoders constrain the effectiveness of generic
backdoor defenses compared to end-to-end training from scratch. In this work,
we investigate how to mitigate potential backdoor risks in resource-constrained
transfer learning scenarios. Specifically, we conduct an exhaustive analysis of
existing defense strategies, revealing that many follow a reactive workflow
based on assumptions that do not scale to unknown threats, novel attack types,
or different training paradigms. In response, we introduce a proactive mindset
focused on identifying clean elements and propose the Trusted Core (T-Core)
Bootstrapping framework, which emphasizes the importance of pinpointing
trustworthy data and neurons to enhance model security. Our empirical
evaluations demonstrate the effectiveness and superiority of T-Core,
specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning
attacks, and 14 baseline defenses across five benchmark datasets, addressing
four scenarios of 3 potential backdoor threats.

</details>


### [160] [Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation](https://arxiv.org/abs/2504.11992)
*Pascal Schlachter,Jonathan Fuss,Bin Yang*

Main category: cs.LG

TL;DR: The paper analyzes pseudo-labeling in online source-free universal domain adaptation (SF-UniDA), revealing gaps between current methods and ideal performance, and highlights the importance of pseudo-label accuracy over quantity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of category shifts in online SF-UniDA and understand the impact of pseudo-labeling on adaptation outcomes.

Method: Systematic analysis through controlled experiments with simulated pseudo-labeling, comparing contrastive and cross-entropy losses.

Result: A significant gap exists between state-of-the-art and ideal pseudo-labeling performance; contrastive loss works well with moderate accuracy, while cross-entropy excels with near-perfect labels.

Conclusion: Pseudo-label accuracy is crucial, and prioritizing high-confidence labels is beneficial, providing insights for future SF-UniDA advancements.

Abstract: A domain (distribution) shift between training and test data often hinders
the real-world performance of deep neural networks, necessitating unsupervised
domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged
as a solution for practical scenarios where access to source data is restricted
and target data is received as a continuous stream. However, the open-world
nature of many real-world applications additionally introduces category shifts
meaning that the source and target label spaces may differ. Online source-free
universal domain adaptation (SF-UniDA) addresses this challenge. Existing
methods mainly rely on self-training with pseudo-labels, yet the relationship
between pseudo-labeling and adaptation outcomes has not been studied yet. To
bridge this gap, we conduct a systematic analysis through controlled
experiments with simulated pseudo-labeling, offering valuable insights into
pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap
between the current state-of-the-art and the upper bound of adaptation achieved
with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables
effective adaptation even with moderate pseudo-label accuracy, while a
cross-entropy loss, though less robust to pseudo-label errors, achieves
superior results when pseudo-labeling approaches perfection. Lastly, our
findings indicate that pseudo-label accuracy is in general more crucial than
quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels
is beneficial. Overall, our study highlights the critical role of
pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive
future advancements in the field. Our code is available at
https://github.com/pascalschlachter/PLAnalysis.

</details>


### [161] [A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs](https://arxiv.org/abs/2504.11997)
*Kihyuk Hong,Ambuj Tewari*

Main category: cs.LG

TL;DR: A new value iteration method for reinforcement learning in linear MDPs avoids prohibitive computations by clipping over visited states only, matching prior regret bounds with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior methods for average-reward reinforcement learning in linear MDPs require impractical computations over large or infinite state spaces.

Method: Introduces a value iteration algorithm with efficient clipping, minimizing over visited states instead of the entire state space.

Result: Achieves the same regret bound as previous work while being computationally efficient, independent of state space size.

Conclusion: The proposed method is both statistically and computationally efficient, addressing a key limitation of prior approaches.

Abstract: We study reinforcement learning in infinite-horizon average-reward settings
with linear MDPs. Previous work addresses this problem by approximating the
average-reward setting by discounted setting and employing a value
iteration-based algorithm that uses clipping to constrain the span of the value
function for improved statistical efficiency. However, the clipping procedure
requires computing the minimum of the value function over the entire state
space, which is prohibitive since the state space in linear MDP setting can be
large or even infinite. In this paper, we introduce a value iteration method
with efficient clipping operation that only requires computing the minimum of
value functions over the set of states visited by the algorithm. Our algorithm
enjoys the same regret bound as the previous work while being computationally
efficient, with computational complexity that is independent of the size of the
state space.

</details>


### [162] [Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition](https://arxiv.org/abs/2504.12011)
*Heesoo Jung,Hogun Park*

Main category: cs.LG

TL;DR: The paper introduces BSG, a framework for balancing smoothness in graph self-supervised learning (SSL), addressing limitations of existing methods by decomposing the SSL objective into three balanced terms.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods in graphs, like contrastive learning, may not effectively capture essential graph properties, such as neighbor representation similarity, leading to polarized performance on downstream tasks.

Method: The BSG framework decomposes the SSL objective into neighbor loss, minimal loss, and divergence loss, introducing novel loss functions to balance these terms. Theoretical analysis and experiments validate the approach.

Result: BSG achieves state-of-the-art performance on node classification and link prediction tasks across multiple datasets, outperforming existing methods.

Conclusion: Balancing the three SSL terms improves performance across diverse downstream tasks, demonstrating the effectiveness of the BSG framework in graph SSL.

Abstract: Self-supervised learning (SSL) in graphs has garnered significant attention,
particularly in employing Graph Neural Networks (GNNs) with pretext tasks
initially designed for other domains, such as contrastive learning and feature
reconstruction. However, it remains uncertain whether these methods effectively
reflect essential graph properties, precisely representation similarity with
its neighbors. We observe that existing methods position opposite ends of a
spectrum driven by the graph embedding smoothness, with each end corresponding
to outperformance on specific downstream tasks. Decomposing the SSL objective
into three terms via an information-theoretic framework with a neighbor
representation variable reveals that this polarization stems from an imbalance
among the terms, which existing methods may not effectively maintain. Further
insights suggest that balancing between the extremes can lead to improved
performance across a wider range of downstream tasks. A framework, BSG
(Balancing Smoothness in Graph SSL), introduces novel loss functions designed
to supplement the representation quality in graph-based SSL by balancing the
derived three terms: neighbor loss, minimal loss, and divergence loss. We
present a theoretical analysis of the effects of these loss functions,
highlighting their significance from both the SSL and graph smoothness
perspectives. Extensive experiments on multiple real-world datasets across node
classification and link prediction consistently demonstrate that BSG achieves
state-of-the-art performance, outperforming existing methods. Our
implementation code is available at https://github.com/steve30572/BSG.

</details>


### [163] [Active Human Feedback Collection via Neural Contextual Dueling Bandits](https://arxiv.org/abs/2504.12016)
*Arun Verma,Xiaoqiang Lin,Zhongxiang Dai,Daniela Rus,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: Neural-ADB is a neural contextual dueling bandit algorithm for efficiently collecting human preference feedback when the reward function is non-linear, outperforming linear assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume linear reward functions, which are unrealistic for applications like online recommendation and LLM alignment. Neural-ADB addresses this gap.

Method: Neural-ADB uses a neural contextual dueling bandit framework to handle non-linear reward functions, validated under the Bradley-Terry-Luce model.

Result: Theoretical and experimental results show Neural-ADB's policy sub-optimality gap decreases sub-linearly with dataset growth.

Conclusion: Neural-ADB provides a practical and principled solution for non-linear reward scenarios, validated by synthetic datasets.

Abstract: Collecting human preference feedback is often expensive, leading recent works
to develop principled algorithms to select them more efficiently. However,
these works assume that the underlying reward function is linear, an assumption
that does not hold in many real-life applications, such as online
recommendation and LLM alignment. To address this limitation, we propose
Neural-ADB, an algorithm based on the neural contextual dueling bandit
framework that provides a principled and practical method for collecting human
preference feedback when the underlying latent reward function is non-linear.
We theoretically show that when preference feedback follows the
Bradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by
Neural-ADB decreases at a sub-linear rate as the preference dataset increases.
Our experimental results on problem instances derived from synthetic preference
datasets further validate the effectiveness of Neural-ADB.

</details>


### [164] [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
*David Khachaturov,Robert Mullins,Ilia Shumailov,Sumanth Dathathri*

Main category: cs.LG

TL;DR: The paper explores mimicry in text, where humans and LLMs unintentionally imitate synthetic text properties, challenging current watermarking and detection methods.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLM misuse (e.g., misinformation) led to countermeasures like detectors and watermarking, but unintentional mimicry by humans or unwatermarked LLMs may undermine these efforts.

Method: Investigates conversational adaptation (mimicry) by humans and LLMs, including watermarking signals, in various settings.

Result: Demonstrates that mimicry occurs, challenging assumptions and suggesting the need for lower false positives and longer word sequences in watermarking.

Conclusion: Current watermarking methods may be unreliable due to mimicry; improvements are needed for long-term reliability.

Abstract: Recent advancements in Large Language Models (LLMs) raised concerns over
potential misuse, such as for spreading misinformation. In response two counter
measures emerged: machine learning-based detectors that predict if text is
synthetic, and LLM watermarking, which subtly marks generated text for
identification and attribution. Meanwhile, humans are known to adjust language
to their conversational partners both syntactically and lexically. By
implication, it is possible that humans or unwatermarked LLMs could
unintentionally mimic properties of LLM generated text, making counter measures
unreliable. In this work we investigate the extent to which such conversational
adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that
both humans and LLMs end up mimicking, including the watermarking signal even
in seemingly improbable settings. This challenges current academic assumptions
and suggests that for long-term watermarking to be reliable, the likelihood of
false positives needs to be significantly lower, while longer word sequences
should be used for seeding watermarking mechanisms.

</details>


### [165] [FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning](https://arxiv.org/abs/2504.12025)
*Yu Zhang,Qingfeng Du,Jiaqi Lv*

Main category: cs.LG

TL;DR: FedEPA is a novel FL framework for multimodal learning, addressing data heterogeneity and limited labeled data via personalized aggregation and unsupervised modality alignment.


<details>
  <summary>Details</summary>
Motivation: Existing FL systems often assume unimodal client data and struggle with limited labeled data, limiting real-world applicability.

Method: FedEPA uses personalized local model aggregation and unsupervised modality alignment, including contrastive learning and multimodal feature fusion.

Result: FedEPA outperforms existing FL methods in multimodal classification tasks with limited labeled data.

Conclusion: FedEPA effectively addresses challenges in multimodal FL, improving performance under data constraints.

Abstract: Federated Learning (FL) enables decentralized model training across multiple
parties while preserving privacy. However, most FL systems assume clients hold
only unimodal data, limiting their real-world applicability, as institutions
often possess multimodal data. Moreover, the lack of labeled data further
constrains the performance of most FL methods. In this work, we propose FedEPA,
a novel FL framework for multimodal learning. FedEPA employs a personalized
local model aggregation strategy that leverages labeled data on clients to
learn personalized aggregation weights, thereby alleviating the impact of data
heterogeneity. We also propose an unsupervised modality alignment strategy that
works effectively with limited labeled data. Specifically, we decompose
multimodal features into aligned features and context features. We then employ
contrastive learning to align the aligned features across modalities, ensure
the independence between aligned features and context features within each
modality, and promote the diversity of context features. A multimodal feature
fusion strategy is introduced to obtain a joint embedding. The experimental
results show that FedEPA significantly outperforms existing FL methods in
multimodal classification tasks under limited labeled data conditions.

</details>


### [166] [Generative Deep Learning Framework for Inverse Design of Fuels](https://arxiv.org/abs/2504.12075)
*Kiran K. Yalamanchi,Pinaki Pal,Balaji Mohan,Abdullah S. AlRamadan,Jihad A. Badra,Yuanjiang Pei*

Main category: cs.LG

TL;DR: A generative deep learning framework (Co-VAE) combined with QSPR techniques accelerates inverse fuel design, optimizing molecular reconstruction and RON prediction.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional fuel screening by capturing complex structure-property relationships for discovering high-RON fuels.

Method: Co-VAE integrates property prediction with VAE latent space, uses GDB-13 and RON data, hyperparameter tuning, regression refinement, and differential evolution for candidate identification.

Result: The model efficiently explores chemical space, identifying high-RON fuel candidates and improving reconstruction and prediction accuracy.

Conclusion: The framework is adaptable for additional fuel properties and synthesizability, enhancing de novo fuel design.

Abstract: In the present work, a generative deep learning framework combining a
Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative
structure-property relationship (QSPR) techniques is developed to enable
accelerated inverse design of fuels. The Co-VAE integrates a property
prediction component coupled with the VAE latent space, enhancing molecular
reconstruction and accurate estimation of Research Octane Number (RON) (chosen
as the fuel property of interest). A subset of the GDB-13 database, enriched
with a curated RON database, is used for model training. Hyperparameter tuning
is further utilized to optimize the balance among reconstruction fidelity,
chemical validity, and RON prediction. An independent regression model is then
used to refine RON prediction, while a differential evolution algorithm is
employed to efficiently navigate the VAE latent space and identify promising
fuel molecule candidates with high RON. This methodology addresses the
limitations of traditional fuel screening approaches by capturing complex
structure-property relationships within a comprehensive latent representation.
The generative model provides a flexible tool for systematically exploring vast
chemical spaces, paving the way for discovering fuels with superior anti-knock
properties. The demonstrated approach can be readily extended to incorporate
additional fuel properties and synthesizability criteria to enhance
applicability and reliability for de novo design of new fuels.

</details>


### [167] [Neural Contextual Bandits Under Delayed Feedback Constraints](https://arxiv.org/abs/2504.12086)
*Mohammadali Moghimi,Sharu Theresa Jose,Shana Moothedath*

Main category: cs.LG

TL;DR: A new algorithm, Delayed NeuralUCB, addresses delayed reward feedback in neural contextual bandits, with a variant, Delayed NeuralTS, using Thompson Sampling. Both show effectiveness in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Delayed reward feedback is common in applications like online recommendations and clinical trials, where outcomes take time to manifest. Existing methods struggle with such delays.

Method: The proposed Delayed NeuralUCB uses UCB-based exploration, while Delayed NeuralTS employs Thompson Sampling. Both handle random, unknown delays in reward feedback.

Result: The algorithms achieve bounded cumulative regret under sub-exponential delay assumptions and perform well on real-world datasets like MNIST and Mushroom.

Conclusion: The proposed algorithms effectively manage delayed feedback, making them suitable for complex real-world applications.

Abstract: This paper presents a new algorithm for neural contextual bandits (CBs) that
addresses the challenge of delayed reward feedback, where the reward for a
chosen action is revealed after a random, unknown delay. This scenario is
common in applications such as online recommendation systems and clinical
trials, where reward feedback is delayed because the outcomes or results of a
user's actions (such as recommendations or treatment responses) take time to
manifest and be measured. The proposed algorithm, called Delayed NeuralUCB,
uses an upper confidence bound (UCB)-based exploration strategy. Under the
assumption of independent and identically distributed sub-exponential reward
delays, we derive an upper bound on the cumulative regret over a T-length
horizon. We further consider a variant of the algorithm, called Delayed
NeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments
on real-world datasets, such as MNIST and Mushroom, along with comparisons to
benchmark approaches, demonstrate that the proposed algorithms effectively
manage varying delays and are well-suited for complex real-world scenarios.

</details>


### [168] [Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis](https://arxiv.org/abs/2504.12151)
*Miaosen Luo,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: KAN-MCP integrates interpretable Kolmogorov-Arnold Networks (KAN) with the robust Multimodal Clean Pareto (MCPareto) framework to address interpretability and modality imbalance in Multimodal Sentiment Analysis (MSA).


<details>
  <summary>Details</summary>
Motivation: The challenges of interpretability in multimodal fusion and modality imbalance due to inter-modal information disparities drive the need for a transparent and robust solution.

Method: KAN provides interpretable analysis of cross-modal interactions, while MCPareto enhances robustness via DRD-MIB for denoising and dimensionality reduction, and dynamic gradient balancing.

Result: Superior performance on benchmark datasets (CMU-MOSI, CMU-MOSEI, CH-SIMS v2) with intuitive visualization through KAN's interpretable architecture.

Conclusion: KAN-MCP successfully combines interpretability and robustness, offering a high-performing and transparent solution for MSA.

Abstract: Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack
of interpretability in the decision logic of multimodal fusion and modality
imbalance caused by disparities in inter-modal information density. To address
these issues, we propose KAN-MCP, a novel framework that integrates the
interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the
Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its
univariate function decomposition to achieve transparent analysis of
cross-modal interactions. This structural design allows direct inspection of
feature transformations without relying on external interpretation tools,
thereby ensuring both high expressiveness and interpretability. Second, the
proposed MCPareto enhances robustness by addressing modality imbalance and
noise interference. Specifically, we introduce the Dimensionality Reduction and
Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises
and reduces feature dimensionality. This approach provides KAN with
discriminative low-dimensional inputs to reduce the modeling complexity of KAN
while preserving critical sentiment-related information. Furthermore, MCPareto
dynamically balances gradient contributions across modalities using the
purified features output by DRD-MIB, ensuring lossless transmission of
auxiliary signals and effectively alleviating modality imbalance. This synergy
of interpretability and robustness not only achieves superior performance on
benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers
an intuitive visualization interface through KAN's interpretable architecture.

</details>


### [169] [Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications](https://arxiv.org/abs/2504.12156)
*Mustafa Cavus*

Main category: cs.LG

TL;DR: The paper explores predictive multiplicity in survival analysis, introducing measures to quantify it and demonstrating its impact on model reliability in high-stakes applications like predictive maintenance.


<details>
  <summary>Details</summary>
Motivation: Predictive multiplicity undermines model reliability, but its implications in survival analysis, crucial for tasks like maintenance scheduling, remain unexplored.

Method: The authors define formal measures (ambiguity, discrepancy, obscurity) to quantify predictive multiplicity in survival models and apply them to benchmark datasets.

Result: Findings show ambiguity reaches 40-45%, discrepancy follows a similar trend, and obscurity is mild but concentrated, indicating conflicting risk estimates across models.

Conclusion: Predictive multiplicity must be measured and communicated to ensure reliable decision-making in survival-based applications.

Abstract: In many applications, especially those involving prediction, models may yield
near-optimal performance yet significantly disagree on individual-level
outcomes. This phenomenon, known as predictive multiplicity, has been formally
defined in binary, probabilistic, and multi-target classification, and
undermines the reliability of predictive systems. However, its implications
remain unexplored in the context of survival analysis, which involves
estimating the time until a failure or similar event while properly handling
censored data. We frame predictive multiplicity as a critical concern in
survival-based models and introduce formal measures -- ambiguity, discrepancy,
and obscurity -- to quantify it. This is particularly relevant for downstream
tasks such as maintenance scheduling, where precise individual risk estimates
are essential. Understanding and reporting predictive multiplicity helps build
trust in models deployed in high-stakes environments. We apply our methodology
to benchmark datasets from predictive maintenance, extending the notion of
multiplicity to survival models. Our findings show that ambiguity steadily
increases, reaching up to 40-45% of observations; discrepancy is lower but
exhibits a similar trend; and obscurity remains mild and concentrated in a few
models. These results demonstrate that multiple accurate survival models may
yield conflicting estimations of failure risk and degradation progression for
the same equipment. This highlights the need to explicitly measure and
communicate predictive multiplicity to ensure reliable decision-making in
process health management.

</details>


### [170] [Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning](https://arxiv.org/abs/2504.12181)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys is a battery-aware FL framework that optimizes energy use by scheduling client participation based on battery levels, reducing redundant computations and improving stability.


<details>
  <summary>Details</summary>
Motivation: Addressing high energy consumption in FL, especially in energy-harvesting systems with fluctuating device availability.

Method: Cyclic client participation based on battery levels, clustering clients, and scheduling training before transmission to minimize energy use.

Result: Outperforms existing methods in energy efficiency and performance consistency, even with non-i.i.d. data and infrequent charging.

Conclusion: FedBacys offers a robust, resource-aware scheduling strategy for EHFL, balancing communication and computation costs.

Abstract: Federated Learning (FL) has emerged as a promising framework for distributed
learning, but its growing complexity has led to significant energy consumption,
particularly from computations on the client side. This challenge is especially
critical in energy-harvesting FL (EHFL) systems, where device availability
fluctuates due to limited and time-varying energy resources. We propose
FedBacys, a battery-aware FL framework that introduces cyclic client
participation based on users' battery levels to cope with these issues.
FedBacys enables clients to save energy and strategically perform local
training just before their designated transmission time by clustering clients
and scheduling their involvement sequentially. This design minimizes redundant
computation, reduces system-wide energy usage, and improves learning stability.
Our experiments demonstrate that FedBacys outperforms existing approaches in
terms of energy efficiency and performance consistency, exhibiting robustness
even under non-i.i.d. training data distributions and with very infrequent
battery charging. This work presents the first comprehensive evaluation of
cyclic client participation in EHFL, incorporating both communication and
computation costs into a unified, resource-aware scheduling strategy.

</details>


### [171] [SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields](https://arxiv.org/abs/2504.12262)
*David Keetae Park,Xihaier Luo,Guang Zhao,Seungjun Lee,Miruna Oprescu,Shinjae Yoo*

Main category: cs.LG

TL;DR: SCENT is a novel transformer-based framework for scalable spatiotemporal learning, unifying interpolation, reconstruction, and forecasting, with superior performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in spatiotemporal learning, such as high dimensionality, irregular data, and scalability constraints, particularly in scientific domains.

Method: SCENT uses a transformer-based encoder-processor-decoder with learnable queries, query-wise cross-attention, and sparse attention for scalability.

Result: Demonstrates state-of-the-art performance in simulations and real-world experiments, with efficient evaluation at arbitrary resolutions.

Conclusion: SCENT effectively tackles spatiotemporal learning challenges, offering a scalable and high-performing solution.

Abstract: Spatiotemporal learning is challenging due to the intricate interplay between
spatial and temporal dependencies, the high dimensionality of the data, and
scalability constraints. These challenges are further amplified in scientific
domains, where data is often irregularly distributed (e.g., missing values from
sensor failures) and high-volume (e.g., high-fidelity simulations), posing
additional computational and modeling difficulties. In this paper, we present
SCENT, a novel framework for scalable and continuity-informed spatiotemporal
representation learning. SCENT unifies interpolation, reconstruction, and
forecasting within a single architecture. Built on a transformer-based
encoder-processor-decoder backbone, SCENT introduces learnable queries to
enhance generalization and a query-wise cross-attention mechanism to
effectively capture multi-scale dependencies. To ensure scalability in both
data size and model complexity, we incorporate a sparse attention mechanism,
enabling flexible output representations and efficient evaluation at arbitrary
resolutions. We validate SCENT through extensive simulations and real-world
experiments, demonstrating state-of-the-art performance across multiple
challenging tasks while achieving superior scalability.

</details>


### [172] [Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)](https://arxiv.org/abs/2504.12270)
*ChenNingZhi Sheng,Rafal Kustra,Davide Chicco*

Main category: cs.LG

TL;DR: The study evaluates clustering algorithms (K-means, Hierarchical Clustering, PAM) on CLSA cognitive data to identify dementia-related clusters, using various metrics. K-means and PAM performed similarly, while Hierarchical Clustering differed. Key metrics include entropy, separation index, and Adjusted Rand Index.


<details>
  <summary>Details</summary>
Motivation: To identify clinically relevant clusters in cognitive data for dementia research and improve understanding of clustering techniques in healthcare.

Method: Applied K-means, Hierarchical Clustering, and PAM to CLSA data (18,891 participants). Evaluated using internal (e.g., silhouette width, entropy) and comparison metrics (e.g., ARI).

Result: K-means and PAM yielded similar results, differing from Hierarchical Clustering. Key metrics were entropy, separation index, and Adjusted Rand Index.

Conclusion: The study aids dementia research and suggests applying these metrics in healthcare. It enhances understanding of clustering techniques for medical data.

Abstract: Purpose: The primary goal of this study is to explore the application of
evaluation metrics to different clustering algorithms using the data provided
from the Canadian Longitudinal Study (CLSA), focusing on cognitive features.
The objective of our work is to discover potential clinically relevant clusters
that contribute to the development of dementia over time-based on cognitive
changes. Method: The CLSA dataset includes 18,891 participants with data
available at both baseline and follow-up assessments, to which clustering
algorithms were applied. The clustering methodologies employed in this analysis
are K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning
Around Medoids (PAM). We use multiple evaluation metrics to assess our
analysis. For internal evaluation metrics, we use: Average silhouette Width,
Within and Between the sum of square Ratio (WB.Ratio), Entropy,
Calinski-Harabasz Index (CH Index), and Separation Index. For clustering
comparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index
(ARI), Rand Index (RI), and Variation Information. Results: Using evaluation
metrics to compare the results of the three clustering techniques, K-means and
Partitioning Around Medoids (PAM) produced similar results. In contrast, there
are significant differences between K-means clustering and Hierarchical
Clustering. Our study highlights the importance of the two internal evaluation
metrics: entropy and separation index. In between clustering comparison
metrics, the Adjusted Rand Index is a key tool. Conclusion: The study results
have the potential to contribute to understanding dementia. Researchers can
also benefit by applying the suggested evaluation metrics to other areas of
healthcare research. Overall, our study improves the understanding of using
clustering techniques and evaluation metrics to reveal complex patterns in
medical data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [173] [Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images](https://arxiv.org/abs/2504.11491)
*Mansoor Hayat,Supavadee Aramvith,Subrata Bhattacharjee,Nouman Ahmad*

Main category: eess.IV

TL;DR: The paper introduces Attention GhostUNet++, a deep learning model for precise segmentation of abdominal adipose tissue and liver, outperforming baseline models with high Dice coefficients.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of SAT, VAT, and liver is crucial for assessing body composition and health risks like diabetes and cardiovascular disease.

Method: The model integrates Channel, Spatial, and Depth Attention mechanisms into Ghost UNet++ for automated segmentation, evaluated on AATTCT-IDS and LiTS datasets.

Result: Achieved Dice coefficients of 0.9430 (VAT), 0.9639 (SAT), and 0.9652 (liver), surpassing baselines.

Conclusion: Attention GhostUNet++ improves feature refinement, contextual understanding, and efficiency, offering a robust solution despite minor boundary detail limitations.

Abstract: Accurate segmentation of abdominal adipose tissue, including subcutaneous
(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is
essential for understanding body composition and associated health risks such
as type 2 diabetes and cardiovascular disease. This study proposes Attention
GhostUNet++, a novel deep learning model incorporating Channel, Spatial, and
Depth Attention mechanisms into the Ghost UNet++ bottleneck for automated,
precise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model
achieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for
liver segmentation, surpassing baseline models. Despite minor limitations in
boundary detail segmentation, the proposed model significantly enhances feature
refinement, contextual understanding, and computational efficiency, offering a
robust solution for body composition analysis. The implementation of the
proposed Attention GhostUNet++ model is available
at:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus.

</details>


### [174] [Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework](https://arxiv.org/abs/2504.11469)
*Guillaume Garret,Antoine Vacavant,Carole Frindel*

Main category: eess.IV

TL;DR: The paper introduces an explainability pipeline for 3D vessel segmentation to assess how models use global anatomical context, revealing a reliance on localized cues over global structures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency in deep learning models for medical image segmentation, especially in vascular applications where global anatomical context is crucial.

Method: A novel pipeline combining gradient-based attribution, graph-guided point selection, and blob-based analysis of saliency maps, applied to IRCAD and Bullitt datasets.

Result: Models rely heavily on localized attribution blobs near points of interest, with little correlation to global vessel properties like connectivity or thickness.

Conclusion: The study highlights the need for structured explainability tools and reveals current models' limitations in leveraging global anatomical context.

Abstract: Deep learning models have achieved impressive performance in medical image
segmentation, yet their black-box nature limits clinical adoption. In vascular
applications, trustworthy segmentation should rely on both local image cues and
global anatomical structures, such as vessel connectivity or branching.
However, the extent to which models leverage such global context remains
unclear. We present a novel explainability pipeline for 3D vessel segmentation,
combining gradient-based attribution with graph-guided point selection and a
blob-based analysis of Saliency maps. Using vascular graphs extracted from
ground truth, we define anatomically meaningful points of interest (POIs) and
assess the contribution of input voxels via Saliency maps. These are analyzed
at both global and local scales using a custom blob detector. Applied to IRCAD
and Bullitt datasets, our analysis shows that model decisions are dominated by
highly localized attribution blobs centered near POIs. Attribution features
show little correlation with vessel-level properties such as thickness,
tubularity, or connectivity -- suggesting limited use of global anatomical
reasoning. Our results underline the importance of structured explainability
tools and highlight the current limitations of segmentation models in capturing
global vascular context.

</details>


### [175] [Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD](https://arxiv.org/abs/2504.11474)
*Byunggun Kim,Younghun Kwon*

Main category: eess.IV

TL;DR: A transformer model for ADHD diagnosis using rs-fMRI learns spatiotemporal features and correlations, outperforming other variants with 77.78% accuracy.


<details>
  <summary>Details</summary>
Motivation: ADHD is a common mental disorder in both children and adults, necessitating effective diagnostic tools.

Method: Proposes a CNN-based embedding block, local temporal attention, and ROI-rank based masking to learn BOLD signals and distinguish ROIs.

Result: Achieved 77.78% accuracy, 76.60% specificity, 79.22% sensitivity, and 79.30% AUC, outperforming other transformer models.

Conclusion: The spatiotemporal enhanced transformer is effective for ADHD diagnosis, leveraging fMRI data for improved biomarker identification.

Abstract: In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of
the common mental diseases discovered not only in children but also in adults.
In this context, we propose a ADHD diagnosis transformer model that can
effectively simultaneously find important brain spatiotemporal biomarkers from
resting-state functional magnetic resonance (rs-fMRI). This model not only
learns spatiotemporal individual features but also learns the correlation with
full attention structures specialized in ADHD diagnosis. In particular, it
focuses on learning local blood oxygenation level dependent (BOLD) signals and
distinguishing important regions of interest (ROI) in the brain. Specifically,
the three proposed methods for ADHD diagnosis transformer are as follows.
First, we design a CNN-based embedding block to obtain more expressive
embedding features in brain region attention. It is reconstructed based on the
previously CNN-based ADHD diagnosis models for the transformer. Next, for
individual spatiotemporal feature attention, we change the attention method to
local temporal attention and ROI-rank based masking. For the temporal features
of fMRI, the local temporal attention enables to learn local BOLD signal
features with only simple window masking. For the spatial feature of fMRI,
ROI-rank based masking can distinguish ROIs with high correlation in ROI
relationships based on attention scores, thereby providing a more specific
biomarker for ADHD diagnosis. The experiment was conducted with various types
of transformer models. To evaluate these models, we collected the data from 939
individuals from all sites provided by the ADHD-200 competition. Through this,
the spatiotemporal enhanced transformer for ADHD diagnosis outperforms the
performance of other different types of transformer variants. (77.78ACC
76.60SPE 79.22SEN 79.30AUC)

</details>


### [176] [Deciphering scrolls with tomography: A training experiment](https://arxiv.org/abs/2504.11485)
*Sonia Foschiatti,Axel Kittenberger,Otmar Scherzer*

Main category: eess.IV

TL;DR: A non-destructive educational lab setup using visible light and software to simulate virtual recovery of ancient damaged documents.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recovering severely damaged ancient documents without physical unwrapping, using safer alternatives to X-rays.

Method: Developed an experimental setup with visible light and a didactic software pipeline for virtual reconstruction of wrapped scrolls.

Result: A practical and educational method for students to virtually reconstruct ancient texts without harmful X-rays.

Conclusion: The proposed lab setup provides a safe, effective, and educational approach to simulating the recovery of ancient documents.

Abstract: The recovery of severely damaged ancient written documents has proven to be a
major challenge for many scientists, mainly due to the impracticality of
physical unwrapping them. Non-destructive techniques, such as X-ray computed
tomography (CT), combined with computer vision algorithms, have emerged as a
means of facilitating the virtual reading of the hidden contents of the damaged
documents. This paper proposes an educational laboratory aimed at simulating
the entire process of acquisition and virtual recovery of the ancient works. We
have developed an experimental setup that uses visible light to replace the
detrimental X-rays, and a didactic software pipeline that allows students to
virtually reconstruct a transparent rolled sheet with printed text on it, the
wrapped scroll.

</details>


### [177] [Learned enclosure method for experimental EIT data](https://arxiv.org/abs/2504.11512)
*Sara Sippola,Siiri Rautio,Andreas Hauptmann,Takanori Ide,Samuli Siltanen*

Main category: eess.IV

TL;DR: The paper proposes a hybrid method combining the enclosure method with neural networks to estimate the convex hull of inclusions in EIT, outperforming traditional least squares fitting.


<details>
  <summary>Details</summary>
Motivation: EIT's inverse problem is nonlinear and ill-posed, prompting the need for innovative solutions combining analytical and machine learning approaches.

Method: The method integrates Ikehata's enclosure method with neural networks to estimate the convex hull of inclusions from boundary measurements.

Result: The proposed approach outperforms the classical enclosure method with least squares fitting on both simulated and experimental data.

Conclusion: Combining analytical methods with machine learning improves accuracy in solving EIT's inverse problem, particularly for estimating inclusion convex hulls.

Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method with
diverse applications, including medical imaging and non-destructive testing.
The inverse problem of reconstructing internal electrical conductivity from
boundary measurements is nonlinear and highly ill-posed, making it difficult to
solve accurately. In recent years, there has been growing interest in combining
analytical methods with machine learning to solve inverse problems. In this
paper, we propose a method for estimating the convex hull of inclusions from
boundary measurements by combining the enclosure method proposed by Ikehata
with neural networks. We demonstrate its performance using experimental data.
Compared to the classical enclosure method with least squares fitting, the
learned convex hull achieves superior performance on both simulated and
experimental data.

</details>


### [178] [HyperKING: Quantum-Classical Generative Adversarial Networks for Hyperspectral Image Restoration](https://arxiv.org/abs/2504.11782)
*Chia-Hsiang Lin,Si-Sheng Young*

Main category: eess.IV

TL;DR: A hybrid quantum-classical GAN framework (HyperKING) is proposed for hyperspectral SRS, outperforming classical methods in tasks like tensor completion and noise removal.


<details>
  <summary>Details</summary>
Motivation: Quantum GANs show superior potential but are limited by qubit resources, prompting the development of a hybrid framework for larger hyperspectral images.

Method: Combines quantum (provably fully expressive) and classical (convolutional layers) architectures to process 128x128 hyperspectral images.

Result: Achieves significant improvements (3dB) in hyperspectral tensor completion, noise removal, and blind source separation.

Conclusion: HyperKING demonstrates the feasibility and superiority of hybrid quantum-classical GANs for advanced SRS applications.

Abstract: Quantum machine intelligence starts showing its impact on satellite remote
sensing (SRS). Also, recent literature exhibits that quantum generative
intelligences encompass superior potential than their classical counterpart,
motivating us to develop quantum generative adversarial networks (GANs) for
SRS. However, existing quantum GANs are restricted by the limited quantum bit
(qubit) resources of current quantum computers and process merely a small 2x2
grayscale image, far from being applicable to SRS. Recently, the novel concept
of hybrid quantum-classical GAN, a quantum generator with a classical
discriminator, has upgraded the order to 28x28 (still grayscale), whereas it is
still insufficient for SRS. This motivates us to design a radically new hybrid
framework, where both generator and discriminator are hybrid architectures. We
demonstrate this feasibility, leading to a breakthrough of processing 128x128
hyperspectral images for SRS. Specifically, we design the quantum part with
mathematically provable quantum full expressibility (FE) to address core signal
processing tasks, wherein the FE property allows the quantum network to realize
any valid quantum operator with appropriate training. The classical part,
composed of convolutional layers, treats the read-in (compressing the optical
information into limited qubits) and read-out (addressing the quantum collapse
effect) procedures. The proposed innovative hybrid quantum GAN, named
Hyperspectral Knot-like IntelligeNt dIscrimiNator and Generator (HyperKING),
where knot partly symbolizes the quantum entanglement and partly the compressed
quantum domain in the central part of the network architecture. HyperKING
significantly surpasses the classical approaches in hyperspectral tensor
completion, mixed noise removal (about 3dB improvement), and blind source
separation results.

</details>


### [179] [TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images Segmentation](https://arxiv.org/abs/2504.11825)
*Kangbo Ma*

Main category: eess.IV

TL;DR: TextDiffSeg is a text-guided diffusion model for 3D medical image segmentation, improving accuracy and reducing computational costs by integrating text and image data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of Diffusion Probabilistic Models (DPMs) in 3D medical image segmentation, such as high computational costs and poor global context capture.

Method: Proposes TextDiffSeg, a framework combining 3D volumetric data with natural language descriptions using cross-modal embedding and attention mechanisms.

Result: Outperforms existing methods in segmentation tasks for kidney and pancreas tumors, and multi-organ scenarios, with validated key components.

Conclusion: TextDiffSeg offers an efficient, accurate solution for 3D medical image segmentation, with broad clinical applicability.

Abstract: Diffusion Probabilistic Models (DPMs) have demonstrated significant potential
in 3D medical image segmentation tasks. However, their high computational cost
and inability to fully capture global 3D contextual information limit their
practical applications. To address these challenges, we propose a novel
text-guided diffusion model framework, TextDiffSeg. This method leverages a
conditional diffusion framework that integrates 3D volumetric data with natural
language descriptions, enabling cross-modal embedding and establishing a shared
semantic space between visual and textual modalities. By enhancing the model's
ability to recognize complex anatomical structures, TextDiffSeg incorporates
innovative label embedding techniques and cross-modal attention mechanisms,
effectively reducing computational complexity while preserving global 3D
contextual integrity. Experimental results demonstrate that TextDiffSeg
consistently outperforms existing methods in segmentation tasks involving
kidney and pancreas tumors, as well as multi-organ segmentation scenarios.
Ablation studies further validate the effectiveness of key components,
highlighting the synergistic interaction between text fusion, image feature
extractor, and label encoder. TextDiffSeg provides an efficient and accurate
solution for 3D medical image segmentation, showcasing its broad applicability
in clinical diagnosis and treatment planning.

</details>


### [180] [Novel-view X-ray Projection Synthesis through Geometry-Integrated Deep Learning](https://arxiv.org/abs/2504.11953)
*Daiqi Liu,Fuxin Fan,Andreas Maier*

Main category: eess.IV

TL;DR: The paper introduces DL-GIPS, a model that synthesizes new X-ray projections from a single existing one, reducing radiation exposure and simplifying clinical processes.


<details>
  <summary>Details</summary>
Motivation: Traditional X-ray imaging requires multiple projections, increasing radiation exposure and complexity. DL-GIPS aims to minimize these drawbacks by generating new views from a single projection.

Method: DL-GIPS manipulates geometry and texture features from an initial X-ray projection to match new angles, then synthesizes the final projection using an advanced image generation process.

Result: The model effectively synthesizes X-ray projections, demonstrated through lung imaging, showing potential to revolutionize stereoscopic and volumetric imaging.

Conclusion: DL-GIPS offers a promising solution to reduce radiation exposure and streamline X-ray imaging, with broad applicability in medical diagnostics.

Abstract: X-ray imaging plays a crucial role in the medical field, providing essential
insights into the internal anatomy of patients for diagnostics, image-guided
procedures, and clinical decision-making. Traditional techniques often require
multiple X-ray projections from various angles to obtain a comprehensive view,
leading to increased radiation exposure and more complex clinical processes.
This paper explores an innovative approach using the DL-GIPS model, which
synthesizes X-ray projections from new viewpoints by leveraging a single
existing projection. The model strategically manipulates geometry and texture
features extracted from an initial projection to match new viewing angles. It
then synthesizes the final projection by merging these modified geometry
features with consistent texture information through an advanced image
generation process. We demonstrate the effectiveness and broad applicability of
the DL-GIPS framework through lung imaging examples, highlighting its potential
to revolutionize stereoscopic and volumetric imaging by minimizing the need for
extensive data acquisition.

</details>


### [181] [Modality-Independent Explainable Detection of Inaccurate Organ Segmentations Using Denoising Autoencoders](https://arxiv.org/abs/2504.12203)
*Levente Lippenszky,István Megyeri,Krisztian Koos,Zsófia Karancsi,Borbála Deák-Karancsi,András Frontó,Árpád Makk,Attila Rádics,Erhan Bas,László Ruskó*

Main category: eess.IV

TL;DR: A denoising autoencoder-based method detects inaccurate organ segmentations in radiation therapy planning, outperforming existing approaches and working across MR and CT scans.


<details>
  <summary>Details</summary>
Motivation: Inaccurate organ segmentations in radiation therapy can lead to suboptimal treatment, necessitating a reliable detection method.

Method: Noise was added to ground truth organ segmentations, and autoencoders were trained to denoise them, providing visual reconstructions of inaccuracies.

Result: The method is imaging-modality independent (works on MR and CT) and outperforms existing approaches for most organs.

Conclusion: The denoising autoencoder method offers explainable and superior detection of suboptimal organ segmentations.

Abstract: In radiation therapy planning, inaccurate segmentations of organs at risk can
result in suboptimal treatment delivery, if left undetected by the clinician.
To address this challenge, we developed a denoising autoencoder-based method to
detect inaccurate organ segmentations. We applied noise to ground truth organ
segmentations, and the autoencoders were tasked to denoise them. Through the
application of our method to organ segmentations generated on both MR and CT
scans, we demonstrated that the method is independent of imaging modality. By
providing reconstructions, our method offers visual information about
inaccurate regions of the organ segmentations, leading to more explainable
detection of suboptimal segmentations. We compared our method to existing
approaches in the literature and demonstrated that it achieved superior
performance for the majority of organs.

</details>


### [182] [Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography](https://arxiv.org/abs/2504.12249)
*Zhijin He,Alan B. McMillan*

Main category: eess.IV

TL;DR: The study evaluates radiomics and deep learning models for disease detection in chest radiography, comparing their accuracy and robustness to guide clinical AI integration.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding which AI models (radiomics or deep learning) perform better for disease detection in medical imaging, especially in data-limited scenarios.

Method: Systematic comparison of radiomics-based models (Decision Trees, Gradient Boosting, Random Forests, SVM, MLP) and deep learning models (CNNs, ViTs) using performance metrics across varying sample sizes.

Result: Insights into the efficacy of each model type, highlighting contexts where specific AI approaches may enhance diagnostic capabilities.

Conclusion: Provides guidance for selecting AI models in clinical practice, emphasizing timely and reliable diagnosis in automated environments.

Abstract: The application of artificial intelligence (AI) in medical imaging has
revolutionized diagnostic practices, enabling advanced analysis and
interpretation of radiological data. This study presents a comprehensive
evaluation of radiomics-based and deep learning-based approaches for disease
detection in chest radiography, focusing on COVID-19, lung opacity, and viral
pneumonia. While deep learning models, particularly convolutional neural
networks (CNNs) and vision transformers (ViTs), learn directly from image data,
radiomics-based models extract and analyze quantitative features, potentially
providing advantages in data-limited scenarios. This study systematically
compares the diagnostic accuracy and robustness of various AI models, including
Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines
(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against
state-of-the-art computer vision deep learning architectures. Performance
metrics across varying sample sizes reveal insights into each model's efficacy,
highlighting the contexts in which specific AI approaches may offer enhanced
diagnostic capabilities. The results aim to inform the integration of AI-driven
diagnostic tools in clinical practice, particularly in automated and
high-throughput environments where timely, reliable diagnosis is critical. This
comparative study addresses an essential gap, establishing guidance for the
selection of AI models based on clinical and operational needs.

</details>


### [183] [Correlation Ratio for Unsupervised Learning of Multi-modal Deformable Registration](https://arxiv.org/abs/2504.12265)
*Xiaojian Chen,Yihao Liu,Shuwen Wei,Aaron Carass,Yong Du,Junyu Chen*

Main category: eess.IV

TL;DR: Proposes a differentiable correlation ratio for deep learning-based multi-modal deformable image registration, validated on neuroimaging data, and explores trade-offs in a Bayesian framework.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods underexplore the correlation ratio, a historically useful similarity measure for multi-modal image registration.

Method: Extends the non-differentiable correlation ratio using Parzen windowing approximation for backpropagation and employs a Bayesian framework to study trade-offs.

Result: Validated on a multi-modal neuroimaging dataset, showing effectiveness of the proposed differentiable correlation ratio.

Conclusion: The differentiable correlation ratio and Bayesian framework enhance multi-modal deformable image registration performance.

Abstract: In recent years, unsupervised learning for deformable image registration has
been a major research focus. This approach involves training a registration
network using pairs of moving and fixed images, along with a loss function that
combines an image similarity measure and deformation regularization. For
multi-modal image registration tasks, the correlation ratio has been a
widely-used image similarity measure historically, yet it has been
underexplored in current deep learning methods. Here, we propose a
differentiable correlation ratio to use as a loss function for learning-based
multi-modal deformable image registration. This approach extends the
traditionally non-differentiable implementation of the correlation ratio by
using the Parzen windowing approximation, enabling backpropagation with deep
neural networks. We validated the proposed correlation ratio on a multi-modal
neuroimaging dataset. In addition, we established a Bayesian training framework
to study how the trade-off between the deformation regularizer and similarity
measures, including mutual information and our proposed correlation ratio,
affects the registration performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [184] [Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph](https://arxiv.org/abs/2504.11502)
*Jatin Nainani,Chia-Tung Ho,Anirudh Dhurka,Haoxing Ren*

Main category: cs.SE

TL;DR: A timing analysis agent using multi-LLMs and hierarchical planning automates VLSI timing report analysis, achieving high pass-rates (98% single-report, 90% multi-report).


<details>
  <summary>Details</summary>
Motivation: Smaller metal pitches and more devices in VLSI design increase timing debug complexity, requiring efficient automation to reduce turnaround times.

Method: Proposes a timing analysis agent with multi-LLMs, hierarchical planning, and a Timing Debug Relation Graph (TDRG) for automated report analysis.

Result: Achieves 98% pass-rate for single-report and 90% for multi-report benchmarks in industrial designs.

Conclusion: The agent effectively automates timing analysis, demonstrating adaptability and high accuracy in industrial settings.

Abstract: Timing analysis is an essential and demanding verification method for Very
Large Scale Integrated (VLSI) circuit design and optimization. In addition, it
also serves as the cornerstone of the final sign-off, determining whether the
chip is ready to be sent to the semiconductor foundry for fabrication.
Recently, as the technology advance relentlessly, smaller metal pitches and the
increasing number of devices have led to greater challenges and longer
turn-around-time for experienced human designers to debug timing issues from
the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient
and intelligent methodology is highly necessary and essential for debugging
timing issues and reduce the turnaround times. Recently, Large Language Models
(LLMs) have shown great promise across various tasks in language understanding
and interactive decision-making, incorporating reasoning and actions. In this
work, we propose a timing analysis agent, that is empowered by multi-LLMs task
solving, and incorporates a novel hierarchical planning and solving flow to
automate the analysis of timing reports from commercial tool. In addition, we
build a Timing Debug Relation Graph (TDRG) that connects the reports with the
relationships of debug traces from experienced timing engineers. The timing
analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG)
approach, that includes agent and coding to retrieve data accurately, on the
developed TDRG. In our studies, the proposed timing analysis agent achieves an
average 98% pass-rate on a single-report benchmark and a 90% pass-rate for
multi-report benchmark from industrial designs, demonstrating its effectiveness
and adaptability.

</details>


### [185] [Unravelling Technical debt topics through Time, Programming Languages and Repository](https://arxiv.org/abs/2504.11714)
*Karthik Shivashankar,Antonio Martini*

Main category: cs.SE

TL;DR: The study analyzes Technical Debt (TD) topics in software engineering over time, languages, and repositories using BERTopic and sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the diversity and temporal evolution of TD topics.

Method: Explorative analysis of GitHub issues (2015-2023) using BERTopic for topic modeling and sentiment analysis.

Result: Categorization of TD topics and their progression, along with sentiment insights.

Conclusion: Provides nuanced understanding of TD trends and shifts over time, languages, and repositories.

Abstract: This study explores the dynamic landscape of Technical Debt (TD) topics in
software engineering by examining its evolution across time, programming
languages, and repositories. Despite the extensive research on identifying and
quantifying TD, there remains a significant gap in understanding the diversity
of TD topics and their temporal development. To address this, we have conducted
an explorative analysis of TD data extracted from GitHub issues spanning from
2015 to September 2023. We employed BERTopic for sophisticated topic modelling.
This study categorises the TD topics and tracks their progression over time.
Furthermore, we have incorporated sentiment analysis for each identified topic,
providing a deeper insight into the perceptions and attitudes associated with
these topics. This offers a more nuanced understanding of the trends and shifts
in TD topics through time, programming language, and repository.

</details>


### [186] [On the calibration of Just-in-time Defect Prediction](https://arxiv.org/abs/2504.12051)
*Xhulja Shahini,Jone Bartel,Klaus Pohl*

Main category: cs.SE

TL;DR: The paper evaluates the calibration of JIT defect prediction models, finding miscalibration issues and inconsistent improvements from post-calibration methods.


<details>
  <summary>Details</summary>
Motivation: To address misclassification errors in JIT defect prediction by ensuring reliable confidence scores for predictions.

Method: Evaluated calibration of three JIT DP techniques and tested post-calibration methods.

Result: All models showed miscalibration (ECE 2-35%), with post-calibration not consistently effective.

Conclusion: JIT DP models need better calibration methods to ensure reliable confidence scores.

Abstract: Just in time defect prediction (JIT DP) leverages ML to identify defect-prone
code commits, enabling quality assurance (QA) teams to allocate resources more
efficiently by focusing on commits that are most likely to contain defects.
Although JIT DP techniques have introduced improvements in terms of predictive
accuracy, they are still susceptible to misclassification errors such as false
positives and negatives. This can lead to wasted resources or undetected
defects, a particularly critical concern when QA resources are limited. To
mitigate these challenges and preserve the practical utility of JIT DP tools,
it becomes essential to estimate the reliability of the predictions, i.e.,
computing confidence scores. Such scores can help practitioners determine the
trustworthiness of predictions and thus prioritize them efficiently. A simple
approach to computing confidence scores is to extract, alongside each
prediction, the corresponding prediction probabilities and use them as
indicators of confidence. However, for these probabilities to reliably serve as
confidence scores, the predictive model must be well-calibrated. This means
that the prediction probabilities must accurately represent the true likelihood
of each prediction being correct. Miscalibration, common in modern ML models,
distorts probability scores such that they do not align with the actual
correctness probability. In this study, we evaluate the calibration of three
JIT DP techniques to determine whether and to what extent they exhibit poor
calibration. Furthermore, we assess whether post-calibration methods can
improve the calibration of existing JIT defect prediction models. Our results
reveal that all evaluated JIT DP models exhibit some level of miscalibration,
with ECE ranging from 2-35%. Furthermore, post-calibration methods do not
consistently improve the calibration.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [187] [Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels](https://arxiv.org/abs/2504.11667)
*Cemil Vahapoglu,Timothy J. O'Shea,Wan Liu,Tamoghna Roy,Sennur Ulukus*

Main category: cs.IT

TL;DR: A novel unsupervised deep learning framework combining depthwise separable convolutions and transformers improves beamforming in MU-SIMO systems under imperfect CSI, outperforming ZFBF and MMSE baselines.


<details>
  <summary>Details</summary>
Motivation: Enhancing throughput and reliable communication in dense urban MU-SIMO systems by addressing high-dimensional data challenges and imperfect CSI.

Method: Integration of depthwise separable convolutions and transformers for unsupervised beamforming weight generation.

Result: Proposed NNBF framework outperforms ZFBF and MMSE in spectral efficiency and BLER.

Conclusion: The framework effectively maximizes sum-rate and ensures reliable communication, demonstrating superiority over traditional methods.

Abstract: The literature is abundant with methodologies focusing on using transformer
architectures due to their prominence in wireless signal processing and their
capability to capture long-range dependencies via attention mechanisms. In
particular, depthwise separable convolutions enhance parameter efficiency for
the process of high-dimensional data characteristics of MIMO systems. In this
work, we introduce a novel unsupervised deep learning framework that integrates
depthwise separable convolutions and transformers to generate beamforming
weights under imperfect channel state information (CSI) for a multi-user
single-input multiple-output (MU-SIMO) system in dense urban environments. The
primary goal is to enhance throughput by maximizing sum-rate while ensuring
reliable communication. Spectral efficiency and block error rate (BLER) are
considered as performance metrics. Experiments are carried out under various
conditions to compare the performance of the proposed NNBF framework against
baseline methods zero-forcing beamforming (ZFBF) and minimum mean square error
(MMSE) beamforming. Experimental results demonstrate the superiority of the
proposed framework over the baseline techniques.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [188] [Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks](https://arxiv.org/abs/2504.12210)
*Tingyang Sun,Tuan Nguyen,Ting He*

Main category: cs.NI

TL;DR: The paper proposes a joint design of communication schemes and mixing matrices for decentralized federated learning (DFL) to improve performance in multi-hop bandwidth-limited edge networks, reducing training time by over 80% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: DFL faces performance challenges due to extensive parameter exchanges in edge networks, and existing solutions lack realistic communication models for multi-hop bandwidth-limited scenarios.

Method: The authors jointly design the communication scheme and mixing matrix, formulating each as a tractable optimization problem and developing an efficient algorithm with guaranteed performance.

Result: Evaluations on real topology and data show an 80% reduction in total training time compared to baselines, with no accuracy loss and improved computational efficiency over state-of-the-art methods.

Conclusion: The proposed approach effectively addresses DFL performance challenges in edge networks, offering significant efficiency gains while maintaining accuracy.

Abstract: Decentralized federated learning (DFL) is a promising machine learning
paradigm for bringing artificial intelligence (AI) capabilities to the network
edge. Running DFL on top of edge networks, however, faces severe performance
challenges due to the extensive parameter exchanges between agents. Most
existing solutions for these challenges were based on simplistic communication
models, which cannot capture the case of learning over a multi-hop
bandwidth-limited network. In this work, we address this problem by jointly
designing the communication scheme for the overlay network formed by the agents
and the mixing matrix that controls the communication demands between the
agents. By carefully analyzing the properties of our problem, we cast each
design problem into a tractable optimization and develop an efficient algorithm
with guaranteed performance. Our evaluations based on real topology and data
show that the proposed algorithm can reduce the total training time by over
$80\%$ compared to the baseline without sacrificing accuracy, while
significantly improving the computational efficiency over the state of the art.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [189] [Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach](https://arxiv.org/abs/2504.11889)
*Donghee Han,Hwanjun Song,Mun Yong Yi*

Main category: cs.IR

TL;DR: A Query-to-Recommendation approach using LLMs improves recommendation efficiency, diversity, and performance, addressing challenges like inefficiency and sensitivity to item order.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommendation methods struggle with inefficiency, sensitivity to item order, poor scalability, and unrealistic evaluation due to random negative sampling.

Method: Proposes a Query-to-Recommendation approach where LLMs generate personalized queries to retrieve relevant items from the entire candidate pool, avoiding pre-selection.

Result: Experiments show up to 57% improvement, with an average gain of 31%, demonstrating strong zero-shot performance and further gains when combined with existing models.

Conclusion: The method enhances recommendation performance and diversity, works well for less popular items, and integrates seamlessly into existing systems without additional training.

Abstract: Existing large language model LLM-based recommendation methods face several
challenges, including inefficiency in handling large candidate pools,
sensitivity to item order within prompts ("lost in the middle" phenomenon) poor
scalability, and unrealistic evaluation due to random negative sampling. To
address these issues, we propose a Query-to-Recommendation approach that
leverages LLMs to generate personalized queries for retrieving relevant items
from the entire candidate pool, eliminating the need for candidate
pre-selection. This method can be integrated into an ID-based recommendation
system without additional training, enhances recommendation performance and
diversity through LLMs' world knowledge, and performs well even for less
popular item groups. Experiments on three datasets show up to 57 percent
improvement, with an average gain of 31 percent, demonstrating strong zero-shot
performance and further gains when ensembled with existing models.

</details>


### [190] [RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems](https://arxiv.org/abs/2504.11510)
*Xiaohua Feng,Yuyuan Li,Fengyuan Yu,Ke Xiong,Junjie Fang,Li Zhang,Tianyu Du,Chaochao Chen*

Main category: cs.IR

TL;DR: RAID is an in-training defense method for recommender systems that protects against attribute inference attacks by making protected attributes indistinguishable while maintaining recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Attribute inference attacks exploit exposed user profiles in recommender systems, compromising privacy. Existing defenses are limited to post-training or suffer from unstable training. RAID addresses these gaps.

Method: RAID defines a defensive objective to ensure protected attributes are independent of labels, solving a constrained Wasserstein barycenter problem using optimal transport to align users with a centroid distribution.

Result: Experiments on four datasets show RAID effectively mitigates attacks and outperforms existing methods.

Conclusion: RAID provides a robust in-training defense against attribute inference attacks without compromising recommendation quality.

Abstract: In various networks and mobile applications, users are highly susceptible to
attribute inference attacks, with particularly prevalent occurrences in
recommender systems. Attackers exploit partially exposed user profiles in
recommendation models, such as user embeddings, to infer private attributes of
target users, such as gender and political views. The goal of defenders is to
mitigate the effectiveness of these attacks while maintaining recommendation
performance. Most existing defense methods, such as differential privacy and
attribute unlearning, focus on post-training settings, which limits their
capability of utilizing training data to preserve recommendation performance.
Although adversarial training extends defenses to in-training settings, it
often struggles with convergence due to unstable training processes. In this
paper, we propose RAID, an in-training defense method against attribute
inference attacks in recommender systems. In addition to the recommendation
objective, we define a defensive objective to ensure that the distribution of
protected attributes becomes independent of class labels, making users
indistinguishable from attribute inference attacks. Specifically, this
defensive objective aims to solve a constrained Wasserstein barycenter problem
to identify the centroid distribution that makes the attribute
indistinguishable while complying with recommendation performance constraints.
To optimize our proposed objective, we use optimal transport to align users
with the centroid distribution. We conduct extensive experiments on four
real-world datasets to evaluate RAID. The experimental results validate the
effectiveness of RAID and demonstrate its significant superiority over existing
methods in multiple aspects.

</details>


### [191] [Optimizing Compound Retrieval Systems](https://arxiv.org/abs/2504.12063)
*Harrie Oosterhuis,Rolf Jagerman,Zhen Qin,Xuanhui Wang*

Main category: cs.IR

TL;DR: The paper introduces compound retrieval systems, a broader class of retrieval systems that use multiple models, including LLMs, for better trade-offs between effectiveness and efficiency compared to cascading approaches.


<details>
  <summary>Details</summary>
Motivation: Current cascading approaches in retrieval systems balance quality and computational costs but limit model interactions. The paper aims to explore broader interactions, especially with LLMs, for improved rankings.

Method: Proposes compound retrieval systems, which learn where to apply component models and how to aggregate their predictions, combining BM25 with LLM relevance predictions.

Result: Optimized compound systems outperform cascading approaches in effectiveness-efficiency trade-offs, even in self-supervised settings.

Conclusion: The work encourages innovative thinking in model interactions for retrieval systems, showcasing the potential of compound approaches.

Abstract: Modern retrieval systems do not rely on a single ranking model to construct
their rankings. Instead, they generally take a cascading approach where a
sequence of ranking models are applied in multiple re-ranking stages. Thereby,
they balance the quality of the top-K ranking with computational costs by
limiting the number of documents each model re-ranks. However, the cascading
approach is not the only way models can interact to form a retrieval system.
  We propose the concept of compound retrieval systems as a broader class of
retrieval systems that apply multiple prediction models. This encapsulates
cascading models but also allows other types of interactions than top-K
re-ranking. In particular, we enable interactions with large language models
(LLMs) which can provide relative relevance comparisons. We focus on the
optimization of compound retrieval system design which uniquely involves
learning where to apply the component models and how to aggregate their
predictions into a final ranking. This work shows how our compound approach can
combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM
relevance predictions, while optimizing a given ranking metric and efficiency
target. Our experimental results show optimized compound retrieval systems
provide better trade-offs between effectiveness and efficiency than cascading
approaches, even when applied in a self-supervised manner.
  With the introduction of compound retrieval systems, we hope to inspire the
information retrieval field to more out-of-the-box thinking on how prediction
models can interact to form rankings.

</details>


### [192] [PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage](https://arxiv.org/abs/2504.11509)
*Wenyi Zhang,Ju Jia,Xiaojun Jia,Yihao Huang,Xinfeng Li,Cong Wu,Lina Wang*

Main category: cs.IR

TL;DR: Proposes PATFinger, a training-free fingerprinting scheme for verifying multimodal dataset usage, outperforming baselines by 30%.


<details>
  <summary>Details</summary>
Motivation: Addresses gaps in cross-modal dataset verification, avoiding intrusive methods that degrade accuracy and non-intrusive methods with unstable behaviors.

Method: Uses global optimal perturbation (GOP) and adaptive prompts to capture dataset-specific traits without model triggers.

Result: PATFinger improves verification accuracy by 30% over existing methods.

Conclusion: PATFinger offers a robust, training-free solution for cross-modal dataset verification.

Abstract: The multimodal datasets can be leveraged to pre-train large-scale
vision-language models by providing cross-modal semantics. Current endeavors
for determining the usage of datasets mainly focus on single-modal dataset
ownership verification through intrusive methods and non-intrusive techniques,
while cross-modal approaches remain under-explored. Intrusive methods can adapt
to multimodal datasets but degrade model accuracy, while non-intrusive methods
rely on label-driven decision boundaries that fail to guarantee stable
behaviors for verification. To address these issues, we propose a novel
prompt-adapted transferable fingerprinting scheme from a training-free
perspective, called PATFinger, which incorporates the global optimal
perturbation (GOP) and the adaptive prompts to capture dataset-specific
distribution characteristics. Our scheme utilizes inherent dataset attributes
as fingerprints instead of compelling the model to learn triggers. The GOP is
derived from the sample distribution to maximize embedding drifts between
different modalities. Subsequently, our PATFinger re-aligns the adaptive prompt
with GOP samples to capture the cross-modal interactions on the carefully
crafted surrogate model. This allows the dataset owner to check the usage of
datasets by observing specific prediction behaviors linked to the PATFinger
during retrieval queries. Extensive experiments demonstrate the effectiveness
of our scheme against unauthorized multimodal dataset usage on various
cross-modal retrieval architectures by 30% over state-of-the-art baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [193] [Language and Knowledge Representation: A Stratified Approach](https://arxiv.org/abs/2504.11492)
*Mayukh Bagchi*

Main category: cs.DB

TL;DR: The thesis addresses representation heterogeneity by proposing a stratified top-down approach using UKC, teleontology, and the kTelos methodology, with proof-of-concepts in DataScientia and JIDEP.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity is intrinsic to representations, as different observers encode reality differently using varied concepts, language, and knowledge.

Method: A top-down approach with stratified representation formalism, UKC for language, teleontology for knowledge, and the LiveKnowledge catalog for reuse. The kTelos methodology integrates these components.

Result: Proof-of-concepts in DataScientia (data catalogs) and JIDEP (materials modelling) demonstrate the approach's effectiveness.

Conclusion: The thesis presents a solution to representation heterogeneity and suggests future research directions.

Abstract: The thesis proposes the problem of representation heterogeneity to emphasize
the fact that heterogeneity is an intrinsic property of any representation,
wherein, different observers encode different representations of the same
target reality in a stratified manner using different concepts, language and
knowledge (as well as data). The thesis then advances a top-down solution
approach to the above stratified problem of representation heterogeneity in
terms of several solution components, namely: (i) a representation formalism
stratified into concept level, language level, knowledge level and data level
to accommodate representation heterogeneity, (ii) a top-down language
representation using Universal Knowledge Core (UKC), UKC namespaces and domain
languages to tackle the conceptual and language level heterogeneity, (iii) a
top-down knowledge representation using the notions of language teleontology
and knowledge teleontology to tackle the knowledge level heterogeneity, (iv)
the usage and further development of the existing LiveKnowledge catalog for
enforcing iterative reuse and sharing of language and knowledge
representations, and, (v) the kTelos methodology integrating the solution
components above to iteratively generate the language and knowledge
representations absolving representation heterogeneity. The thesis also
includes proof-of-concepts of the language and knowledge representations
developed for two international research projects - DataScientia (data
catalogs) and JIDEP (materials modelling). Finally, the thesis concludes with
future lines of research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [194] [Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' "Typo" Correction](https://arxiv.org/abs/2504.11622)
*Seyyed Ali Ayati,Jin Hyun Park,Yichen Cai,Marcus Botacin*

Main category: cs.CR

TL;DR: The paper proposes integrating Visual Transformers (VTs) and Large Language Models (LLMs) to enhance Acoustic Side-Channel Attacks (ASCAs) by improving robustness under noisy conditions and correcting mispredictions.


<details>
  <summary>Details</summary>
Motivation: Current ASCA models lack robustness in noisy environments, necessitating better contextual understanding and error correction.

Method: Uses VTs for long-term contextual learning and LLMs (like GPT-4o or smaller fine-tuned models) for error correction.

Result: VTs outperform CNNs in keystroke classification, and LLMs significantly improve error correction, even with smaller models.

Conclusion: Combining VTs and LLMs provides a practical solution for robust ASCAs, with lightweight LLMs offering comparable performance to larger models.

Abstract: The large integration of microphones into devices increases the opportunities
for Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture
keystrokes' audio signals that might reveal sensitive information. However, the
current State-Of-The-Art (SOTA) models for ASCAs, including Convolutional
Neural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit
limited robustness under realistic noisy conditions. Solving this problem
requires either: (i) an increased model's capacity to infer contextual
information from longer sequences, allowing the model to learn that an
initially noisily typed word is the same as a futurely collected non-noisy
word, or (ii) an approach to fix misidentified information from the contexts,
as one does not type random words, but the ones that best fit the conversation
context. In this paper, we demonstrate that both strategies are viable and
complementary solutions for making ASCAs practical. We observed that no
existing solution leverages advanced transformer architectures' power for these
tasks and propose that: (i) Visual Transformers (VTs) are the candidate
solutions for capturing long-term contextual information and (ii)
transformer-powered Large Language Models (LLMs) are the candidate solutions to
fix the ``typos'' (mispredictions) the model might make. Thus, we here present
the first-of-its-kind approach that integrates VTs and LLMs for ASCAs.
  We first show that VTs achieve SOTA performance in classifying keystrokes
when compared to the previous CNN benchmark. Second, we demonstrate that LLMs
can mitigate the impact of real-world noise. Evaluations on the natural
sentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA
pipeline boosts the performance of error-correction tasks; and (ii) the
comparable performance can be attained by a lightweight, fine-tuned smaller LLM
(67 times smaller than GPT-4o), using...

</details>


### [195] [MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks](https://arxiv.org/abs/2504.11575)
*Furqan Rustam,Islam Obaidat,Anca Delia Jurcut*

Main category: cs.CR

TL;DR: Proposes an online, continuous learning methodology for DDoS detection in Multi-Environment networks, using a multi-level framework with two ML models for real-time adaptation and high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based DDoS detection systems struggle with adaptability to new attack strategies and lack real-time capabilities.

Method: Develops a realistic M-En dataset using NS-3, employs a multi-level framework (MULTI-LF) with two ML models (M1 for fast detection, M2 for accuracy), and includes human intervention for model updates.

Result: Achieves 0.999 classification accuracy, 0.866s prediction latency, 3.632MB memory usage, and 10.05% CPU utilization.

Conclusion: The MULTI-LF framework effectively addresses real-time DDoS detection challenges in M-En networks with high accuracy and efficiency.

Abstract: Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment
(M-En) networks presents significant challenges due to diverse malicious
traffic patterns and the evolving nature of cyber threats. Existing AI-based
detection systems struggle to adapt to new attack strategies and lack real-time
attack detection capabilities with high accuracy and efficiency. This study
proposes an online, continuous learning methodology for DDoS detection in M-En
networks, enabling continuous model updates and real-time adaptation to
emerging threats, including zero-day attacks. First, we develop a unique M-En
network dataset by setting up a realistic, real-time simulation using the NS-3
tool, incorporating both victim and bot devices. DDoS attacks with varying
packet sizes are simulated using the DDoSim application across IoT and
traditional IP-based environments under M-En network criteria. Our approach
employs a multi-level framework (MULTI-LF) featuring two machine learning
models: a lightweight Model 1 (M1) trained on a selective, critical packet
dataset for fast and efficient initial detection, and a more complex, highly
accurate Model 2 (M2) trained on extensive data. When M1 exhibits low
confidence in its predictions, the decision is escalated to M2 for verification
and potential fine-tuning of M1 using insights from M2. If both models
demonstrate low confidence, the system flags the incident for human
intervention, facilitating model updates with human-verified categories to
enhance adaptability to unseen attack patterns. We validate the MULTI-LF
through real-world simulations, demonstrating superior classification accuracy
of 0.999 and low prediction latency of 0.866 seconds compared to established
baselines. Furthermore, we evaluate performance in terms of memory usage (3.632
MB) and CPU utilization (10.05%) in real-time scenarios.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [196] [Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime](https://arxiv.org/abs/2504.12000)
*Thorben Markmann,Michiel Straat,Sebastian Peitz,Barbara Hammer*

Main category: physics.flu-dyn

TL;DR: RL outperforms PD control in reducing convective heat transfer in 2D RBC systems, showing strong generalization and improved efficiency with reward shaping.


<details>
  <summary>Details</summary>
Motivation: To explore RL's effectiveness in reducing convective heat transfer in turbulent systems, comparing it to classical control methods.

Method: Used single-agent PPO for RL, compared to linear PD controllers, with reward shaping to accelerate training.

Result: RL reduced convection by up to 33% (moderate turbulence) and 10% (high turbulence), outperforming PD control and generalizing well.

Conclusion: RL is effective for flow control in turbulent systems, with reward shaping enhancing training efficiency and performance.

Abstract: Data-driven flow control has significant potential for industry, energy
systems, and climate science. In this work, we study the effectiveness of
Reinforcement Learning (RL) for reducing convective heat transfer in the 2D
Rayleigh-B\'enard Convection (RBC) system under increasing turbulence. We
investigate the generalizability of control across varying initial conditions
and turbulence levels and introduce a reward shaping technique to accelerate
the training. RL agents trained via single-agent Proximal Policy Optimization
(PPO) are compared to linear proportional derivative (PD) controllers from
classical control theory. The RL agents reduced convection, measured by the
Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly
turbulent settings, clearly outperforming PD control in all settings. The
agents showed strong generalization performance across different initial
conditions and to a significant extent, generalized to higher degrees of
turbulence. The reward shaping improved sample efficiency and consistently
stabilized the Nusselt Number to higher turbulence levels.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [197] [GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization](https://arxiv.org/abs/2504.11840)
*Huizhe Zhang,Jintang Li,Yuchang Zhu,Liang Chen,Zibin Zheng*

Main category: cs.NE

TL;DR: A linear-time Graph Transformer using Spiking Vector Quantization (GT-SVQ) is proposed for node classification, achieving competitive performance with faster inference speed.


<details>
  <summary>Details</summary>
Motivation: Graph Transformers (GTs) face scalability issues due to quadratic complexity and high energy consumption. Spiking Neural Networks (SNNs) offer lower computational overhead, inspiring the integration of spiking neurons into GTs.

Method: GT-SVQ reconstructs codebooks from spiking neuron outputs and injects them into self-attention blocks, enabling linear complexity. It also addresses codebook collapse and reduces reliance on complex machinery.

Result: GT-SVQ achieves competitive performance on node classification tasks and is up to 130x faster in inference compared to other GTs.

Conclusion: GT-SVQ effectively combines the strengths of GTs and SNNs, offering a scalable and efficient solution for graph representation learning.

Abstract: Graph Transformers (GTs), which simultaneously integrate message-passing and
self-attention mechanisms, have achieved promising empirical results in some
graph prediction tasks. Although these approaches show the potential of
Transformers in capturing long-range graph topology information, issues
concerning the quadratic complexity and high computing energy consumption
severely limit the scalability of GTs on large-scale graphs. Recently, as
brain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the
development of graph representation learning methods with lower computational
and storage overhead through the unique event-driven spiking neurons. Inspired
by these characteristics, we propose a linear-time Graph Transformer using
Spiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ
reconstructs codebooks based on rate coding outputs from spiking neurons, and
injects the codebooks into self-attention blocks to aggregate global
information in linear complexity. Besides, spiking vector quantization
effectively alleviates codebook collapse and the reliance on complex machinery
(distance measure, auxiliary loss, etc.) present in previous vector
quantization-based graph learning methods. In experiments, we compare GT-SVQ
with other state-of-the-art baselines on node classification datasets ranging
from small to large. Experimental results show that GT-SVQ has achieved
competitive performances on most datasets while maintaining up to 130x faster
inference speed compared to other GTs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [198] [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
*Peter Stockinger*

Main category: cs.AI

TL;DR: The paper explores information design by separating semantic content from visual representation, using semantic modeling and multimodal visualization for practical applications like audiovisual data analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between semantic content and its visual/multimodal expression, leveraging structural semiotics and linguistics.

Method: Semantic modeling via conceptual networks/graphs, integrating thesauri or ontologies, and applying these in tools like OKAPI for data analysis and visualization.

Result: Enhanced interoperability and flexibility in communication systems, enabling richer use of digital data through visual storytelling and document reengineering.

Conclusion: The approach fosters collaborative and intelligent use of digital data, demonstrated in practical applications like audiovisual analysis and multimodal visualization.

Abstract: 1) Introduction and Conceptual Framework: This document explores the concept
of information design by dividing it into two major practices: defining the
meaning of a corpus of textual data and its visual or multimodal
representation. It draws on expertise in enriching textual corpora,
particularly audiovisual ones, and transforming them into multiple narrative
formats. The text highlights a crucial distinction between the semantic content
of a domain and the modalities of its graphic expression, illustrating this
approach with concepts rooted in structural semiotics and linguistics
traditions.
  2) Modeling and Conceptual Design: The article emphasizes the importance of
semantic modeling, often achieved through conceptual networks or graphs. These
tools enable the structuring of knowledge within a domain by accounting for
relationships between concepts, contexts of use, and specific objectives.
Stockinger also highlights the constraints and challenges involved in creating
dynamic and adaptable models, integrating elements such as thesauri or
interoperable ontologies to facilitate the analysis and publication of complex
corpora.
  3) Applications and Multimodal Visualization: The text concludes by examining
the practical application of these models in work environments like OKAPI,
developed to analyze, publish, and reuse audiovisual data. It also discusses
innovative approaches such as visual storytelling and document reengineering,
which involve transforming existing content into new resources tailored to
various contexts. These methods emphasize interoperability, flexibility, and
the intelligence of communication systems, paving the way for richer and more
collaborative use of digital data. The content of this document was presented
during the "Semiotics of Information Design" Day organized by Anne
Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on
June 21, 2018, in Bordeaux.

</details>


### [199] [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
*Haokun Liu,Sicong Huang,Jingyu Hu,Yangqiaoyu Zhou,Chenhao Tan*

Main category: cs.AI

TL;DR: HypoBench is a benchmark for evaluating LLMs in hypothesis generation, testing practical utility, generalizability, and discovery rate across real-world and synthetic tasks. Results show current methods find valid patterns but struggle with synthetic data, recovering only 38.8% of ground-truth hypotheses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation for hypothesis generation methods in LLMs and define what makes a good hypothesis.

Method: Introduces HypoBench, a benchmark with 7 real-world and 5 synthetic tasks (194 datasets), evaluating 4 LLMs and 6 hypothesis-generation methods.

Result: Existing methods discover valid patterns but perform poorly on synthetic data, recovering only 38.8% of ground-truth hypotheses as task difficulty increases.

Conclusion: HypoBench highlights challenges in hypothesis generation and serves as a resource for improving AI systems in scientific discovery.

Abstract: There is growing interest in hypothesis generation with large language models
(LLMs). However, fundamental questions remain: what makes a good hypothesis,
and how can we systematically evaluate methods for hypothesis generation? To
address this, we introduce HypoBench, a novel benchmark designed to evaluate
LLMs and hypothesis generation methods across multiple aspects, including
practical utility, generalizability, and hypothesis discovery rate. HypoBench
includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.
We evaluate four state-of-the-art LLMs combined with six existing
hypothesis-generation methods. Overall, our results suggest that existing
methods are capable of discovering valid and novel patterns in the data.
However, the results from synthetic datasets indicate that there is still
significant room for improvement, as current hypothesis generation methods do
not fully uncover all relevant or meaningful patterns. Specifically, in
synthetic settings, as task difficulty increases, performance significantly
drops, with best models and methods only recovering 38.8% of the ground-truth
hypotheses. These findings highlight challenges in hypothesis generation and
demonstrate that HypoBench serves as a valuable resource for improving AI
systems designed to assist scientific discovery.

</details>


### [200] [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
*Dayeon Ki,Tianyi Zhou,Marine Carpuat,Gang Wu,Puneet Mathur,Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: The paper introduces GraphicBench, a benchmark for evaluating LLM-powered agents in creative graphic design tasks, and GraphicTown, an agent framework. While LLMs can generate workflows, execution challenges remain.


<details>
  <summary>Details</summary>
Motivation: To explore LLM agents' capabilities in open-ended creative design tasks, which are underexplored compared to well-defined tasks.

Method: Developed GraphicBench (1,079 queries across four design types) and GraphicTown (a framework with three experts and 46 tools). Tested six LLMs for workflow generation and execution.

Result: LLMs can generate workflows integrating explicit and implicit constraints, but execution often fails due to spatial reasoning, global coordination, and action retrieval issues.

Conclusion: GraphicBench serves as a valuable testbed for advancing LLM-agent planning and execution in creative design, despite current challenges.

Abstract: Large Language Model (LLM)-powered agents have unlocked new possibilities for
automating human tasks. While prior work has focused on well-defined tasks with
specified goals, the capabilities of agents in creative design tasks with
open-ended goals remain underexplored. We introduce GraphicBench, a new
planning benchmark for graphic design that covers 1,079 user queries and input
images across four design types. We further present GraphicTown, an LLM agent
framework with three design experts and 46 actions (tools) to choose from for
executing each step of the planned workflows in web environments. Experiments
with six LLMs demonstrate their ability to generate workflows that integrate
both explicit design constraints from user queries and implicit commonsense
constraints. However, these workflows often do not lead to successful execution
outcomes, primarily due to challenges in: (1) reasoning about spatial
relationships, (2) coordinating global dependencies across experts, and (3)
retrieving the most appropriate action per step. We envision GraphicBench as a
challenging yet valuable testbed for advancing LLM-agent planning and execution
in creative design tasks.

</details>


### [201] [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
*Yiyou Sun,Georgia Zhou,Hao Wang,Dacheng Li,Nouha Dziri,Dawn Song*

Main category: cs.AI

TL;DR: The paper analyzes how supervised fine-tuning (SFT) enhances language models' mathematical reasoning, identifying a ladder-like difficulty structure in the AIME24 dataset and tier-specific requirements for progression.


<details>
  <summary>Details</summary>
Motivation: To understand the specific capabilities improved by SFT in mathematical reasoning tasks and how reasoning evolves across difficulty tiers.

Method: Detailed analysis of model performance on the AIME24 dataset, categorizing questions into four tiers (Easy, Medium, Hard, Exh) and examining progression requirements.

Result: Easy to Medium progression requires R1 reasoning with minimal SFT; Hard-tier accuracy plateaus at 65%; Exh-tier challenges remain unsolved. Dataset size scaling is more effective than curation.

Conclusion: The study offers a roadmap for advancing language models in mathematical reasoning, emphasizing dataset size over curation and highlighting unresolved challenges in higher tiers.

Abstract: Recent supervised fine-tuning (SFT) approaches have significantly improved
language models' performance on mathematical reasoning tasks, even when models
are trained at a small scale. However, the specific capabilities enhanced
through such fine-tuning remain poorly understood. In this paper, we conduct a
detailed analysis of model performance on the AIME24 dataset to understand how
reasoning capabilities evolve. We discover a ladder-like structure in problem
difficulty, categorize questions into four tiers (Easy, Medium, Hard, and
Extremely Hard (Exh)), and identify the specific requirements for advancing
between tiers. We find that progression from Easy to Medium tier requires
adopting an R1 reasoning style with minimal SFT (500-1K instances), while
Hard-level questions suffer from frequent model's errors at each step of the
reasoning chain, with accuracy plateauing at around 65% despite logarithmic
scaling. Exh-level questions present a fundamentally different challenge; they
require unconventional problem-solving skills that current models uniformly
struggle with. Additional findings reveal that carefully curated small-scale
datasets offer limited advantage-scaling dataset size proves far more
effective. Our analysis provides a clearer roadmap for advancing language model
capabilities in mathematical reasoning.

</details>


### [202] [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
*Tom Everitt,Cristina Garbacea,Alexis Bellot,Jonathan Richens,Henry Papadatos,Siméon Campos,Rohin Shah*

Main category: cs.AI

TL;DR: The paper evaluates goal-directedness in LLMs, finding it consistent across tasks but not fully achieved, and suggests its use for monitoring progress and design choices.


<details>
  <summary>Details</summary>
Motivation: To measure how effectively LLMs use their capabilities toward given goals, termed goal-directedness, to improve monitoring and design of agentic properties.

Method: Evaluated goal-directedness using tasks requiring information gathering, cognitive effort, and plan execution, with subtasks to infer capabilities. Tested models from Google DeepMind, OpenAI, and Anthropic.

Result: Goal-directedness is consistent across tasks, differs from task performance, and is moderately sensitive to motivational prompts. Most models are not fully goal-directed.

Conclusion: Goal-directedness evaluations can enhance LLM progress monitoring and inform deliberate design of agentic properties.

Abstract: To what extent do LLMs use their capabilities towards their given goal? We
take this as a measure of their goal-directedness. We evaluate
goal-directedness on tasks that require information gathering, cognitive
effort, and plan execution, where we use subtasks to infer each model's
relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,
and Anthropic show that goal-directedness is relatively consistent across
tasks, differs from task performance, and is only moderately sensitive to
motivational prompts. Notably, most models are not fully goal-directed. We hope
our goal-directedness evaluations will enable better monitoring of LLM
progress, and enable more deliberate design choices of agentic properties in
LLMs.

</details>


### [203] [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
*Nada Shahin,Leila Ismail*

Main category: cs.AI

TL;DR: The paper introduces ADAT, an Adaptive Transformer for sign language translation, addressing issues of fine-grained temporal dependency recognition and computational inefficiency. It outperforms baselines in accuracy and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Transformer-based sign language translation systems struggle with fine-grained temporal dependencies and high computational complexity, limiting their accuracy and efficiency.

Method: The proposed ADAT incorporates enhanced feature extraction and adaptive feature weighting via a gating mechanism to improve contextual relevance and reduce training overhead.

Result: ADAT improves BLEU-4 accuracy by 0.1% and reduces training time by 14.33% on PHOENIX14T, and achieves higher accuracy (8.7%) and faster training (2.8%) in sign-to-text tasks.

Conclusion: ADAT effectively balances accuracy and efficiency in sign language translation, demonstrating superior performance over existing baselines.

Abstract: Current sign language machine translation systems rely on recognizing hand
movements, facial expressions and body postures, and natural language
processing, to convert signs into text. Recent approaches use Transformer
architectures to model long-range dependencies via positional encoding.
However, they lack accuracy in recognizing fine-grained, short-range temporal
dependencies between gestures captured at high frame rates. Moreover, their
high computational complexity leads to inefficient training. To mitigate these
issues, we propose an Adaptive Transformer (ADAT), which incorporates
components for enhanced feature extraction and adaptive feature weighting
through a gating mechanism to emphasize contextually relevant features while
reducing training overhead and maintaining translation accuracy. To evaluate
ADAT, we introduce MedASL, the first public medical American Sign Language
dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the
encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing
training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text
experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on
PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on
MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,
ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its
dual-stream structure.

</details>


### [204] [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
*Mahmoud Salhab,Marwan Elghitany,Shameed Sait,Syed Sibghat Ullah,Mohammad Abusheikh,Hasan Abusheikh*

Main category: cs.AI

TL;DR: The paper presents a weakly supervised learning approach to train an Arabic ASR model using the Conformer architecture, achieving SOTA performance without costly manual transcriptions.


<details>
  <summary>Details</summary>
Motivation: Developing high-performance ASR models for low-resource languages like Arabic is challenging due to the scarcity of labeled datasets.

Method: The model is trained from scratch on 15,000 hours of weakly annotated speech data (MSA and DA) using the Conformer architecture.

Result: The approach achieves state-of-the-art performance on standard benchmarks despite no human-verified labels.

Conclusion: Weak supervision is a scalable, cost-efficient alternative to traditional supervised methods, enabling improved ASR in low-resource settings.

Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction
in diverse applications like conversational agents, industrial robotics, call
center automation, and automated subtitling. However, developing
high-performance ASR models remains challenging, particularly for low-resource
languages like Arabic, due to the scarcity of large, labeled speech datasets,
which are costly and labor-intensive to produce. In this work, we employ weakly
supervised learning to train an Arabic ASR model using the Conformer
architecture. Our model is trained from scratch on 15,000 hours of weakly
annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal
Arabic (DA), eliminating the need for costly manual transcriptions. Despite the
absence of human-verified labels, our approach attains state-of-the-art (SOTA)
performance, exceeding all previous efforts in the field of Arabic ASR on the
standard benchmarks. By demonstrating the effectiveness of weak supervision as
a scalable, cost-efficient alternative to traditional supervised approaches,
paving the way for improved ASR systems in low resource settings.

</details>


### [205] [Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation](https://arxiv.org/abs/2504.11671)
*Ji Ma*

Main category: cs.AI

TL;DR: The paper explores how character and context influence LLM behavior in social science settings, proposing methods to probe and modify internal representations in a Dictator Game.


<details>
  <summary>Details</summary>
Motivation: To understand and regulate how social concepts are encoded in LLMs, addressing gaps in behavior shaping for fairness and prosocial applications.

Method: Extracts and manipulates "vectors of variable variations" from the LLM's internal state to alter decision-making in a Dictator Game.

Result: Demonstrates that manipulating these vectors can significantly change how variables like gender influence the model's behavior.

Conclusion: Provides a principled approach to studying and engineering social concepts in LLMs, with implications for alignment, debiasing, and social simulations.

Abstract: Large language models (LLMs) increasingly serve as human-like decision-making
agents in social science and applied settings. These LLM-agents are typically
assigned human-like characters and placed in real-life contexts. However, how
these characters and contexts shape an LLM's behavior remains underexplored.
This study proposes and tests methods for probing, quantifying, and modifying
an LLM's internal representations in a Dictator Game -- a classic behavioral
experiment on fairness and prosocial behavior. We extract ``vectors of variable
variations'' (e.g., ``male'' to ``female'') from the LLM's internal state.
Manipulating these vectors during the model's inference can substantially alter
how those variables relate to the model's decision-making. This approach offers
a principled way to study and regulate how social concepts can be encoded and
engineered within transformer-based models, with implications for alignment,
debiasing, and designing AI agents for social simulations in both academic and
commercial applications.

</details>


### [206] [Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records](https://arxiv.org/abs/2504.11792)
*Md Sultan Al Nahian,Chris Delcher,Daniel Harris,Peter Akpunonu,Ramakanth Kavuluru*

Main category: cs.AI

TL;DR: GPT-4o LLM outperforms traditional ML models in predicting drug overdose risk from insurance claims, even in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs' ability to process long textual data and prior knowledge for improving drug overdose risk prediction.

Method: Evaluate GPT-4o's performance in fine-tuned and zero-shot settings using longitudinal insurance claims records, comparing it to traditional ML models.

Result: LLMs outperform traditional models in some settings and can predict risk without task-specific training.

Conclusion: LLMs show promise for clinical decision support in drug overdose risk prediction.

Abstract: The ability to predict drug overdose risk from a patient's medical records is
crucial for timely intervention and prevention. Traditional machine learning
models have shown promise in analyzing longitudinal medical records for this
task. However, recent advancements in large language models (LLMs) offer an
opportunity to enhance prediction performance by leveraging their ability to
process long textual data and their inherent prior knowledge across diverse
tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in
predicting drug overdose events using patients' longitudinal insurance claims
records. We evaluate its performance in both fine-tuned and zero-shot settings,
comparing them to strong traditional machine learning methods as baselines. Our
results show that LLMs not only outperform traditional models in certain
settings but can also predict overdose risk in a zero-shot setting without
task-specific training. These findings highlight the potential of LLMs in
clinical decision support, particularly for drug overdose risk prediction.

</details>


### [207] [Adapting a World Model for Trajectory Following in a 3D Game](https://arxiv.org/abs/2504.12299)
*Marko Tot,Shu Ishida,Abdelhak Lemkhenter,David Bignell,Pallavi Choudhury,Chris Lovett,Luis França,Matheus Ribeiro Furtado de Mendonça,Tarun Gupta,Darren Gehring,Sam Devlin,Sergio Valcarcel Macua,Raluca Georgescu*

Main category: cs.AI

TL;DR: The paper explores using Inverse Dynamics Models (IDMs) with various encoders and policy heads for trajectory following in a 3D game, addressing distribution shift and stochasticity. It evaluates performance in diverse and low-data settings.


<details>
  <summary>Details</summary>
Motivation: To improve imitation learning in complex environments like 3D games by addressing distribution shift and stochasticity, ensuring robust trajectory replication.

Method: Applied IDMs with different encoders (e.g., DINOv2) and policy heads (GPT-style, MLP-style) in the game Bleeding Edge. Investigated future alignment strategies for mitigating aleatoric uncertainty.

Result: GPT-style policy head with a scratch-trained encoder excelled in diverse data; DINOv2 with GPT-style performed best in low-data settings; both heads were comparable when pre-trained and fine-tuned.

Conclusion: The optimal configuration for trajectory following depends on the data setting, with GPT-style heads showing versatility across scenarios.

Abstract: Imitation learning is a powerful tool for training agents by leveraging
expert knowledge, and being able to replicate a given trajectory is an integral
part of it. In complex environments, like modern 3D video games, distribution
shift and stochasticity necessitate robust approaches beyond simple action
replay. In this study, we apply Inverse Dynamics Models (IDM) with different
encoders and policy heads to trajectory following in a modern 3D video game --
Bleeding Edge. Additionally, we investigate several future alignment strategies
that address the distribution shift caused by the aleatoric uncertainty and
imperfections of the agent. We measure both the trajectory deviation distance
and the first significant deviation point between the reference and the agent's
trajectory and show that the optimal configuration depends on the chosen
setting. Our results show that in a diverse data setting, a GPT-style policy
head with an encoder trained from scratch performs the best, DINOv2 encoder
with the GPT-style policy head gives the best results in the low data regime,
and both GPT-style and MLP-style policy heads had comparable results when
pre-trained on a diverse setting and fine-tuned for a specific behaviour
setting.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [208] [FEAT: Free energy Estimators with Adaptive Transport](https://arxiv.org/abs/2504.11516)
*Jiajun He,Yuanqi Du,Francisco Vargas,Yuanqing Wang,Carla P. Gomes,José Miguel Hernández-Lobato,Eric Vanden-Eijnden*

Main category: stat.ML

TL;DR: FEAT is a new framework for free energy estimation using learned transports and stochastic interpolants, offering consistent, minimum-variance estimators and unifying equilibrium/non-equilibrium methods.


<details>
  <summary>Details</summary>
Motivation: Free energy estimation is a critical challenge in scientific domains, requiring improved accuracy and theoretical foundations.

Method: FEAT uses learned transports via stochastic interpolants, leveraging escorted Jarzynski equality and controlled Crooks theorem, with variational bounds.

Result: FEAT outperforms existing learning-based methods in toy examples, molecular simulations, and quantum field theory.

Conclusion: FEAT provides a principled, unified framework for neural free energy calculations, validated by experimental results.

Abstract: We present Free energy Estimators with Adaptive Transport (FEAT), a novel
framework for free energy estimation -- a critical challenge across scientific
domains. FEAT leverages learned transports implemented via stochastic
interpolants and provides consistent, minimum-variance estimators based on
escorted Jarzynski equality and controlled Crooks theorem, alongside
variational upper and lower bounds on free energy differences. Unifying
equilibrium and non-equilibrium methods under a single theoretical framework,
FEAT establishes a principled foundation for neural free energy calculations.
Experimental validation on toy examples, molecular simulations, and quantum
field theory demonstrates improvements over existing learning-based methods.

</details>


### [209] [Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations](https://arxiv.org/abs/2504.11554)
*Chengkun Li,Bobby Huggins,Petrus Mikkola,Luigi Acerbi*

Main category: stat.ML

TL;DR: NFR is a novel offline Bayesian inference method using normalizing flow regression to approximate posteriors without additional sampling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference with expensive likelihood evaluations is challenging; NFR aims to simplify this by leveraging existing log-density evaluations.

Method: NFR uses normalizing flow regression, tailored priors, and likelihood functions for robust posterior and evidence estimation.

Result: NFR shows superior or comparable performance on synthetic benchmarks and real-world neuroscience and biology applications.

Conclusion: NFR is a promising solution for computationally expensive Bayesian inference, especially when reusing model evaluations.

Abstract: Bayesian inference with computationally expensive likelihood evaluations
remains a significant challenge in many scientific domains. We propose
normalizing flow regression (NFR), a novel offline inference method for
approximating posterior distributions. Unlike traditional surrogate approaches
that require additional sampling or inference steps, NFR directly yields a
tractable posterior approximation through regression on existing log-density
evaluations. We introduce training techniques specifically for flow regression,
such as tailored priors and likelihood functions, to achieve robust posterior
and model evidence estimation. We demonstrate NFR's effectiveness on synthetic
benchmarks and real-world applications from neuroscience and biology, showing
superior or comparable performance to existing methods. NFR represents a
promising approach for Bayesian inference when standard methods are
computationally prohibitive or existing model evaluations can be recycled.

</details>


### [210] [Towards Interpretable Deep Generative Models via Causal Representation Learning](https://arxiv.org/abs/2504.11609)
*Gemma E. Moran,Bryon Aragam*

Main category: stat.ML

TL;DR: The paper reviews causal representation learning (CRL), an emerging field aiming to create interpretable and transferable generative AI by integrating causality into neural networks. It connects CRL to classical statistical models and discusses its applications, methods, and open questions.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks' black-box nature obscures learned representations, making them hard to interpret. CRL addresses this by building interpretable models using causality.

Method: CRL combines latent variable models, causal graphical models, and deep learning. The paper reviews progress, focusing on statistical connections, identifiability, and implementation strategies.

Result: CRL offers a framework for flexible, interpretable, and transferable generative AI, bridging classical statistics and modern deep learning.

Conclusion: CRL is a promising direction for interpretable AI, but open statistical questions remain. The paper highlights its potential and challenges.

Abstract: Recent developments in generative artificial intelligence (AI) rely on
machine learning techniques such as deep learning and generative modeling to
achieve state-of-the-art performance across wide-ranging domains. These
methods' surprising performance is due in part to their ability to learn
implicit "representations'' of complex, multi-modal data. Unfortunately, deep
neural networks are notoriously black boxes that obscure these representations,
making them difficult to interpret or analyze. To resolve these difficulties,
one approach is to build new interpretable neural network models from the
ground up. This is the goal of the emerging field of causal representation
learning (CRL) that uses causality as a vector for building flexible,
interpretable, and transferable generative AI. CRL can be seen as a culmination
of three intrinsically statistical problems: (i) latent variable models such as
factor analysis; (ii) causal graphical models with latent variables; and (iii)
nonparametric statistics and deep learning. This paper reviews recent progress
in CRL from a statistical perspective, focusing on connections to classical
models and statistical and causal identifiablity results. This review also
highlights key application areas, implementation strategies, and open
statistical questions in CRL.

</details>


### [211] [Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations](https://arxiv.org/abs/2504.11610)
*Tianjian Yang,Wei Vivian Li*

Main category: stat.ML

TL;DR: GPCCA is an unsupervised method for integrating multi-modal data, handling missing values, and improving clustering accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for models to integrate diverse data modalities and handle missing data in bioinformatics and other domains.

Method: Generalized Probabilistic Canonical Correlation Analysis (GPCCA), which integrates multi-modal data, handles missing values, and reduces dimensionality.

Result: GPCCA outperforms existing methods in simulations and works well with cancer genomics and multi-view image data.

Conclusion: GPCCA is robust and broadly applicable, with an available R package for wider use.

Abstract: Background: The integration and analysis of multi-modal data are increasingly
essential across various domains including bioinformatics. As the volume and
complexity of such data grow, there is a pressing need for computational models
that not only integrate diverse modalities but also leverage their
complementary information to improve clustering accuracy and insights,
especially when dealing with partial observations with missing data. Results:
We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an
unsupervised method for the integration and joint dimensionality reduction of
multi-modal data. GPCCA addresses key challenges in multi-modal data analysis
by handling missing values within the model, enabling the integration of more
than two modalities, and identifying informative features while accounting for
correlations within individual modalities. The model demonstrates robustness to
various missing data patterns and provides low-dimensional embeddings that
facilitate downstream clustering and analysis. In a range of simulation
settings, GPCCA outperforms existing methods in capturing essential patterns
across modalities. Additionally, we demonstrate its applicability to
multi-omics data from TCGA cancer datasets and a multi-view image dataset.
Conclusion: GPCCA offers a useful framework for multi-modal data integration,
effectively handling missing data and providing informative low-dimensional
embeddings. Its performance across cancer genomics and multi-view image data
highlights its robustness and potential for broad application. To make the
method accessible to the wider research community, we have released an R
package, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.

</details>


### [212] [Discrimination-free Insurance Pricing with Privatized Sensitive Attributes](https://arxiv.org/abs/2504.11775)
*Tianhe Zhang,Suhan Liu,Peng Shi*

Main category: stat.ML

TL;DR: The paper addresses fairness in machine learning, focusing on insurance pricing, and proposes a method to ensure fairness without direct access to sensitive attributes.


<details>
  <summary>Details</summary>
Motivation: Fairness in AI is critical, but insurance pricing has unique fairness challenges due to regulatory constraints and specialized definitions of fairness.

Method: Proposes an efficient method for fair model construction in insurance, using privatized sensitive attributes and ensuring statistical guarantees.

Result: The method adapts to transparency requirements, complies with regulations, and ensures fairness without direct access to sensitive data.

Conclusion: The approach effectively balances fairness and regulatory demands in insurance pricing.

Abstract: Fairness has emerged as a critical consideration in the landscape of machine
learning algorithms, particularly as AI continues to transform decision-making
across societal domains. To ensure that these algorithms are free from bias and
do not discriminate against individuals based on sensitive attributes such as
gender and race, the field of algorithmic bias has introduced various fairness
concepts, along with methodologies to achieve these notions in different
contexts. Despite the rapid advancement, not all sectors have embraced these
fairness principles to the same extent. One specific sector that merits
attention in this regard is insurance. Within the realm of insurance pricing,
fairness is defined through a distinct and specialized framework. Consequently,
achieving fairness according to established notions does not automatically
ensure fair pricing in insurance. In particular, regulators are increasingly
emphasizing transparency in pricing algorithms and imposing constraints on
insurance companies on the collection and utilization of sensitive consumer
attributes. These factors present additional challenges in the implementation
of fairness in pricing algorithms. To address these complexities and comply
with regulatory demands, we propose an efficient method for constructing fair
models that are tailored to the insurance domain, using only privatized
sensitive attributes. Notably, our approach ensures statistical guarantees,
does not require direct access to sensitive attributes, and adapts to varying
transparency requirements, addressing regulatory demands while ensuring
fairness in insurance pricing.

</details>


### [213] [Approximation Bounds for Transformer Networks with Application to Regression](https://arxiv.org/abs/2504.12175)
*Yuling Jiao,Yanming Lai,Defeng Sun,Yang Wang,Bokai Yan*

Main category: stat.ML

TL;DR: The paper analyzes Transformer networks' approximation capabilities for Hölder and Sobolev functions, extends results to nonparametric regression with dependent data, and introduces a novel proof strategy inspired by the Kolmogorov-Arnold theorem.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the approximation power of Transformers for sequence-to-sequence mappings with Hölder and Sobolev functions, and apply this to nonparametric regression with dependent observations.

Method: Establishes upper bounds for Transformers approximating Hölder and Sobolev functions, derives convergence rates for nonparametric regression under β-mixing data, and proposes a proof strategy using column averaging in self-attention.

Result: Transformers achieve approximation error ε with parameter scaling ε^(-d_x n / γ), matching FNNs and RNNs. Explicit convergence rates are derived for regression, and self-attention's interpretability is enhanced.

Conclusion: Transformers are effective for approximating Hölder and Sobolev functions, with applications in nonparametric regression. The proof strategy offers new insights into self-attention mechanisms.

Abstract: We explore the approximation capabilities of Transformer networks for
H\"older and Sobolev functions, and apply these results to address
nonparametric regression estimation with dependent observations. First, we
establish novel upper bounds for standard Transformer networks approximating
sequence-to-sequence mappings whose component functions are H\"older continuous
with smoothness index $\gamma \in (0,1]$. To achieve an approximation error
$\varepsilon$ under the $L^p$-norm for $p \in [1, \infty]$, it suffices to use
a fixed-depth Transformer network whose total number of parameters scales as
$\varepsilon^{-d_x n / \gamma}$. This result not only extends existing findings
to include the case $p = \infty$, but also matches the best known upper bounds
on number of parameters previously obtained for fixed-depth FNNs and RNNs.
Similar bounds are also derived for Sobolev functions. Second, we derive
explicit convergence rates for the nonparametric regression problem under
various $\beta$-mixing data assumptions, which allow the dependence between
observations to weaken over time. Our bounds on the sample complexity impose no
constraints on weight magnitudes. Lastly, we propose a novel proof strategy to
establish approximation bounds, inspired by the Kolmogorov-Arnold
representation theorem. We show that if the self-attention layer in a
Transformer can perform column averaging, the network can approximate
sequence-to-sequence H\"older functions, offering new insights into the
interpretability of self-attention mechanisms.

</details>


### [214] [Leave-One-Out Stable Conformal Prediction](https://arxiv.org/abs/2504.12189)
*Kiljae Lee,Yuan Zhang*

Main category: stat.ML

TL;DR: LOO-StabCP speeds up full conformal prediction using leave-one-out stability, outperforming RO-StabCP in efficiency and accuracy for multiple predictions.


<details>
  <summary>Details</summary>
Motivation: Balancing computational efficiency and prediction accuracy in conformal prediction, especially for multiple predictions, is challenging.

Method: Proposes Leave-One-Out Stable Conformal Prediction (LOO-StabCP), leveraging leave-one-out stability to avoid sample splitting and improve speed.

Result: Theoretical stability bounds for RLM, SGD, kernel methods, neural networks, and bagging. Superior numerical performance on synthetic and real-world data.

Conclusion: LOO-StabCP improves test power in screening problems by better utilizing training data compared to split conformal methods.

Abstract: Conformal prediction (CP) is an important tool for distribution-free
predictive uncertainty quantification. Yet, a major challenge is to balance
computational efficiency and prediction accuracy, particularly for multiple
predictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),
a novel method to speed up full conformal using algorithmic stability without
sample splitting. By leveraging leave-one-out stability, our method is much
faster in handling a large number of prediction requests compared to existing
method RO-StabCP based on replace-one stability. We derived stability bounds
for several popular machine learning tools: regularized loss minimization (RLM)
and stochastic gradient descent (SGD), as well as kernel method, neural
networks and bagging. Our method is theoretically justified and demonstrates
superior numerical performance on synthetic and real-world data. We applied our
method to a screening problem, where its effective exploitation of training
data led to improved test power compared to state-of-the-art method based on
split conformal.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [215] [FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry](https://arxiv.org/abs/2504.11519)
*Mohammad Farahmand,Amoon Jamzad,Fahimeh Fooladgar,Laura Connolly,Martin Kaufmann,Kevin Yi Mi Ren,John Rudan,Doug McKay,Gabor Fichtinger,Parvin Mousavi*

Main category: physics.med-ph

TL;DR: The paper introduces FACT, a foundation model for classifying cancer tissue margins using REIMS data, achieving state-of-the-art performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Accurate real-time margin assessment during cancer surgeries is critical, but labeled data scarcity in surgical contexts poses challenges.

Method: FACT, a foundation model adapted from text-audio association, is pretrained using supervised contrastive learning with triplet loss, outperforming baselines.

Result: FACT achieves an AUROC of 82.4% ± 0.8, demonstrating superior performance over self-supervised and semi-supervised alternatives.

Conclusion: Foundation models like FACT, pretrained with novel methods, can effectively classify REIMS data with limited labels, enhancing surgical margin assessment.

Abstract: Purpose: Accurately classifying tissue margins during cancer surgeries is
crucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass
Spectrometry (REIMS), a tool for real-time intraoperative margin assessment,
generates spectra that require machine learning models to support clinical
decision-making. However, the scarcity of labeled data in surgical contexts
presents a significant challenge. This study is the first to develop a
foundation model tailored specifically for REIMS data, addressing this
limitation and advancing real-time surgical margin assessment. Methods: We
propose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is
an adaptation of a foundation model originally designed for text-audio
association, pretrained using our proposed supervised contrastive approach
based on triplet loss. An ablation study is performed to compare our proposed
model against other models and pretraining methods. Results: Our proposed model
significantly improves the classification performance, achieving
state-of-the-art performance with an AUROC of $82.4\% \pm 0.8$. The results
demonstrate the advantage of our proposed pretraining method and selected
backbone over the self-supervised and semi-supervised baselines and alternative
models. Conclusion: Our findings demonstrate that foundation models, adapted
and pretrained using our novel approach, can effectively classify REIMS data
even with limited labeled examples. This highlights the viability of foundation
models for enhancing real-time surgical margin assessment, particularly in
data-scarce clinical environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [216] [Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids](https://arxiv.org/abs/2504.11650)
*Shengyuan Yan,Farzad Vazinram,Zeynab Kaseb,Lindsay Spoor,Jochen Stiasny,Betul Mamudi,Amirhossein Heydarian Ardakani,Ugochukwu Orji,Pedro P. Vergara,Yu Xiang,Jerry Guo*

Main category: eess.SY

TL;DR: The paper proposes three strategies to improve Newton-Raphson initialization for power flow calculations, addressing convergence challenges in modern power grids.


<details>
  <summary>Details</summary>
Motivation: Power grids operating near capacity limits face ill-conditioned cases and convergence issues, necessitating robust solutions for stable grid operation.

Method: Three approaches: (i) analytical bounds on voltages, (ii) data-driven models (supervised learning/PINNs), and (iii) reinforcement learning for incremental voltage adjustments.

Result: All methods effectively reduce iterations and avoid divergence in benchmark systems.

Conclusion: The findings enable more efficient real-time grid operations, supporting smarter and resilient electricity networks.

Abstract: Power flow (PF) calculations are fundamental to power system analysis to
ensure stable and reliable grid operation. The Newton-Raphson (NR) method is
commonly used for PF analysis due to its rapid convergence when initialized
properly. However, as power grids operate closer to their capacity limits,
ill-conditioned cases and convergence issues pose significant challenges. This
work, therefore, addresses these challenges by proposing strategies to improve
NR initialization, hence minimizing iterations and avoiding divergence. We
explore three approaches: (i) an analytical method that estimates the basin of
attraction using mathematical bounds on voltages, (ii) Two data-driven models
leveraging supervised learning or physics-informed neural networks (PINNs) to
predict optimal initial guesses, and (iii) a reinforcement learning (RL)
approach that incrementally adjusts voltages to accelerate convergence. These
methods are tested on benchmark systems. This research is particularly relevant
for modern power systems, where high penetration of renewables and
decentralized generation require robust and scalable PF solutions. In
experiments, all three proposed methods demonstrate a strong ability to provide
an initial guess for Newton-Raphson method to converge with fewer steps. The
findings provide a pathway for more efficient real-time grid operations, which,
in turn, support the transition toward smarter and more resilient electricity
networks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [217] [Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations](https://arxiv.org/abs/2504.11555)
*Yahya Sattar,Sunmook Choi,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: math.OC

TL;DR: The paper explores controlling a linear dynamical system with bilinear observations, revealing challenges like non-convex cost-to-go and non-affine optimal controllers, unlike standard LQG control.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the complexities and deviations from standard LQG control when observations are bilinear, highlighting the lack of Separation Principle and non-affine optimal controllers.

Method: The paper analyzes the problem theoretically, derives conditions for bounded Kalman filter covariance, and validates findings through numerical experiments in synthetic settings.

Result: Results show the standard LQG controller can locally maximize cost, optimal controllers are nonlinear and non-unique, and input-dependent observability plays a key role.

Conclusion: The study concludes that bilinear observations introduce significant challenges in control design, requiring nonlinear and non-unique solutions, with implications for practical applications.

Abstract: We consider the problem of controlling a linear dynamical system from
bilinear observations with minimal quadratic cost. Despite the similarity of
this problem to standard linear quadratic Gaussian (LQG) control, we show that
when the observation model is bilinear, neither does the Separation Principle
hold, nor is the optimal controller affine in the estimated state. Moreover,
the cost-to-go is non-convex in the control input. Hence, finding an analytical
expression for the optimal feedback controller is difficult in general. Under
certain settings, we show that the standard LQG controller locally maximizes
the cost instead of minimizing it. Furthermore, the optimal controllers
(derived analytically) are not unique and are nonlinear in the estimated state.
We also introduce a notion of input-dependent observability and derive
conditions under which the Kalman filter covariance remains bounded. We
illustrate our theoretical results through numerical experiments in multiple
synthetic settings.

</details>


### [218] [Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events](https://arxiv.org/abs/2504.11570)
*Haozhe Lei,Ya-Ting Yang,Tao Li,Zilin Bian,Fan Zuo,Sundeep Rangan,Kaan Ozbay*

Main category: math.OC

TL;DR: TAMPA is an adaptive patrolling algorithm for real-time incident management during major events, outperforming stationary and random methods by 87.5% and 114.2%, respectively.


<details>
  <summary>Details</summary>
Motivation: Major events stress transportation networks, requiring efficient and adaptive patrol solutions.

Method: TAMPA integrates predictive traffic modeling and real-time complaint estimation, using dynamic programming and the Dvoretzky-Kiefer-Wolfowitz inequality for adaptive patrol route adjustments.

Result: Simulations show TAMPA improves performance by 87.5% over stationary methods and 114.2% over random strategies.

Conclusion: TAMPA is effective for real-time incident management, with future enhancements planned for adaptability and predictive accuracy using digital twin technology.

Abstract: This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm
(TAMPA), designed to improve real-time incident management during major events
like sports tournaments and concerts. Such events significantly stress
transportation networks, requiring efficient and adaptive patrol solutions.
TAMPA integrates predictive traffic modeling and real-time complaint
estimation, dynamically optimizing patrol deployment. Using dynamic
programming, the algorithm continuously adjusts patrol strategies within short
planning windows, effectively balancing immediate response and efficient
routing. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects
significant shifts in complaint patterns, triggering proactive adjustments in
patrol routes. Theoretical analyses ensure performance remains closely aligned
with optimal solutions. Simulation results from an urban traffic network
demonstrate TAMPA's superior performance, showing improvements of approximately
87.5\% over stationary methods and 114.2\% over random strategies. Future work
includes enhancing adaptability and incorporating digital twin technology for
improved predictive accuracy, particularly relevant for events like the 2026
FIFA World Cup at MetLife Stadium.

</details>


### [219] [Efficient identification of linear, parameter-varying, and nonlinear systems with noise models](https://arxiv.org/abs/2504.11982)
*Alberto Bemporad,Roland Tóth*

Main category: math.OC

TL;DR: A system identification procedure for estimating diverse state-space dynamical models (LTI, LPV, NL) with general noise models, using ANNs for parameterization and achieving fast, accurate training.


<details>
  <summary>Details</summary>
Motivation: To address the need for a versatile and efficient method for identifying a wide range of dynamical models, including nonlinear and noise-affected systems.

Method: Parameterizes nonlinear functions with ANNs, optimizes a prediction-error criterion using a constrained quasi-Newton approach and automatic differentiation.

Result: Achieves faster training (seconds vs. hours) and superior accuracy compared to state-of-the-art ANN methods.

Conclusion: The proposed method is consistent, efficient, and accurate for LTI, LPV, and NL system identification.

Abstract: We present a general system identification procedure capable of estimating of
a broad spectrum of state-space dynamical models, including linear
time-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)
dynamics, along with rather general classes of noise models. Similar to the LTI
case, we show that for this general class of model structures, including the NL
case, the model dynamics can be separated into a deterministic process and a
stochastic noise part, allowing to seamlessly tune the complexity of the
combined model both in terms of nonlinearity and noise modeling. We
parameterize the involved nonlinear functional relations by means of artificial
neural-networks (ANNs), although alternative parametric nonlinear mappings can
also be used. To estimate the resulting model structures, we optimize a
prediction-error-based criterion using an efficient combination of a
constrained quasi-Newton approach and automatic differentiation, achieving
training times in the order of seconds compared to existing state-of-the-art
ANN methods which may require hours for models of similar complexity. We
formally establish the consistency guarantees for the proposed approach and
demonstrate its superior estimation accuracy and computational efficiency on
several benchmark LTI, LPV, and NL system identification problems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [220] [Recent Advance in 3D Object and Scene Generation: A Survey](https://arxiv.org/abs/2504.11734)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: A survey on 3D content generation, covering object and scene creation methods, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of manual 3D modeling by leveraging AI and novel 3D representations to meet growing industry demands.

Method: Systematic review of static 3D object and scene generation, categorizing techniques like supervised learning, generative models, layout-guided synthesis, and rule-driven modeling.

Result: Comprehensive framework for 3D generation technologies, highlighting advancements and persistent challenges.

Conclusion: Provides structured insights into current 3D generation methods and suggests future research directions to inspire further exploration.

Abstract: In recent years, the demand for 3D content has grown exponentially with
intelligent upgrading of interactive media, extended reality (XR), and
Metaverse industries. In order to overcome the limitation of traditional manual
modeling approaches, such as labor-intensive workflows and prolonged production
cycles, revolutionary advances have been achieved through the convergence of
novel 3D representation paradigms and artificial intelligence generative
technologies. In this survey, we conduct a systematically review of the
cutting-edge achievements in static 3D object and scene generation, as well as
establish a comprehensive technical framework through systematic
categorization. Specifically, we initiate our analysis with mainstream 3D
object representations, followed by in-depth exploration of two principal
technical pathways in object generation: data-driven supervised learning
methods and deep generative model-based approaches. Regarding scene generation,
we focus on three dominant paradigms: layout-guided compositional synthesis, 2D
prior-based scene generation, and rule-driven modeling. Finally, we critically
examine persistent challenges in 3D generation and propose potential research
directions for future investigation. This survey aims to provide readers with a
structured understanding of state-of-the-art 3D generation technologies while
inspiring researchers to undertake more exploration in this domain.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [221] [Robust Markov stability for community detection at a scale learned based on the structure](https://arxiv.org/abs/2504.11621)
*Samin Aref,Sanchaai Mathiyarasan*

Main category: cs.SI

TL;DR: The paper introduces PyGenStabilityOne (PO), a hyperparameter-free multi-scale community detection algorithm that automatically selects a robust partition at a suitable scale using a pre-trained ML model.


<details>
  <summary>Details</summary>
Motivation: Single-scale community detection methods often fail to produce robust and suitable partitions. Existing multi-scale methods like PyGenStability lack a principled way to select the best partition.

Method: Combines Markov stability with a pre-trained gradient boosting model for scale selection, trained on 10k benchmark networks.

Result: PO outperforms 25 out of 29 algorithms in performance comparisons, offering accuracy and robustness without user input.

Conclusion: PO is a superior, hyperparameter-free method for community detection, eliminating the need for manual scale selection.

Abstract: Community detection, the unsupervised task of clustering nodes of a graph,
finds applications across various fields. The common approaches for community
detection involve optimizing an objective function to partition the nodes into
communities at a single scale of granularity. However, the single-scale
approaches often fall short of producing partitions that are robust and at a
suitable scale. The existing algorithm, PyGenStability, returns multiple robust
partitions for a network by optimizing the multi-scale Markov stability
function. However, in cases where the suitable scale is not known or assumed by
the user, there is no principled method to select a single robust partition at
a suitable scale from the multiple partitions that PyGenStability produces. Our
proposed method combines the Markov stability framework with a pre-trained
machine learning model for scale selection to obtain one robust partition at a
scale that is learned based on the graph structure. This automatic scale
selection involves using a gradient boosting model pre-trained on hand-crafted
and embedding-based network features from a labeled dataset of 10k benchmark
networks. This model was trained to predicts the scale value that maximizes the
similarity of the output partition to the planted partition of the benchmark
network. Combining our scale selection algorithm with the PyGenStability
algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale
community detection algorithm that returns one robust partition at a suitable
scale without the need for any assumptions, input, or tweaking from the user.
We compare the performance of PO against 29 algorithms and show that it
outperforms 25 other algorithms by statistically meaningful margins. Our
results facilitate choosing between community detection algorithms, among which
PO stands out as the accurate, robust, and hyperparameter-free method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [222] [Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets](https://arxiv.org/abs/2504.11504)
*Woojin Kim,Hyeoncheol Kim*

Main category: cs.CY

TL;DR: The paper explores counterfactual fairness in educational machine learning models, addressing algorithmic bias and its causal impacts on students.


<details>
  <summary>Details</summary>
Motivation: Algorithmic bias in educational settings raises fairness concerns, especially regarding individual fairness in causal contexts like counterfactual fairness.

Method: Counterfactual fairness analysis is conducted on benchmark educational datasets using machine learning models.

Result: The study shows that counterfactual fairness offers insights into the causality of sensitive attributes and individual fairness in education.

Conclusion: Counterfactual fairness is a valuable approach for understanding and addressing algorithmic bias in educational data.

Abstract: As machine learning models are increasingly used in educational settings,
from detecting at-risk students to predicting student performance, algorithmic
bias and its potential impacts on students raise critical concerns about
algorithmic fairness. Although group fairness is widely explored in education,
works on individual fairness in a causal context are understudied, especially
on counterfactual fairness. This paper explores the notion of counterfactual
fairness for educational data by conducting counterfactual fairness analysis of
machine learning models on benchmark educational datasets. We demonstrate that
counterfactual fairness provides meaningful insight into the causality of
sensitive attributes and causal-based individual fairness in education.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [223] [Strengthening Anomaly Awareness](https://arxiv.org/abs/2504.11520)
*Adam Banda,Charanjit K. Khosa,Veronica Sanz*

Main category: hep-ph

TL;DR: A refined Anomaly Awareness framework improves unsupervised anomaly detection by adding minimal supervision to VAEs through a two-stage training process, enhancing sensitivity to unseen anomalies.


<details>
  <summary>Details</summary>
Motivation: To enhance unsupervised anomaly detection by incorporating limited labeled anomaly data to improve model generalization and performance.

Method: A two-stage training strategy: unsupervised training on background data followed by fine-tuning with labeled anomalies to increase reconstruction errors for anomalies.

Result: Improved sensitivity to unseen anomalies across diverse datasets (MNIST, CICIDS, LHCO2020, SMEFT), achieving better separation between normal and anomalous samples.

Conclusion: Targeted fine-tuning with limited anomaly information significantly boosts the performance and generalization of unsupervised anomaly detection models.

Abstract: We present a refined version of the Anomaly Awareness framework for enhancing
unsupervised anomaly detection. Our approach introduces minimal supervision
into Variational Autoencoders (VAEs) through a two-stage training strategy: the
model is first trained in an unsupervised manner on background data, and then
fine-tuned using a small sample of labeled anomalies to encourage larger
reconstruction errors for anomalous samples.
  We validate the method across diverse domains, including the MNIST dataset
with synthetic anomalies, network intrusion data from the CICIDS benchmark,
collider physics data from the LHCO2020 dataset, and simulated events from the
Standard Model Effective Field Theory (SMEFT). The latter provides a realistic
example of subtle kinematic deviations in Higgs boson production. In all cases,
the model demonstrates improved sensitivity to unseen anomalies, achieving
better separation between normal and anomalous samples. These results indicate
that even limited anomaly information, when incorporated through targeted
fine-tuning, can substantially improve the generalization and performance of
unsupervised models for anomaly detection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [224] [Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery](https://arxiv.org/abs/2504.11495)
*Yiting Wang,Yunxin Fan,Fei Liu*

Main category: cs.RO

TL;DR: A framework combining sparse keypoint tracking and probabilistic modeling improves tool-tissue interaction modeling in robotic surgery by integrating expert annotations and dynamic tissue deformations.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for modeling tool-tissue interactions rely on rigid assumptions or labor-intensive annotations, limiting flexibility and accuracy.

Method: The proposed framework uses sparse keypoint tracking, PCA for dynamic local transformations, and a Task-Parameterized Gaussian Mixture Model (TP-GMM) to integrate data-driven observations with clinical expertise.

Result: The method effectively predicts relative tool-tissue poses and enhances visual understanding of robotic surgical motions directly from video data.

Conclusion: The framework advances robotic surgery by combining expert knowledge with flexible, data-driven modeling of deformable tissues.

Abstract: Accurate modeling of tool-tissue interactions in robotic surgery requires
precise tracking of deformable tissues and integration of surgical domain
knowledge. Traditional methods rely on labor-intensive annotations or rigid
assumptions, limiting flexibility. We propose a framework combining sparse
keypoint tracking and probabilistic modeling that propagates expert-annotated
landmarks across endoscopic frames, even with large tissue deformations.
Clustered tissue keypoints enable dynamic local transformation construction via
PCA, and tool poses, tracked similarly, are expressed relative to these frames.
Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)
integrates data-driven observations with labeled clinical expertise,
effectively predicting relative tool-tissue poses and enhancing visual
understanding of robotic surgical motions directly from video data.

</details>


### [225] [Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning](https://arxiv.org/abs/2504.11493)
*Azizul Zahid,Jie Fan,Farong Wang,Ashton Dy,Sai Swaminathan,Fei Liu*

Main category: cs.RO

TL;DR: A multimodal framework aligns human and robot actions in pick-and-place tasks, achieving ~71% accuracy for both models.


<details>
  <summary>Details</summary>
Motivation: To evaluate alignment in human-robot decision-making for collaboration and imitation learning in unstructured environments.

Method: Uses RGB video for human demonstrations and voxelized RGB-D space for robot demonstrations, combining ResNet for human intention and Perceiver Transformer for robot action prediction.

Result: Human model: 71.67% accuracy; robot model: 71.8% accuracy after 2000 training epochs.

Conclusion: The framework shows promise for aligning complex, multimodal human-robot behaviors in manipulation tasks.

Abstract: Understanding action correspondence between humans and robots is essential
for evaluating alignment in decision-making, particularly in human-robot
collaboration and imitation learning within unstructured environments. We
propose a multimodal demonstration learning framework that explicitly models
human demonstrations from RGB video with robot demonstrations in voxelized
RGB-D space. Focusing on the "pick and place" task from the RH20T dataset, we
utilize data from 5 users across 10 diverse scenes. Our approach combines
ResNet-based visual encoding for human intention modeling and a Perceiver
Transformer for voxel-based robot action prediction. After 2000 training
epochs, the human model reaches 71.67% accuracy, and the robot model achieves
71.8% accuracy, demonstrating the framework's potential for aligning complex,
multimodal human and robot behaviors in manipulation tasks.

</details>


### [226] [DM-OSVP++: One-Shot View Planning Using 3D Diffusion Models for Active RGB-Based Object Reconstruction](https://arxiv.org/abs/2504.11674)
*Sicong Pan,Liren Jin,Xuying Huang,Cyrill Stachniss,Marija Popović,Maren Bennewitz*

Main category: cs.RO

TL;DR: The paper proposes a one-shot view planning method for active object reconstruction using 3D diffusion models as priors, improving efficiency by eliminating online replanning.


<details>
  <summary>Details</summary>
Motivation: Active object reconstruction is essential for robotics, but generating informative views efficiently is challenging.

Method: Leverages 3D diffusion models conditioned on initial multi-view images to generate an approximate object model, integrating geometric and textural distributions for view planning.

Result: Validated in simulations and real-world experiments, showing effectiveness of 3D diffusion priors for one-shot view planning.

Conclusion: The approach successfully integrates 3D diffusion priors to enhance active object reconstruction efficiency.

Abstract: Active object reconstruction is crucial for many robotic applications. A key
aspect in these scenarios is generating object-specific view configurations to
obtain informative measurements for reconstruction. One-shot view planning
enables efficient data collection by predicting all views at once, eliminating
the need for time-consuming online replanning. Our primary insight is to
leverage the generative power of 3D diffusion models as valuable prior
information. By conditioning on initial multi-view images, we exploit the
priors from the 3D diffusion model to generate an approximate object model,
serving as the foundation for our view planning. Our novel approach integrates
the geometric and textural distributions of the object model into the view
planning process, generating views that focus on the complex parts of the
object to be reconstructed. We validate the proposed active object
reconstruction system through both simulation and real-world experiments,
demonstrating the effectiveness of using 3D diffusion priors for one-shot view
planning.

</details>


### [227] [An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World](https://arxiv.org/abs/2504.11698)
*Xingwu Ji,Haochen Niu,Dexin Duan,Rendong Ying,Fei Wen,Peilin Liu*

Main category: cs.RO

TL;DR: A self-supervised online adaptation framework for monocular visual odometry is proposed to improve generalization in diverse environments.


<details>
  <summary>Details</summary>
Motivation: The diversity of open-world scenarios challenges the generalization of learned robotic navigation systems, leading to unreliable depth and pose estimation in novel environments.

Method: The framework includes a lightweight depth estimation network with refiner modules, a self-supervised learning objective, and modules for sparse depth densification and dynamic consistency enhancement.

Result: The method demonstrates robustness and generalization on urban, in-house datasets and a robot platform, outperforming state-of-the-art approaches.

Conclusion: The proposed system effectively adapts to novel environments online, enhancing the reliability of visual odometry in diverse scenarios.

Abstract: Recently, learning-based robotic navigation systems have gained extensive
research attention and made significant progress. However, the diversity of
open-world scenarios poses a major challenge for the generalization of such
systems to practical scenarios. Specifically, learned systems for scene
measurement and state estimation tend to degrade when the application scenarios
deviate from the training data, resulting to unreliable depth and pose
estimation. Toward addressing this problem, this work aims to develop a visual
odometry system that can fast adapt to diverse novel environments in an online
manner. To this end, we construct a self-supervised online adaptation framework
for monocular visual odometry aided by an online-updated depth estimation
module. Firstly, we design a monocular depth estimation network with
lightweight refiner modules, which enables efficient online adaptation. Then,
we construct an objective for self-supervised learning of the depth estimation
module based on the output of the visual odometry system and the contextual
semantic information of the scene. Specifically, a sparse depth densification
module and a dynamic consistency enhancement module are proposed to leverage
camera poses and contextual semantics to generate pseudo-depths and valid masks
for the online adaptation. Finally, we demonstrate the robustness and
generalization capability of the proposed method in comparison with
state-of-the-art learning-based approaches on urban, in-house datasets and a
robot platform. Code is publicly available at:
https://github.com/jixingwu/SOL-SLAM.

</details>
