<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 143]
- [cs.CV](#cs.CV) [Total: 173]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.SD](#cs.SD) [Total: 15]
- [cs.LG](#cs.LG) [Total: 187]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 16]
- [eess.IV](#eess.IV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/pdf/2506.09147)
*Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian*

Main category: cs.CL

TL;DR: The paper introduces LLM-as-a-qualitative-judge, an LLM-based evaluation method for NLG systems, focusing on structured reports of common issues instead of numerical scores. It includes open-ended analysis and issue clustering, validated with human annotations.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge methods in NLG focus on numerical scores, lacking qualitative insights for system improvement. This work aims to provide actionable feedback for developers.

Method: The approach involves open-ended per-instance issue analysis and clustering using an intuitive cumulative algorithm. It is evaluated with ~300 annotations from 12 NLG datasets.

Result: LLM-as-a-qualitative-judge correctly identifies issues in 2/3 cases and produces reports similar to human annotators.

Conclusion: The method effectively provides qualitative insights for NLG system improvements, bridging the gap between numerical evaluation and actionable feedback.

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/pdf/2506.09175)
*Peidong Wang, Jian Xue, Rui Zhao, Junkun Chen, Aswin Shanmugam Subramanian, Jinyu Li*

Main category: cs.CL

TL;DR: A phrase dictionary biasing method improves speech translation by leveraging phrase mappings, outperforming phrase list biasing by 21% for streaming models and boosting multimodal models by 85% in phrase recall.


<details>
  <summary>Details</summary>
Motivation: Phrases are crucial for understanding conversations but are rare in training data, making their correct translation challenging in speech tasks.

Method: Proposes a phrase dictionary biasing method using source-to-target phrase mappings, applied to streaming speech translation and multimodal large language models.

Result: Outperforms phrase list biasing by 21% for streaming models and achieves 85% relative improvement in phrase recall for multimodal models.

Conclusion: The phrase dictionary biasing method effectively enhances phrase translation in both streaming and multimodal models.

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/pdf/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: The study explores how generative CNNs generalize phonotactic rules from raw audio, showing convolutional layers can generalize beyond lexical constraints when the FC bottleneck is narrowed.


<details>
  <summary>Details</summary>
Motivation: To understand if DNNs can generalize phonotactic rules independently of lexical learning and the impact of reducing the FC bottleneck.

Method: Train generative CNNs on raw audio waveforms, shrink the FC bottleneck, and probe generalizations by bypassing the FC with randomized feature maps.

Result: Convolutional layers generalize phonetic dependencies beyond lexical constraints, even with a narrow FC bottleneck.

Conclusion: CNNs can dynamically generalize phonotactic rules, suggesting their potential for lexically-independent phonetic learning.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/pdf/2506.09251)
*Ziyang Cai, Nayoung Lee, Avi Schwarzschild, Samet Oymak, Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: Transformer models can transfer length generalization across related tasks, improving extrapolation to longer inputs.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer models generalize to longer inputs by investigating task association and transfer effects.

Method: Study length generalization via task association, using auxiliary tasks to train models and analyze transfer effects across algorithmic tasks.

Result: Models generalize better to longer inputs when trained with related auxiliary tasks, and pretrained models show similar transfer effects.

Conclusion: Transformers reuse computational structures across tasks, enhancing generalization, with attention head reuse playing a key role.

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/pdf/2506.09259)
*Zhuofang Li, Rafal Kocielnik, Fereshteh Soltani, Penphob, Boonyarungsrit, Animashree Anandkumar, R. Michael Alvarez*

Main category: cs.CL

TL;DR: The paper introduces a method to detect and classify prosocial behaviors in online game chats using unsupervised discovery and a novel Self-Anchored Attention Model (SAAM), improving performance by 7.9% over existing techniques.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on toxic content detection, but prosocial communication is equally important for moderation and fostering positive interactions. Limited resources exist for identifying prosocial behaviors in game chats.

Method: Combined unsupervised discovery with game domain expert collaboration to categorize prosocial behaviors. Proposed SAAM, leveraging the entire training set as anchors to improve performance in low-resource settings.

Result: SAAM achieved a 7.9% improvement over existing techniques. The method was successfully applied to Call of Duty: Modern Warfare II, demonstrating its effectiveness.

Conclusion: This research pioneers NLP for prosocial behavior classification in game chats, shifting moderation focus from toxicity to promoting positive interactions.

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/pdf/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: The paper explores whether LLMs possess a theory of mind by investigating their ability to infer intentions in cooperative multi-agent reinforcement learning (MARL) settings.


<details>
  <summary>Details</summary>
Motivation: Understanding if LLMs can model others' intentions is crucial for effective collaboration between humans and AI, mirroring human social reasoning.

Method: The study uses cooperative MARL with LLM-based agents to analyze their ability to infer and reason about intentions through natural language interactions.

Result: The approach aims to enhance AI agents' adaptability and cooperation with both artificial and human partners.

Conclusion: The work advances hybrid human-AI systems for seamless collaboration, with significant implications for future human-artificial interactions.

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [7] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/pdf/2506.09277)
*Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Sarath Chandar, Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: A framework for measuring LLM-generated self-NLE faithfulness by comparing explanations with internal hidden states.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing self-NLE faithfulness lack examination of neural activity, leading to unfaithful explanations.

Method: Proposes a flexible framework comparing self-NLE with interpretations of the model's internal hidden states.

Result: Provides deep insights into self-NLE faithfulness and connects explanations to model reasoning.

Conclusion: Advances understanding of self-NLE faithfulness and aids in generating more faithful explanations.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [8] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/pdf/2506.09513)
*Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang, Qifeng Bai, Tingyang Xu*

Main category: cs.CL

TL;DR: ReasonMed introduces a large medical reasoning dataset and a refined training strategy, achieving state-of-the-art performance in medical QA.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance LLMs' capabilities in knowledge-intensive medical question answering, which remains underexplored despite their success in other domains.

Method: Developed ReasonMed, a 370k-example dataset, using a multi-agent verification and refinement process with an Error Refiner. Combined detailed Chain-of-Thought reasoning with concise answer summaries for fine-tuning.

Result: ReasonMed-7B outperforms prior sub-10B models by 4.17% and exceeds LLaMA3.1-70B on PubMedQA by 4.60%.

Conclusion: The approach sets a new benchmark for medical reasoning models, demonstrating the effectiveness of refined datasets and strategic fine-tuning.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [9] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/pdf/2506.09301)
*Cesare Spinoso-Di Piano, David Austin, Pablo Piantanida, Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: The paper introduces $(RSA)^2$, a framework for interpreting figurative language by modeling rhetorical strategies, achieving state-of-the-art performance on irony interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing RSA frameworks struggle with figurative language due to the need for setting-specific motivation modeling.

Method: The $(RSA)^2$ framework models rhetorical strategies instead of speaker motivations, combined with LLMs.

Result: Achieves state-of-the-art performance on the PragMega+ irony dataset.

Conclusion: $(RSA)^2$ effectively interprets figurative language without requiring explicit motivation modeling.

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [10] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/pdf/2506.09375)
*Massa Baali, Shuo Han, Syed Abdul Hannan, Purusottam Samal, Karanveer Singh, Soham Deshmukh, Rita Singh, Bhiksha Raj*

Main category: cs.CL

TL;DR: CoLMbo is a Speaker Language Model (SLM) that generates detailed speaker descriptions, addressing limitations of traditional speaker recognition systems by using prompt-based conditioning.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker recognition systems lack the ability to provide detailed speaker characteristics like dialect, gender, and age. CoLMbo aims to overcome this by integrating a speaker encoder with prompt-based conditioning.

Method: CoLMbo combines a speaker encoder with prompt-based conditioning to dynamically generate detailed captions about speaker attributes, adapting to user-defined prompts.

Result: The model excels in zero-shot scenarios and provides customized descriptions, including dialect and age-related traits, enhancing speaker profiling.

Conclusion: CoLMbo represents a significant advancement in speaker recognition by enabling context-rich, detailed speaker descriptions beyond traditional classification tasks.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [11] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/pdf/2506.09315)
*Yao Xiao, Heidi Christensen, Stefan Goetze*

Main category: cs.CL

TL;DR: The paper improves Alzheimer's dementia (AD) detection using Mistral-7B, boosting accuracy by 3.33% over existing methods and offering interpretable decision boundaries.


<details>
  <summary>Details</summary>
Motivation: To enhance AD detection accuracy and provide clearer decision-making processes compared to opaque existing methods.

Method: Extends the paired perplexity approach using Mistral-7B, fine-tunes the model, and analyzes its interpretability and language pattern learning.

Result: Achieves 3.33% higher accuracy than current methods and 6.35% over ADReSS 2020 benchmarks, with interpretable decision boundaries.

Conclusion: The approach effectively detects AD, learns AD language patterns, and opens avenues for model interpretation and data augmentation.

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [12] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/pdf/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: The paper introduces novel methodologies for aligning large language models (LLMs) with human expectations, focusing on data collection, training, and evaluation. Key contributions include Lion for adversarial distillation, WebR for automated data synthesis, LTE for knowledge integration, BMC for preference optimization, and FollowBench for constraint adherence evaluation.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human expectations efficiently and effectively is a critical challenge, as existing methods rely on manual curation or proprietary models, limiting scalability and diversity.

Method: Proposes Lion for adversarial distillation, WebR for automated data synthesis, LTE for meta-learning-based knowledge updates, BMC for token-level preference optimization, and FollowBench for evaluating constraint adherence.

Result: The methodologies achieve state-of-the-art zero-shot reasoning, improved data diversity and scalability, superior alignment in QA and mathematical reasoning, and expose weaknesses in current models' constraint adherence.

Conclusion: The introduced frameworks significantly advance LLM alignment, offering scalable, diverse, and effective solutions for data, training, and evaluation, with insights for future improvements.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [13] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/pdf/2506.09340)
*Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, Chaochao Lu*

Main category: cs.CL

TL;DR: RePO improves RL for LLMs by using replay strategies to enhance policy optimization, outperforming GRPO with higher efficiency and performance gains.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs and low data efficiency of GRPO in optimizing LLMs via RL.

Method: Introduces RePO, leveraging diverse replay strategies to retrieve off-policy samples from a replay buffer for broader policy optimization.

Result: RePO achieves significant performance gains (18.4 and 4.1 points for two models) and increases effective optimization steps by 48% with a 15% computational cost rise.

Conclusion: RePO is a more efficient and effective method for RL-based optimization of LLMs compared to GRPO.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [14] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/pdf/2506.09342)
*Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat*

Main category: cs.CL

TL;DR: The study explores latent multi-head attention (MLA) for small language models, showing MLA+RoPE with half-rank dimensions reduces memory by 45% with minimal quality loss, outperforming standard attention.


<details>
  <summary>Details</summary>
Motivation: To investigate efficiency-quality trade-offs in small language models using MLA variants, aiming for memory-efficient deployment without compromising performance.

Method: Training 30M-parameter GPT models on synthetic stories, comparing MHA, MLA, and MLA+RoPE with half-rank latent dimensions.

Result: MLA+RoPE (r=d/2) reduces KV-cache memory by 45% with only a 0.3% validation loss increase, outperforming vanilla attention by 2% with RoPE. Inference speeds up 1.4x over full-rank MLA.

Conclusion: MLA+RoPE offers a Pareto improvement for memory-constrained models, with RoPE being crucial for performance. The approach balances efficiency and quality effectively.

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [15] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/pdf/2506.09349)
*Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang, Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Jieping Ye*

Main category: cs.CL

TL;DR: OmniDRCA is a parallel speech-text foundation model using joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment, achieving SOTA performance in speech-text tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods either lack modality awareness in speech-text generation or rely on interleaved modeling. OmniDRCA aims to improve parallel joint modeling for better mutual modality awareness.

Method: OmniDRCA employs joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment to process speech and text in parallel.

Result: OmniDRCA achieves SOTA performance in parallel joint speech-text modeling and is competitive with interleaved models on Spoken Question Answering benchmarks.

Conclusion: OmniDRCA demonstrates the effectiveness of parallel joint modeling and contrastive alignment, with potential for extension to full-duplex conversational scenarios.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [16] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/pdf/2506.09351)
*Yuchen Feng, Bowen Shen, Naibin Gu, Jiaxuan Zhao, Peng Fu, Zheng Lin, Weiping Wang*

Main category: cs.CL

TL;DR: DIVE is a Diversity-Enhanced reconstruction method for MoE LLMs, improving training efficiency by leveraging expert diversity from pruned models.


<details>
  <summary>Details</summary>
Motivation: Existing MoE LLM reconstruction methods lack expert diversity, causing redundancy. DIVE addresses this by using pruned models to enhance diversity.

Method: DIVE involves domain affinity mining, pruning-based expert reconstruction, and efficient retraining of routers, experts, and normalization modules.

Result: DIVE achieves higher training efficiency with minimal accuracy loss, outperforming other methods with the same activated parameters.

Conclusion: DIVE effectively enhances expert diversity and training efficiency in MoE LLMs, offering a practical solution for reconstruction.

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [17] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/pdf/2506.08672)
*Yang Liu, Jiaqi Li, Zilong Zheng*

Main category: cs.CL

TL;DR: RuleReasoner, a reinforced rule-based reasoning method, outperforms large reasoning models (LRMs) in both in-distribution and out-of-distribution tasks, offering computational efficiency and robust generalization.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of whether small reasoning models (SRMs) can effectively learn rule-based reasoning with robust generalization across diverse tasks and domains.

Method: Introduces RuleReasoner, which uses a domain-aware dynamic sampling approach to resample training batches based on historical rewards, enabling flexible online learning for reinforcement learning (RL).

Result: RuleReasoner outperforms frontier LRMs by 4.1% on ID tasks and 10.4% on OOD tasks, with higher computational efficiency.

Conclusion: RuleReasoner demonstrates that SRMs can achieve robust rule-based reasoning, offering a simpler and more efficient alternative to LRMs.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [18] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/pdf/2506.09359)
*Qingyun Zeng, Simin Ma, Arash Niknafs, Ashish Basran, Carol Szabo*

Main category: cs.CL

TL;DR: The paper explores using LLMs to evaluate semantic and weak semantic equivalence in Text-to-SQL systems, addressing challenges in ambiguous queries and multiple valid SQL interpretations.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has advanced NL2SQL systems, but evaluating semantic equivalence of generated SQL remains challenging due to ambiguous queries and multiple valid interpretations.

Method: The study uses LLMs to assess semantic and weak semantic equivalence, analyzing common patterns of SQL equivalence and inequivalence.

Result: The paper identifies challenges in LLM-based evaluation of SQL equivalence, highlighting practical and semantic issues.

Conclusion: LLMs show promise for evaluating SQL equivalence but face challenges in handling ambiguity and multiple valid interpretations.

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [19] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/pdf/2506.09367)
*Zhengyuan Liu, Stella Xin Yin, Dion Hoe-Lian Goh, Nancy F. Chen*

Main category: cs.CL

TL;DR: COGENT is a curriculum-oriented framework for generating grade-appropriate educational content, addressing challenges in AI-generated STEM education materials.


<details>
  <summary>Details</summary>
Motivation: Generative AI struggles to align with curriculum standards and maintain readability in STEM education, especially for younger students.

Method: COGENT integrates science concepts, core ideas, and learning objectives, controls readability, and uses a "wonder-based" approach for engagement.

Result: COGENT produces grade-appropriate content comparable or superior to human references, validated by LLM and human evaluation.

Conclusion: COGENT offers a scalable solution for adaptive, high-quality educational resources.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [20] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/pdf/2506.09381)
*Austin McCutcheon, Thiago E. A. de Oliveira, Aleksandr Zheleznov, Chris Brogly*

Main category: cs.CL

TL;DR: The paper explores automatic differentiation of perceived low-quality vs. high-quality news headlines/links using ML models, achieving up to 90.3% accuracy with DistilBERT.


<details>
  <summary>Details</summary>
Motivation: The rise of online news necessitates tools to distinguish low-quality content, prompting an investigation into automated methods.

Method: Twelve ML models were tested on a balanced dataset of 57.5M news links/headlines (2018-2024) with 115 linguistic features, using expert-derived labels.

Result: Bagging classifier performed well (88.1% accuracy), while fine-tuned DistilBERT achieved the highest accuracy (90.3%) but required more training time.

Conclusion: Both traditional NLP features and deep learning models can effectively classify news quality, with trade-offs in performance and training time.

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [21] [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/pdf/2506.00267)
*Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: A novel pipeline for collecting spontaneous speech data is introduced, addressing the scarcity of high-quality natural dialogue datasets.


<details>
  <summary>Details</summary>
Motivation: The lack of spontaneous speech data in existing datasets, which mostly contain scripted dialogues, hinders the development of advanced speech processing models.

Method: A pipeline for eliciting and recording natural dialogues is developed, resulting in a dataset of 100+ hours of spontaneous speech.

Result: The dataset fosters fluid, natural conversations and diverse topics, providing a reproducible framework for future data collection.

Conclusion: The paper lays groundwork for addressing the data shortage and plans to expand the dataset, benefiting the research community.

Abstract: The success of large language models has driven interest in developing
similar speech processing capabilities. However, a key challenge is the
scarcity of high-quality spontaneous speech data, as most existing datasets
contain scripted dialogues. To address this, we present a novel pipeline for
eliciting and recording natural dialogues and release our dataset with 100+
hours of spontaneous speech. Our approach fosters fluid, natural conversations
while encouraging a diverse range of topics and interactive exchanges. Unlike
traditional methods, it facilitates genuine interactions, providing a
reproducible framework for future data collection. This paper introduces our
dataset and methodology, laying the groundwork for addressing the shortage of
spontaneous speech data. We plan to expand this dataset in future stages,
offering a growing resource for the research community.

</details>


### [22] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/pdf/2506.09391)
*Haoran Zhao, Robert D. Hawkins*

Main category: cs.CL

TL;DR: Large language models (LLMs) show politeness strategies but overuse negative politeness, raising concerns about pragmatic alignment.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs use context-sensitive politeness strategies like humans, balancing informational and social goals.

Method: Compare human and LLM responses in constrained and open-ended tasks, analyzing politeness strategies.

Result: Larger models replicate human politeness preferences but overuse negative strategies, even in positive contexts. Human evaluators prefer LLM responses in open-ended tasks.

Conclusion: LLMs handle politeness well but misalign in strategy use, highlighting challenges for pragmatic alignment in AI.

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [23] [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/pdf/2506.00628)
*Niyati Bafna, Matthew Wiesner*

Main category: cs.CL

TL;DR: The paper explores why LID models fail on accented speech, identifies misclassification patterns, and proposes methods to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Prior research shows LID models perform poorly on accented speech, but the causes and solutions are not well understood.

Method: Analyzes failure modes, tests model invariance, and introduces input chunking and sequence-level integration to improve performance.

Result: Identifies misclassification patterns, enhances robustness to accents, and improves LID performance on accented speech.

Conclusion: Simple methods like input chunking and sequence-level integration can significantly improve LID model performance on accented speech.

Abstract: Prior research indicates that LID model performance significantly declines on
accented speech; however, the specific causes, extent, and characterization of
these errors remain under-explored. (i) We identify a common failure mode on
accented speech whereby LID systems often misclassify L2 accented speech as the
speaker's native language or a related language. (ii) We present evidence
suggesting that state-of-the-art models are invariant to permutations of short
spans of speech, implying they classify on the basis of short phonotactic
features indicative of accent rather than language. Our analysis reveals a
simple method to enhance model robustness to accents through input chunking.
(iii) We present an approach that integrates sequence-level information into
our model without relying on monolingual ASR systems; this reduces
accent-language confusion and significantly enhances performance on accented
speech while maintaining comparable results on standard LID.

</details>


### [24] [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/pdf/2506.00975)
*Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao*

Main category: cs.CL

TL;DR: The paper introduces Next-Token-Pair Prediction (NTPP), a novel method for improving speech language models (SLMs) using dual-channel speech data, enhancing conversational abilities and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Current SLMs lack full utilization of dual-channel speech data, which captures human conversation dynamics. The goal is to improve natural, fluid spoken interactions.

Method: Proposes NTPP, a generative modeling paradigm for speaker-independent dual-channel spoken dialogue learning using decoder-only architectures.

Result: NTPP significantly improves turn-taking prediction, response coherence, naturalness, and reduces inference latency compared to existing methods.

Conclusion: NTPP advances SLMs by effectively leveraging dual-channel data, offering practical efficiency for real-time applications.

Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.

</details>


### [25] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/pdf/2506.09393)
*Xinyi Gao, Qiucheng Wu, Yang Zhang, Xuechen Liu, Kaizhi Qian, Ying Xu, Shiyu Chang*

Main category: cs.CL

TL;DR: KT$^2$ is a probabilistic knowledge tracing framework using a tree-structured hierarchy of concepts to improve performance in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Existing KT methods struggle in low-resource, online classroom settings where data is sparse and updates are frequent.

Method: KT$^2$ employs a Hidden Markov Tree Model to track student mastery hierarchically, using an EM algorithm and incremental updates.

Result: KT$^2$ outperforms baselines in online, low-resource scenarios.

Conclusion: The hierarchical approach of KT$^2$ effectively addresses low-resource challenges in knowledge tracing.

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [26] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/pdf/2506.09408)
*Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, Shun-Feng Su*

Main category: cs.CL

TL;DR: Token Constraint Decoding (TCD) improves LLM robustness to input noise, boosting performance by up to 39% for weaker models.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to minor input perturbations despite strong MCQA performance.

Method: Introduces TCD, an inference-time algorithm enforcing token-level alignment, paired with prompt engineering.

Result: TCD restores degraded performance, with up to +39% gains for weaker models, and regularizes overconfident outputs.

Conclusion: TCD is a practical, model-agnostic method for enhancing LLM robustness in real-world applications.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [27] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/pdf/2506.09414)
*Xiujun Zhou, Pingjian Zhang, Deyou Tang*

Main category: cs.CL

TL;DR: PGDA-KGQA is a prompt-guided generative framework for KGQA that enhances training data diversity and multi-hop reasoning through innovative augmentation strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current KGQA methods, such as data scarcity, lack of multi-hop reasoning samples, and semantic distortion, to improve model generalization and accuracy.

Method: PGDA-KGQA uses a unified prompt-design paradigm with LLMs to generate diverse (question, logical form) pairs. It includes single-hop pseudo questions, semantic-preserving rewriting, and answer-guided reverse path exploration for multi-hop questions.

Result: Achieves improvements on WebQSP (2.8% F1, 1.2% Hits@1, 3.1% Accuracy) and ComplexWebQuestions (1.8% F1, 1.1% Hits@1, 2.4% Accuracy).

Conclusion: PGDA-KGQA effectively addresses data diversity and multi-hop reasoning challenges, enhancing KGQA performance.

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [28] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/pdf/2506.09424)
*Md Messal Monem Miah, Adrita Anika, Xi Shi, Ruihong Huang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs and LMMs for automated deception detection across diverse datasets, showing fine-tuned LLMs excel in textual tasks while LMMs struggle with cross-modal cues.


<details>
  <summary>Details</summary>
Motivation: Detecting deception digitally is critical but challenging, necessitating evaluation of advanced models like LLMs and LMMs.

Method: Assessed open-source and commercial LLMs on datasets (RLTD, MU3D, OpSpam) using zero-shot, few-shot, and fine-tuned approaches. Analyzed auxiliary features and prompting strategies.

Result: Fine-tuned LLMs achieve state-of-the-art performance in textual deception detection; LMMs underperform with cross-modal cues.

Conclusion: LLMs show promise for real-world deception detection but have limitations, especially in multimodal contexts.

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [29] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/pdf/2506.09428)
*Fei Ding, Baiqiao Wang*

Main category: cs.CL

TL;DR: A novel SFT method reduces catastrophic forgetting in LLMs without needing original pre-training data, preserving generalization while improving task-specific performance.


<details>
  <summary>Details</summary>
Motivation: Address the issue of catastrophic forgetting and diminished general capabilities in LLMs during SFT, especially when original data is inaccessible.

Method: Reconstructs the likely SFT instruction distribution, uses multi-model screening to select optimal data, and mixes it with new data for SFT.

Result: Preserves generalization capabilities in general domains while enhancing task-specific performance.

Conclusion: The proposed method offers a cost-effective solution to mitigate catastrophic forgetting in SFT without relying on original data.

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [30] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/pdf/2506.09440)
*GigaChat team, Mamedov Valentin, Evgenii Kosarev, Gregory Leleytner, Ilya Shchuckin, Valeriy Berezovskiy, Daniil Smirnov, Dmitry Kozlov, Sergei Averkiev, Lukyanenko Ivan, Aleksandr Proshunin, Ainur Israfilova, Ivan Baskov, Artem Chervyakov, Emil Shakirov, Mikhail Kolesov, Daria Khomich, Darya Latortseva, Sergei Porkhun, Yury Fedorov, Oleg Kutuzov, Polina Kudriavtseva, Sofiia Soldatova, Kolodin Egor, Stanislav Pyatkin, Dzmitry Menshykh, Grafov Sergei, Eldar Damirov, Karlov Vladimir, Ruslan Gaitukiev, Arkadiy Shatenov, Alena Fenogenova, Nikita Savushkin, Fedor Minkin*

Main category: cs.CL

TL;DR: The paper introduces the GigaChat family of Russian LLMs, detailing their architecture, training, and performance on benchmarks, with open-source models released for broader use.


<details>
  <summary>Details</summary>
Motivation: Address the lack of foundational Russian LLMs due to high computational costs, aiming to expand NLP research and industrial applications.

Method: Developed GigaChat models in various sizes, including base and instruction-tuned versions, with detailed architecture and training processes.

Result: Evaluated performance on Russian and English benchmarks, compared with multilingual models, and demonstrated top models via API, Telegram bot, and Web interface.

Conclusion: Released open-source GigaChat models to support Russian NLP research and industrial solutions, filling a critical gap in the field.

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [31] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/pdf/2506.09450)
*Prameshwar Thiyagarajan, Vaishnavi Parimi, Shamant Sai, Soumil Garg, Zhangir Meirbek, Nitin Yarlagadda, Kevin Zhu, Chris Kim*

Main category: cs.CL

TL;DR: UniToMBench is a unified benchmark for evaluating Theory of Mind (ToM) in LLMs, combining SimToM and TOMBENCH strengths with multi-interaction tasks and evolving scenarios. It shows GPT-4o models excel in emotional/belief tasks but vary in knowledge-based tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs accurately predicting human mental states (ToM) by creating a systematic benchmark.

Method: Developed UniToMBench with 1,000 hand-written scenarios, integrating perspective-taking techniques and diverse metrics.

Result: GPT-4o models achieve >80% accuracy in emotional/belief tasks but show variability in knowledge-based tasks.

Conclusion: UniToMBench is a valuable tool for assessing and improving ToM capabilities in LLMs, revealing current strengths and limitations.

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [32] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/pdf/2506.09457)
*Zeguan Xiao, Yun Chen, Guanhua Chen*

Main category: cs.CL

TL;DR: POET addresses the reward-generation gap in DAAs by equal-length truncation of responses, improving performance in alignment tasks.


<details>
  <summary>Details</summary>
Motivation: The misalignment between training objectives and generation performance in DAAs, particularly the neglect of prefix tokens, motivates the need for POET.

Method: POET truncates preferred and dispreferred responses to equal lengths, ensuring attention to prefix tokens during optimization.

Result: POET improves DPO and SimPO, achieving up to 15.6 points in AlpacaEval 2 and better downstream task performance.

Conclusion: Addressing the reward-generation gap via POET enhances DAAs' effectiveness in aligning LLMs with human preferences.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [33] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/pdf/2506.09495)
*Ilanit Sobol, Shir Lissak, Refael Tikochinski, Tal Nakash, Anat Brunstein Klomek, Eyal Fruchter, Roi Reichart*

Main category: cs.CL

TL;DR: The study explores suicidal behaviors on YouTube using computational and expert-driven methods, identifying unique digital indicators like YouTube Engagement and Mental Health Struggles, and differences in motivation between pre-attempt and post-attempt video uploaders.


<details>
  <summary>Details</summary>
Motivation: Suicide is a major cause of death, and social media data can provide new insights into suicidal behavior, bridging the gap between digital footprints and clinical knowledge.

Method: Three approaches were used: computational bottom-up (LLM-based topic modeling), hybrid (expert review of topics), and top-down (psychological assessment of narratives). The study analyzed 181 YouTube channels of individuals with suicide attempts and 134 control channels.

Result: Five topics were linked to suicide attempts, with two showing temporal changes. YouTube Engagement, a platform-specific indicator, was missed by experts. Motivational differences were found between pre-attempt and post-attempt uploaders.

Conclusion: The study highlights the value of combining digital behavior analysis with clinical insights to better understand suicidality, revealing unique indicators and motivational shifts.

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [34] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/pdf/2506.09501)
*Jiayi Yuan, Hao Li, Xinheng Ding, Wenya Xie, Yu-Jhe Li, Wentian Zhao, Kun Wan, Jing Shi, Xia Hu, Zirui Liu*

Main category: cs.CL

TL;DR: The paper highlights the fragility of LLM performance reproducibility due to system configuration changes, tracing the issue to floating-point arithmetic. It introduces LayerCast for stable inference.


<details>
  <summary>Details</summary>
Motivation: Benchmark scores for LLMs assume accuracy and reproducibility, but system configurations like GPU settings can cause significant performance variations, especially in reasoning models.

Method: The study conducts controlled experiments across hardware, software, and precision settings to quantify output divergence. It also develops LayerCast, a pipeline using 16-bit weights and FP32 computations.

Result: Experiments show up to 9% accuracy variation and 9,000 token length differences in responses due to GPU and batch size changes. Floating-point precision is identified as a key factor.

Conclusion: Floating-point precision impacts LLM reproducibility but is often overlooked. LayerCast offers a solution by balancing memory efficiency and numerical stability.

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [35] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/pdf/2506.09507)
*Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo*

Main category: cs.CL

TL;DR: The paper proposes a unified rotary position embedding (RoPE) method to integrate Transformers and State Space Models (SSMs), addressing positional encoding incompatibility. The resulting hybrid model, \model, outperforms standard Transformers in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: The integration of Transformers and SSMs is hindered by incompatible positional encoding mechanisms (explicit RoPE in Transformers vs. implicit convolutions in SSMs), leading to performance issues.

Method: A unified rotary position embedding (\ourRoPE) is introduced to align positional encoding for both self-attention and state-space components, enabling the hybrid architecture \model.

Result: \model achieves 42.3% faster training and 29.5% faster inference at 4K sequence length, with over 4% higher accuracy on language modeling benchmarks. It also scales better, with larger versions showing greater gains.

Conclusion: Unified positional encoding resolves incompatibility in hybrid models, enabling efficient and high-performance long-context modeling.

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [36] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/pdf/2506.09542)
*Dingjun Wu, Yukun Yan, Zhenghao Liu, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG enhances RAG by integrating knowledge graphs (KGs) and spreading activation, improving factual accuracy and interpretability. It outperforms vanilla RAG by 3.8% to 13.8%.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods rely on single knowledge sources and lack cognitive mechanisms for activating relevant knowledge.

Method: Proposes KG-Infused RAG, which retrieves KG facts, expands queries, and combines corpus passages with structured facts. Preference learning is used to improve key pipeline stages.

Result: Outperforms vanilla RAG by 3.8% to 13.8% on five QA benchmarks and enhances Self-RAG when integrated.

Conclusion: KG-Infused RAG is an effective, versatile plug-and-play enhancement for corpus-based RAG methods.

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [37] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/pdf/2506.09556)
*Georgios Chatzichristodoulou, Despoina Kosmopoulou, Antonios Kritikos, Anastasia Poulopoulou, Efthymios Georgiou, Athanasios Katsamanis, Vassilis Katsouros, Alexandros Potamianos*

Main category: cs.CL

TL;DR: MEDUSA, a multimodal framework with a four-stage pipeline, addresses SER challenges like class imbalance and emotion ambiguity, achieving top performance in a 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: SER is difficult due to subjective emotions and uneven data representation in naturalistic conditions.

Method: Uses a four-stage pipeline: ensemble training with DeepSER (cross-modal transformer fusion), Manifold MixUp, and a meta-classifier. Incorporates soft targets, balanced sampling, and multitask learning.

Result: Ranked 1st in the Interspeech 2025 Challenge for Categorical Emotion Recognition.

Conclusion: MEDUSA effectively tackles SER challenges through multimodal fusion and advanced training techniques.

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [38] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/pdf/2506.09558)
*Eleni Gkovedarou, Joke Daems, Luna De Bruyne*

Main category: cs.CL

TL;DR: The study examines gender bias in Google Translate and DeepL for English-to-Greek translations, introducing GendEL dataset. GPT-4o shows potential for bias mitigation but isn't flawless.


<details>
  <summary>Details</summary>
Motivation: Addressing growing concerns about gender bias in MT systems, especially in understudied language pairs like English-to-Greek.

Method: Analyzed gender bias in two MT systems using GendEL, a dataset of 240 gender-ambiguous and unambiguous sentences, and tested GPT-4o for bias mitigation.

Result: Persistent gender bias found in MT systems; GPT-4o performed better in providing gendered/neutral alternatives but still had biases.

Conclusion: MT systems struggle with gender inclusivity; GPT-4o offers promise but requires further refinement to fully mitigate bias.

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [39] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/pdf/2506.09560)
*Stefan Krsteski, Matea Tashkovska, Borjan Sazdov, Hristijan Gjoreski, Branislav Gerazov*

Main category: cs.CL

TL;DR: The paper introduces resources to advance LLMs for Macedonian, including a large corpus, an instruction dataset, and an evaluation suite. They train an 8B-parameter model (domestic-yak) that outperforms baselines and rivals larger models.


<details>
  <summary>Details</summary>
Motivation: To address the limited capabilities of LLMs for low-resource languages like Macedonian, the authors aim to provide tools and datasets to support research and adoption.

Method: They compile a 40GB Macedonian corpus, a 106k-instruction dataset, and a benchmark suite. They train domestic-yak, an 8B-parameter model, and evaluate it against baselines.

Result: domestic-yak outperforms all 8B-parameter models and matches larger models. Native speakers prefer it for grammatical and cultural accuracy.

Conclusion: The released resources and model set a foundation for advancing LLMs in underrepresented languages, with all materials publicly available.

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [40] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/pdf/2506.09566)
*Blaž Škrlj, Boshko Koloski, Senja Pollak, Nada Lavrač*

Main category: cs.CL

TL;DR: A survey on integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) to enhance reasoning and knowledge tasks, highlighting gaps and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore how structured knowledge from KGs can improve LLMs' factual grounding and reasoning, and vice versa.

Method: Systematic categorization into KG-enhanced LLMs and LLM-augmented KGs, with analysis of scalability, efficiency, and data quality.

Result: Identified mutual benefits and gaps, emphasizing the need for scalable, efficient, and high-quality integration.

Conclusion: Proposes future research in neuro-symbolic integration, dynamic KG updating, data reliability, and ethics for advanced knowledge systems.

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [41] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/pdf/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: High Intrinsic Dimension (ID) sequences are less likely to be memorized by language models, especially in overparameterized models and sparse exposure scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand how latent structure (measured by ID) affects memorization in language models, addressing privacy and intellectual property concerns.

Method: Investigates the role of Intrinsic Dimension (ID) as a geometric proxy for sequence complexity in modulating memorization.

Result: High-ID sequences suppress memorization compared to low-ID ones, particularly in overparameterized models and sparse exposure.

Conclusion: Memorization is shaped by the interplay of model scale, exposure frequency, and sequence complexity (ID).

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [42] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/pdf/2506.09627)
*Nicolas Audinet de Pieuchon, Adel Daoud, Connor T. Jerzak, Moa Johansson, Richard Johansson*

Main category: cs.CL

TL;DR: The paper compares debiasing methods (DSL and PPI) for LLM annotations, finding DSL often outperforms PPI in bias reduction but lacks consistency across datasets, highlighting a bias-variance tradeoff.


<details>
  <summary>Details</summary>
Motivation: LLMs provide cost-effective text annotation but introduce bias compared to experts, affecting downstream analyses. Debiasing methods like DSL and PPI aim to mitigate this, but their finite-sample performance is unclear.

Method: The study evaluates DSL and PPI by analyzing their performance scaling with expert annotations and comparing them across tasks.

Result: DSL often reduces bias and improves efficiency more than PPI, but its performance varies across datasets. Both methods work well with large datasets.

Conclusion: The findings reveal a bias-variance tradeoff in debiasing methods, emphasizing the need for metrics to assess their efficiency in finite samples.

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [43] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/pdf/2506.09641)
*Anna Stein, Kevin Tang*

Main category: cs.CL

TL;DR: The study compares NDL and N-gram models for acoustic word duration, finding N-grams outperform NDL but information-theoretic enhancements improve NDL.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of NDL and N-gram models in predicting acoustic word duration, focusing on probabilistic reduction.

Method: Three models were tested: NDL with information-theoretic formulas, traditional NDL, and N-gram predictors, using the Buckeye corpus.

Result: N-gram model outperformed both NDL models, but information-theoretic enhancements improved NDL performance.

Conclusion: Highlights the need for combining information-theoretic metrics and discriminative learning in modeling acoustic reduction.

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [44] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/pdf/2506.09643)
*Harry Walsh, Maksym Ivashechkin, Richard Bowden*

Main category: cs.CL

TL;DR: The paper proposes using Sign Language Production techniques to augment small sign language datasets, improving Sign Language Translation model performance by up to 19%.


<details>
  <summary>Details</summary>
Motivation: Sign languages are low-resource, making dataset collection challenging. Augmenting existing datasets can enhance translation models.

Method: Uses skeleton-based production, sign stitching, and generative models (SignGAN, SignSplat) to create dataset variations.

Result: Performance of Sign Language Translation models improved by up to 19%.

Conclusion: The methods enable robust translation systems in resource-limited settings.

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [45] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/pdf/2506.09645)
*Tianjun Yao, Haoxuan Li, Zhiqiang Shen, Pan Li, Tongliang Liu, Kun Zhang*

Main category: cs.CL

TL;DR: RAPL is a novel framework for graph retrieval in KGQA, addressing limitations of existing methods with a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning, achieving superior performance and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs offer a structured alternative, but current graph-based retrievers struggle with generalization.

Method: RAPL introduces (1) a two-stage labeling strategy combining heuristic signals and parametric models, (2) a model-agnostic graph transformation for enhanced representation, and (3) a path-based reasoning strategy.

Result: RAPL outperforms state-of-the-art methods by 2.66%-20.34%, reduces performance gaps between LLMs, and shows strong generalizability.

Conclusion: RAPL improves graph retrieval for KGQA, offering better performance and generalizability, with potential for broader applications.

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [46] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/pdf/2506.09657)
*Nikolas Evkarpidi, Elena Tutubalina*

Main category: cs.CL

TL;DR: A system for QA over tabular data integrates text-to-SQL, text-to-code, self-correction, RAG, and an E2E module, achieving 80% accuracy and top-13 ranking in SemEval 2025 Task 8.


<details>
  <summary>Details</summary>
Motivation: To improve QA accuracy over tabular data by combining multiple modules and leveraging LLMs.

Method: Integration of text-to-SQL, text-to-code, self-correction, RAG, and an E2E module orchestrated by an LLM.

Result: 80% accuracy, top-13 ranking among 38 teams, comparable performance to proprietary LLMs.

Conclusion: The pipeline significantly improves accuracy for open-source models and addresses challenges in QA over tables.

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [47] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/pdf/2506.09669)
*Lihu Chen, Gaël Varoquaux*

Main category: cs.CL

TL;DR: A method for detecting knowledge boundaries in LLMs using Query-Level Uncertainty, improving adaptive inference and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' awareness of their knowledge limits to enable adaptive strategies like RAG or abstention, fostering efficient and trustworthy AI.

Method: Introduces a training-free approach, Internal Confidence, leveraging self-evaluations across layers and tokens to detect knowledge boundaries without token generation.

Result: Outperforms baselines in factual QA and mathematical reasoning; enables efficient RAG and model cascading, reducing costs while maintaining performance.

Conclusion: Internal Confidence effectively identifies knowledge boundaries, aiding adaptive inference and cost-efficient AI deployment.

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [48] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/pdf/2506.09672)
*Hao Xiong, Chuanyuan Tan, Wenliang Chen*

Main category: cs.CL

TL;DR: The paper addresses issues in Unstructured Knowledge Editing (UKE) by introducing datasets for locality evaluation and optimizing fine-tuning methods, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To improve UKE by addressing the lack of locality evaluation and the failure of fine-tuning methods in unstructured knowledge updates.

Method: Constructed datasets (UnKEBench-Loc and AKEW-Loc) for locality evaluation and identified factors affecting fine-tuning performance, leading to an optimized method (FT-UKE).

Result: FT-UKE outperforms existing SOTA methods, with performance gains increasing in batch editing scenarios.

Conclusion: The study provides a robust training recipe for UKE, demonstrating the effectiveness of optimized fine-tuning methods.

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [49] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/pdf/2506.09684)
*Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar*

Main category: cs.CL

TL;DR: The paper proposes a probabilistic framework for uncertainty quantification (UQ) in large language models (LLMs), introducing Inv-Entropy as a new measure and GAAP for perturbation diversity.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods for LLMs lack a probabilistic foundation and are often heuristic, necessitating a more robust approach.

Method: The paper introduces a dual random walk perspective, models input-output pairs as Markov chains, and proposes a probabilistic framework with Inv-Entropy. GAAP, a genetic algorithm-based perturbation method, is also introduced.

Result: Inv-Entropy outperforms existing semantic UQ methods, and the framework supports flexible uncertainty measures and metrics.

Conclusion: The proposed framework provides a robust, flexible, and theoretically grounded approach to UQ in LLMs, validated by extensive experiments.

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [50] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/pdf/2506.09790)
*Zhenran Xu, Yiyu Wang, Xue Yang, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1 is a large reasoning model for automated workflow generation in AI art, trained with a two-stage framework and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The steep learning curve for crafting effective AI workflows on platforms like ComfyUI necessitates an automated solution.

Method: ComfyUI-R1 uses a two-stage training framework: CoT fine-tuning for domain adaptation and reinforcement learning for reasoning capability, guided by hybrid rewards.

Result: The 7B-parameter model achieves 97% format validity and high F1 scores, surpassing GPT-4o and Claude series.

Conclusion: Long chain-of-thought reasoning and code-based workflow transformation are critical for synthesizing intricate AI art workflows.

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [51] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/pdf/2506.09796)
*Andreas Säuberli, Diego Frassinelli, Barbara Plank*

Main category: cs.CL

TL;DR: LLMs' human-like response behavior in educational assessments is evaluated, showing potential but limited suitability for zero-shot piloting.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can replace human participants in test development by evaluating their psychometric plausibility.

Method: Evaluates 18 LLMs using classical test theory and item response theory on multiple-choice datasets across reading, U.S. history, and economics.

Result: Larger models are overly confident but more human-like when calibrated; LLMs correlate better with humans in reading but not strongly enough for zero-shot use.

Conclusion: LLMs are not yet suitable for zero-shot piloting in educational assessments due to weak human-like correlations.

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [52] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/pdf/2506.09820)
*Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT is a post-training framework to improve LRMs' efficiency and accuracy in mathematical reasoning by integrating Code Interpreters (CI) through Hint-Engineering.


<details>
  <summary>Details</summary>
Motivation: LRMs struggle with complex math operations, and direct CI integration is inefficient. CoRT aims to optimize LRM-CI interaction.

Method: Synthesizes code-integrated reasoning data via Hint-Engineering, post-trains models (1.5B to 32B) with fine-tuning and reinforcement learning.

Result: Achieves 4-8% absolute improvements on math reasoning tasks and reduces token usage by 30-50%.

Conclusion: CoRT effectively enhances LRMs' mathematical reasoning by leveraging CI, with significant performance gains and efficiency improvements.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [53] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/pdf/2506.09827)
*Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer*

Main category: cs.CL

TL;DR: The paper introduces EmoNet-Voice, a new benchmark for evaluating AI's emotional understanding in speech, featuring large-scale datasets and synthetic audio with expert validation.


<details>
  <summary>Details</summary>
Motivation: Current speech emotion recognition datasets lack granularity, privacy, or rely on acted portrayals, necessitating a robust benchmark.

Method: EmoNet-Voice includes a pre-training dataset (EmoNet-Voice Big) and a benchmark dataset (EmoNet-Voice Bench) with synthetic audio and expert annotations.

Result: Empathic Insight Voice models achieve high agreement with human experts, with findings like anger being easier to detect than low-arousal states.

Conclusion: EmoNet-Voice provides a privacy-preserving, fine-grained benchmark for SER, advancing AI's emotional understanding capabilities.

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [54] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/pdf/2506.09833)
*Omar Sherif, Ali Hamdi*

Main category: cs.CL

TL;DR: EGPA introduces synthetic skeleton data to address data imbalance and detect subtle movement errors in rehabilitation, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing rehabilitation systems struggle with data imbalance and detecting subtle movement errors, especially in home-based settings.

Method: EGPA generates synthetic skeleton data by simulating clinically relevant movement mistakes and uses an attention-based graph convolutional network.

Result: Experiments show a 27.6% reduction in mean absolute error and a 45.8% gain in error classification accuracy.

Conclusion: EGPA enhances automated movement quality assessment in rehabilitation, offering clinical and home-based applications.

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [55] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/pdf/2506.09847)
*Tomas Peterka, Matyas Bohacek*

Main category: cs.CL

TL;DR: The paper introduces a dataset for detecting misattributed imagery in news, focusing on location and date relevance, and evaluates LLMs for these tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in detecting out-of-context and misattributed imagery in misinformation by focusing on provenance (location and date) rather than just semantics.

Method: Created the News Media Provenance Dataset with provenance-tagged images and evaluated six LLMs on location (LOR) and date/time (DTOR) relevance tasks.

Result: Zero-shot performance on LOR is promising, but DTOR performance is lacking, suggesting need for specialized solutions.

Conclusion: The dataset and tasks highlight challenges in provenance-based detection, calling for future work to improve DTOR performance.

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [56] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/pdf/2506.09853)
*Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang*

Main category: cs.CL

TL;DR: The paper addresses challenges in Chain-of-Thought (CoT) prompting for LLMs—sufficiency and necessity of reasoning steps—using a causal framework to improve efficiency and reduce redundancy.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning capabilities of LLMs by ensuring CoT steps are both sufficient and necessary, addressing inefficiencies in current methods.

Method: Proposes a causal framework incorporating Probability of Sufficiency and Necessity to quantify step influence, enabling automated step addition/pruning.

Result: Experiments show improved reasoning efficiency and reduced token usage without accuracy loss on math and commonsense benchmarks.

Conclusion: The framework offers a cost-effective way to enhance LLM reasoning performance by optimizing CoT steps.

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [57] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/pdf/2506.09886)
*Rodion Oblovatny, Alexandra Bazarova, Alexey Zaytsev*

Main category: cs.CL

TL;DR: A novel method detects hallucinations in LLMs by analyzing probabilistic divergence between prompt and response hidden-state distributions, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs are problematic, and current detection methods often rely on external knowledge or auxiliary models. This work aims to provide a model-intrinsic solution.

Method: The approach measures distributional distances between prompt and response hidden states, using deep learnable kernels for enhanced sensitivity.

Result: The method achieves state-of-the-art performance on benchmarks and remains robust even without kernel training.

Conclusion: The proposed solution is scalable and effective for detecting hallucinations in LLMs without external dependencies.

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [58] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/pdf/2506.09890)
*Yuxin Chen, Yiran Zhao, Yang Zhang, An Zhang, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Tat-Seng Chua, Michael Qizhe Shieh, Wenxuan Zhang*

Main category: cs.CL

TL;DR: LLMs develop a core language-agnostic parameter space, enabling abstract thought beyond specific languages, supported by shared neurons. Neuron-specific training strategies are proposed for enhanced multilingual performance.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that LLMs 'think' in English by identifying a language-agnostic parameter space and shared neurons that support multilingual generalization.

Method: Identify language-related neurons (shared or exclusive) and analyze their evolution in LLMs. Propose neuron-specific training strategies for different development stages.

Result: Shared neurons increase in importance, forming a core language-agnostic space, while exclusive neurons diminish. Proposed training strategies improve multilingual performance.

Conclusion: LLMs develop abstract thought through shared neurons, and tailored training strategies can enhance their multilingual capabilities.

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [59] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/pdf/2506.09902)
*Zheng Zhao, Clara Vania, Subhradeep Kayal, Naila Khan, Shay B. Cohen, Emine Yilmaz*

Main category: cs.CL

TL;DR: PersonaLens is a benchmark for evaluating personalization in task-oriented AI assistants, addressing gaps in existing benchmarks by using diverse user profiles and LLM-based agents for assessment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for personalization in AI assistants are limited to chit-chat or narrow domains, failing to capture task-oriented complexities.

Method: Introduces PersonaLens with diverse user profiles, a user agent for dialogues, and a judge agent using LLM-as-a-Judge to evaluate personalization, response quality, and task success.

Result: Experiments show significant variability in personalization capabilities of current LLM assistants.

Conclusion: PersonaLens provides crucial insights for advancing conversational AI systems by addressing the challenges of personalized task-oriented assistance.

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [60] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/pdf/2506.09917)
*Wendi Zhou, Ameer Saadat-Yazd, Nadin Kokciyan*

Main category: cs.CL

TL;DR: The paper introduces ASESUM, a novel summarization system for online reviews that captures aspect-centric opinions without relying on predefined aspects, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The impracticality of manually summarizing vast online reviews necessitates automated systems that can extract and summarize key opinions effectively.

Method: The proposed ASESUM framework extracts aspect-centric arguments, measures their salience and validity, and adapts to varying domains without predefined aspects.

Result: Experiments on a real-world dataset show ASESUM's superiority in capturing diverse perspectives compared to existing methods.

Conclusion: ASESUM effectively automates opinion summarization by focusing on critical aspects and providing grounded summaries, advancing the field beyond extractive and abstractive approaches.

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [61] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/pdf/2506.09942)
*Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li*

Main category: cs.CL

TL;DR: The paper introduces VerIF, a verification method combining rule-based and LLM-based verification for reinforcement learning in instruction-following tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored best practices for reinforcement learning in instruction following, particularly the verification challenge.

Method: Proposes VerIF, integrating rule-based code verification and LLM-based verification (e.g., QwQ-32B), supported by the VerInstruct dataset (22,000 instances).

Result: Significant improvements in benchmarks, state-of-the-art performance for comparable models, and unaffected general capabilities.

Conclusion: VerIF can enhance RL recipes for instruction-following tasks; datasets and models are released for future research.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [62] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/pdf/2506.09944)
*Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, Xi Ye*

Main category: cs.CL

TL;DR: The paper introduces QRHEAD and QR-RETRIEVER, improving retrieval in long-context LMs, achieving significant performance gains and strong zero-shot results.


<details>
  <summary>Details</summary>
Motivation: Enhancing retrieval capabilities in long-context language models by identifying and utilizing query-focused attention heads.

Method: Identify QRHEAD via query-focused attention aggregation and develop QR-RETRIEVER for efficient retrieval using attention mass.

Result: 10%+ performance gains on multi-hop reasoning tasks and strong zero-shot results on BEIR, outperforming competitors.

Conclusion: QRHEAD and QR-RETRIEVER advance retrieval in LMs, offering interpretability and practical utility.

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [63] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/pdf/2506.09967)
*Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa introduces SAE-Tuning, a cost-effective method to enhance reasoning in language models, achieving near-RL performance at a fraction of the cost and time.


<details>
  <summary>Details</summary>
Motivation: To efficiently elicit strong reasoning abilities in language models without expensive RL training.

Method: Uses sparse autoencoder tuning (SAE-Tuning) to extract reasoning abilities from a source model and transfer them to a target model via supervised fine-tuning.

Result: Achieves >97% of RL-trained performance at >2000x lower cost (~$1) and >450x faster (~20 minutes). Also shows generalizable and modular reasoning abilities.

Conclusion: SAE-Tuning is a scalable, efficient alternative to RL for enhancing reasoning in language models, with potential for broader application.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [64] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/pdf/2506.09975)
*Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: Detecting AI-generated social media posts is challenging due to short, informal text. A dataset of 505,159 AI-generated posts shows detection drops when attackers don't share their fine-tuned models, confirmed by human studies. Fine-tuning vulnerabilities impact all detection domains.


<details>
  <summary>Details</summary>
Motivation: Social media is a key attack vector for influence campaigns using AI-generated posts, making detection crucial despite challenges like short text and informal language.

Method: Created a dataset of 505,159 AI-generated posts from various LLMs across 11 topics, testing detection under realistic assumptions (no model sharing). Conducted human studies and ablation experiments.

Result: Detection is effective with known models but drops significantly when attackers withhold fine-tuned models. Human studies confirm this, and ablation experiments reveal detection vulnerabilities.

Conclusion: Fine-tuning LLMs poses a major challenge for detection, impacting all domains. Realistic threat scenarios require improved detection methods.

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [65] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/pdf/2506.09983)
*Hiroshi Matsuda, Chunpeng Ma, Masayuki Asahara*

Main category: cs.CL

TL;DR: A novel step-by-step instruction strategy for LLMs improves dependency parsing accuracy across 17 languages, with multilingual fine-tuning enhancing cross-language generalization.


<details>
  <summary>Details</summary>
Motivation: Standard prompting in LLMs often fails to produce structurally valid outputs in dependency parsing, prompting the need for a more effective method.

Method: Proposes a step-by-step instruction strategy: universal POS tagging first, followed by syntactic head and dependency label prediction, using a simplified CoNLL-U format.

Result: Achieves state-of-the-art accuracy on Universal Dependencies datasets for 17 languages, with no hallucination or contamination. Multilingual fine-tuning boosts cross-language generalization.

Conclusion: Explicit reasoning steps in LLM-based parsing are effective, offering a scalable, format-consistent alternative to bracket-based approaches.

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [66] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/pdf/2506.09992)
*Amel Muminovic, Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: The study evaluates large language models for detecting toxic comments in Serbian, Croatian, and Bosnian, showing that context-augmented prompts improve performance, with Gemini 1.5 Pro achieving the best balance.


<details>
  <summary>Details</summary>
Motivation: Online toxic language harms communities, especially in regions with limited moderation tools and labeled data.

Method: Tested four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, Claude 3 Opus) in zero-shot and context-augmented modes on a manually labeled dataset of 4,500 YouTube/TikTok comments.

Result: Context-augmented mode improved recall and F1 score, with Gemini 1.5 Pro achieving F1=0.82 and accuracy=0.82. GPT-4.1 had the highest precision and lowest false positives.

Conclusion: Adding minimal context and optimizing prompts can enhance toxic language detection in low-resource Balkan languages.

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [67] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/pdf/2506.09996)
*Yang Li, Qiang Sheng, Yehan Yang, Xueyao Zhang, Juan Cao*

Main category: cs.CL

TL;DR: The paper introduces a streaming content monitor (SCM) for partial detection of harmful content in LLM outputs, reducing latency while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing moderation methods for LLMs either cause high latency (full detection) or suffer from performance gaps (partial detection with full-detection-trained moderators).

Method: Constructs FineHarm dataset with fine-grained annotations and proposes SCM, trained with dual supervision for token-level harmfulness judgment.

Result: SCM achieves 0.95+ macro F1 score by viewing only 18% of tokens, comparable to full detection, and improves safety alignment.

Conclusion: SCM effectively balances latency and accuracy in moderation and enhances LLM safety alignment.

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


### [68] [AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes](https://arxiv.org/pdf/2305.14725)
*Barry Menglong Yao, Sijia Wang, Yu Chen, Qifan Wang, Minqian Liu, Zhiyang Xu, Licheng Yu, Lifu Huang*

Main category: cs.CL

TL;DR: The paper introduces attribute-aware multimodal entity linking, proposing AMELI, a benchmark dataset and knowledge base, and a new method incorporating entity attributes for improved disambiguation.


<details>
  <summary>Details</summary>
Motivation: To enhance multimodal entity linking by leveraging structured attributes of entities, addressing gaps in existing methods that overlook such meta-information.

Method: Constructs AMELI dataset and KB, tests state-of-the-art architectures, and introduces a new approach integrating entity attributes from text and images.

Result: Attributes significantly improve disambiguation, with the proposed method outperforming existing models.

Conclusion: Attributes are crucial for multimodal entity linking, and the new approach sets a benchmark for future research.

Abstract: We propose attribute-aware multimodal entity linking, where the input
consists of a mention described with a text paragraph and images, and the goal
is to predict the corresponding target entity from a multimodal knowledge base
(KB) where each entity is also accompanied by a text description, visual
images, and a collection of attributes that present the meta-information of the
entity in a structured format. To facilitate this research endeavor, we
construct AMELI, encompassing a new multimodal entity linking benchmark dataset
that contains 16,735 mentions described in text and associated with 30,472
images, and a multimodal knowledge base that covers 34,690 entities along with
177,873 entity images and 798,216 attributes. To establish baseline performance
on AMELI, we experiment with several state-of-the-art architectures for
multimodal entity linking and further propose a new approach that incorporates
attributes of entities into disambiguation. Experimental results and extensive
qualitative analysis demonstrate that extracting and understanding the
attributes of mentions from their text descriptions and visual images play a
vital role in multimodal entity linking. To the best of our knowledge, we are
the first to integrate attributes in the multimodal entity linking task. The
programs, model checkpoints, and the dataset are publicly available at
https://github.com/VT-NLP/Ameli.

</details>


### [69] [DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing](https://arxiv.org/pdf/2402.16733)
*Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh*

Main category: cs.CL

TL;DR: The paper introduces DREsS, a large-scale dataset for rubric-based automated essay scoring (AES) in EFL education, addressing gaps in previous models by including real-classroom data and synthetic samples for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Previous AES models lacked relevance to EFL education and provided only holistic scores due to inadequate datasets.

Method: The authors release DREsS, comprising three sub-datasets: real-classroom essays (DREsS_New), standardized existing datasets (DREsS_Std.), and synthetic samples (DREsS_CASE) generated via CASE, a corruption-based augmentation strategy.

Result: DREsS includes 48.9K samples, with synthetic data improving baseline results by 45.44%.

Conclusion: DREsS enables more accurate and practical AES systems for EFL writing education.

Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign
Language (EFL) writing education, offering real-time essay scores for students
and instructors. However, previous AES models were trained on essays and scores
irrelevant to the practical scenarios of EFL writing education and usually
provided a single holistic score due to the lack of appropriate datasets. In
this paper, we release DREsS, a large-scale, standard dataset for rubric-based
automated essay scoring with 48.9K samples in total. DREsS comprises three
sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a
real-classroom dataset with 2.3K essays authored by EFL undergraduate students
and scored by English education experts. We also standardize existing
rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a
corruption-based augmentation strategy for essays, which generates 40.1K
synthetic samples of DREsS_CASE and improves the baseline results by 45.44%.
DREsS will enable further research to provide a more accurate and practical AES
system for EFL writing education.

</details>


### [70] [Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation](https://arxiv.org/pdf/2404.01129)
*Bohao Yang, Kun Zhao, Dong Liu, Liang Zhan, Chenghua Lin*

Main category: cs.CL

TL;DR: A novel framework combining AMR-enhanced SLMs and LLMs improves open-domain dialogue evaluation by better handling adversarial negative responses and achieving high correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to evaluate adversarial negative responses effectively due to high lexical overlap but semantic incongruity, leading to low correlation with human judgments.

Method: Integrates AMR-enhanced SLMs with LLMs, using AMR graphs for semantic representation and combining SLM predictions with AMR knowledge in LLM prompts.

Result: Superior performance over baselines, with AMR graphs significantly boosting results. Strong correlation with human judgments across datasets.

Conclusion: The proposed framework sets a new benchmark for dialogue evaluation, demonstrating the value of AMR and hybrid SLM-LLM approaches.

Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention,
yet remains challenging due to the complexity of assessing response
appropriateness. Traditional evaluation metrics, typically trained with true
positive and randomly selected negative responses, tend to assign higher scores
to responses that share greater content similarity with contexts. However,
adversarial negative responses, despite possessing high lexical overlap with
contexts, can be semantically incongruous. Consequently, existing metrics
struggle to effectively evaluate such responses, resulting in low correlations
with human judgments. While recent studies have demonstrated the effectiveness
of Large Language Models (LLMs) for open-domain dialogue evaluation, they still
face challenges in handling adversarial negative examples. We propose a novel
evaluation framework that integrates Abstract Meaning Representation (AMR)
enhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly
incorporate AMR graph information through a gating mechanism for enhanced
semantic representation learning, while both SLM predictions and AMR knowledge
are integrated into LLM prompts for robust evaluation. Extensive experiments on
open-domain dialogue evaluation tasks demonstrate the superiority of our method
compared to state-of-the-art baselines. Our comprehensive ablation studies
reveal that AMR graph information contributes substantially more to performance
improvements. Our framework achieves strong correlations with human judgments
across multiple datasets, establishing a new benchmark for dialogue evaluation.
Our code and data are publicly available.

</details>


### [71] [Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR](https://arxiv.org/pdf/2405.14259)
*Chan-Jan Hsu, Yi-Chang Chen, Feng-Ting Liao, Pei-Chen Ho, Yu-Hsiang Wang, Po-Chun Hsu, Da-shan Shiu*

Main category: cs.CL

TL;DR: GFD is a novel shallow fusion framework for integrating LLMs into ASR and OCR systems, enabling seamless fusion across mismatched token spaces without re-training. It outperforms cascaded methods and adapts to in-context learning.


<details>
  <summary>Details</summary>
Motivation: To enhance cross-modal text recognition systems (ASR and OCR) by integrating LLMs efficiently, overcoming token space mismatches and enabling adaptive learning.

Method: GFD operates at the byte level for likelihood calculation, allowing plug-and-play fusion with auto-regressive models. It leverages intermediate and frequent LLM interactions.

Result: GFD surpasses cascaded methods in English and Mandarin benchmarks, achieving up to 17.7% WER reduction in adaptive ASR settings.

Conclusion: GFD is an effective, plug-and-play solution for integrating LLMs into ASR and OCR, improving performance and enabling adaptive learning.

Abstract: We propose "Generative Fusion Decoding" (GFD), a novel shallow fusion
framework designed to integrate large language models (LLMs) into cross-modal
text recognition systems for automatic speech recognition (ASR) and optical
character recognition (OCR). We derive the necessary formulations to enable GFD
to operate across mismatched token spaces of different models by calculating
likelihood at the byte level, thereby enabling seamless fusion and synchronous
progression during the decoding process. GFD is plug-and-play by design, making
it readily compatible with various auto-regressive models without the need for
any re-training. GFD proves effective for general ASR and OCR tasks through
intermediate and frequent interactions with LLMs, surpassing cascaded methods
in English and Mandarin benchmarks. In addition, GFD transfers in-context
learning abilities of LLMs and allows for adaptive ASR in instruction-aware and
long-context settings, yielding significant WER reductions of up to 17.7\%.

</details>


### [72] [Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/pdf/2406.06144)
*Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Juntao Dai, Yunhuai Liu, Yaodong Yang*

Main category: cs.CL

TL;DR: The paper explores the 'elasticity' of post-alignment LLMs, showing they revert to pre-training behaviors upon further fine-tuning, undermining alignment efforts.


<details>
  <summary>Details</summary>
Motivation: To investigate whether alignment fine-tuning has robust effects or superficial impacts, given anomalies in LLM behavior.

Method: Combines theoretical analysis (compression theory) and empirical experiments on models of varying scales to demonstrate elasticity.

Result: Alignment is disproportionately undermined by fine-tuning; elasticity correlates with model size and pre-training data.

Conclusion: Addressing LLM elasticity is crucial to mitigate resistance to alignment.

Abstract: Large language models (LLMs) may exhibit unintended or undesirable behaviors.
Recent works have concentrated on aligning LLMs to mitigate harmful outputs.
Despite these efforts, some anomalies indicate that even a well-conducted
alignment process can be easily circumvented, whether intentionally or
accidentally. Does alignment fine-tuning yield have robust effects on models,
or are its impacts merely superficial? In this work, we make the first
exploration of this phenomenon from both theoretical and empirical
perspectives. Empirically, we demonstrate the $\mathbf{elasticity}$ of
post-alignment models, i.e., the tendency to revert to the behavior
distribution formed during the pre-training phase upon further fine-tuning.
Leveraging compression theory, we formally deduce that fine-tuning
disproportionately undermines alignment relative to pre-training, potentially
by orders of magnitude. We validate the presence of elasticity through
experiments on models of varying types and scales. Specifically, we find that
model performance declines rapidly before reverting to the pre-training
distribution, after which the rate of decline drops significantly. Furthermore,
we further reveal that elasticity positively correlates with the increased
model size and the expansion of pre-training data. Our findings underscore the
need to address the inherent elasticity of LLMs to mitigate their resistance to
alignment. The model weight and code are available at
pku-lm-resist-alignment.github.io.

</details>


### [73] [Standard Language Ideology in AI-Generated Language](https://arxiv.org/pdf/2406.08726)
*Genevieve Smith, Eve Fleisig, Madeline Bossi, Ishita Rustagi, Xavier Yin*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) reinforce standard language ideology, particularly Standard American English (SAE), and its impact on minoritized communities. It proposes a taxonomy of issues and recommendations for more inclusive AI language generation.


<details>
  <summary>Details</summary>
Motivation: To highlight how LLMs perpetuate standard language ideology, disadvantaging minoritized language communities, and to advocate for structural changes in AI development.

Method: The authors present a faceted taxonomy of problems and introduce the concept of 'standard AI-generated language ideology,' analyzing its implications.

Result: LLMs reinforce SAE as the linguistic default, creating tensions around desirable system behavior and the imitation of diverse English varieties.

Conclusion: The paper recommends structural shifts in AI research and funding to support more inclusive and emancipatory outcomes for diverse language communities.

Abstract: Standard language ideology is reflected and reinforced in language generated
by large language models (LLMs). We present a faceted taxonomy of open problems
that illustrate how standard language ideology manifests in AI-generated
language, alongside implications for minoritized language communities and
society more broadly. We introduce the concept of standard AI-generated
language ideology, a process through which LLMs position "standard"
languages--particularly Standard American English (SAE)--as the linguistic
default, reinforcing the perception that SAE is the most "appropriate"
language. We then discuss ongoing tensions around what constitutes desirable
system behavior, as well as advantages and drawbacks of generative AI tools
attempting, or refusing, to imitate different English language varieties.
Rather than prescribing narrow technical fixes, we offer three recommendations
for researchers, practitioners, and funders that focus on shifting structural
conditions and supporting more emancipatory outcomes for diverse language
communities.

</details>


### [74] [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/pdf/2406.14230)
*Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie*

Main category: cs.CL

TL;DR: GETA is a generative evolving testing approach to dynamically assess LLMs' value alignment, addressing the limitations of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks for evaluating LLMs' ethical alignment suffer from evaluation chronoeffect, leading to outdated or saturated assessments as models evolve.

Method: GETA uses adaptive testing to dynamically generate test items tailored to model capabilities, learning a joint distribution of item difficulty and model conformity.

Result: GETA effectively creates difficulty-tailored test items and provides evaluations consistent with models' performance on unseen data.

Conclusion: GETA offers a robust, evolving framework for assessing LLMs' value alignment, addressing the shortcomings of static benchmarks.

Abstract: Warning: Contains harmful model outputs. Despite significant advancements,
the propensity of Large Language Models (LLMs) to generate harmful and
unethical content poses critical challenges. Measuring value alignment of LLMs
becomes crucial for their regulation and responsible deployment. Although
numerous benchmarks have been constructed to assess social bias, toxicity, and
ethical issues in LLMs, those static benchmarks suffer from evaluation
chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak
into training data or become saturated, overestimating ever-developing LLMs. To
tackle this problem, we propose GETA, a novel generative evolving testing
approach based on adaptive testing methods in measurement theory. Unlike
traditional adaptive testing methods that rely on a static test item pool, GETA
probes the underlying moral boundaries of LLMs by dynamically generating test
items tailored to model capability. GETA co-evolves with LLMs by learning a
joint distribution of item difficulty and model value conformity, thus
effectively addressing evaluation chronoeffect. We evaluated various popular
LLMs with GETA and demonstrated that 1) GETA can dynamically create
difficulty-tailored test items and 2) GETA's evaluation results are more
consistent with models' performance on unseen OOD and i.i.d. items, laying the
groundwork for future evaluation paradigms.

</details>


### [75] [CaLMQA: Exploring culturally specific long-form question answering across 23 languages](https://arxiv.org/pdf/2406.17761)
*Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi*

Main category: cs.CL

TL;DR: The paper introduces CaLMQA, a multilingual dataset for culturally specific long-form QA, highlighting LLMs' limitations in handling such questions, especially in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored ability of LLMs in generating long-form answers to culturally specific questions across diverse languages.

Method: Created CaLMQA (51.7K questions in 23 languages) by crawling community forums and hiring native speakers for under-resourced languages. Evaluated LLM answers for factuality, relevance, and quality.

Result: LLMs exhibit critical errors (e.g., wrong language, repetition) in low-resource languages and more factual errors in culturally specific questions.

Conclusion: CaLMQA enables future research in multilingual and culturally aware QA, revealing LLMs' current limitations.

Abstract: Despite rising global usage of large language models (LLMs), their ability to
generate long-form answers to culturally specific questions remains unexplored
in many languages. To fill this gap, we perform the first study of textual
multilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally
specific questions across 23 different languages. We define culturally specific
questions as those that refer to concepts unique to one or a few cultures, or
have different answers depending on the cultural or regional context. We obtain
these questions by crawling naturally-occurring questions from community web
forums in high-resource languages, and by hiring native speakers to write
questions in under-resourced, rarely-studied languages such as Fijian and
Kirundi. Our data collection methodologies are translation-free, enabling the
collection of culturally unique questions like "Kuber iki umwami wa mbere
w'uburundi yitwa Ntare?" (Kirundi; English translation: "Why was the first king
of Burundi called Ntare (Lion)?"). We evaluate factuality, relevance and
surface-level quality of LLM-generated long-form answers, finding that (1) for
many languages, even the best models make critical surface-level errors (e.g.,
answering in the wrong language, repetition), especially for low-resource
languages; and (2) answers to culturally specific questions contain more
factual errors than answers to culturally agnostic questions -- questions that
have consistent meaning and answer across many cultures. We release CaLMQA to
facilitate future research in cultural and multilingual long-form QA.

</details>


### [76] [CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses](https://arxiv.org/pdf/2407.13329)
*Lorenzo Paolini, Sahar Vahdati, Angelo Di Iorio, Robert Wardenga, Ivan Heibi, Silvio Peroni*

Main category: cs.CL

TL;DR: CiteFusion is an ensemble framework for multi-class Citation Intent Classification, using SciBERT and XLNet models, achieving state-of-the-art performance on SciCite and ACL-ARC datasets. It enhances interpretability with SHAP and investigates the role of structural context.


<details>
  <summary>Details</summary>
Motivation: To evaluate research impact and promote transparent scholarly communication by understanding citation motivations.

Method: One-vs-all decomposition into binary sub-tasks, leveraging SciBERT and XLNet models, aggregated via a neural network meta-classifier. SHAP is used for interpretability, and section titles are incorporated for context.

Result: Achieves Macro-F1 scores of 89.60% on SciCite and 76.24% on ACL-ARC, demonstrating robustness in imbalanced and data-scarce scenarios.

Conclusion: CiteFusion provides a high-performance, interpretable solution for citation intent classification, with practical applications like a released web-based tool.

Abstract: Understanding the motivations underlying scholarly citations is essential to
evaluate research impact and pro-mote transparent scholarly communication. This
study introduces CiteFusion, an ensemble framework designed to address the
multi-class Citation Intent Classification task on two benchmark datasets:
SciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the
multi-class task into class-specific binary sub-tasks, leveraging complementary
pairs of SciBERT and XLNet models, independently tuned, for each citation
intent. The outputs of these base models are aggregated through a feedforward
neural network meta-classifier to reconstruct the original classification task.
To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed
to analyze token-level contributions, and interactions among base models,
providing transparency into the classification dynamics of CiteFusion, and
insights about the kind of misclassifications of the ensem-ble. In addition,
this work investigates the semantic role of structural context by incorporating
section titles, as framing devices, into input sentences, assessing their
positive impact on classification accuracy. CiteFusion ul-timately demonstrates
robust performance in imbalanced and data-scarce scenarios: experimental
results show that CiteFusion achieves state-of-the-art performance, with
Macro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to
ensure interoperability and reusability, citation intents from both datasets
sche-mas are mapped to Citation Typing Ontology (CiTO) object properties,
highlighting some overlaps. Finally, we describe and release a web-based
application that classifies citation intents leveraging the CiteFusion models
developed on SciCite.

</details>


### [77] [MMREC: LLM Based Multi-Modal Recommender System](https://arxiv.org/pdf/2408.04211)
*Jiahao Tian, Jinman Zhao, Zhenkai Wang, Zhicheng Ding*

Main category: cs.CL

TL;DR: A novel framework using LLMs and deep learning improves recommender systems by integrating multi-modal data (text and images) into a unified latent space, enhancing recommendation accuracy and relevance.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of content and the need to leverage natural language and image data for better user preference understanding drive this research.

Method: The proposed framework uses LLMs and deep learning to process multi-modal data, unifying it in a latent space for simplified learning in the ranking model.

Result: Experiments show the model's improved discriminative power with multi-modal data, leading to more accurate and relevant recommendations.

Conclusion: The study highlights the potential of LLMs and multi-modal integration to advance recommender systems, offering personalized and context-aware recommendations.

Abstract: The importance of recommender systems is growing rapidly due to the
exponential increase in the volume of content generated daily. This surge in
content presents unique challenges for designing effective recommender systems.
Key among these challenges is the need to effectively leverage the vast amounts
of natural language data and images that represent user preferences. This paper
presents a novel approach to enhancing recommender systems by leveraging Large
Language Models (LLMs) and deep learning techniques. The proposed framework
aims to improve the accuracy and relevance of recommendations by incorporating
multi-modal information processing and by the use of unified latent space
representation. The study explores the potential of LLMs to better understand
and utilize natural language data in recommendation contexts, addressing the
limitations of previous methods. The framework efficiently extracts and
integrates text and image information through LLMs, unifying diverse modalities
in a latent space to simplify the learning process for the ranking model.
Experimental results demonstrate the enhanced discriminative power of the model
when utilizing multi-modal information. This research contributes to the
evolving field of recommender systems by showcasing the potential of LLMs and
multi-modal data integration to create more personalized and contextually
relevant recommendations.

</details>


### [78] [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/pdf/2408.14352)
*Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri*

Main category: cs.CL

TL;DR: LogProber is a new algorithm for detecting contamination in LLMs by focusing on question familiarity, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Contamination in LLMs, where test data leaks into training, hampers fair performance evaluation. Current detection methods for short text sequences are limited.

Method: LogProber detects contamination in a black-box setting by analyzing familiarity with questions rather than answers.

Result: The method is efficient and tackles drawbacks of concurrent approaches, though some contamination forms may still evade detection.

Conclusion: LogProber offers a practical solution for contamination detection, but algorithm design impacts its effectiveness against certain contamination types.

Abstract: In machine learning, contamination refers to situations where testing data
leak into the training set. The issue is particularly relevant for the
evaluation of the performance of Large Language Models (LLMs), which are
generally trained on gargantuan, and generally opaque, corpora of text scraped
from the world wide web. Developing tools to detect contamination is therefore
crucial to be able to fairly and properly track the evolution of the
performance of LLMs. To date, only a few recent studies have attempted to
address the issue of quantifying and detecting contamination in short text
sequences, such as those commonly found in benchmarks. However, these methods
have limitations that can sometimes render them impractical.In the present
paper, we introduce LogProber, a novel, efficient algorithm that we show to be
able to detect contamination in a black box setting that tries to tackle some
of these drawbacks by focusing on the familiarity with the question rather than
the answer. Here, we explore the properties of the proposed method in
comparison with concurrent approaches, identify its advantages and limitations,
and illustrate how different forms of contamination can go undetected depending
on the design of the detection algorithm.

</details>


### [79] [Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic](https://arxiv.org/pdf/2408.16326)
*Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun*

Main category: cs.CL

TL;DR: Critic-CoT enhances LLM reasoning by enabling System-2-like self-critique through step-wise CoT and distant-supervision, improving performance on tasks like GSM8K and MATH.


<details>
  <summary>Details</summary>
Motivation: Current self-critic methods in LLMs rely on basic prompts (System-1-like), limiting reasoning. The relationship between critique and task-solving abilities is underexplored.

Method: Critic-CoT uses step-wise Chain-of-Thought (CoT) reasoning and distant-supervision data to enable analytic self-critique and refinement.

Result: Experiments show improved task-solving performance on GSM8K and MATH by filtering invalid solutions and iterative refinement.

Conclusion: Critique and task-solving abilities in LLMs can mutually reinforce each other, enhancing overall reasoning performance.

Abstract: Self-critic has become a crucial mechanism for enhancing the reasoning
performance of LLMs. However, current approaches mainly involve basic prompts
for intuitive instance-level feedback, which resembles System-1 processes and
limits the reasoning capabilities. Moreover, there is a lack of in-depth
investigations into the relationship between LLM's ability to criticize and its
task-solving performance. To address these issues, we propose Critic-CoT, a
novel framework that pushes LLMs toward System-2-like critic capability.
Through a step-wise CoT reasoning paradigm and the automatic construction of
distant-supervision data without human annotation, Critic-CoT enables LLMs to
engage in slow, analytic self-critique and refinement, thereby improving their
reasoning abilities. Experiments on GSM8K and MATH demonstrate that our
enhanced model significantly boosts task-solving performance by filtering out
invalid solutions or iterative refinement. Furthermore, we investigate the
intrinsic correlation between critique and task-solving abilities within LLMs,
discovering that these abilities can mutually reinforce each other rather than
conflict.

</details>


### [80] [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/pdf/2409.00598)
*Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang*

Main category: cs.CL

TL;DR: A method to auto-generate pseudo-harmful prompts for evaluating LLMs, revealing a trade-off between false refusals and safety, and showing jailbreak defenses increase false refusals.


<details>
  <summary>Details</summary>
Motivation: Addressing false refusals of harmless prompts by LLMs, which frustrate users and risk public backlash against safety alignment.

Method: Proposes a method to auto-generate diverse, content-controlled pseudo-harmful prompts, creating the PHTest dataset for evaluation.

Result: Evaluation of 20 LLMs shows a trade-off between false refusals and safety, with jailbreak defenses increasing false refusals.

Conclusion: The method and PHTest dataset aid in developing safer, more usable LLMs by better evaluating and fine-tuning their responses.

Abstract: Safety-aligned large language models (LLMs) sometimes falsely refuse
pseudo-harmful prompts, like "how to kill a mosquito," which are actually
harmless. Frequent false refusals not only frustrate users but also provoke a
public backlash against the very values alignment seeks to protect. In this
paper, we propose the first method to auto-generate diverse,
content-controlled, and model-dependent pseudo-harmful prompts. Using this
method, we construct an evaluation dataset called PHTest, which is ten times
larger than existing datasets, covers more false refusal patterns, and
separately labels controversial prompts. We evaluate 20 LLMs on PHTest,
uncovering new insights due to its scale and labeling. Our findings reveal a
trade-off between minimizing false refusals and improving safety against
jailbreak attacks. Moreover, we show that many jailbreak defenses significantly
increase the false refusal rates, thereby undermining usability. Our method and
dataset can help developers evaluate and fine-tune safer and more usable LLMs.
Our code and dataset are available at
https://github.com/umd-huang-lab/FalseRefusal

</details>


### [81] [MOSAIC: Multiple Observers Spotting AI Content](https://arxiv.org/pdf/2409.07615)
*Matthieu Dubois, François Yvon, Pablo Piantanida*

Main category: cs.CL

TL;DR: The paper proposes an ensemble-based method to detect AI-generated text by leveraging multiple LLMs, improving robustness over single or fixed-pair models.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has increased the spread of harmful or fake content, necessitating better tools to distinguish AI-generated from human-written text.

Method: The approach ensembles several LLMs, combining their probability distributions to detect AI-generated content more robustly than perplexity-based or fixed-pair methods.

Result: Experiments show the ensemble method outperforms single or fixed-pair models, achieving robust detection across domains.

Conclusion: The proposed ensemble approach effectively combines the strengths of multiple LLMs, enhancing detection performance and generalizability.

Abstract: The dissemination of Large Language Models (LLMs), trained at scale, and
endowed with powerful text-generating abilities, has made it easier for all to
produce harmful, toxic, faked or forged content. In response, various proposals
have been made to automatically discriminate artificially generated from
human-written texts, typically framing the problem as a binary classification
problem. Early approaches evaluate an input document with a well-chosen
detector LLM, assuming that low-perplexity scores reliably signal machine-made
content. More recent systems instead consider two LLMs and compare their
probability distributions over the document to further discriminate when
perplexity alone cannot. However, using a fixed pair of models can induce
brittleness in performance. We extend these approaches to the ensembling of
several LLMs and derive a new, theoretically grounded approach to combine their
respective strengths. Our experiments, conducted with various generator LLMs,
indicate that this approach effectively leverages the strengths of each model,
resulting in robust detection performance across multiple domains. Our code and
data are available at https://github.com/BaggerOfWords/MOSAIC .

</details>


### [82] [Explaining word embeddings with perfect fidelity: Case study in research impact prediction](https://arxiv.org/pdf/2409.15912)
*Lucie Dvorackova, Marcin P. Joachimiak, Michal Cerny, Adriana Kubecova, Vilem Sklenak, Tomas Kliegr*

Main category: cs.CL

TL;DR: SMER is a new feature importance method for logistic regression models using word embeddings, offering perfect fidelity and better explanations than LIME.


<details>
  <summary>Details</summary>
Motivation: Existing embedding-based models lack direct explainability, and methods like LIME have questionable fidelity.

Method: Introduces SMER, a feature importance method for logistic regression models trained on word embeddings, ensuring exact correspondence to the model's predictions.

Result: SMER outperforms LIME in explanation quality, demonstrated through experiments on 50,000 CORD-19 papers.

Conclusion: SMER provides reliable and theoretically perfect explanations for word embedding-based models, improving interpretability.

Abstract: Best performing approaches for scholarly document quality prediction are
based on embedding models, which do not allow direct explanation of classifiers
as distinct words no longer correspond to the input features for model
training. Although model-agnostic explanation methods such as Local
interpretable model-agnostic explanations (LIME) can be applied, these produce
results with questionable correspondence to the ML model. We introduce a new
feature importance method, Self-model Rated Entities (SMER), for logistic
regression-based classification models trained on word embeddings. We show that
SMER has theoretically perfect fidelity with the explained model, as its
prediction corresponds exactly to the average of predictions for individual
words in the text. SMER allows us to reliably determine which words or entities
positively contribute to predicting impactful articles. Quantitative and
qualitative evaluation is performed through five diverse experiments conducted
on 50.000 research papers from the CORD-19 corpus. Through an AOPC curve
analysis, we experimentally demonstrate that SMER produces better explanations
than LIME for logistic regression.

</details>


### [83] [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment](https://arxiv.org/pdf/2410.08193)
*Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh*

Main category: cs.CL

TL;DR: GenARM introduces an autoregressive reward model for test-time alignment of LLMs, outperforming prior methods and matching training-time performance.


<details>
  <summary>Details</summary>
Motivation: Traditional alignment methods for LLMs are costly and inflexible, requiring repeated training for diverse preferences. Test-time methods using trajectory-level RMs are inefficient for autoregressive generation.

Method: GenARM uses an autoregressive reward model (ARM) to predict next-token rewards, enabling efficient alignment of frozen LLMs without retraining.

Result: GenARM outperforms test-time baselines, matches training-time performance, supports weak-to-strong guidance, and enables multi-objective alignment.

Conclusion: GenARM provides a cost-effective, flexible solution for aligning LLMs with human preferences, addressing limitations of existing methods.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but require
careful alignment with human preferences. Traditional training-time methods
finetune LLMs using human preference datasets but incur significant training
costs and require repeated training to handle diverse user preferences.
Test-time alignment methods address this by using reward models (RMs) to guide
frozen LLMs without retraining. However, existing test-time approaches rely on
trajectory-level RMs which are designed to evaluate complete responses, making
them unsuitable for autoregressive text generation that requires computing
next-token rewards from partial responses. To address this, we introduce
GenARM, a test-time alignment approach that leverages the Autoregressive Reward
Model--a novel reward parametrization designed to predict next-token rewards
for efficient and effective autoregressive generation. Theoretically, we
demonstrate that this parametrization can provably guide frozen LLMs toward any
distribution achievable by traditional RMs within the KL-regularized
reinforcement learning framework. Experimental results show that GenARM
significantly outperforms prior test-time alignment baselines and matches the
performance of training-time methods. Additionally, GenARM enables efficient
weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high
costs of training larger models. Furthermore, GenARM supports multi-objective
alignment, allowing real-time trade-offs between preference dimensions and
catering to diverse user preferences without retraining. Our project page is
available at: https://genarm.github.io.

</details>


### [84] [Guidelines for Fine-grained Sentence-level Arabic Readability Annotation](https://arxiv.org/pdf/2410.08674)
*Nizar Habash, Hanada Taha-Thomure, Khalid N. Elmadani, Zeina Zeino, Abdallah Abushmaes*

Main category: cs.CL

TL;DR: The paper introduces BAREC, a large-scale Arabic readability corpus with 69,441 sentences labeled across 19 levels, refined through educator feedback, and reports high annotator agreement (81.8%).


<details>
  <summary>Details</summary>
Motivation: To create a fine-grained, sentence-level readability assessment resource for Arabic, addressing linguistic, pedagogical, and cognitive factors.

Method: Refined annotation guidelines based on the Taha/Arabi21 framework through iterative training with native Arabic educators.

Result: High inter-annotator agreement (81.8% Quadratic Weighted Kappa) and benchmarking of automatic readability models across multiple granularities.

Conclusion: BAREC and its guidelines are publicly available, offering a valuable resource for Arabic readability research.

Abstract: This paper presents the annotation guidelines of the Balanced Arabic
Readability Evaluation Corpus (BAREC), a large-scale resource for fine-grained
sentence-level readability assessment in Arabic. BAREC includes 69,441
sentences (1M+ words) labeled across 19 levels, from kindergarten to
postgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined
through iterative training with native Arabic-speaking educators. We highlight
key linguistic, pedagogical, and cognitive factors in determining readability
and report high inter-annotator agreement: Quadratic Weighted Kappa 81.8%
(substantial/excellent agreement) in the last annotation phase. We also
benchmark automatic readability models across multiple classification
granularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are
publicly available.

</details>


### [85] [How Do Multilingual Language Models Remember Facts?](https://arxiv.org/pdf/2410.14387)
*Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard*

Main category: cs.CL

TL;DR: The paper explores how knowledge recall mechanisms in multilingual LLMs differ from English-only models, revealing language-dependent and independent aspects, and introduces the concept of Function Vectors (FVs).


<details>
  <summary>Details</summary>
Motivation: To understand how knowledge recall mechanisms in LLMs generalize to non-English languages and multilingual models, addressing a gap in prior research focused on English monolingual models.

Method: Analyzed three multilingual LLMs, applying patching to intermediate representations to localize language roles in recall, identifying Function Vectors (FVs) in the last token representation.

Result: Recall mechanisms in English largely apply to multilingual contexts, with subject enrichment being language-independent and object extraction language-dependent. FVs encode query language and content.

Conclusion: Multilingual LLMs have unique recall mechanisms, necessitating tailored methodologies for knowledge evaluation, fact editing, and acquisition in multilingual contexts.

Abstract: Large Language Models (LLMs) store and retrieve vast amounts of factual
knowledge acquired during pre-training. Prior research has localized and
identified mechanisms behind knowledge recall; however, it has only focused on
English monolingual models. The question of how these mechanisms generalize to
non-English languages and multilingual LLMs remains unexplored. In this paper,
we address this gap by conducting a comprehensive analysis of three
multilingual LLMs. First, we show that previously identified recall mechanisms
in English largely apply to multilingual contexts, with nuances based on
language and architecture. Next, through patching intermediate representations,
we localize the role of language during recall, finding that subject enrichment
is language-independent, while object extraction is language-dependent.
Additionally, we discover that the last token representation acts as a Function
Vector (FV), encoding both the language of the query and the content to be
extracted from the subject. Furthermore, in decoder-only LLMs, FVs compose
these two pieces of information in two separate stages. These insights reveal
unique mechanisms in multilingual LLMs for recalling information, highlighting
the need for new methodologies -- such as knowledge evaluation, fact editing,
and knowledge acquisition -- that are specifically tailored for multilingual
LLMs.

</details>


### [86] [Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models](https://arxiv.org/pdf/2410.17131)
*Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Ben He, Le Sun, Jingren Zhou, Junyang Lin*

Main category: cs.CL

TL;DR: The paper introduces Self-Steering Optimization (SSO), an algorithm for autonomously generating high-quality preference data to improve alignment systems without manual annotation.


<details>
  <summary>Details</summary>
Motivation: Prior research lacks quality control in automated alignment, leading to inaccurate data. SSO addresses this gap by focusing on high-quality data generation.

Method: SSO uses a specialized optimization objective to create a data generator from the policy model, producing accurate and on-policy preference data.

Result: SSO outperforms baselines in human preference alignment and reward optimization, validated on Llama 3 and Qwen 2 models.

Conclusion: SSO is a scalable framework for preference optimization, advancing automated alignment techniques.

Abstract: The key to effective alignment lies in high-quality preference data. Recent
research has focused on automated alignment, which involves developing
alignment systems with minimal human intervention. However, prior research has
predominantly focused on developing data generation methods, while insufficient
attention has been paid to quality control mechanisms, which often produce
inaccurate and unhelpful data, leading to unpredictable benefits during
iterative optimization. In this paper, we present Self-Steering Optimization
($SSO$), an algorithm that autonomously generates high-quality preference data,
eliminating manual annotation requirements. $SSO$ employs a specialized
optimization objective to build a data generator from the policy model itself,
which is used to produce accurate and on-policy data. We demonstrate $SSO$'s
effectiveness through comprehensive experiments on two series of models: Llama
3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$
consistently outperforms baselines in human preference alignment and reward
optimization. Further analysis validates $SSO$ as a scalable framework for
preference optimization, benefiting the advancement in automated alignment
techniques.

</details>


### [87] [Code-Switching Curriculum Learning for Multilingual Transfer in LLMs](https://arxiv.org/pdf/2411.02460)
*Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, Hwaran Lee*

Main category: cs.CL

TL;DR: The paper proposes Code-Switching Curriculum Learning (CSCL) to improve cross-lingual transfer in LLMs by mimicking human second language acquisition stages, showing significant gains in performance for languages like Korean, Japanese, and Indonesian.


<details>
  <summary>Details</summary>
Motivation: Address the performance drop of LLMs in low-resource languages due to imbalanced pre-training data by leveraging human-like code-switching techniques.

Method: CSCL progressively trains models with a curriculum: token-level code-switching, sentence-level code-switching, and monolingual corpora, tested on Qwen 2, Gemma 2, and Phi 3.5.

Result: CSCL significantly improves cross-lingual transfer, especially for low-resource languages, and mitigates spurious correlations between language resources and safety alignment.

Conclusion: CSCL offers a robust, efficient framework for equitable language transfer in LLMs, particularly beneficial for low-resource settings.

Abstract: Large language models (LLMs) now exhibit near human-level performance in
various tasks, but their performance drops drastically after a handful of
high-resource languages due to the imbalance in pre-training data. Inspired by
the human process of second language acquisition, particularly
code-switching$\unicode{x2014}$the practice of language alternation in a
conversation$\unicode{x2014}$we propose code-switching curriculum learning
(CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of
human language learning by progressively training models with a curriculum
consisting of 1) token-level code-switching, 2) sentence-level code-switching,
and 3) monolingual corpora. Using Qwen 2 as our underlying model, we
demonstrate the efficacy of the CSCL in improving language transfer to Korean,
achieving significant performance gains compared to monolingual continual
pre-training methods. Ablation studies reveal that both token- and
sentence-level code-switching significantly enhance cross-lingual transfer and
that curriculum learning amplifies these effects. We also extend our findings
into various languages, including Japanese (high-resource) and Indonesian
(low-resource), and using two additional models (Gemma 2 and Phi 3.5). We
further show that CSCL mitigates spurious correlations between language
resources and safety alignment, presenting a robust, efficient framework for
more equitable language transfer in LLMs. We observe that CSCL is effective for
low-resource settings where high-quality, monolingual corpora for language
transfer are hardly available.

</details>


### [88] [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/pdf/2411.12768)
*Nay Myat Min, Long H. Pham, Yige Li, Jun Sun*

Main category: cs.CL

TL;DR: CROW defends LLMs against backdoor attacks by enforcing internal consistency in hidden representations during finetuning, outperforming existing methods without needing clean models or trigger knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to backdoor attacks, and current defenses fail for text generation tasks.

Method: Proposes Internal Consistency Regularization (CROW), which enforces layer-wise consistency via adversarial perturbations and regularization during finetuning.

Result: CROW significantly reduces attack success rates across diverse backdoor strategies while maintaining generative performance.

Conclusion: CROW is an effective, architecture-agnostic defense for LLMs against backdoor attacks.

Abstract: Large Language Models (LLMs) are vulnerable to backdoor attacks that
manipulate outputs via hidden triggers. Existing defense methods--designed for
vision/text classification tasks--fail for text generation. We propose Internal
Consistency Regularization (CROW), a defense leveraging the observation that
backdoored models exhibit unstable layer-wise hidden representations when
triggered, while clean models show smooth transitions. CROW enforces
consistency across layers via adversarial perturbations and regularization
during finetuning, neutralizing backdoors without requiring clean reference
models or trigger knowledge--only a small clean dataset. Experiments across
Llama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's
effectiveness: it achieves significant reductions in attack success rates
across diverse backdoor strategies (sentiment steering, targeted refusal, code
injection) while preserving generative performance. CROW's
architecture-agnostic design enables practical deployment.

</details>


### [89] [Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning](https://arxiv.org/pdf/2411.17304)
*Milena Chadimová, Eduard Jurášek, Tomáš Kliegr*

Main category: cs.CL

TL;DR: The paper introduces 'hashing,' a method to mask bias-inducing words in LLMs, reducing cognitive biases and improving performance across various tasks and models.


<details>
  <summary>Details</summary>
Motivation: To address cognitive biases and reliance on external knowledge in LLMs by masking problematic terms.

Method: Masking bias-inducing words with hash-like identifiers, tested across 490 prompts and multiple LLMs (LLama, ChatGPT, etc.).

Result: Significant improvements in bias reduction and task performance, though hallucination rates varied by model.

Conclusion: Hashing effectively reduces biases and improves LLM performance, but its impact depends on the model and task.

Abstract: This paper introduces a novel method, referred to as "hashing", which
involves masking potentially bias-inducing words in large language models
(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and
reliance on external knowledge. The method was tested across three sets of
experiments involving a total of 490 prompts. Statistical analysis using
chi-square tests showed significant improvements in all tested scenarios, which
covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first
experiment, hashing decreased the fallacy rate in a modified version of the
"Linda" problem aimed at evaluating susceptibility to cognitive biases. In the
second experiment, it improved LLM results on the frequent itemset extraction
task. In the third experiment, we found hashing is also effective when the
Linda problem is presented in a tabular format rather than text, indicating
that the technique works across various input representations. Overall, the
method was shown to improve bias reduction and incorporation of external
knowledge. Despite bias reduction, hallucination rates were inconsistently
reduced across types of LLM models. These findings suggest that masking
bias-inducing terms can improve LLM performance, although its effectiveness is
model- and task-dependent.

</details>


### [90] [Retrofitting Large Language Models with Dynamic Tokenization](https://arxiv.org/pdf/2411.18553)
*Darius Feher, Ivan Vulić, Benjamin Minixhofer*

Main category: cs.CL

TL;DR: Dynamic tokenization improves LM efficiency and fairness by reducing token sequence lengths by >20% with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Static tokenizers degrade efficiency and language capabilities, especially in non-English languages.

Method: Propose dynamic tokenization using a subword-merging algorithm and pre-trained embedding-prediction hypernetwork.

Result: Reduces token sequence lengths by >20% in 14 languages with <2% performance degradation.

Conclusion: Dynamic tokenization enhances inference speed and fairness, making LMs more adaptable and equitable.

Abstract: Current language models (LMs) use a fixed, static subword tokenizer. This
default choice typically results in degraded efficiency and language
capabilities, especially in languages other than English. To address this
issue, we challenge the static design and propose retrofitting LMs with dynamic
tokenization: a way to dynamically decide on token boundaries based on the
input text via a subword-merging algorithm inspired by byte-pair encoding. We
merge frequent subword sequences in a batch, then apply a pre-trained
embedding-prediction hypernetwork to compute the token embeddings on-the-fly.
For encoder-style models (e.g., XLM-R), this on average reduces token sequence
lengths by >20% across 14 languages while degrading performance by less than
2%. The same method applied to pre-filling and scoring in decoder-style models
(e.g., Mistral-7B) results in minimal performance degradation at up to 17%
reduction in sequence length. Overall, we find that dynamic tokenization can
mitigate the limitations of static tokenization by substantially improving
inference speed and promoting fairness across languages, enabling more
equitable and adaptable LMs.

</details>


### [91] [Steps are all you need: Rethinking STEM Education with Prompt Engineering](https://arxiv.org/pdf/2412.05023)
*Krishnasai Addala, Kabir Dev Paul Baghel, Navya Gupta, Rishitej Reddy Vyalla, Chhavi Kirtani, Avinash Anand, Rajiv Ratn Shah*

Main category: cs.CL

TL;DR: The paper explores improving Physics QA tasks using MoE models and analogical prompting, addressing LLM limitations like math ability and hallucination. It introduces Analogical CoT prompting for smaller models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of few-shot and Chain-of-Thought prompting in LLMs for Physics QA, such as poor math ability and hallucination.

Method: Uses a Mixture of Experts (MoE) model and analogical prompting, and proposes Analogical CoT prompting for smaller models.

Result: Shows improved performance over baseline LLMs and surveys the limits of prompting techniques.

Conclusion: Proposes Analogical CoT prompting to help smaller models leverage analogical prompting, addressing their lack of specialist training data.

Abstract: Few shot and Chain-of-Thought prompting have shown promise when applied to
Physics Question Answering Tasks, but are limited by the lack of mathematical
ability inherent to LLMs, and are prone to hallucination. By utilizing a
Mixture of Experts (MoE) Model, along with analogical prompting, we are able to
show improved model performance when compared to the baseline on standard LLMs.
We also survey the limits of these prompting techniques and the effects they
have on model performance. Additionally, we propose Analogical CoT prompting, a
prompting technique designed to allow smaller, open source models to leverage
Analogical prompting, something they have struggled with, possibly due to a
lack of specialist training data.

</details>


### [92] [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/pdf/2412.05342)
*Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: MuPaS is a multi-party fine-tuning framework for LLMs, improving their performance in multi-party dialogues (MPD) over traditional dyadic fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are fine-tuned for dyadic dialogues, limiting their effectiveness in multi-party scenarios like meetings or discussions.

Method: Introduces MuPaS, a framework for fine-tuning LLMs on multi-party dialogue datasets, with two training strategies for MPD simulation.

Result: MuPaS achieves state-of-the-art multi-party responses, better next-speaker prediction, and high-quality utterances, even in out-of-distribution scenarios.

Conclusion: MuPaS bridges LLM training with complex multi-party applications, enabling broader use in conversation generation and virtual environments.

Abstract: Large Language Models (LLM) are usually fine-tuned to participate in dyadic
or two-party dialogues, which can not adapt well to multi-party dialogues
(MPD), which hinders their applications in such scenarios including
multi-personal meetings, discussions and daily communication. Previous
LLM-based researches mainly focus on the multi-agent framework, while their
base LLMs are still pairwisely fine-tuned. In this work, we design a
multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue
datasets, and prove such a straightforward framework can let the LLM align with
the multi-party conversation style efficiently and effectively. We also design
two training strategies which can convert MuPaS into the MPD simulator.
Substantial experiments show that MuPaS can achieve state-of-the-art
multi-party response, higher accuracy of the-next-speaker prediction, higher
human and automatic evaluated utterance qualities, and can even generate
reasonably with out-of-distribution scene, topic and role descriptions. The
MuPaS framework bridges the LLM training with more complicated multi-party
applications, such as conversation generation, virtual rehearsal or
meta-universe.

</details>


### [93] [Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering](https://arxiv.org/pdf/2412.05453)
*Krishnasai Addala, Kabir Dev Paul Baghel, Dhruv Jain, Navya Gupta, Rishitej Reddy Vyalla, Chhavi Kirtani, Avinash Anand, Rajiv Ratn Shah*

Main category: cs.CL

TL;DR: The study uses LLM-generated knowledge graphs to decompose physics questions into sub-questions, improving fidelity and educational quality.


<details>
  <summary>Details</summary>
Motivation: To enhance model response quality in Question Answering tasks by leveraging knowledge graphs for logically consistent sub-question generation.

Method: Employing LLMs to construct knowledge graphs that guide sub-question generation, comparing results to traditional techniques.

Result: Sub-questions from knowledge graphs show significantly improved logical consistency with the original questions.

Conclusion: The approach enhances educational content quality and demonstrates LLMs' potential in transforming educational methodologies.

Abstract: This study explores the effectiveness of using knowledge graphs generated by
large language models to decompose high school-level physics questions into
sub-questions. We introduce a pipeline aimed at enhancing model response
quality for Question Answering tasks. By employing LLMs to construct knowledge
graphs that capture the internal logic of the questions, these graphs then
guide the generation of subquestions. We hypothesize that this method yields
sub-questions that are more logically consistent with the original questions
compared to traditional decomposition techniques. Our results show that
sub-questions derived from knowledge graphs exhibit significantly improved
fidelity to the original question's logic. This approach not only enhances the
learning experience by providing clearer and more contextually appropriate
sub-questions but also highlights the potential of LLMs to transform
educational methodologies. The findings indicate a promising direction for
applying AI to improve the quality and effectiveness of educational content.

</details>


### [94] [7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/pdf/2412.06845)
*Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang*

Main category: cs.CL

TL;DR: Moxin 7B is a fully open-source LLM addressing transparency and reproducibility issues in LLMs by releasing all components (code, data, checkpoints). It includes base, instruct, reasoning, and vision language models, achieving top performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: The commercialization of LLMs has raised concerns about transparency and reproducibility. Open-source LLMs often lack essential components like training code and data, hindering innovation.

Method: Developed Moxin 7B with full openness (code, data, checkpoints). Fine-tuned base model into instruct and reasoning models using SOTA frameworks and data. Also created a vision language model.

Result: Superior performance in zero-shot, few-shot, and chain-of-thought evaluations.

Conclusion: Moxin 7B demonstrates the feasibility and benefits of fully open-source LLMs, promoting transparency and innovation.

Abstract: Recently, Large Language Models (LLMs) have undergone a significant
transformation, marked by a rapid rise in both their popularity and
capabilities. Leading this evolution are proprietary LLMs like GPT-4 and
GPT-o1, which have captured widespread attention in the AI community due to
their remarkable performance and versatility. Simultaneously, open-source LLMs,
such as LLaMA, have made great contributions to the ever-increasing popularity
of LLMs due to the ease to customize and deploy the models across diverse
applications. Although open-source LLMs present unprecedented opportunities for
innovation and research, the commercialization of LLMs has raised concerns
about transparency, reproducibility, and safety. Many open-source LLMs fail to
meet fundamental transparency requirements by withholding essential components
like training code and data, which may hinder further innovations on LLMs. To
mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,
adhering to principles of open science, open source, open data, and open
access. We release the pre-training code and configurations, training and
fine-tuning datasets, and intermediate and final checkpoints, aiming to make
continuous commitments to fully open-source LLMs. After pre-training the base
model, we finetune the Moxin Base model with SOTA post-training framework and
instruction data to obtain Moxin Instruct model. To improve the reasoning
capability, we further finetune our Instruct model with chain-of-thought data
distilled from DeepSeek R1, and then use Group Relative Policy Optimization
(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin
Reasoning model. Moreover, we develop our vision language model based on our
Moxin model. Experiments show that our models achieve superior performance in
various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT
evaluation.

</details>


### [95] [Irony Detection, Reasoning and Understanding in Zero-shot Learning](https://arxiv.org/pdf/2501.16884)
*Peiling Yi, Yuhan Xia, Yunfei Long*

Main category: cs.CL

TL;DR: The paper addresses challenges in irony detection generalization and introduces the IDADP framework for LLMs to improve irony detection and reasoning.


<details>
  <summary>Details</summary>
Motivation: Overcoming dataset-specific limitations and enhancing irony detection models for diverse real-world scenarios.

Method: Using irony-focused prompts generated from the IDADP framework for LLMs to transform ironic text into its intended meaning.

Result: Improved irony detection and generation of coherent, human-readable reasoning.

Conclusion: Identifies future research directions like advancing contextual awareness, hybrid symbolic-neural methods, and multimodal data integration to enhance LLMs' irony detection capabilities.

Abstract: The generalisation of irony detection faces significant challenges, leading
to substantial performance deviations when detection models are applied to
diverse real-world scenarios. In this study, we find that irony-focused
prompts, as generated from our IDADP framework for LLMs, can not only overcome
dataset-specific limitations but also generate coherent, human-readable
reasoning, transforming ironic text into its intended meaning. Based on our
findings and in-depth analysis, we identify several promising directions for
future research aimed at enhancing LLMs' zero-shot capabilities in irony
detection, reasoning, and comprehension. These include advancing contextual
awareness in irony detection, exploring hybrid symbolic-neural methods, and
integrating multimodal data, among others.

</details>


### [96] [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/pdf/2502.05202)
*Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Gaurav Jain, Oren Pereg, Moshe Wasserblat, David Harel*

Main category: cs.CL

TL;DR: New speculative decoding methods remove the shared-vocabulary constraint, enabling any off-the-shelf model as a drafter, achieving up to 2.8x speedup without retraining.


<details>
  <summary>Details</summary>
Motivation: Accelerating LLM inference is crucial, but existing SD methods limit drafter selection due to vocabulary constraints.

Method: Three new SD methods are introduced, allowing any model as a drafter without shared vocabulary or retraining.

Result: Empirical tests show up to 2.8x speedup on tasks like summarization and programming.

Conclusion: This work expands SD's practicality by removing vocabulary constraints and enabling off-the-shelf models.

Abstract: Accelerating the inference of large language models (LLMs) is a critical
challenge in generative AI. Speculative decoding (SD) methods offer substantial
efficiency gains by generating multiple tokens using a single target forward
pass. However, existing SD approaches require the drafter and target models to
share the same vocabulary, thus limiting the pool of possible drafters, often
necessitating the training of a drafter from scratch. We present three new SD
methods that remove this shared-vocabulary constraint. All three methods
preserve the target distribution (i.e., they are lossless) and work with
off-the-shelf models without requiring additional training or modifications.
Empirically, on summarization, programming, and long-context tasks, our
algorithms demonstrate significant speedups of up to 2.8x over standard
autoregressive decoding. By enabling any off-the-shelf model to serve as a
drafter and requiring no retraining, this work substantially broadens the
applicability of the SD framework in practice.

</details>


### [97] [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/pdf/2502.16033)
*Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, Xin Eric Wang*

Main category: cs.CL

TL;DR: The paper introduces the MMIR benchmark to evaluate MLLMs' ability to detect and reason about inconsistencies in multimodal content, revealing gaps in current models' performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation for MLLMs in handling real-world inconsistencies in layout-rich content.

Method: Proposes the MMIR benchmark with 534 samples containing synthetic errors across five categories, evaluating six MLLMs.

Result: Models with dedicated multimodal reasoning outperform others, but open-source models struggle. Single-modality prompting shows limited gains.

Conclusion: Highlights the need for advanced multimodal reasoning and future research on inconsistency handling.

Abstract: Existing Multimodal Large Language Models (MLLMs) are predominantly trained
and tested on consistent visual-textual inputs, leaving open the question of
whether they can handle inconsistencies in real-world, layout-rich content. To
bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)
benchmark to assess MLLMs' ability to detect and reason about semantic
mismatches in artifacts such as webpages, presentation slides, and posters.
MMIR comprises 534 challenging samples, each containing synthetically injected
errors across five reasoning-heavy categories: Factual Contradiction, Identity
Misattribution, Contextual Mismatch, Quantitative Discrepancy, and
Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing
that models with dedicated multimodal reasoning capabilities, such as o1,
substantially outperform their counterparts while open-source models remain
particularly vulnerable to inconsistency errors. Detailed error analyses
further show that models excel in detecting pairwise inconsistencies but
struggle with inconsistencies confined to single elements in complex layouts.
Probing experiments reveal that single-modality prompting, including
Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,
revealing a key bottleneck in cross-modal reasoning. Our findings highlight the
need for advanced multimodal reasoning and point to future research on
multimodal inconsistency.

</details>


### [98] [Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation](https://arxiv.org/pdf/2502.19830)
*Yiwei Li, Ji Zhang, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Jiayi Shi, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

Main category: cs.CL

TL;DR: Self-consistency is reframed as a dynamic distributional alignment problem, with a proposed confidence-driven temperature calibration mechanism improving performance in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored dynamics behind self-consistency's efficacy and address the trade-off between high and low decoding temperatures.

Method: A confidence-driven mechanism dynamically calibrates temperature to sharpen sampling under uncertainty and promote exploration when confidence is high.

Result: Outperforms fixed-diversity baselines in mathematical reasoning tasks, improving average and best-case performance under limited samples.

Conclusion: Self-consistency is a synchronization challenge between sampling dynamics and evolving answer distributions, with dynamic temperature calibration offering a solution.

Abstract: Self-consistency improves reasoning by aggregating diverse stochastic
samples, yet the dynamics behind its efficacy remain underexplored. We reframe
self-consistency as a dynamic distributional alignment problem, revealing that
decoding temperature not only governs sampling randomness but also actively
shapes the latent answer distribution. Given that high temperatures require
prohibitively large sample sizes to stabilize, while low temperatures risk
amplifying biases, we propose a confidence-driven mechanism that dynamically
calibrates temperature: sharpening the sampling distribution under uncertainty
to align with high-probability modes, and promoting exploration when confidence
is high. Experiments on mathematical reasoning tasks show this approach
outperforms fixed-diversity baselines under limited samples, improving both
average and best-case performance across varying initial temperatures without
additional data or modules. This establishes self-consistency as a
synchronization challenge between sampling dynamics and evolving answer
distributions.

</details>


### [99] [AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification](https://arxiv.org/pdf/2503.01940)
*Xuan Zhang, Yongliang Shen, Zhe Zheng, Linjuan Wu, Wenqi Zhang, Yuchen Yan, Qiuying Peng, Jun Wang, Weiming Lu*

Main category: cs.CL

TL;DR: AskToAct improves tool learning by automating training data construction and adding error-correction mechanisms, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: User queries are often ambiguous, but current clarification methods rely on limited datasets and lack error correction, leading to inefficiencies.

Method: AskToAct maps queries to tool solutions, removes key parameters for automated data generation, and uses error-correction pairs and selective masking for robustness.

Result: It achieves 57% accuracy in intent recovery, 10.46% efficiency gain, and generalizes well to unseen APIs with fewer resources.

Conclusion: AskToAct is a scalable, efficient, and robust framework for interactive clarification in tool learning.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
tool learning. In real-world scenarios, user queries are often ambiguous and
incomplete, requiring effective clarification. However, existing interactive
clarification approaches face two critical limitations: reliance on manually
constructed datasets, which inherently constrains training data scale and
diversity, and lack of error correction mechanisms during multi-turn
clarification, leading to error accumulation that compromises both accuracy and
efficiency. We present AskToAct, which addresses these challenges by exploiting
the structural mapping between queries and their tool invocation solutions. Our
key insight is that tool parameters naturally represent explicit user intents.
By systematically removing key parameters from queries while retaining them as
ground truth, we enable automated construction of high-quality training data.
We further enhance model robustness through error-correction pairs and
selective masking, enabling dynamic error detection during clarification
interactions. Comprehensive experiments demonstrate that AskToAct significantly
outperforms existing approaches, achieving above 57% accuracy in recovering
critical unspecified intents and enhancing clarification efficiency by an
average of 10.46% while maintaining high accuracy in tool invocation. Our
framework exhibits robust performance across different model architectures and
successfully generalizes to entirely unseen APIs without additional training,
achieving performance comparable to GPT-4o with substantially fewer
computational resources.

</details>


### [100] [Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization](https://arxiv.org/pdf/2503.02450)
*Yilun Qiu, Xiaoyan Zhao, Yang Zhang, Yimeng Bai, Wenjie Wang, Hong Cheng, Fuli Feng, Tat-Seng Chua*

Main category: cs.CL

TL;DR: DPL introduces inter-user comparative analysis to enhance LLM personalization by extracting meaningful differences, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM personalization methods overlook inter-user differences, which are crucial for accurate preference modeling.

Method: DPL selects representative users for comparison and extracts task-relevant differences to customize LLM generation.

Result: Experiments show DPL significantly improves LLM personalization on real-world datasets.

Conclusion: DPL addresses a key limitation in LLM personalization by leveraging inter-user differences, offering a promising direction for future work.

Abstract: Personalizing Large Language Models (LLMs) has become a critical step in
facilitating their widespread application to enhance individual life
experiences. In pursuit of personalization, distilling key preference
information from an individual's historical data as instructional preference
context to customize LLM generation has emerged as a promising direction.
However, these methods face a fundamental limitation by overlooking the
inter-user comparative analysis, which is essential for identifying the
inter-user differences that truly shape preferences. To address this
limitation, we propose Difference-aware Personalization Learning (DPL), a novel
approach that emphasizes extracting inter-user differences to enhance LLM
personalization. DPL strategically selects representative users for comparison
and establishes a structured standard to extract meaningful, task-relevant
differences for customizing LLM generation. Extensive experiments on real-world
datasets demonstrate that DPL significantly enhances LLM personalization. We
release our code at https://github.com/SnowCharmQ/DPL.

</details>


### [101] [Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference](https://arxiv.org/pdf/2503.04793)
*Wenjie Qiu, Yi-Chen Li, Xuqin Zhang, Tianyi Zhang, Yihang Zhang, Zongzhang Zhang, Yang Yu*

Main category: cs.CL

TL;DR: The paper proposes an intermediate-grained reward model for aligning LLMs with human preferences by scoring sentences and aggregating them into response-level scores, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current reward models are coarse-grained or lack semantic clarity at the token level, hindering effective alignment of LLMs with human preferences.

Method: The approach segments responses into sentences, assigns scores to each, and uses a novel attention mechanism to aggregate sentence-level rewards into a response-level score.

Result: The method outperforms response-level reward models by 2.7% on RewardBench and surpasses baselines on AlpacaEval.

Conclusion: Sentence-level reward modeling improves alignment performance, offering a more effective approach than existing coarse or token-level methods.

Abstract: Learning reward models from human preference datasets and subsequently
optimizing language models via reinforcement learning has emerged as a
fundamental paradigm for aligning LLMs with human preferences. The performance
of the reward model plays a crucial role in the effectiveness of alignment.
Previous reward models operate at a coarse-grained level, requiring the
generation of a complete response to obtain a reward value. The sparse reward
may present challenges for downstream reinforcement learning. While recent
efforts have attempted to learn token-level reward models, the lack of explicit
semantic information makes it difficult to model the credit of every individual
token. In this paper, we propose assigning scores to every sentence,
introducing an intermediate-grained reward model. By segmenting the complete
response into sentences and applying differential operations to reward output
at the start and end positions of each sentence, we can effectively model the
rewards of sentences. Moreover, a novel attention mechanism is introduced to
aggregate the scores of all sentences into a response-level score, which allows
it to be trained using the Bradley-Terry model. On common benchmarks, our
method outperforms the response-level reward model by 2.7% on RewardBench (for
reward modeling evaluation) and surpasses all baselines on AlpacaEval (for
alignment evaluation).

</details>


### [102] [Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering](https://arxiv.org/pdf/2503.11314)
*Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, Zhiqiang Zhang*

Main category: cs.CL

TL;DR: The paper investigates whether long chain-of-thoughts (CoTs) reasoning is a general capability in LLMs, finding it distinct from vanilla CoTs and requiring domain-specific representations. It proposes GLoRE, a representation engineering method, which proves effective in experiments.


<details>
  <summary>Details</summary>
Motivation: To determine if long CoT reasoning is a general capability of LLMs and explore its transferability across tasks.

Method: Empirical analysis from a representation perspective, proposing GLoRE for enhancing long CoT reasoning.

Result: LLMs encode long CoT reasoning as a general capability, with GLoRE showing effectiveness in in-domain and cross-domain scenarios.

Conclusion: Long CoT reasoning is a general LLM capability, and GLoRE successfully enhances it, demonstrating broad applicability.

Abstract: Recent advancements in long chain-of-thoughts(long CoTs) have significantly
improved the reasoning capabilities of large language models(LLMs). Existing
work finds that the capability of long CoT reasoning can be efficiently
elicited by tuning on only a few examples and can easily transfer to other
tasks. This motivates us to investigate whether long CoT reasoning is a general
capability for LLMs. In this work, we conduct an empirical analysis for this
question from the perspective of representation. We find that LLMs do encode
long CoT reasoning as a general capability, with a clear distinction from
vanilla CoTs. Furthermore, domain-specific representations are also required
for the effective transfer of long CoT reasoning. Inspired by these findings,
we propose GLoRE, a novel representation engineering method to unleash the
general long CoT reasoning capabilities of LLMs. Extensive experiments
demonstrate the effectiveness and efficiency of GLoRE in both in-domain and
cross-domain scenarios.

</details>


### [103] [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/pdf/2503.18491)
*Shuo Yang, Siwen Luo, Soyeon Caren Han, Eduard Hovy*

Main category: cs.CL

TL;DR: MAGIC-VQA integrates commonsense knowledge with LVLMs for robust VQA, using explicit knowledge, post-processing, and GNN-based augmentation.


<details>
  <summary>Details</summary>
Motivation: LVLMs lack integrated commonsense knowledge, limiting VQA robustness in real-world scenarios.

Method: Three-stage process: explicit knowledge integration, by-type post-processing, and GNN-based implicit knowledge augmentation.

Result: Achieves state-of-the-art performance on benchmarks, improving commonsense reasoning in VQA.

Conclusion: MAGIC-VQA bridges the gap between commonsense knowledge and LVLM-driven reasoning without extensive pre-training.

Abstract: Visual Question Answering (VQA) requires reasoning across visual and textual
modalities, yet Large Vision-Language Models (LVLMs) often lack integrated
commonsense knowledge, limiting their robustness in real-world scenarios. To
address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by
systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs
a three-stage process: (1) Explicit Knowledge Integration from external
sources, (2) By-Type Post-Processing for contextual refinement, and (3)
Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for
structured reasoning. While GNNs bring greater depth to structured inference,
they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key
gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating
the need for extensive pre-training or complex prompt tuning. Our framework
achieves state-of-the-art performance on benchmark datasets, significantly
improving commonsense reasoning in VQA.

</details>


### [104] [Style over Substance: Distilled Language Models Reason Via Stylistic Replication](https://arxiv.org/pdf/2504.01738)
*Philip Lippmann, Jie Yang*

Main category: cs.CL

TL;DR: The study explores how distilled reasoning models internalize stylistic patterns from reasoning traces, finding that surface-level patterns significantly influence performance.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which distilled models replicate stylistic patterns in reasoning traces and how these patterns impact reasoning capabilities.

Method: Systematic analysis of reasoning traces, creation of two datasets (emergent and synthetic), and evaluation of models trained on synthetic traces.

Result: Models trained on synthetic traces perform comparably, and performance even improves with altered (incorrect) traces, showing reliance on surface-level patterns.

Conclusion: Stylistic patterns can efficiently enhance reasoning in language models, even when the reasoning itself is flawed.

Abstract: Specialized reasoning language models (RLMs) have demonstrated that scaling
test-time computation through detailed reasoning traces significantly enhances
performance. Although these traces effectively facilitate knowledge
distillation into smaller, instruction-tuned models, the precise nature of
transferred reasoning remains unclear. In this study, we investigate to what
extent distilled models internalize replicated stylistic patterns during
reasoning. To this end, we systematically analyze reasoning traces, identifying
structural and lexical patterns that characterize successful reasoning. We then
introduce two new datasets -- a dataset of emergent reasoning traces and a
synthetic dataset explicitly constructed to replicate these stylistic patterns
-- to precisely examine their influence on distilled models' reasoning
capabilities. We find that models trained on the synthetic traces achieve
comparable performance, indicating that distilled reasoning abilities rely
significantly on surface-level patterns. Surprisingly, we observe an increase
in performance even when the synthetic traces are altered to lead to the wrong
answer. Our findings highlight how stylistic patterns can be leveraged to
efficiently enhance LM reasoning across diverse model families.

</details>


### [105] [One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image](https://arxiv.org/pdf/2504.02132)
*Ezzeldin Shereen, Dan Ristea, Shae McFadden, Burak Hasircioglu, Vasilios Mavroudis, Chris Hicks*

Main category: cs.CL

TL;DR: The paper introduces poisoning attacks on multi-modal retrieval augmented generation (M-RAG) systems, demonstrating how adversarial images in the knowledge base can cause denial-of-service or spread misinformation.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in M-RAG systems, which rely on factual knowledge bases to prevent hallucinations in large multi-modal models, by showing how adversaries can exploit these systems.

Method: Proposes two attacks: a universal attack causing DoS and a targeted attack spreading misinformation, using a multi-objective gradient-based adversarial approach to craft injected images.

Result: Evaluated on visual document retrieval datasets, the attacks proved effective against various state-of-the-art retrievers and generators, even with defenses in place.

Conclusion: The study highlights the susceptibility of M-RAG systems to poisoning attacks, emphasizing the need for robust defenses against such adversarial manipulations.

Abstract: Multi-modal retrieval augmented generation (M-RAG) is instrumental for
inhibiting hallucinations in large multi-modal models (LMMs) through the use of
a factual knowledge base (KB). However, M-RAG introduces new attack vectors for
adversaries that aim to disrupt the system by injecting malicious entries into
the KB. In this paper, we present the first poisoning attack against M-RAG
targeting visual document retrieval applications where the KB contains images
of document pages. We propose two attacks, each of which require injecting only
a single adversarial image into the KB. Firstly, we propose a universal attack
that, for any potential user query, influences the response to cause a
denial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted
attack against one or a group of user queries, with the goal of spreading
targeted misinformation. For both attacks, we use a multi-objective
gradient-based adversarial approach to craft the injected image while
optimizing for both retrieval and generation. We evaluate our attacks against
several visual document retrieval datasets, a diverse set of state-of-the-art
retrievers (embedding models) and generators (LMMs), demonstrating the attack
effectiveness in both the universal and targeted settings. We additionally
present results including commonly used defenses, various attack
hyper-parameter settings, ablations, and attack transferability.

</details>


### [106] [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/pdf/2504.04745)
*Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: LLMs perform worse with AMR for short contexts but improve for long contexts like dialogue summarization. Larger LLMs benefit more, and AMR reconstruction is effective.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs use structured linguistic representations (AMR) for context in language tasks.

Method: Analyzed AMR encoding in short/long contexts using quantized and instruction-tuned LLMs (Llama 3.1, Phi-3, Mistral 7B).

Result: AMR degrades performance for short contexts but boosts it for long contexts (e.g., SAMSum dataset). Larger LLMs show more improvement. AMR reconstruction achieves 81% similarity.

Conclusion: AMR is beneficial for long-context tasks in advanced LLMs but not for short contexts or smaller models.

Abstract: This paper evaluates the ability of Large Language Models (LLMs) to leverage
contextual information in the form of structured linguistic representations.
Specifically, we examine the impact of encoding both short and long contexts
using Abstract Meaning Representation (AMR) structures across a diverse set of
language tasks. We perform our analysis using 8-bit quantized and
instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our
results indicate that, for tasks involving short contexts, augmenting the
prompt with the AMR of the original language context often degrades the
performance of the underlying LLM. However, for tasks that involve long
contexts, such as dialogue summarization in the SAMSum dataset, this
enhancement improves LLM performance, for example, by increasing the zero-shot
cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more
evident in the newer and larger LLMs, but does not extend to the older or
smaller ones. In addition, we observe that LLMs can effectively reconstruct the
original text from a linearized AMR, achieving a cosine similarity of 81% in
the best-case scenario.

</details>


### [107] [Assessment of Evolving Large Language Models in Upper Secondary Mathematics](https://arxiv.org/pdf/2504.12347)
*Mika Setälä, Pieta Sikström, Ville Heilala, Tommi Kärkkäinen*

Main category: cs.CL

TL;DR: LLMs' mathematical reasoning improves over time, achieving near-perfect scores on a high-stakes Finnish exam, showcasing their potential in education.


<details>
  <summary>Details</summary>
Motivation: To evaluate the evolving mathematical capabilities of LLMs using a rigorous educational benchmark.

Method: Testing various LLMs on the Finnish matriculation examination, a high-stakes digital test for upper secondary education.

Result: Initial moderate performance improved to near-perfect or perfect scores, matching top student performance.

Conclusion: LLMs show rapid advancements in mathematical proficiency, indicating their potential as educational tools.

Abstract: Large language models (LLMs) have shown increasing promise in educational
settings, yet their mathematical reasoning has been considered evolving. This
study evaluates the mathematical capabilities of various LLMs using the Finnish
matriculation examination, a high-stakes digital test for upper secondary
education. Initial tests yielded moderate performance corresponding to
mid-range grades, but later evaluations demonstrated substantial improvements
as the language models evolved. Remarkably, some models achieved near-perfect
or perfect scores, matching top student performance and qualifying for
university admission. Our findings highlight the rapid advances in the
mathematical proficiency of LLMs and illustrate their potential as underlying
tools to support learning and teaching in a variety of ways.

</details>


### [108] [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/pdf/2504.12663)
*Xiaotian Zhang, Ruizhe Chen, Yang Feng, Zuozhu Liu*

Main category: cs.CL

TL;DR: Persona-judge introduces a training-free method for aligning language models with human preferences by leveraging intrinsic model judgments, avoiding costly reward signals and annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning language models with human preferences are limited by scalability and computational costs due to reliance on reward signals and annotated data.

Method: Persona-judge uses a discriminative paradigm where a draft model generates tokens based on preferences, and a judge model cross-validates them, eliminating the need for external rewards.

Result: Experiments show Persona-judge is scalable and computationally efficient, enabling personalized alignment without additional training.

Conclusion: Persona-judge provides an adaptive and efficient solution for personalized alignment, advancing customized language model applications.

Abstract: Aligning language models with human preferences presents significant
challenges, particularly in achieving personalization without incurring
excessive computational costs. Existing methods rely on reward signals and
additional annotated data, limiting their scalability and adaptability to
diverse human values. To address these challenges, we introduce Persona-judge,
a novel discriminative paradigm that enables training-free personalized
alignment with unseen preferences. Instead of optimizing policy parameters
through external reward feedback, Persona-judge leverages the intrinsic
preference judgment capabilities of the model. Specifically, a draft model
generates candidate tokens conditioned on a given preference, while a judge
model, embodying another preference, cross-validates the predicted tokens
whether to be accepted. Experimental results demonstrate that Persona-judge,
using the inherent preference evaluation mechanisms of the model, offers a
scalable and computationally efficient solution to personalized alignment,
paving the way for more adaptive customized alignment. Our code is available
here.

</details>


### [109] [Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items](https://arxiv.org/pdf/2505.01015)
*Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces the Value Portrait benchmark to evaluate language models' value orientations, addressing biases in existing benchmarks by using real-life interactions and psychometric validation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for language models are biased and lack real-world relevance, necessitating a more authentic and reliable evaluation framework.

Method: The Value Portrait benchmark uses real-life user-LLM interactions and psychometric validation via human ratings correlated with their actual value scores.

Result: Evaluation of 44 LLMs shows prioritization of Benevolence, Security, and Self-Direction values, with biases in demographic group perceptions.

Conclusion: The benchmark provides a reliable, real-world-aligned method for assessing LLM values, revealing biases and value emphases.

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage. Second, each item is rated by human
subjects based on its similarity to their own thoughts, and correlations
between these ratings and the subjects' actual value scores are derived. This
psychometrically validated approach ensures that items strongly correlated with
specific values serve as reliable items for assessing those values. Through
evaluating 44 LLMs with our benchmark, we find that these models prioritize
Benevolence, Security, and Self-Direction values while placing less emphasis on
Tradition, Power, and Achievement values. Also, our analysis reveals biases in
how LLMs perceive various demographic groups, deviating from real human data.

</details>


### [110] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/pdf/2505.06987)
*Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: The paper introduces straQ*, a Q-learning-based framework for emotional support conversation (ESC) to improve long-term satisfaction by optimizing LLM strategies.


<details>
  <summary>Details</summary>
Motivation: Existing ESC studies using LLMs lack a state model perspective, leading to suboptimal long-term solutions.

Method: Leverages Q-learning on LLMs for planning and strategy optimization in ESC.

Result: straQ* outperforms baselines like direct inference, self-refine, and finite state machines.

Conclusion: straQ* provides a robust solution for ESC by integrating Q-learning with LLMs for better long-term outcomes.

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.

</details>


### [111] [Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective](https://arxiv.org/pdf/2505.07859)
*Daniel Franzen, Jan Disselhoff, David Hartmann*

Main category: cs.CL

TL;DR: The paper introduces a method to improve LLMs' performance on the ARC-AGI benchmark using task-specific data augmentation and a depth-first search algorithm, achieving state-of-the-art results with low inference costs.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LLMs in abstract reasoning, particularly on the challenging ARC-AGI benchmark.

Method: Leverage task-specific data augmentations and a depth-first search algorithm for solution generation and scoring, using the LLM as both generator and scorer.

Result: Achieves 71.6% (286.5/400 tasks) on ARC-AGI, with low inference cost (~2ct per task).

Conclusion: The method offers transparency, reproducibility, and cost-efficiency, distinguishing it from closed-source alternatives.

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge
for large language models (LLMs), exposing limitations in their abstract
reasoning abilities. In this work, we leverage task-specific data augmentations
throughout the training, generation, and scoring phases, and employ a
depth-first search algorithm to generate diverse, high-probability candidate
solutions. Furthermore, we utilize the LLM not only as a generator but also as
a scorer, using its output probabilities to select the most promising
solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the
public ARC-AGI evaluation set, demonstrating state-of-the-art performance among
publicly available approaches. While concurrent closed-source work has reported
higher scores, our method distinguishes itself through its transparency,
reproducibility, and remarkably low inference cost, averaging only around 2ct
per task on readily available hardware (we assume a price of 36ct/hour for a
Nvidia 4090 GPU).

</details>


### [112] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/pdf/2505.13990)
*Tingfeng Hui, Pengyu Zhu, Bowen Ping, Ling Tang, Guanting Dong, Yaqi Zhang, Sen Su*

Main category: cs.CL

TL;DR: DecIF is a fully autonomous framework that generates diverse, high-quality instruction-following data using LLMs, outperforming existing methods in flexibility and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for instruction-following in LLMs rely on pre-existing documents, limiting flexibility and generalizability. DecIF addresses this by autonomously generating data.

Method: DecIF uses meta-decomposition to guide LLMs in generating instructions and responses. It iteratively produces meta-information, combines it with constraints, and resolves inconsistencies. Responses are validated using atomic-level criteria.

Result: DecIF demonstrates superior performance in instruction-following tasks, with strong flexibility, scalability, and generalizability in generating high-quality data.

Conclusion: DecIF is an effective, autonomous solution for synthesizing instruction-following data, enhancing LLM capabilities without external dependencies.

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [113] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/pdf/2505.16234)
*Wei Zhang, Zhenhong Zhou, Kun Wang, Junfeng Fang, Yuanhe Zhang, Rui Wang, Ge Zhang, Xavier Li, Li Sun, Lingjuan Lyu, Yang Liu, Sen Su*

Main category: cs.CL

TL;DR: LIFEBench evaluates LLMs' ability to follow length instructions, revealing most models struggle with longer outputs despite claims, with reasoning LLMs performing best.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook length constraints in LLM outputs, despite their importance for practical applications.

Method: Introduces LIFEBench, a benchmark with 10,800 instances across 4 tasks and lengths from 16 to 8192 words, evaluating 26 LLMs.

Result: Most models perform well with short lengths but fail at longer outputs, even long-context LLMs. Reasoning LLMs outperform others.

Conclusion: LIFEBench highlights LLMs' limitations in following length instructions, providing insights for future improvements.

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [114] [Discovering Forbidden Topics in Language Models](https://arxiv.org/pdf/2505.17441)
*Can Rager, Chris Wendler, Rohit Gandikota, David Bau*

Main category: cs.CL

TL;DR: The paper introduces refusal discovery, a task to identify topics a language model refuses to discuss, and proposes Iterated Prefill Crawler (IPC) for this purpose. It benchmarks IPC on various models, revealing censorship patterns and alignment failures.


<details>
  <summary>Details</summary>
Motivation: To uncover biases, boundaries, and alignment issues in AI systems by identifying topics they refuse to discuss.

Method: Develops IPC, a method using token prefilling to discover forbidden topics, and tests it on multiple models, including Tulu-3-8B, Claude-Haiku, and variants of Llama-3.3-70B.

Result: IPC retrieves 31 out of 36 topics in Tulu-3-8B, detects censorship tuning in DeepSeek-R1-70B, and elicits CCP-aligned refusals in Perplexity-R1-1776-70B.

Conclusion: Refusal discovery is crucial for detecting AI biases and alignment failures, as demonstrated by IPC's effectiveness across diverse models.

Abstract: Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses
token prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawler to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits "thought suppression" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.

</details>


### [115] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/pdf/2505.20354)
*Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li*

Main category: cs.CL

TL;DR: The paper addresses data leakage and evaluation issues in protein-text models, proposing a retrieval-enhanced method for improved protein-to-text generation.


<details>
  <summary>Details</summary>
Motivation: To tackle data leakage in benchmarks and inadequate NLP metrics for protein-text models.

Method: Reorganizes datasets and introduces a biological entity-based evaluation framework, proposing a retrieval-enhanced method.

Result: The retrieval-enhanced method outperforms fine-tuned LLMs in protein-to-text generation, especially in training-free scenarios.

Conclusion: The proposed framework and method improve accuracy and efficiency in protein-text understanding and generation.

Abstract: In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.

</details>


### [116] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/pdf/2505.24593)
*Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu*

Main category: cs.CL

TL;DR: The paper proposes a cross-level attribution algorithm to analyze sparse Mixture-of-Experts (MoE) models, revealing efficiency patterns, expert roles, and robustness insights.


<details>
  <summary>Details</summary>
Motivation: Interpretability of heterogeneous MoE models is underexplored, and existing methods fail to capture dynamic routing-expert interactions.

Method: A cross-level attribution algorithm is applied to sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) and compared to dense models.

Result: MoE models show 37% higher efficiency via a 'mid-activation, late-amplification' pattern, with shared and routed experts handling general and specialized tasks, respectively. Robustness varies with depth.

Conclusion: The study advances MoE interpretability, providing design principles for balancing efficiency, specialization, and robustness.

Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in sparse
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.

</details>


### [117] [Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards](https://arxiv.org/pdf/2506.00103)
*Ruipeng Jia, Yunyi Yang, Yongbo Gai, Kai Luo, Shihao Huang, Jianhe Lin, Xiaoxi Jiang, Guanjun Jiang*

Main category: cs.CL

TL;DR: The paper introduces RLVR-based training for non-verifiable tasks like creative writing, using a pairwise Generative Reward Model (GenRM) and Bootstrapped Relative Policy Optimization (BRPO) to avoid reward hacking and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Address the gap in reinforcement learning for non-verifiable tasks (e.g., creative writing) where subjective quality assessment lacks definitive references, overcoming limitations of scalar reward models.

Method: Proposes a writing-principle-based pairwise GenRM and BRPO algorithm to transform subjective assessments into verifiable rewards and enable dynamic, reference-free comparisons during RL training.

Result: Demonstrates consistent improvement and resistance to reward hacking in Writing-Zero, achieving competitive results on writing benchmarks.

Conclusion: Suggests unifying rule-based, reference-based, and reference-free reward modeling under RLVR for a scalable RL training paradigm applicable to all language tasks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has enabled large
language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks
with objective ground-truth answers, such as mathematics and code generation.
However, a significant gap remains for non-verifiable tasks, like creative
writing and open-ended dialogue, where quality assessment is inherently
subjective and lacks definitive references. Existing approaches for these
domains often rely on scalar reward models trained with human preferences,
which suffer from limited generalization and are prone to reward hacking, such
as over-explanation and length bias. In this work, we propose a unified
RLVR-based training paradigm that bridges the gap between non-verifiable tasks
and verifiable rewards. We introduce a writing-principle-based pairwise
Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy
Optimization (BRPO) algorithm. The pairwise writing GenRM leverages
self-principled critique to transform subjective assessments into reliable,
verifiable rewards, while BRPO enables dynamic, reference-free pairwise
comparison by leveraging a bootstrapped response as temporary reference from
within group rollouts during RL training. Our approach empowers LLMs to develop
robust writing capabilities without supervised fine-tuning, as demonstrated by
Writing-Zero, which shows consistent improvement and strong resistance to
reward hacking compared to scalar reward baselines. Furthermore, our method
achieves competitive results on both in-house and open-source writing
benchmarks. Our findings suggest the potential to unify rule-based,
reference-based, and reference-free reward modeling under the RLVR framework,
thus paving the way for a comprehensive and scalable RL training paradigm
applicable across all language tasks.

</details>


### [118] [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/pdf/2506.01687)
*Anya Sims, Thom Foster, Klara Kaleb, Tuan-Duy H. Nguyen, Joseph Lee, Jakob N. Foerster, Yee Whye Teh, Cong Lu*

Main category: cs.CL

TL;DR: StochasTok is a stochastic tokenization method that improves subword-level understanding in LLMs without high computational costs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with subword-level tasks due to tokenization obscuring word structure, and existing solutions are costly or inconsistent.

Method: Introduces StochasTok, a stochastic tokenization scheme that randomly splits tokens during training to expose internal word structure.

Result: Pretraining with StochasTok enhances LLM performance on subword-level tasks like character counting and substring identification. Post-training also improves existing models.

Conclusion: StochasTok offers a simple, efficient solution for better subword understanding in LLMs, with potential for broader applications.

Abstract: Subword-level understanding is integral to numerous tasks, including
understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,
and wordplay. Despite this, current large language models (LLMs) still often
struggle with seemingly simple subword-level tasks like How many 'r's in
'strawberry'?. A key factor behind these failures is tokenization which
obscures the fine-grained structure of words. Current alternatives, such as
character-level and dropout tokenization methods, significantly increase
computational costs and provide inconsistent improvements. In this paper we
revisit tokenization and introduce StochasTok, a simple, efficient stochastic
tokenization scheme that randomly splits tokens during training, allowing LLMs
to 'see' their internal structure. Our experiments show that pretraining with
StochasTok substantially improves LLMs' downstream performance across multiple
subword-level language games, including character counting, substring
identification, and math tasks. Furthermore, StochasTok's simplicity allows
seamless integration at any stage of the training pipeline; and we demonstrate
that post-training with StochasTok can instill improved subword understanding
into existing pretrained models, thus avoiding costly pretraining from scratch.
These dramatic improvements achieved with a minimal change suggest StochasTok
holds exciting potential when applied to larger, more capable models. Code
open-sourced at: https://github.com/anyasims/stochastok.

</details>


### [119] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/pdf/2506.02404)
*Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qian-wen Zhang, Di Yin, Xing Sun, Xiao Huang*

Main category: cs.CL

TL;DR: GraphRAG-Bench is introduced to rigorously evaluate GraphRAG models with challenging, domain-specific questions, diverse tasks, and a holistic evaluation framework, revealing insights into graph architectures and reasoning improvements.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of GraphRAG models are limited in scope and fail to assess reasoning capacity comprehensively, necessitating a more rigorous benchmark.

Method: GraphRAG-Bench features college-level, domain-specific questions requiring multi-hop reasoning, diverse task types, and a holistic evaluation framework covering the entire GraphRAG pipeline.

Result: Application of nine GraphRAG methods to GraphRAG-Bench quantifies reasoning improvements and provides insights into graph architectures and retrieval efficacy.

Conclusion: GraphRAG-Bench offers a comprehensive evaluation tool for GraphRAG models, highlighting the impact of graph-based structuring on reasoning capabilities and guiding future research.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [120] [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/pdf/2506.03949)
*Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu*

Main category: cs.CL

TL;DR: The paper introduces TableEval, a benchmark for evaluating LLMs on realistic TableQA tasks, addressing limitations of existing benchmarks like simplicity and monolingual focus. It includes diverse table structures, multilingual data, and proposes SEAT for semantic accuracy evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing TableQA benchmarks are limited by simple flat tables, monolingual data, and data leakage, failing to capture real-world complexities like diverse structures and multilingual scenarios.

Method: TableEval is introduced with diverse table structures (concise, hierarchical, nested) from four domains and three languages. SEAT, a new evaluation framework, assesses semantic accuracy at the sub-question level.

Result: SEAT shows high agreement with human judgment. Experiments reveal gaps in state-of-the-art LLMs' ability to handle complex TableQA tasks.

Conclusion: TableEval and SEAT address critical limitations in TableQA evaluation, highlighting LLMs' shortcomings and providing insights for future improvements.

Abstract: LLMs have shown impressive progress in natural language processing. However,
they still face significant challenges in TableQA, where real-world
complexities such as diverse table structures, multilingual data, and
domain-specific reasoning are crucial. Existing TableQA benchmarks are often
limited by their focus on simple flat tables and suffer from data leakage.
Furthermore, most benchmarks are monolingual and fail to capture the
cross-lingual and cross-domain variability in practical applications. To
address these limitations, we introduce TableEval, a new benchmark designed to
evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes
tables with various structures (such as concise, hierarchical, and nested
tables) collected from four domains (including government, finance, academia,
and industry reports). Besides, TableEval features cross-lingual scenarios with
tables in Simplified Chinese, Traditional Chinese, and English. To minimize the
risk of data leakage, we collect all data from recent real-world documents.
Considering that existing TableQA metrics fail to capture semantic accuracy, we
further propose SEAT, a new evaluation framework that assesses the alignment
between model responses and reference answers at the sub-question level.
Experimental results have shown that SEAT achieves high agreement with human
judgment. Extensive experiments on TableEval reveal critical gaps in the
ability of state-of-the-art LLMs to handle these complex, real-world TableQA
tasks, offering insights for future improvements. We make our dataset available
here: https://github.com/wenge-research/TableEval.

</details>


### [121] [Context Is Not Comprehension: Unmasking LLM reasoning blind spots with VLO](https://arxiv.org/pdf/2506.04907)
*Alex Pan, Mary-Anne Williams*

Main category: cs.CL

TL;DR: The paper introduces Verbose ListOps (VLO), a benchmark to test LLMs' ability to handle nested reasoning, showing current models fail despite excelling at simpler tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations focus on explicit fact recall, ignoring the complexity of human-like nested reasoning. VLO addresses this gap.

Method: VLO programmatically embeds nested computations into stories, requiring models to track internal state, not just locate explicit values.

Result: Leading LLMs perform poorly on VLO (10k tokens), despite near-perfect accuracy on simpler ListOps tasks.

Conclusion: VLO's extensible framework is a critical tool for testing advanced reasoning architectures, essential for automating knowledge work.

Abstract: The dominant evaluation of Large Language Models has centered on their
ability to surface explicit facts from increasingly vast contexts. While
today's best models demonstrate near-perfect recall on these tasks, this
apparent success is overly simplistic and non-representative of the complexity
of human reasoning which is often highly nested. We introduce Verbose ListOps
(VLO), a novel benchmark designed to isolate this failure. VLO programmatically
weaves deterministic, nested computations into coherent stories, forcing models
to track and update internal state rather than simply locate explicit values.
Our experiments show that leading LLMs, capable of solving the raw ListOps
equations with near-perfect accuracy, collapse in performance on VLO at just
10k tokens. The extensibility of VLO's generation framework to any verifiable
reasoning pattern will be a critical tool, enabling model developers to move
beyond context windows and robustly test new reasoning architectures; a
necessary step to automating the world's knowledge work.

</details>


### [122] [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/pdf/2506.05176)
*Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: The Qwen3 Embedding series improves text embedding and reranking using Qwen3 foundation models, offering multilingual capabilities and diverse model sizes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To advance text embedding and reranking by leveraging Qwen3 LLMs' multilingual understanding and generation, addressing diverse deployment needs.

Method: Multi-stage training pipeline combining unsupervised pre-training and supervised fine-tuning, with model merging for robustness. Qwen3 LLMs synthesize diverse training data.

Result: State-of-the-art performance on benchmarks like MTEB, excelling in multilingual and retrieval tasks. Models are publicly available.

Conclusion: The Qwen3 Embedding series is a robust, adaptable solution for embedding and reranking, with strong multilingual performance and open availability.

Abstract: In this work, we introduce the Qwen3 Embedding series, a significant
advancement over its predecessor, the GTE-Qwen series, in text embedding and
reranking capabilities, built upon the Qwen3 foundation models. Leveraging the
Qwen3 LLMs' robust capabilities in multilingual text understanding and
generation, our innovative multi-stage training pipeline combines large-scale
unsupervised pre-training with supervised fine-tuning on high-quality datasets.
Effective model merging strategies further ensure the robustness and
adaptability of the Qwen3 Embedding series. During the training process, the
Qwen3 LLMs serve not only as backbone models but also play a crucial role in
synthesizing high-quality, rich, and diverse training data across multiple
domains and languages, thus enhancing the training pipeline. The Qwen3
Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both
embedding and reranking tasks, addressing diverse deployment scenarios where
users can optimize for either efficiency or effectiveness. Empirical
evaluations demonstrate that the Qwen3 Embedding series achieves
state-of-the-art results across diverse benchmarks. Notably, it excels on the
multilingual evaluation benchmark MTEB for text embedding, as well as in
various retrieval tasks, including code retrieval, cross-lingual retrieval and
multilingual retrieval. To facilitate reproducibility and promote
community-driven research and development, the Qwen3 Embedding models are
publicly available under the Apache 2.0 license.

</details>


### [123] [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/pdf/2506.05387)
*Jaydip Sen, Saptarshi Sengupta, Subhasis Dasgupta*

Main category: cs.CL

TL;DR: ASTS improves LTS for LLMs by adding dynamic entropy thresholding and multi-objective scoring, outperforming traditional methods in fluency, diversity, and coherence.


<details>
  <summary>Details</summary>
Motivation: Address limitations of top-k and nucleus sampling in balancing fluency, diversity, and coherence in text generation.

Method: Proposes ASTS, an enhanced LTS algorithm with dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments.

Result: ASTS reduces repetition, enhances semantic alignment, and improves fluency, outperforming existing methods in benchmarks.

Conclusion: ASTS is a superior decoding strategy for LLMs, offering better coherence, diversity, and efficiency.

Abstract: This chapter explores advancements in decoding strategies for large language
models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)
algorithm. Traditional decoding methods, such as top-k and nucleus sampling,
often struggle to balance fluency, diversity, and coherence in text generation.
To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)
is proposed as an improved version of LTS, incorporating dynamic entropy
thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS
ensures contextually coherent and diverse text generation while maintaining
computational efficiency. Its performance is evaluated across multiple
benchmarks, including story generation and abstractive summarization, using
metrics such as perplexity, MAUVE, and diversity scores. Experimental results
demonstrate that ASTS outperforms existing sampling techniques by reducing
repetition, enhancing semantic alignment, and improving fluency.

</details>


### [124] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/pdf/2506.06395)
*Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, Ivan Oseledets*

Main category: cs.CL

TL;DR: RLSC uses a model's self-confidence as reward signals for post-training, improving accuracy on math benchmarks without human annotations or external rewards.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs rely on costly human annotations or external reward models, which RLSC aims to eliminate.

Method: RLSC leverages the model's own confidence as reward signals, requiring minimal samples and no labels.

Result: RLSC improves accuracy by up to +21.7% on math benchmarks with only 16 samples per question and 10-20 training steps.

Conclusion: RLSC offers a simple, scalable post-training method for LLMs, reducing dependency on external supervision.

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,
RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on
Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a
simple, scalable post-training method for inference models, requiring only a
small number of samples and unlabelled supervision.

</details>


### [125] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/pdf/2506.06821)
*Yuhan Cao, Zian Chen, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He*

Main category: cs.CL

TL;DR: The paper explores LLMs' ability to generate test case generators for code checking, introduces TCGBench for benchmarking, and finds LLMs struggle with targeted bug-exposing test cases, though performance improves with curated datasets.


<details>
  <summary>Details</summary>
Motivation: To investigate the unexplored potential of LLMs in generating test case generators for code checking, particularly in competition-level programming.

Method: Proposes TCGBench, a benchmark with two tasks: generating valid test case generators and targeted ones to expose bugs. Tests state-of-the-art LLMs and analyzes their performance.

Result: LLMs can generate valid test case generators but struggle with targeted ones. Performance improves with a manually curated dataset.

Conclusion: LLMs show promise in test case generation but need enhancement for targeted bug detection, with curated datasets aiding performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [126] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044)
*LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong*

Main category: cs.CL

TL;DR: The paper addresses limitations of Multimodal Large Language Models (MLLMs) in medical applications by proposing a data curation procedure, introducing the medical-specialized MLLM Lingshu, and developing MedEvalKit for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing medical MLLMs lack coverage of medical knowledge beyond imaging, are prone to hallucinations, and lack reasoning for complex medical scenarios.

Method: Proposes a data curation procedure for rich medical knowledge, introduces Lingshu (a medical-specialized MLLM), and uses reinforcement learning for reasoning. Develops MedEvalKit for evaluation.

Result: Lingshu outperforms existing open-source multimodal models on tasks like multimodal QA, text-based QA, and medical report generation.

Conclusion: The proposed approach enhances medical MLLMs by addressing data and reasoning limitations, demonstrating superior performance in medical tasks.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [127] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/pdf/2506.07664)
*Lei Xu, Sirui Chen, Yuxuan Huang, Chaochao Lu*

Main category: cs.CL

TL;DR: Proposes a method to enhance LLM mathematical reasoning by generating structured problem-solving code and labeled intermediate steps, creating a high-quality dataset and benchmark.


<details>
  <summary>Details</summary>
Motivation: Challenges in LLM mathematical reasoning due to complex logic and precise computation needs; existing methods lack quality and complexity.

Method: Extracts structural information with generated problem-solving code to guide data generation, applied to MATH and GSM8K datasets.

Result: Produces 39K problems with labeled steps and a 6.1K-problem benchmark; model performance declines with longer reasoning. Fine-tuning validates dataset effectiveness.

Conclusion: The method and dataset improve LLM reasoning, with potential for future research. Code and data are publicly available.

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities. Our code
and data are available at https://github.com/OpenCausaLab/StructuralGeneration.

</details>


### [128] [AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/pdf/2506.07751)
*Silin Gao, Antoine Bosselut, Samy Bengio, Emmanuel Abbe*

Main category: cs.CL

TL;DR: The paper introduces AbstRaL, a method using reinforcement learning to improve abstract reasoning in LLMs, countering distribution shifts and enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: Smaller LLMs often struggle with robustness under distribution shifts. The paper aims to address this by abstracting reasoning problems rather than generating synthetic data.

Method: The proposed method, AbstRaL, uses reinforcement learning to train LLMs on granular abstraction data, promoting faithful abstractions.

Result: AbstRaL significantly reduces performance degradation on GSM perturbation benchmarks compared to supervised fine-tuning.

Conclusion: Abstracting reasoning problems via RL (AbstRaL) is more effective than synthetic data generation for improving LLM robustness under distribution shifts.

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [129] [LLM-BT-Terms: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/pdf/2506.08174)
*Li Weigang, Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: LLM-BT automates multilingual terminology standardization using LLM-powered back-translation, ensuring high term consistency and cross-lingual robustness.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of technical terms in fields like AI and quantum computing challenges manual standardization, requiring automated solutions for consistency.

Method: LLM-BT uses back-translation (English -> intermediate language -> English) and a multi-path workflow (Retrieve -> Generate -> Verify -> Optimize) for term validation.

Result: Achieves over 90% term preservation, BLEU scores >0.45, and 100% Portuguese term accuracy.

Conclusion: LLM-BT redefines back-translation as dynamic semantic embedding, enabling machine-human collaboration for multilingual standardization.

Abstract: The rapid expansion of English technical terminology presents a significant
challenge to traditional expert-based standardization, particularly in rapidly
developing areas such as artificial intelligence and quantum computing. Manual
approaches face difficulties in maintaining consistent multilingual
terminology. To address this, we introduce LLM-BT, a back-translation framework
powered by large language models (LLMs) designed to automate terminology
verification and standardization through cross-lingual semantic alignment. Our
key contributions include: (1) term-level consistency validation: by performing
English -> intermediate language -> English back-translation, LLM-BT achieves
high term consistency across different models (such as GPT-4, DeepSeek, and
Grok). Case studies demonstrate over 90 percent of terms are preserved either
exactly or semantically; (2) multi-path verification workflow: we develop a
novel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which
supports both serial paths (e.g., English -> Simplified Chinese -> Traditional
Chinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese
-> English). BLEU scores and term-level accuracy indicate strong cross-lingual
robustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy
reaching 100 percent; (3) back-translation as semantic embedding: we
reinterpret back-translation as a form of dynamic semantic embedding that
uncovers latent trajectories of meaning. In contrast to static embeddings,
LLM-BT offers transparent, path-based embeddings shaped by the evolution of the
models. This reframing positions back-translation as an active mechanism for
multilingual terminology standardization, fostering collaboration between
machines and humans - machines preserve semantic integrity, while humans
provide cultural interpretation.

</details>


### [130] [Unable to Forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/pdf/2506.08184)
*Chupei Wang, Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: LLMs struggle with intra-context interference, where earlier information disrupts recall of newer updates, revealing a working memory bottleneck.


<details>
  <summary>Details</summary>
Motivation: To study the under-researched effects of intra-context interference in LLMs and its impact on retrieval accuracy.

Method: Adapts the proactive interference (PI) paradigm from cognitive science, introducing PI-LLM to evaluate LLMs by streaming semantically related updates and querying final values.

Result: LLM retrieval accuracy declines log-linearly as interference accumulates, with errors from retrieving overwritten values. Prompt engineering mitigates interference poorly.

Conclusion: LLMs face a fundamental constraint in disentangling interference, indicating a working memory bottleneck. Solutions should focus on suppressing irrelevant content during retrieval.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [131] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/pdf/2506.08364)
*Jash Rajesh Parekh, Pengcheng Jiang, Jiawei Han*

Main category: cs.CL

TL;DR: CC-RAG enhances RAG by modeling causal dependencies with structured triples and graph chaining, outperforming standard RAG and zero-shot LLMs in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' challenges in understanding cause-effect relationships in specialized domains where flat retrieval lacks causal structure.

Method: Integrates zero-shot triple extraction and theme-aware graph chaining into RAG, constructing a DAG of causal triples for multi-hop inference.

Result: Outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity, validated by LLM and human evaluations.

Conclusion: Explicit causal modeling improves LLM accuracy and interpretability, especially in domains where flat retrieval fails.

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [132] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/pdf/2506.08371)
*Zikai Xiao, Ziyang Wang, Wen Ma, Yan Zhang, Wei Shen, Yan Wang, Luqi Gong, Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper addresses performance degradation in LLMs for long contexts, proposing a training-free method (PCD) to mitigate the issue by leveraging attention contrasts.


<details>
  <summary>Details</summary>
Motivation: LLMs degrade in performance for long contexts, and current solutions are costly. The study explores statistical behaviors and cost-effective approaches.

Method: Proposes Positional Contrastive Decoding (PCD), contrasting logits from long-aware and local-aware attention to focus on gains from short-to-long training.

Result: PCD effectively alleviates attention score degradation and achieves state-of-the-art performance on long-context benchmarks.

Conclusion: PCD offers a cost-effective solution to long-context performance issues in LLMs, demonstrating superior results.

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [133] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/pdf/2506.08403)
*Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Hu Song, Linfeng Zhang*

Main category: cs.CL

TL;DR: TACTIC is a cognitively informed multi-agent framework for machine translation, outperforming state-of-the-art models like GPT-4.1 and DeepSeek-R1.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent translation frameworks lack insights from cognitive translation studies, which highlight human strategies like balancing literal/free translation and iterative refinement.

Method: TACTIC integrates six functionally distinct agents (drafting, refinement, evaluation, scoring, context reasoning, external knowledge) to simulate human cognitive processes in translation.

Result: TACTIC achieves superior performance on FLORES-200 and WMT24 benchmarks, surpassing GPT-4.1 and DeepSeek-R1 in metrics like XCOMET and COMETKIWI-23.

Conclusion: The framework effectively leverages LLMs by grounding translation workflows in cognitive theory, demonstrating significant improvements in translation quality.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [134] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/pdf/2506.08433)
*Hernán Maina, Nicolás Wolovick, Luciana Benotti*

Main category: cs.CL

TL;DR: The paper explores how numerical precision formats and data parallelization affect training speed and accuracy for LLMs, aiming to make domain adaptation more accessible in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: High costs and cultural biases in LLM training limit accessibility. Domain adaptation is promising but computationally expensive, especially for groups with limited infrastructure.

Method: Evaluates impact of numerical precision formats and data parallelization on training speed and model accuracy.

Result: Findings show trade-offs between efficiency and accuracy, aiding domain adaptation in resource-constrained environments.

Conclusion: The study provides insights for improving energy efficiency and accessibility in LLM training, particularly for low-resource settings.

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precision formats and data parallelization strategies impacts both
training speed (as a proxy to energy and hardware consumption) and model
accuracy, with the goal of facilitating domain adaptation in low-resource
environments. Our findings are relevant to any setting where energy efficiency,
accessibility, or limited hardware availability are key concerns.

</details>


### [135] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/pdf/2506.08700)
*Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: ClimateViz is a benchmark for scientific fact-checking using charts, showing current models underperform humans in chart-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking overlooks scientific charts, which are crucial for quantitative evidence.

Method: Introduced ClimateViz with 49,862 claims linked to 2,896 charts, evaluated multimodal models in zero-shot and few-shot settings.

Result: Best models (Gemini 2.5, InternVL 2.5) achieved 76.2-77.8% accuracy, below human performance (89.3-92.7%). Explanation-augmented outputs helped some models.

Conclusion: Current models struggle with chart-based reasoning, highlighting the need for improved multimodal fact-checking tools.

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [136] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/pdf/2506.08738)
*Dror Kris Markus, Fabrizio Gilardi, Daria Stetsenko*

Main category: cs.CL

TL;DR: The study examines the integration of ethical and societal values in AI research, finding that while interdisciplinary teams lead in societal focus, computer science-only teams are increasingly contributing to this area.


<details>
  <summary>Details</summary>
Motivation: To understand how ethical and societal values are being incorporated into AI research and whether interdisciplinary collaboration is driving this shift.

Method: Analysis of over 100,000 AI-related papers on ArXiv (2014-2024) using a classifier to identify societal content.

Result: Interdisciplinary teams are more likely to produce societally-oriented research, but computer science-only teams are growing in societal output, addressing domains like fairness, safety, healthcare, and misinformation.

Conclusion: The findings challenge assumptions about interdisciplinary collaboration's role in societal AI and raise questions about AI governance and the unique contributions of non-technical fields.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [137] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/pdf/2506.08768)
*Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini*

Main category: cs.CL

TL;DR: The paper benchmarks reasoning-focused LLMs, especially DeepSeek models, on Arabic NLP tasks, revealing performance boosts from in-context examples, DeepSeek's superiority over GPT-4-mini, and LoRA fine-tuning benefits.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' performance on Arabic data, given its linguistic complexity and underexplored potential in reasoning tasks.

Method: Evaluates LLMs using zero-shot, few-shot, and fine-tuning strategies across 15 Arabic NLP tasks, focusing on DeepSeek models.

Result: Key findings include significant performance boosts from few-shot examples (13 F1 points), DeepSeek outperforming GPT-4-mini (12 F1 points), and LoRA fine-tuning adding 8 F1/BLEU points.

Conclusion: The study highlights the effectiveness of in-context learning and fine-tuning for Arabic NLP, with DeepSeek models showing strong reasoning capabilities.

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [138] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/pdf/2506.08885)
*Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, Amitava Das*

Main category: cs.CL

TL;DR: The paper exposes a geometric blind spot in LLM alignment, introduces ALKALI (a benchmark) and GRACE (a defense framework), and proposes AVQI (a metric) to quantify latent alignment failures.


<details>
  <summary>Details</summary>
Motivation: Adversarial threats against LLMs are outpacing defenses, with attacks exploiting latent geometry to evade detection.

Method: Introduces ALKALI for benchmarking, GRACE for alignment (latent space regularization), and AVQI for measuring alignment failure.

Result: GRACE reduces Attack Success Rates by up to 39%, and AVQI reveals unsafe completions mimicking safe geometry.

Conclusion: The work highlights latent camouflage vulnerabilities and offers tools (ALKALI, GRACE, AVQI) to improve LLM safety.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [139] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/pdf/2506.08952)
*Clara Lachenmaier, Judith Sieker, Sina Zarrieß*

Main category: cs.CL

TL;DR: The paper examines how LLMs handle common ground and misinformation in political discourse, finding challenges in their ability to correct false beliefs.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' capability in managing mutual understanding and addressing misinformation, especially in politically charged contexts.

Method: Evaluated LLMs' responses to direct knowledge questions and loaded questions with misinformation, analyzing grounding behavior and political bias.

Result: LLMs struggle with grounding and rejecting false beliefs, posing concerns for their role in political misinformation.

Conclusion: LLMs face significant challenges in mitigating misinformation, highlighting risks in political discourse.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [140] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/pdf/2506.09003)
*Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, Junyang Lin*

Main category: cs.CL

TL;DR: SWE-Flow is a data synthesis framework using Test-Driven Development (TDD) to generate incremental development steps from unit tests, creating verifiable TDD tasks.


<details>
  <summary>Details</summary>
Motivation: Existing software engineering data relies on human-submitted issues, lacking automation and precision in capturing development steps.

Method: SWE-Flow constructs a Runtime Dependency Graph (RDG) to infer function interactions and generate structured development schedules, producing partial codebases, tests, and modifications.

Result: Generated 16,061 training and 2,020 test instances (SWE-Flow-Eval benchmark), showing improved TDD-based coding performance when fine-tuning models.

Conclusion: SWE-Flow automates TDD task generation, enhances model performance, and provides open resources for further research.

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [141] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/pdf/2506.09009)
*Hakyung Sung, Gyu-Ho Shin, Chanyoung Lee, You Kyung Sung, Boo Kyung Jung*

Main category: cs.CL

TL;DR: A semi-automated framework aligns XPOS sequences with UPOS categories for L2 Korean, improving annotation consistency and model performance.


<details>
  <summary>Details</summary>
Motivation: To enhance morphosyntactic analysis for L2 Korean by aligning XPOS and UPOS categories and expanding the corpus.

Method: Introduce a semi-automated framework for XPOS-UPOS alignment, annotate 2,998 new sentences, and evaluate using NLP toolkits.

Result: Aligned dataset improves annotation consistency and boosts morphosyntactic tagging and dependency-parsing accuracy.

Conclusion: XPOS-UPOS alignment enhances L2 Korean analysis, especially with limited annotated data.

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [142] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/pdf/2506.09021)
*Hakyung Sung, Karla Csuros, Min-Chang Sung*

Main category: cs.CL

TL;DR: Human and LLM proofreading improve intelligibility in second language writing, with LLMs showing more generative edits and consistent outcomes across models.


<details>
  <summary>Details</summary>
Motivation: To compare the effectiveness and consistency of human and LLM proofreading in enhancing second language writing.

Method: Examined lexical and syntactic interventions by humans and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical texts.

Result: Both human and LLM proofreading improved bigram lexical features. LLMs were more generative, using diverse vocabulary and syntactic structures, with high consistency across models.

Conclusion: LLM proofreading is effective and consistent, offering a generative approach to enhancing writing intelligibility.

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [143] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/pdf/2506.09047)
*Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov*

Main category: cs.CL

TL;DR: The paper investigates the performance gap between visual and textual tasks in Vision-Language Models (VLMs) by analyzing task-specific circuits and proposes a method to reduce this gap by patching visual representations.


<details>
  <summary>Details</summary>
Motivation: VLMs perform better on textual tasks than analogous visual tasks, and the study aims to understand and mitigate this discrepancy.

Method: The authors compare circuits (task-specific computational sub-graphs) across modalities, analyze data representations, and patch visual tokens from later layers into earlier ones.

Result: Patching visual representations closes a third of the performance gap between modalities on average.

Conclusion: The study reveals insights into the multi-modal performance gap and offers a training-free solution to improve VLM performance on visual tasks.

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [144] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/pdf/2506.09650)
*Kunyu Peng, Junchao Huang, Xiangsheng Huang, Di Wen, Junwei Zheng, Yufan Chen, Kailun Yang, Jiamin Wu, Chongqing Hao, Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces a new task of textual reference-guided human action segmentation in multi-person settings, proposes a dataset (RHAS133), and presents HopaDIFF, a novel framework achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing action segmentation methods focus on single-person activities, ignoring multi-person scenarios. This work addresses this gap by leveraging textual descriptions to specify target persons.

Method: The authors propose HopaDIFF, a holistic-partial aware Fourier-conditioned diffusion framework, using cross-input gate attentional xLSTM for long-range reasoning and Fourier conditions for fine-grained control.

Result: HopaDIFF achieves state-of-the-art performance on the new RHAS133 dataset, outperforming existing methods.

Conclusion: The work pioneers multi-person action segmentation with textual guidance, introduces a benchmark dataset, and demonstrates the effectiveness of HopaDIFF.

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [145] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/pdf/2506.09066)
*Maoyu Wang, Yao Lu, Jiaqi Nie, Zeyu Wang, Yun Lin, Qi Xuan, Guan Gui*

Main category: cs.CV

TL;DR: ReStNet proposes a dynamic network stitching method for adapting pre-trained models to diverse IoT device constraints, achieving flexible efficiency-accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Deploying fixed pre-trained models across heterogeneous IoT devices is challenging due to varying computational resources. Traditional compression methods lack flexibility.

Method: ReStNet stitches pre-trained models by selecting optimal points via Centered Kernel Alignment (CKA), fine-tuning only the stitching layer, and supports homogeneous and heterogeneous model combinations.

Result: ReStNet achieves flexible accuracy-efficiency trade-offs, reduces training costs, and adapts to varying resource constraints effectively.

Conclusion: ReStNet offers a scalable and efficient solution for deploying pre-trained models in resource-constrained IoT environments.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [146] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/pdf/2506.09067)
*Zhiyu Xue, Reza Abbasi-Asl, Ramtin Pedarsani*

Main category: cs.CV

TL;DR: The paper proposes an inference-time defense strategy for Med-VLMs to mitigate harmful queries without degrading general performance, using synthetic clinical demonstrations.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in Med-VLMs, such as handling harmful queries and avoiding over-defense, remain underexplored.

Method: A novel inference-time defense strategy leveraging synthetic clinical demonstrations to balance security and performance.

Result: The strategy enhances safety without significant performance loss, with increased demonstration budgets alleviating over-defense.

Conclusion: A mixed demonstration strategy is introduced as a trade-off for balancing security and performance under few-shot constraints.

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [147] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/pdf/2506.09068)
*Sriram Krishna, Sravan Chittupalli, Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP is a generative prior for modeling 3D bimanual hand-object interactions, extending single-hand priors to address data scarcity.


<details>
  <summary>Details</summary>
Motivation: Limited bimanual interaction data motivates the extension of existing single-hand generative priors.

Method: BG-HOP extends single-hand generative priors to model bimanual interactions and synthesize grasps.

Result: The model successfully generates bimanual interactions and grasps for given objects.

Conclusion: BG-HOP demonstrates promise in capturing joint hand-object distributions, with code and models made public.

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [148] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/pdf/2506.09071)
*Peilin Li, Jun Yin, Jing Zhong, Ran Luo, Pengyu Zeng, Miao Zhang*

Main category: cs.CV

TL;DR: SAAF is an automatic segmentation model for building facades using multimodal semantic guidance, combining text and image features for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency in building information models and CAD by automating wall and window segmentation.

Method: Uses multimodal semantic feature extraction and an end-to-end training framework to map text descriptions to image segmentation.

Result: Outperforms existing methods in mIoU metric, showing high precision across diverse datasets.

Conclusion: Advances architectural computer vision and explores multimodal learning applications in architecture.

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [149] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/pdf/2506.09079)
*Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces two datasets (DarkEventInfer, MixVidQA) and a model (VersaVid-R1) to advance video-based reasoning, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Video-based reasoning is underdeveloped due to lack of quality data and training methods.

Method: Created datasets (DarkEventInfer, MixVidQA) and trained VersaVid-R1 using reinforcement learning with diverse rewards.

Result: VersaVid-R1 outperforms existing models in video understanding, reasoning, and captioning tasks.

Conclusion: The work bridges the gap in video reasoning, demonstrating superior performance across benchmarks.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [150] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/pdf/2506.09081)
*Zheqi He, Yesheng Liu, Jing-shu Zheng, Xuejing Li, Richeng Xuan, Jin-Ge Yao, Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM is an open-source framework for evaluating multimodal models on vision-language tasks, offering flexibility, efficiency, and accurate insights.


<details>
  <summary>Details</summary>
Motivation: To address the need for a comprehensive and efficient evaluation framework for multimodal models across diverse tasks.

Method: Decouples model inference from evaluation, uses advanced tools (e.g., vLLM, SGLang) and asynchronous data loading for efficiency.

Result: FlagEvalMM provides accurate and efficient evaluation, highlighting model strengths and limitations.

Conclusion: FlagEvalMM is a valuable tool for advancing multimodal research, publicly available on GitHub.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [151] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/pdf/2506.09082)
*Zheda Mai, Arpita Chowdhury, Zihe Wang, Sooyoung Jeon, Lemeng Wang, Jiacheng Hou, Jihyung Kil, Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench is introduced to evaluate vision foundation models (VFMs) by disentangling 14 Atomic Visual Abilities (AVAs), addressing gaps in current VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VQA benchmarks have blind spots: misalignment of instruction tuning data and inability to pinpoint specific visual shortcomings.

Method: AVA-Bench decouples 14 AVAs, ensuring matched training and test distributions for precise evaluation.

Result: AVA-Bench reveals distinct ability fingerprints of VFMs and shows a 0.5B LLM is as effective as a 7B LLM but more efficient.

Conclusion: AVA-Bench provides a transparent and comprehensive benchmark to guide future VFM development.

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [152] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/pdf/2506.09083)
*Jerry Lin, Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow is a semi-automatic labeling tool for computer vision, enhancing efficiency with features like pixel-precise corrections, data augmentation, label propagation, and an adaptable YOLOE-based auto-labeling module.


<details>
  <summary>Details</summary>
Motivation: Manual data labeling is slow and error-prone, especially for large-scale tasks, necessitating more efficient tools.

Method: BakuFlow integrates a live magnifier, interactive data augmentation, label propagation for videos, and a modified YOLOE framework for flexible auto-labeling.

Result: The tool reduces labeling workload and improves efficiency, particularly for object detection and tracking.

Conclusion: BakuFlow is effective for dynamic, real-world datasets, streamlining annotation in computer vision and industrial applications.

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [153] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/pdf/2506.09984)
*Zhenzhi Wang, Jiaqi Yang, Jianwen Jiang, Chao Liang, Gaojie Lin, Zerong Zheng, Ceyuan Yang, Dahua Lin*

Main category: cs.CV

TL;DR: A novel framework for multi-concept human animation with precise, region-specific control of conditions from text, image, and audio, addressing limitations of single-entity methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human animation lack precise control for multiple concepts and interactions, limiting applications. This work aims to enable high-quality, controllable multi-concept videos.

Method: The framework uses a mask predictor to infer layout information from reference images, ensuring region-specific binding of conditions. Local audio conditions are injected iteratively for alignment.

Result: The method achieves high-quality generation of multi-concept human-centric videos with explicit layout control, outperforming implicit and existing methods.

Conclusion: The proposed framework effectively addresses the limitations of single-entity assumptions, enabling precise control and high-quality results for multi-concept human animation.

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [154] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/pdf/2506.09106)
*Xiaofeng Zhang, Michelle Lin, Simon Lacoste-Julien, Aaron Courville, Yash Goyal*

Main category: cs.CV

TL;DR: The paper investigates bias in unconditional generative AI models, finding small attribute shifts but highlighting classifier sensitivity in bias evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about representational harm and discriminatory outcomes in generative AI, especially in unconditional generation where bias mechanisms are unclear.

Method: Training unconditional image generative models and using a bias evaluation framework to study shifts between training and generated distributions.

Result: Detected attribute shifts are small but sensitive to the attribute classifier, especially for attributes on a spectrum.

Conclusion: Emphasizes the need for better labeling practices, scrutiny of evaluation frameworks, and understanding the social complexity of attributes in bias assessment.

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [155] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/pdf/2506.09109)
*Arnav Yayavaram, Siddharth Yayavaram, Simran Khanuja, Michael Saxon, Graham Neubig*

Main category: cs.CV

TL;DR: CAIRe is a new metric for evaluating cultural relevance in text-to-image models, outperforming baselines by 28% F1 and aligning well with human judgments.


<details>
  <summary>Details</summary>
Motivation: Addressing cross-cultural biases in text-to-image models, which are hindered by performance trade-offs and lack of reliable bias measurement.

Method: Introduces CAIRe, a framework that grounds image entities in a knowledge base for graded cultural relevance judgments.

Result: CAIRe surpasses baselines by 28% F1 and achieves Pearson's correlations of 0.56 and 0.66 with human ratings on culturally universal datasets.

Conclusion: CAIRe effectively measures cultural relevance, demonstrating strong alignment with human judgment across diverse image sources.

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [156] [Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals](https://arxiv.org/pdf/2506.09510)
*Changhao Peng, Yuqi Ye, Wei Gao*

Main category: cs.CV

TL;DR: The paper introduces a generalized Gaussian entropy model and a Mean Error Discriminator (MED) to improve probability estimation and dynamic likelihood interval adjustment in point cloud attribute compression, enhancing rate-distortion performance.


<details>
  <summary>Details</summary>
Motivation: Current entropy models in point cloud compression underutilize information in entropy parameters, limiting accuracy and performance.

Method: Proposes a generalized Gaussian entropy model with a shape parameter for better probability estimation and MED for dynamic likelihood interval adjustment.

Result: Significantly improves rate-distortion performance on VAE-based models and is applicable to other compression tasks.

Conclusion: The new model and MED enhance compression efficiency and accuracy, with broader applicability beyond point clouds.

Abstract: Gaussian and Laplacian entropy models are proved effective in learned point
cloud attribute compression, as they assist in arithmetic coding of latents.
However, we demonstrate through experiments that there is still unutilized
information in entropy parameters estimated by neural networks in current
methods, which can be used for more accurate probability estimation. Thus we
introduce generalized Gaussian entropy model, which controls the tail shape
through shape parameter to more accurately estimate the probability of latents.
Meanwhile, to the best of our knowledge, existing methods use fixed likelihood
intervals for each integer during arithmetic coding, which limits model
performance. We propose Mean Error Discriminator (MED) to determine whether the
entropy parameter estimation is accurate and then dynamically adjust likelihood
intervals. Experiments show that our method significantly improves
rate-distortion (RD) performance on three VAE-based models for point cloud
attribute compression, and our method can be applied to other compression
tasks, such as image and video compression.

</details>


### [157] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/pdf/2506.09113)
*Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0 is a high-performance video generation model addressing prompt adherence, motion plausibility, and visual quality through multi-source data, efficient architecture, and optimized post-training.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle to balance prompt following, motion plausibility, and visual quality.

Method: Combines multi-source data curation, efficient architecture, post-training optimization, and model acceleration techniques.

Result: Achieves high-quality, fast video generation with superior spatiotemporal fluidity and precise instruction adherence.

Conclusion: Seedance 1.0 outperforms state-of-the-art models in quality and speed, excelling in complex scenarios.

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [158] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/pdf/2506.09229)
*Sungwon Hwang, Hyojin Jang, Kinam Kim, Minho Park, Jaegul choo*

Main category: cs.CV

TL;DR: The paper introduces CREPA, a method to improve video diffusion model fine-tuning by aligning hidden states across frames for better semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning video diffusion models for specific attributes is challenging and underexplored, despite its practical importance.

Method: Proposes Cross-frame Representation Alignment (CREPA), aligning hidden states of a frame with external features from neighboring frames.

Result: CREPA improves visual fidelity and semantic coherence in large-scale VDMs like CogVideoX-5B and Hunyuan Video.

Conclusion: CREPA is broadly applicable and effective for enhancing video diffusion model fine-tuning.

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [159] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/pdf/2506.09237)
*Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: PatchGuard introduces an adversarially robust AD and AL method using pseudo anomalies and a ViT-based architecture, outperforming previous methods in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Current AD and AL methods are vulnerable to adversarial attacks due to limited training data (only normal samples). PatchGuard aims to address this by incorporating pseudo anomalies and localization masks.

Method: Uses Foreground-Aware Pseudo-Anomalies in a ViT-based framework with adversarial training guided by a novel loss function.

Result: Achieves performance gains of 53.2% in AD and 68.5% in AL in adversarial settings, while maintaining competitive accuracy in non-adversarial scenarios.

Conclusion: PatchGuard significantly improves robustness in AD and AL, validated by experiments on industrial and medical datasets.

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [160] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/pdf/2506.09278)
*Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer, Wenshan Wang*

Main category: cs.CV

TL;DR: UFM introduces a unified model for dense image correspondence, outperforming specialized methods in both flow and wide-baseline matching with higher accuracy, lower error, and faster speed.


<details>
  <summary>Details</summary>
Motivation: Dense image correspondence is crucial for applications like visual odometry and 3D reconstruction, but existing methods treat wide-baseline and optical flow tasks separately. UFM aims to unify these approaches.

Method: UFM uses a generic transformer architecture to regress (u,v) flow directly, trained on unified data for co-visible pixels. It avoids coarse-to-fine cost volumes, simplifying training and improving accuracy for large flows.

Result: UFM outperforms state-of-the-art methods: 28% more accurate than Unimatch, 62% less error and 6.7x faster than RoMa.

Conclusion: UFM demonstrates unified training can surpass specialized methods, enabling fast, general-purpose correspondence and opening new research directions.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [161] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/pdf/2506.09299)
*Sindhu Boddu, Arindam Mukherjee*

Main category: cs.CV

TL;DR: A lightweight, energy-efficient YOLOv4-Tiny model is optimized via INT8 quantization for aerial emergency object detection, achieving comparable performance to YOLOv5-small while significantly reducing model size and improving speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of publicly available drone-view emergency imagery, the paper aims to provide a real-time, efficient solution for emergency response using edge devices.

Method: The YOLOv4-Tiny model is trained on a custom aerial emergency dataset (10,820 images) and optimized through post-training INT8 quantization.

Result: The quantized model reduces size by 71% (22.5 MB to 6.4 MB) and improves inference speed by 44%, with comparable detection performance.

Conclusion: The quantized YOLOv4-Tiny is highly suitable for real-time emergency detection on low-power edge devices.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [162] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/pdf/2506.09300)
*Sindhu Boddu, Arindam Mukherjee*

Main category: cs.CV

TL;DR: A quantized YOLOv4-Tiny model was deployed on a Raspberry Pi 5 for real-time aerial emergency object detection, showing reduced power usage and robust accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient real-time object detection in emergency scenarios using resource-constrained edge devices.

Method: YOLOv4-Tiny was quantized to INT8 using TensorFlow Lite post-training quantization and evaluated for speed, power, and thermal performance.

Result: Achieved 28.2 ms inference time, 13.85 W power consumption, and maintained accuracy for emergency classes.

Conclusion: Quantized models on edge devices are viable for safety-critical emergency applications.

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [163] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/pdf/2506.09327)
*Tong Wang, Guanzhou Chen, Xiaodong Zhang, Chenxi Liu, Jiaqi Wang, Xiaoliang Tan, Wenchao Guo, Qingyuan Yang, Kaiqi Zhang*

Main category: cs.CV

TL;DR: A multi-modal self-supervised learning framework for remote sensing image interpretation, leveraging RGB, multi-spectral data, and DSM, outperforms existing methods across 26 tasks.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled data is costly and time-consuming to acquire, necessitating an efficient pre-training method.

Method: Proposes a framework with adaptive masking, cross-modal masking, and multi-task self-supervised objectives to capture inter-modal correlations and intra-modal features.

Result: Achieves superior performance on 15 datasets, e.g., 78.30% mIoU for Potsdam segmentation and 0.182 RMSE for US3D depth estimation.

Conclusion: The framework effectively addresses the labeled data challenge and advances remote sensing tasks.

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [164] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/pdf/2506.09343)
*Yuxing Long, Jiyao Zhang, Mingjie Pan, Tianshu Wu, Taewhan Kim, Hao Dong*

Main category: cs.CV

TL;DR: The paper introduces CheckManual, a benchmark for manual-based appliance manipulation, addressing gaps in prior research by leveraging manuals for robot task planning.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks focus on using appliance manuals for robot manipulation, either limiting manuals to QA tasks or ignoring them entirely. This paper aims to bridge this gap.

Method: The authors propose a data generation pipeline using CAD models to create manuals, then design challenges, metrics, and simulator environments for evaluation. They also introduce ManualPlan, a manipulation planning model.

Result: CheckManual is established as a benchmark, and ManualPlan serves as a baseline model for manual-based appliance manipulation tasks.

Conclusion: The work highlights the importance of manuals for robot manipulation and provides a foundational benchmark and model for future research.

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [165] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/pdf/2506.09345)
*Songping Wang, Xiantao Hu, Yueming Lyu, Caifeng Shan*

Main category: cs.CV

TL;DR: A tri-modal action recognition solution using data enhancement, transfer learning, and multimodal spatial-temporal feature extraction achieves top accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in tri-modal action recognition due to scarce data by leveraging multimodal information.

Method: Data enhancement, transfer learning with RGB datasets, 2D CNNs with TSM for spatial-temporal features, and prediction enhancement techniques like SWA, Ensemble, and TTA.

Result: Achieved Top-1 accuracy of 99% and Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed solution demonstrates superiority in tri-modal action recognition by effectively utilizing multimodal information and advanced techniques.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [166] [Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression](https://arxiv.org/pdf/2506.01234)
*Woojin Cho, Steve Andreas Immanuel, Junhyuk Heo, Darongsae Kwon*

Main category: cs.CV

TL;DR: ImpliSat is a framework using Implicit Neural Representations (INR) and Fourier modulation for efficient compression and reconstruction of multispectral satellite data.


<details>
  <summary>Details</summary>
Motivation: High dimensionality, large data volumes, and diverse spatial resolutions in multispectral satellite images pose challenges for compression and analysis.

Method: Leverages INR to model images as continuous functions and introduces a Fourier modulation algorithm for dynamic adjustment to spectral and spatial characteristics.

Result: Efficient compression and reconstruction while preserving critical image details.

Conclusion: ImpliSat effectively addresses challenges in multispectral satellite data handling.

Abstract: Multispectral satellite images play a vital role in agriculture, fisheries,
and environmental monitoring. However, their high dimensionality, large data
volumes, and diverse spatial resolutions across multiple channels pose
significant challenges for data compression and analysis. This paper presents
ImpliSat, a unified framework specifically designed to address these challenges
through efficient compression and reconstruction of multispectral satellite
data. ImpliSat leverages Implicit Neural Representations (INR) to model
satellite images as continuous functions over coordinate space, capturing fine
spatial details across varying spatial resolutions. Furthermore, we introduce a
Fourier modulation algorithm that dynamically adjusts to the spectral and
spatial characteristics of each band, ensuring optimal compression while
preserving critical image details.

</details>


### [167] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/pdf/2506.09350)
*Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang*

Main category: cs.CV

TL;DR: AAPT transforms a pre-trained latent video diffusion model into a real-time, interactive video generator using autoregressive adversarial post-training.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models are too slow for real-time use, limiting adoption in interactive applications.

Method: Proposes autoregressive adversarial post-training (AAPT) for one-step latent frame generation, leveraging adversarial training and KV cache efficiency.

Result: Achieves real-time 24fps video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 for up to a minute.

Conclusion: AAPT enables efficient, real-time, and interactive video generation with reduced error accumulation.

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [168] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/pdf/2506.09357)
*Junchao Zhou*

Main category: cs.CV

TL;DR: A novel variational framework for 2D image segmentation using shape analysis and diffeomorphic transformations, requiring minimal training data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and deep learning approaches have limitations; the former lack flexibility, while the latter need large datasets.

Method: Uses LDDMM framework for curve deformation guided by a loss function comparing the curve to the image gradient field, implemented in Python with GPU acceleration.

Result: Accurate segmentation with a flexible, theoretically grounded methodology.

Conclusion: The proposed framework offers a data-efficient and theoretically sound alternative for image segmentation.

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [169] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/pdf/2506.09363)
*Hongguang Zhu, Yunchao Wei, Mengyu Wang, Siyu Jiao, Yan Fang, Jiannan Huang, Yao Zhao*

Main category: cs.CV

TL;DR: SAGE introduces semantic-augment erasing and a global-local retention mechanism to improve concept erasure in diffusion models, ensuring safer text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: To address safety risks like unsafe content and copyright infringement in diffusion models by escaping the 'word concept abyss' and enabling generalized concept erasure.

Method: Semantic-augment erasing transforms word erasure into domain erasure via cyclic self-check and self-erasure, while a global-local retention mechanism preserves irrelevant concepts.

Result: SAGE outperforms existing methods in safe generation, demonstrating comprehensive superiority.

Conclusion: SAGE effectively unlearns unsafe concepts while retaining irrelevant ones, enhancing the safety of diffusion models.

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [170] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/pdf/2506.09369)
*Zeran Ke, Bin Tan, Xianwei Zheng, Yujun Shen, Tianfu Wu, Nan Xue*

Main category: cs.CV

TL;DR: ScaleLSD is a scalable self-supervised learning model for Line Segment Detection (LSD) that outperforms non-deep LSD methods in accuracy and versatility.


<details>
  <summary>Details</summary>
Motivation: To develop a domain-agnostic, robust LSD model that works well for any natural images by leveraging scalable self-supervised learning.

Method: Revisits and streamlines fundamental LSD designs to create ScaleLSD, a high-performing and efficient learner trained on over 10M unlabeled images.

Result: ScaleLSD detects more line segments accurately and outperforms non-deep LSD in zero-shot detection, 3D geometry estimation, and multiview mapping.

Conclusion: ScaleLSD is the first deep LSD approach to surpass non-deep methods in all tested aspects, enhancing image line geometry versatility.

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [171] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/pdf/2506.09378)
*Qijian Tian, Xin Tan, Jingyu Gong, Yuan Xie, Lizhuang Ma*

Main category: cs.CV

TL;DR: UniForward is a feed-forward model for unified 3D scene and semantic field reconstruction from sparse-view images, achieving real-time performance and high-quality results without needing camera parameters or ground truth depth.


<details>
  <summary>Details</summary>
Motivation: To combine 3D scenes with semantic fields for better environment perception, addressing challenges like embedding semantics into 3D representations and generalizable real-time reconstruction.

Method: Proposes UniForward, a model predicting 3D Gaussians with semantic features from uncalibrated images, using a dual-branch decoder and loss-guided view sampling.

Result: Achieves state-of-the-art performance in novel view synthesis and segmentation, enabling real-time reconstruction and view-consistent semantic rendering.

Conclusion: UniForward successfully unifies 3D scene and semantic field reconstruction, offering practical applicability and high-quality results.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [172] [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/pdf/2506.09385)
*Jialong Zuo, Yongtai Deng, Mengdan Tan, Rui Jin, Dongyue Wu, Nong Sang, Liang Pan, Changxin Gao*

Main category: cs.CV

TL;DR: The paper introduces OM-ReID, a multi-modal person re-identification problem, and proposes ORBench dataset and ReID5o framework for effective retrieval across five modalities.


<details>
  <summary>Details</summary>
Motivation: Existing ReID methods and datasets are limited to few modalities, failing to address real-world scenarios requiring diverse multi-modal queries.

Method: Constructed ORBench dataset with 1,000 identities across five modalities (RGB, infrared, sketch, color pencil, text). Proposed ReID5o, a framework for unified encoding and cross-modal alignment.

Result: ReID5o outperforms other models on ORBench, demonstrating its effectiveness in multi-modal ReID.

Conclusion: ORBench and ReID5o provide a robust platform and solution for OM-ReID, addressing dataset scarcity and enabling multi-modal retrieval.

Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a
person-of-interest via the descriptive query, regardless of whether the query
is a single modality or a combination of multiple modalities. However, existing
methods and datasets remain constrained to limited modalities, failing to meet
this requirement. Therefore, we investigate a new challenging problem called
Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve
effective retrieval with varying multi-modal queries. To address dataset
scarcity, we construct ORBench, the first high-quality multi-modal dataset
comprising 1,000 unique identities across five modalities: RGB, infrared, color
pencil, sketch, and textual description. This dataset also has significant
superiority in terms of diversity, such as the painting perspectives and
textual information. It could serve as an ideal platform for follow-up
investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal
learning framework for person ReID. It enables synergistic fusion and
cross-modal alignment of arbitrary modality combinations in a single model,
with a unified encoding and multi-expert routing mechanism proposed. Extensive
experiments verify the advancement and practicality of our ORBench. A wide
range of possible models have been evaluated and compared on it, and our
proposed ReID5o model gives the best performance. The dataset and code will be
made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.

</details>


### [173] [Improving Out-of-Distribution Detection via Dynamic Covariance Calibration](https://arxiv.org/pdf/2506.09399)
*Kaiyu Guo, Zijian Wang, Brian C. Lovell, Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: The paper proposes a dynamic method to improve Out-of-Distribution (OOD) detection by adjusting prior geometry in real-time, addressing distortions from ill-distributed samples.


<details>
  <summary>Details</summary>
Motivation: Prior subspace-based OOD detection methods fail to handle geometry distortions caused by ill-distributed samples due to static prior extraction.

Method: The approach dynamically updates the prior covariance matrix using real-time input features, refining information while preserving essential data characteristics.

Result: The method significantly enhances OOD detection across multiple pre-trained models, including CIFAR and ImageNet-1k datasets.

Conclusion: Dynamic adjustment of prior geometry effectively improves OOD detection, outperforming static methods.

Abstract: Out-of-Distribution (OOD) detection is essential for the trustworthiness of
AI systems. Methods using prior information (i.e., subspace-based methods) have
shown effective performance by extracting information geometry to detect OOD
data with a more appropriate distance metric. However, these methods fail to
address the geometry distorted by ill-distributed samples, due to the
limitation of statically extracting information geometry from the training
distribution. In this paper, we argue that the influence of ill-distributed
samples can be corrected by dynamically adjusting the prior geometry in
response to new data. Based on this insight, we propose a novel approach that
dynamically updates the prior covariance matrix using real-time input features,
refining its information. Specifically, we reduce the covariance along the
direction of real-time input features and constrain adjustments to the residual
space, thus preserving essential data characteristics and avoiding effects on
unintended directions in the principal space. We evaluate our method on two
pre-trained models for the CIFAR dataset and five pre-trained models for
ImageNet-1k, including the self-supervised DINO model. Extensive experiments
demonstrate that our approach significantly enhances OOD detection across
various models. The code is released at https://github.com/workerbcd/ooddcc.

</details>


### [174] [SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/pdf/2506.09403)
*Xinya Liu, Jianghao Wu, Tao Lu, Shaoting Zhang, Guotai Wang*

Main category: cs.CV

TL;DR: Proposes SRPL-SFDA, a SAM-guided method for Source-Free Domain Adaptation (SFDA) in medical image segmentation, improving pseudo-label quality and outperforming state-of-the-art SFDA methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in SFDA for medical image segmentation, such as insufficient supervision in the target domain and privacy concerns.

Method: Uses SAM-guided reliable pseudo-labels with Test-Time Tri-branch Intensity Enhancement (T3IE), a selection module for reliable pseudo-labels, and reliability-aware training.

Result: Enhances pseudo-label quality, improves SFDA performance, and outperforms state-of-the-art methods, nearing supervised training performance.

Conclusion: SRPL-SFDA is effective for SFDA in medical image segmentation, leveraging SAM's zero-shot ability and reliability-aware training.

Abstract: Domain Adaptation (DA) is crucial for robust deployment of medical image
segmentation models when applied to new clinical centers with significant
domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal
with privacy concerns and access constraints on source-domain data during
adaptation to target-domain data. However, SFDA faces challenges such as
insufficient supervision in the target domain with unlabeled images. In this
work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels
method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch
Intensity Enhancement (T3IE) that not only improves quality of raw
pseudo-labels in the target domain, but also leads to SAM-compatible inputs
with three channels to better leverage SAM's zero-shot inference ability for
refining the pseudo-labels; 2) A reliable pseudo-label selection module that
rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs
(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training
procedure in the unlabeled target domain where reliable pseudo-labels are used
for supervision and unreliable parts are regularized by entropy minimization.
Experiments conducted on two multi-domain medical image segmentation datasets
for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA
effectively enhances pseudo-label quality in the unlabeled target domain, and
improves SFDA performance by leveraging the reliability-aware training; 2)
SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is
close to that of supervised training in the target domain. The code of this
work is available online: https://github.com/HiLab-git/SRPL-SFDA.

</details>


### [175] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/pdf/2506.09411)
*Vaclav Knapp, Matyas Bohacek*

Main category: cs.CV

TL;DR: Proposes a method for synthetic human action video generation using pose transfer to address uncanny features in synthetic data, improving action recognition performance and scaling few-shot datasets.


<details>
  <summary>Details</summary>
Motivation: Synthetic data for video understanding tasks often has uncanny features, limiting its effectiveness in tasks like sign language translation and gesture recognition.

Method: Uses controllable 3D Gaussian avatar models for pose transfer to generate synthetic human action videos.

Result: Improves performance on action recognition tasks (tested on Toyota Smarthome and NTU RGB+D datasets) and scales few-shot datasets by adding diversity.

Conclusion: The method enhances synthetic data utility, addresses underrepresented groups, and is open-sourced with the RANDOM People dataset.

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [176] [Noise Conditional Variational Score Distillation](https://arxiv.org/pdf/2506.09416)
*Xinyu Peng, Ziyang Zheng, Yaoming Wang, Han Li, Nuowen Kan, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong*

Main category: cs.CV

TL;DR: NCVSD distills diffusion models into generative denoisers by leveraging unconditional score functions, enabling fast generation and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between diffusion models and efficient generative denoisers while preserving iterative refinement benefits.

Method: Integrates unconditional score functions into Variational Score Distillation (VSD) to learn denoisers for various noise levels.

Result: Outperforms teacher diffusion models, matches larger consistency models, and achieves record LPIPS in inverse problems with fewer NFEs.

Conclusion: NCVSD offers scalable, efficient generative denoising with flexible sampling and improved performance.

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel
method for distilling pretrained diffusion models into generative denoisers. We
achieve this by revealing that the unconditional score function implicitly
characterizes the score function of denoising posterior distributions. By
integrating this insight into the Variational Score Distillation (VSD)
framework, we enable scalable learning of generative denoisers capable of
approximating samples from the denoising posterior distribution across a wide
range of noise levels. The proposed generative denoisers exhibit desirable
properties that allow fast generation while preserve the benefit of iterative
refinement: (1) fast one-step generation through sampling from pure Gaussian
noise at high noise levels; (2) improved sample quality by scaling the
test-time compute with multi-step sampling; and (3) zero-shot probabilistic
inference for flexible and controllable sampling. We evaluate NCVSD through
extensive experiments, including class-conditional image generation and inverse
problem solving. By scaling the test-time compute, our method outperforms
teacher diffusion models and is on par with consistency models of larger sizes.
Additionally, with significantly fewer NFEs than diffusion-based methods, we
achieve record-breaking LPIPS on inverse problems.

</details>


### [177] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/pdf/2506.09417)
*Yunxiao Shi, Yinhao Zhu, Shizhong Han, Jisoo Jeong, Amin Ansari, Hong Cai, Fatih Porikli*

Main category: cs.CV

TL;DR: ODG combines BEV and sparse points representations for efficient 3D occupancy prediction, addressing limitations of both methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D occupancy prediction are either computationally expensive or suffer from information loss (BEV) or inefficiency (sparse points).

Method: ODG uses a dual-branch design: a query-based sparse points branch and a BEV branch, with cross-attention to share 3D information.

Result: ODG outperforms on Occ3D-nuScenes and Occ3D-Waymo benchmarks and offers competitive inference speed.

Conclusion: ODG effectively balances accuracy and efficiency for 3D occupancy prediction in autonomous driving.

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, ODG, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG
also delivers competitive inference speed when compared to the latest efficient
approaches.

</details>


### [178] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/pdf/2506.09427)
*Yukang Feng, Jianwen Sun, Chuanhao Li, Zizhen Li, Jiaxin Ai, Fanrui Zhang, Yifan Chang, Sizhuo Zhou, Shenglin Zhang, Yu Dai, Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces InterSyn, a large-scale multimodal dataset, and SynJudge, an evaluation tool, to improve LMMs' ability to generate tightly interleaved image-text outputs.


<details>
  <summary>Details</summary>
Motivation: Current LMMs struggle with generating tightly interleaved image-text outputs due to limited training datasets.

Method: The SEIR method constructs InterSyn, a dataset with multi-turn, instruction-driven dialogues and automated quality refinement. SynJudge evaluates multimodal outputs.

Result: SEIR improves dataset quality, and LMMs trained on InterSyn show uniform performance gains.

Conclusion: InterSyn and SynJudge advance multimodal systems by addressing dataset and evaluation limitations.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [179] [A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning](https://arxiv.org/pdf/2506.09429)
*Swadhin Das, Divyansh Mundra, Priyanshu Dayal, Raksha Sharma*

Main category: cs.CV

TL;DR: A lightweight transformer architecture with knowledge distillation and edge-aware enhancement improves remote sensing image captioning by reducing computational costs and capturing fine-grained details.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models for remote sensing image captioning face high computational costs and neglect fine-grained structural features.

Method: Proposes a lightweight transformer with reduced encoder dimensionality, a distilled GPT-2 decoder, knowledge distillation, and edge-aware enhancement.

Result: Significantly improves caption quality compared to state-of-the-art methods.

Conclusion: The approach effectively balances performance and efficiency while capturing fine-grained spatial details.

Abstract: Transformer-based models have achieved strong performance in remote sensing
image captioning by capturing long-range dependencies and contextual
information. However, their practical deployment is hindered by high
computational costs, especially in multi-modal frameworks that employ separate
transformer-based encoders and decoders. In addition, existing remote sensing
image captioning models primarily focus on high-level semantic extraction while
often overlooking fine-grained structural features such as edges, contours, and
object boundaries. To address these challenges, a lightweight transformer
architecture is proposed by reducing the dimensionality of the encoder layers
and employing a distilled version of GPT-2 as the decoder. A knowledge
distillation strategy is used to transfer knowledge from a more complex teacher
model to improve the performance of the lightweight network. Furthermore, an
edge-aware enhancement strategy is incorporated to enhance image representation
and object boundary understanding, enabling the model to capture fine-grained
spatial details in remote sensing images. Experimental results demonstrate that
the proposed approach significantly improves caption quality compared to
state-of-the-art methods.

</details>


### [180] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/pdf/2506.09445)
*Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha*

Main category: cs.CV

TL;DR: TOGA is a vision-language model for weakly supervised video QA with temporal grounding, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing video QA with temporal grounding without temporal annotations, leveraging weak supervision.

Method: TOGA is instruct-tuned to jointly generate answers and temporal grounding using pseudo labels and consistency constraints.

Result: State-of-the-art performance on NExT-GQA, MSVD-QA, and ActivityNet-QA benchmarks.

Conclusion: Jointly generating answers and grounding improves performance, validating TOGA's effectiveness in weakly supervised setups.

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [181] [Harmonizing and Merging Source Models for CLIP-based Domain Generalization](https://arxiv.org/pdf/2506.09446)
*Yuhe Ding, Jian Liang, Bo Jiang, Zi Wang, Aihua Zheng, Bin Luo*

Main category: cs.CV

TL;DR: HAM improves CLIP-based domain generalization by harmonizing and merging source models to avoid conflicts and enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from sample and optimization conflicts during multi-source training, hindering generalization.

Method: HAM enriches source samples, harmonizes model updates, and merges models redundantly-aware.

Result: Achieves state-of-the-art performance on five benchmark datasets.

Conclusion: HAM effectively consolidates domain information and enhances generalization.

Abstract: CLIP-based domain generalization aims to improve model generalization to
unseen domains by leveraging the powerful zero-shot classification capabilities
of CLIP and multiple source datasets. Existing methods typically train a single
model across multiple source domains to capture domain-shared information.
However, this paradigm inherently suffers from two types of conflicts: 1)
sample conflicts, arising from noisy samples and extreme domain shifts among
sources; and 2) optimization conflicts, stemming from competition and
trade-offs during multi-source training. Both hinder the generalization and
lead to suboptimal solutions. Recent studies have shown that model merging can
effectively mitigate the competition of multi-objective optimization and
improve generalization performance. Inspired by these findings, we propose
Harmonizing and Merging (HAM), a novel source model merging framework for
CLIP-based domain generalization. During the training process of the source
models, HAM enriches the source samples without conflicting samples, and
harmonizes the update directions of all models. Then, a redundancy-aware
historical model merging method is introduced to effectively integrate
knowledge across all source models. HAM comprehensively consolidates source
domain information while enabling mutual enhancement among source models,
ultimately yielding a final model with optimal generalization capabilities.
Extensive experiments on five widely used benchmark datasets demonstrate the
effectiveness of our approach, achieving state-of-the-art performance.

</details>


### [182] [Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization](https://arxiv.org/pdf/2506.09460)
*Amirreza Khoshbakht, Erchan Aptoula*

Main category: cs.CV

TL;DR: Proposes a novel open-set domain generalization framework for hyperspectral image classification, addressing unknown classes and domain shifts without target domain data.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in open-set scenarios with unknown classes and require target domain data, leading to negative transfer and poor performance.

Method: Combines Spectrum-Invariant Frequency Disentanglement (SIFD), Dual-Channel Residual Network (DCRN), Evidential Deep Learning (EDL), and Spectral-Spatial Uncertainty Disentanglement (SSUD) for domain-agnostic feature extraction, robust learning, uncertainty quantification, and reliable open-set classification.

Result: Achieves performance comparable to state-of-the-art domain adaptation methods without target domain data during training.

Conclusion: The framework effectively addresses open-set domain generalization challenges in hyperspectral image classification.

Abstract: Open-set domain generalization(OSDG) for hyperspectral image classification
presents significant challenges due to the presence of unknown classes in
target domains and the need for models to generalize across multiple unseen
domains without target-specific adaptation. Existing domain adaptation methods
assume access to target domain data during training and fail to address the
fundamental issue of domain shift when unknown classes are present, leading to
negative transfer and reduced classification performance. To address these
limitations, we propose a novel open-set domain generalization framework that
combines four key components: Spectrum-Invariant Frequency Disentanglement
(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network
(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning
(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty
Disentanglement (SSUD) for reliable open-set classification. The SIFD module
extracts domain-invariant spectral features in the frequency domain through
attention-weighted frequency analysis and domain-agnostic regularization, while
DCRN captures complementary spectral and spatial information via parallel
pathways with adaptive fusion. EDL provides principled uncertainty estimation
using Dirichlet distributions, enabling the SSUD module to make reliable
open-set decisions through uncertainty-aware pathway weighting and adaptive
rejection thresholding. Experimental results on three cross-scene hyperspectral
classification tasks show that our approach achieves performance comparable to
state-of-the-art domain adaptation methods while requiring no access to the
target domain during training. The implementation will be made available at
https://github.com/amir-khb/SSUDOSDG upon acceptance.

</details>


### [183] [Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing](https://arxiv.org/pdf/2506.09469)
*Maria Damanaki, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos*

Main category: cs.CV

TL;DR: A novel Cooperative MOT framework for 3D LiDAR scenes improves tracking by fusing multi-agent data via graph topology-aware optimization.


<details>
  <summary>Details</summary>
Motivation: Single-agent MOT struggles with occlusions and sensor failures, necessitating multi-agent collaboration for better environmental understanding.

Method: Uses a fully connected graph topology and Graph Laplacian optimization to refine bounding box positions and associate them with tracked objects.

Result: Outperforms baseline frameworks like DMSTrack and V2V4Real on the V2V4Real dataset.

Conclusion: The proposed method enhances localization and tracking accuracy by leveraging multi-agent coherence.

Abstract: Multi-Object Tracking (MOT) plays a crucial role in autonomous driving
systems, as it lays the foundations for advanced perception and precise path
planning modules. Nonetheless, single agent based MOT lacks in sensing
surroundings due to occlusions, sensors failures, etc. Hence, the integration
of multiagent information is essential for comprehensive understanding of the
environment. This paper proposes a novel Cooperative MOT framework for tracking
objects in 3D LiDAR scene by formulating and solving a graph topology-aware
optimization problem so as to fuse information coming from multiple vehicles.
By exploiting a fully connected graph topology defined by the detected bounding
boxes, we employ the Graph Laplacian processing optimization technique to
smooth the position error of bounding boxes and effectively combine them. In
that manner, we reveal and leverage inherent coherences of diverse multi-agent
detections, and associate the refined bounding boxes to tracked objects at two
stages, optimizing localization and tracking accuracies. An extensive
evaluation study has been conducted, using the real-world V2V4Real dataset,
where the proposed method significantly outperforms the baseline frameworks,
including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various
testing sequences.

</details>


### [184] [Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning](https://arxiv.org/pdf/2506.09473)
*Cheng Chen, Yunpeng Zhai, Yifan Zhao, Jinyang Gao, Bolin Ding, Jia Li*

Main category: cs.CV

TL;DR: The paper introduces a reinforcement learning framework for multi-modal demonstration selection in Large Vision-Language Models (LVLMs) to improve in-context learning (ICL).


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods rely on pre-defined or heuristic demonstration selection, which is inadequate for diverse tasks and ignores interactions between demonstrations.

Method: A reinforcement learning framework is proposed to explore and exploit policies for adaptive multi-modal demonstration selection.

Result: The approach outperforms existing methods on four Visual Question-Answering datasets, enhancing few-shot LVLM generalization.

Conclusion: The framework autonomously refines demonstration selection, improving ICL effectiveness in LVLMs.

Abstract: In-context learning (ICL), a predominant trend in instruction learning, aims
at enhancing the performance of large language models by providing clear task
guidance and examples, improving their capability in task understanding and
execution. This paper investigates ICL on Large Vision-Language Models (LVLMs)
and explores the policies of multi-modal demonstration selection. Existing
research efforts in ICL face significant challenges: First, they rely on
pre-defined demonstrations or heuristic selecting strategies based on human
intuition, which are usually inadequate for covering diverse task requirements,
leading to sub-optimal solutions; Second, individually selecting each
demonstration fails in modeling the interactions between them, resulting in
information redundancy. Unlike these prevailing efforts, we propose a new
exploration-exploitation reinforcement learning framework, which explores
policies to fuse multi-modal information and adaptively select adequate
demonstrations as an integrated whole. The framework allows LVLMs to optimize
themselves by continually refining their demonstrations through
self-exploration, enabling the ability to autonomously identify and generate
the most effective selection policies for in-context learning. Experimental
results verify the superior performance of our approach on four Visual
Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing
the generalization capability of few-shot LVLMs.

</details>


### [185] [Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries](https://arxiv.org/pdf/2506.09476)
*Tianxiang Hao, Lixian Zhang, Yingjia Zhang, Mengxuan Chen, Jinxiao Zhang, Haohuan Fu*

Main category: cs.CV

TL;DR: The paper introduces Urban1960SatBench, a novel annotated segmentation dataset for historical satellite imagery, and Urban1960SatUSM, an unsupervised segmentation framework to address quality degradation and annotation absence in such data.


<details>
  <summary>Details</summary>
Motivation: Historical satellite imagery provides insights into early urban development but suffers from quality issues and lack of annotations, hindering semantic segmentation.

Method: The authors create Urban1960SatBench, an annotated dataset, and propose Urban1960SatUSM, an unsupervised framework using confidence-aware alignment and focal-confidence loss for robust segmentation.

Result: Urban1960SatUSM outperforms existing methods on Urban1960SatBench, enabling better segmentation of historical urban scenes.

Conclusion: The work advances quantitative studies of long-term urban change by addressing challenges in historical satellite imagery segmentation.

Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,
offers rare insights into understanding early urban development and long-term
transformation. However, severe quality degradation (e.g., distortion,
misalignment, and spectral scarcity) and annotation absence have long hindered
semantic segmentation on such historical RS imagery. To bridge this gap and
enhance understanding of urban development, we introduce
$\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on
historical satellite imagery with the earliest observation time among all
existing segmentation datasets, along with a benchmark framework for
unsupervised segmentation tasks, $\textbf{Urban1960SatUSM}$. First,
$\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic
segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering
1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the
earliest segmentation dataset of its kind, it provides a pioneering benchmark
for historical urban understanding. Second,
$\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel
unsupervised semantic segmentation framework for historical RS imagery. It
employs a confidence-aware alignment mechanism and focal-confidence loss based
on a self-supervised learning architecture, which generates robust
pseudo-labels and adaptively prioritizes prediction difficulty and label
reliability to improve unsupervised segmentation on noisy historical data
without manual supervision. Experiments show Urban1960SatUSM significantly
outperforms existing unsupervised segmentation methods on Urban1960SatSeg for
segmenting historical urban scenes, promising in paving the way for
quantitative studies of long-term urban change using modern computer vision.
Our benchmark and supplementary material are available at
https://github.com/Tianxiang-Hao/Urban1960SatSeg.

</details>


### [186] [TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation](https://arxiv.org/pdf/2506.09479)
*Zetian Song, Jiaye Fu, Jiaqi Zhang, Xiaohan Lu, Chuanmin Jia, Siwei Ma, Wen Gao*

Main category: cs.CV

TL;DR: TinySplat is a feedforward method for compressing 3D Gaussian Splatting (3DGS) representations, reducing storage costs by 100x while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS compression methods are incompatible with feedforward approaches, leading to high storage costs. TinySplat addresses this by eliminating redundancy without scene-wise optimization.

Method: TinySplat integrates View-Projection Transformation (VPT) for geometric redundancy, Visibility-Aware Basis Reduction (VABR) for perceptual redundancy, and a video codec for spatial redundancy.

Result: Achieves 100x compression, comparable quality at 6% storage size, and significantly faster encoding (25%) and decoding (1%) times.

Conclusion: TinySplat offers an efficient, training-free solution for compressing 3DGS representations, enabling practical deployment of feedforward methods.

Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from sparse input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.

</details>


### [187] [Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression](https://arxiv.org/pdf/2506.09482)
*Dingcheng Zhen, Qian Qiao, Tan Yu, Kangxi Wu, Ziwei Zhang, Siyuan Liu, Shunshun Yin, Ming Tao*

Main category: cs.CV

TL;DR: TransDiff combines AR Transformer and diffusion models for image generation, achieving superior performance on ImageNet 256x256 with faster inference and introducing MRAR for further improvements.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AR Transformer and diffusion models for better image generation performance and efficiency.

Method: Jointly encodes labels and images into semantic features using AR Transformer and diffusion models, and introduces MRAR for multi-reference autoregression.

Result: Achieves FID of 1.61, IS of 293.4, and faster inference (x2 vs. AR Transformer, x112 vs. diffusion-only). MRAR further reduces FID to 1.42.

Conclusion: TransDiff sets a new benchmark in image generation, with MRAR enhancing diversity and quality.

Abstract: We introduce TransDiff, the first image generation model that marries
Autoregressive (AR) Transformer with diffusion models. In this joint modeling
framework, TransDiff encodes labels and images into high-level semantic
features and employs a diffusion model to estimate the distribution of image
samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms
other image generation models based on standalone AR Transformer or diffusion
models. Specifically, TransDiff achieves a Fr\'echet Inception Distance (FID)
of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster
inference latency compared to state-of-the-art methods based on AR Transformer
and x112 faster inference compared to diffusion-only models. Furthermore,
building on the TransDiff model, we introduce a novel image generation paradigm
called Multi-Reference Autoregression (MRAR), which performs autoregressive
generation by predicting the next image. MRAR enables the model to reference
multiple previously generated images, thereby facilitating the learning of more
diverse representations and improving the quality of generated images in
subsequent iterations. By applying MRAR, the performance of TransDiff is
improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open
up a new frontier in the field of image generation.

</details>


### [188] [HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](https://arxiv.org/pdf/2506.09518)
*Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang*

Main category: cs.CV

TL;DR: HAIF-GS improves dynamic 3D scene reconstruction from monocular videos by addressing redundant updates, insufficient motion supervision, and non-rigid deformations using anchor-driven deformation.


<details>
  <summary>Details</summary>
Motivation: Dynamic 3D scene reconstruction from monocular videos is challenging due to inconsistent motion representations in existing methods, leading to inefficiencies and poor quality.

Method: HAIF-GS uses sparse anchor-driven deformation with an Anchor Filter, Induced Flow-Guided Deformation, and Hierarchical Anchor Propagation to model motion coherently.

Result: HAIF-GS outperforms prior methods in rendering quality, temporal coherence, and efficiency on synthetic and real-world benchmarks.

Conclusion: HAIF-GS provides a robust solution for dynamic 3D reconstruction, addressing key limitations of existing approaches.

Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.

</details>


### [189] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/pdf/2506.09522)
*Beomsik Cho, Jaehyung Kim*

Main category: cs.CV

TL;DR: ReVisiT is a decoding method for LVLMs that improves visual grounding by referencing vision tokens during text generation, outperforming baselines with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Conventional LVLM decoding strategies often fail to utilize visual information effectively, leading to ungrounded responses. Existing solutions require extra training or resources.

Method: ReVisiT projects vision tokens into text token space, dynamically selects relevant tokens via divergence minimization, and refines output distributions for better visual semantics.

Result: ReVisiT enhances visual grounding on benchmarks, outperforming baselines with up to 2× lower computational costs.

Conclusion: ReVisiT offers a simple, effective solution for improving LVLM visual grounding without additional training or resources.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [190] [Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS](https://arxiv.org/pdf/2506.09534)
*Tao Wang, Mengyu Li, Geduo Zeng, Cheng Meng, Qiong Zhang*

Main category: cs.CV

TL;DR: A novel optimal transport-based method for compacting 3D Gaussian Splatting (3DGS) reduces redundancy while maintaining rendering quality, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: 3DGS often uses millions of redundant Gaussian primitives, straining memory and rendering resources. Existing compaction methods lack global fidelity guarantees.

Method: Proposes an optimal transport perspective to cast 3DGS compaction as Gaussian mixture reduction, minimizing transport divergence and decoupling appearance from geometry.

Result: Achieves negligible quality loss with only 10% of Gaussians and outperforms state-of-the-art compaction techniques.

Conclusion: The method offers an efficient, agnostic solution for lightweight neural rendering, applicable to various 3DGS pipelines.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance
field rendering, but it typically requires millions of redundant Gaussian
primitives, overwhelming memory and rendering budgets. Existing compaction
approaches address this by pruning Gaussians based on heuristic importance
scores, without global fidelity guarantee. To bridge this gap, we propose a
novel optimal transport perspective that casts 3DGS compaction as global
Gaussian mixture reduction. Specifically, we first minimize the composite
transport divergence over a KD-tree partition to produce a compact geometric
representation, and then decouple appearance from geometry by fine-tuning color
and opacity attributes with far fewer Gaussian primitives. Experiments on
benchmark datasets show that our method (i) yields negligible loss in rendering
quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;
and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.
Notably, our method is applicable to any stage of vanilla or accelerated 3DGS
pipelines, providing an efficient and agnostic pathway to lightweight neural
rendering.

</details>


### [191] [AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches](https://arxiv.org/pdf/2506.09538)
*Wenjun Ji, Yuxiang Fu, Luyang Ying, Deng-Ping Fan, Yuyi Wang, Ming-Ming Cheng, Ivor Tsang, Qing Guo*

Main category: cs.CV

TL;DR: The paper introduces Angle-Robust Concept Learning (AngleRoCL) to enhance the angle robustness of text-to-image adversarial patches, improving attack effectiveness across multiple views.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect the angle robustness of adversarial patches, limiting their real-world effectiveness. This study aims to address this gap.

Method: Proposes AngleRoCL, which learns generalizable text embeddings to guide T2I models in generating angle-robust patches.

Result: AngleRoCL significantly improves angle robustness, with over 50% average relative improvement in attack effectiveness across multiple angles.

Conclusion: The research advances understanding of angle-robust patches and explores the link between textual concepts and physical properties in T2I-generated content.

Abstract: Cutting-edge works have demonstrated that text-to-image (T2I) diffusion
models can generate adversarial patches that mislead state-of-the-art object
detectors in the physical world, revealing detectors' vulnerabilities and
risks. However, these methods neglect the T2I patches' attack effectiveness
when observed from different views in the physical world (i.e., angle
robustness of the T2I adversarial patches). In this paper, we study the angle
robustness of T2I adversarial patches comprehensively, revealing their
angle-robust issues, demonstrating that texts affect the angle robustness of
generated patches significantly, and task-specific linguistic instructions fail
to enhance the angle robustness. Motivated by the studies, we introduce
Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that
learns a generalizable concept (i.e., text embeddings in implementation)
representing the capability of generating angle-robust patches. The learned
concept can be incorporated into textual prompts and guides T2I models to
generate patches with their attack effectiveness inherently resistant to
viewpoint variations. Through extensive simulation and physical-world
experiments on five SOTA detectors across multiple views, we demonstrate that
AngleRoCL significantly enhances the angle robustness of T2I adversarial
patches compared to baseline methods. Our patches maintain high attack success
rates even under challenging viewing conditions, with over 50% average relative
improvement in attack effectiveness across multiple angles. This research
advances the understanding of physically angle-robust patches and provides
insights into the relationship between textual concepts and physical properties
in T2I-generated contents.

</details>


### [192] [3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection](https://arxiv.org/pdf/2506.09541)
*Yi Zhang, Yi Wang, Yawen Cui, Lap-Pui Chau*

Main category: cs.CV

TL;DR: 3DGeoDet is a geometry-aware 3D object detection method using RGB images, improving performance via explicit and implicit 3D representations without 3D supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of 3D geometric cues in image-based 3D object detection, which causes ambiguity in image-to-3D correspondences.

Method: Generates 3D geometric representations using predicted depth, combining explicit voxel occupancy attention and implicit TSDF for enhanced 3D awareness.

Result: Outperforms state-of-the-art methods, with significant mAP improvements on SUN RGB-D, ScanNetV2, and KITTI datasets.

Conclusion: 3DGeoDet effectively improves 3D geometry comprehension and detection performance, demonstrating general-purpose applicability.

Abstract: This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection
approach that effectively handles single- and multi-view RGB images in indoor
and outdoor environments, showcasing its general-purpose applicability. The key
challenge for image-based 3D object detection tasks is the lack of 3D geometric
cues, which leads to ambiguity in establishing correspondences between images
and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D
geometric representations in both explicit and implicit manners based on
predicted depth information. Specifically, we utilize the predicted depth to
learn voxel occupancy and optimize the voxelized 3D feature volume explicitly
through the proposed voxel occupancy attention. To further enhance 3D
awareness, the feature volume is integrated with an implicit 3D representation,
the truncated signed distance function (TSDF). Without requiring supervision
from 3D signals, we significantly improve the model's comprehension of 3D
geometry by leveraging intermediate 3D representations and achieve end-to-end
training. Our approach surpasses the performance of state-of-the-art
image-based methods on both single- and multi-view benchmark datasets across
diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D
dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19
AP3D@0.7 improvement on the KITTI dataset. The project page is available at:
https://cindy0725.github.io/3DGeoDet/.

</details>


### [193] [GLD-Road:A global-local decoding road network extraction model for remote sensing images](https://arxiv.org/pdf/2506.09553)
*Ligao Deng, Yupeng Deng, Yu Meng, Jingbo Chen, Zhihao Xi, Diyou Liu, Qifeng Chu*

Main category: cs.CV

TL;DR: GLD-Road is a two-stage model for road network extraction, combining global efficiency and local precision, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of road networks is costly, and current deep learning methods either sacrifice accuracy for speed or vice versa.

Method: GLD-Road uses a two-stage approach: global detection of road nodes and connections, followed by local iterative refinement of broken roads.

Result: GLD-Road improves APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3), and reduces retrieval time by 40% vs. Sat2Graph and 92% vs. RNGDet++.

Conclusion: GLD-Road effectively balances speed and accuracy, making it a superior solution for road network extraction.

Abstract: Road networks are crucial for mapping, autonomous driving, and disaster
response. While manual annotation is costly, deep learning offers efficient
extraction. Current methods include postprocessing (prone to errors), global
parallel (fast but misses nodes), and local iterative (accurate but slow). We
propose GLD-Road, a two-stage model combining global efficiency and local
precision. First, it detects road nodes and connects them via a Connect Module.
Then, it iteratively refines broken roads using local searches, drastically
reducing computation. Experiments show GLD-Road outperforms state-of-the-art
methods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also
reduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++
(local). The experimental results are available at
https://github.com/ucas-dlg/GLD-Road.

</details>


### [194] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/pdf/2506.09557)
*Zhaoyang Wei, Chenhui Qiang, Bowen Jiang, Xumeng Han, Xuehui Yu, Zhenjun Han*

Main category: cs.CV

TL;DR: AD^2-Bench is the first Chain-of-Thought benchmark for autonomous driving in adverse weather and complex scenes, featuring 5.4k annotated CoT instances and revealing MLLMs' accuracy below 60%.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack rigorous evaluation of CoT reasoning in challenging autonomous driving scenarios, necessitating a dedicated benchmark.

Method: AD^2-Bench is designed with comprehensive data coverage, fine-grained annotations for multi-step reasoning, and a tailored evaluation framework.

Result: State-of-the-art MLLMs achieve below 60% accuracy on AD^2-Bench, underscoring its difficulty and the need for improvement.

Conclusion: AD^2-Bench serves as a standardized platform to advance MLLMs' reasoning in autonomous driving, addressing critical gaps in current benchmarks.

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [195] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/pdf/2506.09634)
*Yanzhao Shi, Xiaodan Zhang, Junzhong Ji, Haoning Jiang, Chengxin Zheng, Yinong Wang, Liangqiong Qu*

Main category: cs.CV

TL;DR: HSENet improves 3D medical image analysis by combining dual-3D vision encoders and Spatial Packer for better visual-language understanding, outperforming existing methods in retrieval, report generation, and question answering.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models (MLLMs) focus on 2D medical images, limiting their ability to capture 3D anatomical structures and leading to diagnostic errors.

Method: HSENet uses dual-3D vision encoders for global and fine-grained details, pre-trained with diagnostic reports, and Spatial Packer to compress 3D regions into visual tokens for LLM integration.

Result: Achieves state-of-the-art performance in 3D language-visual retrieval (+5.96% gain), report generation (+8.01% gain), and visual question answering (+1.99% gain).

Conclusion: HSENet effectively bridges 3D medical imaging and language understanding, enhancing diagnostic accuracy and workflow efficiency.

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [196] [SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields](https://arxiv.org/pdf/2506.09565)
*Qijing Li, Jingxiang Sun, Liang An, Zhaoqi Su, Hongwen Zhang, Yebin Liu*

Main category: cs.CV

TL;DR: SemanticSplat is a feed-forward method for holistic 3D scene understanding, unifying 3D Gaussians with semantic attributes to improve geometry, appearance, and semantics modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods either lack holistic comprehension or require dense input views, limiting practicality.

Method: SemanticSplat fuses feature fields (e.g., LSeg, SAM) with a cost volume representation and uses a two-stage distillation framework for sparse-view reconstruction.

Result: The method achieves coherent and accurate scene comprehension, excelling in tasks like promptable and open-vocabulary segmentation.

Conclusion: SemanticSplat advances 3D scene understanding by unifying multi-modal semantics with geometry and appearance, offering practical deployment benefits.

Abstract: Holistic 3D scene understanding, which jointly models geometry, appearance,
and semantics, is crucial for applications like augmented reality and robotic
interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM)
are limited to extracting language-based semantics from scenes, failing to
achieve holistic scene comprehension. Additionally, they suffer from
low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene
optimization methods rely on dense input views, which reduces practicality and
increases complexity during deployment. In this paper, we propose
SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which
unifies 3D Gaussians with latent semantic attributes for joint
geometry-appearance-semantics modeling. To predict the semantic anisotropic
Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a
cost volume representation that stores cross-view feature similarities,
enhancing coherent and accurate scene comprehension. Leveraging a two-stage
distillation framework, SemanticSplat reconstructs a holistic multi-modal
semantic feature field from sparse-view images. Experiments demonstrate the
effectiveness of our method for 3D scene understanding tasks like promptable
and open-vocabulary segmentation. Video results are available at
https://semanticsplat.github.io.

</details>


### [197] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/pdf/2506.09644)
*Dongxu Liu, Yuang Peng, Haomiao Tang, Yuwei Chen, Chunrui Han, Zheng Ge, Daxin Jiang, Mingxue Liao*

Main category: cs.CV

TL;DR: DGAE improves autoencoder performance under high compression by using a diffusion model to guide the decoder, achieving smaller latent spaces and better results.


<details>
  <summary>Details</summary>
Motivation: Addressing training instability and performance degradation in autoencoders under high compression, while minimizing latent space dimensionality.

Method: Proposes DGAE, which employs a diffusion model to guide the decoder for better signal recovery from compressed representations.

Result: DGAE mitigates performance degradation, achieves state-of-the-art results with a 2x smaller latent space, and improves diffusion model convergence.

Conclusion: DGAE offers a compact and efficient solution for high-compression autoencoders, enhancing generative model performance.

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [198] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/pdf/2506.09612)
*Mingxiao LI, mang ning, Marie-Francine Moens*

Main category: cs.CV

TL;DR: A novel training-free sampling strategy, Zigzag Sampling with Asymmetric Prompts and Visual Sharing, improves subject consistency in text-to-image generation for visual storytelling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for maintaining subject consistency in visual storytelling are either resource-intensive or ineffective.

Method: Proposes zigzag sampling with asymmetric prompts and a visual sharing module to enhance consistency without training.

Result: Outperforms previous methods in generating coherent and consistent visual stories, validated by metrics and evaluations.

Conclusion: The approach offers a resource-efficient and effective solution for consistent visual story generation.

Abstract: Text-to-image generation models have made significant progress in producing
high-quality images from textual descriptions, yet they continue to struggle
with maintaining subject consistency across multiple images, a fundamental
requirement for visual storytelling. Existing methods attempt to address this
by either fine-tuning models on large-scale story visualization datasets, which
is resource-intensive, or by using training-free techniques that share
information across generations, which still yield limited success. In this
paper, we introduce a novel training-free sampling strategy called Zigzag
Sampling with Asymmetric Prompts and Visual Sharing to enhance subject
consistency in visual story generation. Our approach proposes a zigzag sampling
mechanism that alternates between asymmetric prompting to retain subject
characteristics, while a visual sharing module transfers visual cues across
generated images to %further enforce consistency. Experimental results, based
on both quantitative metrics and qualitative evaluations, demonstrate that our
method significantly outperforms previous approaches in generating coherent and
consistent visual stories. The code is available at
https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [199] [ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting](https://arxiv.org/pdf/2506.09626)
*Giacomo Rosin, Muhammad Rameez Ur Rahman, Sebastiano Vascon*

Main category: cs.CV

TL;DR: The paper introduces ECAM, a contrastive learning-based module to improve collision avoidance in human trajectory forecasting by integrating environmental context.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory forecasting methods often neglect environmental impact, leading to collisions. ECAM addresses this gap.

Method: ECAM uses contrastive learning to enhance collision avoidance and integrates with existing models.

Result: Experiments on ETH/UCY dataset show a 40-50% reduction in collision rates when ECAM is added to state-of-the-art methods.

Conclusion: ECAM effectively improves collision avoidance in trajectory forecasting, as demonstrated by significant collision rate reductions.

Abstract: Human trajectory forecasting is crucial in applications such as autonomous
driving, robotics and surveillance. Accurate forecasting requires models to
consider various factors, including social interactions, multi-modal
predictions, pedestrian intention and environmental context. While existing
methods account for these factors, they often overlook the impact of the
environment, which leads to collisions with obstacles. This paper introduces
ECAM (Environmental Collision Avoidance Module), a contrastive learning-based
module to enhance collision avoidance ability with the environment. The
proposed module can be integrated into existing trajectory forecasting models,
improving their ability to generate collision-free predictions. We evaluate our
method on the ETH/UCY dataset and quantitatively and qualitatively demonstrate
its collision avoidance capabilities. Our experiments show that
state-of-the-art methods significantly reduce (-40/50%) the collision rate when
integrated with the proposed module. The code is available at
https://github.com/CVML-CFU/ECAM.

</details>


### [200] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/pdf/2506.09677)
*Bin Zhu, Hailong Yin, Jingjing Chen, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper evaluates reasoning models' vulnerability to gaslighting negation prompts, revealing significant accuracy drops, and introduces GaslightingBench-R to further test this weakness.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of reasoning models against misleading user input, an underexplored area.

Method: Systematic evaluation of three state-of-the-art reasoning models (OpenAI's o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) on multimodal benchmarks (MMMU, MathVista, CharXiv), followed by the creation of GaslightingBench-R.

Result: Accuracy drops of 25-29% under gaslighting prompts, worsening to over 53% on GaslightingBench-R.

Conclusion: Reasoning models have fundamental robustness limitations, showing a gap between step-by-step reasoning and belief persistence.

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [201] [Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation](https://arxiv.org/pdf/2506.09663)
*Haowen Wang, Xiaoping Yuan, Zhao Jin, Zhen Zhao, Zhengping Che, Yousong Xue, Jin Tian, Yakun Huang, Jian Tang*

Main category: cs.CV

TL;DR: DeGSS introduces a unified framework for 3D representation of articulated objects using deformable Gaussian fields, enabling unsupervised part segmentation and precise kinematic modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with unified 3D representations of articulated objects without human annotation, limiting applications.

Method: DeGSS encodes objects as deformable 3D Gaussian fields, modeling interaction states as smooth deformations and enabling unsupervised part segmentation.

Result: The method outperforms existing approaches in accuracy and stability, validated on synthetic and real-world datasets.

Conclusion: DeGSS provides a robust, unsupervised solution for 3D representation of articulated objects, advancing part-level reconstruction and kinematic modeling.

Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D
representations of their geometry and motion are critical for numerous
applications. However, in the absence of human annotation, existing approaches
still struggle to build a unified representation for objects that contain
multiple movable parts. We introduce DeGSS, a unified framework that encodes
articulated objects as deformable 3D Gaussian fields, embedding geometry,
appearance, and motion in one compact representation. Each interaction state is
modeled as a smooth deformation of a shared field, and the resulting
deformation trajectories guide a progressive coarse-to-fine part segmentation
that identifies distinct rigid components, all in an unsupervised manner. The
refined field provides a spatially continuous, fully decoupled description of
every part, supporting part-level reconstruction and precise modeling of their
kinematic relationships. To evaluate generalization and realism, we enlarge the
synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset
that pairs RGB captures with accurately reverse-engineered 3D models. Extensive
experiments demonstrate that our method outperforms existing methods in both
accuracy and stability.

</details>


### [202] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/pdf/2506.09695)
*Changwei Wu, Yifei Chen, Yuxin Du, Jinying Zong, Jie Dong, Mingxuan Liu, Yong Peng, Jin Fan, Feiwei Qin, Changmiao Wang*

Main category: cs.CV

TL;DR: FasterSNN, a hybrid spiking neural network, improves Alzheimer's Disease diagnosis by combining LIF neurons, region-adaptive convolution, and multi-scale spiking attention for efficient and stable 3D MRI processing.


<details>
  <summary>Details</summary>
Motivation: Early AD diagnosis is challenging due to subjective assessments and costly imaging. Deep learning methods are inefficient, while SNNs, though promising, suffer from weak expressiveness and unstable training.

Method: FasterSNN integrates LIF neurons with region-adaptive convolution and multi-scale spiking attention to process 3D MRI efficiently.

Result: Benchmark tests show FasterSNN achieves competitive performance with improved efficiency and stability.

Conclusion: FasterSNN is a practical solution for AD screening, balancing accuracy and computational efficiency.

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [203] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/pdf/2506.09668)
*Maik Dannecker, Vasiliki Sideri-Lampretsa, Sophie Starck, Angeline Mihailov, Mathieu Milh, Nadine Girard, Guillaume Auzias, Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA is a novel framework for creating high-resolution, multimodal brain atlases in low-data settings, outperforming traditional methods in speed and flexibility.


<details>
  <summary>Details</summary>
Motivation: Studying rapid neurodevelopment in fetal and neonatal brains requires accurate atlases, but existing methods depend on large datasets, which are scarce for pathological cases.

Method: CINeMA operates in latent space, avoiding intensive image registration and enabling flexible conditioning on anatomical features like age and pathologies.

Result: CINeMA reduces atlas construction time from days to minutes and supports tasks like segmentation, age prediction, and synthetic data creation.

Conclusion: CINeMA is a versatile and efficient tool for brain research, with applications in segmentation, prediction, and data augmentation.

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [204] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/pdf/2506.09691)
*Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune*

Main category: cs.CV

TL;DR: The paper proposes an inference-time technique to improve compositionality in dual encoder Vision-Language Models (VLMs) by structuring image and text inputs into smaller segments and aggregating their similarities, achieving performance gains without training.


<details>
  <summary>Details</summary>
Motivation: Dual encoder VLMs like CLIP struggle with compositionality, limiting retrieval performance. While training approaches have been explored, inference-time techniques are understudied.

Method: The method involves: i) dividing images into crops, ii) extracting text segments (objects, attributes, relations), iii) aligning crops with text segments using a VLM, and iv) aggregating similarities for the final score.

Result: The approach consistently improves VLM performance on compositionality tasks, especially for attribute-object binding, without requiring training. Image crop processing is key to the gains.

Conclusion: Inference-time techniques hold promise for enhancing VLMs, with further improvements possible in specific areas.

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [205] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/pdf/2506.09718)
*Xulin Ma, Jiankai Tang, Zhang Jiang, Songqin Cheng, Yuanchun Shi, Dong LI, Xin Liu, Daniel McDuff, Xiaojing Liu, Yuntao Wang*

Main category: cs.CV

TL;DR: The paper introduces LADH, a dataset for long-term rPPG monitoring in challenging environments, showing RGB+IR fusion improves accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges like lighting variations and occlusions in long-term rPPG for personal care, especially in high-altitude settings.

Method: Uses synchronized RGB and IR facial videos from 21 participants across five scenarios, with ground-truth physiological signals.

Result: RGB+IR fusion reduces heart rate estimation MAE to 4.99 BPM; multi-task learning boosts performance for multiple indicators.

Conclusion: LADH dataset and RGB+IR fusion enhance rPPG robustness, with open-source data and code for further research.

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [206] [CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings](https://arxiv.org/pdf/2506.09699)
*Mattia Nardon, Mikel Mujika Agirre, Ander González Tomé, Daniel Sedano Algarabel, Josep Rueda Collell, Ana Paola Caro, Andrea Caraffa, Fabio Poiesi, Paul Ian Chippendale, Davide Boscaini*

Main category: cs.CV

TL;DR: CHIP is a new dataset for 6D pose estimation of chairs in industrial settings, addressing gaps in existing benchmarks by including real-world challenges like occlusions and distractors.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack realism for industrial applications, focusing on household objects or artificial setups. CHIP fills this gap by providing a dataset for chairs manipulated by robots in real-world industrial environments.

Method: CHIP includes 77,811 RGBD images of seven chairs, annotated with ground-truth 6D poses derived from robot kinematics. It uses three RGBD sensing technologies and introduces challenges like occlusions and fine-grained distractors.

Result: Benchmarking with three zero-shot 6D pose estimation methods reveals significant room for improvement, emphasizing the dataset's unique challenges.

Conclusion: CHIP addresses a critical need for realistic industrial 6D pose estimation benchmarks and will be publicly released to advance research in this area.

Abstract: Accurate 6D pose estimation of complex objects in 3D environments is
essential for effective robotic manipulation. Yet, existing benchmarks fall
short in evaluating 6D pose estimation methods under realistic industrial
conditions, as most datasets focus on household objects in domestic settings,
while the few available industrial datasets are limited to artificial setups
with objects placed on tables. To bridge this gap, we introduce CHIP, the first
dataset designed for 6D pose estimation of chairs manipulated by a robotic arm
in a real-world industrial environment. CHIP includes seven distinct chairs
captured using three different RGBD sensing technologies and presents unique
challenges, such as distractor objects with fine-grained differences and severe
occlusions caused by the robotic arm and human operators. CHIP comprises 77,811
RGBD images annotated with ground-truth 6D poses automatically derived from the
robot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP
using three zero-shot 6D pose estimation methods, assessing performance across
different sensor types, localization priors, and occlusion levels. Results show
substantial room for improvement, highlighting the unique challenges posed by
the dataset. CHIP will be publicly released.

</details>


### [207] [The Four Color Theorem for Cell Instance Segmentation](https://arxiv.org/pdf/2506.09724)
*Ye Zhang, Yu Zhou, Yifeng Wang, Jun Xiao, Ziyue Wang, Yongbing Zhang, Jianxu Chen*

Main category: cs.CV

TL;DR: A novel cell instance segmentation method using a four-color encoding scheme inspired by the four-color theorem, simplifying instance differentiation and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Accurately distinguishing tightly touching cells in biomedical images is challenging; existing methods struggle to balance performance and computational efficiency.

Method: Proposes a four-color encoding scheme (cells as countries, tissues as oceans) to transform instance segmentation into a constrained semantic segmentation problem with four classes. Includes an asymptotic training strategy and encoding transformation to address training instability.

Result: Achieves state-of-the-art performance across various modes, demonstrating effectiveness.

Conclusion: The four-color encoding approach simplifies cell instance segmentation while maintaining high performance, offering a promising solution for biomedical image analysis.

Abstract: Cell instance segmentation is critical to analyzing biomedical images, yet
accurately distinguishing tightly touching cells remains a persistent
challenge. Existing instance segmentation frameworks, including
detection-based, contour-based, and distance mapping-based approaches, have
made significant progress, but balancing model performance with computational
efficiency remains an open problem. In this paper, we propose a novel cell
instance segmentation method inspired by the four-color theorem. By
conceptualizing cells as countries and tissues as oceans, we introduce a
four-color encoding scheme that ensures adjacent instances receive distinct
labels. This reformulation transforms instance segmentation into a constrained
semantic segmentation problem with only four predicted classes, substantially
simplifying the instance differentiation process. To solve the training
instability caused by the non-uniqueness of four-color encoding, we design an
asymptotic training strategy and encoding transformation method. Extensive
experiments on various modes demonstrate our approach achieves state-of-the-art
performance. The code is available at https://github.com/zhangye-zoe/FCIS.

</details>


### [208] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/pdf/2506.09736)
*Yuting Li, Lai Wei, Kaipeng Zheng, Jingyuan Huang, Linghe Kong, Lichao Sun, Weiran Huang*

Main category: cs.CV

TL;DR: Language-only models with image captions can outperform MLLMs, suggesting poor visual integration. A visual perturbation framework improves robustness without extra training, enhancing reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs generate accurate visual descriptions but struggle to integrate them in reasoning, as shown by language-only models outperforming them.

Method: Proposes a visual perturbation framework with three strategies: distractor concatenation, dominance-preserving mixup, and random rotation, integrated into post-training pipelines.

Result: Consistent improvements in mathematical reasoning across datasets, with gains comparable to algorithmic changes. Competitive performance achieved with Qwen2.5-VL-7B.

Conclusion: Visual perturbation is crucial for multimodal reasoning, with each strategy uniquely enhancing different aspects. Better reasoning stems from better visual processing.

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [209] [MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition](https://arxiv.org/pdf/2506.09735)
*Chuang Ma, Shaokai Zhao, Dongdong Zhou, Yu Pei, Zhiguo Luo, Liang Xie, Ye Yan, Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MPFNet, a Multi-Prior Fusion Network, to improve micro-expression recognition by leveraging multi-source prior knowledge and progressive training. It achieves state-of-the-art results on key datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition (MER) is challenging due to brief duration and low intensity. Existing methods use simplistic prior knowledge, missing multi-source potential.

Method: Proposes MPFNet with two encoders (GFE and AFE) based on I3D and CA mechanisms. Introduces variants MPFNet-P and MPFNet-C inspired by infant cognitive development.

Result: Achieves accuracies of 0.811, 0.924, and 0.857 on SMIC, CASME II, and SAMM datasets, setting state-of-the-art on SMIC and SAMM.

Conclusion: MPFNet effectively integrates multi-source prior knowledge, significantly improving MER accuracy and balancing performance across categories.

Abstract: Micro-expression recognition (MER), a critical subfield of affective
computing, presents greater challenges than macro-expression recognition due to
its brief duration and low intensity. While incorporating prior knowledge has
been shown to enhance MER performance, existing methods predominantly rely on
simplistic, singular sources of prior knowledge, failing to fully exploit
multi-source information. This paper introduces the Multi-Prior Fusion Network
(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We
propose two complementary encoders: the Generic Feature Encoder (GFE) and the
Advanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with
Coordinate Attention (CA) mechanisms, to improve the model's ability to capture
spatiotemporal and channel-specific features. Inspired by developmental
psychology, we present two variants of MPFNet--MPFNet-P and
MPFNet-C--corresponding to two fundamental modes of infant cognitive
development: parallel and hierarchical processing. These variants enable the
evaluation of different strategies for integrating prior knowledge. Extensive
experiments demonstrate that MPFNet significantly improves MER accuracy while
maintaining balanced performance across categories, achieving accuracies of
0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.
To the best of our knowledge, our approach achieves state-of-the-art
performance on the SMIC and SAMM datasets.

</details>


### [210] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/pdf/2506.09740)
*Qin Zhou, Zhiyang Zhang, Jinglong Wang, Xiaobin Li, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu*

Main category: cs.CV

TL;DR: The paper evaluates text-image alignment in diffusion models using zero-shot referring image segmentation, identifies misalignment causes, and proposes ELBO-T2IAlign for calibration.


<details>
  <summary>Details</summary>
Motivation: Current methods assume perfect text-image alignment in diffusion models, which is often incorrect, leading to misalignment in tasks like segmentation and image editing.

Method: Uses zero-shot referring image segmentation to assess alignment, identifies causes of misalignment (e.g., small or rare objects), and introduces ELBO-T2IAlign for calibration.

Result: Misalignment occurs with small, occluded, or rare objects; ELBO-T2IAlign effectively calibrates alignment without training.

Conclusion: ELBO-T2IAlign is a generic, training-free solution for improving text-image alignment in diffusion models, validated by experiments.

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [211] [Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets](https://arxiv.org/pdf/2506.09745)
*Yangrui Zhu, Junhua Bao, Yipan Wei, Yapeng Li, Bo Du*

Main category: cs.CV

TL;DR: The paper introduces Multi-Modal Heterogeneous Category-set Learning (MMHCL) to address inconsistencies in category distributions across modalities. It proposes CSCF, a model that aligns features, selects discriminative modalities, and refines predictions using class similarity, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal data often has inconsistent category sets, hindering cross-modal information utilization for recognition.

Method: Proposes CSCF: aligns modality features to a shared space, selects discriminative modalities via uncertainty estimation, and refines predictions using class similarity.

Result: CSCF outperforms SOTA methods on benchmark datasets, effectively solving MMHCL.

Conclusion: CSCF successfully addresses MMHCL by leveraging cross-modal information and class similarity, improving recognition across heterogeneous category sets.

Abstract: Existing multimodal methods typically assume that different modalities share
the same category set. However, in real-world applications, the category
distributions in multimodal data exhibit inconsistencies, which can hinder the
model's ability to effectively utilize cross-modal information for recognizing
all categories. In this work, we propose the practical setting termed
Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are
trained in heterogeneous category sets of multi-modal data and aim to recognize
complete classes set of all modalities during test. To effectively address this
task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).
Specifically, CSCF aligns modality-specific features to a shared semantic space
to enable knowledge transfer between seen and unseen classes. It then selects
the most discriminative modality for decision fusion through uncertainty
estimation. Finally, it integrates cross-modal information based on class
similarity, where the auxiliary modality refines the prediction of the dominant
one. Experimental results show that our method significantly outperforms
existing state-of-the-art (SOTA) approaches on multiple benchmark datasets,
effectively addressing the MMHCL task.

</details>


### [212] [Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints](https://arxiv.org/pdf/2506.09748)
*Xiangkai Zhang, Xiang Zhou, Mao Chen, Yuchen Lu, Xu Yang, Zhiyong Liu*

Main category: cs.CV

TL;DR: A hierarchical cross-source image matching method for UAV absolute localization integrates semantic-aware coarse matching with fine-grained matching, achieving high accuracy without GNSS.


<details>
  <summary>Details</summary>
Motivation: Absolute localization for UAVs is challenging without GNSS. Existing vision-based methods struggle with cross-source discrepancies and temporal variations.

Method: Combines semantic-aware coarse matching (using vision foundation models) with lightweight fine-grained matching for pixel-level accuracy.

Result: Superior accuracy and robustness demonstrated on benchmark datasets and a new CS-UAV dataset.

Conclusion: The proposed method effectively overcomes limitations of traditional image matching for UAV localization in GNSS-denied scenarios.

Abstract: Absolute localization, aiming to determine an agent's location with respect
to a global reference, is crucial for unmanned aerial vehicles (UAVs) in
various applications, but it becomes challenging when global navigation
satellite system (GNSS) signals are unavailable. Vision-based absolute
localization methods, which locate the current view of the UAV in a reference
satellite map to estimate its position, have become popular in GNSS-denied
scenarios. However, existing methods mostly rely on traditional and low-level
image matching, suffering from difficulties due to significant differences
introduced by cross-source discrepancies and temporal variations. To overcome
these limitations, in this paper, we introduce a hierarchical cross-source
image matching method designed for UAV absolute localization, which integrates
a semantic-aware and structure-constrained coarse matching module with a
lightweight fine-grained matching module. Specifically, in the coarse matching
module, semantic features derived from a vision foundation model first
establish region-level correspondences under semantic and structural
constraints. Then, the fine-grained matching module is applied to extract fine
features and establish pixel-level correspondences. Building upon this, a UAV
absolute visual localization pipeline is constructed without any reliance on
relative localization techniques, mainly by employing an image retrieval module
before the proposed hierarchical image matching modules. Experimental
evaluations on public benchmark datasets and a newly introduced CS-UAV dataset
demonstrate superior accuracy and robustness of the proposed method under
various challenging conditions, confirming its effectiveness.

</details>


### [213] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/pdf/2506.09777)
*Anton Razzhigaev, Matvey Mikhalchuk, Klim Kireev, Igor Udovichenko, Andrey Kuznetsov, Aleksandr Petiushko*

Main category: cs.CV

TL;DR: DarkerBB reconstructs facial images from black-box models using only similarity scores, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the privacy threat of facial image reconstruction from black-box models, especially when only similarity scores are available.

Method: Zero-order optimization within a PCA-derived eigenface space.

Result: Achieves state-of-the-art verification accuracies on LFW, AgeDB-30, and CFP-FP benchmarks with competitive query efficiency.

Conclusion: DarkerBB is effective for model inversion using only similarity scores, posing a significant privacy concern.

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [214] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/pdf/2506.09782)
*Nicola Farronato, Florian Scheidegger, Mattia Rigotti, Cristiano Malossi, Michele Magno, Haotong Qin*

Main category: cs.CV

TL;DR: Q-SAM2 introduces a low-bit quantization method for SAM2, improving efficiency while maintaining accuracy through calibration and QAT.


<details>
  <summary>Details</summary>
Motivation: SAM2's high computational and memory costs limit its use in resource-constrained scenarios.

Method: Q-SAM2 uses linear layer calibration for initialization and a QAT pipeline with clipping to adapt to quantization.

Result: Q-SAM2 achieves high accuracy and efficiency, outperforming existing methods, especially in 2-bit quantization.

Conclusion: Q-SAM2 is effective for both quantization-aware training and post-training quantization, with significant accuracy improvements.

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [215] [Accurate and efficient zero-shot 6D pose estimation with frozen foundation models](https://arxiv.org/pdf/2506.09784)
*Andrea Caraffa, Davide Boscaini, Fabio Poiesi*

Main category: cs.CV

TL;DR: FreeZeV2 is a training-free method for 6D pose estimation of unseen objects, improving accuracy and efficiency over its predecessor through sparse feature extraction, feature-aware scoring, and modular design.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing 6D pose estimation to novel objects without task-specific training, which demands significant computational resources.

Method: Leverages pre-trained geometric and vision foundation models, introduces sparse feature extraction, feature-aware scoring, and a modular design supporting segmentation model ensembles.

Result: Achieves state-of-the-art performance on the BOP Benchmark, with 8x speedup and 5% accuracy improvement over FreeZe, and additional 8% accuracy gain with segmentation ensembles.

Conclusion: FreeZeV2 demonstrates that task-specific training is unnecessary for accurate and efficient 6D pose estimation, setting a new benchmark in the field.

Abstract: Estimating the 6D pose of objects from RGBD data is a fundamental problem in
computer vision, with applications in robotics and augmented reality. A key
challenge is achieving generalization to novel objects that were not seen
during training. Most existing approaches address this by scaling up training
on synthetic data tailored to the task, a process that demands substantial
computational resources. But is task-specific training really necessary for
accurate and efficient 6D pose estimation of novel objects? To answer No!, we
introduce FreeZeV2, the second generation of FreeZe: a training-free method
that achieves strong generalization to unseen objects by leveraging geometric
and vision foundation models pre-trained on unrelated data. FreeZeV2 improves
both accuracy and efficiency over FreeZe through three key contributions: (i) a
sparse feature extraction strategy that reduces inference-time computation
without sacrificing accuracy; (ii) a feature-aware scoring mechanism that
improves both pose selection during RANSAC-based 3D registration and the final
ranking of pose candidates; and (iii) a modular design that supports ensembles
of instance segmentation models, increasing robustness to segmentation masks
errors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,
where it establishes a new state-of-the-art in 6D pose estimation of unseen
objects. When using the same segmentation masks, FreeZeV2 achieves a remarkable
8x speedup over FreeZe while also improving accuracy by 5%. When using
ensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy
while still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall
Method at the BOP Challenge 2024.

</details>


### [216] [DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision](https://arxiv.org/pdf/2506.09814)
*Xiandong Zou, Ruihao Xia, Hongsong Wang, Pan Zhou*

Main category: cs.CV

TL;DR: The paper introduces 3D-MeshPref, a large-scale unpaired 3D preference dataset, and RewardCS, a reward model trained on it. DreamCS, their framework, integrates RewardCS into text-to-3D pipelines, improving alignment with human preferences and reducing geometric artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods struggle with human preference alignment due to reliance on 2D reward models, causing geometric artifacts. The authors aim to address this by directly learning 3D preferences.

Method: They create 3D-MeshPref (a 3D preference dataset) and train RewardCS using a Cauchy-Schwarz divergence objective. DreamCS integrates RewardCS into text-to-3D pipelines.

Result: DreamCS outperforms prior methods, producing geometrically faithful and human-preferred 3D assets.

Conclusion: The proposed approach effectively aligns 3D generation with human preferences, overcoming limitations of 2D-based methods.

Abstract: While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.

</details>


### [217] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/pdf/2506.09836)
*Junli Deng, Ping Shi, Qipei Li, Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat enhances Gaussian Splatting for dynamic scenes with dynamic-static separation, hierarchical motion modeling, and opacity estimation, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with real-world dynamic scene complexity, prompting the need for a more robust solution.

Method: Classifies static/dynamic elements using deformation offset statistics and 2D motion flow, employs hierarchical motion modeling, and integrates opacity estimation.

Result: Outperforms state-of-the-art methods in accuracy, realism, and efficiency on challenging datasets.

Conclusion: DynaSplat offers a superior, intuitive, and compact approach to dynamic scene reconstruction.

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [218] [MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion](https://arxiv.org/pdf/2506.09834)
*Chuang Maa, Yu Peia, Jianhang Zhanga, Shaokai Zhaoa, Bowen Jib, Liang Xiea, Ye Yana, Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MMME, a novel multimodal dataset for micro-expression (ME) analysis, combining visual and physiological signals to improve recognition and spotting performance.


<details>
  <summary>Details</summary>
Motivation: Existing ME research relies on single visual modality, missing emotional insights from physiological signals. A multimodal approach is needed for practical applications.

Method: The study develops the MMME dataset, synchronizing facial action signals (MEs), EEG, and peripheral physiological signals (PPG, RSP, SKT, EDA, ECG). It includes 634 MEs, 2,841 macro-expressions, and 2,890 multimodal trials.

Result: Experiments show integrating MEs with physiological signals significantly enhances recognition and spotting performance. MMME is the most comprehensive ME dataset in modality diversity.

Conclusion: MMME enables exploration of ME neural mechanisms and visual-physiological synergy, shifting ME research from single-modality to multimodal fusion.

Abstract: Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an
individual's genuine emotional state. Their analysis has attracted considerable
interest due to its promising applications in fields such as healthcare,
criminal investigation, and human-computer interaction. However, existing ME
research is limited to single visual modality, overlooking the rich emotional
information conveyed by other physiological modalities, resulting in ME
recognition and spotting performance far below practical application needs.
Therefore, exploring the cross-modal association mechanism between ME visual
features and physiological signals (PS), and developing a multimodal fusion
framework, represents a pivotal step toward advancing ME analysis. This study
introduces a novel ME dataset, MMME, which, for the first time, enables
synchronized collection of facial action signals (MEs), central nervous system
signals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming
the constraints of existing ME corpora, MMME comprises 634 MEs, 2,841
macro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,
establishing a robust foundation for investigating ME neural mechanisms and
conducting multimodal fusion-based analyses. Extensive experiments validate the
dataset's reliability and provide benchmarks for ME analysis, demonstrating
that integrating MEs with PS significantly enhances recognition and spotting
performance. To the best of our knowledge, MMME is the most comprehensive ME
dataset to date in terms of modality diversity. It provides critical data
support for exploring the neural mechanisms of MEs and uncovering the
visual-physiological synergistic effects, driving a paradigm shift in ME
research from single-modality visual analysis to multimodal fusion. The dataset
will be publicly available upon acceptance of this paper.

</details>


### [219] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/pdf/2506.09839)
*Chen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, Si Liu*

Main category: cs.CV

TL;DR: The paper introduces OctoNav-Bench and OctoNav-R1, aiming to develop generalist navigation agents capable of following free-form, multi-modal instructions. It includes a large-scale benchmark and a method combining MLLMs with a hybrid training paradigm.


<details>
  <summary>Details</summary>
Motivation: To address the fragmentation in navigation research by creating a unified approach for generalist agents that handle diverse instructions and modalities.

Method: Proposes OctoNav-Bench (a benchmark with diverse instruction-trajectory pairs) and OctoNav-R1 (a VLA-type model trained via a Hybrid Training Paradigm, including TBA-SFT and Nav-GPRO stages).

Result: OctoNav-R1 outperforms previous methods, demonstrating superior performance in embodied navigation.

Conclusion: The work advances generalist navigation agents by integrating multi-modal capabilities and reasoning, validated by the success of OctoNav-R1.

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [220] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/pdf/2506.09846)
*Panagiotis Kaliosis, John Pavlopoulos*

Main category: cs.CV

TL;DR: Proposes a novel loss function using Wasserstein distance to align character frequency distributions, improving handwritten text recognition accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Handwritten text recognition is challenging due to evolving and context-dependent handwriting, causing models to underperform on specific subsets.

Method: Introduces a loss function incorporating Wasserstein distance to penalize divergence from target character frequency distributions, and integrates it into guided decoding.

Result: Enhances accuracy and robustness under intra-dataset shifts, improving generalization and performance across datasets and architectures.

Conclusion: The method effectively addresses challenges in handwritten text recognition and can improve existing models without retraining.

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [221] [IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments](https://arxiv.org/pdf/2506.09849)
*Florian Bordes, Quentin Garrido, Justine T Kao, Adina Williams, Michael Rabbat, Emmanuel Dupoux*

Main category: cs.CV

TL;DR: IntPhys 2 is a video benchmark testing deep learning models' intuitive physics understanding, focusing on four core principles. State-of-the-art models struggle, performing at chance levels, unlike humans.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve deep learning models' intuitive physics understanding, inspired by early childhood development.

Method: Uses a violation of expectation framework in controlled virtual environments to test four principles: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.

Result: Models perform poorly (50% accuracy), far below human performance, revealing a gap in intuitive physics understanding.

Conclusion: Highlights the need for better model architectures and training methods to achieve human-like intuitive physics understanding.

Abstract: We present IntPhys 2, a video benchmark designed to evaluate the intuitive
physics understanding of deep learning models. Building on the original IntPhys
benchmark, IntPhys 2 focuses on four core principles related to macroscopic
objects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.
These conditions are inspired by research into intuitive physical understanding
emerging during early childhood. IntPhys 2 offers a comprehensive suite of
tests, based on the violation of expectation framework, that challenge models
to differentiate between possible and impossible events within controlled and
diverse virtual environments. Alongside the benchmark, we provide performance
evaluations of several state-of-the-art models. Our findings indicate that
while these models demonstrate basic visual understanding, they face
significant challenges in grasping intuitive physics across the four principles
in complex scenes, with most models performing at chance levels (50%), in stark
contrast to human performance, which achieves near-perfect accuracy. This
underscores the gap between current models and human-like intuitive physics
understanding, highlighting the need for advancements in model architectures
and training methodologies.

</details>


### [222] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/pdf/2506.09953)
*Benjamin Reichman, Constantin Patsch, Jack Truxal, Atishay Jain, Larry Heck*

Main category: cs.CV

TL;DR: The paper introduces a dataset for visually grounded dialogue tasks requiring external knowledge, extending OK-VQA to videos, and provides baselines for future challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the challenge of combining visual recognition over time with external knowledge in a dialogue setting, where questions rely on information not visually present.

Method: A dataset of 2,017 videos with 5,986 human-annotated dialogues (40,954 turns) is created, where questions require external knowledge beyond the video content.

Result: Baselines are provided, highlighting the complexity of integrating visual and external knowledge in dialogues.

Conclusion: The dataset and baselines pave the way for future research in visually grounded dialogue systems requiring external knowledge.

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [223] [Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation](https://arxiv.org/pdf/2506.09881)
*Siyu Chen, Ting Han, Chengzheng Fu, Changshe Zhang, Chaolei Wang, Jinhe Su, Guorong Cai, Meiliu Wu*

Main category: cs.CV

TL;DR: Vireo is a novel framework for Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS), unifying OVSS and DGSS. It leverages frozen Visual Foundation Models and depth-based features, introducing GeoText Prompts, CMPE, and DOV-VEH for robust performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for pixel-level segmentation of unseen categories and robustness across unseen domains, crucial for real-world applications like autonomous driving.

Method: Vireo uses frozen Visual Foundation Models and depth-based features, with GeoText Prompts, CMPE, and DOV-VEH to align visual-textual modalities and enhance robustness.

Result: Vireo achieves state-of-the-art performance, significantly outperforming existing methods in domain generalization and open-vocabulary recognition.

Conclusion: Vireo provides a unified, scalable solution for robust visual understanding in diverse environments, with code publicly available.

Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in
semantic segmentation (DGSS) highlight a subtle complementarity that motivates
Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS
aims to generate pixel-level masks for unseen categories while maintaining
robustness across unseen domains, a critical capability for real-world
scenarios such as autonomous driving in adverse conditions. We introduce Vireo,
a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS
and DGSS for the first time. Vireo builds upon the frozen Visual Foundation
Models (VFMs) and incorporates scene geometry via Depth VFMs to extract
domain-invariant structural features. To bridge the gap between visual and
textual modalities under domain shift, we propose three key components: (1)
GeoText Prompts, which align geometric features with language cues and
progressively refine VFM encoder representations; (2) Coarse Mask Prior
Embedding (CMPE) for enhancing gradient flow for faster convergence and
stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding
Head (DOV-VEH), which fuses refined structural and semantic features for robust
prediction. Comprehensive evaluation on these components demonstrates the
effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art
performance and surpasses existing methods by a large margin in both domain
generalization and open-vocabulary recognition, offering a unified and scalable
solution for robust visual understanding in diverse and dynamic environments.
Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.

</details>


### [224] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/pdf/2506.09883)
*Seonho Lee, Jiho Choi, Inha Kang, Jiwook Kim, Junsung Park, Hyunjung Shim*

Main category: cs.CV

TL;DR: A lightweight framework, Geometric Distillation, enhances 2D-trained VLMs with 3D spatial understanding using geometric cues from 3D models, improving performance on 3D tasks without architectural changes.


<details>
  <summary>Details</summary>
Motivation: VLMs lack 3D spatial understanding, limiting their effectiveness in spatially grounded tasks. This work aims to bridge this gap efficiently.

Method: Proposes Geometric Distillation, distilling sparse correspondences, relative depth relations, and dense cost volumes from 3D models into VLMs without altering their architecture.

Result: Outperforms prior methods on 3D vision-language reasoning and perception benchmarks, achieving better 3D reasoning with lower computational cost.

Conclusion: Demonstrates a scalable way to equip 2D-trained VLMs with 3D understanding, expanding their applicability in multimodal tasks.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [225] [The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge](https://arxiv.org/pdf/2506.09885)
*Haoru Wang, Kai Ye, Yangyan Li, Wenzheng Chen, Baoquan Chen*

Main category: cs.CV

TL;DR: The paper explores reducing 3D knowledge dependence in novel view synthesis (NVS), showing that methods with less 3D bias scale better with data. It proposes a framework minimizing 3D inductive bias and pose dependence, achieving results comparable to pose-dependent methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of inferring 3D structure from sparse or unposed 2D images without per-scene optimization, and the under-explored role of 3D knowledge in NVS.

Method: A novel NVS framework that eliminates 3D inductive bias and pose dependence, learning implicit 3D awareness directly from sparse 2D images.

Result: The method generates photorealistic, 3D-consistent novel views, matching performance of pose-dependent methods.

Conclusion: Reducing 3D knowledge dependence is feasible and effective, especially with large-scale data, as demonstrated by the proposed framework.

Abstract: We consider the problem of generalizable novel view synthesis (NVS), which
aims to generate photorealistic novel views from sparse or even unposed 2D
images without per-scene optimization. This task remains fundamentally
challenging, as it requires inferring 3D structure from incomplete and
ambiguous 2D observations. Early approaches typically rely on strong 3D
knowledge, including architectural 3D inductive biases (e.g., embedding
explicit 3D representations, such as NeRF or 3DGS, into network design) and
ground-truth camera poses for both input and target views. While recent efforts
have sought to reduce the 3D inductive bias or the dependence on known camera
poses of input views, critical questions regarding the role of 3D knowledge and
the necessity of circumventing its use remain under-explored. In this work, we
conduct a systematic analysis on the 3D knowledge and uncover a critical trend:
the performance of methods that requires less 3D knowledge accelerates more as
data scales, eventually achieving performance on par with their 3D
knowledge-driven counterparts, which highlights the increasing importance of
reducing dependence on 3D knowledge in the era of large-scale data. Motivated
by and following this trend, we propose a novel NVS framework that minimizes 3D
inductive bias and pose dependence for both input and target views. By
eliminating this 3D knowledge, our method fully leverages data scaling and
learns implicit 3D awareness directly from sparse 2D images, without any 3D
inductive bias or pose annotation during training. Extensive experiments
demonstrate that our model generates photorealistic and 3D-consistent novel
views, achieving even comparable performance with methods that rely on posed
inputs, thereby validating the feasibility and effectiveness of our
data-centric paradigm. Project page:
https://pku-vcl-geometry.github.io/Less3Depend/ .

</details>


### [226] [EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks](https://arxiv.org/pdf/2506.09895)
*Athinoulla Konstantinou, Georgios Leontidis, Mamatha Thota, Aiden Durrant*

Main category: cs.CV

TL;DR: EquiCaps, a capsule-based method, leverages intrinsic pose-awareness for self-supervised learning, outperforming state-of-the-art equivariant methods in pose estimation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore capsule networks' inherent ability for pose-aware representations without needing specialized predictors for equivariance.

Method: Introduces EquiCaps, a capsule-based approach, and tests it with multi-geometric transformations using the 3DIEBench-T dataset.

Result: Achieves a supervised-level R² of 0.78 in rotation prediction, outperforming SIE and CapsIE by 0.05 and 0.04 R², respectively.

Conclusion: EquiCaps demonstrates robust equivariant performance under complex transformations, highlighting the potential of predictor-free capsule architectures.

Abstract: Learning self-supervised representations that are invariant and equivariant
to transformations is crucial for advancing beyond traditional visual
classification tasks. However, many methods rely on predictor architectures to
encode equivariance, despite evidence that architectural choices, such as
capsule networks, inherently excel at learning interpretable pose-aware
representations. To explore this, we introduce EquiCaps (Equivariant Capsule
Network), a capsule-based approach to pose-aware self-supervision that
eliminates the need for a specialised predictor for enforcing equivariance.
Instead, we leverage the intrinsic pose-awareness capabilities of capsules to
improve performance in pose estimation tasks. To further challenge our
assumptions, we increase task complexity via multi-geometric transformations to
enable a more thorough evaluation of invariance and equivariance by introducing
3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical
results demonstrate that EquiCaps outperforms prior state-of-the-art
equivariant methods on rotation prediction, achieving a supervised-level $R^2$
of 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE
and CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to
non-capsule-based equivariant approaches, EquiCaps maintains robust equivariant
performance under combined geometric transformations, underscoring its
generalisation capabilities and the promise of predictor-free capsule
architectures.

</details>


### [227] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/pdf/2506.09932)
*Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel*

Main category: cs.CV

TL;DR: HadaNorm, a novel linear transformation, improves quantization for diffusion models by normalizing activations and using Hadamard transformations, achieving better efficiency-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face high memory and computational demands, limiting deployment on resource-constrained devices. Standard PTQ methods struggle with outliers and require transformations for higher compression.

Method: HadaNorm normalizes activations feature channels and applies Hadamard transformations before quantization, mitigating outliers and enabling aggressive activation quantization.

Result: HadaNorm reduces quantization error across transformer blocks and outperforms state-of-the-art methods in efficiency-performance trade-offs.

Conclusion: HadaNorm effectively addresses quantization challenges for diffusion models, offering superior performance and efficiency.

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [228] [CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects](https://arxiv.org/pdf/2506.09897)
*Tao Liu, Zhenchao Cui*

Main category: cs.CV

TL;DR: E-FPN-BS addresses TOD issues by enhancing high-level features for low-level use, introducing CEM and FBSM for feature fusion and dynamic amplification, and DCLoss for balanced gradients.


<details>
  <summary>Details</summary>
Motivation: Standard label assignment in TOD leaves high-level features untrained, causing semantic dead-ends and weak low-level context.

Method: Proposes E-FPN-BS with CEM for feature fusion, FBSM for dynamic amplification, and DCLoss for gradient balance.

Result: Outperforms benchmarks, demonstrating strong generalization.

Conclusion: E-FPN-BS effectively resolves feature misuse and gradient imbalance in TOD.

Abstract: Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid
networks: high-level features (P5-P6) frequently receive zero positive anchors
under standard label assignment protocols, leaving their semantic
representations untrained due to exclusion from loss computation. This creates
dual deficiencies: (1) Stranded high-level features become semantic dead-ends
without gradient updates, while (2) low-level features lack essential semantic
context for robust classification. We propose E-FPN-BS that systematically
converts wasted high-level semantics into low-level feature enhancements. To
address these issues, we propose E-FPN-BS, a novel architecture integrating
multi-scale feature enhancement and adaptive optimization. First, our Context
Enhancement Module(CEM) employs dual-branch processing to align and compress
high-level features for effective global-local fusion. Second, the
Foreground-Background Separation Module (FBSM) generates spatial gating masks
that dynamically amplify discriminative regions. To address gradient imbalance
across object scales, we further propose a Dynamic Gradient-Balanced Loss
(DCLoss) that automatically modulates loss contributions via scale-aware
gradient equilibrium. Extensive experiments across multiple benchmark datasets
demonstrate the outstanding performance and generalization ability of our
approach.

</details>


### [229] [Only-Style: Stylistic Consistency in Image Generation without Content Leakage](https://arxiv.org/pdf/2506.09916)
*Tilemachos Aravanis, Panagiotis Filntisis, Petros Maragos, George Retsinas*

Main category: cs.CV

TL;DR: Only-Style mitigates content leakage in style-consistent image generation by localizing leakage and adaptively tuning style alignment, achieving robust results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of separating semantic content from stylistic elements in image generation to prevent content leakage.

Method: Localizes content leakage during inference and adaptively tunes style alignment parameters in affected patches.

Result: Significant improvement over state-of-the-art methods, achieving stylistic consistency without content leakage.

Conclusion: Only-Style effectively balances style consistency and leakage elimination, with a novel evaluation framework for quantifying success.

Abstract: Generating images in a consistent reference visual style remains a
challenging computer vision task. State-of-the-art methods aiming for
style-consistent generation struggle to effectively separate semantic content
from stylistic elements, leading to content leakage from the image provided as
a reference to the targets. To address this challenge, we propose Only-Style: a
method designed to mitigate content leakage in a semantically coherent manner
while preserving stylistic consistency. Only-Style works by localizing content
leakage during inference, allowing the adaptive tuning of a parameter that
controls the style alignment process, specifically within the image patches
containing the subject in the reference image. This adaptive process best
balances stylistic consistency with leakage elimination. Moreover, the
localization of content leakage can function as a standalone component, given a
reference-target image pair, allowing the adaptive tuning of any
method-specific parameter that provides control over the impact of the
stylistic reference. In addition, we propose a novel evaluation framework to
quantify the success of style-consistent generations in avoiding undesired
content leakage. Our approach demonstrates a significant improvement over
state-of-the-art methods through extensive evaluation across diverse instances,
consistently achieving robust stylistic consistency without undesired content
leakage.

</details>


### [230] [MetricHMR: Metric Human Mesh Recovery from Monocular Images](https://arxiv.org/pdf/2506.09919)
*He Zhang, Chentao Song, Hongwen Zhang, Tao Yu*

Main category: cs.CV

TL;DR: MetricHMR introduces a method for accurate metric human mesh recovery from monocular images, addressing scale and depth ambiguity with a novel ray-map approach.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods struggle with scale and depth ambiguity, limiting their accuracy in global translation and body shape reconstruction.

Method: The approach uses a standard perspective projection model and introduces a ray map to encode bounding-box info, camera parameters, and geometric cues for End2End metric HMR.

Result: MetricHMR achieves state-of-the-art performance in metric pose, shape, and global translation estimation across diverse scenarios.

Conclusion: The method demonstrates the effectiveness of the standard perspective projection model and ray-map encoding for accurate metric HMR.

Abstract: We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric
human mesh recovery with accurate global translation from monocular images. In
contrast to existing HMR methods that suffer from severe scale and depth
ambiguity, MetricHMR is able to produce geometrically reasonable body shape and
global translation in the reconstruction results. To this end, we first
systematically analyze previous HMR methods on camera models to emphasize the
critical role of the standard perspective projection model in enabling
metric-scale HMR. We then validate the acceptable ambiguity range of metric HMR
under the standard perspective projection model. Finally, we contribute a novel
approach that introduces a ray map based on the standard perspective projection
to jointly encode bounding-box information, camera parameters, and geometric
cues for End2End metric HMR without any additional metric-regularization
modules. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, even compared with sequential HMR methods, in
metric pose, shape, and global translation estimation across both indoor and
in-the-wild scenarios.

</details>


### [231] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/pdf/2506.09943)
*Aaron Foss, Chloe Evans, Sasha Mitts, Koustuv Sinha, Ammar Rizvi, Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA is a new VQA benchmark dataset focusing on causality in real-world videos, challenging models with five question types and revealing gaps in current multimodal models.


<details>
  <summary>Details</summary>
Motivation: Existing VQA benchmarks lack focus on causality in real-world scenarios, limiting models' ability to predict outcomes of actions and events.

Method: CausalVQA introduces five question types (counterfactual, hypothetical, anticipation, planning, descriptive) with quality controls to prevent shortcuts.

Result: Current multimodal models perform significantly below humans, especially on anticipation and hypothetical questions.

Conclusion: CausalVQA highlights the need for improved spatial-temporal reasoning and physical understanding in models for real-world predictions.

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [232] [Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering](https://arxiv.org/pdf/2506.09920)
*Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou*

Main category: cs.CV

TL;DR: The paper introduces a method for hyperspectral image clustering using a structural-spectral graph convolutional operator and evidence-guided adaptive edge learning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for hyperspectral image clustering struggle with exploiting spectral information and inaccurate superpixel graphs, leading to semantic confusion.

Method: Proposes SSGCO for spatial-spectral feature extraction and EGAEL for adaptive edge refinement, integrated into a contrastive learning framework.

Result: Achieves 2.61% to 6.06% higher clustering accuracy on four datasets compared to existing methods.

Conclusion: The proposed SSGCO and EGAEL effectively enhance clustering performance by better utilizing spectral and spatial features.

Abstract: Hyperspectral image (HSI) clustering assigns similar pixels to the same class
without any annotations, which is an important yet challenging task. For
large-scale HSIs, most methods rely on superpixel segmentation and perform
superpixel-level clustering based on graph neural networks (GNNs). However,
existing GNNs cannot fully exploit the spectral information of the input HSI,
and the inaccurate superpixel topological graph may lead to the confusion of
different class semantics during information aggregation. To address these
challenges, we first propose a structural-spectral graph convolutional operator
(SSGCO) tailored for graph-structured HSI superpixels to improve their
representation quality through the co-extraction of spatial and spectral
features. Second, we propose an evidence-guided adaptive edge learning (EGAEL)
module that adaptively predicts and refines edge weights in the superpixel
topological graph. We integrate the proposed method into a contrastive learning
framework to achieve clustering, where representation learning and clustering
are simultaneously conducted. Experiments demonstrate that the proposed method
improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best
compared methods on four HSI datasets. Our code is available at
https://github.com/jhqi/SSGCO-EGAEL.

</details>


### [233] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/pdf/2506.09952)
*Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu*

Main category: cs.CV

TL;DR: UniPre3D is the first unified pre-training method for point clouds of any scale and 3D models of any architecture, using Gaussian primitives and differentiable rendering for pixel-level supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of unified 3D models and pre-training methods effective for both object- and scene-level point clouds.

Method: Predicts Gaussian primitives, uses differentiable Gaussian splatting for rendering, and integrates 2D features from pre-trained image models.

Result: Validated through extensive experiments across diverse object- and scene-level tasks.

Conclusion: UniPre3D offers a universal and effective pre-training solution for 3D vision tasks.

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [234] [LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation](https://arxiv.org/pdf/2506.09935)
*Jiangyong Huang, Xiaojian Ma, Xiongkun Linghu, Yue Fan, Junchao He, Wenxin Tan, Qing Li, Song-Chun Zhu, Yixin Chen, Baoxiong Jia, Siyuan Huang*

Main category: cs.CV

TL;DR: LEO-VL, a 3D-VL model using condensed feature grids (CFG), improves efficiency and scalability for 3D vision-language tasks, achieving SOTA on benchmarks like SQA3D.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in 3D-VL models' capability and robustness compared to 2D models, hindered by inefficient scene representation and data scalability.

Method: Proposes LEO-VL with CFG for efficient scene representation, enabling large-scale training with 700k 3D-VL data across tasks like captioning and dialogue. Introduces SceneDPO for robustness.

Result: Achieves state-of-the-art on benchmarks (SQA3D, MSQA, Beacon3D). Ablations confirm CFG efficiency, task diversity importance, and data curation validity.

Conclusion: LEO-VL advances scalable and robust 3D-VL generalists, with SceneDPO enhancing model robustness.

Abstract: Developing 3D-VL generalists capable of understanding 3D scenes and following
natural language instructions to perform a wide range of tasks has been a
long-standing goal in the 3D-VL community. Despite recent progress, 3D-VL
models still lag behind their 2D counterparts in capability and robustness,
falling short of the generalist standard. A key obstacle to developing 3D-VL
generalists lies in data scalability, hindered by the lack of an efficient
scene representation. We propose LEO-VL, a 3D-VL model built upon condensed
feature grid (CFG), an efficient scene representation that bridges 2D
perception and 3D spatial structure while significantly reducing token
overhead. This efficiency unlocks large-scale training towards 3D-VL
generalist, for which we curate over 700k high-quality 3D-VL data spanning four
domains of real-world indoor scenes and five tasks such as captioning and
dialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA
benchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the
efficiency of our representation, the importance of task and scene diversity,
and the validity of our data curation principle. Furthermore, we introduce
SceneDPO, a novel post-training objective that enhances the robustness of 3D-VL
models. We hope our findings contribute to the advancement of scalable and
robust 3D-VL generalists.

</details>


### [235] [Vision Generalist Model: A Survey](https://arxiv.org/pdf/2506.09954)
*Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu*

Main category: cs.CV

TL;DR: The paper reviews vision generalist models, their design, techniques, and applications, while addressing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The success of generalist models in NLP inspires their application to diverse vision tasks, despite challenges in unifying representations.

Method: The paper reviews datasets, tasks, benchmarks, framework designs, and performance-enhancing techniques, with insights from related domains.

Result: A comprehensive overview of vision generalist models, their capabilities, and real-world applications is provided.

Conclusion: Challenges in vision generalist models are examined, with suggestions for future research directions.

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [236] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/pdf/2506.09958)
*Sushant Gautam, Michael A. Riegler, Pål Halvorsen*

Main category: cs.CV

TL;DR: Kvasir-VQA-x1 is a new, large-scale dataset for GI endoscopy, enhancing clinical complexity and visual diversity to improve MedVQA systems.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing datasets by providing deeper clinical reasoning and visual diversity for better clinical decision support.

Method: Systematic use of large language models to generate 159,549 question-answer pairs, stratified by complexity, and visual augmentations for robustness testing.

Result: A dataset supporting two evaluation tracks: standard VQA performance and robustness against visual perturbations.

Conclusion: Kvasir-VQA-x1 aims to advance reliable multimodal AI for clinical use, adhering to FAIR principles and being openly accessible.

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [237] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/pdf/2506.09965)
*Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces 'drawing to reason in space,' a novel paradigm for LVLMs to enhance spatial reasoning by using visual drawing operations, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing text-centric multimodal reasoning methods lack precise geometric understanding and spatial tracking, limiting performance in spatial tasks.

Method: The proposed method equips LVLMs with drawing operations (e.g., bounding boxes, auxiliary lines) and uses a three-stage training framework: cold-start training, reflective rejection sampling, and reinforcement learning.

Result: The model, VILASR, outperforms existing methods by 18.4% on average across diverse spatial reasoning benchmarks.

Conclusion: Visual drawing operations significantly enhance LVLMs' spatial reasoning, overcoming limitations of text-centric approaches.

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [238] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/pdf/2506.09988)
*Ron Yosef, Moran Yanuka, Yonatan Bitton, Dani Lischinski*

Main category: cs.CV

TL;DR: EditInspector is a benchmark for evaluating text-guided image edits, revealing current models' limitations and proposing improved methods.


<details>
  <summary>Details</summary>
Motivation: The rise of text-guided image editing necessitates a framework to verify and assess edit quality.

Method: Introduces EditInspector, a benchmark based on human annotations, and evaluates SoTA models across multiple dimensions.

Result: Current models perform poorly in comprehensive edit evaluation and hallucinate in change descriptions. Proposed methods outperform SoTA in artifact detection and difference captioning.

Conclusion: EditInspector highlights gaps in current models and offers better solutions for evaluating text-guided edits.

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [239] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/pdf/2505.14156)
*Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan*

Main category: cs.CV

TL;DR: SGR combines text-based and graph-based approaches using LLMs for session search, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current methods lack integration of graph structures and word-level semantics in session search.

Method: SGR converts session graphs to text using symbolic grammar, then uses LLMs with self-supervised tasks for graph understanding.

Result: Superior performance on AOL and Tiangong-ST datasets.

Conclusion: SGR bridges traditional search and modern LLMs, offering a novel methodology.

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [240] [Vectorized Region Based Brush Strokes for Artistic Rendering](https://arxiv.org/pdf/2506.09969)
*Jeripothula Prudviraj, Vikram Jamwal*

Main category: cs.CV

TL;DR: The paper introduces an image-to-painting method that improves stroke quality and alignment with artistic principles by incorporating semantic guidance, stroke parameter computation, and sequential rendering.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between static artwork and its creation process, addressing limitations of current stroke-based painting systems that often fail to align with artistic intent.

Method: The method uses semantic guidance for targeted regions, computes brush stroke parameters, and establishes a sequential rendering process for strokes.

Result: Experiments show the method produces high-fidelity paintings with superior stroke quality across various image types (faces, paintings, photos).

Conclusion: The proposed method effectively aligns with artistic principles and enhances the painting process through structured stroke rendering.

Abstract: Creating a stroke-by-stroke evolution process of a visual artwork tries to
bridge the emotional and educational gap between the finished static artwork
and its creation process. Recent stroke-based painting systems focus on
capturing stroke details by predicting and iteratively refining stroke
parameters to maximize the similarity between the input image and the rendered
output. However, these methods often struggle to produce stroke compositions
that align with artistic principles and intent. To address this, we explore an
image-to-painting method that (i) facilitates semantic guidance for brush
strokes in targeted regions, (ii) computes the brush stroke parameters, and
(iii) establishes a sequence among segments and strokes to sequentially render
the final painting. Experimental results on various input image types, such as
face images, paintings, and photographic images, show that our method aligns
with a region-based painting strategy while rendering a painting with high
fidelity and superior stroke quality.

</details>


### [241] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/pdf/2506.09993)
*Jaewon Min, Jin Hyeon Kim, Paul Hyunbin Cho, Jaeeun Lee, Jihye Park, Minkyu Park, Sangpil Kim, Hyunhee Park, Seungryong Kim*

Main category: cs.CV

TL;DR: TAIR introduces a novel image restoration task focusing on textual fidelity, proposing TeReDiff, a multi-task diffusion framework, and SA-Text, a large-scale benchmark. It outperforms existing methods in text recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods struggle with textual fidelity in degraded images, often generating incorrect text-like patterns (text-image hallucination).

Method: Proposes TeReDiff, a multi-task diffusion framework integrating diffusion model features with a text-spotting module for joint training and rich text representation extraction.

Result: Outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy.

Conclusion: TAIR and TeReDiff effectively address text-image hallucination, improving both visual and textual restoration.

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [242] [Efficient Part-level 3D Object Generation via Dual Volume Packing](https://arxiv.org/pdf/2506.09980)
*Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, Tsung-Yi Lin*

Main category: cs.CV

TL;DR: Proposes a part-level 3D object generation framework for editable and semantically meaningful parts, improving quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods generate fused meshes, limiting part-level editing. Addressing the challenge of varying part counts in objects.

Method: Uses a dual volume packing strategy to organize parts into complementary volumes for complete and interleaved assembly.

Result: Achieves better quality, diversity, and generalization compared to previous image-based part-level generation methods.

Conclusion: The framework enables high-quality, part-level 3D object generation from a single image, enhancing editability and semantic meaning.

Abstract: Recent progress in 3D object generation has greatly improved both the quality
and efficiency. However, most existing methods generate a single mesh with all
parts fused together, which limits the ability to edit or manipulate individual
parts. A key challenge is that different objects may have a varying number of
parts. To address this, we propose a new end-to-end framework for part-level 3D
object generation. Given a single input image, our method generates
high-quality 3D objects with an arbitrary number of complete and semantically
meaningful parts. We introduce a dual volume packing strategy that organizes
all parts into two complementary volumes, allowing for the creation of complete
and interleaved parts that assemble into the final object. Experiments show
that our model achieves better quality, diversity, and generalization than
previous image-based part-level generation methods.

</details>


### [243] [ReSim: Reliable World Simulation for Autonomous Driving](https://arxiv.org/pdf/2506.09981)
*Jiazhi Yang, Kashyap Chitta, Shenyuan Gao, Long Chen, Yuqian Shao, Xiaosong Jia, Hongyang Li, Andreas Geiger, Xiangyu Yue, Li Chen*

Main category: cs.CV

TL;DR: ReSim enhances driving scenario simulation by integrating diverse non-expert data with real-world trajectories, improving fidelity and controllability for hazardous behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing driving world models, trained on safe expert data, fail to simulate hazardous or non-expert behaviors, limiting their use in policy evaluation.

Method: ReSim combines real-world and simulator data (e.g., CARLA) using a diffusion transformer video generator, with strategies to improve controllability and fidelity.

Result: ReSim achieves 44% higher visual fidelity, over 50% better controllability, and boosts planning/policy selection by 2% and 25% on NAVSIM.

Conclusion: ReSim reliably simulates diverse driving scenarios, including hazardous behaviors, and integrates a Video2Reward module for practical applications.

Abstract: How can we reliably simulate future driving scenarios under a wide range of
ego driving behaviors? Recent driving world models, developed exclusively on
real-world driving data composed mainly of safe expert trajectories, struggle
to follow hazardous or non-expert behaviors, which are rare in such data. This
limitation restricts their applicability to tasks such as policy evaluation. In
this work, we address this challenge by enriching real-world human
demonstrations with diverse non-expert data collected from a driving simulator
(e.g., CARLA), and building a controllable world model trained on this
heterogeneous corpus. Starting with a video generator featuring a diffusion
transformer architecture, we devise several strategies to effectively integrate
conditioning signals and improve prediction controllability and fidelity. The
resulting model, ReSim, enables Reliable Simulation of diverse open-world
driving scenarios under various actions, including hazardous non-expert ones.
To close the gap between high-fidelity simulation and applications that require
reward signals to judge different actions, we introduce a Video2Reward module
that estimates a reward from ReSim's simulated future. Our ReSim paradigm
achieves up to 44% higher visual fidelity, improves controllability for both
expert and non-expert actions by over 50%, and boosts planning and policy
selection performance on NAVSIM by 2% and 25%, respectively.

</details>


### [244] [AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation](https://arxiv.org/pdf/2506.09982)
*Zijie Wu, Chaohui Yu, Fan Wang, Xiang Bai*

Main category: cs.CV

TL;DR: AnimateAnyMesh is a feed-forward framework for text-driven animation of 3D meshes, using DyMeshVAE and Rectified Flow for efficient, high-quality generation.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality animated 3D models is challenging due to spatio-temporal complexity and lack of 4D training data.

Method: Leverages DyMeshVAE to compress/reconstruct dynamic mesh sequences and Rectified Flow for text-conditional generation.

Result: Generates semantically accurate, temporally coherent animations in seconds, outperforming existing methods.

Conclusion: Advances 4D content creation accessibility; data, code, and models will be open-released.

Abstract: Recent advances in 4D content generation have attracted increasing attention,
yet creating high-quality animated 3D models remains challenging due to the
complexity of modeling spatio-temporal distributions and the scarcity of 4D
training data. In this paper, we present AnimateAnyMesh, the first feed-forward
framework that enables efficient text-driven animation of arbitrary 3D meshes.
Our approach leverages a novel DyMeshVAE architecture that effectively
compresses and reconstructs dynamic mesh sequences by disentangling spatial and
temporal features while preserving local topological structures. To enable
high-quality text-conditional generation, we employ a Rectified Flow-based
training strategy in the compressed latent space. Additionally, we contribute
the DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text
annotations. Experimental results demonstrate that our method generates
semantically accurate and temporally coherent mesh animations in a few seconds,
significantly outperforming existing approaches in both quality and efficiency.
Our work marks a substantial step forward in making 4D content creation more
accessible and practical. All the data, code, and models will be open-released.

</details>


### [245] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/pdf/2506.09987)
*Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, Nicolas Ballas, Mahmoud Assran*

Main category: cs.CV

TL;DR: The paper introduces the Minimal Video Pairs (MVP) benchmark to address shortcut solutions in video QA tasks, ensuring accurate assessment of video-language models' physical understanding.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are prone to score inflation due to superficial cues, necessitating a more robust evaluation method.

Method: MVP includes 55K multiple-choice QA pairs with minimal-change pairs to counter shortcuts, sourced from diverse video datasets.

Result: Human performance is 92.9%, while the best model scores 40.2% (random: 25%), highlighting the benchmark's rigor.

Conclusion: MVP effectively mitigates shortcuts, providing a reliable measure of true spatio-temporal reasoning in video-language models.

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


### [246] [Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes](https://arxiv.org/pdf/2506.09989)
*Yiming Dou, Wonseok Oh, Yuqing Luo, Antonio Loquercio, Andrew Owens*

Main category: cs.CV

TL;DR: A method to predict sounds of human hands interacting with 3D scenes using rectified flow models trained on action-sound pairs.


<details>
  <summary>Details</summary>
Motivation: To enable interactive 3D scene reconstructions by predicting sounds of physical interactions.

Method: Record videos of hand manipulations, train a rectified flow model on action-sound pairs, and query the model for sound predictions based on hand poses.

Result: Generated sounds accurately reflect material properties and actions, often indistinguishable from real sounds.

Conclusion: The approach successfully predicts realistic sounds for hand interactions in 3D scenes, enhancing interactivity.

Abstract: We study the problem of making 3D scene reconstructions interactive by asking
the following question: can we predict the sounds of human hands physically
interacting with a scene? First, we record a video of a human manipulating
objects within a 3D scene using their hands. We then use these action-sound
pairs to train a rectified flow model to map 3D hand trajectories to their
corresponding audio. At test time, a user can query the model for other
actions, parameterized as sequences of hand poses, to estimate their
corresponding sounds. In our experiments, we find that our generated sounds
accurately convey material properties and actions, and that they are often
indistinguishable to human observers from real sounds. Project page:
https://www.yimingdou.com/hearing_hands/

</details>


### [247] [PlayerOne: Egocentric World Simulator](https://arxiv.org/pdf/2506.09995)
*Yuanpeng Tu, Hao Luo, Xi Chen, Xiang Bai, Fan Wang, Hengshuang Zhao*

Main category: cs.CV

TL;DR: PlayerOne is the first egocentric realistic world simulator, enabling immersive exploration and dynamic video generation aligned with real human motion.


<details>
  <summary>Details</summary>
Motivation: To create a simulator for immersive, unrestricted exploration in dynamic environments, aligning virtual scenes with real human motion.

Method: Uses a coarse-to-fine pipeline: pretraining on text-video pairs for coarse understanding, then finetuning on motion-video data. Includes part-disentangled motion injection and joint reconstruction for scene consistency.

Result: Demonstrates precise control of human movements and world-consistent modeling in diverse scenarios.

Conclusion: PlayerOne pioneers egocentric real-world simulation, opening new frontiers in world modeling and applications.

Abstract: We introduce PlayerOne, the first egocentric realistic world simulator,
facilitating immersive and unrestricted exploration within vividly dynamic
environments. Given an egocentric scene image from the user, PlayerOne can
accurately construct the corresponding world and generate egocentric videos
that are strictly aligned with the real scene human motion of the user captured
by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that
first performs pretraining on large-scale egocentric text-video pairs for
coarse-level egocentric understanding, followed by finetuning on synchronous
motion-video data extracted from egocentric-exocentric video datasets with our
automatic construction pipeline. Besides, considering the varying importance of
different components, we design a part-disentangled motion injection scheme,
enabling precise control of part-level movements. In addition, we devise a
joint reconstruction framework that progressively models both the 4D scene and
video frames, ensuring scene consistency in the long-form video generation.
Experimental results demonstrate its great generalization ability in precise
control of varying human movements and worldconsistent modeling of diverse
scenarios. It marks the first endeavor into egocentric real-world simulation
and can pave the way for the community to delve into fresh frontiers of world
modeling and its diverse applications.

</details>


### [248] [Effective Data Augmentation With Diffusion Models](https://arxiv.org/pdf/2302.07944)
*Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov*

Main category: cs.CV

TL;DR: The paper introduces a method using text-to-image diffusion models for data augmentation to enhance semantic diversity in images, improving few-shot classification and real-world tasks like weed recognition.


<details>
  <summary>Details</summary>
Motivation: Standard data augmentation lacks diversity in high-level semantic attributes, limiting its effectiveness in tasks requiring varied visual concepts.

Method: Leverages pre-trained text-to-image diffusion models to edit images and alter their semantics, generalizing from few labeled examples.

Result: Shows improved accuracy in few-shot image classification and a real-world weed recognition task.

Conclusion: The proposed method addresses the diversity gap in data augmentation, enhancing performance in practical applications.

Abstract: Data augmentation is one of the most prevalent tools in deep learning,
underpinning many recent advances, including those from classification,
generative models, and representation learning. The standard approach to data
augmentation combines simple transformations like rotations and flips to
generate new images from existing ones. However, these new images lack
diversity along key semantic axes present in the data. Current augmentations
cannot alter the high-level semantic attributes, such as animal species present
in a scene, to enhance the diversity of data. We address the lack of diversity
in data augmentation with image-to-image transformations parameterized by
pre-trained text-to-image diffusion models. Our method edits images to change
their semantics using an off-the-shelf diffusion model, and generalizes to
novel visual concepts from a few labelled examples. We evaluate our approach on
few-shot image classification tasks, and on a real-world weed recognition task,
and observe an improvement in accuracy in tested domains.

</details>


### [249] [Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes](https://arxiv.org/pdf/2311.09652)
*Aniket Dashpute, Jiazhang Wang, James Taylor, Oliver Cossairt, Ashok Veeraraghavan, Florian Willomitzer*

Main category: cs.CV

TL;DR: A novel event-based structured light system enables fast, high-accuracy 3D imaging of mixed reflectance scenes by decomposing reflections and combining triangulation with deflectometry.


<details>
  <summary>Details</summary>
Motivation: Existing 3D imaging systems are limited to specific surface types (diffuse or specular), lacking versatility for mixed reflectance scenes.

Method: Uses epipolar constraints to decompose reflections, triangulates diffuse surfaces, and leverages them as a virtual screen for deflectometry of specular parts.

Result: Achieves fast (14Hz) and motion-robust reconstructions with <600μm depth error for mixed scenes, and an ultrafast mode (250Hz) for diffuse scenes.

Conclusion: The system bridges the gap between event-based and frame-based systems, offering high accuracy and versatility for mixed reflectance scenes.

Abstract: Event-based structured light systems have recently been introduced as an
exciting alternative to conventional frame-based triangulation systems for the
3D measurements of diffuse surfaces. Important benefits include the fast
capture speed and the high dynamic range provided by the event camera - albeit
at the cost of lower data quality. So far, both low-accuracy event-based and
high-accuracy frame-based 3D imaging systems are tailored to a specific surface
type, such as diffuse or specular, and can not be used for a broader class of
object surfaces ("mixed reflectance scenes"). In this work, we present a novel
event-based structured light system that enables fast 3D imaging of mixed
reflectance scenes with high accuracy. On the captured events, we use epipolar
constraints that intrinsically enable decomposing the measured reflections into
diffuse, two-bounce specular, and other multi-bounce reflections. The diffuse
surfaces in the scene are reconstructed using triangulation. Then, the
reconstructed diffuse scene parts are leveraged as a "display" to evaluate the
specular scene parts via deflectometry. This novel procedure allows us to use
the entire scene as a virtual screen, using only a scanning laser and an event
camera. The resulting system achieves fast and motion-robust (14Hz)
reconstructions of mixed reflectance scenes with < 600 ${\mu}m$ depth error.
Moreover, we introduce an "ultrafast" capture mode (250Hz) for the 3D
measurement of diffuse scenes.

</details>


### [250] [Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning](https://arxiv.org/pdf/2312.05219)
*Houting Li, Mengxuan Dong, Lok Ming Lui*

Main category: cs.CV

TL;DR: A novel method combining 3D facial models with deep learning (ResNet) improves facial classification accuracy, achieving 100% individual, 95.4% gender, and 83.5% expression classification.


<details>
  <summary>Details</summary>
Motivation: Accurate facial attribute analysis is crucial for applications like human-computer interaction and security systems.

Method: Integration of 3D facial models with deep learning (ResNet) to extract useful information for classification tasks.

Result: Achieved 100% individual, 95.4% gender, and 83.5% expression classification accuracy.

Conclusion: The method shows promise for advancing facial analysis and recognition research.

Abstract: Accurate analysis and classification of facial attributes are essential in
various applications, from human-computer interaction to security systems. In
this work, a novel approach to enhance facial classification and recognition
tasks through the integration of 3D facial models with deep learning methods
was proposed. We extract the most useful information for various tasks using
the 3D Facial Model, leading to improved classification accuracy. Combining 3D
facial insights with ResNet architecture, our approach achieves notable
results: 100% individual classification, 95.4% gender classification, and 83.5%
expression classification accuracy. This method holds promise for advancing
facial analysis and recognition research.

</details>


### [251] [Understanding Long Videos with Multimodal Language Models](https://arxiv.org/pdf/2403.16998)
*Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo*

Main category: cs.CV

TL;DR: LLM-based approaches perform well on long-video tasks even with limited video info. Injecting video-specific info via object-centric modalities improves performance, leading to state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs' world knowledge and reasoning skills impact long-video understanding and improve performance by integrating video-specific info.

Method: Extract object-centric info from videos using vision tools, fuse it via natural language, and integrate into an LLM-based framework (MVU).

Result: MVU achieves state-of-the-art performance on video understanding benchmarks and shows strong generality in robotics tasks.

Conclusion: LLMs' reasoning enhances video understanding; integrating video-specific info further boosts performance, demonstrating MVU's effectiveness and versatility.

Abstract: Large Language Models (LLMs) have allowed recent LLM-based approaches to
achieve excellent performance on long-video understanding benchmarks. We
investigate how extensive world knowledge and strong reasoning skills of
underlying LLMs influence this strong performance. Surprisingly, we discover
that LLM-based approaches can yield surprisingly good accuracy on long-video
tasks with limited video information, sometimes even with no video specific
information. Building on this, we explore injecting video-specific information
into an LLM-based framework. We utilize off-the-shelf vision tools to extract
three object-centric information modalities from videos, and then leverage
natural language as a medium for fusing this information. Our resulting
Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art
performance across multiple video understanding benchmarks. Strong performance
also on robotics domain tasks establish its strong generality. Code:
https://github.com/kahnchana/mvu

</details>


### [252] [TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/pdf/2404.12803)
*Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Yangfan He, Kuan Lu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang*

Main category: cs.CV

TL;DR: The paper introduces Square-10M, a high-quality instruction-tuning dataset for text-centric VQA, generated using closed-source MLLMs. The model TextSquare outperforms open-source and top-tier models, highlighting the importance of dataset scale and reasoning data.


<details>
  <summary>Details</summary>
Motivation: Open-source models lag behind leading models like GPT4V and Gemini due to insufficient high-quality instruction-tuning data. The goal is to bridge this gap by creating a large, high-quality dataset.

Method: The Square method involves four steps: Self-Questioning, Answering, Reasoning, and Evaluation, to generate the Square-10M dataset.

Result: TextSquare surpasses open-source and top-tier models, achieving 62.2% on OCRBench and outperforming GPT4V/Gemini in 6 of 10 benchmarks. Reasoning data improves accuracy and reduces hallucinations.

Conclusion: Scaling text-centric VQA datasets exponentially improves model performance, validating the necessity of large, high-quality datasets like Square-10M.

Abstract: Text-centric visual question answering (VQA) has made great strides with the
development of Multimodal Large Language Models (MLLMs), yet open-source models
still fall short of leading models like GPT4V and Gemini, partly due to a lack
of extensive, high-quality instruction tuning data. To this end, we introduce a
new approach for creating a massive, high-quality instruction-tuning dataset,
Square-10M, which is generated using closed-source MLLMs. The data construction
process, termed Square, consists of four steps: Self-Questioning, Answering,
Reasoning, and Evaluation. Our experiments with Square-10M led to three key
findings: 1) Our model, TextSquare, considerably surpasses open-source previous
state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).
It even outperforms top-tier models like GPT4V and Gemini in 6 of 10
text-centric benchmarks. 2) Additionally, we demonstrate the critical role of
VQA reasoning data in offering comprehensive contextual insights for specific
questions. This not only improves accuracy but also significantly mitigates
hallucinations. Specifically, TextSquare scores an average of 75.1% across four
general VQA and hallucination evaluation datasets, outperforming previous
state-of-the-art models. 3) Notably, the phenomenon observed in scaling
text-centric VQA datasets reveals a vivid pattern: the exponential increase of
instruction tuning data volume is directly proportional to the improvement in
model performance, thereby validating the necessity of the dataset scale and
the high quality of Square-10M.

</details>


### [253] [MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering](https://arxiv.org/pdf/2405.11985)
*Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yangfan He, Kuan Lu, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, Can Huang*

Main category: cs.CV

TL;DR: MTVQA introduces a multilingual TEC-VQA benchmark with human-annotated QA pairs in 9 languages, highlighting performance gaps in MLLMs and offering training data to improve multilingual text-centric VQA.


<details>
  <summary>Details</summary>
Motivation: Existing TEC-VQA benchmarks focus on high-resource languages, and translation-based methods suffer from visual-textual misalignment and other issues. MTVQA addresses these gaps.

Method: MTVQA features 6,778 QA pairs across 2,116 images in 9 languages, with human expert annotations. It evaluates MLLMs like Qwen2-VL, GPT-4o, and others.

Result: MLLMs perform significantly worse than humans (e.g., Qwen2-VL scores 30.9 vs. human 79.7). Fine-tuning with MTVQA data improves performance.

Conclusion: MTVQA provides a valuable benchmark for multilingual TEC-VQA, encouraging further research in visual text comprehension.

Abstract: Text-Centric Visual Question Answering (TEC-VQA) in its proper format not
only facilitates human-machine interaction in text-centric visual environments
but also serves as a de facto gold proxy to evaluate AI models in the domain of
text-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks
have focused on high-resource languages like English and Chinese. Despite
pioneering works to expand multilingual QA pairs in non-text-centric VQA
datasets through translation engines, the translation-based protocol encounters
a substantial "visual-textual misalignment" problem when applied to TEC-VQA.
Specifically, it prioritizes the text in question-answer pairs while
disregarding the visual text present in images. Moreover, it fails to address
complexities related to nuanced meaning, contextual distortion, language bias,
and question-type diversity. In this work, we tackle multilingual TEC-VQA by
introducing MTVQA, the first benchmark featuring high-quality human expert
annotations across 9 diverse languages, consisting of 6,778 question-answer
pairs across 2,116 images. Further, by comprehensively evaluating numerous
state-of-the-art Multimodal Large Language Models~(MLLMs), including Qwen2-VL,
GPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that
there is still a large room for performance improvement (Qwen2-VL scoring 30.9
versus 79.7 for human performance), underscoring the value of MTVQA.
Additionally, we supply multilingual training data within the MTVQA dataset,
demonstrating that straightforward fine-tuning with this data can substantially
enhance multilingual TEC-VQA performance. We aspire that MTVQA will offer the
research community fresh insights and stimulate further exploration in
multilingual visual text comprehension. The project homepage is available at
https://bytedance.github.io/MTVQA/.

</details>


### [254] [Unseen Visual Anomaly Generation](https://arxiv.org/pdf/2406.01078)
*Han Sun, Yunkang Cao, Hao Dong, Olga Fink*

Main category: cs.CV

TL;DR: AnomalyAny uses Stable Diffusion to generate diverse, realistic unseen anomalies from a single normal sample, improving anomaly detection performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of anomalous data in visual anomaly detection by creating authentic synthetic anomalies.

Method: Leverages Stable Diffusion with attention-guided anomaly optimization and prompt-guided refinement for high-quality anomaly generation.

Result: Demonstrates effectiveness in generating realistic anomalies and enhancing anomaly detection on MVTec AD and VisA datasets.

Conclusion: AnomalyAny offers a scalable solution for realistic anomaly generation, boosting downstream AD tasks.

Abstract: Visual anomaly detection (AD) presents significant challenges due to the
scarcity of anomalous data samples. While numerous works have been proposed to
synthesize anomalous samples, these synthetic anomalies often lack authenticity
or require extensive training data, limiting their applicability in real-world
scenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel
framework that leverages Stable Diffusion (SD)'s image generation capabilities
to generate diverse and realistic unseen anomalies. By conditioning on a single
normal sample during test time, AnomalyAny is able to generate unseen anomalies
for arbitrary object types with text descriptions. Within AnomalyAny, we
propose attention-guided anomaly optimization to direct SD attention on
generating hard anomaly concepts. Additionally, we introduce prompt-guided
anomaly refinement, incorporating detailed descriptions to further improve the
generation quality. Extensive experiments on MVTec AD and VisA datasets
demonstrate AnomalyAny's ability in generating high-quality unseen anomalies
and its effectiveness in enhancing downstream AD performance.

</details>


### [255] [LieRE: Lie Rotational Positional Encodings](https://arxiv.org/pdf/2406.10322)
*Sophie Ostmeier, Brian Axelrod, Maya Varma, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz*

Main category: cs.CV

TL;DR: LieRE generalizes RoPE to high-dimensional rotation matrices, improving performance on 2D/3D tasks and offering better generalization.


<details>
  <summary>Details</summary>
Motivation: RoPE's limitations in handling high-dimensional data and limited representational capacity motivate the development of LieRE.

Method: LieRE leverages Lie group structure to generalize RoPE to high-dimensional rotation matrices.

Result: LieRE achieves 1.5% improvement on 2D tasks and 1% on 3D tasks, with superior generalization to higher resolutions.

Conclusion: LieRE is a computationally efficient solution for high-dimensional positional encoding, outperforming RoPE.

Abstract: Transformer architectures depend on explicit position encodings to capture
token positional information. Rotary Position Encoding (RoPE) has emerged as a
popular choice in language models due to its efficient encoding of relative
position information through key-query rotations. However, RoPE faces
significant limitations beyond language processing: it is constrained to
one-dimensional sequence data and, even with learnable phases, offers limited
representational capacity. We address these challenges with Lie Relative
Encodings (LieRE), which generalizes RoPE to high-dimensional rotation matrices
by leveraging their Lie group structure. Through extensive evaluation on three
image datasets across 2D and 3D classification tasks, LieRE achieves 1.5%
improvement over state-of-the-art baselines on 2D tasks and 1% on 3D tasks,
while demonstrating superior generalization to higher resolutions. Our
implementation is computationally efficient, with results reproducible on 4
A100 GPUs in 30 minutes on CIFAR100. Our code is available at
https://github.com/StanfordMIMI/LieRE.

</details>


### [256] [Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments](https://arxiv.org/pdf/2406.16439)
*Shilei Cao, Juepeng Zheng, Yan Liu, Baoquan Zhao, Ziqi Yuan, Weijia Li, Runmin Dong, Haohuan Fu*

Main category: cs.CV

TL;DR: AMROD improves CTTA for object detection by addressing low-quality pseudo-labels and ineffective parameter restoration, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To tackle issues of fixed pseudo-labeling thresholds and stochastic parameter restoration in CTTA for detection models.

Method: Uses object-level contrastive learning, adaptive monitoring for dynamic thresholds, and adaptive randomized restoration.

Result: Outperforms existing methods, with a 3.2 mAP improvement and 20% efficiency boost on Cityscapes-to-Cityscapes-C task.

Conclusion: AMROD effectively enhances CTTA for object detection by refining feature representation and preserving critical knowledge.

Abstract: Real-world application models are commonly deployed in dynamic environments,
where the target domain distribution undergoes temporal changes. Continual
Test-Time Adaptation (CTTA) has recently emerged as a promising technique to
gradually adapt a source-trained model to continually changing target domains.
Despite recent advancements in addressing CTTA, two critical issues remain: 1)
Fixed thresholds for pseudo-labeling in existing methodologies lead to
low-quality pseudo-labels, as model confidence varies across categories and
domains; 2) Stochastic parameter restoration methods for mitigating
catastrophic forgetting fail to preserve critical information effectively, due
to their intrinsic randomness. To tackle these challenges for detection models
in CTTA scenarios, we present AMROD, featuring three core components. Firstly,
the object-level contrastive learning module extracts object-level features for
contrastive learning to refine the feature representation in the target domain.
Secondly, the adaptive monitoring module dynamically skips unnecessary
adaptation and updates the category-specific threshold based on predicted
confidence scores to enable efficiency and improve the quality of
pseudo-labels. Lastly, the adaptive randomized restoration mechanism
selectively reset inactive parameters with higher possibilities, ensuring the
retention of essential knowledge. We demonstrate the effectiveness of AMROD on
four CTTA object detection tasks, where AMROD outperforms existing methods,
especially achieving a 3.2 mAP improvement and a 20\% increase in efficiency on
the Cityscapes-to-Cityscapes-C CTTA task. The code of this work is available at
https://github.com/ShileiCao/AMROD.

</details>


### [257] [BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for Semantic- and Spatial-Aware 3D Object Detection](https://arxiv.org/pdf/2406.19048)
*Yang Song, Lin Wang*

Main category: cs.CV

TL;DR: BiCo-Fusion is a bidirectional complementary LiDAR-camera fusion framework for 3D object detection, enhancing semantic and spatial awareness by adaptively fusing enhanced features from both modalities.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods in 3D object detection preserve the drawbacks of LiDAR (lack of semantic detail) and cameras (lack of accurate 3D spatial information), diluting the final representation.

Method: Proposes BiCo-Fusion with Pre-Fusion (VEM and IEM) to enhance LiDAR and camera features bidirectionally, followed by Unified Fusion (U-Fusion) for adaptive feature fusion.

Result: Extensive experiments show BiCo-Fusion outperforms prior methods.

Conclusion: BiCo-Fusion effectively addresses the limitations of direct fusion, achieving robust semantic- and spatial-aware 3D object detection.

Abstract: 3D object detection is an important task that has been widely applied in
autonomous driving. To perform this task, a new trend is to fuse multi-modal
inputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these
two modalities by unifying them in the same 3D space. However, during direct
fusion in a unified space, the drawbacks of both modalities (LiDAR features
struggle with detailed semantic information and the camera lacks accurate 3D
spatial information) are also preserved, diluting semantic and spatial
awareness of the final unified representation. To address the issue, this
letter proposes a novel bidirectional complementary LiDAR-camera fusion
framework, called BiCo-Fusion that can achieve robust semantic- and
spatial-aware 3D object detection. The key insight is to fuse LiDAR and camera
features in a bidirectional complementary way to enhance the semantic awareness
of the LiDAR and the 3D spatial awareness of the camera. The enhanced features
from both modalities are then adaptively fused to build a semantic- and
spatial-aware unified representation. Specifically, we introduce Pre-Fusion
consisting of a Voxel Enhancement Module (VEM) to enhance the semantic
awareness of voxel features from 2D camera features and Image Enhancement
Module (IEM) to enhance the 3D spatial awareness of camera features from 3D
voxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse
the enhanced features from the last stage to build a unified representation.
Extensive experiments demonstrate the superiority of our BiCo-Fusion against
the prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.

</details>


### [258] [CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference](https://arxiv.org/pdf/2407.12736)
*Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram*

Main category: cs.CV

TL;DR: CHOSEN is a software-hardware co-design framework for deploying Vision Transformers (ViTs) on FPGAs, addressing challenges like non-linear calculations and high computational demands. It improves throughput by 1.5x and 1.42x on DeiT-S and DeiT-B models.


<details>
  <summary>Details</summary>
Motivation: ViTs face deployment challenges on FPGAs due to non-linear calculations and high computational/memory demands.

Method: CHOSEN uses multi-kernel design, approximate non-linear functions, efficient logic block usage, and a novel compiler for optimal hardware configuration.

Result: Achieves 1.5x and 1.42x throughput improvement on DeiT-S and DeiT-B models.

Conclusion: CHOSEN effectively optimizes ViT deployment on FPGAs, outperforming state-of-the-art accelerators.

Abstract: Vision Transformers (ViTs) represent a groundbreaking shift in machine
learning approaches to computer vision. Unlike traditional approaches, ViTs
employ the self-attention mechanism, which has been widely used in natural
language processing, to analyze image patches. Despite their advantages in
modeling visual tasks, deploying ViTs on hardware platforms, notably
Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges.
These challenges stem primarily from the non-linear calculations and high
computational and memory demands of ViTs. This paper introduces CHOSEN, a
software-hardware co-design framework to address these challenges and offer an
automated framework for ViT deployment on the FPGAs in order to maximize
performance. Our framework is built upon three fundamental contributions:
multi-kernel design to maximize the bandwidth, mainly targeting benefits of
multi DDR memory banks, approximate non-linear functions that exhibit minimal
accuracy degradation, and efficient use of available logic blocks on the FPGA,
and efficient compiler to maximize the performance and memory-efficiency of the
computing kernels by presenting a novel algorithm for design space exploration
to find optimal hardware configuration that achieves optimal throughput and
latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a
1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.

</details>


### [259] [XMeCap: Meme Caption Generation with Sub-Image Adaptability](https://arxiv.org/pdf/2407.17152)
*Yuyan Chen, Songzhou Yan, Zhihong Zhu, Zhixu Li, Yanghua Xiao*

Main category: cs.CV

TL;DR: The paper introduces the XMeCap framework for meme captioning, combining supervised fine-tuning and reinforcement learning to improve humor understanding in multi-modal contexts.


<details>
  <summary>Details</summary>
Motivation: Humor, especially in memes, is challenging for machines due to its cultural and multi-modal nature. The study focuses on multi-image memes to advance captioning.

Method: The XMeCap framework uses supervised fine-tuning and reinforcement learning with a reward model considering global and local visual-text similarities.

Result: XMeCap outperforms baselines, scoring 75.85 for single-image and 66.32 for multi-image memes, improving by 6.75% and 8.56%, respectively.

Conclusion: The research advances meme studies and demonstrates machine potential in multi-modal humor understanding and generation.

Abstract: Humor, deeply rooted in societal meanings and cultural details, poses a
unique challenge for machines. While advances have been made in natural
language processing, real-world humor often thrives in a multi-modal context,
encapsulated distinctively by memes. This paper poses a particular emphasis on
the impact of multi-images on meme captioning. After that, we introduce the
\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning
and reinforcement learning based on an innovative reward model, which factors
in both global and local similarities between visuals and text. Our results,
benchmarked against contemporary models, manifest a marked improvement in
caption generation for both single-image and multi-image memes, as well as
different meme categories. \textsc{XMeCap} achieves an average evaluation score
of 75.85 for single-image memes and 66.32 for multi-image memes, outperforming
the best baseline by 6.75\% and 8.56\%, respectively. This research not only
establishes a new frontier in meme-related studies but also underscores the
potential of machines in understanding and generating humor in a multi-modal
setting.

</details>


### [260] [FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering](https://arxiv.org/pdf/2408.12894)
*Yunji Seo, Young Sun Choi, Hyun Seung Son, Youngjung Uh*

Main category: cs.CV

TL;DR: FLoD introduces a flexible multi-level 3DGS representation to adapt to varying GPU hardware, balancing quality and memory usage.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods are hardware-specific, lacking adaptability to different GPU setups, which limits their practical application.

Method: FLoD uses level-specific 3D scale constraints and a level-by-level training strategy to create a multi-level 3DGS representation, enabling selective rendering at varying detail levels.

Result: FLoD offers adjustable rendering options for diverse GPU settings, maintaining real-time performance and quality-memory trade-offs.

Conclusion: FLoD is the first 3DGS method to integrate LoD principles flexibly, showing promise for future 3DGS frameworks.

Abstract: 3D Gaussian Splatting (3DGS) and its subsequent works are restricted to
specific hardware setups, either on only low-cost or on only high-end
configurations. Approaches aimed at reducing 3DGS memory usage enable rendering
on low-cost GPU but compromise rendering quality, which fails to leverage the
hardware capabilities in the case of higher-end GPU. Conversely, methods that
enhance rendering quality require high-end GPU with large VRAM, making such
methods impractical for lower-end devices with limited memory capacity.
Consequently, 3DGS-based works generally assume a single hardware setup and
lack the flexibility to adapt to varying hardware constraints.
  To overcome this limitation, we propose Flexible Level of Detail (FLoD) for
3DGS. FLoD constructs a multi-level 3DGS representation through level-specific
3D scale constraints, where each level independently reconstructs the entire
scene with varying detail and GPU memory usage. A level-by-level training
strategy is introduced to ensure structural consistency across levels.
Furthermore, the multi-level structure of FLoD allows selective rendering of
image regions at different detail levels, providing additional memory-efficient
rendering options. To our knowledge, among prior works which incorporate the
concept of Level of Detail (LoD) with 3DGS, FLoD is the first to follow the
core principle of LoD by offering adjustable options for a broad range of GPU
settings.
  Experiments demonstrate that FLoD provides various rendering options with
trade-offs between quality and memory usage, enabling real-time rendering under
diverse memory constraints. Furthermore, we show that FLoD generalizes to
different 3DGS frameworks, indicating its potential for integration into future
state-of-the-art developments.

</details>


### [261] [Holistic Uncertainty Estimation For Open-Set Recognition](https://arxiv.org/pdf/2408.14229)
*Leonid Erlygin, Alexey Zaytsev*

Main category: cs.CV

TL;DR: HolUE is a Bayesian probabilistic model for holistic uncertainty estimation in open-set recognition, addressing gallery and embedding uncertainties, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate uncertainty estimation is crucial in open-set recognition, especially when probe samples may belong to unknown identities, and existing methods like probabilistic embeddings partially address this.

Method: HolUE uses a Bayesian probabilistic model to account for gallery uncertainty (overlapping classes) and embedding uncertainty, tested on datasets like IJB-C and VoxBlink.

Result: HolUE outperforms other uncertainty estimation methods, including sample quality-based approaches, in identifying recognition errors.

Conclusion: HolUE provides a robust solution for uncertainty estimation in open-set recognition, validated across diverse datasets and protocols.

Abstract: Accurate uncertainty estimation is a critical challenge in open-set
recognition, where a probe biometric sample may belong to an unknown identity.
It can be addressed through sample quality estimation via probabilistic
embeddings. However, the low variance of probabilistic embedding only partly
implies a low identification error probability: an embedding of a sample could
be close to several classes in a gallery, thus yielding high uncertainty
despite high sample quality. We propose HolUE - a holistic uncertainty
estimation method based on a Bayesian probabilistic model; it is aware of two
sources of ambiguity in the open-set recognition system: (1) the gallery
uncertainty caused by overlapping classes and (2) the uncertainty of
embeddings. Challenging open-set recognition datasets, such as IJB-C for the
image domain and VoxBlink for the audio domain, serve as a testbed for our
method. We also provide a new open-set recognition protocol for the
identification of whales and dolphins. In all cases, HolUE better identifies
recognition errors than alternative uncertainty estimation methods, including
those based solely on sample quality.

</details>


### [262] [MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling](https://arxiv.org/pdf/2409.16160)
*Yifang Men, Yuan Yao, Miaomiao Cui, Liefeng Bo*

Main category: cs.CV

TL;DR: MIMO is a novel framework for character video synthesis that achieves scalability, motion generality, and scene interaction by encoding 2D videos into 3D spatial codes.


<details>
  <summary>Details</summary>
Motivation: Existing 3D methods require multi-view captures, limiting applicability, while 2D methods struggle with pose generality and scene interaction. MIMO addresses these limitations.

Method: MIMO encodes 2D videos into 3D spatial codes (identity, motion, scene) using monocular depth estimation and hierarchical decomposition.

Result: The framework enables flexible user control, complex motion expression, and 3D-aware synthesis, demonstrating effectiveness in experiments.

Conclusion: MIMO provides a unified solution for scalable, interactive, and realistic character video synthesis.

Abstract: Character video synthesis aims to produce realistic videos of animatable
characters within lifelike scenes. As a fundamental problem in the computer
vision and graphics community, 3D works typically require multi-view captures
for per-case training, which severely limits their applicability of modeling
arbitrary characters in a short time. Recent 2D methods break this limitation
via pre-trained diffusion models, but they struggle for pose generality and
scene interaction. To this end, we propose MIMO, a novel framework which can
not only synthesize character videos with controllable attributes (i.e.,
character, motion and scene) provided by simple user inputs, but also
simultaneously achieve advanced scalability to arbitrary characters, generality
to novel 3D motions, and applicability to interactive real-world scenes in a
unified framework. The core idea is to encode the 2D video to compact spatial
codes, considering the inherent 3D nature of video occurrence. Concretely, we
lift the 2D frame pixels into 3D using monocular depth estimators, and
decompose the video clip to three spatial components (i.e., main human,
underlying scene, and floating occlusion) in hierarchical layers based on the
3D depth. These components are further encoded to canonical identity code,
structured motion code and full scene code, which are utilized as control
signals of synthesis process. The design of spatial decomposed modeling enables
flexible user control, complex motion expression, as well as 3D-aware synthesis
for scene interactions. Experimental results demonstrate effectiveness and
robustness of the proposed method.

</details>


### [263] [Multimodal Pragmatic Jailbreak on Text-to-image Models](https://arxiv.org/pdf/2409.19149)
*Tong Liu, Zhixin Lai, Jiawen Wang, Gengyuan Zhang, Shuo Chen, Philip Torr, Vera Demberg, Volker Tresp, Jindong Gu*

Main category: cs.CV

TL;DR: The paper introduces a novel jailbreak method for text-to-image (T2I) models, where safe images and text combine to create unsafe content. It benchmarks nine models, revealing high unsafety rates, and evaluates existing filters' ineffectiveness.


<details>
  <summary>Details</summary>
Motivation: To address growing safety concerns in generative models, the study explores a new type of jailbreak that bypasses traditional safety measures by combining seemingly safe elements.

Method: The authors propose a dataset to evaluate T2I models under this jailbreak, benchmark nine models (including commercial ones), and assess the effectiveness of common filters.

Result: All tested models are vulnerable, with unsafe generation rates ranging from 10% to 70%. Existing filters fail to mitigate the jailbreak.

Conclusion: The work highlights a critical vulnerability in T2I models and calls for further development of more secure and reliable systems.

Abstract: Diffusion models have recently achieved remarkable advancements in terms of
image quality and fidelity to textual prompts. Concurrently, the safety of such
generative models has become an area of growing concern. This work introduces a
novel type of jailbreak, which triggers T2I models to generate the image with
visual text, where the image and the text, although considered to be safe in
isolation, combine to form unsafe content. To systematically explore this
phenomenon, we propose a dataset to evaluate the current diffusion-based
text-to-image (T2I) models under such jailbreak. We benchmark nine
representative T2I models, including two closed-source commercial models.
Experimental results reveal a concerning tendency to produce unsafe content:
all tested models suffer from such type of jailbreak, with rates of unsafe
generation ranging from around 10\% to 70\% where DALLE 3 demonstrates almost
the highest unsafety. In real-world scenarios, various filters such as keyword
blocklists, customized prompt filters, and NSFW image filters, are commonly
employed to mitigate these risks. We evaluate the effectiveness of such filters
against our jailbreak and found that, while these filters may be effective for
single modality detection, they fail to work against our jailbreak. We also
investigate the underlying reason for such jailbreaks, from the perspective of
text rendering capability and training data. Our work provides a foundation for
further development towards more secure and reliable T2I models. Project page
at https://multimodalpragmatic.github.io/.

</details>


### [264] [EMMA: Efficient Visual Alignment in Multi-Modal LLMs](https://arxiv.org/pdf/2410.02080)
*Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami*

Main category: cs.CV

TL;DR: EMMA is a lightweight cross-modality module for efficient fusion of visual and textual encodings in MLLMs, improving performance and robustness with minimal added parameters.


<details>
  <summary>Details</summary>
Motivation: Challenges in optimally fusing visual encodings within language models for task-specific adaptability, despite progress in enhancing language components.

Method: Proposes EMMA, featuring an efficient early fusion mechanism, interpretability analysis, and comprehensive experiments.

Result: Boosts performance by up to 9.3% across tasks and improves robustness against hallucinations.

Conclusion: EMMA offers an efficient solution for multi-modal fusion in MLLMs, balancing performance and model complexity.

Abstract: Multi-modal Large Language Models (MLLMs) have recently exhibited impressive
general-purpose capabilities by leveraging vision foundation models to encode
the core concepts of images into representations. These are then combined with
instructions and processed by the language model to generate high-quality
responses. Despite significant progress in enhancing the language component,
challenges persist in optimally fusing visual encodings within the language
model for task-specific adaptability. Recent research has focused on improving
this fusion through modality adaptation modules but at the cost of
significantly increased model complexity and training data needs. In this
paper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight
cross-modality module designed to efficiently fuse visual and textual
encodings, generating instruction-aware visual representations for the language
model. Our key contributions include: (1) an efficient early fusion mechanism
that integrates vision and language representations with minimal added
parameters (less than 0.2% increase in model size), (2) an in-depth
interpretability analysis that sheds light on the internal mechanisms of the
proposed method; (3) comprehensive experiments that demonstrate notable
improvements on both specialized and general benchmarks for MLLMs. Empirical
results show that EMMA boosts performance across multiple tasks by up to 9.3%
while significantly improving robustness against hallucinations. Our code is
available at https://github.com/SaraGhazanfari/EMMA

</details>


### [265] [Dynamic Negative Guidance of Diffusion Models](https://arxiv.org/pdf/2410.14398)
*Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni*

Main category: cs.CV

TL;DR: Dynamic Negative Guidance (DNG) improves upon Negative Prompting (NP) by addressing its limitations with a time and state-dependent modulation, enhancing safety, class balance, and image quality without extra training.


<details>
  <summary>Details</summary>
Motivation: Conventional NP assumes a constant guidance scale, leading to suboptimal results due to the non-stationarity of the reverse process. This paper aims to overcome this limitation.

Method: DNG modulates guidance dynamically by estimating posterior class probabilities during denoising, leveraging the discrete Markov Chain for efficiency.

Result: DNG outperforms NP in class-removal tasks on MNIST and CIFAR10, offering better safety, class balance, and image quality. It also works effectively with Stable Diffusion.

Conclusion: DNG provides a principled, efficient alternative to NP, improving guidance accuracy and reducing invasiveness in diffusion models.

Abstract: Negative Prompting (NP) is widely utilized in diffusion models, particularly
in text-to-image applications, to prevent the generation of undesired features.
In this paper, we show that conventional NP is limited by the assumption of a
constant guidance scale, which may lead to highly suboptimal results, or even
complete failure, due to the non-stationarity and state-dependence of the
reverse process. Based on this analysis, we derive a principled technique
called Dynamic Negative Guidance, which relies on a near-optimal time and state
dependent modulation of the guidance without requiring additional training.
Unlike NP, negative guidance requires estimating the posterior class
probability during the denoising process, which is achieved with limited
additional computational overhead by tracking the discrete Markov Chain during
the generative process. We evaluate the performance of DNG class-removal on
MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation
of class balance and image quality when compared with baseline methods.
Furthermore, we show that it is possible to use DNG with Stable Diffusion to
obtain more accurate and less invasive guidance than NP.

</details>


### [266] [Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization](https://arxiv.org/pdf/2411.13610)
*Hao Ju, Shaofei Huang, Si Liu, Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper introduces Video2BEV, a new paradigm for drone geo-localization by transforming drone videos into Bird's Eye View (BEV) to improve matching accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing image-based drone geo-localization methods underutilize video data and struggle with occlusions and viewpoint disparities.

Method: The Video2BEV paradigm uses Gaussian Splatting for 3D scene reconstruction and BEV projection, along with a diffusion-based module for generating hard negative samples.

Result: Experiments on the UniV dataset show Video2BEV achieves competitive recall rates and outperforms conventional methods, especially in occluded scenarios.

Conclusion: Video2BEV offers a robust solution for drone geo-localization by leveraging video data and BEV transformation, validated by the new UniV dataset.

Abstract: Existing approaches to drone visual geo-localization predominantly adopt the
image-based setting, where a single drone-view snapshot is matched with images
from other platforms. Such task formulation, however, underutilizes the
inherent video output of the drone and is sensitive to occlusions and viewpoint
disparity. To address these limitations, we formulate a new video-based drone
geo-localization task and propose the Video2BEV paradigm. This paradigm
transforms the video into a Bird's Eye View (BEV), simplifying the subsequent
\textbf{inter-platform} matching process. In particular, we employ Gaussian
Splatting to reconstruct a 3D scene and obtain the BEV projection. Different
from the existing transform methods, \eg, polar transform, our BEVs preserve
more fine-grained details without significant distortion. To facilitate the
discriminative \textbf{intra-platform} representation learning, our Video2BEV
paradigm also incorporates a diffusion-based module for generating hard
negative samples. To validate our approach, we introduce UniV, a new
video-based geo-localization dataset that extends the image-based
University-1652 dataset. UniV features flight paths at $30^\circ$ and
$45^\circ$ elevation angles with increased frame rates of up to 10 frames per
second (FPS). Extensive experiments on the UniV dataset show that our Video2BEV
paradigm achieves competitive recall rates and outperforms conventional
video-based methods. Compared to other competitive methods, our proposed
approach exhibits robustness at lower elevations with more occlusions.

</details>


### [267] [Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual Conversion in Visual Reasoning for Multimodal Large Language Models](https://arxiv.org/pdf/2411.18142)
*Jingming Liu, Yumeng Li, Boyuan Xiao, Yichang Jian, Ziang Qin, Tianjia Shao, Yao-Xiang Ding, Kun Zhou*

Main category: cs.CV

TL;DR: MLLMs struggle with visual tasks like counting and puzzles due to perceptual bottlenecks. The proposed 'autonomous imagination' method decomposes visual-to-textual conversion into iterative steps, enabling MLLMs to solve previously challenging tasks without retraining.


<details>
  <summary>Details</summary>
Motivation: MLLMs face difficulties in visual tasks despite their success in textual reasoning. The challenge lies in converting complex visual inputs into textual information for reasoning.

Method: Introduces 'autonomous imagination,' where MLLMs iteratively modify visual inputs (e.g., isolating objects, rearranging puzzle pieces) to decompose the visual-to-textual conversion process.

Result: MLLMs can now solve tasks beyond their initial perceptual capability without retraining, demonstrating the effectiveness of closed-loop visual modification.

Conclusion: Closed-loop visual modification is a viable approach to decompose visual reasoning tasks into manageable substeps, enhancing MLLM performance.

Abstract: Under pure textual modality, Large Language Models (LLMs) have demonstrated
remarkable success in complex reasoning tasks by decomposing them into simpler
sub-problems. However, Multimodal Large Language Models (MLLMs) still struggle
with some seemingly straightforward visual tasks, such as counting and solving
jigsaw puzzles. We argue that these tasks challenge the ability of
visual-to-textual conversion, where MLLMs convert visual information perceived
from the input scene, to textual information for further reasoning and
generating the answer. If the complexity of the visual input is beyond the
perceptual capability of the MLLMs, without decomposing this conversion
process, simply scaling inference-time reasoning cannot solve the task because
it repeatedly encounters the same perceptual bottleneck. We propose an
approach, autonomous imagination, to enable MLLMs to iteratively modify visual
inputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate
visual states, decomposing visual-to-textual conversion into closed-loop visual
modification steps. We show that, without any retraining, MLLMs can now solve
tasks initially beyond their perceptual capability, highlighting that
closed-loop visual modification can be an effective way of decomposing the
visual reasoning task into solvable substeps. Project page:
https://future-item.github.io/autoimagine-site/

</details>


### [268] [Spectral Image Tokenizer](https://arxiv.org/pdf/2412.09607)
*Carlos Esteves, Mohammed Suhail, Ameesh Makadia*

Main category: cs.CV

TL;DR: The paper proposes a wavelet-based image tokenizer for autoregressive image generation, offering advantages like multiscale processing, resolution flexibility, and improved conditioning.


<details>
  <summary>Details</summary>
Motivation: Current image tokenizers use raster scan order, which is suboptimal for autoregressive modeling. The paper aims to improve this by leveraging the image spectrum for better compression and generation.

Method: The authors use a discrete wavelet transform (DWT) to tokenize images in a coarse-to-fine manner, enabling multiscale processing and resolution-independent reconstruction.

Result: The proposed tokenizer improves reconstruction metrics, supports multiscale generation, and enables applications like text-guided upsampling and editing.

Conclusion: Wavelet-based tokenization enhances autoregressive image generation by addressing limitations of raster scan methods, offering practical benefits for diverse tasks.

Abstract: Image tokenizers map images to sequences of discrete tokens, and are a
crucial component of autoregressive transformer-based image generation. The
tokens are typically associated with spatial locations in the input image,
arranged in raster scan order, which is not ideal for autoregressive modeling.
In this paper, we propose to tokenize the image spectrum instead, obtained from
a discrete wavelet transform (DWT), such that the sequence of tokens represents
the image in a coarse-to-fine fashion. Our tokenizer brings several advantages:
1) it leverages that natural images are more compressible at high frequencies,
2) it can take and reconstruct images of different resolutions without
retraining, 3) it improves the conditioning for next-token prediction --
instead of conditioning on a partial line-by-line reconstruction of the image,
it takes a coarse reconstruction of the full image, 4) it enables partial
decoding where the first few generated tokens can reconstruct a coarse version
of the image, 5) it enables autoregressive models to be used for image
upsampling. We evaluate the tokenizer reconstruction metrics as well as
multiscale image generation, text-guided image upsampling and editing.

</details>


### [269] [MVTamperBench: Evaluating Robustness of Vision-Language Models](https://arxiv.org/pdf/2412.19794)
*Amit Agarwal, Srikant Panda, Angeline Charles, Bhargava Kumar, Hitesh Patel, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Hansa Meghwani, Karan Gupta, Dong-Kyu Chae*

Main category: cs.CV

TL;DR: MVTamperBench is a benchmark evaluating MLLM robustness against five tampering techniques, revealing variability in resilience and challenging the assumption that larger models are inherently more robust.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored vulnerability of MLLMs to adversarial tampering and manipulations in video understanding.

Method: Introduces MVTamperBench, a benchmark with ~3.4K original videos expanded into ~17K tampered clips, covering 19 video manipulation tasks, and evaluates 45 MLLMs.

Result: Substantial variability in resilience across tampering types; larger parameter counts do not guarantee robustness.

Conclusion: MVTamperBench sets a new standard for developing tamper-resilient MLLMs in safety-critical applications, with open code and data for further research.

Abstract: Multimodal Large Language Models (MLLMs), are recent advancement of
Vision-Language Models (VLMs) that have driven major advances in video
understanding. However, their vulnerability to adversarial tampering and
manipulations remains underexplored. To address this gap, we introduce
\textbf{MVTamperBench}, a benchmark that systematically evaluates MLLM
robustness against five prevalent tampering techniques: rotation, masking,
substitution, repetition, and dropping; based on real-world visual tampering
scenarios such as surveillance interference, social media content edits, and
misinformation injection. MVTamperBench comprises ~3.4K original videos,
expanded into over ~17K tampered clips covering 19 distinct video manipulation
tasks. This benchmark challenges models to detect manipulations in spatial and
temporal coherence. We evaluate 45 recent MLLMs from 15+ model families. We
reveal substantial variability in resilience across tampering types and show
that larger parameter counts do not necessarily guarantee robustness.
MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in
safety-critical applications, including detecting clickbait, preventing harmful
content distribution, and enforcing policies on media platforms. We release all
code, data, and benchmark to foster open research in trustworthy video
understanding.
  Code: https://amitbcp.github.io/MVTamperBench/ Data:
https://huggingface.co/datasets/Srikant86/MVTamperBench

</details>


### [270] [ICONS: Influence Consensus for Vision-Language Data Selection](https://arxiv.org/pdf/2501.00654)
*Xindi Wu, Mengzhou Xia, Rulin Shao, Zhiwei Deng, Pang Wei Koh, Olga Russakovsky*

Main category: cs.CV

TL;DR: ICONS is a gradient-based method for selecting impactful training data in vision-language models, improving efficiency and performance by prioritizing cross-task valuable examples.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for vision-language models are inefficient due to redundancy and lack of multitask optimization, leading to unnecessary computational costs.

Method: ICONS uses gradient-based influence estimation and majority voting across tasks to identify consistently valuable data points.

Result: Selected subsets (20% of data) retain ~98.6-98.8% performance of full datasets and can outperform full data training at 60% selection.

Conclusion: ICONS enables efficient, scalable vision-language model training with strong generalization to unseen tasks and architectures.

Abstract: Training vision-language models via instruction tuning often relies on large
mixtures of data spanning diverse tasks and domains. However, these mixtures
frequently include redundant information, increasing computational costs
without proportional performance gains, necessitating more effective data
selection strategies. Existing methods typically rely on task-agnostic
heuristics to estimate data importance or focus on optimizing single tasks in
isolation, limiting their effectiveness in multitask settings. In this work, we
introduce ICONS, a gradient-based Influence CONsensus approach for
vision-language data Selection. Our method leverages first-order training
dynamics to estimate the influence of individual training examples on
validation performance and aggregates these estimates across tasks via majority
voting over task-specific influences. This cross-task consensus identifies data
points that are consistently valuable across tasks, enabling us to prioritize
examples that drive overall performance. The voting-based design further
mitigates issues such as score calibration and outlier sensitivity, resulting
in robust and scalable data selection for diverse multitask mixtures. With only
20% of the data from LLaVA-665K and Cambrian-7M, our selected subsets retain
98.6% and 98.8% of the performance achieved with full datasets, and can even
surpass full data training at a 60% selection ratio on LLaVA-665K. Our approach
also generalizes to unseen tasks and architectures, demonstrating strong
transfer. We release two compact, high-utility subsets, LLaVA-ICONS-133K and
Cambrian-ICONS-1.4M, preserving impactful training examples for efficient and
scalable vision-language model development.

</details>


### [271] [Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion](https://arxiv.org/pdf/2501.04606)
*Yangfan He, Sida Li, Jianhui Wang, Kun Li, Xinyuan Song, Xinhang Yuan, Keqin Li, Kuan Lu, Menghao Huo, Jingqun Tang, Yi Xin, Jiaqi Chen, Miao Zhang, Xueqian Wang*

Main category: cs.CV

TL;DR: The paper proposes GE-Adapter, a method to improve temporal and spatial consistency in text-to-video editing using pre-trained diffusion models, addressing issues like high training costs and poor coherence.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image (T2I) generation methods for video editing suffer from poor temporal consistency, high training costs, or limited coherence. The goal is to enhance consistency and efficiency without resource-intensive training.

Method: GE-Adapter integrates temporal-spatial and semantic consistency with Baliteral DDIM inversion. It includes FTC Blocks for temporal consistency, SCD Blocks for spatial coherence, and a TSC Module for semantic alignment.

Result: The method improves perceptual quality, text-image alignment, and temporal coherence on the MSR-VTT dataset, achieving better fidelity and frame-to-frame coherence.

Conclusion: GE-Adapter offers a practical and efficient solution for text-to-video editing, enhancing consistency and reducing resource demands.

Abstract: Recent advancements in text-to-image (T2I) generation using diffusion models
have enabled cost-effective video-editing applications by leveraging
pre-trained models, eliminating the need for resource-intensive training.
However, the frame-independence of T2I generation often results in poor
temporal consistency. Existing methods address this issue through temporal
layer fine-tuning or inference-based temporal propagation, but these approaches
suffer from high training costs or limited temporal coherence. To address these
challenges, we propose a General and Efficient Adapter (GE-Adapter) that
integrates temporal-spatial and semantic consistency with Baliteral DDIM
inversion. This framework introduces three key components: (1) Frame-based
Temporal Consistency Blocks (FTC Blocks) to capture frame-specific features and
enforce smooth inter-frame transitions via temporally-aware loss functions; (2)
Channel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral
filters to enhance spatial coherence by reducing noise and artifacts; and (3)
Token-based Semantic Consistency Module (TSC Module) to maintain semantic
alignment using shared prompt tokens and frame-specific tokens. Our method
significantly improves perceptual quality, text-image alignment, and temporal
coherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves
enhanced fidelity and frame-to-frame coherence, offering a practical solution
for T2V editing.

</details>


### [272] [SmartEraser: Remove Anything from Images using Masked-Region Guidance](https://arxiv.org/pdf/2501.08279)
*Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, Houqiang Li*

Main category: cs.CV

TL;DR: SmartEraser introduces Masked-Region Guidance for object removal, outperforming traditional mask-and-inpaint methods by leveraging masked regions as guidance.


<details>
  <summary>Details</summary>
Motivation: The mask-and-inpaint paradigm lacks contextual information for masked areas, leading to unstable performance. SmartEraser addresses this by retaining masked regions as guidance.

Method: SmartEraser uses Masked-Region Guidance, retaining masked regions in the input to guide removal. It also introduces Syn4Removal, a dataset for training and evaluation.

Result: SmartEraser significantly outperforms existing methods, especially in complex scenes, by accurately identifying and removing objects while preserving context.

Conclusion: The Masked-Region Guidance paradigm and SmartEraser offer a robust solution for object removal, improving accuracy and context preservation.

Abstract: Object removal has so far been dominated by the mask-and-inpaint paradigm,
where the masked region is excluded from the input, leaving models relying on
unmasked areas to inpaint the missing region. However, this approach lacks
contextual information for the masked area, often resulting in unstable
performance. In this work, we introduce SmartEraser, built with a new removing
paradigm called Masked-Region Guidance. This paradigm retains the masked region
in the input, using it as guidance for the removal process. It offers several
distinct advantages: (a) it guides the model to accurately identify the object
to be removed, preventing its regeneration in the output; (b) since the user
mask often extends beyond the object itself, it aids in preserving the
surrounding context in the final result. Leveraging this new paradigm, we
present Syn4Removal, a large-scale object removal dataset, where instance
segmentation data is used to copy and paste objects onto images as removal
targets, with the original images serving as ground truths. Experimental
results demonstrate that SmartEraser significantly outperforms existing
methods, achieving superior performance in object removal, especially in
complex scenes with intricate compositions.

</details>


### [273] [TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval](https://arxiv.org/pdf/2501.10935)
*Shuai Lyu, Zijing Tian, Zhonghong Ou, Yifan Zhu, Xiao Zhang, Qiankun Ha, Haoran Luo, Meina Song*

Main category: cs.CV

TL;DR: TSVC introduces a tripartite learning mechanism for robust image-text retrieval, addressing noisy correspondence (NC) with a Coordinator, Master, and Assistant model, and a soft label estimation method.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal retrieval methods assume well-aligned data pairs, ignoring noisy correspondence (NC), leading to performance degradation. Homogeneous models in co-teaching paradigms limit additional information.

Method: TSVC uses a tripartite cooperative learning mechanism (Coordinator, Master, Assistant) and a soft label estimation method based on mutual information variation. A new loss function enhances robustness.

Result: Experiments on three datasets show TSVC's superior retrieval accuracy and stable training performance, even with increasing noise ratios.

Conclusion: TSVC effectively addresses NC in cross-modal retrieval, improving robustness and performance through diverse model collaboration and soft label estimation.

Abstract: Cross-modal retrieval maps data under different modality via semantic
relevance. Existing approaches implicitly assume that data pairs are
well-aligned and ignore the widely existing annotation noise, i.e., noisy
correspondence (NC). Consequently, it inevitably causes performance
degradation. Despite attempts that employ the co-teaching paradigm with
identical architectures to provide distinct data perspectives, the differences
between these architectures are primarily stemmed from random initialization.
Thus, the model becomes increasingly homogeneous along with the training
process. Consequently, the additional information brought by this paradigm is
severely limited. In order to resolve this problem, we introduce a Tripartite
learning with Semantic Variation Consistency (TSVC) for robust image-text
retrieval. We design a tripartite cooperative learning mechanism comprising a
Coordinator, a Master, and an Assistant model. The Coordinator distributes
data, and the Assistant model supports the Master model's noisy label
prediction with diverse data. Moreover, we introduce a soft label estimation
method based on mutual information variation, which quantifies the noise in new
samples and assigns corresponding soft labels. We also present a new loss
function to enhance robustness and optimize training effectiveness. Extensive
experiments on three widely used datasets demonstrate that, even at increasing
noise ratios, TSVC exhibits significant advantages in retrieval accuracy and
maintains stable training performance.

</details>


### [274] [Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration](https://arxiv.org/pdf/2501.16583)
*Long Peng, Xin Di, Zhanfeng Feng, Wenbo Li, Renjing Pei, Yang Wang, Xueyang Fu, Yang Cao, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: TAMambaIR is a texture-aware image restoration method that balances quality and efficiency by focusing on complex textures and using a novel state-space model.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-range dependencies and ignore spatial degradation characteristics, leading to suboptimal trade-offs between quality and efficiency.

Method: Proposes a Texture-Aware State Space Model and a Multi-Directional Perception Block to enhance texture awareness and efficiency.

Result: Achieves state-of-the-art performance in super-resolution, deraining, and low-light enhancement with improved efficiency.

Conclusion: TAMambaIR is a robust and efficient framework for image restoration, addressing key limitations of current methods.

Abstract: Image restoration aims to recover details and enhance contrast in degraded
images. With the growing demand for high-quality imaging (\textit{e.g.}, 4K and
8K), achieving a balance between restoration quality and computational
efficiency has become increasingly critical. Existing methods, primarily based
on CNNs, Transformers, or their hybrid approaches, apply uniform deep
representation extraction across the image. However, these methods often
struggle to effectively model long-range dependencies and largely overlook the
spatial characteristics of image degradation (regions with richer textures tend
to suffer more severe damage), making it hard to achieve the best trade-off
between restoration quality and efficiency. To address these issues, we propose
a novel texture-aware image restoration method, TAMambaIR, which simultaneously
perceives image textures and achieves a trade-off between performance and
efficiency. Specifically, we introduce a novel Texture-Aware State Space Model,
which enhances texture awareness and improves efficiency by modulating the
transition matrix of the state-space equation and focusing on regions with
complex textures. Additionally, we design a {Multi-Directional Perception
Block} to improve multi-directional receptive fields while maintaining low
computational overhead. Extensive experiments on benchmarks for image
super-resolution, deraining, and low-light image enhancement demonstrate that
TAMambaIR achieves state-of-the-art performance with significantly improved
efficiency, establishing it as a robust and efficient framework for image
restoration.

</details>


### [275] [Traveling Waves Integrate Spatial Information Through Time](https://arxiv.org/pdf/2502.06034)
*Mozes Jacobs, Roberto C. Budzinski, Lyle Muller, Demba Ba, T. Anderson Keller*

Main category: cs.CV

TL;DR: Traveling waves in neural networks enhance spatial integration, outperforming local models in tasks requiring global context.


<details>
  <summary>Details</summary>
Motivation: To explore how traveling waves can transfer and integrate spatial information in neural networks, inspired by wave dynamics in physics.

Method: Introduces convolutional recurrent neural networks that generate traveling waves for spatial integration, treating wave-like activations as visual representations.

Result: Models with traveling waves outperform local feed-forward networks and rival non-local U-Net models in global spatial tasks, with fewer parameters.

Conclusion: Traveling waves offer efficiency, training stability, and a framework for linking artificial networks to biological neural activity.

Abstract: Traveling waves of neural activity are widely observed in the brain, but
their precise computational function remains unclear. One prominent hypothesis
is that they enable the transfer and integration of spatial information across
neural populations. However, few computational models have explored how
traveling waves might be harnessed to perform such integrative processing.
Drawing inspiration from the famous "Can one hear the shape of a drum?" problem
-- which highlights how normal modes of wave dynamics encode geometric
information -- we investigate whether similar principles can be leveraged in
artificial neural networks. Specifically, we introduce convolutional recurrent
neural networks that learn to produce traveling waves in their hidden states in
response to visual stimuli, enabling spatial integration. By then treating
these wave-like activation sequences as visual representations themselves, we
obtain a powerful representational space that outperforms local feed-forward
networks on tasks requiring global spatial context. In particular, we observe
that traveling waves effectively expand the receptive field of locally
connected neurons, supporting long-range encoding and communication of
information. We demonstrate that models equipped with this mechanism solve
visual semantic segmentation tasks demanding global integration, significantly
outperforming local feed-forward models and rivaling non-local U-Net models
with fewer parameters. As a first step toward traveling-wave-based
communication and visual representation in artificial networks, our findings
suggest wave-dynamics may provide efficiency and training stability benefits,
while simultaneously offering a new framework for connecting models to
biological recordings of neural activity.

</details>


### [276] [ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models](https://arxiv.org/pdf/2502.19409)
*Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott*

Main category: cs.CV

TL;DR: ImageChain enhances MLLMs for sequential image reasoning by modeling visual sequences as multi-turn conversations, improving next-scene description tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with sequential image reasoning, treating images independently despite multi-image pre-training.

Method: ImageChain interleaves images with textual descriptions in a dialogue format to capture temporal dependencies, optimizing for next-scene description tasks.

Result: Achieves 3.7% to 19% improvement in SimRate and robust zero-shot performance across domains like comics and robotics.

Conclusion: Instruction-tuning in a multimodal, multi-turn conversation design bridges static image understanding and temporally-aware reasoning.

Abstract: Reasoning over sequences of images remains a challenge for multimodal large
language models (MLLMs). While recent models incorporate multi-image data
during pre-training, they still struggle to recognize sequential structures,
often treating images independently. This work introduces ImageChain, a
framework that enhances MLLMs with sequential reasoning capabilities over image
data by modeling visual sequences as a multi-turn conversation. In ImageChain,
images are interleaved with corresponding textual descriptions to form a
controlled dialogue that explicitly captures temporal dependencies and
narrative progression. Our method optimizes for the task of next-scene
description, where the model generates a context-aware description of an
upcoming scene based on preceding visual and textual cues. We demonstrate that
our approach improves performance on the next-scene description task --
achieving an average improvement from 3.7% to 19% in SimRate, a metric that
quantifies semantic similarity to human-annotated ground truths. Moreover,
ImageChain achieves robust zero-shot out-of-domain performance in applications
ranging from comics to robotics. Extensive experiments validate that
instruction-tuning in a multimodal, multi-turn conversation design is key to
bridging the gap between static image understanding and temporally-aware
reasoning.

</details>


### [277] [FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts](https://arxiv.org/pdf/2502.21059)
*Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He*

Main category: cs.CV

TL;DR: FC-Attack uses auto-generated flowcharts to jailbreak MLLMs, achieving high success rates (up to 96% via images). Defenses like AdaShield can mitigate the attack but reduce utility.


<details>
  <summary>Details</summary>
Motivation: MLLMs are vulnerable to multimodal jailbreak attacks, especially through visual prompts, posing safety risks.

Method: FC-Attack fine-tunes an LLM to generate step descriptions for harmful queries, converts them into flowcharts (vertical, horizontal, S-shaped), and combines them with benign text prompts.

Result: Achieves up to 96% attack success rate via images and 78% via videos. Font styles and steps impact performance.

Conclusion: FC-Attack demonstrates significant jailbreak potential; AdaShield offers partial defense but compromises utility.

Abstract: Multimodal Large Language Models (MLLMs) have become powerful and widely
adopted in some practical applications. However, recent research has revealed
their vulnerability to multimodal jailbreak attacks, whereby the model can be
induced to generate harmful content, leading to safety risks. Although most
MLLMs have undergone safety alignment, recent research shows that the visual
modality is still vulnerable to jailbreak attacks. In our work, we discover
that by using flowcharts with partially harmful information, MLLMs can be
induced to provide additional harmful details. Based on this, we propose a
jailbreak attack method based on auto-generated flowcharts, FC-Attack.
Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a
step-description generator based on benign datasets. The generator is then used
to produce step descriptions corresponding to a harmful query, which are
transformed into flowcharts in 3 different shapes (vertical, horizontal, and
S-shaped) as visual prompts. These flowcharts are then combined with a benign
textual prompt to execute the jailbreak attack on MLLMs. Our evaluations on
Advbench show that FC-Attack attains an attack success rate of up to 96% via
images and up to 78% via videos across multiple MLLMs. Additionally, we
investigate factors affecting the attack performance, including the number of
steps and the font styles in the flowcharts. We also find that FC-Attack can
improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the
font style. To mitigate the attack, we explore several defenses and find that
AdaShield can largely reduce the jailbreak performance but with the cost of
utility drop.

</details>


### [278] [Spatial Reasoning with Denoising Models](https://arxiv.org/pdf/2502.21075)
*Christopher Wewer, Bart Pogodzinski, Bernt Schiele, Jan Eric Lenssen*

Main category: cs.CV

TL;DR: SRMs use denoising generative models for reasoning over continuous variables, improving accuracy in complex tasks by addressing hallucination and optimizing generation order.


<details>
  <summary>Details</summary>
Motivation: Current generative models struggle with hallucination in complex spatial distributions, prompting the need for a framework to improve reasoning accuracy.

Method: SRMs infer continuous representations of unobserved variables using denoising generative models, with benchmark tasks to measure hallucination.

Result: SRMs improve reasoning task accuracy from <1% to >50% by predicting generation order and optimizing sampling strategies.

Conclusion: SRMs effectively address hallucination in generative models, demonstrating the importance of generation order and sequentialization.

Abstract: We introduce Spatial Reasoning Models (SRMs), a framework to perform
reasoning over sets of continuous variables via denoising generative models.
SRMs infer continuous representations on a set of unobserved variables, given
observations on observed variables. Current generative models on spatial
domains, such as diffusion and flow matching models, often collapse to
hallucination in case of complex distributions. To measure this, we introduce a
set of benchmark tasks that test the quality of complex reasoning in generative
models and can quantify hallucination. The SRM framework allows to report key
findings about importance of sequentialization in generation, the associated
order, as well as the sampling strategies during training. It demonstrates, for
the first time, that order of generation can successfully be predicted by the
denoising network itself. Using these findings, we can increase the accuracy of
specific reasoning tasks from <1% to >50%. Our project website provides
additional videos, code, and the benchmark datasets:
https://geometric-rl.mpi-inf.mpg.de/srm

</details>


### [279] [Question-Aware Gaussian Experts for Audio-Visual Question Answering](https://arxiv.org/pdf/2503.04459)
*Hongyeob Kim, Inyoung Jung, Dayoon Suh, Youjia Zhang, Sangmin Lee, Sungeun Hong*

Main category: cs.CV

TL;DR: QA-TIGER is a novel framework for AVQA that explicitly uses question information and models continuous temporal dynamics with Gaussian-based modeling and MoE, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing AVQA methods implicitly use question info and uniform frame sampling, missing key details. QA-TIGER addresses these limitations.

Method: Uses Gaussian-based modeling to focus on relevant frames, injects question info explicitly, and employs MoE for tailored temporal experts.

Result: Achieves state-of-the-art performance on multiple AVQA benchmarks.

Conclusion: QA-TIGER improves AVQA by better leveraging question info and temporal dynamics, setting a new benchmark.

Abstract: Audio-Visual Question Answering (AVQA) requires not only question-based
multimodal reasoning but also precise temporal grounding to capture subtle
dynamics for accurate prediction. However, existing methods mainly use question
information implicitly, limiting focus on question-specific details.
Furthermore, most studies rely on uniform frame sampling, which can miss key
question-relevant frames. Although recent Top-K frame selection methods aim to
address this, their discrete nature still overlooks fine-grained temporal
details. This paper proposes QA-TIGER, a novel framework that explicitly
incorporates question information and models continuous temporal dynamics. Our
key idea is to use Gaussian-based modeling to adaptively focus on both
consecutive and non-consecutive frames based on the question, while explicitly
injecting question information and applying progressive refinement. We leverage
a Mixture of Experts (MoE) to flexibly implement multiple Gaussian models,
activating temporal experts specifically tailored to the question. Extensive
experiments on multiple AVQA benchmarks show that QA-TIGER consistently
achieves state-of-the-art performance. Code is available at
https://aim-skku.github.io/QA-TIGER/

</details>


### [280] [AugGen: Synthetic Augmentation Can Improve Discriminative Models](https://arxiv.org/pdf/2503.11544)
*Parsa Rahimi, Damien Teney, Sebastien Marcel*

Main category: cs.CV

TL;DR: AugGen introduces a self-contained synthetic augmentation technique for face recognition, eliminating reliance on external datasets and outperforming existing methods with 1-12% performance gains.


<details>
  <summary>Details</summary>
Motivation: Address privacy and ethical challenges in face recognition by reducing dependency on large-scale datasets and external resources.

Method: AugGen uses a class-conditional generative model trained on the target dataset to generate synthetic data, avoiding external dependencies.

Result: Achieves 1-12% performance improvements across 8 benchmarks, surpassing real-data-only and synthetic-data approaches.

Conclusion: Synthetic data, when carefully integrated, enhances face recognition performance and mitigates privacy concerns.

Abstract: The increasing reliance on large-scale datasets in machine learning poses
significant privacy and ethical challenges, particularly in sensitive domains
such as face recognition (FR). Synthetic data generation offers a promising
alternative; however, most existing methods depend heavily on external datasets
or pre-trained models, increasing complexity and resource demands. In this
paper, we introduce AugGen, a self-contained synthetic augmentation technique.
AugGen strategically samples from a class-conditional generative model trained
exclusively on the target FR dataset, eliminating the need for external
resources. Evaluated across 8 FR benchmarks, including IJB-C and IJB-B, our
method achieves 1-12% performance improvements, outperforming models trained
solely on real data and surpassing state-of-the-art synthetic data generation
approaches, while using less real data. Notably, these gains often exceed those
from architectural modifications, underscoring the value of synthetic
augmentation in data-limited scenarios. Our findings demonstrate that carefully
integrated synthetic data can both mitigate privacy constraints and
substantially enhance discriminative performance in face recognition. Paper
website: https://parsa-ra.github.io/auggen/.

</details>


### [281] [ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation](https://arxiv.org/pdf/2503.12348)
*Mo Zhou, Jianwei Wang, Xuanmeng Zhang, Dylan Campbell, Kai Wang, Long Yuan, Wenjie Zhang, Xuemin Lin*

Main category: cs.CV

TL;DR: ProbDiffFlow is a training-free framework for single-frame optical flow estimation, addressing limitations of existing methods by generating diverse future frames and aggregating results into a probabilistic flow distribution.


<details>
  <summary>Details</summary>
Motivation: Traditional optical flow methods require consecutive frames, which are often unavailable. Single-frame approaches rely on labeled data and produce deterministic predictions, failing to capture motion uncertainty.

Method: ProbDiffFlow uses a diffusion-based model to generate diverse future frames, estimates motion from these using a pre-trained optical flow model, and aggregates results into a probabilistic flow distribution.

Result: Experiments show ProbDiffFlow outperforms existing methods in accuracy, diversity, and efficiency on synthetic and real-world datasets.

Conclusion: ProbDiffFlow provides a robust, training-free solution for single-frame optical flow estimation, capturing motion uncertainty and eliminating the need for task-specific training.

Abstract: This paper studies optical flow estimation, a critical task in motion
analysis with applications in autonomous navigation, action recognition, and
film production. Traditional optical flow methods require consecutive frames,
which are often unavailable due to limitations in data acquisition or
real-world scene disruptions. Thus, single-frame optical flow estimation is
emerging in the literature. However, existing single-frame approaches suffer
from two major limitations: (1) they rely on labeled training data, making them
task-specific, and (2) they produce deterministic predictions, failing to
capture motion uncertainty. To overcome these challenges, we propose
ProbDiffFlow, a training-free framework that estimates optical flow
distributions from a single image. Instead of directly predicting motion,
ProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates
diverse plausible future frames using a diffusion-based model, then estimates
motion from these synthesized samples using a pre-trained optical flow model,
and finally aggregates the results into a probabilistic flow distribution. This
design eliminates the need for task-specific training while capturing multiple
plausible motions. Experiments on both synthetic and real-world datasets
demonstrate that ProbDiffFlow achieves superior accuracy, diversity, and
efficiency, outperforming existing single-image and two-frame baselines.

</details>


### [282] [Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition](https://arxiv.org/pdf/2503.17132)
*Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot*

Main category: cs.CV

TL;DR: The paper introduces two SNN frameworks (TS-SNN and 3D-SNN) for event-based human action recognition, addressing long-term temporal processing limitations and outperforming existing methods on new and existing datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy-preserving human action recognition by leveraging the synergy between SNNs and event cameras, overcoming SNNs' limitations in processing long-term temporal data.

Method: Proposes TS-SNN (temporal segment-based) and 3D-SNN (3D convolutional) frameworks to extract and process long-term temporal information. Introduces a new dataset, FallingDetection-CeleX.

Result: The frameworks outperform state-of-the-art SNN methods on the new dataset and three other neuromorphic datasets, demonstrating improved handling of long-range temporal data.

Conclusion: The proposed SNN frameworks effectively address temporal processing challenges in event-based HAR, offering superior performance and enabling further research with the new dataset.

Abstract: This paper explores the promising interplay between spiking neural networks
(SNNs) and event-based cameras for privacy-preserving human action recognition
(HAR). The unique feature of event cameras in capturing only the outlines of
motion, combined with SNNs' proficiency in processing spatiotemporal data
through spikes, establishes a highly synergistic compatibility for event-based
HAR. Previous studies, however, have been limited by SNNs' ability to process
long-term temporal information, essential for precise HAR. In this paper, we
introduce two novel frameworks to address this: temporal segment-based SNN
(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The
\textit{TS-SNN} extracts long-term temporal information by dividing actions
into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements
with 3D components to facilitate the transmission of temporal information. To
promote further research in event-based HAR, we create a dataset,
\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V
event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive
experimental results show that our proposed frameworks surpass state-of-the-art
SNN methods on our newly collected dataset and three other neuromorphic
datasets, showcasing their effectiveness in handling long-range temporal
information for event-based HAR.

</details>


### [283] [TerraMind: Large-Scale Generative Multimodality for Earth Observation](https://arxiv.org/pdf/2504.11171)
*Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, Nicolas Longépé*

Main category: cs.CV

TL;DR: TerraMind is a multimodal foundation model for Earth observation, combining token-level and pixel-level data for cross-modal learning. It introduces 'Thinking-in-Modalities' (TiM) and achieves state-of-the-art performance in EO benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a versatile, any-to-any generative model for Earth observation that leverages dual-scale representations for improved performance and flexibility.

Method: Pretrained on nine geospatial modalities using a dual-scale early fusion approach, combining token-level and pixel-level data. Introduces TiM for generating artificial data during finetuning and inference.

Result: Achieves beyond state-of-the-art performance in benchmarks like PANGAEA and enables zero-shot and few-shot applications.

Conclusion: TerraMind is a groundbreaking model for EO, offering advanced capabilities and open-sourced resources for broader use.

Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation
model for Earth observation (EO). Unlike other multimodal models, TerraMind is
pretrained on dual-scale representations combining both token-level and
pixel-level data across modalities. On a token level, TerraMind encodes
high-level contextual information to learn cross-modal relationships, while on
a pixel level, TerraMind leverages fine-grained representations to capture
critical spatial nuances. We pretrained TerraMind on nine geospatial modalities
of a global, large-scale dataset. In this paper, we demonstrate that (i)
TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and
few-shot applications for Earth observation, (ii) TerraMind introduces
"Thinking-in-Modalities" (TiM) -- the capability of generating additional
artificial data during finetuning and inference to improve the model output --
and (iii) TerraMind achieves beyond state-of-the-art performance in
community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the
model weights, and our code are open-sourced under a permissive license.

</details>


### [284] [Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos](https://arxiv.org/pdf/2504.18756)
*Rezowan Shuvo, M S Mekala, Eyad Elyan*

Main category: cs.CV

TL;DR: Proposes MSBATN, a transformer-based model with hierarchical sliding window attention, to improve surgical action segmentation by addressing boundary ambiguity and variability in surgeon approaches.


<details>
  <summary>Details</summary>
Motivation: Variability in surgeon approaches and ambiguous action boundaries complicate surgical workflow analysis, necessitating better segmentation methods.

Method: Uses Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention and a unified loss function for action classification and boundary detection.

Result: Achieves state-of-the-art performance with superior F1 scores at 25% and 50% thresholds on three surgical datasets.

Conclusion: MSBATN effectively addresses challenges in surgical action segmentation, offering precise boundary detection and improved performance.

Abstract: Understanding actions within surgical workflows is critical for evaluating
post-operative outcomes and enhancing surgical training and efficiency.
Capturing and analyzing long sequences of actions in surgical settings is
challenging due to the inherent variability in individual surgeon approaches,
which are shaped by their expertise and preferences. This variability
complicates the identification and segmentation of distinct actions with
ambiguous boundary start and end points. The traditional models, such as
MS-TCN, which rely on large receptive fields, that causes over-segmentation, or
under-segmentation, where distinct actions are incorrectly aligned. To address
these challenges, we propose the Multi-Stage Boundary-Aware Transformer Network
(MSBATN) with hierarchical sliding window attention to improve action
segmentation. Our approach effectively manages the complexity of varying action
durations and subtle transitions by accurately identifying start and end action
boundaries in untrimmed surgical videos. MSBATN introduces a novel unified loss
function that optimises action classification and boundary detection as
interconnected tasks. Unlike conventional binary boundary detection methods,
our innovative boundary weighing mechanism leverages contextual information to
precisely identify action boundaries. Extensive experiments on three
challenging surgical datasets demonstrate that MSBATN achieves state-of-the-art
performance, with superior F1 scores at 25% and 50%. thresholds and competitive
results across other metrics.

</details>


### [285] [Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain](https://arxiv.org/pdf/2505.01267)
*Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang*

Main category: cs.CV

TL;DR: The paper proposes a frequency-domain adversarial purification method to better preserve image content and structure while removing perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based adversarial purification methods damage normal semantics due to lack of perturbation distribution information in the pixel domain.

Method: Decomposes images into amplitude and phase spectra, focusing on less damaged low-frequency components to preserve content. Replaces low-frequency amplitude and projects phase in the reverse process.

Result: The method outperforms current defense techniques by effectively eliminating perturbations while minimizing damage to the original image.

Conclusion: Frequency-domain purification preserves image integrity better than pixel-domain methods, offering superior adversarial defense.

Abstract: The diffusion-based adversarial purification methods attempt to drown
adversarial perturbations into a part of isotropic noise through the forward
process, and then recover the clean images through the reverse process. Due to
the lack of distribution information about adversarial perturbations in the
pixel domain, it is often unavoidable to damage normal semantics. We turn to
the frequency domain perspective, decomposing the image into amplitude spectrum
and phase spectrum. We find that for both spectra, the damage caused by
adversarial perturbations tends to increase monotonically with frequency. This
means that we can extract the content and structural information of the
original clean sample from the frequency components that are less damaged.
Meanwhile, theoretical analysis indicates that existing purification methods
indiscriminately damage all frequency components, leading to excessive damage
to the image. Therefore, we propose a purification method that can eliminate
adversarial perturbations while maximizing the preservation of the content and
structure of the original image. Specifically, at each time step during the
reverse process, for the amplitude spectrum, we replace the low-frequency
components of the estimated image's amplitude spectrum with the corresponding
parts of the adversarial image. For the phase spectrum, we project the phase of
the estimated image into a designated range of the adversarial image's phase
spectrum, focusing on the low frequencies. Empirical evidence from extensive
experiments demonstrates that our method significantly outperforms most current
defense methods.

</details>


### [286] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/pdf/2505.04088)
*Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He*

Main category: cs.CV

TL;DR: The paper proposes a Siamese Motion Mamba Tracker (SMMT) for TIR object tracking, addressing challenges like occlusion and motion blur with bidirectional state-space modeling and self-attention.


<details>
  <summary>Details</summary>
Motivation: TIR object tracking faces issues like occlusion and motion blur, degrading tracker performance.

Method: SMMT integrates bidirectional state-space modeling and self-attention, uses parameter-sharing, and introduces a motion edge-aware regression loss.

Result: SMMT outperforms on benchmarks like LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017.

Conclusion: SMMT achieves superior TIR tracking performance by effectively handling motion blur and occlusion.

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [287] [ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment](https://arxiv.org/pdf/2506.02459)
*Martin JJ. Bucher, Iro Armeni*

Main category: cs.CV

TL;DR: ReSpace is a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D indoor scene synthesis and editing oversimplify semantics, lack editing support, or ignore spatial reasoning. ReSpace addresses these gaps.

Method: Uses a compact structured scene representation with explicit room boundaries, dual-stage training (supervised fine-tuning and preference alignment), and zero-shot LLM for editing.

Result: Surpasses state-of-the-art on object addition and maintains competitive performance in full scene synthesis.

Conclusion: ReSpace offers a robust solution for text-driven 3D scene synthesis and editing, combining rich semantics, spatial reasoning, and user control.

Abstract: Scene synthesis and editing has emerged as a promising direction in computer
graphics. Current trained approaches for 3D indoor scenes either oversimplify
object semantics through one-hot class encodings (e.g., 'chair' or 'table'),
require masked diffusion for editing, ignore room boundaries, or rely on floor
plan renderings that fail to capture complex layouts. In contrast, LLM-based
methods enable richer semantics via natural language (e.g., 'modern studio with
light wood furniture') but do not support editing, remain limited to
rectangular layouts or rely on weak spatial reasoning from implicit world
models. We introduce ReSpace, a generative framework for text-driven 3D indoor
scene synthesis and editing using autoregressive language models. Our approach
features a compact structured scene representation with explicit room
boundaries that frames scene editing as a next-token prediction task. We
leverage a dual-stage training approach combining supervised fine-tuning and
preference alignment, enabling a specially trained language model for object
addition that accounts for user instructions, spatial geometry, object
semantics, and scene-level composition. For scene editing, we employ a
zero-shot LLM to handle object removal and prompts for addition. We further
introduce a novel voxelization-based evaluation that captures fine-grained
geometry beyond 3D bounding boxes. Experimental results surpass
state-of-the-art on object addition while maintaining competitive results on
full scene synthesis.

</details>


### [288] [HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation](https://arxiv.org/pdf/2506.02472)
*Halil Ismail Helvaci, Justin Philip Huber, Jihye Bae, Sen-ching Samson Cheung*

Main category: cs.CV

TL;DR: The paper introduces HRTR, a transformer-based model for fine-grained, sub-second action detection in stroke rehabilitation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Stroke rehabilitation requires precise tracking of patient movements, but existing methods struggle with fine-grained, sub-second action detection.

Method: Proposes HRTR, a single-stage transformer model for high-resolution, sub-second action localization and classification, eliminating multi-stage methods.

Result: HRTR achieves superior performance: ES of 70.1 on StrokeRehab Video, 69.4 on StrokeRehab IMU, and 88.4 on 50Salads.

Conclusion: HRTR is effective for fine-grained, sub-second action detection in stroke rehabilitation and general datasets.

Abstract: Stroke rehabilitation often demands precise tracking of patient movements to
monitor progress, with complexities of rehabilitation exercises presenting two
critical challenges: fine-grained and sub-second (under one-second) action
detection. In this work, we propose the High Resolution Temporal Transformer
(HRTR), to time-localize and classify high-resolution (fine-grained),
sub-second actions in a single-stage transformer, eliminating the need for
multi-stage methods and post-processing. Without any refinements, HRTR
outperforms state-of-the-art systems on both stroke related and general
datasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on
StrokeRehab IMU, and 88.4 on 50Salads.

</details>


### [289] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/pdf/2506.02550)
*Qiaohui Chu, Haoyu Zhang, Yisen Feng, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie*

Main category: cs.CV

TL;DR: A three-stage framework for the Ego4D LTA task, combining feature extraction, action recognition, and LLM-based anticipation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To advance long-term action anticipation by leveraging foundation models and improving recognition accuracy.

Method: Three stages: feature extraction (visual encoder), action recognition (Transformer with verb-noun co-occurrence), and anticipation (fine-tuned LLM).

Result: Achieved first place in the CVPR 2025 challenge, setting a new benchmark.

Conclusion: The framework effectively combines visual and textual models for superior long-term action prediction.

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [290] [ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions](https://arxiv.org/pdf/2506.03107)
*Di Chang, Mingdeng Cao, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, Peng Wang*

Main category: cs.CV

TL;DR: ByteMorph introduces a framework for instruction-based image editing focusing on non-rigid motions, featuring a large dataset (ByteMorph-6M) and a baseline model (ByteMorpher).


<details>
  <summary>Details</summary>
Motivation: Existing methods and datasets are limited to static scenes or rigid transformations, leaving expressive edits involving dynamic motion underexplored.

Method: ByteMorph includes a dataset (ByteMorph-6M) with 6M image pairs and a baseline model (ByteMorpher) based on Diffusion Transformer (DiT). Data generation uses motion guidance, compositing, and automated captioning.

Result: ByteMorph-6M and ByteMorpher address diverse non-rigid motions, with a curated evaluation benchmark (ByteMorph-Bench).

Conclusion: ByteMorph fills a gap in handling dynamic motion edits, offering a robust dataset and model for future research.

Abstract: Editing images with instructions to reflect non-rigid motions, camera
viewpoint shifts, object deformations, human articulations, and complex
interactions, poses a challenging yet underexplored problem in computer vision.
Existing approaches and datasets predominantly focus on static scenes or rigid
transformations, limiting their capacity to handle expressive edits involving
dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive
framework for instruction-based image editing with an emphasis on non-rigid
motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong
baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.
ByteMorph-6M includes over 6 million high-resolution image editing pairs for
training, along with a carefully curated evaluation benchmark ByteMorph-Bench.
Both capture a wide variety of non-rigid motion types across diverse
environments, human figures, and object categories. The dataset is constructed
using motion-guided data generation, layered compositing techniques, and
automated captioning to ensure diversity, realism, and semantic coherence. We
further conduct a comprehensive evaluation of recent instruction-based image
editing methods from both academic and commercial domains.

</details>


### [291] [Zero-Shot Temporal Interaction Localization for Egocentric Videos](https://arxiv.org/pdf/2506.03662)
*Erhang Zhang, Junyi Ma, Yin-Dong Zheng, Yixuan Zhou, Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot method for temporal interaction localization (TIL) in egocentric videos, improving accuracy and efficiency with self-adaptive sampling and closed-loop feedback.


<details>
  <summary>Details</summary>
Motivation: Current methods for HOI action localization suffer from domain bias and inefficiency, while existing zero-shot approaches lack precision.

Method: EgoLoc uses a self-adaptive sampling strategy with 2D/3D observations and closed-loop feedback for refinement.

Result: Outperforms state-of-the-art baselines in temporal interaction localization.

Conclusion: EgoLoc offers a robust solution for TIL in egocentric videos, with plans to release code and data.

Abstract: Locating human-object interaction (HOI) actions within video serves as the
foundation for multiple downstream tasks, such as human behavior analysis and
human-robot skill transfer. Current temporal action localization methods
typically rely on annotated action and object categories of interactions for
optimization, which leads to domain bias and low deployment efficiency.
Although some recent works have achieved zero-shot temporal action localization
(ZS-TAL) with large vision-language models (VLMs), their coarse-grained
estimations and open-loop pipelines hinder further performance improvements for
temporal interaction localization (TIL). To address these issues, we propose a
novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp
actions for human-object interaction in egocentric videos. EgoLoc introduces a
self-adaptive sampling strategy to generate reasonable visual prompts for VLM
reasoning. By absorbing both 2D and 3D observations, it directly samples
high-quality initial guesses around the possible contact/separation timestamps
of HOI according to 3D hand velocities, leading to high inference accuracy and
efficiency. In addition, EgoLoc generates closed-loop feedback from visual and
dynamic cues to further refine the localization results. Comprehensive
experiments on the publicly available dataset and our newly proposed benchmark
demonstrate that EgoLoc achieves better temporal interaction localization for
egocentric videos compared to state-of-the-art baselines. We will release our
code and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.

</details>


### [292] [Is Perturbation-Based Image Protection Disruptive to Image Editing?](https://arxiv.org/pdf/2506.04394)
*Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati*

Main category: cs.CV

TL;DR: Current perturbation-based image protection methods fail to fully prevent diffusion-based editing, often yielding desirable outputs instead of noisy images.


<details>
  <summary>Details</summary>
Motivation: To address the misuse of diffusion models for spreading misinformation and plagiarizing copyrighted materials by evaluating the effectiveness of perturbation-based protection methods.

Method: Experiments with various perturbation-based protection methods across multiple domains (natural scenes, artworks) and editing tasks (image-to-image generation, style editing).

Result: Perturbation-based protection often fails, as diffusion-based editing still produces desirable outputs adhering to prompts, sometimes even improving edits.

Conclusion: Perturbation-based methods are insufficient for robust image protection against diffusion-based editing, as they may unintentionally enhance edits.

Abstract: The remarkable image generation capabilities of state-of-the-art diffusion
models, such as Stable Diffusion, can also be misused to spread misinformation
and plagiarize copyrighted materials. To mitigate the potential risks
associated with image editing, current image protection methods rely on adding
imperceptible perturbations to images to obstruct diffusion-based editing. A
fully successful protection for an image implies that the output of editing
attempts is an undesirable, noisy image which is completely unrelated to the
reference image. In our experiments with various perturbation-based image
protection methods across multiple domains (natural scene images and artworks)
and editing tasks (image-to-image generation and style editing), we discover
that such protection does not achieve this goal completely. In most scenarios,
diffusion-based editing of protected images generates a desirable output image
which adheres precisely to the guidance prompt. Our findings suggest that
adding noise to images may paradoxically increase their association with given
text prompts during the generation process, leading to unintended consequences
such as better resultant edits. Hence, we argue that perturbation-based methods
may not provide a sufficient solution for robust image protection against
diffusion-based editing.

</details>


### [293] [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/pdf/2506.04704)
*Youngwan Lee, Kangsan Kim, Kwanyong Park, Ilcahe Jung, Soojin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang*

Main category: cs.CV

TL;DR: The paper introduces HoliSafe, a comprehensive safety dataset and benchmark for Vision-Language Models (VLMs), and SafeLLaVA, a novel VLM with safety enhancements like a learnable meta token and safety head. These innovations improve safety performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current VLM safety approaches are limited by partial coverage of harmful content and reliance on data-centric tuning, leaving models vulnerable to unseen threats.

Method: Proposes HoliSafe, a dataset covering all safe/unsafe image-text combinations, and SafeLLaVA, a VLM with a safety meta token and dedicated safety head for intrinsic safety.

Result: SafeLLaVA achieves state-of-the-art safety performance, and HoliSafe exposes vulnerabilities in existing models.

Conclusion: HoliSafe and SafeLLaVA advance robust and interpretable VLM safety, encouraging further research in multimodal alignment.

Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models
(VLMs), current approaches face two main shortcomings. 1) Existing
safety-tuning datasets and benchmarks only partially consider how image-text
interactions can yield harmful content, often overlooking contextually unsafe
outcomes from seemingly benign pairs. This narrow coverage leaves VLMs
vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely
primarily on data-centric tuning, with limited architectural innovations to
intrinsically strengthen safety. We address these gaps by introducing a
holistic safety dataset and benchmark, HoliSafe, that spans all five
safe/unsafe image-text combinations, providing a more robust basis for both
training and evaluation. We further propose SafeLLaVA, a novel VLM augmented
with a learnable safety meta token and a dedicated safety head. The meta token
encodes harmful visual cues during training, intrinsically guiding the language
model toward safer responses, while the safety head offers interpretable
harmfulness classification aligned with refusal rationales. Experiments show
that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety
performance across multiple VLM benchmarks. Additionally, the HoliSafe
benchmark itself reveals critical vulnerabilities in existing models. We hope
that HoliSafe and SafeLLaVA will spur further research into robust and
interpretable VLM safety, expanding future avenues for multimodal alignment.

</details>


### [294] [ContentV: Efficient Training of Video Generation Models with Limited Compute](https://arxiv.org/pdf/2506.05343)
*Wenfeng Lin, Renjie Chen, Boyuan Liu, Shiyue Yan, Ruoyu Feng, Jiangchuan Wei, Yichen Zhang, Yimeng Zhou, Chao Feng, Jiao Ran, Qi Wu, Zuotao Liu, Mingyu Guo*

Main category: cs.CV

TL;DR: ContentV is an 8B-parameter text-to-video model achieving SOTA performance with efficient training, leveraging minimalist architecture, multi-stage training, and cost-effective RLHF.


<details>
  <summary>Details</summary>
Motivation: To address the escalating computational costs in video generation by developing an efficient training recipe.

Method: Uses a minimalist architecture, multi-stage training with flow matching, and reinforcement learning with human feedback (RLHF).

Result: Achieves state-of-the-art performance (85.14 on VBench) with diverse, high-quality video generation.

Conclusion: ContentV demonstrates efficient, high-quality video generation from text prompts, with open-sourced code and models.

Abstract: Recent advances in video generation demand increasingly efficient training
recipes to mitigate escalating computational costs. In this report, we present
ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art
performance (85.14 on VBench) after training on 256 x 64GB Neural Processing
Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality
videos across multiple resolutions and durations from text prompts, enabled by
three key innovations: (1) A minimalist architecture that maximizes reuse of
pre-trained image generation models for video generation; (2) A systematic
multi-stage training strategy leveraging flow matching for enhanced efficiency;
and (3) A cost-effective reinforcement learning with human feedback framework
that improves generation quality without requiring additional human
annotations. All the code and models are available at:
https://contentv.github.io.

</details>


### [295] [Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?](https://arxiv.org/pdf/2506.05765)
*Taiga Shinozaki, Tomoki Doi, Amane Watahiki, Satoshi Nishida, Hitomi Yanaka*

Main category: cs.CV

TL;DR: The paper investigates whether large vision language models (LVLMs) can discern genuine vs. fake visual illusions, revealing they rely on prior knowledge rather than genuine visual understanding.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in assessing machine cognition of visual illusions by distinguishing genuine (actual vs. apparent feature discrepancies) and fake illusions (no discrepancies).

Method: Introduces a VQA dataset with genuine and fake illusions, evaluates LVLMs' performance on discerning actual vs. apparent features.

Result: LVLMs predict similar answers for both illusion types, suggesting reliance on prior knowledge, not visual understanding.

Conclusion: LVLMs lack genuine visual understanding of illusions, highlighting limitations in their cognitive capabilities.

Abstract: Humans are susceptible to optical illusions, which serve as valuable tools
for investigating sensory and cognitive processes. Inspired by human vision
studies, research has begun exploring whether machines, such as large vision
language models (LVLMs), exhibit similar susceptibilities to visual illusions.
However, studies often have used non-abstract images and have not distinguished
actual and apparent features, leading to ambiguous assessments of machine
cognition. To address these limitations, we introduce a visual question
answering (VQA) dataset, categorized into genuine and fake illusions, along
with corresponding control images. Genuine illusions present discrepancies
between actual and apparent features, whereas fake illusions have the same
actual and apparent features even though they look illusory due to the similar
geometric configuration. We evaluate the performance of LVLMs for genuine and
fake illusion VQA tasks and investigate whether the models discern actual and
apparent features. Our findings indicate that although LVLMs may appear to
recognize illusions by correctly answering questions about both feature types,
they predict the same answers for both Genuine Illusion and Fake Illusion VQA
questions. This suggests that their responses might be based on prior knowledge
of illusions rather than genuine visual understanding. The dataset is available
at https://github.com/ynklab/FILM

</details>


### [296] [MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks](https://arxiv.org/pdf/2506.05982)
*Zonglin Wu, Yule Xue, Xin Wei, Yiren Song*

Main category: cs.CV

TL;DR: The paper introduces MCA-Bench, a unified benchmarking suite for evaluating the security of diverse CAPTCHA types, revealing vulnerabilities and proposing design principles for improvement.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified, large-scale, multimodal benchmark for CAPTCHA security evaluation motivates the creation of MCA-Bench.

Method: MCA-Bench integrates various CAPTCHA types into a single protocol, using a shared vision-language model to fine-tune specialized cracking agents for each category.

Result: Experiments show MCA-Bench effectively maps CAPTCHA vulnerabilities and provides the first quantitative analysis of challenge complexity, interaction depth, and solvability.

Conclusion: The paper proposes actionable design principles for CAPTCHA hardening and identifies open challenges, encouraging community collaboration.

Abstract: As automated attack techniques rapidly advance, CAPTCHAs remain a critical
defense mechanism against malicious bots. However, existing CAPTCHA schemes
encompass a diverse range of modalities -- from static distorted text and
obfuscated images to interactive clicks, sliding puzzles, and logic-based
questions -- yet the community still lacks a unified, large-scale, multimodal
benchmark to rigorously evaluate their security robustness. To address this
gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking
suite that integrates heterogeneous CAPTCHA types into a single evaluation
protocol. Leveraging a shared vision-language model backbone, we fine-tune
specialized cracking agents for each CAPTCHA category, enabling consistent,
cross-modal assessments. Extensive experiments reveal that MCA-Bench
effectively maps the vulnerability spectrum of modern CAPTCHA designs under
varied attack settings, and crucially offers the first quantitative analysis of
how challenge complexity, interaction depth, and model solvability interrelate.
Based on these findings, we propose three actionable design principles and
identify key open challenges, laying the groundwork for systematic CAPTCHA
hardening, fair benchmarking, and broader community collaboration. Datasets and
code are available online.

</details>


### [297] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/pdf/2506.06733)
*Ruoxuan Zhang, Jidong Gao, Bin Wen, Hongxia Xie, Chenming Zhang, Hong-Han Shuai, Wen-Huang Cheng*

Main category: cs.CV

TL;DR: RecipeGen is a large-scale benchmark for recipe-based text-to-image, image-to-video, and text-to-video generation, addressing the lack of fine-grained alignment in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack alignment between recipe goals, instructions, and visuals, hindering applications in food computing.

Method: RecipeGen introduces 26,453 recipes with 196,724 images and 4,491 videos, along with domain-specific metrics for evaluation.

Result: The benchmark evaluates T2I, I2V, and T2V models, providing insights for future recipe generation models.

Conclusion: RecipeGen fills a gap in food computing datasets and offers a foundation for future research in recipe-based generation tasks.

Abstract: Creating recipe images is a key challenge in food computing, with
applications in culinary education and multimodal recipe assistants. However,
existing datasets lack fine-grained alignment between recipe goals, step-wise
instructions, and visual content. We present RecipeGen, the first large-scale,
real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video
(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,
196,724 images, and 4,491 videos, covering diverse ingredients, cooking
procedures, styles, and dish types. We further propose domain-specific
evaluation metrics to assess ingredient fidelity and interaction modeling,
benchmark representative T2I, I2V, and T2V models, and provide insights for
future recipe generation models. Project page is available now.

</details>


### [298] [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/pdf/2506.07497)
*Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang*

Main category: cs.CV

TL;DR: Genesis is a framework for generating multi-view driving videos and LiDAR sequences with consistency, using a two-stage architecture and shared latent space. It achieves top performance on benchmarks and aids downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To create a unified solution for generating consistent multi-view driving videos and LiDAR sequences, addressing the need for coherent multi-modal data in autonomous driving research.

Method: Uses a two-stage architecture: a DiT-based video diffusion model with 3D-VAE encoding and a BEV-aware LiDAR generator with NeRF-based rendering. Integrates modalities via a shared latent space and employs DataCrafter for semantic guidance.

Result: Achieves state-of-the-art performance on nuScenes (FVD 16.95, FID 4.24, Chamfer 0.611) and improves downstream tasks like segmentation and 3D detection.

Conclusion: Genesis effectively generates high-quality, semantically consistent multi-modal data, proving its utility for autonomous driving applications.

Abstract: We present Genesis, a unified framework for joint generation of multi-view
driving videos and LiDAR sequences with spatio-temporal and cross-modal
consistency. Genesis employs a two-stage architecture that integrates a
DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR
generator with NeRF-based rendering and adaptive sampling. Both modalities are
directly coupled through a shared latent space, enabling coherent evolution
across visual and geometric domains. To guide the generation with structured
semantics, we introduce DataCrafter, a captioning module built on
vision-language models that provides scene-level and instance-level
supervision. Extensive experiments on the nuScenes benchmark demonstrate that
Genesis achieves state-of-the-art performance across video and LiDAR metrics
(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including
segmentation and 3D detection, validating the semantic fidelity and practical
utility of the generated data.

</details>


### [299] [SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding](https://arxiv.org/pdf/2506.07737)
*Xuemei Chen, Huamin Wang, Hangchi Shen, Shukai Duan, Shiping Wen, Tingwen Huang*

Main category: cs.CV

TL;DR: The paper proposes SpikeSMOKE, a low-power 3D object detection architecture using spiking neural networks (SNNs), enhanced by a cross-scale gated coding mechanism (CSGC) and a lightweight residual block. It achieves competitive performance on the KITTI dataset while significantly reducing energy consumption.


<details>
  <summary>Details</summary>
Motivation: Addressing the high energy consumption in 3D object detection, especially in applications like autonomous driving, by leveraging the low-power characteristics of SNNs.

Method: Introduces SpikeSMOKE with CSGC for feature enhancement and a lightweight residual block to reduce computation.

Result: Improved detection performance (e.g., +2.82 AP on Easy category) and reduced energy consumption (e.g., 72.2% reduction in hard category) compared to baseline.

Conclusion: SpikeSMOKE offers an efficient, low-power solution for monocular 3D object detection with minimal performance trade-offs.

Abstract: Low energy consumption for 3D object detection is an important research area
because of the increasing energy consumption with their wide application in
fields such as autonomous driving. The spiking neural networks (SNNs) with
low-power consumption characteristics can provide a novel solution for this
research. Therefore, we apply SNNs to monocular 3D object detection and propose
the SpikeSMOKE architecture in this paper, which is a new attempt for low-power
monocular 3D object detection. As we all know, discrete signals of SNNs will
generate information loss and limit their feature expression ability compared
with the artificial neural networks (ANNs).In order to address this issue,
inspired by the filtering mechanism of biological neuronal synapses, we propose
a cross-scale gated coding mechanism(CSGC), which can enhance feature
representation by combining cross-scale fusion of attentional methods and gated
filtering mechanisms.In addition, to reduce the computation and increase the
speed of training, we present a novel light-weight residual block that can
maintain spiking computing paradigm and the highest possible detection
performance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,
the proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,
Moderate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by
AP|R11 at 0.7 IoU threshold, respectively. It is important to note that the
results of SpikeSMOKE can significantly reduce energy consumption compared to
the results on SMOKE. For example,the energy consumption can be reduced by
72.2% on the hard category, while the detection performance is reduced by only
4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3
times and computation by 10 times compared to SMOKE.

</details>


### [300] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/pdf/2506.07943)
*Yizhen Li, Dell Zhang, Xuelong Li, Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger introduces a two-stage RS approach using Digital Twin (DT) representation to decouple perception from reasoning, achieving state-of-the-art performance by leveraging LLMs for explicit reasoning.


<details>
  <summary>Details</summary>
Motivation: Current RS methods disrupt spatial relationships due to tokenization; DTwinSeger aims to preserve these relationships and improve reasoning.

Method: Transform images into structured DT representations, then use LLMs for reasoning. Includes supervised fine-tuning and dataset Seg-DT.

Result: Achieves top performance on RS and referring segmentation benchmarks.

Conclusion: DT representation effectively bridges vision and text, enabling complex reasoning with LLMs alone.

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [301] [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](https://arxiv.org/pdf/2506.07986)
*Zhengyao Lv, Tianlin Pan, Chenyang Si, Zhaoxi Chen, Wangmeng Zuo, Ziwei Liu, Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: The paper proposes Temperature-Adjusted Cross-modal Attention (TACA) to improve text-image alignment in MM-DiT models by addressing token imbalance and timestep-aware attention issues.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art MM-DiT models struggle with precise alignment between text prompts and generated content due to cross-modal attention suppression and lack of timestep-aware weighting.

Method: Introduces TACA, a parameter-efficient method using temperature scaling and timestep-dependent adjustment, combined with LoRA fine-tuning.

Result: TACA significantly enhances text-image alignment on T2I-CompBench, improving object appearance, attribute binding, and spatial relationships in models like FLUX and SD3.5.

Conclusion: Balancing cross-modal attention is crucial for semantic fidelity in text-to-image diffusion models, and TACA offers a computationally efficient solution.

Abstract: Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress
in text-driven visual generation. However, even state-of-the-art MM-DiT models
like FLUX struggle with achieving precise alignment between text prompts and
generated content. We identify two key issues in the attention mechanism of
MM-DiT, namely 1) the suppression of cross-modal attention due to token
imbalance between visual and textual modalities and 2) the lack of
timestep-aware attention weighting, which hinder the alignment. To address
these issues, we propose \textbf{Temperature-Adjusted Cross-modal Attention
(TACA)}, a parameter-efficient method that dynamically rebalances multimodal
interactions through temperature scaling and timestep-dependent adjustment.
When combined with LoRA fine-tuning, TACA significantly enhances text-image
alignment on the T2I-CompBench benchmark with minimal computational overhead.
We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating
its ability to improve image-text alignment in terms of object appearance,
attribute binding, and spatial relationships. Our findings highlight the
importance of balancing cross-modal attention in improving semantic fidelity in
text-to-image diffusion models. Our codes are publicly available at
\href{https://github.com/Vchitect/TACA}

</details>


### [302] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/pdf/2506.08010)
*Nick Jiang, Amil Dravid, Alexei Efros, Yossi Gandelsman*

Main category: cs.CV

TL;DR: The paper identifies and addresses high-norm tokens in Vision Transformers that cause noisy attention maps, proposing a training-free method to mitigate these artifacts by shifting activations to an untrained token.


<details>
  <summary>Details</summary>
Motivation: To understand and resolve the issue of high-norm tokens degrading attention maps and downstream performance in Vision Transformers without requiring retraining.

Method: Shifts high-norm activations from identified neurons to an untrained token, mimicking the effect of register tokens without retraining.

Result: Produces cleaner attention maps, improves downstream task performance, and matches models trained with register tokens.

Conclusion: Test-time registers offer a training-free solution to improve pre-trained models, enhancing interpretability and performance.

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


### [303] [Toward Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/pdf/2506.08048)
*Zheng Han, Jun Zhou, Jialun Pei, Jing Qin, Yingfang Fan, Qi Dou*

Main category: cs.CV

TL;DR: A data-driven biomechanics algorithm is proposed for AR-guided surgical navigation, combining FEM-level accuracy with computational efficiency and a human-in-the-loop mechanism for surgeon interaction.


<details>
  <summary>Details</summary>
Motivation: Existing FEM-based methods are computationally expensive and struggle with large anatomical changes, compromising AR guidance accuracy.

Method: A data-driven biomechanics algorithm with a human-in-the-loop mechanism allows surgeons to correct misalignments interactively.

Result: Achieves a mean target registration error of 3.42 mm, reduced to 2.78 mm with surgeon prompts, outperforming state-of-the-art methods.

Conclusion: The framework enhances accuracy, efficiency, and surgeon-algorithm collaboration, improving reliability in computer-assisted surgeries.

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [304] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/pdf/2506.08137)
*Oishee Bintey Hoque, Abhijin Adiga, Aniruddha Adiga, Siddharth Chaudhary, Madhav V. Marathe, S. S. Ravi, Kirti Rajagopalan, Amanda Wilson, Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS is a novel framework combining semantic segmentation and graph-based refinement to improve canal and road network mapping from noisy ground truth.


<details>
  <summary>Details</summary>
Motivation: Accurate canal and road network mapping is crucial for water and infrastructure management, but incomplete ground truth hinders learning approaches.

Method: IGraSS integrates a semantic segmentation module (using RGB, NDWI, DEM) with a graph-based refinement module to improve ground truth by leveraging graph-level properties like reachability and connectivity.

Result: IGraSS reduces unreachable canal segments from ~18% to 3% and enhances canal identification. It also generalizes to road networks with different constraints.

Conclusion: IGraSS is effective for refining noisy ground truth and mapping infrastructure networks, demonstrating robustness and generalizability.

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [305] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/pdf/2506.08194)
*Mateusz Michalkiewicz, Anekha Sokhal, Tadeusz Michalkiewicz, Piotr Pawlikowski, Mahsa Baktashmotlagh, Varun Jampani, Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ is a benchmark evaluating geometric reasoning in vision and vision-language models, revealing significant gaps in current models despite their success on standard tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the true geometric understanding of vision and vision-language models, which remains unclear despite their strong performance on benchmarks.

Method: Introduces GIQ, a benchmark with synthetic and real-world images of 224 diverse polyhedra, tested via monocular 3D reconstruction, symmetry detection, mental rotation, and zero-shot classification.

Result: Current models struggle with basic geometric forms, symmetry detection, and detailed differentiation, while vision-language assistants perform poorly on complex polyhedra.

Conclusion: GIQ highlights critical gaps in geometric intelligence, offering a platform to improve geometry-aware representation learning.

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [306] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/pdf/2506.08324)
*Guandong Li, Mengxia Ye*

Main category: cs.CV

TL;DR: STNet, a novel network architecture with a Spatial-Spectral Transformer module, improves hyperspectral image classification by decoupling spatial and spectral attention and using gating mechanisms, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges like high-dimensional data, sparse ground objects, and spectral redundancy in hyperspectral image classification lead to overfitting and poor generalization. STNet aims to better extract and fuse spatial-spectral information.

Method: STNet uses a dual-design Spatial-Spectral Transformer module: decoupling spatial and spectral attention, and employing gating mechanisms (adaptive attention fusion gating and GFFN) for intelligent regulation.

Result: STNet outperforms mainstream methods on IN, UP, and KSC datasets, enhancing feature extraction and reducing overfitting without increasing network size.

Conclusion: STNet effectively addresses hyperspectral classification challenges, offering superior performance and generalization with innovative attention and gating designs.

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [307] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/pdf/2506.08356)
*Shivang Chopra, Gabriela Sanchez-Rodriguez, Lingchao Mao, Andrew J Feola, Jing Li, Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE is a vision-language framework that dynamically adapts visual representation for medical imaging by using modality-specific experts, improving alignment and retrieval across diverse modalities.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks use uniform feature extraction, ignoring modality-specific needs in medical imaging. MedMoE addresses this gap.

Method: Uses a Mixture-of-Experts (MoE) module with specialized branches for multi-scale feature extraction, based on a Swin Transformer backbone.

Result: Improves alignment and retrieval performance across medical imaging benchmarks without modality-specific supervision.

Conclusion: Modality-specialized visual representations enhance clinical vision-language systems.

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [308] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/pdf/2506.08650)
*Peter Grönquist, Stepan Tulyakov, Dengxin Dai*

Main category: cs.CV

TL;DR: The paper introduces the Neural Physical Model (NPM) for consistent color reproduction across cameras, outperforming existing methods in adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Ensuring consistent color reproduction across multiple cameras is challenging due to sensor and optics variations, and existing methods have limitations like poor adaptability or high computational costs.

Method: The Neural Physical Model (NPM) is a lightweight, physically-informed approach that simulates raw images under specified illumination to estimate transformations between devices. It adapts to varying illumination and supports training with or without paired data.

Result: Experiments on datasets like NUS and BeyondRGB show NPM outperforms state-of-the-art methods, achieving robust chromatic consistency across sensors and optical systems.

Conclusion: NPM provides a practical and effective solution for raw-to-raw conversion, addressing key challenges in color consistency for modern devices.

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [309] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/pdf/2506.08729)
*Dieuwertje Alblas, Patryk Rygiel, Julian Suk, Kaj O. Kappe, Marieke Hofman, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper proposes an SE(3)-symmetric transformer model to predict AAA growth using 3D vascular surface data, improving personalized monitoring over current diameter-based methods.


<details>
  <summary>Details</summary>
Motivation: Current AAA monitoring relies on diameter thresholds, ignoring 3D shape complexities. Personalized growth predictions could enhance surveillance.

Method: An SE(3)-symmetric transformer model predicts AAA growth using local, multi-physical features on vascular surfaces, trained on 113 CTAs from 24 patients.

Result: The model achieves a median diameter error of 1.18 mm in growth prediction and 93% accuracy in identifying patients needing repair within two years.

Conclusion: Local directional AAA growth prediction from vascular surfaces is feasible and may improve personalized surveillance strategies.

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [310] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2506.08772)
*Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng*

Main category: cs.CV

TL;DR: The paper introduces RS-MTDF, a semi-supervised semantic segmentation framework for remote sensing, leveraging Vision Foundation Models (VFMs) to bridge distribution gaps and improve performance.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in remote sensing requires costly annotations. Semi-supervised methods struggle with distribution mismatches between labeled and unlabeled data.

Method: RS-MTDF uses multiple frozen VFMs (e.g., DINOv2, CLIP) as teachers for feature-level distillation, fusing their knowledge into the student decoder.

Result: RS-MTDF achieves state-of-the-art performance on three datasets, outperforming others in label ratios and IoU scores.

Conclusion: Multi-teacher VFM guidance enhances generalization and semantic understanding in remote sensing segmentation, validated by ablation studies.

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
To alleviate this issue, we attempt to introduce the Vision Foundation Models
(VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs
possess robust generalization capabilities that can effectively bridge this
distribution gap and provide strong semantic priors for SSS. Inspired by this,
we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework
that leverages the powerful semantic knowledge embedded in VFMs to guide
semi-supervised learning in remote sensing. Specifically, RS-MTDF employs
multiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing
feature-level distillation to align student features with their robust
representations. To further enhance discriminative power, the distilled
knowledge is seamlessly fused into the student decoder. Extensive experiments
on three challenging remote sensing datasets demonstrate that RS-MTDF
consistently achieves state-of-the-art performance. Notably, our method
outperforms existing approaches across various label ratios on LoveDA and
secures the highest IoU in the majority of semantic categories. These results
underscore the efficacy of multi-teacher VFM guidance in significantly
enhancing both generalization and semantic understanding for remote sensing
segmentation. Ablation studies further validate the contribution of each
proposed module.

</details>


### [311] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/pdf/2506.08777)
*Keyi Liu, Weidong Yang, Ben Fei, Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene is a novel SSL framework using 3D Gaussian Splatting for efficient and explicit 3D scene pre-training, outperforming existing methods in 3D object detection.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for point cloud pre-training rely on implicit scene representations and 2D reconstruction, limiting geometric understanding and computational efficiency.

Method: Gaussian2Scene employs a two-stage training strategy: a dual-branch masked autoencoder for 2D/3D representations, followed by supervised learning with Gaussian primitives and RGB images.

Result: The framework shows consistent improvements in downstream 3D object detection tasks.

Conclusion: Gaussian2Scene enhances geometric and cross-modal learning, offering a more efficient and effective SSL solution for 3D vision tasks.

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [312] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/pdf/2506.08817)
*Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT introduces a dataset and benchmark for enhancing spatiotemporal video understanding using Chain-of-Thought methodologies, revealing current VLMs' limitations.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack nuanced spatiotemporal understanding in video analysis, necessitating a specialized dataset.

Method: Developed Video-CoT with 192,000 question-answer pairs and 23,000 CoT-annotated samples, plus a benchmark for evaluation.

Result: Experiments show VLMs struggle with spatiotemporal tasks, underscoring the dataset's value.

Conclusion: Video-CoT advances multimedia understanding and supports future video analysis innovations.

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [313] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/pdf/2506.08849)
*Jingguo Qu, Xinyang Han, Tonghuan Xiao, Jia Ai, Juan Wu, Tong Zhao, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying*

Main category: cs.CV

TL;DR: The paper proposes domain adaptation methods for vision-language foundation models to improve ultrasound image analysis, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Manual contouring in medical ultrasonography is labor-intensive and inconsistent, while vision-language models face domain gaps between natural and medical images.

Method: Fine-tuning pipeline for vision-language models using a large language model as text refiner with adaptation strategies and task-driven heads.

Result: Evaluated on six ultrasound datasets, the method improves performance for segmentation and classification, surpassing state-of-the-art models.

Conclusion: The approach effectively bridges the domain gap, enhancing ultrasound image analysis with publicly available source code.

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at https://github.com/jinggqu/NextGen-UIA.

</details>


### [314] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/pdf/2506.08900)
*José Morano, Botond Fazekas, Emese Sükei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie, Ursula Schmidt-Erfurth, Hrvoje Bogunović*

Main category: cs.CV

TL;DR: MIRAGE is a multimodal foundation model for OCT and SLO image analysis, outperforming existing models in classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AI models for ophthalmology lack validation and focus on single modalities, prompting the need for a robust, multimodal solution.

Method: Proposes MIRAGE, a foundation model for OCT and SLO images, and introduces a new evaluation benchmark.

Result: MIRAGE outperforms general and specialized models in classification and segmentation tasks.

Conclusion: MIRAGE is a suitable foundation for robust AI systems in retinal OCT analysis, with publicly available resources.

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [315] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/pdf/2506.08908)
*Jiajun Li, Yue Ma, Xinyu Zhang, Qingyan Wei, Songhua Liu, Linfeng Zhang*

Main category: cs.CV

TL;DR: SkipVAR introduces adaptive acceleration for VAR models by addressing step and unconditional branch redundancies, achieving significant speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: High-frequency components in VAR models cause latency, but computational redundancy in these steps is understudied.

Method: Proposes step-skipping and unconditional branch replacement, combined in SkipVAR, a sample-adaptive framework using frequency info.

Result: Achieves 1.81x overall acceleration, 2.62x speedup on GenEval, with 0.88 SSIM, maintaining quality.

Conclusion: SkipVAR effectively accelerates VAR models adaptively, confirming the value of frequency-aware, training-free methods.

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [316] [Do Multiple Instance Learning Models Transfer?](https://arxiv.org/pdf/2506.09022)
*Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood*

Main category: cs.CV

TL;DR: Pretrained MIL models outperform scratch-trained ones in computational pathology, even across different organs, and pancancer pretraining enhances generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about MIL model transferability in computational pathology, especially with small datasets.

Method: Systematically evaluated 11 pretrained MIL models across 21 tasks for morphological and molecular subtype prediction.

Result: Pretrained MIL models consistently outperformed scratch-trained models, with pancancer pretraining showing strong generalization.

Conclusion: MIL models are adaptable, and transfer learning boosts performance in computational pathology; a standardized resource is provided.

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [317] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/pdf/2506.09176)
*Haoyuan Cai, Zhenghao Peng, Bolei Zhou*

Main category: cs.AI

TL;DR: AIM, a robot-gated IIL algorithm, reduces human supervision by adaptively requesting demonstrations using a proxy Q-function, improving efficiency and safety.


<details>
  <summary>Details</summary>
Motivation: Current IIL methods demand high cognitive effort from human supervisors, prompting the need for adaptive intervention.

Method: AIM uses a proxy Q-function to assess agent-expert alignment, adjusting intervention requests dynamically.

Result: AIM reduces human take-over costs by 40% and improves learning efficiency compared to Thrifty-DAgger.

Conclusion: AIM enhances IIL by reducing expert effort, improving demonstration quality, and minimizing unnecessary interactions.

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [318] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/pdf/2506.09250)
*C. Opus, A. Lawsen*

Main category: cs.AI

TL;DR: The paper critiques Shojaee et al.'s findings on LRMs' "accuracy collapse," attributing it to flawed experimental design rather than inherent reasoning failures.


<details>
  <summary>Details</summary>
Motivation: To address misconceptions about LRMs' reasoning capabilities due to experimental limitations.

Method: Analyzed three key issues in the original study: token limits, evaluation framework flaws, and unsolvable benchmark instances. Conducted controlled experiments with alternative methods.

Result: Preliminary experiments showed high accuracy on previously reported failure cases when experimental artifacts were controlled.

Conclusion: Emphasizes the need for meticulous experimental design in evaluating AI reasoning.

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [319] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/pdf/2506.09344)
*Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni is a unified multimodal model for processing images, text, audio, and video, excelling in speech and image generation without needing separate models or task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To create a single model capable of handling diverse multimodal tasks efficiently, eliminating the need for specialized models or redesigns.

Method: Uses dedicated encoders and a MoE architecture (Ling) with modality-specific routers to process and fuse multimodal inputs, integrating advanced decoders for audio and image generation.

Result: Demonstrates strong performance in unified perception and generation across modalities, matching GPT-4o in modality support.

Conclusion: Ming-Omni is a powerful, open-source solution for multimodal tasks, encouraging further research and development.

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [320] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/pdf/2506.09390)
*Kehan Zheng, Jinfeng Zhou, Hongning Wang*

Main category: cs.AI

TL;DR: LLMs exhibit human-like bounded rationality in strategic games but apply heuristics more rigidly and lack dynamic sensitivity.


<details>
  <summary>Details</summary>
Motivation: To compare LLMs and humans in strategic decision-making using behavioral game-theory paradigms.

Method: Experimental evaluation of LLMs in Rock-Paper-Scissors and Prisoner's Dilemma games, mirroring human studies.

Result: LLMs replicate human heuristics but rigidly, showing weaker adaptability to game dynamics.

Conclusion: Current LLMs partially mimic human bounded rationality; improved training for flexibility and context awareness is needed.

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [321] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/pdf/2506.09420)
*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Chunyu Miao, Dongyuan Li, Aiwei Liu, Yue Zhou, Yankai Chen, Weizhi Zhang, Yangning Li, Liancheng Fang, Renhe Jiang, Philip S. Yu*

Main category: cs.AI

TL;DR: The paper argues against fully autonomous AI agents, advocating for LLM-based Human-Agent Systems (LLM-HAS) where AI collaborates with humans for better reliability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current autonomous AI systems lack reliability, transparency, and human understanding, prompting a need for collaborative human-AI systems.

Method: Proposes LLM-HAS, where humans guide AI, with examples from healthcare, finance, and software development.

Result: Human-AI teamwork outperforms solo AI in complex tasks, offering more trustworthy and adaptable solutions.

Conclusion: AI progress should focus on enhancing human capabilities through collaboration, not autonomy.

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [322] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/pdf/2506.09498)
*Jaesik Yoon, Hyeonseo Cho, Yoshua Bengio, Sungjin Ahn*

Main category: cs.AI

TL;DR: Fast-MCTD improves MCTD by enabling parallel rollouts and reducing rollout length, achieving up to 100x speedup while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: MCTD's computational overhead limits its practicality due to sequential tree search and iterative denoising.

Method: Fast-MCTD integrates Parallel MCTD (parallel rollouts with delayed updates) and Sparse MCTD (trajectory coarsening).

Result: Fast-MCTD achieves up to 100x speedup over MCTD, outperforming Diffuser in speed while maintaining or improving planning performance.

Conclusion: Fast-MCTD is a scalable and efficient solution for diffusion-based inference-time reasoning.

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [323] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/pdf/2506.09655)
*Kaixuan Xu, Jiajun Chai, Sicheng Li, Yuqian Fu, Yuanheng Zhu, Dongbin Zhao*

Main category: cs.AI

TL;DR: DipLLM, a fine-tuned LLM-based agent, simplifies Diplomacy's complex action space using autoregressive factorization, achieving superior performance with minimal data compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AI methods for Diplomacy require extensive computational resources due to equilibrium search. LLMs offer a resource-efficient alternative but face challenges in handling the game's complexity.

Method: DipLLM uses autoregressive factorization to break multi-unit action assignment into unit-level decisions, fine-tuning an LLM with minimal data to learn equilibrium policies.

Result: DipLLM outperforms the state-of-the-art Cicero model using only 1.5% of its required data.

Conclusion: Fine-tuned LLMs like DipLLM show promise for complex strategic decision-making in multiplayer games, offering efficiency and performance gains.

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [324] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/pdf/2506.09656)
*Wei Zeng, Hengshu Zhu, Chuan Qin, Han Wu, Yihang Cheng, Sirui Zhang, Xiaowei Jin, Yinuo Shen, Zhenxing Wang, Feimin Zhong, Hui Xiong*

Main category: cs.AI

TL;DR: The paper reviews value alignment in AI agent systems, organizing value principles hierarchically, categorizing application scenarios, and evaluating alignment methods, while proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: The shift to multi-agent AI systems and the complexity of LLM applications raise situational risks, necessitating alignment of agent goals with human values.

Method: The paper hierarchically organizes value principles, categorizes agent system scenarios, and systematically evaluates alignment datasets and methods.

Result: A comprehensive review of value alignment in agent systems, including hierarchical value principles, scenario categorization, and alignment evaluation.

Conclusion: The paper highlights the importance of value alignment in AI agents and suggests future research directions to address emerging challenges.

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [325] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/pdf/2506.09659)
*Eltayeb Ahmed, Uljad Berdica, Martha Elliott, Danijela Horak, Jakob N. Foerster*

Main category: cs.AI

TL;DR: Intent Factored Generation (IFG) improves diversity in LLM outputs by factorizing sampling into intent and response stages, enhancing reasoning and conversational tasks without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for diversity in LLM outputs are token-level, leading to repetitive responses and poor exploration in reasoning tasks.

Method: IFG splits sampling into two stages: sampling a semantically dense intent (e.g., summary/keywords) and generating the final response conditioned on the intent and prompt, using varied temperatures for diversity and coherence.

Result: IFG improves pass@k and RL from Verifier Feedback on math/code tasks, enhances conversational diversity with Direct Preference Optimisation, and maintains quality in general language tasks.

Conclusion: IFG is a simple, effective method to increase LLM output diversity while preserving performance, easily integrable into existing algorithms.

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [326] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/pdf/2506.09977)
*Stylianos Loukas Vasileiou, Antonio Rago, Maria Vanina Martinez, William Yeoh*

Main category: cs.AI

TL;DR: People prefer explanation-based belief revisions, often non-minimal, challenging classical belief change theory. AI systems should adapt to align with human reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand human belief revision patterns and improve AI systems' alignment with human reasoning.

Method: Three user studies analyzing how people revise beliefs with explanations, whether provided or self-formulated.

Result: Consistent preference for explanation-based, non-minimal belief revisions across scenarios.

Conclusion: AI systems should incorporate explanation-based belief revision operators to better model human cognition.

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [327] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/pdf/2506.09985)
*Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas*

Main category: cs.AI

TL;DR: The paper presents V-JEPA 2, a self-supervised model trained on internet-scale video and robot data, achieving state-of-the-art performance in motion understanding, action anticipation, and video QA tasks. It also demonstrates zero-shot robotic planning.


<details>
  <summary>Details</summary>
Motivation: To develop AI models that understand and act in the physical world by leveraging large-scale video data and minimal interaction data.

Method: Pre-train V-JEPA 2 on 1M+ hours of video, align it with a language model, and post-train V-JEPA 2-AC on 62 hours of robot videos for planning.

Result: State-of-the-art results in motion understanding (77.3 top-1), action anticipation (39.7 recall-at-5), and video QA (84.0 on PerceptionTest). Zero-shot robotic planning achieved.

Conclusion: Self-supervised learning from web-scale data and minimal robot interaction enables effective world modeling and planning in the physical world.

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [328] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/pdf/2506.06905)
*Akash Gupta, Amos Storkey, Mirella Lapata*

Main category: cs.AI

TL;DR: A meta-learning approach improves few-shot learning in LMMs by distilling task-relevant image features into soft prompts, outperforming ICL and prompt-tuning methods.


<details>
  <summary>Details</summary>
Motivation: Inconsistent ICL performance in smaller LMMs due to irrelevant image embedding information.

Method: Proposes a meta-learning approach with soft prompts distilled from task-relevant image features, integrated via an attention-mapper module.

Result: Outperforms ICL and prompt-tuning on VL-ICL Bench, even under image perturbations.

Conclusion: The method enhances task induction and reasoning in LMMs under low-data regimes.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [329] [Goal Kernel Planning: Linearly-Solvable Non-Markovian Policies for Logical Tasks with Goal-Conditioned Options](https://arxiv.org/pdf/2007.02527)
*Thomas J. Ringstrom, Mohammadhosein Hasanbeig, Alessandro Abate*

Main category: cs.AI

TL;DR: LS-GKDP is a compositional framework combining LMDPs and the Options Framework to efficiently solve non-Markovian Boolean sub-goal tasks with ordering constraints.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of hierarchical planning for tasks requiring non-Markovian policies and logical conditions, aiming for representational reuse and task transfer.

Method: Combines Linearly-Solvable Markov Decision Processes (LMDPs) with the Options Framework, decomposing problems into goal-conditioned options and constructing a goal kernel for abstract transitions.

Result: Enables efficient optimization of meta-policies in lower-dimensional subspaces and supports zero-shot task transfer in certain cases.

Conclusion: LS-GKDP provides a scalable and reusable solution for hierarchical planning in complex, structured tasks.

Abstract: In the domain of hierarchical planning, compositionality, abstraction, and
task transfer are crucial for designing algorithms that can efficiently solve a
variety of problems with maximal representational reuse. Many real-world
problems require non-Markovian policies to handle complex structured tasks with
logical conditions, often leading to prohibitively large state representations;
this requires efficient methods for breaking these problems down and reusing
structure between tasks. To this end, we introduce a compositional framework
called Linearly-Solvable Goal Kernel Dynamic Programming (LS-GKDP) to address
the complexity of solving non-Markovian Boolean sub-goal tasks with ordering
constraints. LS-GKDP combines the Linearly-Solvable Markov Decision Process
(LMDP) formalism with the Options Framework of Reinforcement Learning. LMDPs
can be efficiently solved as a principal eigenvector problem, and options are
policies with termination conditions used as temporally extended actions; with
LS-GKDP we expand LMDPs to control over options for logical tasks. This
involves decomposing a high-dimensional problem down into a set of
goal-condition options for each goal and constructing a goal kernel, which is
an abstract transition kernel that jumps from an option's initial-states to its
termination-states along with an update of the higher-level task-state. We show
how an LMDP with a goal kernel enables the efficient optimization of
meta-policies in a lower-dimensional subspace defined by the task grounding.
Options can also be remapped to new problems within a super-exponential space
of tasks without significant recomputation, and we identify cases where the
solution is invariant to the task grounding, permitting zero-shot task
transfer.

</details>


### [330] [Decomposition Strategies and Multi-shot ASP Solving for Job-shop Scheduling](https://arxiv.org/pdf/2205.07537)
*Mohammed M. S. El-Kholany, Martin Gebser, Konstantin Schekotihin*

Main category: cs.AI

TL;DR: The paper explores decomposing the Job-shop Scheduling Problem (JSP) into time windows for multi-shot ASP solving, improving solution quality within tight runtime limits.


<details>
  <summary>Details</summary>
Motivation: JSP is a complex combinatorial optimization problem; decomposition aims to manage subproblems efficiently and find optimal partial solutions quickly.

Method: Decomposes JSP into time windows, uses multi-shot ASP solving, and incorporates overlapping/compression techniques for iterative scheduling.

Result: Multi-shot ASP solving outperforms single-shot optimization, especially when decomposing heuristic-based initial solutions.

Conclusion: Time window decomposition with multi-shot ASP solving enhances JSP scheduling efficiency and solution quality.

Abstract: The Job-shop Scheduling Problem (JSP) is a well-known and challenging
combinatorial optimization problem in which tasks sharing a machine are to be
arranged in a sequence such that encompassing jobs can be completed as early as
possible. In this paper, we investigate problem decomposition into time windows
whose operations can be successively scheduled and optimized by means of
multi-shot Answer Set Programming (ASP) solving. From a computational
perspective, decomposition aims to split highly complex scheduling tasks into
better manageable subproblems with a balanced number of operations such that
good-quality or even optimal partial solutions can be reliably found in a small
fraction of runtime. We devise and investigate a variety of decomposition
strategies in terms of the number and size of time windows as well as
heuristics for choosing their operations. Moreover, we incorporate time window
overlapping and compression techniques into the iterative scheduling process to
counteract optimization limitations due to the restriction to window-wise
partial schedules. Our experiments on different JSP benchmark sets show that
successive optimization by multi-shot ASP solving leads to substantially better
schedules within tight runtime limits than single-shot optimization on the full
problem. In particular, we find that decomposing initial solutions obtained
with proficient heuristic methods into time windows leads to improved solution
quality.

</details>


### [331] [Pointwise-in-Time Explanation for Linear Temporal Logic Rules](https://arxiv.org/pdf/2306.13956)
*Noel Brindise, Cedric Langbort*

Main category: cs.AI

TL;DR: The paper introduces Rule Status Assessment (RSA), a framework for pointwise-in-time status classification of Linear Temporal Logic (LTL) rules in agent trajectories, aiding post-hoc diagnostics.


<details>
  <summary>Details</summary>
Motivation: To provide a detailed, moment-specific view of agent behavior by assessing rule statuses (active, satisfied, inactive, violated) at individual time steps, enhancing explainability.

Method: Defines RSA on LTL rules, classifying rule statuses at each trajectory time step, allowing user queries for specific rules and times.

Result: Demonstrates RSA's utility as a post-hoc diagnostic tool for systematically tracking agent behavior against rules.

Conclusion: RSA offers a practical, intuitive approach to explain agent behavior by focusing on rule statuses at specific moments, improving transparency.

Abstract: The new field of Explainable Planning (XAIP) has produced a variety of
approaches to explain and describe the behavior of autonomous agents to human
observers. Many summarize agent behavior in terms of the constraints, or
''rules,'' which the agent adheres to during its trajectories. In this work, we
narrow the focus from summary to specific moments in individual trajectories,
offering a ''pointwise-in-time'' view. Our novel framework, which we define on
Linear Temporal Logic (LTL) rules, assigns an intuitive status to any rule in
order to describe the trajectory progress at individual time steps; here, a
rule is classified as active, satisfied, inactive, or violated. Given a
trajectory, a user may query for status of specific LTL rules at individual
trajectory time steps. In this paper, we present this novel framework, named
Rule Status Assessment (RSA), and provide an example of its implementation. We
find that pointwise-in-time status assessment is useful as a post-hoc
diagnostic, enabling a user to systematically track the agent's behavior with
respect to a set of rules.

</details>


### [332] [Generating Likely Counterfactuals Using Sum-Product Networks](https://arxiv.org/pdf/2401.14086)
*Jiri Nemecek, Tomas Pevny, Jakub Marecek*

Main category: cs.AI

TL;DR: The paper proposes a system using Mixed-Integer Optimization (MIO) and Sum-Product Networks (SPNs) to generate high-likelihood, close, and sparse counterfactual explanations for AI decisions.


<details>
  <summary>Details</summary>
Motivation: The need for explainable AI decisions is driven by regulation and user demand, but current methods often sacrifice key criteria like distance from the sample.

Method: The system models the search for likely counterfactual explanations using MIO and employs SPNs to estimate likelihoods, with an MIO formulation for SPNs.

Result: The method provides counterfactual explanations that are both high-likelihood and satisfy common desiderata like closeness and sparsity.

Conclusion: The proposed system effectively balances plausibility and key criteria for counterfactual explanations, with potential broader applications for SPNs.

Abstract: The need to explain decisions made by AI systems is driven by both recent
regulation and user demand. The decisions are often explainable only post hoc.
In counterfactual explanations, one may ask what constitutes the best
counterfactual explanation. Clearly, multiple criteria must be taken into
account, although "distance from the sample" is a key criterion. Recent methods
that consider the plausibility of a counterfactual seem to sacrifice this
original objective. Here, we present a system that provides high-likelihood
explanations that are, at the same time, close and sparse. We show that the
search for the most likely explanations satisfying many common desiderata for
counterfactual explanations can be modeled using Mixed-Integer Optimization
(MIO). We use a Sum-Product Network (SPN) to estimate the likelihood of a
counterfactual. To achieve that, we propose an MIO formulation of an SPN, which
can be of independent interest. The source code with examples is available at
https://github.com/Epanemu/LiCE.

</details>


### [333] [Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA](https://arxiv.org/pdf/2405.20421)
*Qianqi Yan, Xuehai He, Xiang Yue, Xin Eric Wang*

Main category: cs.AI

TL;DR: State-of-the-art Large Multimodal Models (LMMs) perform poorly on medical diagnosis questions under probing evaluation, highlighting reliability issues. The ProbMed dataset is introduced to rigorously assess LMMs, revealing significant limitations in fine-grained medical tasks.


<details>
  <summary>Details</summary>
Motivation: To address the questionable reliability of LMMs in medical Visual Question Answering (Med-VQA) under robust evaluation, especially in specialized diagnostic tasks.

Method: Introduces the Probing Evaluation for Medical Diagnosis (ProbMed) dataset, featuring probing evaluation (negation questions with hallucinated attributes) and procedural diagnosis (reasoning across diagnostic dimensions).

Result: Top models like GPT-4o, GPT-4V, and Gemini Pro perform worse than random guessing on specialized diagnostic questions, while others like LLaVA-Med struggle even with general questions.

Conclusion: Current LMMs are unreliable for fine-grained medical diagnosis, emphasizing the need for more robust evaluation and specialized domain knowledge.

Abstract: Large Multimodal Models (LMMs) have shown remarkable progress in medical
Visual Question Answering (Med-VQA), achieving high accuracy on existing
benchmarks. However, their reliability under robust evaluation is questionable.
This study reveals that when subjected to simple probing evaluation,
state-of-the-art models perform worse than random guessing on medical diagnosis
questions. To address this critical evaluation problem, we introduce the
Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess
LMM performance in medical imaging through probing evaluation and procedural
diagnosis. Particularly, probing evaluation features pairing original questions
with negation questions with hallucinated attributes, while procedural
diagnosis requires reasoning across various diagnostic dimensions for each
image, including modality recognition, organ identification, clinical findings,
abnormalities, and positional grounding. Our evaluation reveals that
top-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than
random guessing on specialized diagnostic questions, indicating significant
limitations in handling fine-grained medical inquiries. Besides, models like
LLaVA-Med struggle even with more general questions, and results from CheXagent
demonstrate the transferability of expertise across different modalities of the
same organ, showing that specialized domain knowledge is still crucial for
improving performance. This study underscores the urgent need for more robust
evaluation to ensure the reliability of LMMs in critical fields like medical
diagnosis, and current LMMs are still far from applicable to those fields.

</details>


### [334] [LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking](https://arxiv.org/pdf/2406.14917)
*Melvin Wong, Jiao Liu, Thiago Rios, Stefan Menzel, Yew Soon Ong*

Main category: cs.AI

TL;DR: LLM2TEA is an AI-driven evolutionary algorithm that combines large language models, text-to-3D generation, and physics simulation to create innovative, functional designs across domains.


<details>
  <summary>Details</summary>
Motivation: To discover interdisciplinary, real-world-compliant designs by leveraging AI for generative multitasking.

Method: Uses LLM for genotype initialization, text-to-3D for phenotype generation, classifiers for semantics, and physics simulation for validation. Introduces novel LLM-based evolutionary operators.

Result: Achieves 97-174% improvement in design diversity and 73% of designs outperform the baseline's top 1%. Designs are functional and 3D-printable.

Conclusion: LLM2TEA effectively bridges AI creativity with practical engineering, enabling tangible, innovative solutions.

Abstract: In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm
(LLM2TEA), the first agentic AI designer within a generative evolutionary
multitasking (GEM) framework that promotes the crossover and synergy of designs
from multiple domains, leading to innovative solutions that transcend
individual disciplines. Of particular interest is the discovery of objects that
are not only innovative but also conform to the physical specifications of the
real world in science and engineering. LLM2TEA comprises a large language model
to initialize a population of genotypes (defined by text prompts) describing
the objects of interest, a text-to-3D generative model to produce phenotypes
from these prompts, a classifier to interpret the semantic representations of
the objects, and a physics simulation model to assess their physical
properties. We propose several novel LLM-based multitask evolutionary operators
to guide the search toward the discovery of high-performing practical objects.
Experimental results in conceptual design optimization validate the
effectiveness of LLM2TEA, revealing from 97\% to 174\% improvement in the
diversity of innovative objects compared to the present text-to-3D generative
model baseline. In addition, more than 73\% of the generated designs have
better physical performance than the top 1\% percentile of the designs
generated in the baseline. Moreover, LLM2TEA generates designs that are not
only aesthetically creative but also functional in real-world applications.
Several of these designs have been successfully 3D-printed, emphasizing the
proposed approach's capacity to transform AI-generated outputs into tangible
physical objects. The designs produced by LLM2TEA meets practical requirements
while showcasing creative and innovative features, underscoring its potential
applications in complex design optimization and discovery.

</details>


### [335] [Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding](https://arxiv.org/pdf/2406.15481)
*Haneul Yoo, Yongjin Yang, Hwaran Lee*

Main category: cs.AI

TL;DR: The paper introduces CSRT, a framework for code-switching red-teaming queries to test LLM safety and multilingual understanding, showing it outperforms existing methods and reveals safety alignment issues.


<details>
  <summary>Details</summary>
Motivation: Addressing safety concerns in LLMs by exploring how code-switching in queries can expose undesirable behaviors.

Method: Developed CSRT to synthesize code-switching queries, tested on 10 LLMs with up to 10 languages.

Result: CSRT achieved 46.7% more attacks than standard methods, revealing multilingual safety gaps.

Conclusion: Code-switching is effective for red-teaming, and LLM safety alignment correlates with language resource availability.

Abstract: As large language models (LLMs) have advanced rapidly, concerns regarding
their safety have become prominent. In this paper, we discover that
code-switching in red-teaming queries can effectively elicit undesirable
behaviors of LLMs, which are common practices in natural language. We introduce
a simple yet effective framework, CSRT, to synthesize codeswitching red-teaming
queries and investigate the safety and multilingual understanding of LLMs
comprehensively. Through extensive experiments with ten state-of-the-art LLMs
and code-switching queries combining up to 10 languages, we demonstrate that
the CSRT significantly outperforms existing multilingual red-teaming
techniques, achieving 46.7% more attacks than standard attacks in English and
being effective in conventional safety domains. We also examine the
multilingual ability of those LLMs to generate and understand codeswitching
texts. Additionally, we validate the extensibility of the CSRT by generating
codeswitching attack prompts with monolingual data. We finally conduct detailed
ablation studies exploring code-switching and propound unintended correlation
between resource availability of languages and safety alignment in existing
multilingual LLMs.

</details>


### [336] [Human-like object concept representations emerge naturally in multimodal large language models](https://arxiv.org/pdf/2407.01067)
*Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, Huiguang He*

Main category: cs.AI

TL;DR: The study explores if LLMs develop human-like object representations, finding strong alignment between model embeddings and human cognition.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs can form human-like object concepts from linguistic and multimodal data.

Method: Combined behavioral and neuroimaging analyses, using 4.7M triplet judgments to derive 66D embeddings for 1,854 objects.

Result: Model embeddings showed semantic clustering like human cognition and aligned with neural activity in key brain regions.

Conclusion: LLMs develop human-like object representations, advancing machine intelligence and human-like AI systems.

Abstract: Understanding how humans conceptualize and categorize natural objects offers
critical insights into perception and cognition. With the advent of Large
Language Models (LLMs), a key question arises: can these models develop
human-like object representations from linguistic and multimodal data? In this
study, we combined behavioral and neuroimaging analyses to explore the
relationship between object concept representations in LLMs and human
cognition. We collected 4.7 million triplet judgments from LLMs and Multimodal
LLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity
structure of 1,854 natural objects. The resulting 66-dimensional embeddings
were stable, predictive, and exhibited semantic clustering similar to human
mental representations. Remarkably, the dimensions underlying these embeddings
were interpretable, suggesting that LLMs and MLLMs develop human-like
conceptual representations of objects. Further analysis showed strong alignment
between model embeddings and neural activity patterns in brain regions such as
EBA, PPA, RSC, and FFA. This provides compelling evidence that the object
representations in LLMs, while not identical to human ones, share fundamental
similarities that reflect key aspects of human conceptual knowledge. Our
findings advance the understanding of machine intelligence and inform the
development of more human-like artificial cognitive systems.

</details>


### [337] [Multiple Greedy Quasi-Newton Methods for Saddle Point Problems](https://arxiv.org/pdf/2408.00241)
*Minheng Xiao, Zhizhong Wu*

Main category: cs.AI

TL;DR: The paper introduces MGSR1-SP, a method for solving SCSC saddle point problems, improving Hessian approximation for better stability and efficiency, with proven linear-quadratic convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently and accurately approximating the squared indefinite Hessian matrix in SCSC saddle point problems, which is crucial for stability and performance in machine learning applications.

Method: MGSR1-SP uses iterative greedy updates to enhance Hessian approximation, with a theoretical analysis of its convergence properties.

Result: Numerical experiments on AUC maximization and adversarial debiasing show MGSR1-SP outperforms state-of-the-art methods in convergence rate.

Conclusion: MGSR1-SP is a promising method for improving performance in machine learning tasks requiring efficient Hessian approximations.

Abstract: This paper introduces the Multiple Greedy Quasi-Newton (MGSR1-SP) method, a
novel approach to solving strongly-convex-strongly-concave (SCSC) saddle point
problems. Our method enhances the approximation of the squared indefinite
Hessian matrix inherent in these problems, significantly improving both
stability and efficiency through iterative greedy updates. We provide a
thorough theoretical analysis of MGSR1-SP, demonstrating its linear-quadratic
convergence rate. Numerical experiments conducted on AUC maximization and
adversarial debiasing problems, compared with state-of-the-art algorithms,
underscore our method's enhanced convergence rate. These results affirm the
potential of MGSR1-SP to improve performance across a broad spectrum of machine
learning applications where efficient and accurate Hessian approximations are
crucial.

</details>


### [338] [Root Cause Attribution of Delivery Risks via Causal Discovery with Reinforcement Learning](https://arxiv.org/pdf/2408.05860)
*Minheng Xiao*

Main category: cs.AI

TL;DR: A novel method combines causal discovery and reinforcement learning to identify root causes of supply chain delivery risks, outperforming traditional analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional root cause analysis fails in complex supply chains due to spurious correlations, necessitating a more accurate approach.

Method: Integrates causal discovery to identify true causal relationships and reinforcement learning to refine the causal graph.

Result: Effectively identifies key drivers of late deliveries (e.g., shipping mode, delivery status) and offers actionable insights.

Conclusion: The approach improves supply chain efficiency, customer satisfaction, and profitability by accurately mitigating delivery risks.

Abstract: This paper presents a novel approach to root cause attribution of delivery
risks within supply chains by integrating causal discovery with reinforcement
learning. As supply chains become increasingly complex, traditional methods of
root cause analysis struggle to capture the intricate interrelationships
between various factors, often leading to spurious correlations and suboptimal
decision-making. Our approach addresses these challenges by leveraging causal
discovery to identify the true causal relationships between operational
variables, and reinforcement learning to iteratively refine the causal graph.
This method enables the accurate identification of key drivers of late
deliveries, such as shipping mode and delivery status, and provides actionable
insights for optimizing supply chain performance. We apply our approach to a
real-world supply chain dataset, demonstrating its effectiveness in uncovering
the underlying causes of delivery delays and offering strategies for mitigating
these risks. The findings have significant implications for improving
operational efficiency, customer satisfaction, and overall profitability within
supply chains.

</details>


### [339] [RConE: Rough Cone Embedding for Multi-Hop Logical Query Answering on Multi-Modal Knowledge Graphs](https://arxiv.org/pdf/2408.11526)
*Mayank Kharbanda, Rajiv Ratn Shah, Raghava Mutharaju*

Main category: cs.AI

TL;DR: RConE is a novel embedding method for answering logical multi-hop queries in Multi-Modal Knowledge Graphs (MMKGs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models lack the ability to utilize dense multi-modal information in MMKGs for logical querying.

Method: RConE embeds KG and queries in the same space, shortlists candidate multi-modal entities, and finds sub-entity solutions.

Result: RConE outperforms state-of-the-art methods on four MMKGs.

Conclusion: RConE is the first to introduce logical constructs in MMKG querying, addressing sub-entity answers effectively.

Abstract: Multi-hop query answering over a Knowledge Graph (KG) involves traversing one
or more hops from the start node to answer a query. Path-based and logic-based
methods are state-of-the-art for multi-hop question answering. The former is
used in link prediction tasks. The latter is for answering complex logical
queries. The logical multi-hop querying technique embeds the KG and queries in
the same embedding space. The existing work incorporates First Order Logic
(FOL) operators, such as conjunction ($\wedge$), disjunction ($\vee$), and
negation ($\neg$), in queries. Though current models have most of the building
blocks to execute the FOL queries, they cannot use the dense information of
multi-modal entities in the case of Multi-Modal Knowledge Graphs (MMKGs). We
propose RConE, an embedding method to capture the multi-modal information
needed to answer a query. The model first shortlists candidate (multi-modal)
entities containing the answer. It then finds the solution (sub-entities)
within those entities. Several existing works tackle path-based
question-answering in MMKGs. However, to our knowledge, we are the first to
introduce logical constructs in querying MMKGs and to answer queries that
involve sub-entities of multi-modal entities as the answer. Extensive
evaluation of four publicly available MMKGs indicates that RConE outperforms
the current state-of-the-art.

</details>


### [340] [Traceable LLM-based validation of statements in knowledge graphs](https://arxiv.org/pdf/2409.07507)
*Daniel Adam, Tomáš Kliegr*

Main category: cs.AI

TL;DR: A method for verifying RDF triples using LLMs avoids relying on LLM internal knowledge, instead comparing statements to external documents. Evaluated on BioRED and SNLI datasets, it shows 88% precision and 44% recall, requiring human oversight. Demonstrated on Wikidata, it suggests feasibility for large-scale KG verification.


<details>
  <summary>Details</summary>
Motivation: LLMs lack traceability for information origins, so the method avoids using their internal knowledge to ensure verifiable arguments.

Method: Compares verified RDF statements to external documents retrieved via web search or Wikipedia, using a retrieval augmented generation (RAG) workflow.

Result: 88% precision and 44% recall on BioRED dataset; comparison with SNLI dataset shows potential for natural language inference tasks.

Conclusion: LLMs can enable large-scale KG verification, though human oversight is needed due to current limitations.

Abstract: This article presents a method for verifying RDF triples using LLMs, with an
emphasis on providing traceable arguments. Because the LLMs cannot currently
reliably identify the origin of the information used to construct the response
to the user prompt, our approach is to avoid using internal LLM factual
knowledge altogether. Instead, verified RDF statements are compared to chunks
of external documents retrieved through a web search or Wikipedia. To assess
the possible application of this retrieval augmented generation (RAG) workflow
on biosciences content, we evaluated 1,719 positive statements from the BioRED
dataset and the same number of newly generated negative statements. The
resulting precision is 88 %, and recall is 44 %. This indicates that the method
requires human oversight. We also evaluated the method on the SNLI dataset,
which allowed us to compare our approach with models specifically tuned for the
natural language inference task. We demonstrate the method on Wikidata, where a
SPARQL query is used to automatically retrieve statements needing verification.
Overall, the results suggest that LLMs could be used for large-scale
verification of statements in KGs, a task previously unfeasible due to human
annotation costs.

</details>


### [341] [Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment](https://arxiv.org/pdf/2410.02197)
*Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, Quanquan Gu*

Main category: cs.AI

TL;DR: The paper introduces preference embedding and General Preference Optimization (GPO) to better model human preferences, outperforming traditional Bradley-Terry (BT) models in expressiveness and alignment with human values.


<details>
  <summary>Details</summary>
Motivation: Traditional reward models like BT lack expressiveness for intransitive preferences, limiting alignment of foundation models with nuanced human values.

Method: Proposes preference embedding for efficient latent space modeling and GPO to generalize RLHF, achieving linear query complexity.

Result: GPM outperforms BT on RewardBench, handles cyclic preferences, and improves performance on tasks like AlpacaEval2.0.

Conclusion: The method enhances alignment of foundation models with human values, demonstrated by superior performance over BT models.

Abstract: Modeling human preferences is crucial for aligning foundation models with
human values. Traditional reward modeling methods, such as the Bradley-Terry
(BT) reward model, fall short in expressiveness, particularly in addressing
intransitive preferences. In this paper, we introduce preference embedding, an
approach that embeds responses into a latent space to capture intricate
preference structures efficiently, achieving linear query complexity.
Additionally, we propose preference score-based General Preference Optimization
(GPO), which generalizes reward-based reinforcement learning from human
feedback (RLHF). Experimental results show that our General Preference
embedding Model (GPM) consistently outperforms the BT reward model on the
RewardBench benchmark and effectively models cyclic preferences where any BT
reward model behaves like a random guess. Furthermore, evaluations on
downstream tasks such as AlpacaEval2.0, following the language model
post-training with GPO and our general preference model, reveal performance
improvements over BT models. These findings indicate that our method may
enhance the alignment of foundation models with nuanced human values. The code
is available at https://github.com/general-preference/general-preference-model.

</details>


### [342] [Improving Handwritten Text Recognition via 3D Attention and Multi-Scale Training](https://arxiv.org/pdf/2410.18374)
*Zi-Rui Wang*

Main category: cs.AI

TL;DR: The paper proposes a new handwritten text recognition network using a 3D attention module and global-local context, achieving state-of-the-art results on Chinese and English datasets.


<details>
  <summary>Details</summary>
Motivation: To improve handwritten text recognition by integrating CTC, hidden Markov models, and encoder-decoder methods with a novel 3D attention module and context features.

Method: Uses 3D blocks from convolutional layers, processes them with a 3D attention module, and combines visual and global-local context features. Trained with CTC and cross-entropy losses.

Result: Achieves comparable performance to state-of-the-art methods on SCUT-HCCDoc, SCUT-EPT, and IAM datasets.

Conclusion: The proposed method effectively integrates attention and context features for robust handwritten text recognition.

Abstract: The segmentation-free research efforts for addressing handwritten text
recognition can be divided into three categories: connectionist temporal
classification (CTC), hidden Markov model and encoder-decoder methods. In this
paper, inspired by the above three modeling methods, we propose a new
recognition network by using a novel three-dimensional (3D) attention module
and global-local context information. Based on the feature maps of the last
convolutional layer, a series of 3D blocks with different resolutions are
split. Then, these 3D blocks are fed into the 3D attention module to generate
sequential visual features. Finally, by integrating the visual features and the
corresponding global-local context features, a well-designed representation can
be obtained. Main canonical neural units including attention mechanisms,
fully-connected layer, recurrent unit and convolutional layer are efficiently
organized into a network and can be jointly trained by the CTC loss and the
cross-entropy loss. Experiments on the latest Chinese handwritten text datasets
(the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset
(the IAM) show that the proposed method can achieve comparable results with the
state-of-the-art methods. The code is available at
https://github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.

</details>


### [343] [MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning](https://arxiv.org/pdf/2411.12977)
*Mircea Lică, Ojas Shirekar, Baptiste Colle, Chirag Raman*

Main category: cs.AI

TL;DR: MindForge, a generative-agent framework, enhances embodied agents' performance in Minecraft by integrating perspective-taking, inter-agent communication, and memory systems, outperforming Voyager in basic tasks and collaborative settings.


<details>
  <summary>Details</summary>
Motivation: Existing embodied agents powered by LLMs, like Voyager, struggle with elementary tasks despite fine-tuning. MindForge aims to improve competence through cultural lifelong learning and explicit perspective-taking.

Method: MindForge introduces: (1) a structured theory of mind, (2) natural inter-agent communication, and (3) a multi-component memory system. It is tested in Minecraft for instructive and collaborative tasks.

Result: MindForge agents outperform Voyager, achieving 3× more tech-tree milestones and 2.3× more unique items. In collaborative settings, performance improves with more communication rounds.

Conclusion: MindForge enables sophisticated behaviors like knowledge transfer, collaborative problem-solving, and adaptation to novel tasks, demonstrating the value of cultural learning in embodied AI.

Abstract: Embodied agents powered by large language models (LLMs), such as Voyager,
promise open-ended competence in worlds such as Minecraft. However, when
powered by open-weight LLMs they still falter on elementary tasks after
domain-specific fine-tuning. We propose MindForge, a generative-agent framework
for cultural lifelong learning through explicit perspective taking. We
introduce three key innovations: (1) a structured theory of mind representation
linking percepts, beliefs, desires, and actions; (2) natural inter-agent
communication; and (3) a multi-component memory system. Following the cultural
learning framework, we test MindForge in both instructive and collaborative
settings within Minecraft. In an instructive setting with GPT-4, MindForge
agents powered by open-weight LLMs significantly outperform their Voyager
counterparts in basic tasks yielding $3\times$ more tech-tree milestones and
collecting $2.3\times$ more unique items than the Voyager baseline.
Furthermore, in fully \textit{collaborative} settings, we find that the
performance of two underachieving agents improves with more communication
rounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate
sophisticated behaviors, including expert-novice knowledge transfer,
collaborative problem solving, and adaptation to out-of-distribution tasks
through accumulated cultural experiences.

</details>


### [344] [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/pdf/2412.04139)
*Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang*

Main category: cs.AI

TL;DR: The paper introduces Monet, a Mixture of Monosemantic Experts for Transformers, to improve interpretability of LLMs by scaling expert counts and enabling direct knowledge manipulation without performance loss.


<details>
  <summary>Details</summary>
Motivation: Understanding and aligning LLM computations with human values is challenging due to polysemanticity (neurons responding to unrelated concepts). Existing methods like Sparse Autoencoders compromise performance.

Method: Monet integrates sparse dictionary learning into Mixture-of-Experts pretraining, scaling experts to 262,144 per layer with parameters growing proportionally to the square root of expert count.

Result: Monet achieves mutual exclusivity of knowledge across experts, enables domain/language/toxicity manipulation, and maintains general performance.

Conclusion: Scaling expert counts enhances mechanistic interpretability and allows direct adjustment of model behavior, advancing transparent LLMs.

Abstract: Understanding the internal computations of large language models (LLMs) is
crucial for aligning them with human values and preventing undesirable
behaviors like toxic content generation. However, mechanistic interpretability
is hindered by polysemanticity -- where individual neurons respond to multiple,
unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to
disentangle these features through sparse dictionary learning, they have
compromised LLM performance due to reliance on post-hoc reconstruction loss. To
address this issue, we introduce Mixture of Monosemantic Experts for
Transformers (Monet) architecture, which incorporates sparse dictionary
learning directly into end-to-end Mixture-of-Experts pretraining. Our novel
expert decomposition method enables scaling the expert count to 262,144 per
layer while total parameters scale proportionally to the square root of the
number of experts. Our analyses demonstrate mutual exclusivity of knowledge
across experts and showcase the parametric knowledge encapsulated within
individual experts. Moreover, Monet allows knowledge manipulation over domains,
languages, and toxicity mitigation without degrading general performance. Our
pursuit of transparent LLMs highlights the potential of scaling expert counts
to enhance mechanistic interpretability and directly resect the internal
knowledge to fundamentally adjust model behavior. The source code and
pretrained checkpoints are available at https://github.com/dmis-lab/Monet.

</details>


### [345] [Logic-Constrained Shortest Paths for Flight Planning](https://arxiv.org/pdf/2412.13235)
*Ricardo Euler, Pedro Maristany de las Casas, Ralf Borndörfer*

Main category: cs.AI

TL;DR: A branch and bound algorithm for the logic-constrained shortest path problem (LCSPP) is proposed, tailored for flight planning with traffic flow restrictions (TFRs). The algorithm's efficiency is demonstrated on real-world data, showing significant improvements with informed choices of node selection, branching rules, and conflicts.


<details>
  <summary>Details</summary>
Motivation: The LCSPP arises in flight planning where ATC imposes TFRs for safety and throughput. Existing MIP and SAT methods don't directly apply, necessitating a tailored approach.

Method: A branch and bound algorithm is developed, focusing on node selection, branching rules, and conflicts. The flight planning problem is modeled as an LCSPP and solved using this algorithm.

Result: The algorithm is tested on real-world data (20000 TFRs) and shows an order of magnitude improvement with optimized rule combinations.

Conclusion: Tailored branch and bound rules significantly enhance LCSPP solutions for flight planning, with practical efficiency demonstrated on industry data.

Abstract: The logic-constrained shortest path problem (LCSPP) combines a one-to-one
shortest path problem with satisfiability constraints imposed on the routing
graph. This setting arises in flight planning, where air traffic control (ATC)
authorities are enforcing a set of traffic flow restrictions (TFRs) on aircraft
routes in order to increase safety and throughput. We propose a new branch and
bound-based algorithm for the LCSPP. The resulting algorithm has three main
degrees of freedom: the node selection rule, the branching rule and the
conflict. While node selection and branching rules have been long studied in
the MIP and SAT communities, most of them cannot be applied out of the box for
the LCSPP. We review the existing literature and develop tailored variants of
the most prominent rules. The conflict, the set of variables to which the
branching rule is applied, is unique to the LCSPP. We analyze its theoretical
impact on the B&B algorithm. In the second part of the paper, we show how to
model the flight planning problem with TFRs as an LCSPP and solve it using the
branch and bound algorithm. We demonstrate the algorithm's efficiency on a
dataset consisting of a global flight graph and a set of around 20000 real TFRs
obtained from our industry partner Lufthansa Systems GmbH. We make this dataset
publicly available. Finally, we conduct an empirical in-depth analysis of
dynamic shortest path algorithms, node selection rules, branching rules and
conflicts. Carefully choosing an appropriate combination yields an improvement
of an order of magnitude compared to an uninformed choice.

</details>


### [346] [Supervision policies can shape long-term risk management in general-purpose AI models](https://arxiv.org/pdf/2501.06137)
*Manuel Cebrian, Emilia Gomez, David Fernandez Llorca*

Main category: cs.AI

TL;DR: The paper explores AI supervision challenges with GPAI models, simulating four policies to manage risk reporting. Priority and diversity policies mitigate high-impact risks but may overlook systemic issues, validated with real-world data.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of GPAI models like LLMs poses challenges for AI supervision, requiring effective policies to manage diverse risk reporting ecosystems.

Method: A simulation framework evaluates four supervision policies (non-prioritized, random, priority-based, diversity-prioritized) using features from risk reporting ecosystems.

Result: Priority and diversity policies are effective for high-impact risks but may neglect systemic issues, skewing risk perception. Validation with real-world data confirms trade-offs.

Conclusion: AI risk supervision policies shape the risk landscape, highlighting trade-offs between mitigating high-impact risks and comprehensive coverage.

Abstract: The rapid proliferation and deployment of General-Purpose AI (GPAI) models,
including large language models (LLMs), present unprecedented challenges for AI
supervisory entities. We hypothesize that these entities will need to navigate
an emergent ecosystem of risk and incident reporting, likely to exceed their
supervision capacity. To investigate this, we develop a simulation framework
parameterized by features extracted from the diverse landscape of risk,
incident, or hazard reporting ecosystems, including community-driven platforms,
crowdsourcing initiatives, and expert assessments. We evaluate four supervision
policies: non-prioritized (first-come, first-served), random selection,
priority-based (addressing the highest-priority risks first), and
diversity-prioritized (balancing high-priority risks with comprehensive
coverage across risk types). Our results indicate that while priority-based and
diversity-prioritized policies are more effective at mitigating high-impact
risks, particularly those identified by experts, they may inadvertently neglect
systemic issues reported by the broader community. This oversight can create
feedback loops that amplify certain types of reporting while discouraging
others, leading to a skewed perception of the overall risk landscape. We
validate our simulation results with several real-world datasets, including one
with over a million ChatGPT interactions, of which more than 150,000
conversations were identified as risky. This validation underscores the complex
trade-offs inherent in AI risk supervision and highlights how the choice of
risk management policies can shape the future landscape of AI risks across
diverse GPAI models used in society.

</details>


### [347] [Reasoning Language Models: A Blueprint](https://arxiv.org/pdf/2501.11223)
*Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Zixuan Chen, Hubert Niewiadomski, Torsten Hoefler*

Main category: cs.AI

TL;DR: The paper proposes a modular framework for Reasoning Language Models (RLMs) to address accessibility and scalability challenges, introducing a versatile blueprint and a prototype implementation (x1).


<details>
  <summary>Details</summary>
Motivation: High costs, proprietary nature, and complexity of RLMs limit accessibility and scalability, prompting the need for a democratized and modular approach.

Method: The paper surveys RLM works, organizes components into a modular framework, and provides mathematical formulations and algorithmic specifications. It introduces x1 for prototyping.

Result: The blueprint unifies diverse reasoning structures and strategies, demonstrated through examples like LLaMA-Berry and QwQ. Insights include multi-phase training and scalable deployments.

Conclusion: The work simplifies RLM construction, democratizes advanced reasoning, and aims to bridge the gap between resource-rich and resource-limited AI development.

Abstract: Reasoning language models (RLMs), also known as Large Reasoning Models
(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have
redefined AI's problem-solving capabilities by extending LLMs with advanced
reasoning mechanisms. Yet, their high costs, proprietary nature, and complex
architectures - uniquely combining reinforcement learning (RL), search
heuristics, and LLMs - present accessibility and scalability challenges. To
address these, we propose a comprehensive blueprint that organizes RLM
components into a modular framework, based on a survey and analysis of all RLM
works. This blueprint incorporates diverse reasoning structures (chains, trees,
graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,
Beam Search), RL concepts (policy, value models and others), supervision
schemes (Outcome-Based and Process-Based Supervision), and other related
concepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent
tools). We also provide detailed mathematical formulations and algorithmic
specifications to simplify RLM implementation. By showing how schemes like
LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,
we demonstrate the blueprint's versatility and unifying potential. To
illustrate its utility, we introduce x1, a modular implementation for rapid RLM
prototyping and experimentation. Using x1 and a literature review, we provide
key insights, such as multi-phase training for policy and value models, and the
importance of familiar training distributions. Finally, we discuss scalable RLM
cloud deployments and we outline how RLMs can integrate with a broader LLM
ecosystem. Our work demystifies RLM construction, democratizes advanced
reasoning capabilities, and fosters innovation, aiming to mitigate the gap
between "rich AI" and "poor AI" by lowering barriers to RLM design and
experimentation.

</details>


### [348] [Monte Carlo Tree Diffusion for System 2 Planning](https://arxiv.org/pdf/2502.07202)
*Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn*

Main category: cs.AI

TL;DR: MCTD integrates diffusion models with MCTS for scalable planning, outperforming diffusion baselines by leveraging tree-structured denoising and adaptive search.


<details>
  <summary>Details</summary>
Motivation: Address the scalability limitations of diffusion-based planners by combining the generative strength of diffusion models with the adaptive search capabilities of MCTS.

Method: Introduces MCTD, a framework that treats denoising as a tree-structured process, enabling iterative evaluation, pruning, and refinement of plans.

Result: Empirical results show MCTD outperforms diffusion baselines, especially in long-horizon tasks, with improved solution quality as inference-time computation scales.

Conclusion: MCTD successfully bridges the gap between diffusion models and MCTS, offering scalable and high-quality planning solutions.

Abstract: Diffusion models have recently emerged as a powerful tool for planning.
However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally
improves with inference-time computation scaling-standard diffusion-based
planners offer only limited avenues for the scalability. In this paper, we
introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates
the generative strength of diffusion models with the adaptive search
capabilities of MCTS. Our method reconceptualizes denoising as a
tree-structured process, allowing partially denoised plans to be iteratively
evaluated, pruned, and refined. By selectively expanding promising trajectories
while retaining the flexibility to revisit and improve suboptimal branches,
MCTD achieves the benefits of MCTS such as controlling exploration-exploitation
trade-offs within the diffusion framework. Empirical results on challenging
long-horizon tasks show that MCTD outperforms diffusion baselines, yielding
higher-quality solutions as inference-time computation increases.

</details>


### [349] [Rethinking Diverse Human Preference Learning through Principal Component Analysis](https://arxiv.org/pdf/2502.13131)
*Feng Luo, Rui Yang, Hao Sun, Chunyuan Deng, Jiarui Yao, Jingyan Shen, Huan Zhang, Hanjie Chen*

Main category: cs.AI

TL;DR: DRMs decompose human preferences from binary comparisons using PCA, offering scalable and interpretable reward models for personalized AI.


<details>
  <summary>Details</summary>
Motivation: Human preferences are diverse and complex, but traditional reward models struggle to capture them, and fine-grained data collection is costly.

Method: DRMs use PCA on embedding differences from binary comparisons to extract orthogonal preference vectors.

Result: DRMs identify meaningful preference dimensions (e.g., helpfulness, safety) and adapt to new users without retraining.

Conclusion: DRMs provide a scalable, interpretable framework for aligning LLMs with diverse human preferences.

Abstract: Understanding human preferences is crucial for improving foundation models
and building personalized AI systems. However, preferences are inherently
diverse and complex, making it difficult for traditional reward models to
capture their full range. While fine-grained preference data can help,
collecting it is expensive and hard to scale. In this paper, we introduce
Decomposed Reward Models (DRMs), a novel approach that extracts diverse human
preferences from binary comparisons without requiring fine-grained annotations.
Our key insight is to represent human preferences as vectors and analyze them
using Principal Component Analysis (PCA). By constructing a dataset of
embedding differences between preferred and rejected responses, DRMs identify
orthogonal basis vectors that capture distinct aspects of preference. These
decomposed rewards can be flexibly combined to align with different user needs,
offering an interpretable and scalable alternative to traditional reward
models. We demonstrate that DRMs effectively extract meaningful preference
dimensions (e.g., helpfulness, safety, humor) and adapt to new users without
additional training. Our results highlight DRMs as a powerful framework for
personalized and interpretable LLM alignment. Our code is available at
https://github.com/amandaluof/DRMs.

</details>


### [350] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems](https://arxiv.org/pdf/2504.20462)
*Qi Wang, Xiao Zhang, Mingyi Li, Yuan Yuan, Mengbai Xiao, Fuzhen Zhuang, Dongxiao Yu*

Main category: cs.AI

TL;DR: TAMO, a tool-assisted LLM agent, addresses challenges in automated root cause analysis (RCA) for microservices by integrating multi-modal data and specialized tools, improving fault localization and repair strategy generation.


<details>
  <summary>Details</summary>
Motivation: The complexity of microservices and cloud-native systems makes traditional RCA inefficient, requiring automated solutions. LLMs offer potential but face challenges like text constraints and dynamic dependencies.

Method: TAMO unifies multi-modal observational data into time-aligned representations, uses specialized tools for localization and classification, and structures key information for LLM prompts.

Result: TAMO performs well in RCA on heterogeneous public datasets with common fault types, proving its effectiveness.

Conclusion: TAMO successfully addresses LLM limitations in RCA for microservices, offering a practical solution for automated fault response.

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [351] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/pdf/2505.04317)
*Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang*

Main category: cs.AI

TL;DR: HCSP, a hierarchical reinforcement learning framework, outperforms baselines in 3v3 multi-drone volleyball by separating strategic and motion control, achieving an 82.9% win rate.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of long-horizon dependencies, tight inter-agent coupling, and underactuated dynamics in multi-drone volleyball.

Method: Proposes HCSP with a three-stage training pipeline: diverse low-level skills, high-level strategy via self-play, and joint fine-tuning through co-self-play.

Result: HCSP achieves an 82.9% win rate, outperforming baselines, and exhibits emergent team behaviors like role switching.

Conclusion: HCSP’s hierarchical design and training scheme effectively address the complexities of multi-drone volleyball.

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9% win rate and a 71.5% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme. The project page
is at https://sites.google.com/view/hi-co-self-play.

</details>


### [352] [Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems](https://arxiv.org/pdf/2505.04646)
*Poria Azadi*

Main category: cs.AI

TL;DR: Autonomous systems exhibit computational unpredictability, making their future behavior undecidable, distinguishing them from non-autonomous systems.


<details>
  <summary>Details</summary>
Motivation: To formally distinguish autonomous systems by their inherent unpredictability and connect this to computational theory and biology.

Method: Developed a formal model linking autonomy to computational undecidability, integrating insights from computational theory and biology.

Result: Proved that truly autonomous systems have undecidable future behavior, grounding a principled distinction from non-autonomous systems.

Conclusion: The findings impact AI, biological modeling, and philosophical debates like free will, highlighting the unique nature of autonomy.

Abstract: This article presents a formal model demonstrating that genuine autonomy, the
ability of a system to self-regulate and pursue objectives, fundamentally
implies computational unpredictability from an external perspective. we
establish precise mathematical connections, proving that for any truly
autonomous system, questions about its future behavior are fundamentally
undecidable. this formal undecidability, rather than mere complexity, grounds a
principled distinction between autonomous and non-autonomous systems. our
framework integrates insights from computational theory and biology,
particularly regarding emergent agency and computational irreducibility, to
explain how novel information and purpose can arise within a physical universe.
the findings have significant implications for artificial intelligence,
biological modeling, and philosophical concepts like free will.

</details>


### [353] [MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO](https://arxiv.org/pdf/2505.13031)
*Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, Ying Shan*

Main category: cs.AI

TL;DR: MindOmni is a unified multimodal LLM that enhances text-to-image systems by integrating reasoning generation via reinforcement learning, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Address limitations in handling multimodal inputs and complex reasoning tasks in text-to-image systems.

Method: Three-phase training: unified vision-language model, supervised fine-tuning with CoT data, and RGPO algorithm for policy updates.

Result: Outperforms existing models, excels in understanding/generation benchmarks, and shows advanced reasoning capabilities.

Conclusion: MindOmni effectively tackles multimodal and reasoning challenges, with public code availability.

Abstract: Recent text-to-image systems face limitations in handling multimodal inputs
and complex reasoning tasks. We introduce MindOmni, a unified multimodal large
language model that addresses these challenges by incorporating reasoning
generation through reinforcement learning. MindOmni leverages a three-phase
training strategy: i) design of a unified vision language model with a
decoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought
(CoT) instruction data, and iii) our proposed Reasoning Generation Policy
Optimization (RGPO) algorithm, utilizing multimodal feedback to effectively
guide policy updates. Experimental results demonstrate that MindOmni
outperforms existing models, achieving impressive performance on both
understanding and generation benchmarks, meanwhile showcasing advanced
fine-grained reasoning generation capabilities, especially with mathematical
reasoning instruction. All codes will be made public at
https://github.com/TencentARC/MindOmni

</details>


### [354] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/pdf/2505.14479)
*Oren Sultan, Eitan Stern, Dafna Shahaf*

Main category: cs.AI

TL;DR: A neuro-symbolic approach combining LLMs with structured components improves proof accuracy in geometry by leveraging analogous problems and formal verification.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with formal domains like mathematical proof generation, requiring a method to enhance their logical reasoning.

Method: The approach retrieves analogous problems to guide LLMs and uses a formal verifier to evaluate and correct generated proofs.

Result: Proof accuracy for OpenAI's o1 model improved by 58%-70%, with both analogous problems and verifier feedback contributing.

Conclusion: Enhancing LLMs to generate provably correct conclusions can boost reliability and unlock critical applications requiring trustworthiness.

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [355] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/pdf/2505.16223)
*Sangyong Lee, Subo Hwang, Dohoon Kim*

Main category: cs.AI

TL;DR: MADCluster is a model-agnostic anomaly detection framework using self-supervised clustering to address the 'hypersphere collapse' problem in deep learning-based methods. It clusters normal data into a single cluster and optimizes a new 'One-directed Adaptive loss'.


<details>
  <summary>Details</summary>
Motivation: To solve the 'hypersphere collapse' issue in existing anomaly detection methods and provide a versatile solution applicable to various deep learning architectures.

Method: MADCluster uses self-supervised clustering with three components: Base Embedder, Cluster Distance Mapping, and Sequence-wise Clustering. It introduces a 'One-directed Adaptive loss' for optimization.

Result: Experiments on four time series datasets show improved performance of comparative models when using MADCluster.

Conclusion: MADCluster is compatible with various architectures, enhancing model performance, and shows promise for broader applications.

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [356] [Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.20728)
*Zesen Lyu, Dandan Zhang, Wei Ye, Fangdi Li, Zhihang Jiang, Yao Yang*

Main category: cs.AI

TL;DR: The paper introduces Jigsaw-Puzzles, a benchmark to evaluate spatial reasoning in VLMs, revealing significant performance gaps compared to humans.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs can match human-like spatial reasoning, a critical cognitive skill, using a novel benchmark.

Method: Developed Jigsaw-Puzzles with 1,100 complex images and five tasks to test VLMs' spatial capabilities, minimizing domain bias.

Result: Top VLM (Gemini-2.5-Pro) scored 77.14% overall, with 30% on Order Generation, far below human performance (>90%).

Conclusion: Jigsaw-Puzzles highlights VLMs' limitations in spatial reasoning, serving as a diagnostic tool for future research.

Abstract: Spatial reasoning is a core component of human cognition, enabling
individuals to perceive, comprehend, and interact with the physical world. It
relies on a nuanced understanding of spatial structures and inter-object
relationships, serving as the foundation for complex reasoning and
decision-making. To investigate whether current vision-language models (VLMs)
exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark
consisting of 1,100 carefully curated real-world images with high spatial
complexity. Based on this dataset, we design five tasks to rigorously evaluate
VLMs' spatial perception, structural understanding, and reasoning capabilities,
while deliberately minimizing reliance on domain-specific knowledge to better
isolate and assess the general spatial reasoning capability. We conduct a
comprehensive evaluation across 24 state-of-the-art VLMs. The results show that
even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy
and performs particularly poorly on the Order Generation task, with only 30.00%
accuracy, far below the performance exceeding 90% achieved by human
participants. This persistent gap underscores the need for continued progress,
positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for
advancing spatial reasoning research in VLMs. Our project page is at
https://zesen01.github.io/jigsaw-puzzles

</details>


### [357] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/pdf/2505.23885)
*Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li*

Main category: cs.AI

TL;DR: Workforce is a hierarchical multi-agent framework for cross-domain task automation, improving transferability and performance without full retraining.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems struggle with domain transfer due to domain-specific designs, requiring costly redesigns and retraining.

Method: Workforce uses a modular architecture with a domain-agnostic Planner, Coordinator, and specialized Workers, coupled with Optimized Workforce Learning (OWL) for training.

Result: Workforce achieves 69.70% accuracy on GAIA, outperforming commercial systems, and OWL-trained models show significant improvements (+16.37%).

Conclusion: The framework enables scalable generalization and modular domain transfer, advancing general-purpose AI assistants.

Abstract: Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [358] [BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies](https://arxiv.org/pdf/2506.00328)
*Kourosh Shahnazari, Seyed Moein Ayyoubzadeh, Mohammadali Keshtparvar*

Main category: cs.AI

TL;DR: BASIL introduces a symbolic, interpretable reinforcement learning method using evolutionary search and quality-diversity optimization, achieving comparable performance to deep RL baselines.


<details>
  <summary>Details</summary>
Motivation: Address the opacity of deep RL policies in safety-critical applications by ensuring interpretability and transparency.

Method: Uses online evolutionary search with quality-diversity optimization to generate symbolic, rule-based policies represented as ordered lists of predicates.

Result: BASIL produces interpretable controllers with compact representations, matching deep RL performance on benchmark tasks.

Conclusion: BASIL successfully combines symbolic expressiveness, evolutionary diversity, and online learning for interpretable policy synthesis.

Abstract: The quest for interpretable reinforcement learning is a grand challenge for
the deployment of autonomous decision-making systems in safety-critical
applications. Modern deep reinforcement learning approaches, while powerful,
tend to produce opaque policies that compromise verification, reduce
transparency, and impede human oversight. To address this, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a systematic approach for
generating symbolic, rule-based policies via online evolutionary search with
quality-diversity (QD) optimization. BASIL represents policies as ordered lists
of symbolic predicates over state variables, ensuring full interpretability and
tractable policy complexity. By using a QD archive, the methodology in the
proposed study encourages behavioral and structural diversity between
top-performing solutions, while a complexity-aware fitness encourages the
synthesis of compact representations. The evolutionary system supports the use
of exact constraints for rule count and system adaptability for balancing
transparency with expressiveness. Empirical comparisons with three benchmark
tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently
synthesizes interpretable controllers with compact representations comparable
to deep reinforcement learning baselines. Herein, this article introduces a new
interpretable policy synthesis method that combines symbolic expressiveness,
evolutionary diversity, and online learning through a unifying framework.

</details>


### [359] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/pdf/2506.02865)
*Mathieu Andreux, Breno Baldas Skuk, Hamza Benchekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Pierre-Louis Cedoz, Antoine Chassang, Mickaël Chen, Alexandra D. Constantinou, Antoine d'Andigné, Hubert de La Jonquière, Aurélien Delfosse, Ludovic Denoyer, Alexis Deprez, Augustin Derupti, Michael Eickenberg, Mathïs Federico, Charles Kantor, Xavier Koegler, Yann Labbé, Matthew C. H. Lee, Erwan Le Jumeau de Kergaradec, Amir Mahla, Avshalom Manevich, Adrien Maret, Charles Masson, Rafaël Maurin, Arturo Mena, Philippe Modard, Axel Moyal, Axel Nguyen Kerbel, Julien Revelle, Mats L. Richter, María Santos, Laurent Sifre, Maxime Theillard, Marc Thibault, Louis Thiry, Léo Tronchon, Nicolas Usunier, Tony Wu*

Main category: cs.AI

TL;DR: Surfer-H is a cost-efficient web agent using Vision-Language Models (VLM) for web tasks, paired with Holo1, a specialized VLM. Holo1 excels in benchmarks, and Surfer-H achieves 92.2% performance on WebVoyager. Both WebClick dataset and Holo1 weights are open-sourced.


<details>
  <summary>Details</summary>
Motivation: To create a cost-efficient web agent capable of performing user-defined tasks on the web using advanced VLMs, and to advance research in agentic systems by providing open-source tools.

Method: Integration of Surfer-H with Holo1, a specialized VLM trained on curated data (open-access web content, synthetic examples, and self-produced agentic data).

Result: Holo1 tops UI benchmarks and WebClick. Surfer-H achieves 92.2% performance on WebVoyager, balancing accuracy and cost-efficiency.

Conclusion: Surfer-H and Holo1 demonstrate high performance in web tasks, and their open-sourcing aims to accelerate research in agentic systems.

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [360] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/pdf/2506.07564)
*Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW is a protocol-level framework for secure and reliable LLM/VLM-based agents, enforcing fine-grained information flow control and robust multi-agent coordination, validated by SAFEFLOWBENCH.


<details>
  <summary>Details</summary>
Motivation: Current agent frameworks lack mechanisms for secure information flow, reliability, and multi-agent coordination, necessitating a principled solution.

Method: SAFEFLOW enforces fine-grained IFC, tracks data provenance, integrity, and confidentiality, and introduces transactional execution, conflict resolution, and secure scheduling.

Result: Agents built with SAFEFLOW maintain high task performance and security under adversarial conditions, outperforming state-of-the-art.

Conclusion: SAFEFLOW and SAFEFLOWBENCH advance reliable autonomy by providing a robust and secure framework for agent ecosystems.

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [361] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/pdf/2506.07736)
*Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, Tat-Seng Chua*

Main category: cs.AI

TL;DR: RSafe is a reasoning-based safeguard for LLMs, using guided reasoning and reinforced alignment to protect against policy-violating content without relying on extensive human-curated datasets.


<details>
  <summary>Details</summary>
Motivation: Existing guard models for LLMs struggle with out-of-distribution threats and require heavy human curation, prompting the need for a more adaptive solution.

Method: RSafe uses a two-stage approach: guided reasoning for risk analysis and reinforced alignment via rule-based RL to optimize safety predictions.

Result: RSafe generalizes safety protection to unseen or adversarial scenarios, adapting to user-specified policies.

Conclusion: RSafe offers a robust, adaptive safeguard for LLMs, addressing limitations of current guard models.

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [362] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/pdf/2506.08399)
*Jiachen Ma, Zhanhui Zhou, Chao Yang, Chaochao Lu*

Main category: cs.AI

TL;DR: SafeCoT improves refusal behavior in VLMs using rule-based CoT supervision, reducing overrefusal and enhancing generalization with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring safe and appropriate responses from VLMs in high-risk or ambiguous scenarios.

Method: Introduces SafeCoT, a lightweight, interpretable framework leveraging rule-based chain-of-thought supervision for safety reasoning.

Result: Significantly reduces overrefusal and enhances generalization across benchmarks, even with limited data.

Conclusion: SafeCoT offers a scalable solution for aligning VLMs with safety-critical objectives.

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [363] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/pdf/2506.08422)
*Ikkei Itoku, David Theil, Evelyn Eichelsdoerfer Uehara, Sreyoshi Bhaduri, Junnosuke Kuroda, Toshi Yumoto, Alex Gil, Natalie Perez, Rajesh Cherukuri, Naumaan Nayyar*

Main category: cs.AI

TL;DR: A novel framework combining LLMs with expert calibration and iterative prompt optimization automates taxonomy alignment, achieving high accuracy (F1-score 0.97) and outperforming human benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional manual taxonomy alignment is costly and subjective, while automated methods lack nuance and transparency.

Method: Integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in taxonomy alignment.

Result: Achieved an F1-score of 0.97, surpassing the human benchmark of 0.68.

Conclusion: The framework effectively scales taxonomy alignment with high-quality mappings and expert oversight for ambiguous cases.

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [364] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/pdf/2506.08424)
*Yong Liang Goh, Zhiguang Cao, Yining Ma, Jianan Zhou, Mohammed Haroon Dupty, Wee Sun Lee*

Main category: cs.AI

TL;DR: SHIELD introduces a novel model for Multi-Task Multi-Distribution VRP, leveraging sparsity and hierarchy to improve generalization on unseen tasks and distributions.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for routing problems overlook complex real-world customer distributions, limiting their practicality.

Method: SHIELD uses Mixture-of-Depths (MoD) for sparsity and a context-based clustering layer for hierarchy, enhancing task/distribution-specific and shared representations.

Result: Superior performance on 9 real-world maps with 16 VRP variants each.

Conclusion: SHIELD's design improves generalization and efficiency, making it a robust solution for realistic VRP challenges.

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [365] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/pdf/2506.09792)
*Wenxuan Wu, Shuai Wang, Xixin Wu, Helen Meng, Haizhou Li*

Main category: cs.SD

TL;DR: The paper explores using pre-trained speech-language and language models (PSLMs/PLMs) to enhance audio-visual target speaker extraction (AV-TSE) by leveraging linguistic knowledge, improving speech quality and intelligibility without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Humans use linguistic knowledge (e.g., syntax, semantics) for speech perception, inspiring the integration of PSLMs/PLMs into AV-TSE to provide auxiliary linguistic constraints.

Method: Proposes incorporating linguistic constraints from PSLMs/PLMs as additional supervision signals for AV-TSE models.

Result: Consistent improvements in speech quality and intelligibility, with robust performance in multi-language and visual cue-impaired scenarios.

Conclusion: Leveraging PSLMs/PLMs enhances AV-TSE effectively, demonstrating broad applicability and robustness.

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [366] [Fractional Fourier Sound Synthesis](https://arxiv.org/pdf/2506.09189)
*Esteban Gutiérrez, Rodrigo Cádiz, Carlos Sing Long, Frederic Font, Xavier Serra*

Main category: cs.SD

TL;DR: The paper discusses using the Fractional Fourier Transform (FrFT) for sound synthesis, offering new time-frequency analysis and sound design techniques.


<details>
  <summary>Details</summary>
Motivation: To explore FrFT's potential in audio processing, enabling unique sound synthesis methods beyond traditional Fourier Transform.

Method: Mathematical analysis of FrFT, historical context, and experimental techniques like alpha-synthesis and alpha-filtering.

Result: Demonstrated innovative sound design capabilities, showcasing FrFT's flexibility in creating complex audio textures.

Conclusion: FrFT is a transformative tool for advancing auditory creativity in music and sound design.

Abstract: This paper explores the innovative application of the Fractional Fourier
Transform (FrFT) in sound synthesis, highlighting its potential to redefine
time-frequency analysis in audio processing. As an extension of the classical
Fourier Transform, the FrFT introduces fractional order parameters, enabling a
continuous interpolation between time and frequency domains and unlocking
unprecedented flexibility in signal manipulation. Crucially, the FrFT also
opens the possibility of directly synthesizing sounds in the alpha-domain,
providing a unique framework for creating timbral and dynamic characteristics
unattainable through conventional methods. This work delves into the
mathematical principles of the FrFT, its historical evolution, and its
capabilities for synthesizing complex audio textures. Through experimental
analyses, we showcase novel sound design techniques, such as alpha-synthesis
and alpha-filtering, which leverage the FrFT's time-frequency rotation
properties to produce innovative sonic results. The findings affirm the FrFT's
value as a transformative tool for composers, sound designers, and researchers
seeking to push the boundaries of auditory creativity.

</details>


### [367] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/pdf/2506.09206)
*Ahmed Adel Attia, Jing Liu, Carl Espy-Wilson*

Main category: cs.SD

TL;DR: The paper introduces SimClass, a dataset with synthesized classroom noise and simulated speech, addressing the lack of large-scale classroom speech data for AI education models.


<details>
  <summary>Details</summary>
Motivation: The scarcity of public classroom datasets and dedicated noise corpora limits AI-driven speech model development for education.

Method: A scalable methodology using game engines synthesizes classroom noise. SimClass pairs a children's speech corpus with YouTube lectures to simulate classroom speech.

Result: SimClass closely approximates real classroom speech in experiments, proving useful for robust speech recognition and enhancement models.

Conclusion: SimClass provides a valuable resource for developing AI-driven speech models in educational settings.

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [368] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/pdf/2506.08524)
*Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu*

Main category: cs.SD

TL;DR: ACORN teaches LLMs physical awareness through sound, using a physics-based simulator to generate training data and a specialized audio encoder.


<details>
  <summary>Details</summary>
Motivation: LLMs lack physical awareness, limiting their understanding of real-world phenomena. ACORN addresses this gap by focusing on sound-based physical phenomena.

Method: ACORN uses a physics-based simulator to create diverse training data (AQA-PHY dataset) and an audio encoder processing magnitude and phase. It connects this encoder to LLMs.

Result: ACORN achieves reasonable performance in tasks like line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation.

Conclusion: ACORN advances LLMs' physical awareness, demonstrating potential for real-world applications.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [369] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/pdf/2506.09448)
*Yui Sudo, Yusuke Fujita, Atsushi Kojima, Tomoya Mizumoto, Lianbo Liu*

Main category: cs.SD

TL;DR: Integrating contextual biasing with OWSM v3.1 improves rare word recognition without retraining the model, boosting performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: SFMs struggle with rare/unseen words, and existing CB methods lack pre-trained knowledge, limiting their performance.

Method: Freezes OWSM v3.1's parameters and integrates a CB method to leverage SFM knowledge for better rare word recognition.

Result: 11.6-point B-WER improvement, 0.9-point overall WER gain, and 7.5% faster processing on LibriSpeech 100 test-clean.

Conclusion: The approach effectively combines CB with SFMs, enhancing rare word recognition while maintaining SFM advantages.

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [370] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/pdf/2506.09487)
*Taesoo Park, Mungwi Jeong, Mingyu Park, Narae Kim, Junyoung Kim, Mujung Kim, Jisang Yoo, Hoyun Lee, Sanghoon Kim, Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2 is an advanced GAN-based vocoder with architectural innovations like AMP modules and MED discriminators for high-fidelity audio generation.


<details>
  <summary>Details</summary>
Motivation: To improve audio generation quality by addressing periodicity modeling and long-range dependencies in GAN-based vocoders.

Method: Introduces AMP modules in the generator and MED discriminators, combined with MRD, for better audio modeling. Evaluated using objective and subjective metrics.

Result: Demonstrates improved audio generation quality through systematic evaluation and comparisons of discriminator configurations.

Conclusion: BemaGANv2 offers a reproducible, high-fidelity solution for audio generation, with code and models publicly available.

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [371] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/pdf/2506.09709)
*Alexander Lobashev, Assel Yermekova, Maria Larchenko*

Main category: cs.SD

TL;DR: Factorized MKL-VC improves cross-lingual voice conversion with 5-second reference audio by replacing kNN regression with a factorized optimal transport map, outperforming kNN-VC and matching FACodec.


<details>
  <summary>Details</summary>
Motivation: To enhance the kNN-VC pipeline for high-quality any-to-any cross-lingual voice conversion with minimal reference audio.

Method: Replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, addressing non-uniform variance.

Result: Significantly improves content preservation and robustness, especially with short reference audio, outperforming kNN-VC and matching FACodec.

Conclusion: Factorized MKL-VC is effective for cross-lingual voice conversion, offering superior performance with minimal reference audio.

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [372] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/pdf/2506.09874)
*Neta Glazer, Aviv Navon, Yael Segal, Aviv Shamsian, Hilit Segev, Asaf Buchnick, Menachem Pirchi, Gil Hetz, Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS is a flow-matching TTS model that generates speech and environmental audio together, addressing the lack of paired data with a self-supervised framework.


<details>
  <summary>Details</summary>
Motivation: Integrating speech with complex background environments is challenging due to the lack of aligned data.

Method: Proposes UmbraTTS, a flow-matching model, and a self-supervised framework to extract speech, background, and transcripts from unannotated recordings.

Result: UmbraTTS outperforms baselines, producing natural, high-quality, environmentally aware audio.

Conclusion: The model enables fine-grained control and diverse, context-aware audio scenes, advancing TTS in complex environments.

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


### [373] [Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers](https://arxiv.org/pdf/2309.09652)
*Peter Ochieng*

Main category: cs.SD

TL;DR: UDPNet accelerates speech synthesis by unrolling the reverse diffusion process into its architecture, predicting latent variables to reduce distortion, and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion models in speech synthesis suffer from timestep embeddings and shared parameters, leading to inefficiencies and distortion. UDPNet aims to address these issues.

Method: UDPNet unrolls the reverse diffusion process into its architecture, with layers corresponding to diffusion steps. It predicts latent variables instead of conventional targets like noise or original data.

Result: UDPNet outperforms state-of-the-art methods in quality and efficiency, reducing speech distortion and generalizing well to unseen speech.

Conclusion: UDPNet is a robust solution for real-time speech synthesis, offering improved performance and generalization.

Abstract: This work introduces UDPNet, a novel architecture designed to accelerate the
reverse diffusion process in speech synthesis. Unlike traditional diffusion
models that rely on timestep embeddings and shared network parameters, UDPNet
unrolls the reverse diffusion process directly into the network architecture,
with successive layers corresponding to equally spaced steps in the diffusion
schedule. Each layer progressively refines the noisy input, culminating in a
high-fidelity estimation of the original data, \(x_0\). Additionally, we
redefine the learning target by predicting latent variables instead of the
conventional \(x_0\) or noise \(\epsilon_0\). This shift addresses the common
issue of large prediction errors in early denoising stages, effectively
reducing speech distortion. Extensive evaluations on single- and multi-speaker
datasets demonstrate that UDPNet consistently outperforms state-of-the-art
methods in both quality and efficiency, while generalizing effectively to
unseen speech. These results position UDPNet as a robust solution for real-time
speech synthesis applications. Sample audio is available at
https://onexpeters.github.io/UDPNet.

</details>


### [374] [Annotation-Free MIDI-to-Audio Synthesis via Concatenative Synthesis and Generative Refinement](https://arxiv.org/pdf/2410.16785)
*Osamu Take, Taketo Akama*

Main category: cs.SD

TL;DR: CoSaRef is a MIDI-to-audio synthesis method that avoids MIDI-audio paired datasets, using concatenative synthesis and diffusion models for diverse, high-quality outputs with fine-grained control.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on MIDI annotations, limiting timbre and expression diversity. CoSaRef aims to overcome this by leveraging unannotated datasets.

Method: CoSaRef combines concatenative synthesis (for initial audio generation) with a diffusion-based model (for refinement), trained without MIDI annotations.

Result: CoSaRef produces realistic tracks with diverse timbres and outperforms MIDI-supervised methods in evaluations.

Conclusion: CoSaRef offers a flexible, high-quality alternative to MIDI-dependent synthesis, enabling detailed control without annotation constraints.

Abstract: Recent MIDI-to-audio synthesis methods using deep neural networks have
successfully generated high-quality, expressive instrumental tracks. However,
these methods require MIDI annotations for supervised training, limiting the
diversity of instrument timbres and expression styles in the output. We propose
CoSaRef, a MIDI-to-audio synthesis method that does not require MIDI-audio
paired datasets. CoSaRef first generates a synthetic audio track using
concatenative synthesis based on MIDI input, then refines it with a
diffusion-based deep generative model trained on datasets without MIDI
annotations. This approach improves the diversity of timbres and expression
styles. Additionally, it allows detailed control over timbres and expression
through audio sample selection and extra MIDI design, similar to traditional
functions in digital audio workstations. Experiments showed that CoSaRef could
generate realistic tracks while preserving fine-grained timbre control via
one-shot samples. Moreover, despite not being supervised on MIDI annotation,
CoSaRef outperformed the state-of-the-art timbre-controllable method based on
MIDI supervision in both objective and subjective evaluation.

</details>


### [375] [AAD-LLM: Neural Attention-Driven Auditory Scene Understanding](https://arxiv.org/pdf/2502.16794)
*Xilin Jiang, Sukru Samet Dindar, Vishal Choudhari, Stephan Bickel, Ashesh Mehta, Guy M McKhann, Daniel Friedman, Adeen Flinker, Nima Mesgarani*

Main category: cs.SD

TL;DR: The paper introduces Intention-Informed Auditory Scene Understanding (II-ASU) and Auditory Attention-Driven LLM (AAD-LLM), a system that uses brain signals to align AI responses with human auditory attention.


<details>
  <summary>Details</summary>
Motivation: Current auditory models lack human-like selective attention, limiting their ability to generate perception-aligned responses.

Method: AAD-LLM integrates intracranial EEG (iEEG) to decode listener attention and refines responses based on inferred attentional states.

Result: AAD-LLM improves performance in speaker description, transcription, and question answering, aligning better with listener intention.

Conclusion: This work pioneers intention-aware auditory AI, paving the way for listener-centered auditory systems.

Abstract: Auditory foundation models, including auditory large language models (LLMs),
process all sound inputs equally, independent of listener perception. However,
human auditory perception is inherently selective: listeners focus on specific
speakers while ignoring others in complex auditory scenes. Existing models do
not incorporate this selectivity, limiting their ability to generate
perception-aligned responses. To address this, we introduce Intention-Informed
Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM
(AAD-LLM), a prototype system that integrates brain signals to infer listener
attention. AAD-LLM extends an auditory LLM by incorporating intracranial
electroencephalography (iEEG) recordings to decode which speaker a listener is
attending to and refine responses accordingly. The model first predicts the
attended speaker from neural activity, then conditions response generation on
this inferred attentional state. We evaluate AAD-LLM on speaker description,
speech transcription and extraction, and question answering in multitalker
scenarios, with both objective and subjective ratings showing improved
alignment with listener intention. By taking a first step toward
intention-aware auditory AI, this work explores a new paradigm where listener
perception informs machine listening, paving the way for future
listener-centered auditory systems. Demo and code available:
https://aad-llm.github.io.

</details>


### [376] [Weakly Supervised Multiple Instance Learning for Whale Call Detection and Temporal Localization in Long-Duration Passive Acoustic Monitoring](https://arxiv.org/pdf/2502.20838)
*Ragib Amin Nihal, Benjamin Yen, Runwu Shi, Kazuhiro Nakadai*

Main category: cs.SD

TL;DR: DSMIL-LocNet uses Multiple Instance Learning for whale call detection and localization with bag-level labels, improving classification and localization in marine monitoring.


<details>
  <summary>Details</summary>
Motivation: Deep learning for marine monitoring often needs precise annotations, which DSMIL-LocNet avoids by using bag-level labels.

Method: A dual-stream model processes 2-30 minute audio segments, combining spectral and temporal features with attention-based instance selection.

Result: Tests show improved classification (F1: 0.8-0.9) and localization precision (0.65-0.70) with longer contexts and medium instances.

Conclusion: MIL frameworks like DSMIL-LocNet can enhance scalable marine ecosystem monitoring.

Abstract: Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates
vast data, but deep learning often requires precise annotations and short
segments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for
whale call detection and localization using only bag-level labels. Our
dual-stream model processes 2-30 minute audio segments, leveraging spectral and
temporal features with attention-based instance selection. Tests on Antarctic
whale data show longer contexts improve classification (F1: 0.8-0.9) while
medium instances ensure localization precision (0.65-0.70). This suggests MIL
can enhance scalable marine monitoring. Code:
https://github.com/Ragib-Amin-Nihal/DSMIL-Loc

</details>


### [377] [An introduction to pitch strength in contemporary popular music analysis and production](https://arxiv.org/pdf/2506.07473)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: The paper explores pitch strength as a low-level feature to enhance generative AI models for music production, highlighting its variability and structural impact in songs.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between high-level text descriptions in AI models and the low-level controls used by studio musicians, focusing on pitch strength as a key perceptual parameter.

Method: Signal and perceptual analyses were conducted to examine pitch strength's variability, structural contributions, and role in polyphonic dissonance.

Result: Pitch strength varies significantly in songs, influences structure, aids in handling dissonance, and may relate to upper harmonics in perceptual richness.

Conclusion: Pitch strength is a promising low-level feature for improving AI music models, aligning them better with practical music production needs.

Abstract: Music information retrieval distinguishes between low- and high-level
descriptions of music. Current generative AI models rely on text descriptions
that are higher level than the controls familiar to studio musicians. Pitch
strength, a low-level perceptual parameter of contemporary popular music, may
be one feature that could make such AI models more suited to music production.
Signal and perceptual analyses suggest that pitch strength (1) varies
significantly across and inside songs; (2) contributes to both small- and
large-scale structure; (3) contributes to the handling of polyphonic
dissonance; and (4) may be a feature of upper harmonics made audible in a
perspective of perceptual richness.

</details>


### [378] [Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A Proposal for Offline Speech Recognition and IoT Integration](https://arxiv.org/pdf/2506.07494)
*Peng Huang, Imdad Ullah, Xiaotong Wei, Tariq Ahamed Ahanger, Najm Hassan, Zawar Hussain Shah*

Main category: cs.SD

TL;DR: Proposes a smart home system using offline speech recognition and IoT to reduce latency, energy use, and dependency on cloud services.


<details>
  <summary>Details</summary>
Motivation: Existing cloud-based AI speech recognition in smart homes causes energy waste, latency, and single-point failures.

Method: Integrates offline keyword spotting (KWS) into appliances and designs a decentralized local IoT network.

Result: Enables low-latency, internet-independent voice control with improved scalability and energy efficiency.

Conclusion: Offline speech recognition and IoT enhance smart home robustness and sustainability.

Abstract: The smart home systems, based on AI speech recognition and IoT technology,
enable people to control devices through verbal commands and make people's
lives more efficient. However, existing AI speech recognition services are
primarily deployed on cloud platforms on the Internet. When users issue a
command, speech recognition devices like ``Amazon Echo'' will post a recording
through numerous network nodes, reach multiple servers, and then receive
responses through the Internet. This mechanism presents several issues,
including unnecessary energy consumption, communication latency, and the risk
of a single-point failure. In this position paper, we propose a smart home
concept based on offline speech recognition and IoT technology: 1) integrating
offline keyword spotting (KWS) technologies into household appliances with
limited resource hardware to enable them to understand user voice commands; 2)
designing a local IoT network with decentralized architecture to manage and
connect various devices, enhancing the robustness and scalability of the
system. This proposal of a smart home based on offline speech recognition and
IoT technology will allow users to use low-latency voice control anywhere in
the home without depending on the Internet and provide better scalability and
energy sustainability.

</details>


### [379] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/pdf/2506.08570)
*Or Tal, Felix Kreuk, Yossi Adi*

Main category: cs.SD

TL;DR: The paper compares Auto-Regressive decoding and Conditional Flow-Matching paradigms in text-to-music generation, isolating their effects to guide future system designs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating diverse SOTA systems in text-to-music generation and identify the impact of modeling paradigms on performance.

Method: A controlled comparison of Auto-Regressive decoding and Conditional Flow-Matching using identical datasets, training configurations, and similar architectures.

Result: The study highlights distinct strengths and limitations of each paradigm, evaluating generation quality, robustness, scalability, conditioning adherence, and editing capabilities.

Conclusion: The findings provide actionable insights for future text-to-music generation systems, emphasizing trade-offs and emergent behaviors of the two paradigms.

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [380] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/pdf/2506.09052)
*Delower Hossain, Ehsan Saghapour, Kevin Song, Jake Y. Chen*

Main category: cs.LG

TL;DR: The paper introduces LlamaAffinity, an AI-based model for predicting antibody-antigen binding affinity, outperforming existing methods with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional experimental methods for antibody affinity measurement are costly and slow, prompting the need for AI-driven solutions.

Method: The model uses a Llama 3 backbone and antibody sequence data from the OAS database to predict binding affinity.

Result: LlamaAffinity achieved superior metrics (e.g., 0.9640 accuracy, 0.9936 AUC-ROC) and faster training (0.46 hours).

Conclusion: The model advances AI-based antibody design, offering a faster, more accurate alternative to traditional methods.

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [381] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/pdf/2506.05683)
*Fardis Nadimi, Payam Abdisarabshali, Kasra Borazjani, Jacob Chakareski, Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: The paper proposes using multi-modal multi-task federated foundation models (FedFMs) to enhance XR systems, addressing challenges under the SHIFT framework and outlining evaluation metrics and design tradeoffs.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of foundation models with federated learning for privacy-preserving, intelligent XR systems.

Method: A modular architecture for FedFMs with coordination paradigms for training and aggregation, addressing XR challenges under the SHIFT dimensions.

Result: Identifies key challenges (SHIFT dimensions) and proposes metrics, dataset requirements, and tradeoffs for FedFMs in XR.

Conclusion: Lays the groundwork for context-aware, privacy-preserving intelligence in next-gen XR systems.

Abstract: Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [382] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/pdf/2506.09080)
*Jiaxiang Chen, Mingxi Zou, Zhuo Wang, Qifan Wang, Dongning Sun, Chi Zhang, Zenglin Xu*

Main category: cs.LG

TL;DR: FinHEAR, a multi-agent framework, enhances financial decision-making by combining LLMs with behavioral economics principles, outperforming baselines in accuracy and risk-adjusted returns.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in capturing human financial decision-making behaviors like expert reliance, loss aversion, and adaptive reasoning.

Method: FinHEAR uses specialized LLM-based agents for historical trend analysis, current event interpretation, and expert-informed precedent retrieval in an event-centric pipeline.

Result: Empirical results show FinHEAR outperforms baselines in trend prediction and trading tasks, achieving higher accuracy and better risk-adjusted returns.

Conclusion: FinHEAR effectively integrates human expertise and adaptive risk-aware reasoning, improving financial decision-making for language models.

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [383] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/pdf/2506.09084)
*Xinyuan Wang, Liang Wu, Yanjie Fu*

Main category: cs.LG

TL;DR: PageLLM optimizes search/recommendation results using user feedback for fine-tuning LLMs, combining page-level and item-level rewards to outperform baselines and boost GMV.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for Whole Page Optimization (WPO) is costly due to the need for extensive human-annotated data. User feedback offers a scalable but noisy alternative.

Method: Proposes PageLLM, a reward-based fine-tuning approach with mixed-grained rewards (page-level for coherence, item-level for relevance).

Result: Validated on public/industrial datasets, PageLLM outperforms baselines and achieves a 0.44% GMV increase in an A/B test with 10M+ users.

Conclusion: PageLLM effectively leverages noisy user feedback for WPO, demonstrating real-world impact.

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [384] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/pdf/2506.09085)
*Xinyuan Wang, Haoyue Bai, Nanxu Gong, Wangyang Ying, Sixun Dong, Xiquan Cui, Yanjie Fu*

Main category: cs.LG

TL;DR: A teaming framework combines LLMs and ML for stable and valid feature transformation, improving downstream performance by 5% and reducing errors by half.


<details>
  <summary>Details</summary>
Motivation: Existing methods (traditional ML and LLMs) fail to address both stable generation and valid generation in feature transformation.

Method: A four-step framework: golden examples generation, feature transformation sequence embedding/search, student LLM feature transformation, and LLM-ML decoder teaming.

Result: 5% improvement in downstream performance, nearly half the error cases, and demonstrated efficiency/robustness.

Conclusion: The teaming policy effectively bridges the gap between LLMs and ML, enhancing feature transformation while uncovering LLMs' data understanding capacity.

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [385] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/pdf/2506.09087)
*Sophie Jaffard, Giulia Mezzadri, Patricia Reynaud-Bouret, Etienne Tanré*

Main category: cs.LG

TL;DR: The paper proposes a biologically plausible Spiking Neural Network (SNN) model for decision-making, bridging cognitive and biological models by incorporating learning mechanisms and using a multivariate Hawkes process for neuron activity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of learning mechanisms in existing models (DDM and Poisson counter) and integrate biologically relevant neural mechanisms into cognitive models.

Method: Develops an SNN model with a learning mechanism, models neuron activities using a multivariate Hawkes process, and shows coupling between DDM and Poisson counter models.

Result: Demonstrates that DDM can be approximated by spiking Poisson neurons and derives a DDM with correlated noise from a Hawkes network.

Conclusion: The work advances the integration of neural mechanisms into cognitive models, enhancing understanding of neural activity and behavior.

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [386] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/pdf/2506.09090)
*Arthur Oghlukyan, Nuria Gomez Blas*

Main category: cs.LG

TL;DR: An enhanced asynchronous AdaBoost framework for federated learning (FL) improves efficiency and robustness across five domains, reducing training time by 20-35% and communication overhead by 30-40% compared to baseline.


<details>
  <summary>Details</summary>
Motivation: To address synchronization and communication inefficiencies in federated learning while maintaining or improving model accuracy.

Method: Incorporates adaptive communication scheduling and delayed weight compensation to reduce synchronization frequency and overhead.

Result: Achieves significant reductions in training time (20-35%) and communication overhead (30-40%), with faster convergence and maintained accuracy.

Conclusion: The enhanced AdaBoost framework is highly efficient and robust, demonstrating broad applicability in diverse FL scenarios.

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [387] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/pdf/2506.09091)
*Kenric Nelson, Igor Oliveira, Amenah Al-Najafi, Fode Zhang, Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: The paper introduces a coupled free energy framework for variational inference, improving accuracy and robustness by accounting for heavy-tailed distributions like generalized Pareto and Student's t. Applied to a coupled variational autoencoder (CVAE), it shows a 3% improvement over standard VAE.


<details>
  <summary>Details</summary>
Motivation: To extend variational inference techniques to handle heavy-tailed distributions and improve model robustness by leveraging coupled free energy.

Method: Uses coupled free energy and ELBO for variational inference, generalizes Fisher Information metric, and designs a CVAE with modified reconstruction metrics.

Result: The CVAE achieves a 3% improvement in Wasserstein-2 or Fréchet Inception Distance over standard VAE on CelebA images after 5 epochs.

Conclusion: The framework successfully enhances variational inference for heavy-tailed distributions, demonstrating practical benefits in model training and outlier reduction.

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [388] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/pdf/2506.09092)
*Wentao Chen, Jiace Zhu, Qi Fan, Yehan Ma, An Zou*

Main category: cs.LG

TL;DR: LLMs augmented with FSR framework generate and optimize CUDA programs, achieving high-performance GPU kernels with up to 179x speedup over human-written code.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating hardware-specific, architecture-aware, and performance-critical GPU code using LLMs.

Method: Proposed Feature Search and Reinforcement (FSR) framework to jointly optimize compilation, correctness, and runtime performance.

Result: FSR ensures correctness and achieves up to 179x speedup in execution speeds compared to human-written code.

Conclusion: Combining LLMs with performance reinforcement like FSR can automate GPU programming for high-performance applications.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [389] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/pdf/2506.09093)
*Bingjie Zhang, Hongkang Li, Changlong Shi, Guowei Rong, He Zhao, Dongsheng Wang, Dandan Guo, Meng Wang*

Main category: cs.LG

TL;DR: LwPTV (Layer-wise Pruning Task Vector) improves multi-task learning by pruning redundant parameters in task vectors, enhancing out-of-domain performance while maintaining in-domain effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current model merging methods for multi-task learning focus on in-domain performance, neglecting out-of-domain efficacy.

Method: Proposes LwPTV, which uses a saliency score to measure parameter redundancy, enabling layer-wise pruning of task vectors.

Result: LwPTV significantly improves out-of-domain performance without compromising in-domain results.

Conclusion: LwPTV is a flexible and effective method for enhancing multi-task learning across diverse domains.

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [390] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/pdf/2506.09096)
*Chaoyang Zhou, Shunyu Liu, Zengmao Wang, Di Wang, Rong-Cheng Tu, Bo Du, Dacheng Tao*

Main category: cs.LG

TL;DR: The paper proposes using generation probabilities to improve reward models for LLMs by ensuring reward consistency across response processes, enhancing generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Current reward models rely on coarse-grained response-level scores, which struggle to identify specific components correlating with rewards, leading to poor generalization.

Method: The paper introduces intra-trajectory consistency regularization, leveraging generation probabilities to propagate fine-grained signals for reward learning.

Result: The proposed method improves performance on RewardBench, enhances DPO-aligned policies, and achieves better best-of-N inference-time verification results.

Conclusion: The approach effectively refines reward models by incorporating fine-grained signals, demonstrating improved generalization and performance in LLMs.

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [391] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/pdf/2506.09099)
*Joshua Barron, Devin White*

Main category: cs.LG

TL;DR: The paper explores the trade-off between memorization and generalization in LLMs, showing that model size influences whether a model excels at memorization or generalization, with no model succeeding at both when tasks are combined.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between memorization and generalization in LLMs, as their interplay remains unclear despite growing evidence of their connection.

Method: Pre-training capacity-limited Transformer models on synthetic tasks: one for generalization (arithmetic extrapolation) and one for memorization (factual recall).

Result: Small models generalize but fail to memorize; larger models memorize but fail to generalize. Intermediate models shift toward memorization. No model succeeds at both tasks when combined.

Conclusion: Pre-training may inherently favor one learning mode over the other, with implications for designing small language models.

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [392] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/pdf/2410.16222)
*Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping*

Main category: cs.LG

TL;DR: A unified threat model is proposed to evaluate jailbreaking attacks on LLMs, revealing lower success rates than previously thought and highlighting the superiority of discrete optimization-based attacks.


<details>
  <summary>Details</summary>
Motivation: To provide a principled comparison of jailbreaking attacks on safety-tuned LLMs, addressing variability in fluency and computational effort.

Method: Develop an N-gram language model on 1T tokens for LLM-agnostic, nonparametric, and interpretable evaluation. Adapt and benchmark popular attacks under this model.

Result: Attack success rates are lower than previously reported; discrete optimization-based attacks outperform LLM-based ones. Effective attacks exploit rare or infrequent bigrams.

Conclusion: The threat model enables interpretable analysis, showing that successful attacks rely on exploiting uncommon text patterns.

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [393] [Feature Shift Localization Network](https://arxiv.org/pdf/2506.09101)
*Míriam Barrabés, Daniel Mas Montserrat, Kapal Dev, Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: FSL-Net is a neural network designed to accurately and scalably localize feature shifts in large, high-dimensional datasets without requiring re-training.


<details>
  <summary>Details</summary>
Motivation: Feature shifts in heterogeneous data sources degrade downstream analysis, but existing solutions for localizing these shifts are either inaccurate or unscalable.

Method: FSL-Net is trained on diverse datasets to learn statistical properties, enabling it to localize shifts in unseen datasets without re-training.

Result: FSL-Net provides fast and accurate feature shift localization in large, high-dimensional datasets.

Conclusion: FSL-Net offers a scalable and efficient solution for localizing feature shifts, addressing a critical gap in data analysis.

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [394] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/pdf/2506.09104)
*Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, An Chen*

Main category: cs.LG

TL;DR: UPQ is a novel framework for quantizing instruction-tuned LLMs to INT2, combining block-wise PTQ and Distill-QAT, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the gap in extending low-bit quantization to instruction-tuned LLMs, enabling deployment on resource-constrained devices.

Method: Progressive quantization (FP16→INT4→INT2) with block-wise PTQ and Distill-QAT, minimizing JSD divergence.

Result: Achieves state-of-the-art performance on MMLU and IFEval benchmarks without proprietary data.

Conclusion: UPQ successfully quantizes instruction-tuned LLMs to INT2, offering a practical solution for resource-limited deployments.

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [395] [Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations](https://arxiv.org/pdf/2312.04540)
*Ahmad Rahimi, Po-Chien Luan, Yuejiang Liu, Frano Rajič, Alexandre Alahi*

Main category: cs.LG

TL;DR: The paper critiques current representations of spatial-temporal agent interactions, introduces a metric learning approach for causal awareness, and proposes a sim-to-real transfer method, showing improved robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the causal awareness of modern representations in modeling spatial-temporal agent interactions, addressing gaps in resilience to perturbations and indirect causal effects.

Method: Introduces a metric learning approach to regularize latent representations with causal annotations and a sim-to-real transfer method via cross-domain multi-task learning.

Result: The approach enhances causal awareness and out-of-distribution robustness, with experiments on pedestrian datasets demonstrating improved generalization even without real-world causal annotations.

Conclusion: The work offers insights into achieving causally-aware representations for multi-agent interactions, with practical implications for motion forecasting and crowd navigation.

Abstract: Modeling spatial-temporal interactions among neighboring agents is at the
heart of multi-agent problems such as motion forecasting and crowd navigation.
Despite notable progress, it remains unclear to which extent modern
representations can capture the causal relationships behind agent interactions.
In this work, we take an in-depth look at the causal awareness of these
representations, from computational formalism to real-world practice. First, we
cast doubt on the notion of non-causal robustness studied in the recent
CausalAgents benchmark. We show that recent representations are already
partially resilient to perturbations of non-causal agents, and yet modeling
indirect causal effects involving mediator agents remains challenging. To
address this challenge, we introduce a metric learning approach that
regularizes latent representations with causal annotations. Our controlled
experiments show that this approach not only leads to higher degrees of causal
awareness but also yields stronger out-of-distribution robustness. To further
operationalize it in practice, we propose a sim-to-real causal transfer method
via cross-domain multi-task learning. Experiments on pedestrian datasets show
that our method can substantially boost generalization, even in the absence of
real-world causal annotations. We hope our work provides a new perspective on
the challenges and pathways towards causally-aware representations of
multi-agent interactions. Our code is available at
https://github.com/vita-epfl/CausalSim2Real.

</details>


### [396] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2506.09105)
*Javier Lopez-Piqueres, Pranav Deshpande, Archan Ray, Mattia J. Villani, Marco Pistoia, Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT is a Tensor Train adapter framework for efficient low-rank fine-tuning of transformers, outperforming LoRA and other methods in parameter reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of independent fine-tuning (like LoRA) by unifying transformer sub-modules into a single shared Tensor Train structure.

Method: MetaTT factorizes all transformer sub-modules using a shared Tensor Train, indexing structural axes like layer and matrix type, and optionally heads and tasks.

Result: MetaTT reduces parameters significantly compared to LoRA while maintaining similar accuracy and outperforming other tensor-based methods.

Conclusion: MetaTT offers a scalable, efficient fine-tuning solution with mature optimization routines and natural extensibility to multi-task scenarios.

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [397] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/pdf/2506.09108)
*Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed A. Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM is a sensor-language foundation model for interpreting wearable sensor data with natural language, achieving superior performance in tasks like activity analysis and healthcare.


<details>
  <summary>Details</summary>
Motivation: Aligning and interpreting sensor data with language is challenging due to the lack of annotated datasets in real-world wearable data.

Method: A hierarchical caption generation pipeline captures statistical, structural, and semantic information, creating a large sensor-language dataset. SensorLM extends multimodal pretraining architectures like CLIP and CoCa.

Result: SensorLM outperforms state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval, showing capabilities like scaling and label efficiency.

Conclusion: SensorLM advances sensor-language understanding, demonstrating strong performance and generalization to unseen tasks.

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [398] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/pdf/2506.09110)
*Jingying Ma, Feng Wu, Qika Lin, Yucheng Xing, Chenyu Liu, Ziyu Jia, Mengling Feng*

Main category: cs.LG

TL;DR: CodeBrain is an EEG foundation model addressing limitations in generalizability and multi-scale dependency capture by using a TFDual-Tokenizer and EEGSSM, achieving strong performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional EEG models lack transferability due to configuration variations. EFMs struggle with heterogeneous representation and multi-scale dependencies.

Method: Two-stage training: (1) TFDual-Tokenizer for temporal/frequency tokenization, (2) EEGSSM combining global convolution and sliding window attention for multi-scale dependency modeling.

Result: CodeBrain demonstrates generalizability on 10 EEG datasets via linear probing, outperforming traditional models.

Conclusion: CodeBrain provides biologically informed, interpretable EEG modeling, advancing neuroscience research.

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [399] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/pdf/2506.09114)
*Jialin Chen, Ziyu Zhao, Gaukhar Nurbek, Aosong Feng, Ali Maatouk, Leandros Tassiulas, Yifeng Gao, Rex Ying*

Main category: cs.LG

TL;DR: TRACE is a multimodal retriever for time-series data, enabling cross-modal retrieval and improving downstream tasks with semantic grounding and alignment.


<details>
  <summary>Details</summary>
Motivation: The need for effective interpretation and retrieval of time-series data, which is often tied to domain-specific contexts, is growing. Existing methods lack semantic grounding and struggle with heterogeneous modalities.

Method: TRACE grounds time-series embeddings in aligned textual context, uses hard negative mining, and supports flexible cross-modal retrieval modes (Text-to-Timeseries and Timeseries-to-Text).

Result: TRACE improves predictive accuracy and interpretability, achieving state-of-the-art performance on forecasting and classification tasks.

Conclusion: TRACE serves as both a powerful encoder for downstream tasks and a general-purpose retriever, enhancing time-series models.

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [400] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/pdf/2506.09163)
*Daniel Jenson, Jhonathan Navott, Piotr Grynfelder, Mengyan Zhang, Makkunda Sharma, Elizaveta Semenova, Seth Flaxman*

Main category: cs.LG

TL;DR: BSA-TNP is a scalable and accurate Neural Process model with translation invariance, efficient training, and high-dimensional support.


<details>
  <summary>Details</summary>
Motivation: Address the tradeoff between accuracy and scalability in Neural Processes, especially for translation-invariant processes.

Method: Introduces BSA-TNP with Kernel Regression Blocks, group-invariant attention biases, and Biased Scan Attention.

Result: Matches/exceeds accuracy of top models, trains faster, handles high dimensions, and scales to 1M test points.

Conclusion: BSA-TNP eliminates unnecessary tradeoffs, offering versatility and efficiency for complex applications.

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [401] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/pdf/2506.09171)
*Samuel Holt, Max Ruiz Luyten, Thomas Pouplin, Mihaela van der Schaar*

Main category: cs.LG

TL;DR: A novel LLM agent framework enhances planning via in-context learning, atomic fact augmentation, and recursive lookahead search, improving adaptability and performance in interactive tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with adapting to new information and multi-step reasoning without fine-tuning, requiring a more efficient method for leveraging past experiences.

Method: The framework uses atomic fact augmentation and recursive lookahead search to dynamically improve prompts for action proposal, world simulation, and state-value estimation.

Result: The agent shows improved performance and adaptability in tasks like TextFrozenLake and ALFWorld, refining behavior without weight updates.

Conclusion: The proposed framework effectively enhances LLM planning and decision-making in interactive environments by leveraging experience and dynamic fact-based augmentation.

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [402] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/pdf/2506.09172)
*Pranav Guruprasad, Yangyue Wang, Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet is an open-source benchmark for evaluating multimodal action models (VLMs/VLAs) with standardized protocols, a large dataset, and tools for research.


<details>
  <summary>Details</summary>
Motivation: To advance general-purpose agentic systems by rigorously evaluating and adapting models across vision, language, and action domains.

Method: Introduces MultiNet, a benchmark with standardized evaluation protocols, open-source software, and a composite dataset (1.3T tokens) for diverse tasks.

Result: MultiNet supports downstream research on VLA generalization limitations.

Conclusion: MultiNet provides a comprehensive framework for evaluating and improving multimodal action models.

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [403] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/pdf/2506.09173)
*Michael Cooper, Rohan Wadhawan, John Michael Giorgi, Chenhao Tan, Davis Liang*

Main category: cs.LG

TL;DR: CuriosiTree is a heuristic-based policy for zero-shot information acquisition in LLMs, using greedy tree search to balance information gain and cost, outperforming baselines in clinical diagnosis.


<details>
  <summary>Details</summary>
Motivation: Decision-makers often lack sufficient information and need cost-effective ways to acquire it, especially in scenarios like clinical diagnosis.

Method: CuriosiTree employs a greedy tree search to estimate expected information gain and strategically selects actions based on gain-cost balance.

Result: Empirical validation shows CuriosiTree outperforms baselines in selecting cost-effective action sequences for accurate diagnosis.

Conclusion: CuriosiTree effectively integrates diverse information sources, enhancing decision-making in information-scarce scenarios.

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [404] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/pdf/2506.09183)
*Mingkang Wu, Devin White, Evelyn Rose, Vernon Lawhern, Nicholas R Waytowich, Yongcan Cao*

Main category: cs.LG

TL;DR: A novel RL method mimics human decision-making by jointly considering multiple tasks, outperforming existing rating-based RL methods.


<details>
  <summary>Details</summary>
Motivation: Current RLHF approaches oversimplify human reasoning by isolating tasks, missing the complexity of human decision-making.

Method: Proposes a reward-free RL method using human ratings, with learnable weights balancing classification and regression models.

Result: Outperforms existing rating-based RL methods and sometimes traditional RL approaches.

Conclusion: The method effectively captures human decision-making uncertainty and adapts strategies, improving alignment with user goals.

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [405] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/pdf/2506.09174)
*Chenheng Xu, Dan Wu, Yixin Zhu, Ying Nian Wu*

Main category: cs.LG

TL;DR: The paper introduces FNF and DBD as dedicated backbones for spatio-temporal modeling in multivariate time series forecasting, addressing limitations of current domain-agnostic approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods repurpose NLP/CV backbones (e.g., Transformers) without addressing time series-specific properties like periodicity, lacking dedicated architectures.

Method: Proposes FNF (unifying local/global time-frequency processing) and DBD (optimizing gradient flow/representation) for spatio-temporal modeling.

Result: Achieves state-of-the-art performance on 11 datasets across five domains without auxiliary techniques.

Conclusion: Properly designed architectures (FNF/DBD) can inherently capture time series properties, potentially transforming forecasting applications.

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [406] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/pdf/2506.09193)
*Yilin Zhuang, Karthik Duraisamy*

Main category: cs.LG

TL;DR: LaDCast is a global latent-diffusion framework for medium-range ensemble weather forecasting, achieving accuracy and efficiency by operating in a learned latent space.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high accuracy and efficient uncertainty quantification in weather forecasting, which overburden traditional methods like ensemble NWP and machine-learning approaches.

Method: Uses an autoencoder to compress ERA5 reanalysis fields into a latent space, a transformer-based diffusion model for sequential updates, and incorporates GeoRoPE, dual-stream attention, and temporal embeddings.

Result: Matches the deterministic and probabilistic skill of IFS-ENS, excels in tracking extreme events like cyclones, and reduces storage/compute requirements significantly.

Conclusion: LaDCast offers a practical solution for high-resolution real-time forecasting, with open-sourced code and models for broader use.

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [407] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2506.09199)
*Hariharan Ramesh, Jyotikrishna Dass*

Main category: cs.LG

TL;DR: FLoRIST is a federated fine-tuning framework for LLMs using LoRA, addressing challenges in communication efficiency, accuracy, and computational cost by avoiding full global weight-update matrix construction and using efficient decomposition.


<details>
  <summary>Details</summary>
Motivation: Existing federated LoRA methods struggle with balancing communication efficiency, model accuracy, and computational cost, especially in heterogeneous client setups.

Method: FLoRIST employs an efficient decomposition pipeline via singular value decomposition on stacked local adapters separately, avoiding full global weight-update matrix construction.

Result: FLoRIST achieves superior communication efficiency and competitive performance across datasets and LLMs in homogeneous and heterogeneous setups.

Conclusion: FLoRIST effectively balances communication efficiency and performance, making it a robust solution for federated fine-tuning of LLMs.

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [408] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2506.09200)
*Val Andrei Fajardo, David B. Emerson, Amandeep Singh, Veronica Chatrath, Marcelo Lotif, Ravi Theja, Alex Cheung, Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG introduces a framework for fine-tuning RAG systems across centralized and federated architectures, improving retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of relying solely on parametric memory in large language models by enhancing RAG systems through fine-tuning.

Method: FedRAG supports state-of-the-art fine-tuning methods for retriever and generator models, offering a simple interface and seamless transition between centralized and federated training.

Result: FedRAG fills a critical gap in the RAG ecosystem by providing a tool for fine-tuning in both centralized and federated settings.

Conclusion: FedRAG is a valuable addition to the RAG ecosystem, enabling improved performance through flexible fine-tuning across architectures.

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [409] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/pdf/2506.09202)
*Hao Hu, Xinqi Wang, Simon Shaolei Du*

Main category: cs.LG

TL;DR: The paper introduces a novel task of clustering trajectories from offline RL datasets, proposing two methods (PG-Kmeans and CAAE) and validating their effectiveness on D4RL and GridWorld datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of clustering trajectories in offline RL datasets, where each cluster represents a policy, leveraging KL-divergence and policy-induced distributions.

Method: Proposes PG-Kmeans (iterative BC policy training and trajectory assignment) and CAAE (VQ-VAE-like framework for clustering).

Result: Both methods effectively partition trajectories into meaningful clusters, demonstrating convergence and handling inherent ambiguities.

Conclusion: PG-Kmeans and CAAE provide a promising framework for policy-based trajectory clustering, with broad applications in offline RL.

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [410] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/pdf/2506.09207)
*William Anderson, Kevin Chung, Youngsoo Choi*

Main category: cs.LG

TL;DR: The paper introduces mLaSDI, a multi-stage version of LaSDI, to improve reduced-order models (ROMs) for PDEs by training autoencoders sequentially to correct errors, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate numerical solutions of PDEs are computationally expensive, and existing ROMs like LaSDI struggle with complex or high-frequency regimes due to autoencoder limitations.

Method: mLaSDI trains multiple autoencoders sequentially, each correcting the errors of the previous stages, to enhance reconstruction and prediction accuracy.

Result: mLaSDI reduces prediction and reconstruction errors and training time compared to LaSDI, especially with small autoencoders.

Conclusion: mLaSDI improves upon LaSDI by addressing autoencoder limitations, offering a more efficient and accurate ROM framework for PDEs.

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [411] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/pdf/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: The paper explores pooling methods for transformer embeddings, showing standard methods (AvgPool, MaxPool, ClsToken) fail under fluctuating signal-to-noise ratios (SNR). An attention-based adaptive pooling method is proposed, outperforming others in robustness across tasks.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of standard pooling methods to SNR fluctuations in transformer embeddings, particularly for reinforcement learning and vision applications.

Method: Frames pooling as vector quantization to minimize signal loss, introduces an attention-based adaptive pooling method, and validates it theoretically and experimentally.

Result: Adaptive pooling approximates the signal-optimal quantizer within error bounds, outperforming standard methods in synthetic and real-world benchmarks (relational reasoning, multi-agent RL, vision).

Conclusion: Attention-based adaptive pooling is robust to SNR variations, offering superior performance over traditional methods in noisy environments.

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [412] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/pdf/2506.09227)
*Jie Ren, Yue Xing, Yingqian Cui, Charu C. Aggarwal, Hui Liu*

Main category: cs.LG

TL;DR: This paper proposes a new taxonomy for LLM unlearning based on the underlying intention (removal vs. suppression), revisits the effectiveness of removal methods, critiques evaluation strategies, and highlights practical challenges.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked dimension of intention in unlearning (removal vs. suppression) and provide a clearer framework for advancing unlearning in generative AI.

Method: Proposes an intention-oriented taxonomy, revisits removal methods, surveys evaluation strategies, and identifies practical challenges.

Result: Finds that many removal methods may functionally behave like suppression, critiques current evaluation metrics, and highlights scalability and sequential unlearning as key challenges.

Conclusion: Offers a comprehensive framework to guide future research and policy decisions on unlearning in generative AI.

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [413] [Learning The Minimum Action Distance](https://arxiv.org/pdf/2506.09276)
*Lorenzo Steccanella, Joshua B. Evans, Özgür Şimşek, Anders Jonsson*

Main category: cs.LG

TL;DR: A self-supervised framework learns minimum action distance (MAD) from state trajectories, enabling tasks like goal-conditioned RL without rewards or actions.


<details>
  <summary>Details</summary>
Motivation: To capture environment structure without relying on rewards or actions, enabling downstream tasks like reward shaping.

Method: Learn MAD as a metric in an embedding space, accommodating symmetric/asymmetric approximations, evaluated on diverse environments.

Result: Accurate MAD representations learned efficiently, outperforming existing methods in quality.

Conclusion: The framework effectively learns MAD, enhancing state representation for diverse RL tasks.

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [414] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/pdf/2506.09247)
*Karl Löwenmark, Daniel Strömbergsson, Chang Liu, Marcus Liwicki, Fredrik Sandin*

Main category: cs.LG

TL;DR: MindRAG integrates LLM-based reasoning with CM workflows to reduce false alarms, improve fault severity estimation, and enhance decision support using multimodal RAG techniques.


<details>
  <summary>Details</summary>
Motivation: Current CM systems rely heavily on human experts, suffer from uncertainty, and high false alarm rates, reducing efficiency.

Method: Proposes MindRAG, a modular framework combining multimodal RAG with novel vector store structures for CM data, leveraging existing annotations and work orders.

Result: Preliminary results show MindRAG improves alarm management and system interpretability.

Conclusion: MindRAG offers a practical solution for automating CM tasks, enhancing efficiency and decision-making in industrial settings.

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [415] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/pdf/2506.09258)
*Vaidotas Simkus, Michael U. Gutmann*

Main category: cs.LG

TL;DR: CFMI is a new imputation method combining continuous normalising flows, flow-matching, and shared conditional modelling, outperforming traditional and modern techniques across various metrics.


<details>
  <summary>Details</summary>
Motivation: To address intractabilities in traditional multiple imputation methods and provide a scalable solution for diverse data types.

Method: Combines continuous normalising flows, flow-matching, and shared conditional modelling.

Result: CFMI matches or outperforms nine classical and state-of-the-art methods on 24 datasets and excels in computational efficiency for time-series data.

Conclusion: CFMI is a versatile, high-performing imputation method suitable for a wide range of data types and dimensionalities.

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [416] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/pdf/2506.09286)
*Mohammadsajad Abavisani, Kseniya Solovyeva, David Danks, Vince Calhoun, Sergey Plis*

Main category: cs.LG

TL;DR: The paper introduces a method using answer set programming (ASP) to derive causal graphs from sub-sampled time series data, improving accuracy and efficiency over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning causal structures from time series data with mismatched measurement frequencies, which leads to ambiguous causal graphs due to sub-sampling.

Method: Uses constraint optimization via ASP to identify optimal causal graphs and equivalence classes, leveraging graph theory to prune solutions.

Result: Validated on simulated and empirical data, showing superior performance (12% F1 improvement) and robustness to sub-sampling.

Conclusion: The ASP-based method outperforms existing approaches, offering more accurate and faster causal graph reconstruction from sub-sampled time series.

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [417] [Uncertainty Prioritized Experience Replay](https://arxiv.org/pdf/2506.09270)
*Rodrigo Carrasco-Davis, Sebastian Lee, Claudia Clopath, Will Dabney*

Main category: cs.LG

TL;DR: The paper proposes using epistemic uncertainty to prioritize transitions in replay buffers, improving reinforcement learning by reducing noise impact.


<details>
  <summary>Details</summary>
Motivation: Traditional prioritization based on temporal difference errors favors noisy transitions, resembling the noisy TV problem, which disrupts learning.

Method: The authors introduce epistemic uncertainty estimation to guide transition prioritization, tested in toy models and the Atari suite.

Result: The method outperforms benchmarks like quantile regression deep Q-learning, demonstrating effectiveness.

Conclusion: Epistemic uncertainty prioritization offers a promising approach to enhance reinforcement learning agents by mitigating noise effects.

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [418] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/pdf/2506.09272)
*Samuel Holt, Max Ruiz Luyten, Antonin Berthon, Mihaela van der Schaar*

Main category: cs.LG

TL;DR: G-Sim is a hybrid framework combining LLM-driven structural design with empirical calibration to build reliable simulators for complex decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing simulator methods often fail to generalize or align empirically, especially when using LLMs. G-Sim addresses these limitations by integrating domain knowledge and flexible calibration techniques.

Method: G-Sim uses an LLM in an iterative loop to design and refine simulator components and causal relationships. It then grounds these structures in reality using likelihood-free and gradient-free calibration methods.

Result: G-Sim produces causally-informed, reliable simulators that mitigate data inefficiency and support robust system-level interventions.

Conclusion: G-Sim offers a scalable solution for constructing accurate simulators, enhancing decision-making in critical domains like healthcare and logistics.

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [419] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/pdf/2506.09347)
*Xuemei Cao, Hanlin Gu, Xin Yang, Bingjun Wei, Haoyang Liang, Xiangkun Wang, Tianrui Li*

Main category: cs.LG

TL;DR: ErrorEraser is a plugin for Continual Learning (CL) that addresses data biases by identifying and erasing erroneous memories, improving performance in both new and old tasks.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods ignore biases in real-world data, leading to spurious correlations that hinder knowledge retention and transfer.

Method: ErrorEraser consists of Error Identification (learning data distribution to spot biases) and Error Erasure (shifting decision space to remove errors). It also uses incremental feature distribution learning to reduce overhead.

Result: ErrorEraser significantly reduces the impact of data biases, achieving higher accuracy and lower forgetting rates across CL methods.

Conclusion: The proposed method effectively mitigates bias-induced errors in CL, enhancing overall performance.

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [420] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/pdf/2506.09279)
*Ziyi Chen, Yiyang Liu, Mattia Prosperi, Krishna Vaddiparti, Robert L Cook, Jiang Bian, Yi Guo, Yonghui Wu*

Main category: cs.LG

TL;DR: The study uses NLP and topic modeling on EHR notes to analyze HIV-related stigma, social, and behavioral factors in PLWHs, revealing key themes and demographic variations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional questionnaires by leveraging EHR data for scalable, efficient assessment of HIV-related stigma and social factors.

Method: Identified 9,140 PLWHs, applied LDA topic modeling with keyword filtering, and manually reviewed topics. Analyzed word frequency and demographic variations.

Result: Uncovered themes like 'Mental Health Concern and Stigma' and 'Limited Healthcare Access.' Demographic differences were noted.

Conclusion: EHR-based NLP provides scalable stigma assessment, improving patient outcomes beyond traditional methods.

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [421] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/pdf/2506.09316)
*Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella*

Main category: cs.LG

TL;DR: The paper introduces DSLA (dual-state linear attention) and Serve, an adaptive distillation framework, to improve efficiency and accuracy in large language models by balancing historical and recent token dependencies.


<details>
  <summary>Details</summary>
Motivation: Address the prohibitive compute and memory costs of LLMs on lengthy inputs and mitigate the short-range bias in sub-quadratic methods like linear attention.

Method: Propose DSLA with dual hidden states for historical context and recency, and Serve for adaptive distillation, replacing Transformer layers with DSLA layers at inference.

Result: Serve achieves 2.3x faster inference than Llama2-7B and 3.0x faster than Zamba-7B while maintaining comparable performance.

Conclusion: DSLA and Serve effectively balance efficiency and accuracy, capturing both global and local dependencies in LLMs.

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [422] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/pdf/2506.09332)
*Zhenqiao Song, Ramith Hettiarachchi, Chuan Li, Jianwen Xie, Lei Li*

Main category: cs.LG

TL;DR: InstructPro is a protein generative model that uses natural language instructions and ligand formulas to design ligand-binding proteins, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Designing proteins that bind to specific ligands is crucial in biology and chemistry, but AI models are limited by scarce protein-ligand complex data. Human-curated text descriptions offer an alternative resource.

Method: Proposes InstructPro, a model trained on a large dataset (InstructProBench) with triples of function descriptions, ligand formulas, and protein sequences. Two variants (1B and 3B parameters) are developed.

Result: InstructPro-1B achieves an 81.52% docking success rate and low RMSD (4.026Å). InstructPro-3B further reduces RMSD to 2.527Å, outperforming ProGen2, ESM3, and Pinal.

Conclusion: InstructPro demonstrates the ability to generate ligand-binding proteins from textual instructions, offering a promising approach for protein design.

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [423] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/pdf/2506.09368)
*Yang Liu, Jing Liu, Chengfang Li, Rui Xi, Wenchao Li, Liang Cao, Jin Wang, Laurence T. Yang, Junsong Yuan, Wei Zhou*

Main category: cs.LG

TL;DR: A survey on anomaly detection and generation using diffusion models (ADGDM), highlighting their synergistic relationship and applications across diverse data types.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of anomaly data scarcity and improve detection and generation capabilities by leveraging diffusion models.

Method: Comprehensive review and taxonomy of ADGDM methods, analyzing anomaly scoring, conditioning strategies, and architectural designs.

Result: Reveals the reinforcing cycle between anomaly detection and generation, advancing both beyond individual potential.

Conclusion: Outlines challenges like scalability and future directions, guiding researchers in leveraging DMs for innovative AD solutions.

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [424] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/pdf/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: The paper analyzes the convergence rate of adversarial classification risk for robust classifiers, providing surrogate risk bounds and extending results to standard learning settings.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the convergence rate of adversarial classification risk for robust classifiers trained via adversarial training.

Method: Derives surrogate risk bounds to quantify the convergence rate and extends these bounds to standard learning settings.

Result: Provides surrogate risk bounds for adversarial classification risk convergence and distribution-dependent bounds for standard learning.

Conclusion: The paper advances understanding of adversarial consistency and offers insights applicable to both adversarial and standard learning scenarios.

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [425] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/pdf/2506.09373)
*Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, Shiyin Lu, Qifeng Chen*

Main category: cs.LG

TL;DR: LPO improves GUI agent interactions by optimizing location preferences using information entropy and dynamic rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents struggle with positional accuracy due to limitations in SFT and reinforcement learning methods.

Method: LPO leverages locational data and information entropy to predict interaction zones, supported by GRPO for exploration.

Result: LPO achieves state-of-the-art performance in offline benchmarks and real-world evaluations.

Conclusion: LPO enhances interaction precision and is a promising advancement for GUI agents.

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [426] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/pdf/2506.09376)
*Bowen Zheng, Tianming Yang*

Main category: cs.LG

TL;DR: The paper identifies limitations in diffusion distillation and proposes using a GAN objective to convert diffusion models into efficient one-step generators, achieving strong performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and performance degradation in diffusion distillation by exploring the role of GAN objectives and redefining diffusion training as generative pre-training.

Method: The study replaces distillation loss with a standalone GAN objective, fine-tunes pre-trained models with most parameters frozen, and analyzes results in the frequency domain.

Result: The approach achieves strong performance with only 0.2M images and near-SOTA results with 5M images, demonstrating the effectiveness of GAN fine-tuning.

Conclusion: Diffusion training serves as powerful generative pre-training, enabling efficient one-step generation models through lightweight GAN fine-tuning.

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [427] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/pdf/2506.09398)
*Haiyang Yu, Yuchao Lin, Xuan Zhang, Xiaofeng Qian, Shuiwang Ji*

Main category: cs.LG

TL;DR: QHNetV2, a novel network, predicts Hamiltonian matrices efficiently using SO(2)-equivariant operations, avoiding costly SO(3) tensor products, and shows strong performance on molecular datasets.


<details>
  <summary>Details</summary>
Motivation: Accelerating electronic structure calculations in physics, chemistry, and materials science by leveraging the relationship between Hamiltonian matrices and SO(2) local frames.

Method: Introduces SO(2)-equivariant operations and performs feature updates within SO(2) local frames, eliminating SO(3) tensor products. Uses continuous SO(2) tensor products for node feature fusion.

Result: Superior performance on QH9 and MD17 datasets, demonstrating strong generalization across molecular structures and trajectories.

Conclusion: SO(2) operations on local frames provide a scalable, symmetry-aware approach for electronic structure learning, with code released in the AIRS library.

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [428] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/pdf/2506.09404)
*Shengda Gu, Kai Li, Junliang Xing, Yifan Zhang, Jian Cheng*

Main category: cs.LG

TL;DR: EAM combines DRL and GAs to enhance exploration and efficiency in solving combinatorial optimization problems.


<details>
  <summary>Details</summary>
Motivation: DRL lacks exploration, while GAs are inefficient. EAM aims to merge their strengths.

Method: EAM refines DRL-generated solutions with GA operations (crossover, mutation) and reinjects them into training.

Result: EAM improves solution quality and training efficiency on benchmarks like TSP, CVRP, PCTSP, and OP.

Conclusion: EAM is a versatile, effective framework for combinatorial optimization, outperforming baselines.

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [429] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/pdf/2506.09433)
*Shurui Gui, Shuiwang Ji*

Main category: cs.LG

TL;DR: CAPT reduces spurious correlations in LLMs by decomposing predictions into unbiased steps, improving generalization without extra fine-tuning biases.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' failure on OOD samples due to spurious correlations from pre-training.

Method: Causality-aware post-training (CAPT) decomposes predictions into event estimation and intervention steps.

Result: 3B-scale models with CAPT outperform SFT and larger LLMs on ID and OOD tasks with minimal fine-tuning samples.

Conclusion: CAPT is effective and sample-efficient for enhancing LLM generalization.

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [430] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/pdf/2506.09438)
*Haoxiang Ye, Tao Sun, Qing Ling*

Main category: cs.LG

TL;DR: The paper analyzes generalization errors in decentralized learning, focusing on data heterogeneity, model initialization, and stochastic gradient noise, while also examining the impact of Byzantine attacks.


<details>
  <summary>Details</summary>
Motivation: Understanding generalization errors in decentralized learning is crucial for real-world scalability, as prior studies overlooked data heterogeneity and relied on stringent assumptions.

Method: The study conducts fine-grained generalization error analysis for attack-free and Byzantine-resilient decentralized learning under mild assumptions, using numerical experiments on convex and non-convex tasks.

Result: Findings highlight the impact of data heterogeneity, model initialization, and stochastic gradient noise on generalization errors, with Byzantine attacks significantly affecting performance.

Conclusion: The study provides insights into factors influencing generalization errors in decentralized learning, emphasizing the need to address data heterogeneity and Byzantine resilience.

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [431] [Safe Screening Rules for Group SLOPE](https://arxiv.org/pdf/2506.09451)
*Runxue Bao, Quanchao Lu, Yanfu Zhang*

Main category: cs.LG

TL;DR: A safe screening rule for Group SLOPE is introduced to improve computational efficiency and memory usage by identifying inactive groups with zero coefficients.


<details>
  <summary>Details</summary>
Motivation: Group SLOPE struggles with block non-separable group effects, leading to high computational costs and memory usage in high-dimensional sparse learning.

Method: A tailored safe screening rule is proposed to detect inactive groups, which can be integrated into existing solvers for batch and stochastic algorithms.

Result: The method efficiently identifies inactive groups, reducing computational costs and memory usage without sacrificing accuracy.

Conclusion: The proposed screening rule enhances Group SLOPE's practicality in high-dimensional scenarios while maintaining theoretical guarantees.

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [432] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/pdf/2506.09452)
*Jay Roberts, Kyle Mylonakis, Sidhartha Roy, Kaan Kale*

Main category: cs.LG

TL;DR: The paper introduces the Stained Glass Transform, a method to provide privacy for LLM inputs while preserving model utility, addressing concerns over plaintext data in shared or multi-tenant AI infrastructures.


<details>
  <summary>Details</summary>
Motivation: High costs and privacy concerns in shared AI infrastructures limit data usage, especially for sensitive data.

Method: Proposes the Stained Glass Transform, a learned, stochastic transformation of word embeddings to ensure privacy without losing utility.

Result: The method theoretically connects to Gaussian Mixture Models' mutual information and shows practical privacy and utility in benchmarks.

Conclusion: The Stained Glass Transform effectively balances privacy and utility for LLM deployments in shared environments.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [433] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/pdf/2506.09496)
*Dingyi Rong, Haotian Lu, Wenzhuo Zheng, Fan Zhang, Shuangjia Zheng, Ning Liu*

Main category: cs.LG

TL;DR: EnerBridge-DPO is a novel framework for generating low-energy, stable protein sequences by integrating Markov Bridges with Direct Preference Optimization (DPO) and an energy constraint loss.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for protein inverse folding focus on sequence recovery rates, often ignoring sequence energy. This work addresses the gap by prioritizing low-energy, stable sequences.

Method: The model combines Markov Bridges and DPO, using energy-based preferences to fine-tune sequences. An energy constraint loss is added to enhance energy-driven learning and predict sequence energy values.

Result: EnerBridge-DPO designs sequences with lower energy while matching state-of-the-art recovery rates and accurately predicts ΔΔG values.

Conclusion: The framework successfully bridges the gap between sequence recovery and energy stability, offering a robust solution for protein inverse folding.

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [434] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/pdf/2506.09454)
*Yuanhao Pu, Defu Lian, Xiaolong Chen, Xu Huang, Jin Chen, Enhong Chen*

Main category: cs.LG

TL;DR: The paper proposes RG$^2$ and RG$^	imes$ losses as efficient alternatives to Softmax Loss for ranking tasks, offering theoretical insights and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Softmax Loss, while effective, faces computational and scalability issues in large-scale ranking tasks.

Method: Derived RG$^2$ and RG$^	imes$ losses via Taylor expansions of Softmax Loss, integrated with ALS optimization.

Result: Empirical results show comparable/superior performance to Softmax Loss with faster convergence.

Conclusion: The framework provides efficient tools and theoretical insights for similarity learning, balancing ranking quality and computational efficiency.

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [435] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/pdf/2506.09499)
*Thomas J. Ringstrom, Paul R. Schrater*

Main category: cs.LG

TL;DR: OKBEs introduce state-time option kernels (STOKs) for reward-free MDPs, optimizing goal completion while avoiding constraints. STOKs are modular, interpretable, and support long-horizon planning.


<details>
  <summary>Details</summary>
Motivation: Address the conflict between reward-maximization and properties like compositionality, modularity, and interpretability in reinforcement learning.

Method: Construct and optimize STOKs, compositional transition kernels, using Chapman-Kolmogorov equations for spatiotemporal predictions and efficient computation.

Result: Enables flexible agents for meta-policies, reusable planning, and verifiable long-horizon planning in high-dimensional models.

Conclusion: OKBEs support scalable, interpretable, and verifiable planning, aligning with intrinsic motivation and dynamic world-models.

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [436] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/pdf/2506.09477)
*Yunhao Tang, Rémi Munos*

Main category: cs.LG

TL;DR: The paper identifies pitfalls in gradient estimation for KL divergence in RL training for LLMs, showing incorrect implementations and their impacts, and provides correct methods.


<details>
  <summary>Details</summary>
Motivation: To address common errors in gradient estimation for KL divergence in RL training for LLMs, as seen in open-source projects and papers.

Method: Analyzes incorrect implementations, demonstrates their flaws with tabular and LLM experiments, and presents the correct gradient estimation method.

Result: Incorrect implementations fail to produce desired KL gradients, while the correct method is validated through experiments.

Conclusion: The paper clarifies proper gradient estimation for KL divergence in RL training, improving implementation accuracy.

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [437] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/pdf/2506.09508)
*Andreas Schlaginhaufen, Reda Ouhamma, Maryam Kamgarpour*

Main category: cs.LG

TL;DR: A meta-algorithm for reinforcement learning from human feedback using trajectory-level preference comparisons, ensuring tractability and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing algorithms that efficiently learn from human feedback while maintaining theoretical guarantees in Markov decision processes.

Method: Proposes a randomized exploration meta-algorithm and an improved batch-based algorithm using optimal experimental design for informative queries.

Result: Achieves regret and last-iterate guarantees, with empirical results showing competitiveness with reward-based RL using fewer queries.

Conclusion: The method is efficient, tractable, and reduces query complexity, making it practical for real-world deployment.

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [438] [Neural Functions for Learning Periodic Signal](https://arxiv.org/pdf/2506.09526)
*Woojin Cho, Minju Jo, Kookjin Lee, Noseong Park*

Main category: cs.LG

TL;DR: A novel neural network architecture improves generalization and extrapolation for periodic signals by leveraging periodic patterns.


<details>
  <summary>Details</summary>
Motivation: Coordinate-based MLPs struggle with overfitting and poor extrapolation, especially for periodic signals.

Method: Proposes a network architecture that extracts and utilizes periodic patterns from data.

Result: Enhanced generalization and better extrapolation performance, validated on differential equations and real-world datasets.

Conclusion: The method effectively addresses limitations of MLPs for periodic signals, improving performance in interpolation and forecasting.

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [439] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/pdf/2506.09532)
*Shuai Wang, Zhenhua Liu, Jiaheng Wei, Xuanwu Yin, Dong Li, Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM is a multimodal process reward model for evaluating reasoning steps, using weak-strong completer consistency for high-quality labels. It achieves strong performance with minimal data and outperforms benchmarks.


<details>
  <summary>Details</summary>
Motivation: High-performance PRMs require costly step-level annotations. Existing methods are noisy and expensive, prompting the need for efficient, high-quality labeling.

Method: Uses prediction consistency between weak and strong completers for reliable labels, with ORM initialization and negative data up-sampling.

Result: Achieves superior performance with 5,000 samples, improving benchmarks by up to 10.2 points and setting SoTA in VisualProcessBench.

Conclusion: Athena-PRM is effective for reasoning step evaluation and enhances model performance, as demonstrated by Athena-7B's benchmark outperformance.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [440] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/pdf/2506.09544)
*Yang Yang, Du Yin, Hao Xue, Flora Salim*

Main category: cs.LG

TL;DR: STOAT is a novel framework for probabilistic forecasting in spatial-temporal causal time series, combining causal inference with spatial dependencies and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing methods often model spatial and temporal dynamics separately and ignore causality-driven probabilistic forecasting, limiting predictive power.

Method: STOAT incorporates a spatial relation matrix for interregional dependencies and uses deep probabilistic models to estimate distribution parameters, enabling calibrated uncertainty modeling.

Result: STOAT outperforms state-of-the-art models like DeepAR and DeepVAR, especially in regions with strong spatial dependencies, as shown in COVID-19 data experiments.

Conclusion: STOAT bridges causal inference and geospatial probabilistic forecasting, offering a generalizable framework for tasks like epidemic management.

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [441] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/pdf/2506.09574)
*Gaurav Chaudhary, Wassim Uddin Mondal, Laxmidhar Behera*

Main category: cs.LG

TL;DR: MOORL is a hybrid offline-online RL framework that combines offline data for initialization and online interactions for exploration, outperforming existing methods with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of sample efficiency and exploration in DRL, particularly in complex domains, and overcoming the limitations of offline RL like OOD actions.

Method: Introduces a meta-policy that adapts across offline and online trajectories, leveraging offline data for initialization and online interactions for exploration.

Result: Theoretical analysis shows enhanced exploration, stable Q-function learning, and consistent improvements over baselines on 28 tasks from D4RL and V-D4RL benchmarks.

Conclusion: MOORL is effective, scalable, and practical for real-world applications, achieving strong performance with minimal computational overhead.

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [442] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/pdf/2506.09593)
*Achim Hekler, Lukas Kuhn, Florian Buettner*

Main category: cs.LG

TL;DR: Foundation models like ConvNeXt, EVA, and BEiT show underconfidence in in-distribution predictions but better calibration under shifts. Post-hoc calibration works well in-distribution but fails under severe shifts.


<details>
  <summary>Details</summary>
Motivation: To investigate the calibration behavior of foundation models, which is crucial for safe deployment in high-stakes applications.

Method: Empirical analysis of foundation models' calibration under in-distribution and distribution shift scenarios, including post-hoc calibration techniques.

Result: Foundation models are underconfident in-distribution but better calibrated under shifts. Post-hoc calibration is effective in-distribution but unreliable under severe shifts.

Conclusion: Calibration behavior of foundation models is complex and non-monotonic, challenging the notion of continuous improvement with architectural innovations.

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [443] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/pdf/2506.09594)
*Wenjin Qin, Hailin Wang, Jingyao Hou, Jianjun Wang*

Main category: cs.LG

TL;DR: The paper introduces fast randomized algorithms for low-rank tensor approximation (LRTA) and a nonconvex modeling framework for large-scale tensor recovery, addressing computational challenges and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing tensor recovery methods ignore tensor scale variations and face high computational costs with large-scale data.

Method: Uses Krylov subspace iteration, block Lanczos bidiagonalization, and random projection for LRTA, and develops a nonconvex framework with new regularization.

Result: Theoretical bounds on approximation error are established, and experiments show the method's superiority over state-of-the-art approaches.

Conclusion: The proposed algorithms and framework are practical, effective, and efficient for large-scale tensor recovery tasks.

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [444] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/pdf/2506.09613)
*Kaiwen Tuo, Huan Wang*

Main category: cs.LG

TL;DR: SparseSSM is a training-free pruning framework for state-space models like Mamba, achieving 50% weight pruning without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods fail for state-space models due to their unique architecture.

Method: Extends optimal brain surgeon (OBS) with layer-wise pruning, Hessian-trace aggregation, and sensitivity analysis.

Result: Prunes 50% of SSM weights without fine-tuning, maintaining zero-shot accuracy.

Conclusion: SparseSSM is state-of-the-art for pruning Mamba-based LLMs.

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [445] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/pdf/2506.09625)
*Ekaterina Filimoshina, Dmitry Shirokov*

Main category: cs.LG

TL;DR: GLGENN is a new equivariant neural network architecture using geometric algebras, outperforming competitors with fewer parameters and less overfitting.


<details>
  <summary>Details</summary>
Motivation: To create a neural network equivariant to all pseudo-orthogonal transformations, including rotations and reflections, for vector spaces with symmetric bilinear forms.

Method: Uses geometric (Clifford) algebras and a weight-sharing parametrization technique to reduce parameters and overfitting.

Result: GLGENN matches or outperforms competitors on equivariant tasks like function estimation and convex hull experiments, using fewer parameters.

Conclusion: GLGENN is a parameter-efficient, high-performing architecture for equivariant tasks.

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [446] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/pdf/2506.09701)
*Vincenzo Collura, Karim Tit, Laura Bussi, Eleonora Giunchiglia, Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT is a model-agnostic algorithm ensuring LLM outputs comply with temporal constraints (LTLf) without retraining, using DFA-guided beam search.


<details>
  <summary>Details</summary>
Motivation: LLMs lack inherent capability to enforce temporal constraints, necessitating a solution like TRIDENT.

Method: Compiles LTLf formulas into a DFA to guide constrained beam search, masking violations and re-ranking paths.

Result: Guarantees constraint satisfaction and improves output quality, validated on image-stream classification and text generation.

Conclusion: TRIDENT effectively enforces temporal constraints while maintaining or enhancing output quality and efficiency.

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [447] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/pdf/2506.09630)
*Pol G. Recasens, Alberto Gutierrez, Jordi Torres, Josep. Ll Berral, Anisa Halimi, Kieran Fraser*

Main category: cs.LG

TL;DR: LLMs for synthetic tabular data generation can propagate biases from in-context examples, leading to unfair downstream outcomes.


<details>
  <summary>Details</summary>
Motivation: To investigate how biases in in-context examples affect synthetic data and downstream fairness, especially in adversarial scenarios.

Method: Systematic study of bias propagation in LLM-generated synthetic data and adversarial bias injection.

Result: Mild in-context biases cause global statistical distortions; adversarial bias injection compromises fairness for protected subgroups.

Conclusion: LLM-based data generation is vulnerable to bias propagation, posing risks in sensitive domains.

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [448] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/pdf/2506.09638)
*Weiying Zheng, Ziyue Lin, Pengxin Guo, Yuyin Zhou, Feifei Wang, Liangqiong Qu*

Main category: cs.LG

TL;DR: FedVLMBench is the first systematic benchmark for federated fine-tuning of Vision-Language Models (VLMs), addressing privacy concerns in domains like healthcare. It evaluates architectures, strategies, and task generalization, revealing key insights and optimal configurations.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning relies on centralized training, which is unsuitable for privacy-sensitive domains. Federated Learning (FL) is introduced to address this, but lacks comprehensive benchmarks.

Method: FedVLMBench integrates two VLM architectures, four fine-tuning strategies, five FL algorithms, and six multimodal datasets across various task scenarios.

Result: A 2-layer MLP connector with concurrent tuning is optimal for encoder-based VLMs in FL. FL methods are more sensitive to data heterogeneity in vision-centric tasks.

Conclusion: FedVLMBench provides tools, datasets, and guidance for advancing privacy-preserving, federated training of multimodal models.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [449] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/pdf/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: AtmosMJ, a deep convolutional network, achieves stable long-range weather forecasts on standard latitude-longitude grids without spherical remapping, challenging the need for non-standard data representations.


<details>
  <summary>Details</summary>
Motivation: To investigate if stable long-range weather forecasts can be achieved on standard grids, challenging the assumption that non-standard spatial domains are necessary.

Method: Introduces AtmosMJ with a Gated Residual Fusion (GRF) mechanism to prevent error accumulation in recursive simulations, operating directly on ERA5 data.

Result: AtmosMJ produces stable forecasts for ~500 days, matches 10-day accuracy of leading models, and requires only 5.7 days of GPU training.

Conclusion: Efficient architectural design, not non-standard data representation, is key to stable and computationally efficient long-range weather prediction.

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [450] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/pdf/2506.09660)
*Baran Can Gül, Stefanos Tziampazis, Nasser Jazdi, Michael Weyrich*

Main category: cs.LG

TL;DR: SyncFed is a time-aware Federated Learning framework that uses synchronization and timestamping to improve model accuracy and freshness by quantifying staleness in client updates.


<details>
  <summary>Details</summary>
Motivation: Challenges like network delays and unsynchronicity in Federated Learning can misalign client contributions, undermining model reliability and convergence. Existing methods lack mechanisms to quantify staleness.

Method: SyncFed employs explicit synchronization and timestamping under NTP to establish a common temporal reference, numerically quantifying staleness for informed weighting during aggregation.

Result: Empirical evaluation shows SyncFed improves model accuracy and information freshness compared to round-based baselines.

Conclusion: SyncFed effectively addresses temporal misalignment in FL, enhancing model reliability and convergence through time-aware synchronization.

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [451] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/pdf/2506.09674)
*Alessandro Licciardi, Davide Leo, Davide Carbone*

Main category: cs.LG

TL;DR: WAFFLE detects malicious clients in Federated Learning (FL) before training using compressed representations (Wavelet or Fourier transforms), improving accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Anomalous clients in FL degrade model performance, but detecting them without raw data access is challenging.

Method: Uses Wavelet Scattering Transform (WST) or Fourier Transform for task-agnostic embeddings, with a lightweight detector trained on public data.

Result: Improves detection accuracy and downstream classification over existing FL anomaly detection methods.

Conclusion: WAFFLE is effective as a pre-training alternative to online detection, with WST offering theoretical advantages.

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [452] [Wasserstein Hypergraph Neural Network](https://arxiv.org/pdf/2506.09682)
*Iulia Duta, Pietro Liò*

Main category: cs.LG

TL;DR: The paper introduces Wasserstein Hypergraph Neural Network (WHGNN), using Sliced Wasserstein Pooling for higher-order relational modeling, outperforming traditional methods in node classification.


<details>
  <summary>Details</summary>
Motivation: Current hypergraph neural networks simplify aggregations to basic pooling, missing higher-order geometric properties. WHGNN aims to preserve these by treating nodes and hyperedges as distributions.

Method: WHGNN models nodes and hyperedges as distributions and aggregates information using Sliced Wasserstein Pooling, capturing geometric properties like shape and spread.

Result: WHGNN achieves top performance in node classification tasks on real-world datasets, demonstrating the benefits of Wasserstein pooling.

Conclusion: WHGNN's distribution-based approach and Wasserstein pooling effectively model higher-order relationships, advancing hypergraph neural networks.

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [453] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/pdf/2506.09742)
*Gusseppe Bravo-Rocca, Peini Liu, Jordi Guitart, Rodrigo M Carrillo-Larco, Ajay Dholakia, David Ellison*

Main category: cs.LG

TL;DR: A cognitive architecture for ML monitoring enhances interpretability by applying feature engineering principles to LLMs, improving decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional ML monitoring outputs are verbose and lack interpretability, hindering effective decision-making.

Method: Proposes a Decision Procedure module with three steps: Refactor (improves data representation), Break Down (decomposes complex info), and Compile (integrates insights). Reduces reliance on LLM-generated planning.

Result: Experiments show higher accuracy and interpretability compared to baselines across domains.

Conclusion: The approach provides robust, interpretable, and actionable insights for ML monitoring.

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [454] [Auto-Compressing Networks](https://arxiv.org/pdf/2506.09714)
*Vaggelis Dorovatas, Georgios Paraskevopoulos, Alexandros Potamianos*

Main category: cs.LG

TL;DR: Auto-Compressing Networks (ACNs) replace short residual connections with long feedforward connections, enabling auto-compression for efficient training and improved representation quality.


<details>
  <summary>Details</summary>
Motivation: Addressing computational redundancy in deep networks without sacrificing representation quality.

Method: ACNs use additive long feedforward connections to dynamically compress information during training.

Result: ACNs reduce catastrophic forgetting by 18%, achieve 30-80% compression, and improve noise robustness, low-data performance, and transfer learning.

Conclusion: ACNs offer a practical, efficient architecture that adapts to task complexity and learns robust representations.

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [455] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/pdf/2506.09738)
*Xin Wang, Zeyang Zhang, Linxin Xiao, Haibo Chen, Chendi Ge, Wenwu Zhu*

Main category: cs.LG

TL;DR: The paper explores Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks, proposing a framework with five key characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal graph learning methods lack generalization across diverse data and tasks, prompting the need for a unified approach like MG-LLM.

Method: The paper proposes a unified framework for multi-modal graph data, tasks, and models, focusing on multi-granularity and multi-scale characteristics. It outlines five desired traits for MG-LLM.

Result: The paper identifies key challenges, reviews related works, and highlights future research directions for MG-LLM. It also summarizes relevant datasets.

Conclusion: The paper aims to advance research on MG-LLM for generalization across multi-modal graph data and tasks.

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [456] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/pdf/2506.09769)
*Haruki Kainuma, Takayuki Nishio*

Main category: cs.LG

TL;DR: Load-aware Tram-FL extends Tram-FL with a scheduling mechanism to minimize training time in decentralized federated learning by optimizing computational and communication loads.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of minimizing total training time in decentralized federated learning while balancing computational and communication loads.

Method: Formulates the scheduling problem as a global optimization task, decomposes it into node-wise subproblems, and introduces a variance constraint for balanced data utilization under non-IID distributions.

Result: Simulations on MNIST and CIFAR-10 show significant reductions in training time and faster convergence compared to baselines.

Conclusion: Load-aware Tram-FL effectively optimizes training scheduling, reducing latency and improving performance in decentralized federated learning.

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [457] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/pdf/2506.09785)
*Alexander Marusov, Alexander Yuhay, Alexey Zaytsev*

Main category: cs.LG

TL;DR: A novel contrastive SSL framework for dependent data, outperforming TS2Vec on benchmarks with accuracy improvements of 4.17% and 2.08%, and 7% higher ROC-AUC on drought classification.


<details>
  <summary>Details</summary>
Motivation: Traditional SSL methods assume semantic independence between samples, which fails for dependent data like temporal and spatio-temporal domains.

Method: Proposes a theoretical framework with hard and soft closeness measures, deriving dependency-aware loss functions for SSL.

Result: Outperforms TS2Vec on UEA and UCR benchmarks (4.17% and 2.08% accuracy gains) and achieves 7% higher ROC-AUC on drought classification.

Conclusion: The proposed framework effectively captures spatio-temporal dependencies, demonstrating superior performance on dependent data tasks.

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [458] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/pdf/2506.09781)
*Chungpa Lee, Sehee Lim, Kibok Lee, Jy-yong Sohn*

Main category: cs.LG

TL;DR: A unified framework for contrastive learning (CL) explains various objectives, highlighting challenges in full-batch and mini-batch settings and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: Prior works lack a comprehensive framework for understanding contrastive learning objectives.

Method: Analyzes cosine similarity between embeddings of positive and negative pairs, introduces an auxiliary loss for mini-batch CL.

Result: Shows misalignment in full-batch settings and variance in mini-batch settings; proposed loss improves small-batch performance.

Conclusion: The framework and auxiliary loss enhance CL performance, especially in small-batch training.

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [459] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/pdf/2506.09803)
*Longzhu He, Chaozhuo Li, Peng Tang, Litian Zhang, Sen Su*

Main category: cs.LG

TL;DR: The paper introduces a data poisoning attack on locally private graph learning protocols, demonstrating its effectiveness and exploring limited defense strategies.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in graph neural networks (GNNs) due to sensitive data, and the overlooked threat of data poisoning in locally private protocols.

Method: The attacker injects fake users, manipulates links, and sends crafted data to compromise the protocol's utility.

Result: The attack is proven effective theoretically and empirically, with defenses found inadequate.

Conclusion: Highlights the need for stronger defenses against data poisoning in privacy-preserving graph learning.

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [460] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/pdf/2506.09810)
*Minoh Jeong, Alfred Hero*

Main category: cs.LG

TL;DR: ProjNCE, a generalization of InfoNCE, unifies supervised and self-supervised contrastive learning, offering a mutual information bound and flexible projection methods, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: The gap in understanding the relationship between supervised contrastive learning (SupCon) and mutual information (MI) motivates the introduction of ProjNCE.

Method: ProjNCE extends InfoNCE with projection functions and an adjustment term for negative pairs, enabling flexible class embedding strategies.

Result: ProjNCE consistently outperforms SupCon and cross-entropy training across datasets and settings.

Conclusion: ProjNCE refines SupCon by providing MI interpretation and projection design improvements, enhancing its foundational contrastive learning role.

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [461] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/pdf/2506.09813)
*Ariel Procaccia, Benjamin Schiffer, Serena Wang, Shirley Zhang*

Main category: cs.LG

TL;DR: The paper formalizes two notions of representation for selecting subsets of evaluation metrics using social choice theory, proving bounds on required metrics and applying them to real-world cases.


<details>
  <summary>Details</summary>
Motivation: Addressing the unclear definition of 'representative' in subset selection of evaluation metrics for efficiency or interpretability.

Method: Introduces positional representation and positional proportionality, proving bounds on metrics needed and generalizing these properties.

Result: Upper and lower bounds on metrics for representation properties, with case studies on LLM and hospital quality evaluation.

Conclusion: The formalized properties provide practical frameworks for representative subset selection in evaluation metrics.

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [462] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/pdf/2506.09862)
*Mikel Casals, Vasilis Belis, Elias F. Combarro, Eduard Alarcón, Sofia Vallecorsa, Michele Grossi*

Main category: cs.LG

TL;DR: The paper introduces Guided Graph Compression (GGC), a framework using a graph autoencoder to compress graph data for improved performance in downstream tasks, including quantum or classical classifiers.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of large graph processing in GNNs and leveraging quantum computing potential, the paper aims to enable realistic QGNN testing by compressing graph data effectively.

Method: The GGC framework employs a graph autoencoder to reduce node count and feature dimensionality, guided by downstream task performance, and is evaluated on the Jet Tagging task.

Result: GGC outperforms standalone autoencoder preprocessing and classical GNN baselines, enhancing performance and enabling QGNN testing on realistic datasets.

Conclusion: GGC provides a scalable solution for graph compression, bridging the gap between quantum and classical approaches for graph-structured data.

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [463] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/pdf/2506.09816)
*Cecilia Casolo, Sören Becker, Niki Kilbertus*

Main category: cs.LG

TL;DR: The paper explores identifiability in sparse linear ODEs, showing they are unidentifiable with positive probability, unlike dense systems, and discusses practical implications.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored sparse regime in linear ODE identifiability, which is practically relevant in biological, social, and physical systems.

Method: Characterizes identifiability of sparse linear ODEs, provides lower bounds for unidentifiability probability, and empirically tests state-of-the-art estimation methods.

Result: Sparse systems are unidentifiable with positive probability, and theoretical unidentifiability manifests in practical estimation methods.

Conclusion: The findings urge rethinking expectations in data-driven dynamical system modeling and enable quantitative trust assessment of learned linear ODEs.

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [464] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/pdf/2506.09824)
*Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, Diana Nurbakova*

Main category: cs.LG

TL;DR: The paper introduces WoLA, a weighted loss method for Byzantine-resilient federated learning, improving gradient alignment in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) faces security threats from Byzantine participants who submit poisonous gradients. Current methods struggle in heterogeneous settings where honest gradients vary widely.

Method: Proposes Worker Label Alignment Loss (WoLA), a weighted loss to align honest gradients and better identify Byzantine ones.

Result: WoLA outperforms state-of-the-art methods in heterogeneous settings, supported by theory and experiments.

Conclusion: WoLA effectively addresses Byzantine threats in FL, especially in heterogeneous environments, enhancing model convergence.

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [465] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/pdf/2506.09998)
*Tim Z. Xiao, Johannes Zenn, Zhen Liu, Weiyang Liu, Robert Bamler, Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The paper introduces Verbalized Rejection Sampling (VRS) to improve LLMs' ability to generate faithful samples from Bernoulli distributions, reducing bias and enhancing reliability without modifying model internals.


<details>
  <summary>Details</summary>
Motivation: LLMs can describe probability distributions but struggle to generate accurate samples, limiting their use in stochastic tasks. This gap is investigated for Bernoulli distributions.

Method: The authors propose VRS, a natural-language adaptation of rejection sampling, prompting LLMs to reason about and accept/reject samples.

Result: VRS significantly reduces sampling bias across models, with theoretical analysis confirming its improvement over direct sampling.

Conclusion: VRS demonstrates how classical probabilistic tools can be verbalized to enhance LLM reliability in sampling tasks without heavy engineering.

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [466] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/pdf/2506.09867)
*Amit Baran Dey, Wasim Arif, Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: A machine learning method classifies oil samples using dielectric properties and a microwave sensor, achieving 99.41% accuracy with random forest.


<details>
  <summary>Details</summary>
Motivation: To enable non-destructive, real-time oil classification for industrial applications using dielectric properties.

Method: Uses a microwave resonant sensor to capture dielectric variations, extracts features, and applies machine learning classifiers.

Result: Achieves 99.41% classification accuracy with random forest.

Conclusion: The system is efficient, compact, and suitable for industrial oil characterization.

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [467] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/pdf/2506.09891)
*Sebastian Hickman, Ilija Trajkovic, Julia Kaltenborn, Francis Pelletier, Alex Archibald, Yaniv Gurwicz, Peer Nowack, David Rolnick, Julien Boussard*

Main category: cs.LG

TL;DR: A physics-informed causal machine learning model is developed to emulate climate dynamics efficiently, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional climate models are computationally expensive, and current machine learning approaches lack physics-informed causal relationships.

Method: Develops an interpretable climate model emulator using causal representation learning and a Bayesian filter for stable long-term autoregressive emulation.

Result: The emulator accurately learns climate dynamics, validated on synthetic and real climate model data.

Conclusion: The proposed method offers an efficient and interpretable alternative to traditional climate modeling.

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [468] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/pdf/2506.09870)
*Maximilian Egger, Rawad Bitar*

Main category: cs.LG

TL;DR: A multi-stage method combining verifiable secret sharing, secure aggregation, and symmetric private information retrieval is proposed to ensure privacy and Byzantine resilience in federated learning with heterogeneous data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining privacy and resilience against Byzantine clients in federated learning, especially when data is heterogeneous.

Method: A co-design of verifiable secret sharing, secure aggregation, and symmetric private information retrieval, evaluated on various attacks.

Result: Outperforms previous techniques in privacy and resilience, with reduced communication overhead via zero-order estimation.

Conclusion: The proposed method effectively balances privacy, resilience, and scalability in heterogeneous federated learning.

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [469] [Learning single-index models via harmonic decomposition](https://arxiv.org/pdf/2506.09887)
*Nirmit Joshi, Hugo Koubbi, Theodor Misiakiewicz, Nathan Srebro*

Main category: cs.LG

TL;DR: The paper explores learning single-index models using spherical harmonics instead of Hermite polynomials, achieving optimal sample complexity or runtime with new estimators.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of prior work by leveraging spherical harmonics to capture the rotational symmetry of single-index models under spherically symmetric inputs.

Method: The authors propose two estimators: one based on tensor unfolding for optimal sample complexity and another using online SGD for optimal runtime.

Result: The approach recovers and clarifies existing results for Gaussian inputs while uncovering new phenomena previously overlooked.

Conclusion: Spherical harmonics provide a natural basis for learning single-index models, with trade-offs between sample complexity and runtime.

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [470] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/pdf/2506.09896)
*Attanasia Garuso, Silvija Kokalj-Filipovic, Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: The paper analyzes how VQVAE suppresses adversarial attacks on high-SNR RF data, showing it reduces attack effectiveness and explores latent space properties for attack detection.


<details>
  <summary>Details</summary>
Motivation: To evaluate VQVAE's ability to mitigate adversarial attacks on digitally modulated waveforms, preserving phase and amplitude modulations.

Method: Created adversarial attacks (phase-preserving and non-preserving), tested classifier accuracy on VQVAE reconstructions, and analyzed latent space distributions.

Result: VQVAE significantly reduces attack effectiveness and reveals latent space properties useful for attack detection.

Conclusion: VQVAE is effective in suppressing adversarial attacks and offers insights into latent space behavior under varying attack strengths.

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [471] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/pdf/2506.09940)
*Jiachen Hu, Rui Ai, Han Zhong, Xiaoyu Chen, Liwei Wang, Zhaoran Wang, Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [472] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/pdf/2506.09901)
*Noel Brindise, Vijeth Hebbar, Riya Shah, Cedric Langbort*

Main category: cs.LG

TL;DR: DNA introduces a method for explainable Reinforcement Learning by generating diverse, near-optimal trajectories for human interpretability and choice.


<details>
  <summary>Details</summary>
Motivation: The goal is to enhance explainability in RL by providing diverse policy options for trajectory-planning agents, aiding human understanding and decision-making.

Method: DNA uses reward shaping in modified Q-learning to produce distinct, epsilon-optimal policies for continuous trajectories in Markov decision processes.

Result: The method successfully generates qualitatively diverse policies, offering meaningful options in simulations, and compares favorably to Quality Diversity approaches.

Conclusion: DNA advances explainable RL and opens new avenues for exploration and adaptive planning in reinforcement learning.

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [473] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/pdf/2506.09923)
*Liou Tang, James Joshi, Ashish Kundu*

Main category: cs.LG

TL;DR: The paper introduces Apollo, a label-only membership inference attack for Machine Unlearning (MU), which infers unlearned samples under a strict threat model.


<details>
  <summary>Details</summary>
Motivation: Existing MU privacy attacks rely on weaker threat models, limiting real-world applicability. Apollo addresses this by requiring only label-output access.

Method: Proposes Apollo, a posteriori label-only attack, to infer unlearned samples without needing the original model.

Result: Apollo achieves high precision in identifying unlearned samples despite stricter access constraints.

Conclusion: Apollo demonstrates effective privacy threats under realistic conditions, highlighting vulnerabilities in MU systems.

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [474] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/pdf/2506.09928)
*Ruixuan Xu, Xiangxiang Weng*

Main category: cs.LG

TL;DR: The paper compares MCMC and VI for approximating the posterior in Probabilistic Matrix Factorization, finding VI faster but MCMC more accurate.


<details>
  <summary>Details</summary>
Motivation: Address the intractability of computing the posterior distribution in PMF by comparing Bayesian inference methods.

Method: Uses MCMC and VI to approximate the posterior in PMF, evaluated on the MovieLens dataset.

Result: VI converges faster, but MCMC provides more accurate posterior estimates.

Conclusion: VI is efficient for speed, while MCMC is preferable for accuracy in PMF.

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [475] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/pdf/2506.09955)
*Yitao Xu, Tong Zhang, Ehsan Pajouheshgar, Sabine Süsstrunk*

Main category: cs.LG

TL;DR: The paper introduces CLAReps, latent codes in CDMs that preserve class-defining features while discarding irrelevant context, enabling robust and interpretable representations. A distillation method, CaDistill, uses CLAReps to transfer core class knowledge efficiently, improving student model robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: CDMs entangle class-defining features with irrelevant context, making it hard to extract robust and interpretable representations. The goal is to disentangle these features for better discriminative learning.

Method: Proposes CLAReps (Canonical LAtent Representations) to extract essential categorical information from CDMs. Develops CaDistill, a diffusion-based feature-distillation paradigm using CLAReps as a compact teacher signal.

Result: CLAReps produce interpretable and compact class summaries. CaDistill achieves strong adversarial robustness and generalization, focusing on class signals rather than spurious cues.

Conclusion: CDMs can serve as compact, interpretable teachers for robust representation learning, not just as image generators.

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [476] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/pdf/2506.09991)
*Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, Beidi Chen*

Main category: cs.LG

TL;DR: Multiverse is a parallel generative model inspired by AR-LLMs, using a MapReduce paradigm for efficient, scalable, and high-performance generation without human annotations.


<details>
  <summary>Details</summary>
Motivation: To overcome the sequential limitations of AR-LLMs by enabling native parallelism in generation, improving efficiency and performance.

Method: Uses a three-stage MapReduce approach (Map, Process, Reduce) and co-designs data, algorithm (Multiverse Attention), and system (Multiverse Engine) for parallel inference.

Result: Multiverse-32B matches leading AR-LLMs in performance (AIME24 & 25 scores of 54% and 46%) and achieves 2x speedup with superior scaling.

Conclusion: Multiverse offers a scalable, efficient alternative to AR-LLMs, with open-sourced tools and data for broader adoption.

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


### [477] [Effective Regularization Through Loss-Function Metalearning](https://arxiv.org/pdf/2010.00788)
*Santiago Gonzalez, Xin Qiu, Risto Miikkulainen*

Main category: cs.LG

TL;DR: TaylorGLO uses evolutionary computation to optimize neural network loss functions, improving performance, training speed, and data utilization by balancing error reduction and overfitting avoidance.


<details>
  <summary>Details</summary>
Motivation: To theoretically validate that evolved loss functions in TaylorGLO discourage overfitting and improve regularization, and to generalize this principle to other techniques like label smoothing.

Method: The paper employs learning rule decomposition to analyze evolved loss functions, revealing a balance between minimizing error and avoiding overfitting.

Result: Theoretical analysis confirms the regularization effect of TaylorGLO, leading to more robust networks and a practical constraint for designing better loss functions.

Conclusion: The study advances understanding of regularization and highlights the potential of evolutionary neural architecture search.

Abstract: Evolutionary computation can be used to optimize several different aspects of
neural network architectures. For instance, the TaylorGLO method discovers
novel, customized loss functions, resulting in improved performance, faster
training, and improved data utilization. A likely reason is that such functions
discourage overfitting, leading to effective regularization. This paper
demonstrates theoretically that this is indeed the case for TaylorGLO. Learning
rule decomposition reveals that evolved loss functions balance two factors: the
pull toward zero error, and a push away from it to avoid overfitting. This is a
general principle that may be used to understand other regularization
techniques as well (as demonstrated in this paper for label smoothing). The
theoretical analysis leads to a constraint that can be utilized to find more
effective loss functions in practice; the mechanism also results in networks
that are more robust (as demonstrated in this paper with adversarial inputs).
The analysis in this paper thus constitutes a first step towards understanding
regularization, and demonstrates the power of evolutionary neural architecture
search in general.

</details>


### [478] [Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems](https://arxiv.org/pdf/2307.08423)
*Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Alex Strasser, Haiyang Yu, YuQing Xie, Xiang Fu, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, Shuiwang Ji*

Main category: cs.LG

TL;DR: The paper provides a technical overview of AI4Science, focusing on AI applications for quantum, atomistic, and continuum systems, addressing challenges like symmetry equivariance and generalization.


<details>
  <summary>Details</summary>
Motivation: To unify and technically treat the interdisciplinary field of AI4Science, particularly for understanding physical systems across scales.

Method: In-depth analysis of deep learning techniques for capturing physics principles, with emphasis on symmetry equivariance, explainability, and uncertainty quantification.

Result: A thorough technical account of AI methods for physical systems, highlighting common challenges and solutions.

Conclusion: The work aims to inspire further community efforts in advancing AI4Science by providing foundational insights and resources.

Abstract: Advances in artificial intelligence (AI) are fueling a new paradigm of
discoveries in natural sciences. Today, AI has started to advance natural
sciences by improving, accelerating, and enabling our understanding of natural
phenomena at a wide range of spatial and temporal scales, giving rise to a new
area of research known as AI for science (AI4Science). Being an emerging
research paradigm, AI4Science is unique in that it is an enormous and highly
interdisciplinary area. Thus, a unified and technical treatment of this field
is needed yet challenging. This work aims to provide a technically thorough
account of a subarea of AI4Science; namely, AI for quantum, atomistic, and
continuum systems. These areas aim at understanding the physical world from the
subatomic (wavefunctions and electron density), atomic (molecules, proteins,
materials, and interactions), to macro (fluids, climate, and subsurface) scales
and form an important subarea of AI4Science. A unique advantage of focusing on
these areas is that they largely share a common set of challenges, thereby
allowing a unified and foundational treatment. A key common challenge is how to
capture physics first principles, especially symmetries, in natural systems by
deep learning methods. We provide an in-depth yet intuitive account of
techniques to achieve equivariance to symmetry transformations. We also discuss
other common technical challenges, including explainability,
out-of-distribution generalization, knowledge transfer with foundation and
large language models, and uncertainty quantification. To facilitate learning
and education, we provide categorized lists of resources that we found to be
useful. We strive to be thorough and unified and hope this initial effort may
trigger more community interests and efforts to further advance AI4Science.

</details>


### [479] [Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics](https://arxiv.org/pdf/2309.16109)
*Han Bao*

Main category: cs.LG

TL;DR: The paper extends prior contrastive learning theory by incorporating feature normalization, showing it prevents representation collapse under cosine loss.


<details>
  <summary>Details</summary>
Motivation: To address the gap in prior work, which ignored feature normalization's role in preventing collapse in non-contrastive learning.

Method: Extends Tian et al.'s theory by analyzing cosine loss dynamics, revealing sixth-order behavior compared to L2 loss's third-order.

Result: Feature normalization robustly prevents collapse, with stable equilibria emerging dynamically even from collapsed initial states.

Conclusion: Feature normalization is crucial for avoiding collapse in non-contrastive learning, offering new theoretical insights.

Abstract: Contrastive learning is a self-supervised representation learning framework,
where two positive views generated through data augmentation are made similar
by an attraction force in a data representation space, while a repulsive force
makes them far from negative examples. Non-contrastive learning, represented by
BYOL and SimSiam, further gets rid of negative examples and improves
computational efficiency. While learned representations may collapse into a
single point due to the lack of the repulsive force at first sight, Tian et al.
(2021) revealed through the learning dynamics analysis that the representations
can avoid collapse if data augmentation is sufficiently stronger than
regularization. However, their analysis does not take into account
commonly-used feature normalization, a normalizer before measuring the
similarity of representations, and hence excessively strong regularization may
collapse the dynamics, which is an unnatural behavior under the presence of
feature normalization. Therefore, we extend the previous theory based on the L2
loss by considering the cosine loss, which involves feature normalization. We
show that the cosine loss induces sixth-order dynamics (while the L2 loss
induces a third-order one), in which a stable equilibrium dynamically emerges
even if there are only collapsed solutions with given initial parameters. Thus,
we offer a new understanding that feature normalization plays an important role
in robustly preventing the dynamics collapse.

</details>


### [480] [Byzantine-Resilient Decentralized Multi-Armed Bandits](https://arxiv.org/pdf/2310.07320)
*Jingxuan Zhu, Alec Koppel, Alvaro Velasquez, Ji Liu*

Main category: cs.LG

TL;DR: A decentralized resilient UCB algorithm is proposed for cooperative multi-armed bandits with Byzantine agents, ensuring regret performance no worse than single-agent UCB1 and better than non-cooperative cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Byzantine agents in decentralized cooperative MAB, which can disrupt reward estimates and confidence sets, modeling real-world scenarios like network attacks or market manipulation.

Method: Develops a decentralized resilient UCB algorithm combining information mixing among agents and truncation of inconsistent/extreme values. Requires each agent to have at least 3f+1 neighbors (f: max Byzantine agents).

Result: Normal agents perform no worse than single-agent UCB1, and cumulative regret is better than non-cooperative cases. Extensions for time-varying graphs and minimax bounds are provided.

Conclusion: The framework effectively mitigates Byzantine behavior, improving cooperative performance with theoretical guarantees and practical validation.

Abstract: In decentralized cooperative multi-armed bandits (MAB), each agent observes a
distinct stream of rewards, and seeks to exchange information with others to
select a sequence of arms so as to minimize its regret. Agents in the
cooperative setting can outperform a single agent running a MAB method such as
Upper-Confidence Bound (UCB) independently. In this work, we study how to
recover such salient behavior when an unknown fraction of the agents can be
Byzantine, that is, communicate arbitrarily wrong information in the form of
reward mean-estimates or confidence sets. This framework can be used to model
attackers in computer networks, instigators of offensive content into
recommender systems, or manipulators of financial markets. Our key contribution
is the development of a fully decentralized resilient upper confidence bound
(UCB) algorithm that fuses an information mixing step among agents with a
truncation of inconsistent and extreme values. This truncation step enables us
to establish that the performance of each normal agent is no worse than the
classic single-agent UCB1 algorithm in terms of regret, and more importantly,
the cumulative regret of all normal agents is strictly better than the
non-cooperative case, provided that each agent has at least 3f+1 neighbors
where f is the maximum possible Byzantine agents in each agent's neighborhood.
Extensions to time-varying neighbor graphs, and minimax lower bounds are
further established on the achievable regret. Experiments corroborate the
merits of this framework in practice.

</details>


### [481] [Using Shapley interactions to understand how models use structure](https://arxiv.org/pdf/2403.13106)
*Divyansh Singhvi, Diganta Misra, Andrej Erkelens, Raghav Jain, Isabel Papadimitriou, Naomi Saphra*

Main category: cs.LG

TL;DR: The paper uses Shapley Taylor interaction indices (STII) to analyze how language and speech models internally structure inputs, revealing correlations with syntactic, semantic, and phonetic linguistic structures.


<details>
  <summary>Details</summary>
Motivation: To understand how language models internally represent linguistic structures like syntax, semantics, and phonetics.

Method: Employed Shapley Taylor interaction indices (STII) to measure pairwise interactions between inputs in models, linking these to linguistic structures.

Result: Autoregressive models show syntactic proximity correlations, while both autoregressive and masked models encode nonlinear interactions in idiomatic phrases. Speech models reflect phonetic coarticulation.

Conclusion: Language models encode linguistic structures internally, with interactions aligning with syntax, non-compositional semantics, and phonetic coarticulation.

Abstract: Language is an intricately structured system, and a key goal of NLP
interpretability is to provide methodological insights for understanding how
language models represent this structure internally. In this paper, we use
Shapley Taylor interaction indices (STII) in order to examine how language and
speech models internally relate and structure their inputs. Pairwise Shapley
interactions measure how much two inputs work together to influence model
outputs beyond if we linearly added their independent influences, providing a
view into how models encode structural interactions between inputs. We relate
the interaction patterns in models to three underlying linguistic structures:
syntactic structure, non-compositional semantics, and phonetic coarticulation.
We find that autoregressive text models encode interactions that correlate with
the syntactic proximity of inputs, and that both autoregressive and masked
models encode nonlinear interactions in idiomatic phrases with
non-compositional semantics. Our speech results show that inputs are more
entangled for pairs where a neighboring consonant is likely to influence a
vowel or approximant, showing that models encode the phonetic interaction
needed for extracting discrete phonemic representations.

</details>


### [482] [Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning](https://arxiv.org/pdf/2405.20761)
*Aditya Shankar, Jérémie Decouchant, Dimitra Gkorou, Rihan Hai, Lydia Y. Chen*

Main category: cs.LG

TL;DR: STV is a privacy-preserving VFL framework for time series forecasting, addressing data privacy, over-fitting, and scalability with strong convergence and low-tuning complexity.


<details>
  <summary>Details</summary>
Motivation: Challenges in VFL for time series forecasting include data privacy, over-fitting on small/noisy datasets, and scalability with strong convergence.

Method: STV uses secret sharing, multi-party computation, and novel N-party algorithms for matrix operations to ensure privacy and exact parameter optimization.

Result: STV matches centralized forecasting accuracy and outperforms state-of-the-art methods by 23.81% in accuracy. It also evaluates scalability via communication costs.

Conclusion: STV is an effective, scalable, and privacy-preserving solution for time series forecasting in VFL, with open-source availability.

Abstract: Vertical federated learning (VFL) is a promising area for time series
forecasting in many applications, such as healthcare and manufacturing.
Critical challenges to address include data privacy and over-fitting on small
and noisy datasets during both training and inference. Additionally, such
forecasting models must scale well with the number of parties while ensuring
strong convergence and low-tuning complexity. We address these challenges and
propose ``Secret-shared Time Series Forecasting with VFL'' (STV), a novel
framework with the following key features: i) a privacy-preserving algorithm
for forecasting with SARIMAX and autoregressive trees on vertically-partitioned
data; ii) decentralised forecasting using secret sharing and multi-party
computation; and iii) novel N-party algorithms for matrix multiplication and
inverse operations for exact parameter optimization, giving strong convergence
with minimal tuning complexity. We evaluate on six representative datasets from
public and industry-specific contexts. Results demonstrate that STV's
forecasting accuracy is comparable to those of centralized approaches. Our
exact optimization outperforms centralized methods, including state-of-the-art
diffusion models and long-short-term memory, by 23.81% on forecasting accuracy.
We also evaluate scalability by examining the communication costs of exact and
iterative optimization to navigate the choice between the two. STV's code and
supplementary material is available online: https://github.com/adis98/STV.

</details>


### [483] [PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints](https://arxiv.org/pdf/2406.12338)
*Carla Schenker, Xiulin Wang, David Horner, Morten A. Rasmussen, Evrim Acar*

Main category: cs.LG

TL;DR: A flexible algorithmic framework for PARAFAC2-based CMTF models is introduced, enabling various constraints and couplings, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing PARAFAC2-based CMTF models have limitations in regularizations and coupling types, prompting the need for a more flexible solution.

Method: The framework uses Alternating Optimization (AO) and the Alternating Direction Method of Multipliers (ADMM) to fit models with constraints on all modes and couplings.

Result: Experiments show the framework's utility, versatility, and superior accuracy and efficiency compared to state-of-the-art methods.

Conclusion: The proposed framework addresses limitations of existing models, offering enhanced flexibility and performance in data fusion tasks.

Abstract: Data fusion models based on Coupled Matrix and Tensor Factorizations (CMTF)
have been effective tools for joint analysis of data from multiple sources.
While the vast majority of CMTF models are based on the strictly multilinear
CANDECOMP/PARAFAC (CP) tensor model, recently also the more flexible PARAFAC2
model has been integrated into CMTF models. PARAFAC2 tensor models can handle
irregular/ragged tensors and have shown to be especially useful for modelling
dynamic data with unaligned or irregular time profiles. However, existing
PARAFAC2-based CMTF models have limitations in terms of possible
regularizations on the factors and/or types of coupling between datasets. To
address these limitations, in this paper we introduce a flexible algorithmic
framework that fits PARAFAC2-based CMTF models using Alternating Optimization
(AO) and the Alternating Direction Method of Multipliers (ADMM). The proposed
framework allows to impose various constraints on all modes and linear
couplings to other matrix-, CP- or PARAFAC2-models. Experiments on various
simulated and a real dataset demonstrate the utility and versatility of the
proposed framework as well as its benefits in terms of accuracy and efficiency
in comparison with state-of-the-art methods.

</details>


### [484] [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/pdf/2406.19384)
*Vedang Lad, Wes Gurnee, Max Tegmark*

Main category: cs.LG

TL;DR: LLMs retain 72-95% accuracy despite layer deletions/swaps, with early/final layers most sensitive. A 4-stage inference framework is proposed.


<details>
  <summary>Details</summary>
Motivation: To understand LLM robustness to structural interventions and uncover depth-dependent computation patterns.

Method: Delete/swap adjacent layers during inference, measure accuracy drop, and analyze layer sensitivity.

Result: Models are robust to middle-layer interventions; early/final layers are critical. A 4-stage inference process is identified.

Conclusion: LLMs exhibit localized sensitivity, motivating a framework for depth-dependent computation interpretation.

Abstract: We investigate the robustness of Large Language Models (LLMs) to structural
interventions by deleting and swapping adjacent layers during inference.
Surprisingly, models retain 72-95% of their original top-1 prediction accuracy
without any fine-tuning. We find that performance degradation is not uniform
across layers: interventions to the early and final layers cause the most
degradation, while the model is remarkably robust to dropping middle layers.
This pattern of localized sensitivity motivates our hypothesis of four stages
of inference, observed across diverse model families and sizes: (1)
detokenization, where local context is integrated to lift raw token embeddings
into higher-level representations; (2) feature engineering, where task- and
entity-specific features are iteratively refined; (3) prediction ensembling,
where hidden states are aggregated into plausible next-token predictions; and
(4) residual sharpening, where irrelevant features are suppressed to finalize
the output distribution. Synthesizing behavioral and mechanistic evidence, we
provide a framework for interpreting depth-dependent computations in LLMs.

</details>


### [485] [Learning Time-Varying Multi-Region Brain Communications via Scalable Markovian Gaussian Processes](https://arxiv.org/pdf/2407.00397)
*Weihan Li, Yule Wang, Chengrui Li, Anqi Wu*

Main category: cs.LG

TL;DR: A novel framework (ADM) using Markovian Gaussian Processes is introduced to model time-varying brain communications, addressing scalability and dynamic interaction challenges in neural data.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to capture time-varying, region-level brain communications or scale to large neural datasets with long recordings.

Method: Combines Gaussian Processes with State Space Models and uses parallel scan inference for efficient scaling, identifying evolving communication patterns.

Result: Validated on synthetic and neural datasets, ADM successfully captures directionality and temporal dynamics of neural interactions.

Conclusion: ADM advances understanding of dynamic brain networks and offers a scalable tool for analyzing neural communication.

Abstract: Understanding and constructing brain communications that capture dynamic
communications across multiple regions is fundamental to modern system
neuroscience, yet current methods struggle to find time-varying region-level
communications or scale to large neural datasets with long recording durations.
We present a novel framework using Markovian Gaussian Processes to learn brain
communications with time-varying temporal delays from multi-region neural
recordings, named Adaptive Delay Model (ADM). Our method combines Gaussian
Processes with State Space Models and employs parallel scan inference
algorithms, enabling efficient scaling to large datasets while identifying
concurrent communication patterns that evolve over time. This time-varying
approach captures how brain region interactions shift dynamically during
cognitive processes. Validated on synthetic and multi-region neural recordings
datasets, our approach discovers both the directionality and temporal dynamics
of neural communication. This work advances our understanding of distributed
neural computation and provides a scalable tool for analyzing dynamic brain
networks.

</details>


### [486] [Metric-Entropy Limits on the Approximation of Nonlinear Dynamical Systems](https://arxiv.org/pdf/2407.01250)
*Yang Pan, Clemens Hutter, Helmut Bölcskei*

Main category: cs.LG

TL;DR: RNNs can optimally approximate nonlinear dynamical systems with Lipschitz properties and fast input forgetting, using refined metric-entropy analysis.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental limits on approximating nonlinear dynamical systems, focusing on RNNs' capabilities.

Method: Refined metric-entropy characterization (order, type, generalized dimension) for exponentially/polynomially Lipschitz fading-memory systems.

Result: RNNs achieve metric-entropy-optimal approximation for these systems.

Conclusion: RNNs are effective for approximating complex nonlinear dynamical systems under specific conditions.

Abstract: This paper is concerned with fundamental limits on the approximation of
nonlinear dynamical systems. Specifically, we show that recurrent neural
networks (RNNs) can approximate nonlinear systems -- that satisfy a Lipschitz
property and forget past inputs fast enough -- in metric-entropy-optimal
manner. As the sets of sequence-to-sequence mappings realized by the dynamical
systems we consider are significantly more massive than function classes
generally analyzed in approximation theory, a refined metric-entropy
characterization is needed, namely in terms of order, type, and generalized
dimension. We compute these quantities for the classes of exponentially- and
polynomially Lipschitz fading-memory systems and show that RNNs can achieve
them.

</details>


### [487] [Identifiable Latent Bandits: Leveraging observational data for personalized decision-making](https://arxiv.org/pdf/2407.16239)
*Ahmet Zahid Balcıoğlu, Newton Mwai, Emil Carlsson, Fredrik D. Johansson*

Main category: cs.LG

TL;DR: The paper introduces an identifiable latent bandit framework for faster personalized decision-making by leveraging historical data and nonlinear independent component analysis.


<details>
  <summary>Details</summary>
Motivation: Historical data alone often fail to provide optimal decisions for new instances, and traditional bandit algorithms require too many trials for practical use.

Method: The proposed method uses nonlinear independent component analysis to learn identifiable latent representations from historical data, enabling faster exploration and personalization.

Result: The framework significantly reduces exploration time compared to classical bandits and outperforms online and offline baselines in simulated and semi-synthetic environments.

Conclusion: The identifiable latent bandit framework offers a practical solution for rapid, personalized decision-making when identification conditions are met.

Abstract: For many decision-making tasks, such as precision medicine, historical data
alone are insufficient to determine the right choice for a new problem instance
or patient. Online algorithms like multi-armed bandits can find optimal
personalized decisions but are notoriously sample-hungry. In practice, training
a bandit for a new individual from scratch is often infeasible, as the number
of trials required is larger than the practical number of decision points.
Latent bandits offer rapid exploration and personalization beyond what context
variables can reveal, provided that a latent variable model can be learned
consistently. In this work, we propose an identifiable latent bandit framework
that leads to optimal decision-making with a shorter exploration time than
classical bandits by learning from historical records of decisions and
outcomes. Our method is based on nonlinear independent component analysis that
provably identifies representations from observational data sufficient to infer
the optimal action in new bandit instances. We verify this strategy in
simulated and semi-synthetic environments, showing substantial improvement over
online and offline learning baselines when identifying conditions are
satisfied.

</details>


### [488] [Electroencephalogram Emotion Recognition via AUC Maximization](https://arxiv.org/pdf/2408.08979)
*Minheng Xiao*

Main category: cs.LG

TL;DR: The paper addresses class imbalance in datasets, focusing on the 'Liking' label in the DEAP dataset. It proposes AUC maximization via numerical optimization, outperforming traditional classifiers like logistic regression and SVM.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in datasets, especially in neuroscience and medical diagnostics, is often overlooked, leading to poor detection of minority classes.

Method: The study uses numerical optimization to maximize AUC, starting with a linear classifier, and compares it to traditional methods like logistic regression and SVM.

Result: The proposed method significantly improves recall (41.6% to 79.7%) and F1-score (0.506 to 0.632).

Conclusion: AUC maximization via numerical optimization is effective for handling imbalanced datasets, enhancing minority class detection.

Abstract: Imbalanced datasets pose significant challenges in areas including
neuroscience, cognitive science, and medical diagnostics, where accurately
detecting minority classes is essential for robust model performance. This
study addresses the issue of class imbalance, using the `Liking' label in the
DEAP dataset as an example. Such imbalances are often overlooked by prior
research, which typically focuses on the more balanced arousal and valence
labels and predominantly uses accuracy metrics to measure model performance. To
tackle this issue, we adopt numerical optimization techniques aimed at
maximizing the area under the curve (AUC), thus enhancing the detection of
underrepresented classes. Our approach, which begins with a linear classifier,
is compared against traditional linear classifiers, including logistic
regression and support vector machines (SVM). Our method significantly
outperforms these models, increasing recall from 41.6\% to 79.7\% and improving
the F1-score from 0.506 to 0.632. These results highlight the efficacy of AUC
maximization via numerical optimization in managing imbalanced datasets,
providing an effective solution for enhancing predictive accuracy in detecting
minority but crucial classes in out-of-sample datasets.

</details>


### [489] [DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images](https://arxiv.org/pdf/2409.06694)
*Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson*

Main category: cs.LG

TL;DR: The paper proposes DANCE, a method using Chaos Game Representation (CGR) and kaleidoscopic images to visualize T-cell receptor (TCR) protein sequences for deep learning-based classification of cancer-targeting TCRs.


<details>
  <summary>Details</summary>
Motivation: TCRs play a key role in cancer immunity, but their analysis is challenging due to their short lengths. Efficient representations are needed to capture their structural and functional details.

Method: The DANCE method converts TCR protein sequences into images using CGR and kaleidoscopic techniques, then employs deep learning models for classification based on cancer cell targets.

Result: The study demonstrates the feasibility of using image-based representations and deep learning to classify TCRs, revealing insights into protein properties through visual patterns.

Conclusion: Combining CGR-based image generation with deep learning offers novel possibilities for protein sequence analysis, particularly in TCR research for cancer immunotherapy.

Abstract: Cancer is a complex disease characterized by uncontrolled cell growth. T cell
receptors (TCRs), crucial proteins in the immune system, play a key role in
recognizing antigens, including those associated with cancer. Recent
advancements in sequencing technologies have facilitated comprehensive
profiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity
and enabling TCR-based immunotherapies. However, analyzing these intricate
biomolecules necessitates efficient representations that capture their
structural and functional information. T-cell protein sequences pose unique
challenges due to their relatively smaller lengths compared to other
biomolecules. An image-based representation approach becomes a preferred choice
for efficient embeddings, allowing for the preservation of essential details
and enabling comprehensive analysis of T-cell protein sequences. In this paper,
we propose to generate images from the protein sequences using the idea of
Chaos Game Representation (CGR) using the Kaleidoscopic images approach. This
Deep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced
Kaleidoscopic Images (called DANCE) provides a unique way to visualize protein
sequences by recursively applying chaos game rules around a central seed point.
we perform the classification of the T cell receptors (TCRs) protein sequences
in terms of their respective target cancer cells, as TCRs are known for their
immune response against cancer disease. The TCR sequences are converted into
images using the DANCE method. We employ deep-learning vision models to perform
the classification to obtain insights into the relationship between the visual
patterns observed in the generated kaleidoscopic images and the underlying
protein properties. By combining CGR-based image generation with deep learning
classification, this study opens novel possibilities in the protein analysis
domain.

</details>


### [490] [Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions](https://arxiv.org/pdf/2409.09778)
*Siqiao Mu, Diego Klabjan*

Main category: cs.LG

TL;DR: Proposes a first-order black-box algorithm for certified machine unlearning on general nonconvex loss functions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the need to efficiently remove data from models without retraining, respecting privacy and utility.

Method: Uses a 'rewinding' approach during training, followed by gradient descent on retained data.

Result: Achieves (ε, δ) certified unlearning and demonstrates superior performance in practical scenarios.

Conclusion: Provides a robust solution for unlearning in nonconvex settings with strong privacy-utility guarantees.

Abstract: Machine unlearning algorithms aim to efficiently remove data from a model
without retraining it from scratch, in order to remove corrupted or outdated
data or respect a user's ``right to be forgotten." Certified machine unlearning
is a strong theoretical guarantee based on differential privacy that quantifies
the extent to which an algorithm erases data from the model weights. In
contrast to existing works in certified unlearning for convex or strongly
convex loss functions, or nonconvex objectives with limiting assumptions, we
propose the first, first-order, black-box (i.e., can be applied to models
pretrained with vanilla gradient descent) algorithm for unlearning on general
nonconvex loss functions, which unlearns by ``rewinding" to an earlier step
during the learning process before performing gradient descent on the loss
function of the retained data points. We prove $(\epsilon, \delta)$ certified
unlearning and performance guarantees that establish the
privacy-utility-complexity tradeoff of our algorithm, and we prove
generalization guarantees for functions that satisfy the Polyak-Lojasiewicz
inequality. Finally, we demonstrate the superior performance of our algorithm
compared to existing methods, within a new experimental framework that more
accurately reflects unlearning user data in practice.

</details>


### [491] [A Generative Framework for Predictive Modeling of Multiple Chronic Conditions Using Graph Variational Autoencoder and Bandit-Optimized Graph Neural Network](https://arxiv.org/pdf/2409.13671)
*Julian Carvajal Rico, Adel Alaeddini, Syed Hasib Akhter Faruqui, Susan P Fisher-Hoch, Joseph B Mccormick*

Main category: cs.LG

TL;DR: A generative framework using GNNs and GVAE constructs patient similarity graphs for predicting MCC, improving accuracy with Laplacian regularization and contextual Bandit evaluation.


<details>
  <summary>Details</summary>
Motivation: Predicting MCC is vital for early intervention and personalized care, but GNNs require existing graph structures, which are lacking for MCC.

Method: Proposes a GVAE-based generative framework to create patient similarity graphs, refined with Laplacian regularization and evaluated by a contextual Bandit.

Result: Validated on 1,592 MCC patients, the method outperforms ε-Greedy and multi-armed Bandit algorithms in prediction accuracy.

Conclusion: The approach enhances MCC prediction, supporting personalized and proactive healthcare management.

Abstract: Predicting the emergence of multiple chronic conditions (MCC) is crucial for
early intervention and personalized healthcare, as MCC significantly impacts
patient outcomes and healthcare costs. Graph neural networks (GNNs) are
effective methods for modeling complex graph data, such as those found in MCC.
However, a significant challenge with GNNs is their reliance on an existing
graph structure, which is not readily available for MCC. To address this
challenge, we propose a novel generative framework for GNNs that constructs a
representative underlying graph structure by utilizing the distribution of the
data to enhance predictive analytics for MCC. Our framework employs a graph
variational autoencoder (GVAE) to capture the complex relationships in patient
data. This allows for a comprehensive understanding of individual health
trajectories and facilitates the creation of diverse patient stochastic
similarity graphs while preserving the original feature set. These variations
of patient stochastic similarity graphs, generated from the GVAE decoder, are
then processed by a GNN using a novel Laplacian regularization technique to
refine the graph structure over time and improves the prediction accuracy of
MCC. A contextual Bandit is designed to evaluate the stochastically generated
graphs and identify the best-performing graph for the GNN model iteratively
until model convergence. We validate the performance of the proposed contextual
Bandit algorithm against $\varepsilon$-Greedy and multi-armed Bandit algorithms
on a large cohort (n = 1,592) of patients with MCC. These advancements
highlight the potential of the proposed approach to transform predictive
healthcare analytics, enabling a more personalized and proactive approach to
MCC management.

</details>


### [492] [The Causal Information Bottleneck and Optimal Causal Variable Abstractions](https://arxiv.org/pdf/2410.00535)
*Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen*

Main category: cs.LG

TL;DR: The paper introduces the Causal Information Bottleneck (CIB), a method to create causally interpretable variable abstractions while preserving causal control over a target variable.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like the Information Bottleneck (IB) ignore causal structures, making them unsuitable for causal tasks. The authors aim to address this gap.

Method: The CIB extends the IB by compressing variables while maintaining causal control over a target variable, ensuring causally interpretable abstractions.

Result: Experimental results show that CIB accurately captures causal relations, providing insights into variable interactions and intervention reasoning.

Conclusion: CIB is a promising approach for causal abstraction tasks, outperforming traditional statistical methods in causal contexts.

Abstract: To effectively study complex causal systems, it is often useful to construct
abstractions of parts of the system by discarding irrelevant details while
preserving key features. The Information Bottleneck (IB) method is a widely
used approach to construct variable abstractions by compressing random
variables while retaining predictive power over a target variable. Traditional
methods like IB are purely statistical and ignore underlying causal structures,
making them ill-suited for causal tasks. We propose the Causal Information
Bottleneck (CIB), a causal extension of the IB, which compresses a set of
chosen variables while maintaining causal control over a target variable. This
method produces abstractions of (sets of) variables which are causally
interpretable, give us insight about the interactions between the abstracted
variables and the target variable, and can be used when reasoning about
interventions. We present experimental results demonstrating that the learned
abstractions accurately capture causal relations as intended.

</details>


### [493] [Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation](https://arxiv.org/pdf/2410.01655)
*Roussel Desmond Nzoyem, David A. W. Barton, Tom Deakin*

Main category: cs.LG

TL;DR: The paper introduces extensions to Contextual Self-Modulation (CSM) for Neural Context Flows (NCFs), addressing limitations in applicability and scalability, and demonstrates their effectiveness across various tasks.


<details>
  <summary>Details</summary>
Motivation: CSM is powerful for meta-learning on physical systems but has limitations in cross-modality and high-data scenarios. The work aims to expand its applicability and scalability.

Method: Two extensions are proposed: $i$CSM for infinite-dimensional variations and StochasticNCF for scalable meta-gradient updates. Higher-order Taylor expansions and integration with FlashCAVIA are also explored.

Result: Experiments on dynamical systems, computer vision, and curve fitting show the extensions' effectiveness. Higher-order approximations do not consistently improve generalization.

Conclusion: CSM's strengths in meta-learning and out-of-distribution tasks are highlighted, especially for physical systems. An open-source library is provided for modular integration.

Abstract: Contextual Self-Modulation (CSM) (Nzoyem et al., 2025) is a potent
regularization mechanism for Neural Context Flows (NCFs) which demonstrates
powerful meta-learning on physical systems. However, CSM has limitations in its
applicability across different modalities and in high-data regimes. In this
work, we introduce two extensions: $i$CSM which expands CSM to
infinite-dimensional variations by embedding the contexts into a function
space, and StochasticNCF which improves scalability by providing a low-cost
approximation of meta-gradient updates through a sampled set of nearest
environments. These extensions are demonstrated through comprehensive
experimentation on a range of tasks, including dynamical systems, computer
vision challenges, and curve fitting problems. Additionally, we incorporate
higher-order Taylor expansions via Taylor-Mode automatic differentiation,
revealing that higher-order approximations do not necessarily enhance
generalization. Finally, we demonstrate how CSM can be integrated into other
meta-learning frameworks with FlashCAVIA, a computationally efficient extension
of the CAVIA meta-learning framework (Zintgraf et al., 2019). Together, these
contributions highlight the significant benefits of CSM and indicate that its
strengths in meta-learning and out-of-distribution tasks are particularly
well-suited to physical systems. Our open-source library, designed for modular
integration of self-modulation into contextual meta-learning workflows, is
available at https://github.com/ddrous/self-mod.

</details>


### [494] [TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised Time Series Representation](https://arxiv.org/pdf/2410.05711)
*Daoyu Wang, Mingyue Cheng, Zhiding Liu, Qi Liu*

Main category: cs.LG

TL;DR: TimeDART is a self-supervised time series pre-training framework combining causal Transformer and denoising diffusion for global and local pattern capture, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods struggle to unify long-term dynamics and local patterns in time series analysis.

Method: Uses a causal Transformer for global trends and denoising diffusion for local patterns, optimized autoregressively.

Result: Outperforms previous methods in forecasting and classification on public datasets.

Conclusion: TimeDART effectively unifies global and local features, demonstrating superior performance.

Abstract: Self-supervised learning has garnered increasing attention in time series
analysis for benefiting various downstream tasks and reducing reliance on
labeled data. Despite its effectiveness, existing methods often struggle to
comprehensively capture both long-term dynamic evolution and subtle local
patterns in a unified manner. In this work, we propose \textbf{TimeDART}, a
novel self-supervised time series pre-training framework that unifies two
powerful generative paradigms to learn more transferable representations.
Specifically, we first employ a causal Transformer encoder, accompanied by a
patch-based embedding strategy, to model the evolving trends from left to
right. Building on this global modeling, we further introduce a denoising
diffusion process to capture fine-grained local patterns through forward
diffusion and reverse denoising. Finally, we optimize the model in an
autoregressive manner. As a result, TimeDART effectively accounts for both
global and local sequence features in a coherent way. We conduct extensive
experiments on public datasets for time series forecasting and classification.
The experimental results demonstrate that TimeDART consistently outperforms
previous compared methods, validating the effectiveness of our approach. Our
code is available at https://github.com/Melmaphother/TimeDART.

</details>


### [495] [CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis](https://arxiv.org/pdf/2411.00696)
*Fuying Wang, Feng Wu, Yihan Tang, Lequan Yu*

Main category: cs.LG

TL;DR: The paper introduces a Cross-Modal Temporal Pattern Discovery (CTPD) framework to improve clinical outcome predictions by identifying and aligning temporal patterns across multimodal EHR data.


<details>
  <summary>Details</summary>
Motivation: Prior work overlooked critical temporal patterns across patients in multimodal EHR data, which are essential for accurate clinical outcome predictions.

Method: The CTPD framework uses shared initial temporal pattern representations refined with slot attention and employs a contrastive-based TPNCE loss for cross-modal alignment, along with reconstruction losses.

Result: Evaluations on MIMIC-III for mortality and phenotype classification tasks show the method outperforms existing approaches.

Conclusion: The CTPD framework effectively captures cross-modal temporal patterns, enhancing clinical prediction accuracy.

Abstract: Integrating multimodal Electronic Health Records (EHR) data, such as
numerical time series and free-text clinical reports, has great potential in
predicting clinical outcomes. However, prior work has primarily focused on
capturing temporal interactions within individual samples and fusing multimodal
information, overlooking critical temporal patterns across patients. These
patterns, such as trends in vital signs like abnormal heart rate or blood
pressure, can indicate deteriorating health or an impending critical event.
Similarly, clinical notes often contain textual descriptions that reflect these
patterns. Identifying corresponding temporal patterns across different
modalities is crucial for improving the accuracy of clinical outcome
predictions, yet it remains a challenging task. To address this gap, we
introduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed
to efficiently extract meaningful cross-modal temporal patterns from multimodal
EHR data. Our approach introduces shared initial temporal pattern
representations which are refined using slot attention to generate temporal
semantic embeddings. To ensure rich cross-modal temporal semantics in the
learned patterns, we introduce a contrastive-based TPNCE loss for cross-modal
alignment, along with two reconstruction losses to retain core information of
each modality. Evaluations on two clinically critical tasks, 48-hour
in-hospital mortality and 24-hour phenotype classification, using the MIMIC-III
database demonstrate the superiority of our method over existing approaches.

</details>


### [496] [Amortized Inference of Causal Models via Conditional Fixed-Point Iterations](https://arxiv.org/pdf/2410.06128)
*Divyat Mahajan, Jannes Gladrow, Agrin Hilmkil, Cheng Zhang, Meyer Scetbon*

Main category: cs.LG

TL;DR: Proposes amortized inference for learning Structural Causal Models (SCMs) using a transformer-based architecture, enabling generalization across datasets without retraining.


<details>
  <summary>Details</summary>
Motivation: Learning SCMs from observed data is challenging and typically requires per-dataset training. This work aims to simplify and generalize the process.

Method: Uses a transformer for dataset embeddings and extends the Fixed-Point Approach (FiP) to infer SCMs conditionally on embeddings.

Result: Matches or outperforms per-dataset baselines, especially in scarce data scenarios, and can generate new data without parameter updates.

Conclusion: Amortized inference is effective for SCM learning, offering scalability and generalization benefits.

Abstract: Structural Causal Models (SCMs) offer a principled framework to reason about
interventions and support out-of-distribution generalization, which are key
goals in scientific discovery. However, the task of learning SCMs from observed
data poses formidable challenges, and often requires training a separate model
for each dataset. In this work, we propose amortized inference of SCMs by
training a single model on multiple datasets sampled from different SCMs. We
first use a transformer-based architecture for amortized learning of dataset
embeddings, and then extend the Fixed-Point Approach (FiP) (Scetbon et al.) to
infer SCMs conditionally on their dataset embeddings. As a byproduct, our
method can generate observational and interventional data from novel SCMs at
inference time, without updating parameters. Empirical results show that our
amortized procedure performs on par with baselines trained specifically for
each dataset on both in and out-of-distribution problems, and also outperforms
them in scare data regimes.

</details>


### [497] [Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity](https://arxiv.org/pdf/2410.08198)
*Shuo Xie, Mohamad Amin Mohamadi, Zhiyuan Li*

Main category: cs.LG

TL;DR: Adam's advantage over SGD in training language models is due to its exploitation of favorable ℓ∞-geometry, unlike SGD which relies on ℓ2-geometry.


<details>
  <summary>Details</summary>
Motivation: The theoretical understanding of why Adam outperforms SGD in training language models is unclear, as existing convergence analyses for both methods are already minimax-optimal in non-convex cases.

Method: The paper provides a new convergence analysis for Adam under novel assumptions of smoothness in ℓ∞-geometry, extending this to blockwise Adam with blockwise smoothness assumptions.

Result: Adam's performance is superior under ℓ∞-geometry, but worsens when this geometry is altered, while SGD remains unaffected. Empirical results confirm this for GPT-2 and ResNet models.

Conclusion: The key advantage of Adam over SGD lies in its ability to exploit favorable ℓ∞-geometry, offering better convergence under specific smoothness conditions.

Abstract: Adam outperforms SGD when training language models. Yet this advantage is not
well-understood theoretically -- previous convergence analysis for Adam and SGD
mainly focuses on the number of steps $T$ and is already minimax-optimal in
non-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In this work, we
argue that the exploitation of nice $\ell_\infty$-geometry is the key advantage
of Adam over SGD. More specifically, we give a new convergence analysis for
Adam under novel assumptions that loss is smooth under $\ell_\infty$-geometry
rather than the more common $\ell_2$-geometry, which yields a much better
empirical smoothness constant for GPT-2 and ResNet models. Our experiments
confirm that Adam performs much worse when the favorable $\ell_\infty$-geometry
is changed while SGD provably remains unaffected. We also extend the
convergence analysis to blockwise Adam under novel blockwise smoothness
assumptions.

</details>


### [498] [An Online Learning Approach to Prompt-based Selection of Generative Models and LLMs](https://arxiv.org/pdf/2410.13287)
*Xiaoyan Hu, Ho-fung Leung, Farzan Farnia*

Main category: cs.LG

TL;DR: The paper proposes an online learning framework (PAK-UCB) to dynamically select the best generative model for different text prompts, improving efficiency by avoiding sub-optimal model queries.


<details>
  <summary>Details</summary>
Motivation: Current methods select generative models based on averaged scores, ignoring prompt-specific performance variations, leading to inefficiency.

Method: PAK-UCB, a contextual bandit algorithm with shared context variables, uses kernel-based functions and RFF for faster online learning.

Result: Experiments show PAK-UCB effectively identifies the best generative model for diverse prompts in text-to-image and image-to-text tasks.

Conclusion: The framework reduces costs and improves performance by adapting model selection to prompt-specific needs.

Abstract: Selecting a sample generation scheme from multiple prompt-based generative
models, including large language models (LLMs) and prompt-guided image and
video generation models, is typically addressed by choosing the model that
maximizes an averaged evaluation score. However, this score-based selection
overlooks the possibility that different models achieve the best generation
performance for different types of text prompts. An online identification of
the best generation model for various input prompts can reduce the costs
associated with querying sub-optimal models. In this work, we explore the
possibility of varying rankings of text-based generative models for different
text prompts and propose an online learning framework to predict the best data
generation model for a given input prompt. The proposed PAK-UCB algorithm
addresses a contextual bandit (CB) setting with shared context variables across
the arms, utilizing the generated data to update kernel-based functions that
predict the score of each model available for unseen text prompts.
Additionally, we leverage random Fourier features (RFF) to accelerate the
online learning process of PAK-UCB. Our numerical experiments on real and
simulated text-to-image and image-to-text generative models show that RFF-UCB
performs successfully in identifying the best generation model across different
sample types. The code is available at:
github.com/yannxiaoyanhu/dgm-online-select.

</details>


### [499] [Revisiting the Equivalence of Bayesian Neural Networks and Gaussian Processes: On the Importance of Learning Activations](https://arxiv.org/pdf/2410.15777)
*Marcin Sendera, Amin Sorkhei, Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: The paper proposes a method to make Bayesian Neural Networks (BNNs) mimic Gaussian Processes (GPs) by using trainable activations and closed-form optimization, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: BNNs lack GP-like properties, such as uncertainty modeling, despite their scalability. The goal is to bridge this gap.

Method: Uses trainable activations and closed-form 2-Wasserstein distance for optimization. Introduces periodic activations and functional priors for GP hyperparameters.

Result: Outperforms or matches heuristic methods with stronger theoretical foundations.

Conclusion: The approach successfully combines BNN scalability with GP-like behavior, offering a robust solution.

Abstract: Gaussian Processes (GPs) provide a convenient framework for specifying
function-space priors, making them a natural choice for modeling uncertainty.
In contrast, Bayesian Neural Networks (BNNs) offer greater scalability and
extendability but lack the advantageous properties of GPs. This motivates the
development of BNNs capable of replicating GP-like behavior. However, existing
solutions are either limited to specific GP kernels or rely on heuristics.
  We demonstrate that trainable activations are crucial for effective mapping
of GP priors to wide BNNs. Specifically, we leverage the closed-form
2-Wasserstein distance for efficient gradient-based optimization of
reparameterized priors and activations. Beyond learned activations, we also
introduce trainable periodic activations that ensure global stationarity by
design, and functional priors conditioned on GP hyperparameters to allow
efficient model selection.
  Empirically, our method consistently outperforms existing approaches or
matches performance of the heuristic methods, while offering stronger
theoretical foundations.

</details>


### [500] [Bias Detection via Maximum Subgroup Discrepancy](https://arxiv.org/pdf/2502.02221)
*Jiří Němeček, Mark Kozdoba, Illia Kryvoviaz, Tomáš Pevný, Jakub Mareček*

Main category: cs.LG

TL;DR: The paper introduces Maximum Subgroup Discrepancy (MSD), a new distance metric for bias evaluation in AI, with linear sample complexity and interpretability, outperforming classical metrics.


<details>
  <summary>Details</summary>
Motivation: Classical bias evaluation metrics like Total Variation and Wasserstein distances have high sample complexity, limiting practical use. A more efficient and interpretable metric is needed.

Method: Proposes MSD, a distance metric focusing on subgroup discrepancies, with a linear sample complexity. Uses Mixed-integer optimization (MIO) for practical evaluation.

Result: MSD shows lower sample complexity, interpretability, and aligns with a general bias detection framework (MSDD). Empirical tests confirm its effectiveness.

Conclusion: MSD is a practical and interpretable metric for bias evaluation, addressing limitations of classical methods and enhancing AI trustworthiness.

Abstract: Bias evaluation is fundamental to trustworthy AI, both in terms of checking
data quality and in terms of checking the outputs of AI systems. In testing
data quality, for example, one may study the distance of a given dataset,
viewed as a distribution, to a given ground-truth reference dataset. However,
classical metrics, such as the Total Variation and the Wasserstein distances,
are known to have high sample complexities and, therefore, may fail to provide
a meaningful distinction in many practical scenarios.
  In this paper, we propose a new notion of distance, the Maximum Subgroup
Discrepancy (MSD). In this metric, two distributions are close if, roughly,
discrepancies are low for all feature subgroups. While the number of subgroups
may be exponential, we show that the sample complexity is linear in the number
of features, thus making it feasible for practical applications. Moreover, we
provide a practical algorithm for evaluating the distance based on
Mixed-integer optimization (MIO). We also note that the proposed distance is
easily interpretable, thus providing clearer paths to fixing the biases once
they have been identified. Finally, we describe a natural general bias
detection framework, termed MSDD distances, and show that MSD aligns well with
this framework. We empirically evaluate MSD by comparing it with other metrics
and by demonstrating the above properties of MSD on real-world datasets.

</details>


### [501] [Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing](https://arxiv.org/pdf/2410.17194)
*Kento Nishi, Rahul Ramesh, Maya Okawa, Mikail Khona, Hidenori Tanaka, Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: Knowledge Editing (KE) algorithms can harm models' factual recall and reasoning. A synthetic task reveals KE distorts entity representations, termed 'representation shattering,' explaining KE's adverse effects.


<details>
  <summary>Details</summary>
Motivation: Recent studies show KE harms models' broader abilities but lack understanding of why. This work aims to uncover the mechanistic reasons behind KE's destructive failures.

Method: A synthetic task trains a Transformer on a structured knowledge graph to study KE's effects, revealing 'representation shattering'—distortion of entity relationships beyond targeted edits.

Result: KE distorts representations of related entities, degrading factual recall and reasoning. Findings are validated with pre-trained Llama and Mamba models.

Conclusion: The study identifies 'representation shattering' as the cause of KE's adverse effects, providing a mechanistic explanation for its impact on model abilities.

Abstract: Knowledge Editing (KE) algorithms alter models' weights to perform targeted
updates to incorrect, outdated, or otherwise unwanted factual associations.
However, recent work has shown that applying KE can adversely affect models'
broader factual recall accuracy and diminish their reasoning abilities.
Although these studies give insights into the potential harms of KE algorithms,
e.g., performance evaluations on benchmarks, little is understood about why
such destructive failures occur. Motivated by this, we define a novel synthetic
task in which a Transformer is trained from scratch to internalize a
"structured" knowledge graph. The structure enforces relationships between
entities of the graph, such that editing a factual association has "trickling
effects" on other entities (e.g., altering X's parent is Y to Z affects who X's
siblings' parent is). Through evaluations of edited models on this task, we
show that KE inadvertently affects representations of entities beyond the
targeted one, distorting relevant structures that allow a model to infer unseen
knowledge about an entity. We call this phenomenon representation shattering
and demonstrate that it degrades models' factual recall and reasoning
performance. We further corroborate our findings in naturalistic settings with
pre-trained Llama and Mamba models as well. Overall, our work yields a precise
mechanistic hypothesis to explain why KE has adverse effects on model
abilities.

</details>


### [502] [WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy Principles](https://arxiv.org/pdf/2411.01357)
*Patrick Mesana, Clément Bénesse, Hadrien Lautraite, Gilles Caporossi, Sébastien Gambs*

Main category: cs.LG

TL;DR: WaKA is a novel attribution method combining LiRA and k-NN, measuring data point contributions to loss distribution without subset sampling. It bridges data attribution and MIA, showing strong correlation with attack success and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To unify data attribution and membership inference attack (MIA) frameworks, providing a versatile tool for privacy risk assessment and data valuation.

Method: Leverages LiRA and k-NN principles, analyzing all possible k-NN constructions from the training set without subset sampling.

Result: Performs close to LiRA as an MIA but more efficiently; robust for data minimization on imbalanced datasets compared to Shapley Values.

Conclusion: WaKA effectively bridges data attribution and MIA, offering computational efficiency and robustness for privacy and data valuation tasks.

Abstract: In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors
Attribution), a novel attribution method that leverages principles from the
LiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers
(k-NN). WaKA efficiently measures the contribution of individual data points to
the model's loss distribution, analyzing every possible k-NN that can be
constructed using the training set, without requiring to sample subsets of the
training set. WaKA is versatile and can be used a posteriori as a membership
inference attack (MIA) to assess privacy risks or a priori for privacy
influence measurement and data valuation. Thus, WaKA can be seen as bridging
the gap between data attribution and membership inference attack (MIA) by
providing a unified framework to distinguish between a data point's value and
its privacy risk. For instance, we have shown that self-attribution values are
more strongly correlated with the attack success rate than the contribution of
a point to the model generalization. WaKA's different usage were also evaluated
across diverse real-world datasets, demonstrating performance very close to
LiRA when used as an MIA on k-NN classifiers, but with greater computational
efficiency. Additionally, WaKA shows greater robustness than Shapley Values for
data minimization tasks (removal or addition) on imbalanced datasets.

</details>


### [503] [Network Dynamics-Based Framework for Understanding Deep Neural Networks](https://arxiv.org/pdf/2501.02436)
*Yuchen Lin, Yong Zhang, Sihan Feng, Hong Zhao*

Main category: cs.LG

TL;DR: A theoretical framework analyzes deep learning dynamics using dynamical systems theory, introducing neuron-level transformations to explain learning phases and phenomena like grokking.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental mechanisms of deep learning and explain key phenomena such as grokking.

Method: Proposes a framework with two neuron-level transformations (order-preserving and non-order-preserving) to analyze learning dynamics and generalization.

Result: Identifies distinct learning phases, transitions, and introduces attraction basins in sample and weight spaces for performance analysis.

Conclusion: The framework provides insights into deep learning advantages and aids in optimizing network architectures and training strategies.

Abstract: Advancements in artificial intelligence call for a deeper understanding of
the fundamental mechanisms underlying deep learning. In this work, we propose a
theoretical framework to analyze learning dynamics through the lens of
dynamical systems theory. We redefine the notions of linearity and nonlinearity
in neural networks by introducing two fundamental transformation units at the
neuron level: order-preserving transformations and non-order-preserving
transformations. Different transformation modes lead to distinct collective
behaviors in weight vector organization, different modes of information
extraction, and the emergence of qualitatively different learning phases.
Transitions between these phases may occur during training, accounting for key
phenomena such as grokking. To further characterize generalization and
structural stability, we introduce the concept of attraction basins in both
sample and weight spaces. The distribution of neurons with different
transformation modes across layers, along with the structural characteristics
of the two types of attraction basins, forms a set of core metrics for
analyzing the performance of learning models. Hyperparameters such as depth,
width, learning rate, and batch size act as control variables for fine-tuning
these metrics. Our framework not only sheds light on the intrinsic advantages
of deep learning, but also provides a novel perspective for optimizing network
architectures and training strategies.

</details>


### [504] [NestQuant: Nested Lattice Quantization for Matrix Products and LLMs](https://arxiv.org/pdf/2502.09720)
*Semyon Savkin, Eitan Porat, Or Ordentlich, Yury Polyanskiy*

Main category: cs.LG

TL;DR: NestQuant is a novel PTQ scheme using self-similar nested lattices, achieving superior efficiency in quantizing LLMs to 4 bits with minimal perplexity gap.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient deployment of large language models (LLMs) through improved post-training quantization (PTQ) techniques.

Method: Proposes NestQuant, a PTQ scheme based on self-similar nested lattices, implemented practically using Gosset lattice for low-complexity matrix multiplication.

Result: Quantizes Llama-3-8B to 4 bits with a perplexity of 6.6, outperforming state-of-the-art methods like SpinQuant, OstQuant, and QuaRot.

Conclusion: NestQuant demonstrates uniform superiority across larger models and benchmarks, making it a practical and efficient PTQ solution.

Abstract: Post-training quantization (PTQ) has emerged as a critical technique for
efficient deployment of large language models (LLMs). This work proposes
NestQuant, a novel PTQ scheme for weights and activations that is based on
self-similar nested lattices. Recent works have mathematically shown such
quantizers to be information-theoretically optimal for low-precision matrix
multiplication. We implement a practical low-complexity version of NestQuant
based on Gosset lattice, making it a drop-in quantizer for any matrix
multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant
quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving
perplexity of 6.6 on wikitext2. This represents more than 55% reduction in
perplexity gap with respect to unquantized model (perplexity of 6.14) compared
to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot
(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation
benchmarks confirm uniform superiority of NestQuant.

</details>


### [505] [Sparser, Better, Faster, Stronger: Sparsity Detection for Efficient Automatic Differentiation](https://arxiv.org/pdf/2501.17737)
*Adrian Hill, Guillaume Dalle*

Main category: cs.LG

TL;DR: The paper introduces an efficient sparsity detection method for Automatic Sparse Differentiation (ASD), enabling faster computation of Jacobian and Hessian matrices in ML, with significant speed-ups in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Jacobian and Hessian matrices are computationally expensive in ML, but their sparsity can be exploited for faster differentiation. Existing methods lack efficient sparsity detection, which this paper addresses.

Method: The paper proposes an operator-overloading-based sparsity detection system that identifies local and global sparsity patterns without modifying user code, making it compatible with existing ML frameworks.

Result: The implementation achieves up to three orders of magnitude speed-up in real-world ML tasks, outperforming standard AD for one-off computations.

Conclusion: The method unlocks efficient computation of Jacobians and Hessians at previously prohibitive scales, advancing the practicality of ASD in ML.

Abstract: From implicit differentiation to probabilistic modeling, Jacobian and Hessian
matrices have many potential use cases in Machine Learning (ML), but they are
viewed as computationally prohibitive. Fortunately, these matrices often
exhibit sparsity, which can be leveraged to speed up the process of Automatic
Differentiation (AD). This paper presents advances in sparsity detection,
previously the performance bottleneck of Automatic Sparse Differentiation
(ASD). Our implementation of sparsity detection is based on operator
overloading, able to detect both local and global sparsity patterns, and
supports flexible index set representations. It is fully automatic and requires
no modification of user code, making it compatible with existing ML codebases.
Most importantly, it is highly performant, unlocking Jacobians and Hessians at
scales where they were considered too expensive to compute. On real-world
problems from scientific ML, graph neural networks and optimization, we show
significant speed-ups of up to three orders of magnitude. Notably, using our
sparsity detection system, ASD outperforms standard AD for one-off
computations, without amortization of either sparsity detection or matrix
coloring.

</details>


### [506] [Generalized Lie Symmetries in Physics-Informed Neural Operators](https://arxiv.org/pdf/2502.00373)
*Amy Xiang Wang, Zakhar Shumaylov, Peter Zaika, Ferdia Sherry, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: PINOs improve PDE solutions by using evolutionary symmetry representatives for better training signals.


<details>
  <summary>Details</summary>
Motivation: Standard point symmetries often lack training signals, limiting PINOs' effectiveness.

Method: Proposed a loss augmentation strategy using evolutionary representatives of point symmetries.

Result: Enhanced neural operator performance with improved data efficiency and accuracy.

Conclusion: Evolutionary symmetry representatives offer a richer training signal for PINOs.

Abstract: Physics-informed neural operators (PINOs) have emerged as powerful tools for
learning solution operators of partial differential equations (PDEs). Recent
research has demonstrated that incorporating Lie point symmetry information can
significantly enhance the training efficiency of PINOs, primarily through
techniques like data, architecture, and loss augmentation. In this work, we
focus on the latter, highlighting that point symmetries oftentimes result in no
training signal, limiting their effectiveness in many problems. To address
this, we propose a novel loss augmentation strategy that leverages evolutionary
representatives of point symmetries, a specific class of generalized symmetries
of the underlying PDE. These generalized symmetries provide a richer set of
generators compared to standard symmetries, leading to a more informative
training signal. We demonstrate that leveraging evolutionary representatives
enhances the performance of neural operators, resulting in improved data
efficiency and accuracy during training.

</details>


### [507] [On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis](https://arxiv.org/pdf/2502.13191)
*Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou*

Main category: cs.LG

TL;DR: SNNs are vulnerable to Membership Inference Attacks (MIAs), and their resilience decreases with increased latency. An input dropout strategy enhances MIA effectiveness, showing SNNs are not inherently more secure than ANNs.


<details>
  <summary>Details</summary>
Motivation: To examine the privacy risks of SNNs, particularly their susceptibility to MIAs, challenging the assumption of their inherent robustness.

Method: Investigates SNNs' vulnerability to MIAs, introduces an input dropout strategy under black box settings, and compares results with ANNs.

Result: SNNs are as vulnerable to MIAs as ANNs, with resilience decreasing as latency increases. The input dropout strategy improves MIA effectiveness.

Conclusion: SNNs do not inherently offer better privacy security than ANNs, and their vulnerabilities should be addressed in future designs.

Abstract: Spiking Neural Networks (SNNs) are increasingly explored for their energy
efficiency and robustness in real-world applications, yet their privacy risks
remain largely unexamined. In this work, we investigate the susceptibility of
SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an
adversary attempts to determine whether a given sample was part of the training
dataset. While prior work suggests that SNNs may offer inherent robustness due
to their discrete, event-driven nature, we find that its resilience diminishes
as latency (T) increases. Furthermore, we introduce an input dropout strategy
under black box setting, that significantly enhances membership inference in
SNNs. Our findings challenge the assumption that SNNs are inherently more
secure, and even though they are expected to be better, our results reveal that
SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial
Neural Networks (ANNs). Our code is available at
https://github.com/sharmaabhijith/MIA_SNN.

</details>


### [508] [PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs](https://arxiv.org/pdf/2502.00963)
*Mauricio Soroco, Jialin Song, Mengzhou Xia, Kye Emond, Weiran Sun, Wuyang Chen*

Main category: cs.LG

TL;DR: PDE-Controller bridges AI and PDEs, enabling LLMs to transform natural language into formal PDE control specifications, outperforming existing models by 62%.


<details>
  <summary>Details</summary>
Motivation: Applied mathematics, especially PDEs, lacks AI exploration despite real-world importance. PDE-Controller aims to fill this gap.

Method: Uses LLMs to convert informal instructions into formal PDE specifications, supported by datasets, reasoning models, and new metrics.

Result: PDE-Controller outperforms open-source and GPT models by up to 62% in utility gain for PDE control.

Conclusion: The framework demonstrates LLMs' potential in scientific challenges, with released data and code for broader use.

Abstract: While recent AI-for-math has made strides in pure mathematics, areas of
applied mathematics, particularly PDEs, remain underexplored despite their
significant real-world applications. We present PDE-Controller, a framework
that enables large language models (LLMs) to control systems governed by
partial differential equations (PDEs). Our approach enables LLMs to transform
informal natural language instructions into formal specifications, and then
execute reasoning and planning steps to improve the utility of PDE control. We
build a holistic solution comprising datasets (both human-written cases and 2
million synthetic samples), math-reasoning models, and novel evaluation
metrics, all of which require significant effort. Our PDE-Controller
significantly outperforms prompting the latest open source and GPT models in
reasoning, autoformalization, and program synthesis, achieving up to a 62%
improvement in utility gain for PDE control. By bridging the gap between
language generation and PDE systems, we demonstrate the potential of LLMs in
addressing complex scientific and engineering challenges. We release all data,
model checkpoints, and code at https://pde-controller.github.io/.

</details>


### [509] [Conformal Prediction as Bayesian Quadrature](https://arxiv.org/pdf/2502.13228)
*Jake C. Snell, Thomas L. Griffiths*

Main category: cs.LG

TL;DR: The paper critiques frequentist-based conformal prediction for uncertainty quantification in machine learning, proposing a Bayesian alternative for richer, interpretable guarantees.


<details>
  <summary>Details</summary>
Motivation: To address limitations of frequentist methods in uncertainty quantification for high-stakes machine learning predictions.

Method: Revisits conformal prediction from a Bayesian perspective and introduces Bayesian quadrature for practical implementation.

Result: The proposed Bayesian approach provides interpretable guarantees and a richer representation of potential losses.

Conclusion: Bayesian methods offer superior uncertainty quantification compared to frequentist conformal prediction in high-stakes scenarios.

Abstract: As machine learning-based prediction systems are increasingly used in
high-stakes situations, it is important to understand how such predictive
models will perform upon deployment. Distribution-free uncertainty
quantification techniques such as conformal prediction provide guarantees about
the loss black-box models will incur even when the details of the models are
hidden. However, such methods are based on frequentist probability, which
unduly limits their applicability. We revisit the central aspects of conformal
prediction from a Bayesian perspective and thereby illuminate the shortcomings
of frequentist guarantees. We propose a practical alternative based on Bayesian
quadrature that provides interpretable guarantees and offers a richer
representation of the likely range of losses to be observed at test time.

</details>


### [510] [Anomaly Detection via Autoencoder Composite Features and NCE](https://arxiv.org/pdf/2502.01920)
*Yalin Liao, Austin J. Brockmeier*

Main category: cs.LG

TL;DR: Proposes a decoupled training approach combining autoencoders (AEs) and noise contrastive estimation (NCE) for unsupervised anomaly detection, improving performance by reducing false negatives.


<details>
  <summary>Details</summary>
Motivation: Autoencoders may generalize and fail to detect anomalies due to small reconstruction errors on abnormal inputs.

Method: Trains an AE and a likelihood model with NCE, using joint latent and reconstruction features for anomaly scoring. Optimizes contrastive noise distribution to reduce false negatives.

Result: Matches state-of-the-art performance on benchmark datasets.

Conclusion: The decoupled approach effectively improves anomaly detection by leveraging AE and NCE jointly.

Abstract: Unsupervised anomaly detection is a challenging task. Autoencoders (AEs) or
generative models are often employed to model the data distribution of normal
inputs and subsequently identify anomalous, out-of-distribution inputs by high
reconstruction error or low likelihood, respectively. However, AEs may
generalize and achieve small reconstruction errors on abnormal inputs. We
propose a decoupled training approach for anomaly detection that both an AE and
a likelihood model trained with noise contrastive estimation (NCE). After
training the AE, NCE estimates a probability density function, to serve as the
anomaly score, on the joint space of the AE's latent representation combined
with features of the reconstruction quality. To further reduce the false
negative rate in NCE we systematically varying the reconstruction features to
augment the training and optimize the contrastive Gaussian noise distribution.
Experimental assessments on multiple benchmark datasets demonstrate that the
proposed approach matches the performance of prevalent state-of-the-art anomaly
detection algorithms.

</details>


### [511] [Discovering Physics Laws of Dynamical Systems via Invariant Function Learning](https://arxiv.org/pdf/2502.04495)
*Shurui Gui, Xiner Li, Shuiwang Ji*

Main category: cs.LG

TL;DR: The paper introduces DIF, a method to learn invariant functions in dynamical systems across diverse environments, ensuring discovery of intrinsic dynamics while ignoring environment-specific mechanisms.


<details>
  <summary>Details</summary>
Motivation: The challenge is to discover intrinsic dynamics in complex environments where changes affect not just coefficients but entire function forms, such as damped or powered pendulum dynamics.

Method: The authors propose DIF, a causal analysis-based method using an encoder-decoder hypernetwork to disentangle invariant functions from environment-specific dynamics, enforced by an information-based principle.

Result: DIF outperforms meta-learning and invariant learning baselines on three ODE systems, demonstrating effectiveness and efficiency. Symbolic regression confirms its ability to uncover intrinsic laws.

Conclusion: DIF successfully discovers invariant functions in complex dynamical systems, validated by quantitative comparisons and symbolic regression, with code available in the AIRS library.

Abstract: We consider learning underlying laws of dynamical systems governed by
ordinary differential equations (ODE). A key challenge is how to discover
intrinsic dynamics across multiple environments while circumventing
environment-specific mechanisms. Unlike prior work, we tackle more complex
environments where changes extend beyond function coefficients to entirely
different function forms. For example, we demonstrate the discovery of ideal
pendulum's natural motion $\alpha^2 \sin{\theta_t}$ by observing pendulum
dynamics in different environments, such as the damped environment $\alpha^2
\sin(\theta_t) - \rho \omega_t$ and powered environment $\alpha^2
\sin(\theta_t) + \rho \frac{\omega_t}{\left|\omega_t\right|}$. Here, we
formulate this problem as an \emph{invariant function learning} task and
propose a new method, known as \textbf{D}isentanglement of \textbf{I}nvariant
\textbf{F}unctions (DIF), that is grounded in causal analysis. We propose a
causal graph and design an encoder-decoder hypernetwork that explicitly
disentangles invariant functions from environment-specific dynamics. The
discovery of invariant functions is guaranteed by our information-based
principle that enforces the independence between extracted invariant functions
and environments. Quantitative comparisons with meta-learning and invariant
learning baselines on three ODE systems demonstrate the effectiveness and
efficiency of our method. Furthermore, symbolic regression explanation results
highlight the ability of our framework to uncover intrinsic laws. Our code has
been released as part of the AIRS library
(\href{https://github.com/divelab/AIRS/tree/main/OpenODE/DIF}{https://github.com/divelab/AIRS/}).

</details>


### [512] [No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces](https://arxiv.org/pdf/2502.04959)
*Daniel Marczak, Simone Magistri, Sebastian Cygert, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer*

Main category: cs.LG

TL;DR: The paper proposes an isotropic merging framework to improve model merging by aligning singular components of task matrices, reducing performance gaps, and incorporating common and task-specific subspaces.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between merged and single-task models by investigating key characteristics of task matrices for effective merging.

Method: Propose an isotropic merging framework that flattens singular value spectra of task matrices, enhances alignment, and incorporates common and task-specific subspaces.

Result: Achieves state-of-the-art performance on vision and language tasks across various task sets and model scales.

Conclusion: Advances understanding of model merging dynamics and provides an effective methodology for merging models without additional training.

Abstract: Model merging integrates the weights of multiple task-specific models into a
single multi-task model. Despite recent interest in the problem, a significant
performance gap between the combined and single-task models remains. In this
paper, we investigate the key characteristics of task matrices -- weight update
matrices applied to a pre-trained model -- that enable effective merging. We
show that alignment between singular components of task-specific and merged
matrices strongly correlates with performance improvement over the pre-trained
model. Based on this, we propose an isotropic merging framework that flattens
the singular value spectrum of task matrices, enhances alignment, and reduces
the performance gap. Additionally, we incorporate both common and task-specific
subspaces to further improve alignment and performance. Our proposed approach
achieves state-of-the-art performance on vision and language tasks across
various sets of tasks and model scales. This work advances the understanding of
model merging dynamics, offering an effective methodology to merge models
without requiring additional training. Code is available at
https://github.com/danielm1405/iso-merging .

</details>


### [513] [Chem42: a Family of chemical Language Models for Target-aware Ligand Generation](https://arxiv.org/pdf/2503.16563)
*Aahan Singh, Engin Tekin, Maryam Nadeem, Nancy A. ElNaker, Mohammad Amaan Sayeed, Natalia Vassilieva, Boulbaba Ben Amor*

Main category: cs.LG

TL;DR: Chem42 is a generative chemical Language Model that integrates target-specific insights to design novel ligands, outperforming existing methods in validity, specificity, and binding affinity.


<details>
  <summary>Details</summary>
Motivation: Current chemical Language Models lack target-specific insights, limiting their ability to generate novel ligands for drug discovery.

Method: Chem42 combines atomic-level interactions with multimodal inputs from Prot42 to create a cross-modal representation for ligand design.

Result: Chem42 excels in chemical validity, target-aware design, and binding affinity, reducing the search space for viable drug candidates.

Conclusion: Chem42 sets a new benchmark in molecule property prediction and ligand design, accelerating drug discovery for precision medicine.

Abstract: Revolutionizing drug discovery demands more than just understanding molecular
interactions - it requires generative models that can design novel ligands
tailored to specific biological targets. While chemical Language Models (cLMs)
have made strides in learning molecular properties, most fail to incorporate
target-specific insights, restricting their ability to drive de-novo ligand
generation. Chem42, a cutting-edge family of generative chemical Language
Models, is designed to bridge this gap. By integrating atomic-level
interactions with multimodal inputs from Prot42, a complementary protein
Language Model, Chem42 achieves a sophisticated cross-modal representation of
molecular structures, interactions, and binding patterns. This innovative
framework enables the creation of structurally valid, synthetically accessible
ligands with enhanced target specificity. Evaluations across diverse protein
targets confirm that Chem42 surpasses existing approaches in chemical validity,
target-aware design, and predicted binding affinity. By reducing the search
space of viable drug candidates, Chem42 could accelerate the drug discovery
pipeline, offering a powerful generative AI tool for precision medicine. Our
Chem42 models set a new benchmark in molecule property prediction, conditional
molecule generation, and target-aware ligand design. The models are publicly
available at huggingface.co/inceptionai.

</details>


### [514] [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/pdf/2502.05003)
*Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto L. Castro, Mahdi Nikdan, Dan Alistarh*

Main category: cs.LG

TL;DR: QuEST advances Quantization-Aware Training (QAT) by achieving optimality at 4-bits and stable convergence at 1-bit, improving quantization accuracy and gradient estimation.


<details>
  <summary>Details</summary>
Motivation: To reduce the costs of large language models (LLMs) by enabling more accurate and efficient training of quantized or sparse models.

Method: QuEST improves QAT via Hadamard normalization, MSE-optimal fitting, and a new trust gradient estimator.

Result: Demonstrates stable scaling laws across hardware-supported precisions and efficient execution on GPUs.

Conclusion: QuEST sets a new state-of-the-art for QAT, enabling highly compressed models without sacrificing accuracy.

Abstract: One approach to reducing the massive costs of large language models (LLMs) is
the use of quantized or sparse representations for training or deployment.
While post-training compression methods are very popular, the question of
obtaining even more accurate compressed models by directly training over such
representations, i.e., Quantization-Aware Training (QAT), is still open: for
example, a recent study (arXiv:2411.04330) put the "optimal" bit-width at which
models can be trained using QAT, while staying accuracy-competitive with
standard FP16/BF16 precision, at 8-bits weights and activations. We advance
this state-of-the-art via a new method called QuEST, for which we demonstrate
optimality at 4-bits and stable convergence as low as 1-bit weights and
activations. QuEST achieves this by improving two key aspects of QAT methods:
(1) accurate and fast quantization of the (continuous) distributions of weights
and activations via Hadamard normalization and MSE-optimal fitting; (2) a new
trust gradient estimator based on the idea of explicitly minimizing the error
between the noisy gradient computed over quantized states and the "true" (but
unknown) full-precision gradient. Experiments on Llama-type architectures show
that QuEST induces stable scaling laws across the entire range of
hardware-supported precisions, and can be extended to sparse representations.
We provide GPU kernel support showing that models produced by QuEST can be
executed efficiently. Our code is available at
https://github.com/IST-DASLab/QuEST.

</details>


### [515] [LEMUR Neural Network Dataset: Towards Seamless AutoML](https://arxiv.org/pdf/2504.10552)
*Arash Torabi Goodarzi, Roman Kochnev, Waleed Khalid, Furui Qin, Tolgay Atinc Uzun, Yashkumar Sanjaybhai Dhameliya, Yash Kanubhai Kathiriya, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte*

Main category: cs.LG

TL;DR: LEMUR is an open-source dataset of neural network models with structured code for diverse tasks, designed to support AutoML, benchmarking, and model analysis.


<details>
  <summary>Details</summary>
Motivation: High-quality datasets are crucial for neural network development, and there's growing interest in datasets of neural networks themselves for benchmarking and AutoML.

Method: LEMUR provides structured model representations and performance data, leveraging Python and PyTorch. It includes tools for evaluation, hyperparameter optimization, and VR deployment.

Result: LEMUR enables seamless extension to new datasets and models, supports model evaluation, and offers an API for comprehensive performance statistics.

Conclusion: LEMUR is a valuable resource for researchers and practitioners, facilitating neural network development and analysis, and is available as open-source under the MIT license.

Abstract: Neural networks are fundamental in artificial intelligence, driving progress
in computer vision and natural language processing. High-quality datasets are
crucial for their development, and there is growing interest in datasets
composed of neural networks themselves to support benchmarking, automated
machine learning (AutoML), and model analysis. We introduce LEMUR, an open
source dataset of neural network models with well-structured code for diverse
architectures across tasks such as object detection, image classification,
segmentation, and natural language processing. LEMUR is primarily designed to
provide a rich source of structured model representations and associated
performance data, enabling the fine-tuning of large language models for AutoML
applications. Leveraging Python and PyTorch, LEMUR enables seamless extension
to new datasets and models while maintaining consistency. It integrates an
Optuna-powered framework for evaluation, hyperparameter optimization,
statistical analysis, and graphical insights. LEMUR VR extension enables the
seamless deployment of models in virtual reality, optimizing their performance
on resource-constrained devices. Providing tools for model evaluation,
preprocessing, and database management, LEMUR supports researchers and
practitioners in developing, testing, and analyzing neural networks. It offers
an API that delivers comprehensive information about neural network models and
their complete performance statistics with a single request, which can be used
in experiments with code-generating large language models. The LEMUR and its
plugins are accessible as open source projects under the MIT license at
https://github.com/ABrain-One/nn-dataset,
https://github.com/ABrain-One/nn-plots and https://github.com/ABrain-One/nn-vr.

</details>


### [516] [Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension](https://arxiv.org/pdf/2502.05075)
*Yijun Dong, Yicheng Li, Yunai Li, Jason D. Lee, Qi Lei*

Main category: cs.LG

TL;DR: Weak-to-strong (W2S) finetuning outperforms weak teachers due to variance reduction in low-dimensional subspaces, analyzed via ridgeless regression.


<details>
  <summary>Details</summary>
Motivation: To understand why W2S finetuning, where a strong model learns from weak pseudo-labels, often surpasses the weak teacher's performance.

Method: Analyzes W2S in ridgeless regression, focusing on low-dimensional feature subspaces and variance reduction.

Result: Shows variance reduction in discrepancy subspaces, with experiments validating the theory on synthetic and real tasks.

Conclusion: W2S benefits from model discrepancy, reducing variance and improving generalization, supported by empirical evidence.

Abstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a
strong (large) student model is trained on pseudo-labels generated by a weak
teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to
understand this phenomenon through the observation that FT often occurs in
intrinsically low-dimensional spaces. Leveraging the low intrinsic
dimensionality of FT, we analyze W2S in the ridgeless regression setting from a
variance reduction perspective. For a strong student-weak teacher pair with
sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s,
\mathcal{V}_w$, we provide an exact characterization of the variance that
dominates the generalization error of W2S. This unveils a virtue of discrepancy
between the strong and weak models in W2S: the variance of the weak teacher is
inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while
reduced by a factor of $\mathrm{dim}(\mathcal{V}_s)/N$ in the subspace of
discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for
W2S. Our analysis further casts light on the sample complexities and the
scaling of performance gap recovery in W2S. The analysis is supported by
experiments on synthetic regression problems, as well as real vision and NLP
tasks.

</details>


### [517] [Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism](https://arxiv.org/pdf/2504.18574)
*Aviv Bick, Eric Xing, Albert Gu*

Main category: cs.LG

TL;DR: The paper examines retrieval mechanisms in Transformer- and SSM-based models, identifying a Gather-and-Aggregate bottleneck in both. Hybrid models combining SSMs with attention layers improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand why SSMs underperform on algorithmic tasks like retrieval compared to Transformers and to bridge this gap.

Method: Analyzes retrieval mechanisms in both architectures, prunes models to isolate critical heads, and tests hybrid models.

Result: Identifies a few critical Gather-and-Aggregate heads as bottlenecks. Hybrid models with attention layers improve retrieval and benchmark scores.

Conclusion: The Transformer-SSM performance gap stems from a few heads, and hybrid models can merge their strengths.

Abstract: State-space models (SSMs) offer efficient alternatives to Transformers for
long sequences, but their fixed-size recurrent state limits capability on
algorithmic tasks, such as retrieving past context. In this work, we examine
how in-context retrieval operates in Transformer- and SSM-based language models
and find that both rely on a similar Gather-and-Aggregate (G&A) mechanism: a
Gather Head extracts relevant information pieces from context, which an
Aggregate Head integrates into a single representation. In both architectures,
G&A concentrates in a few heads, forming critical bottlenecks even for simple
retrieval. For example, we show that disabling a single Gather or Aggregate
Head in a pruned Llama-3.1-8B impairs retrieving the correct answer letter in
MMLU, reducing its accuracy from 66% to 25% (random guessing). Moreover, this
retrieval bottleneck can obscure limited knowledge demands of tasks as the
pruned model succeeds on MMLU with functioning G&A heads yet fails on other
knowledge benchmarks. The bottleneck similarly extends to tasks where SSMs
typically underperform, such as GSM8K, BBH, and dialogue comprehension. We show
that SSMs' retrieval challenges manifest in these heads, creating smoother
attention patterns instead of the sharp token transitions effective G&A
requires. Thus, the Transformer-SSM retrieval gap exists in just a few heads,
rather than the entire language model. This suggests a unified explanation for
Transformer vs. SSM performance gap while showing how to merge their strengths.
We find that pretrained hybrid models, where SSMs are combined with a few
attention layers, delegate the role of Aggregate Heads to attention. Similarly,
replacing a single G&A head in a pretrained SSM with an attention variant
boosts retrieval and benchmark scores.

</details>


### [518] [Towards Foundational Models for Dynamical System Reconstruction: Hierarchical Meta-Learning via Mixture of Experts](https://arxiv.org/pdf/2502.05335)
*Roussel Desmond Nzoyem, Grant Stevens, Amarpal Sahota, David A. W. Barton, Tom Deakin*

Main category: cs.LG

TL;DR: MixER, a novel MoE-based method, addresses hierarchical DSR challenges with a custom gating algorithm, outperforming naive MoEs but lagging in high-data regimes.


<details>
  <summary>Details</summary>
Motivation: Current meta-learning methods struggle with hierarchical DSR due to sparse, loosely related datasets, necessitating a more nuanced approach.

Method: MixER uses a sparse top-1 MoE layer with a gating update algorithm based on K-means and least squares.

Result: MixER scales efficiently to systems of up to ten parametric ODEs but underperforms in high-data regimes with highly related data.

Conclusion: MixER's performance depends on hierarchical data structure, highlighting its niche applicability.

Abstract: As foundational models reshape scientific discovery, a bottleneck persists in
dynamical system reconstruction (DSR): the ability to learn across system
hierarchies. Many meta-learning approaches have been applied successfully to
single systems, but falter when confronted with sparse, loosely related
datasets requiring multiple hierarchies to be learned. Mixture of Experts (MoE)
offers a natural paradigm to address these challenges. Despite their potential,
we demonstrate that naive MoEs are inadequate for the nuanced demands of
hierarchical DSR, largely due to their gradient descent-based gating update
mechanism which leads to slow updates and conflicted routing during training.
To overcome this limitation, we introduce MixER: Mixture of Expert
Reconstructors, a novel sparse top-1 MoE layer employing a custom gating update
algorithm based on $K$-means and least squares. Extensive experiments validate
MixER's capabilities, demonstrating efficient training and scalability to
systems of up to ten parametric ordinary differential equations. However, our
layer underperforms state-of-the-art meta-learners in high-data regimes,
particularly when each expert is constrained to process only a fraction of a
dataset composed of highly related data points. Further analysis with synthetic
and neuroscientific time series suggests that the quality of the contextual
representations generated by MixER is closely linked to the presence of
hierarchical structure in the data.

</details>


### [519] [Griffin: Towards a Graph-Centric Relational Database Foundation Model](https://arxiv.org/pdf/2505.05568)
*Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang*

Main category: cs.LG

TL;DR: Griffin is a foundation model for Relational Databases (RDBs) unifying data encoding and task decoding, outperforming task-specific models with enhanced architecture and pretraining.


<details>
  <summary>Details</summary>
Motivation: To create a unified foundation model for RDBs, addressing limitations of smaller, task-specific models by handling diverse tasks efficiently.

Method: Incorporates cross-attention modules, novel aggregators, and advanced encoders for categorical, numerical, and metadata features, pretrained on single-table and RDB datasets.

Result: Superior or comparable performance to task-specific models, excels in low-data scenarios, and shows strong transferability across new datasets.

Conclusion: Griffin is a promising universally applicable foundation model for RDBs, with potential for broad adoption.

Abstract: We introduce Griffin, the first foundation model attemptation designed
specifically for Relational Databases (RDBs). Unlike previous smaller models
focused on single RDB tasks, Griffin unifies the data encoder and task decoder
to handle diverse tasks. Additionally, we enhance the architecture by
incorporating a cross-attention module and a novel aggregator. Griffin utilizes
pretraining on both single-table and RDB datasets, employing advanced encoders
for categorical, numerical, and metadata features, along with innovative
components such as cross-attention modules and enhanced message-passing neural
networks (MPNNs) to capture the complexities of relational data. Evaluated on
large-scale, heterogeneous, and temporal graphs extracted from RDBs across
various domains (spanning over 150 million nodes), Griffin demonstrates
superior or comparable performance to individually trained models, excels in
low-data scenarios, and shows strong transferability with similarity and
diversity in pretraining across new datasets and tasks, highlighting its
potential as a universally applicable foundation model for RDBs. Code available
at https://github.com/yanxwb/Griffin.

</details>


### [520] [Curvature Tuning: Provable Training-free Model Steering From a Single Parameter](https://arxiv.org/pdf/2502.07783)
*Leyang Hu, Matteo Gamba, Randall Balestriero*

Main category: cs.LG

TL;DR: The paper introduces Curvature Tuning (CT), a method to adjust model decision boundaries by modifying activation functions, improving generalization and robustness over traditional finetuning methods.


<details>
  <summary>Details</summary>
Motivation: Current finetuning methods lack interpretability and rely on heuristic hyperparameters, prompting a shift from weight adaptation to activation function modification.

Method: CT modulates decision boundaries by injecting a hyperparameter into activation functions, projecting models onto smooth function spaces.

Result: CT boosts accuracy (e.g., 7.14%/8.46% for ResNet-50/152) and robust accuracy (e.g., 1032.64%/1494.46% on RobustBench) over baselines.

Conclusion: CT offers an interpretable, parameter-efficient finetuning alternative, enhancing both generalization and robustness.

Abstract: The scaling of model and data sizes has reshaped the AI landscape,
establishing finetuning pretrained models as the standard paradigm for solving
downstream tasks. However, dominant finetuning methods typically rely on weight
adaptation, often lack interpretability, and depend on heuristically chosen
hyperparameters. In this paper, we take a different perspective and shift the
focus from weights to activation functions, viewing them through the lens of
spline operators. We propose Curvature Tuning (CT), an interpretable and
principled steering method that modulates a model's decision boundary by
injecting a single hyperparameter into its activation functions. We show that
CT provably adjusts model decision boundary curvature and, more fundamentally,
projects a model onto a space of smooth functions-thereby complementing current
finetuning methods, whose effect lies primarily in feature adaptation. Making
this hyperparameter trainable gives rise to a novel and highly
parameter-efficient finetuning method. Empirically, CT improves both
generalization and robustness. For example, it boosts downstream accuracy of
ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA
across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark
from RobustBench by 1032.64%/1494.46%. Our code is available at
https://github.com/Leon-Leyang/curvature-tuning.

</details>


### [521] [LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/pdf/2505.08265)
*Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu*

Main category: cs.LG

TL;DR: The paper explores using LLMs to enhance node representations for GNNs, proposes an in-depth analysis via interchange interventions, and introduces an optimization module to improve LLM-GNN information transfer.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs as feature enhancers for GNNs is underexplored, prompting a deeper analysis of their properties and mechanisms.

Method: Constructs a synthetic graph dataset with controllable causal relationships, uses interchange interventions to analyze LLM enhancers and GNNs, and designs an optimization module.

Result: Experiments validate the proposed optimization module across multiple datasets and models.

Conclusion: The study provides insights into LLM-GNN interactions and offers a practical module to enhance their performance.

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [522] [On the Importance of Embedding Norms in Self-Supervised Learning](https://arxiv.org/pdf/2502.09252)
*Andrew Draganov, Sharvaree Vadgama, Sebastian Damrich, Jan Niklas Böhm, Lucas Maes, Dmitry Kobak, Erik Bekkers*

Main category: cs.LG

TL;DR: The paper resolves the contradiction around embedding norms in SSL, showing they govern convergence rates and encode network confidence, with smaller norms indicating unexpected samples.


<details>
  <summary>Details</summary>
Motivation: To clarify the role of embedding norms in SSL, as their significance contradicts the hypersphere-based assumptions of most SSL methods.

Method: Theoretical analysis, simulations, and experiments to study embedding norms' impact on SSL.

Result: Embedding norms influence SSL convergence rates and encode confidence (smaller norms for unexpected samples). Manipulating norms affects convergence speed.

Conclusion: Embedding norms are crucial for understanding and optimizing SSL network behavior.

Abstract: Self-supervised learning (SSL) allows training data representations without a
supervised signal and has become an important paradigm in machine learning.
Most SSL methods employ the cosine similarity between embedding vectors and
hence effectively embed data on a hypersphere. While this seemingly implies
that embedding norms cannot play any role in SSL, a few recent works have
suggested that embedding norms have properties related to network convergence
and confidence. In this paper, we resolve this apparent contradiction and
systematically establish the embedding norm's role in SSL training. Using
theoretical analysis, simulations, and experiments, we show that embedding
norms (i) govern SSL convergence rates and (ii) encode network confidence, with
smaller norms corresponding to unexpected samples. Additionally, we show that
manipulating embedding norms can have large effects on convergence speed. Our
findings demonstrate that SSL embedding norms are integral to understanding and
optimizing network behavior.

</details>


### [523] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/pdf/2505.10482)
*Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu*

Main category: cs.LG

TL;DR: NCDPO improves diffusion policies by reformulating them as noise-conditioned deterministic policies, enabling efficient RL fine-tuning and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies suffer from sub-optimal trajectories due to limited demonstration data, and existing RL fine-tuning struggles with computational intractability.

Method: NCDPO treats denoising steps as differentiable transformations, enabling tractable likelihood evaluation and gradient backpropagation.

Result: NCDPO matches MLP+PPO's sample efficiency, outperforms benchmarks in robot control and multi-agent games, and is robust to denoising timesteps.

Conclusion: NCDPO effectively addresses diffusion policy limitations, offering superior performance and efficiency in diverse decision-making tasks.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [524] [Scalable First-order Method for Certifying Optimal k-Sparse GLMs](https://arxiv.org/pdf/2502.09502)
*Jiachang Liu, Soroosh Shafiee, Andrea Lodi*

Main category: cs.LG

TL;DR: A first-order proximal gradient algorithm is proposed to efficiently compute dual bounds for certifying optimality in sparse GLMs within a BnB framework, outperforming existing methods in scalability and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for certifying optimality in sparse GLMs are computationally intensive or slow, limiting scalability to large problems.

Method: A first-order proximal gradient algorithm solves the perspective relaxation of the problem, with an exact log-linear time proximal operator and a restart strategy for faster convergence.

Result: The method accelerates dual bound computations and effectively provides optimality certificates for large-scale problems, as shown in experiments.

Conclusion: The proposed algorithm enhances scalability and efficiency in certifying optimality for sparse GLMs, addressing limitations of prior methods.

Abstract: This paper investigates the problem of certifying optimality for sparse
generalized linear models (GLMs), where sparsity is enforced through an
$\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can
certify optimality by pruning nodes using dual bounds, existing methods for
computing these bounds are either computationally intensive or exhibit slow
convergence, limiting their scalability to large-scale problems. To address
this challenge, we propose a first-order proximal gradient algorithm designed
to solve the perspective relaxation of the problem within a BnB framework.
Specifically, we formulate the relaxed problem as a composite optimization
problem and demonstrate that the proximal operator of the non-smooth component
can be computed exactly in log-linear time complexity, eliminating the need to
solve a computationally expensive second-order cone program. Furthermore, we
introduce a simple restart strategy that enhances convergence speed while
maintaining low per-iteration complexity. Extensive experiments on synthetic
and real-world datasets show that our approach significantly accelerates dual
bound computations and is highly effective in providing optimality certificates
for large-scale problems.

</details>


### [525] [Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models](https://arxiv.org/pdf/2505.18230)
*Louis Béthune, David Vigouroux, Yilun Du, Rufin VanRullen, Thomas Serre, Victor Boutin*

Main category: cs.LG

TL;DR: The paper proposes a method to derive Riemannian metrics from pretrained Energy-Based Models (EBMs) for computing geodesics in high-dimensional curved manifolds, showing improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: The challenge of estimating Riemannian metrics in high-dimensional curved manifolds motivates the need for scalable, data-aware methods to compute geodesics.

Method: The authors introduce two novel metrics derived from EBMs, which define spatially varying distances to compute geodesics that align with the data manifold's intrinsic geometry.

Result: EBM-derived metrics outperform baselines, producing geodesics closer to the data manifold with lower curvature distortion, validated on synthetic, character, and natural image datasets.

Conclusion: This work pioneers deriving Riemannian metrics from EBMs, enabling scalable, geometry-driven learning for generative modeling and simulation.

Abstract: What is the shortest path between two data points lying in a high-dimensional
space? While the answer is trivial in Euclidean geometry, it becomes
significantly more complex when the data lies on a curved manifold -- requiring
a Riemannian metric to describe the space's local curvature. Estimating such a
metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly
from pretrained Energy-Based Models (EBMs) -- a class of generative models that
assign low energy to high-density regions. These metrics define spatially
varying distances, enabling the computation of geodesics -- shortest paths that
follow the data manifold's intrinsic geometry. We introduce two novel metrics
derived from EBMs and show that they produce geodesics that remain closer to
the data manifold and exhibit lower curvature distortion, as measured by
alignment with ground-truth trajectories. We evaluate our approach on
increasingly complex datasets: synthetic datasets with known data density,
rotated character images with interpretable geometry, and high-resolution
natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established
baselines, especially in high-dimensional settings. Our work is the first to
derive Riemannian metrics from EBMs, enabling data-aware geodesics and
unlocking scalable, geometry-driven learning for generative modeling and
simulation.

</details>


### [526] [Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models](https://arxiv.org/pdf/2502.11420)
*Yingqing Guo, Yukang Yang, Hui Yuan, Mengdi Wang*

Main category: cs.LG

TL;DR: TreeG, a training-free guidance method using tree search, outperforms baselines in non-differentiable and discrete settings for diffusion and flow models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of non-differentiable objectives and discrete data distributions in training-free guidance for controlled generation.

Method: Proposes TreeG, a unified framework with tree search-based path steering, candidate proposal, and evaluation. Three novel algorithms are derived.

Result: TreeG improves performance by 29.01%, 16.6%, and 18.43% in symbolic music, small molecule, and DNA design tasks. Shows scalability via an inference-time scaling law.

Conclusion: TreeG is a scalable, effective solution for training-free guidance in both continuous and discrete settings.

Abstract: Training-free guidance enables controlled generation in diffusion and flow
models, but most methods rely on gradients and assume differentiable
objectives. This work focuses on training-free guidance addressing challenges
from non-differentiable objectives and discrete data distributions. We propose
TreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous
and discrete settings in diffusion and flow models. TreeG offers a unified
framework for training-free guidance by proposing, evaluating, and selecting
candidates at each step, enhanced with tree search over active paths and
parallel exploration. We comprehensively investigate the design space of TreeG
over the candidate proposal module and the evaluation function, instantiating
TreeG into three novel algorithms. Our experiments show that TreeG consistently
outperforms top guidance baselines in symbolic music generation, small molecule
design, and enhancer DNA design with improvements of 29.01%, 16.6%, and 18.43%.
Additionally, we identify an inference-time scaling law showing TreeG's
scalability in inference-time computation.

</details>


### [527] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/pdf/2505.22370)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces SplitLoRA, a novel continual learning method using Low-Rank Adaptation to balance stability and plasticity by optimally partitioning gradient space.


<details>
  <summary>Details</summary>
Motivation: Existing gradient projection methods in continual learning struggle to balance stability (retaining old knowledge) and plasticity (learning new tasks).

Method: Proposes SplitLoRA, which theoretically analyzes gradient subspace partitioning and derives an optimal partition for stability and plasticity.

Result: Experiments show SplitLoRA achieves state-of-the-art performance on multiple datasets.

Conclusion: SplitLoRA effectively balances stability and plasticity in continual learning, outperforming existing methods.

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [528] [A Closer Look at TabPFN v2: Understanding Its Strengths and Extending Its Capabilities](https://arxiv.org/pdf/2502.17361)
*Han-Jia Ye, Si-Yang Liu, Wei-Lun Chao*

Main category: cs.LG

TL;DR: TabPFN v2, a transformer-based model, excels in handling heterogeneous tabular data and achieves high predictive accuracy without needing explicit attribute embeddings. Its limitations in high-dimensional tasks are addressed via a divide-and-conquer strategy.


<details>
  <summary>Details</summary>
Motivation: To understand how TabPFN v2 handles tabular data heterogeneity and achieves high accuracy, while exploring ways to mitigate its limitations in complex tasks.

Method: Examines TabPFN v2's ability to infer attribute relationships without explicit embeddings, tests its feature extraction capability, and introduces a divide-and-conquer strategy for scalability.

Result: TabPFN v2 infers attribute relationships even with randomized inputs and constructs a separable feature space. Its limitations are mitigated by scalable inference strategies.

Conclusion: The study provides insights for designing future tabular foundation models by revealing TabPFN v2's mechanisms and extending its applicability.

Abstract: Tabular datasets are inherently heterogeneous, presenting significant
challenges for developing pre-trained foundation models. The recently
introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2)
achieves unprecedented in-context learning performance across diverse
downstream datasets, marking a pivotal advancement in tabular foundation
models. In this paper, we take a closer look at TabPFN v2 to examine how it
effectively handles heterogeneity and achieves high predictive accuracy, and to
explore how its limitations in high-dimensional, many-category, and large-scale
tasks can be mitigated. We find that TabPFN v2 can infer attribute
relationships even when provided with randomized attribute token inputs,
eliminating the need to explicitly learn dataset-specific attribute embeddings
to address heterogeneity. We further show that TabPFN v2 can be transformed
into a feature extractor, revealing its ability to construct a highly separable
feature space for accurate predictions. Lastly, we demonstrate that TabPFN v2's
limitations can be addressed through a test-time divide-and-conquer strategy,
enabling scalable inference without requiring re-training. By uncovering the
mechanisms behind TabPFN v2's success and introducing strategies to extend its
applicability, this study offers key insights into the design of future tabular
foundation models.

</details>


### [529] [Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](https://arxiv.org/pdf/2502.20383)
*Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen*

Main category: cs.LG

TL;DR: Web AI agents are more vulnerable than standalone LLMs due to factors like embedding user goals, multi-step actions, and observational capabilities, requiring a refined evaluation framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why Web AI agents, despite being safety-aligned, are more vulnerable than standalone LLMs, given their flexibility and exposure to adversarial inputs.

Method: The research employs a component-level analysis and a systematic evaluation framework to identify key vulnerability factors.

Result: Three critical factors increasing vulnerability are identified: embedding user goals, multi-step action generation, and observational capabilities.

Conclusion: The findings emphasize the need for improved security in AI agent design and offer insights for targeted defense strategies.

Abstract: Recent advancements in Web AI agents have demonstrated remarkable
capabilities in addressing complex web navigation tasks. However, emerging
research shows that these agents exhibit greater vulnerability compared to
standalone Large Language Models (LLMs), despite both being built upon the same
safety-aligned models. This discrepancy is particularly concerning given the
greater flexibility of Web AI Agent compared to standalone LLMs, which may
expose them to a wider range of adversarial user inputs. To build a scaffold
that addresses these concerns, this study investigates the underlying factors
that contribute to the increased vulnerability of Web AI agents. Notably, this
disparity stems from the multifaceted differences between Web AI agents and
standalone LLMs, as well as the complex signals - nuances that simple
evaluation metrics, such as success rate, often fail to capture. To tackle
these challenges, we propose a component-level analysis and a more granular,
systematic evaluation framework. Through this fine-grained investigation, we
identify three critical factors that amplify the vulnerability of Web AI
agents; (1) embedding user goals into the system prompt, (2) multi-step action
generation, and (3) observational capabilities. Our findings highlights the
pressing need to enhance security and robustness in AI agent design and provide
actionable insights for targeted defense strategies.

</details>


### [530] [Bayesian Neural Scaling Law Extrapolation with Prior-Fitted Networks](https://arxiv.org/pdf/2505.23032)
*Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee*

Main category: cs.LG

TL;DR: A Bayesian framework using Prior-data Fitted Networks (PFNs) is proposed for neural scaling law extrapolation, outperforming point estimation methods with uncertainty-aware predictions.


<details>
  <summary>Details</summary>
Motivation: Existing scaling law methods lack uncertainty quantification, crucial for decision-making in resource allocation.

Method: Uses PFNs with a designed prior distribution to sample synthetic scaling functions, enabling meta-learning for extrapolation.

Result: Outperforms point estimation and Bayesian methods, especially in data-limited scenarios like active learning.

Conclusion: The approach provides reliable, uncertainty-aware scaling law extrapolation for practical applications.

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [531] [Mechanistic PDE Networks for Discovery of Governing Equations](https://arxiv.org/pdf/2502.18377)
*Adeel Pervez, Efstratios Gavves, Francesco Locatello*

Main category: cs.LG

TL;DR: Mechanistic PDE Networks discover governing PDEs from data using neural networks and a specialized solver.


<details>
  <summary>Details</summary>
Motivation: To model spatiotemporal dynamics efficiently and discover governing PDEs from noisy or complex data.

Method: Uses neural networks to represent PDEs in hidden space and a GPU-capable, parallel, sparse multigrid solver for efficient computation.

Result: Validated on reaction-diffusion and Navier-Stokes equations, showing robustness to noise and effectiveness in PDE discovery.

Conclusion: The framework successfully discovers nonlinear PDEs in complex settings with computational efficiency.

Abstract: We present Mechanistic PDE Networks -- a model for discovery of governing
partial differential equations from data. Mechanistic PDE Networks represent
spatiotemporal data as space-time dependent linear partial differential
equations in neural network hidden representations. The represented PDEs are
then solved and decoded for specific tasks. The learned PDE representations
naturally express the spatiotemporal dynamics in data in neural network hidden
space, enabling increased power for dynamical modeling. Solving the PDE
representations in a compute and memory-efficient way, however, is a
significant challenge. We develop a native, GPU-capable, parallel, sparse, and
differentiable multigrid solver specialized for linear partial differential
equations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE
solver, we propose a discovery architecture that can discover nonlinear PDEs in
complex settings while also being robust to noise. We validate PDE discovery on
a number of PDEs, including reaction-diffusion and Navier-Stokes equations.

</details>


### [532] [FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models](https://arxiv.org/pdf/2503.06928)
*Yanlong Wang, Jian Xu, Tiantian Gao, Hongkang Zhang, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: The paper introduces FinTSBridge, a framework to bridge advanced time series forecasting models with financial asset pricing, using new datasets, metrics, and tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap between cutting-edge time series forecasting models and their application in financial asset pricing.

Method: Constructed financial datasets, validated models, introduced new metrics (msIC, msIR), and designed financial-specific tasks.

Result: Developed FinTSBridge to evaluate forecasting models' effectiveness and robustness in financial domains.

Conclusion: FinTSBridge offers insights into applying advanced forecasting models to financial problems.

Abstract: Despite the growing attention to time series forecasting in recent years,
many studies have proposed various solutions to address the challenges
encountered in time series prediction, aiming to improve forecasting
performance. However, effectively applying these time series forecasting models
to the field of financial asset pricing remains a challenging issue. There is
still a need for a bridge to connect cutting-edge time series forecasting
models with financial asset pricing. To bridge this gap, we have undertaken the
following efforts: 1) We constructed three datasets from the financial domain;
2) We selected over ten time series forecasting models from recent studies and
validated their performance in financial time series; 3) We developed new
metrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series
correlation captured by the models; 4) We designed financial-specific tasks for
these three datasets and assessed the practical performance and application
potential of these forecasting models in important financial problems. We hope
the developed new evaluation suite, FinTSBridge, can provide valuable insights
into the effectiveness and robustness of advanced forecasting models in
finanical domains.

</details>


### [533] [Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors](https://arxiv.org/pdf/2503.08099)
*Runxi Cheng, Feng Xiong, Yongxian Wei, Wanyun Zhu, Chun Yuan*

Main category: cs.LG

TL;DR: WUDI-Merging is a data-free model merging method that minimizes interference between task-specific models by leveraging task vectors, achieving state-of-the-art performance with minimal resources.


<details>
  <summary>Details</summary>
Motivation: Addressing performance degradation due to parameter interference in model merging without requiring additional data or retraining.

Method: Theoretical demonstration of task vectors forming a linear subspace, guiding the WUDI-Merging method to eliminate interference.

Result: Achieves 10.9% improvement over baselines and outperforms test-time adaptation by 3.3%, with minimal computing resources.

Conclusion: WUDI-Merging is a simple, effective solution for data-free model merging, offering superior performance and efficiency.

Abstract: Model merging seeks to integrate task-specific expert models into a unified
architecture while preserving multi-task generalization capabilities, yet
parameter interference between constituent models frequently induces
performance degradation. Although prior work has explored many merging
strategies, resolving interference without additional data for retraining or
test-time computation remains challenging. In this paper, we theoretically
demonstrate that the task vectors of the linear layer constitute an approximate
linear subspace for its corresponding input. Therefore, we can minimize
interference under the guidance of task vectors. Based on this insight, we
propose \textbf{WUDI-Merging} (\textbf{W}hoever started the interference
sho\textbf{U}ld en\textbf{D} \textbf{I}t), a simple yet effective model merging
method that eliminates interference without any additional data or rescaling
coefficients. Comprehensive empirical evaluations across vision and language
benchmarks demonstrate our method's superiority, achieving state-of-the-art
performance in data-free model merging scenarios (average 10.9\% improvement
versus baseline methods) while even outperforming mainstream test-time
adaptation approaches by 3.3\%, and only very few computing resources are
required. The code will be publicly available soon.

</details>


### [534] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/pdf/2506.07298)
*Yijia Dai, Zhaolin Gao, Yahya Sattar, Sarah Dean, Jennifer J. Sun*

Main category: cs.LG

TL;DR: LLMs use in-context learning to model HMM-generated data, achieving near-optimal accuracy and offering practical guidelines for real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of fitting HMMs to real-world data and explore LLMs' potential for modeling sequential data.

Method: Utilize pre-trained LLMs with in-context learning to infer patterns from synthetic and real-world HMM-generated sequences.

Result: LLMs achieve predictive accuracy close to the theoretical optimum and perform competitively on real-world tasks.

Conclusion: ICL in LLMs is a powerful tool for modeling HMMs, advancing understanding of in-context learning and its applications in complex data.

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [535] [Dataset Properties Shape the Success of Neuroimaging-Based Patient Stratification: A Benchmarking Analysis Across Clustering Algorithms](https://arxiv.org/pdf/2503.12066)
*Yuetong Yu, Ruiyang Ge, Ilker Hacihaliloglu, Alexander Rauscher, Roger Tam, Sophia Frangou*

Main category: cs.LG

TL;DR: The study shows that data characteristics (like cluster separation and noise) impact patient stratification accuracy more than algorithm choice, emphasizing the need for realistic data in algorithm development.


<details>
  <summary>Details</summary>
Motivation: To understand how dataset properties (e.g., cluster overlap, noise) affect the accuracy and reproducibility of neuroimaging-based patient stratification methods.

Method: Evaluated four algorithms (HYDRA, SuStaIn, SmileGAN, SurrealGAN) on synthetic brain-morphometry datasets with controlled variations in cluster properties.

Result: Data complexity (e.g., cluster overlap, size imbalance) had a larger impact on accuracy than algorithm choice. Some algorithms struggled with scalability or discrete labeling.

Conclusion: Input data properties critically influence stratification success, highlighting the need for data-centric strategies in algorithm development.

Abstract: Background: Data driven stratification of patients into biologically informed
subtypes holds promise for precision neuropsychiatry, yet neuroimaging-based
clustering methods often fail to generalize across cohorts. While algorithmic
innovations have focused on model complexity, the role of underlying dataset
characteristics remains underexplored. We hypothesized that cluster separation,
size imbalance, noise, and the direction and magnitude of disease-related
effects in the input data critically determine both within-algorithm accuracy
and reproducibility. Methods: We evaluated 4 widely used stratification
algorithms, HYDRA, SuStaIn, SmileGAN, and SurrealGAN, on a suite of synthetic
brain-morphometry cohorts derived from the Human Connectome Project Young Adult
dataset. Three global transformation patterns were applied to 600
pseudo-patients against 508 controls, followed by 4 within-dataset variations
varying cluster count (k=2-6), overlap, and effect magnitude. Algorithm
performance was quantified by accuracy in recovering the known ground-truth
clusters. Results: Across 122 synthetic scenarios, data complexity consistently
outweighed algorithm choice in predicting stratification success.
Well-separated clusters yielded high accuracy for all methods, whereas
overlapping, unequal-sized, or subtle effects reduced accuracy by up to 50%.
SuStaIn could not scale beyond 17 features, HYDRA's accuracy varied
unpredictably with data heterogeneity. SmileGAN and SurrealGAN maintained
robust pattern detection but did not assign discrete cluster labels to
individuals. Conclusions: The study results demonstrate the impact of
statistical properties of input data across algorithms and highlight the need
for using realistic dataset distributions when new algorithms are being
developed and suggest greater focus on data-centric strategies that actively
shape and standardize the input distributions.

</details>


### [536] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/pdf/2506.08022)
*Chenxi Liu, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang*

Main category: cs.LG

TL;DR: MBPO addresses modality imbalance in LMMs by combining adversarial perturbation for hard negatives and GRPO for online training, improving performance and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: LMMs suffer from modality imbalance, favoring language over vision, leading to poor generalization and hallucinations. Existing methods don't address LLM biases or use dynamic data.

Method: MBPO creates an offline dataset with hard negatives via adversarial image perturbation and uses GRPO for online training with verified rewards.

Result: MBPO improves LMM performance on vision-language tasks and reduces hallucinations.

Conclusion: MBPO effectively balances modalities in LMMs, enhancing generalization and reducing biases.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [537] [Towards Unified and Lossless Latent Space for 3D Molecular Latent Diffusion Modeling](https://arxiv.org/pdf/2503.15567)
*Yanchen Luo, Zhiyuan Liu, Yi Zhao, Sihang Li, Hengxing Cai, Kenji Kawaguchi, Tat-Seng Chua, Yang Zhang, Xiang Wang*

Main category: cs.LG

TL;DR: UAE-3D proposes a unified latent space for 3D molecule generation, improving efficiency and quality by eliminating multi-modality complexities and maintaining SE(3) equivariance.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D molecule generation struggle with integrating multi-modal data (atom types, bonds, 3D coordinates) and maintaining SE(3) equivariance, leading to inefficiencies.

Method: UAE-3D uses a multi-modal VAE to compress 3D molecules into latent sequences from a unified space, enabling latent diffusion modeling with a Diffusion Transformer.

Result: The method achieves significant improvements in efficiency and quality, reducing FCD by 72.6% on GEOM-Drugs and enhancing geometric fidelity by over 70%.

Conclusion: UAE-3D sets new benchmarks for 3D molecule generation, demonstrating superior performance in both de novo and conditional tasks.

Abstract: 3D molecule generation is crucial for drug discovery and material science,
requiring models to process complex multi-modalities, including atom types,
chemical bonds, and 3D coordinates. A key challenge is integrating these
modalities of different shapes while maintaining SE(3) equivariance for 3D
coordinates. To achieve this, existing approaches typically maintain separate
latent spaces for invariant and equivariant modalities, reducing efficiency in
both training and sampling. In this work, we propose \textbf{U}nified
Variational \textbf{A}uto-\textbf{E}ncoder for \textbf{3D} Molecular Latent
Diffusion Modeling (\textbf{UAE-3D}), a multi-modal VAE that compresses 3D
molecules into latent sequences from a unified latent space, while maintaining
near-zero reconstruction error. This unified latent space eliminates the
complexities of handling multi-modality and equivariance when performing latent
diffusion modeling. We demonstrate this by employing the Diffusion
Transformer--a general-purpose diffusion model without any molecular inductive
bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9
datasets demonstrate that our method significantly establishes new benchmarks
in both \textit{de novo} and conditional 3D molecule generation, achieving
leading efficiency and quality. On GEOM-Drugs, it reduces FCD by 72.6\% over
the previous best result, while achieving over 70\% relative average
improvements in geometric fidelity.

</details>


### [538] [Improving Discriminator Guidance in Diffusion Models](https://arxiv.org/pdf/2503.16117)
*Alexandre Verine, Ahmed Mehdi Inane, Florian Le Bronnec, Benjamin Negrevergne, Yann Chevaleyre*

Main category: cs.LG

TL;DR: Discriminator Guidance, commonly used with Cross-Entropy loss, may not improve distribution alignment. A new training objective is proposed to minimize KL divergence, showing better sample quality.


<details>
  <summary>Details</summary>
Motivation: Standard Discriminator Guidance with Cross-Entropy loss can increase KL divergence due to overfitting, motivating a better training objective.

Method: Propose a theoretically sound training objective for discriminator guidance to properly minimize KL divergence.

Result: Empirical results show the proposed method consistently improves sample quality over conventional methods.

Conclusion: The new training objective effectively addresses the limitations of standard Discriminator Guidance, enhancing sample quality.

Abstract: Discriminator Guidance has become a popular method for efficiently refining
pre-trained Score-Matching Diffusion models. However, in this paper, we
demonstrate that the standard implementation of this technique does not
necessarily lead to a distribution closer to the real data distribution.
Specifically, we show that training the discriminator using Cross-Entropy loss,
as commonly done, can in fact increase the Kullback-Leibler divergence between
the model and target distributions, particularly when the discriminator
overfits. To address this, we propose a theoretically sound training objective
for discriminator guidance that properly minimizes the KL divergence. We
analyze its properties and demonstrate empirically across multiple datasets
that our proposed method consistently improves over the conventional method by
producing samples of higher quality.

</details>


### [539] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/pdf/2506.08054)
*Yiming Wang, Hao Peng, Senzhang Wang, Haohua Du, Chunyang Liu, Jia Wu, Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer is a novel SpatioTemporal Attention Mixture of Experts network for traffic data imputation, addressing block-wise missing data and nonstationary traffic issues with dynamic graph structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with block-wise missing data and static graph structures, limiting flexibility for nonstationary traffic data.

Method: Uses a Mixture of Experts (MoE) framework and Low-rank guided Sampling Graph ATtention (LrSGAT) to dynamically balance local and global correlations.

Result: Outperforms state-of-the-art methods on four traffic datasets.

Conclusion: STAMImputer effectively handles missing data and spatial correlations, offering a robust solution for traffic data imputation.

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [540] [RocketPPA: Code-Level Power, Performance, and Area Prediction via LLM and Mixture of Experts](https://arxiv.org/pdf/2503.21971)
*Armin Abdollahi, Mehdi Kamal, Massoud Pedram*

Main category: cs.LG

TL;DR: RocketPPA is a fast HDL code-level PPA estimator using an LLM-based regression model with MoE and LoRA, achieving higher accuracy and speed than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate hardware design by providing early, accurate PPA estimations without manual feature engineering or slow synthesis flows.

Method: Uses an LLM-based regression model with MoE architecture and LoRA for efficient training, plus an LLM-based HDL code repair framework for dataset generation.

Result: Improves pass rates for PPA predictions (e.g., 13.6% for area at 10% error) and achieves 20-30x speedup over competitors.

Conclusion: RocketPPA significantly enhances PPA estimation accuracy and speed, benefiting early hardware design stages.

Abstract: This paper presents RocketPPA, a novel ultra-fast power, performance (delay),
and area (PPA) estimator operating directly at the code-level abstraction using
HDL code as input. The key technical innovation is its LLM-based regression
model, which uniquely integrates a large language model (LLM) with a
mixture-of-experts (MoE) architecture composed of multilayer perceptrons
(MLPs). The LLM interprets the input HDL code and then utilizes its final
hidden-layer representations to predict PPA metrics. Low-rank adaptation (LoRA)
is used for parameter-efficient fine-tuning to enable efficient LLM training.
Furthermore, the work includes the development of an LLM-based HDL code repair
framework to generate a large and synthesizable training dataset. Experimental
results on the VerilogEval benchmark demonstrate that RocketPPA achieves
significant improvements in the accuracy of PPA estimation compared to previous
state-of-the-art methods like Llama3-MetRex-8B. Specifically, at a 10% relative
error threshold, RocketPPA enhances the pass rate for area prediction by 13.6%,
delay by 9.4%, and power by 14.7%. At a 20% threshold, the improvements are
9.6% for area, 10.8% for delay, and 18.5% for power. Moreover, RocketPPA
achieves a speedup of over 20x compared to MetRex and 30x over MasterRTL in
processing the test set. The impact of RocketPPA is the potential to
substantially accelerate the hardware design process by providing accurate PPA
estimations early in the design cycle, thus avoiding the overhead of manual
feature engineering and time-consuming synthesis flows.

</details>


### [541] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/pdf/2506.08309)
*Katherine Tieu, Dongqi Fu, Zihao Li, Ross Maciejewski, Jingrui He*

Main category: cs.LG

TL;DR: The paper introduces L-STEP, a learnable spatial-temporal positional encoding method for graph deep learning, addressing limitations of current positional encoding methods and demonstrating superior performance in temporal link prediction.


<details>
  <summary>Details</summary>
Motivation: Current positional encoding methods are limited by pre-defined functions, lack adaptability to evolving graphs, and are computationally expensive with transformers. The paper aims to develop a more effective and efficient solution.

Method: Proposes L-STEP, a learnable positional encoding scheme, and validates its effectiveness using MLPs, robustness tests, and theoretical complexity analysis.

Result: L-STEP outperforms 10 algorithms on 13 datasets in transductive and inductive settings, achieves leading performance on the TGB benchmark, and reduces empirical running time.

Conclusion: L-STEP provides a robust, efficient, and high-performing solution for spatial-temporal positional encoding in graph deep learning.

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, L-STEP obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [542] [Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2504.12501)
*Nathan Lambert*

Main category: cs.LG

TL;DR: A gentle introduction to RLHF, covering its origins, core methods, and advanced topics for those with a quantitative background.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible yet comprehensive guide to RLHF, bridging recent literature and interdisciplinary fields like economics and philosophy.

Method: Starts with foundational concepts, then details optimization stages (instruction tuning, reward modeling, alignment algorithms), and concludes with advanced research questions.

Result: A structured resource for understanding RLHF, from basics to cutting-edge challenges.

Conclusion: The book aims to equip readers with knowledge of RLHF's methods and open questions, fostering further exploration in the field.

Abstract: Reinforcement learning from human feedback (RLHF) has become an important
technical and storytelling tool to deploy the latest machine learning systems.
In this book, we hope to give a gentle introduction to the core methods for
people with some level of quantitative background. The book starts with the
origins of RLHF -- both in recent literature and in a convergence of disparate
fields of science in economics, philosophy, and optimal control. We then set
the stage with definitions, problem formulation, data collection, and other
common math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction tuning to
training a reward model and finally all of rejection sampling, reinforcement
learning, and direct alignment algorithms. The book concludes with advanced
topics -- understudied research questions in synthetic data and evaluation --
and open questions for the field.

</details>


### [543] [Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/pdf/2505.02604)
*Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee*

Main category: cs.LG

TL;DR: The paper explores the existence of continuous low-loss paths in neural network parameter spaces, proposing a new algorithm to investigate these paths beyond just connecting minima.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of low-loss regions in neural networks, challenging the notion of isolated minima and exploring parameter redundancy.

Method: A new algorithm is proposed to investigate low-loss paths in the full parameter space, tested on LeNet5, ResNet18, and Compact Convolutional Transformer architectures.

Result: Experiments confirm the existence of continuous low-loss paths, suggesting the low-loss region is fully connected and continuous.

Conclusion: The findings provide insights into over-parameterization, parameter redundancy, and offer new visualization methods and opportunities for improving model generalization.

Abstract: Visualizations of the loss landscape in neural networks suggest that minima
are isolated points. However, both theoretical and empirical studies indicate
that it is possible to connect two different minima with a path consisting of
intermediate points that also have low loss. In this study, we propose a new
algorithm which investigates low-loss paths in the full parameter space, not
only between two minima. Our experiments on LeNet5, ResNet18, and Compact
Convolutional Transformer architectures consistently demonstrate the existence
of such continuous paths in the parameter space. These results suggest that the
low-loss region is a fully connected and continuous space in the parameter
space. Our findings provide theoretical insight into neural network
over-parameterization, highlighting that parameters collectively define a
high-dimensional low-loss space, implying parameter redundancy exists only
within individual models and not throughout the entire low-loss space.
Additionally, our work also provides new visualization methods and
opportunities to improve model generalization by exploring the low-loss space
that is closer to the origin.

</details>


### [544] [Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning](https://arxiv.org/pdf/2505.16353)
*Céline Comte, Pascal Moyal*

Main category: cs.LG

TL;DR: The paper introduces a scheme for optimizing arrival rates in quasi-reversible queueing systems, proposing a new definition of quasi-reversibility and balanced arrival control policies. It shows these policies preserve quasi-reversibility and applies the framework to optimization and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To generalize and optimize arrival rate control in quasi-reversible queueing systems, extending beyond Whittle networks.

Method: Proposes an alternative definition of quasi-reversibility, introduces balanced arrival control policies, and proves their preservation of quasi-reversibility. Applies the framework to canonical examples and admission control.

Result: Balanced arrival control policies preserve quasi-reversibility, with specified stationary measures. Demonstrated applicability in optimization and reinforcement learning.

Conclusion: The framework successfully generalizes and optimizes arrival rate control in quasi-reversible systems, with practical applications in admission control and learning.

Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.

</details>


### [545] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/pdf/2505.17777)
*Harish G. Ramaswamy, L. A. Prashanth*

Main category: cs.LG

TL;DR: The paper addresses estimating and optimizing utility-based shortfall risk (UBSR) in regression, proposing a gradient-based algorithm with convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: UBSR is a non-linear risk measure, making empirical risk minimization challenging. The work aims to provide theoretical and algorithmic solutions for UBSR optimization.

Method: Derives a concentration bound for UBSR estimation, formulates UBSR optimization as a pseudo-linear problem, and introduces gradient and linear minimization oracles for a bisection-type algorithm.

Result: The proposed algorithm converges to the UBSR-optimal solution, supported by theoretical guarantees.

Conclusion: The paper provides a practical and theoretically sound approach to UBSR optimization in regression problems.

Abstract: We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [546] [What Can RL Bring to VLA Generalization? An Empirical Study](https://arxiv.org/pdf/2505.19789)
*Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang*

Main category: cs.LG

TL;DR: RL fine-tuning (e.g., PPO) improves generalization in VLAs over supervised fine-tuning (SFT), especially in semantic understanding and execution robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of SFT in VLA models by exploring RL's potential for better generalization.

Method: Systematic benchmark for VLA generalization evaluation, comparing RL (PPO, DPO, GRPO) and SFT.

Result: PPO outperforms SFT and other RL methods in semantic and execution robustness, with comparable visual robustness.

Conclusion: PPO is effective for VLA generalization, with a proposed training recipe for practical use.

Abstract: Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io

</details>


### [547] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/pdf/2505.22935)
*Jipeng Li, Yanning Shen*

Main category: cs.LG

TL;DR: Unconditional Graph Diffusion Models (GDMs) can implicitly infer noise levels from corrupted graph structures, eliminating the need for explicit noise conditioning, while maintaining or improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that explicit noise-level conditioning is essential for GDMs by exploring if denoisers can implicitly infer noise levels from corrupted graph structures.

Method: Develop a theoretical framework for Bernoulli edge-flip corruptions and extend it to coupled structure-attribute noise. Evaluate using synthetic and real-world datasets with models like GDSS and DiGress.

Result: Unconditional GDMs perform comparably or better than conditioned ones, with 4-6% fewer parameters and 8-10% faster computation.

Conclusion: Graph data's high-dimensional nature often encodes enough information for denoising, enabling simpler and more efficient GDM architectures.

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [548] [Logits-Based Finetuning](https://arxiv.org/pdf/2505.24461)
*Jingyao Li, Senqiao Yang, Sitong Wu, Han Shi, Chuanyang Zheng, Hong Xu, Jiaya Jia*

Main category: cs.LG

TL;DR: A logits-based fine-tuning framework is proposed to improve LLMs by combining teacher logits with ground truth labels, achieving significant accuracy gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional SFT fails to capture token-level dependencies and linguistic diversity, prompting the need for a more effective training method.

Method: Integrates supervised learning and knowledge distillation by combining teacher logits with ground truth labels to create enriched training targets.

Result: Achieves 18% accuracy gain on Mawps, 22.7% on TabMWP, and 7.28% average improvement across nine mathematical benchmarks.

Conclusion: The proposed framework outperforms prior SFT models, offering a more reliable and effective training approach.

Abstract: In recent years, developing compact and efficient large language models
(LLMs) has emerged as a thriving area of research. Traditional Supervised
Fine-Tuning (SFT), which relies on singular ground truth labels, often fails to
capture token-level dependencies and linguistic diversity. To address these
limitations, we propose a logits-based fine-tuning framework that integrates
the strengths of supervised learning and knowledge distillation. Our approach
constructs enriched training targets by combining teacher logits with ground
truth labels, preserving both correctness and linguistic diversity. This
ensures more reliable and effective training. We constructed a large-scale 1.2M
logits dataset and trained a series of science-focused models. Experimental
results demonstrate that our method achieves significant improvements, with
accuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used
mathematical benchmarks, our method consistently outperforms prior SFT models,
achieving an average improvement of 7.28%. Codes are available at
https://github.com/dvlab-research/Logits-Based-Finetuning.

</details>


### [549] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/pdf/2506.02300)
*Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet*

Main category: cs.LG

TL;DR: The paper introduces a framework to visualize how deep neural networks transition inputs between classes by amplifying gradients in the steerable pyramid domain, revealing decision boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods lack clarity on how models distinguish classes or transition inputs between them.

Method: Uses phase-based motion magnification, decomposes images with the Complex Steerable Pyramid, and amplifies class-conditional gradients for linear extrapolation.

Result: Produces semantically meaningful transformations, highlighting sensitive directions in the model's decision boundaries.

Conclusion: The framework offers a novel, interpretable way to understand neural classifiers' internal representations.

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [550] [Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order](https://arxiv.org/pdf/2506.04430)
*Egor Petrov, Grigoriy Evseev, Aleksey Antonov, Andrey Veprikov, Pavel Plyusnin, Nikolay Bushkov, Stanislav Moiseev, Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper introduces zero-order (ZO) optimization methods, JAGUAR SignSGD and JAGUAR Muon, as efficient alternatives to traditional first-order optimizers for fine-tuning LLMs, offering memory reduction and strong convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional first-order optimizers like SGD and Adam are computationally expensive for fine-tuning large language models (LLMs), prompting the need for more efficient alternatives.

Method: The authors propose JAGUAR SignSGD, a ZO momentum-based algorithm, and JAGUAR Muon, a ZO extension of the Muon optimizer, both designed for parameter-efficient fine-tuning.

Result: The proposed methods achieve or surpass the convergence quality of first-order methods while significantly reducing memory usage, as demonstrated in LLM fine-tuning benchmarks.

Conclusion: ZO optimization methods are a practical and theoretically grounded solution for resource-constrained LLM adaptation, with promising empirical and theoretical results.

Abstract: Fine-tuning Large Language Models (LLMs) is essential for adapting
pre-trained models to downstream tasks. Yet traditional first-order optimizers
such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and
computational costs that scale poorly with model size. In this paper, we
investigate zero-order (ZO) optimization methods as a memory- and
compute-efficient alternative, particularly in the context of
parameter-efficient fine-tuning techniques like LoRA. We propose
$\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO
SignSGD, requiring the same number of parameters as the standard ZO SGD and
only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our
knowledge, this is the first study to establish rigorous convergence guarantees
for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR
Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix
structure of model parameters, and we provide its convergence rate under
arbitrary stochastic noise. Through extensive experiments on challenging LLM
fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or
exceed the convergence quality of standard first-order methods, achieving
significant memory reduction. Our theoretical and empirical results establish
new ZO optimization methods as a practical and theoretically grounded approach
for resource-constrained LLM adaptation. Our code is available at
https://github.com/brain-mmo-lab/ZO_LLM

</details>


### [551] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/pdf/2506.05668)
*Jiajun He, José Miguel Hernández-Lobato, Yuanqi Du, Francisco Vargas*

Main category: cs.LG

TL;DR: The paper introduces the Radon-Nikodym Estimator (RNE), a versatile framework for diffusion inference-time density estimation and control, unifying existing methods with theoretical and practical benefits.


<details>
  <summary>Details</summary>
Motivation: To provide a unified and intuitive framework for density estimation and inference-time control in diffusion models, addressing the need for theoretical clarity and practical versatility.

Method: RNE leverages the density ratio between path distributions, rooted in variational inference and probabilistic principles, to connect and unify existing methods.

Result: RNE achieves strong performance in diffusion density estimation and demonstrates broad applicability in inference-time control tasks like annealing, model composition, and reward-tilting.

Conclusion: RNE offers a theoretically grounded and practically versatile solution for density estimation and control in diffusion models, with promising scaling performance.

Abstract: In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE delivers strong results in diffusion density
estimation, and offers broad applicability to inference-time control tasks --
such as annealing, diffusion model composition, and reward-tilting -- with
promising inference-time scaling performance.

</details>


### [552] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/pdf/2506.05957)
*Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen*

Main category: cs.LG

TL;DR: PrunE is a pruning-based method to improve Graph Neural Networks' OOD generalization by removing spurious edges, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with performance under distribution shifts due to spurious edges, and existing methods fail to reliably identify invariant subgraphs.

Method: PrunE uses two regularization terms: graph size constraint and ε-probability alignment to prune spurious edges.

Result: PrunE achieves superior OOD performance and significantly outperforms state-of-the-art methods.

Conclusion: PrunE effectively improves GNNs' OOD generalization by pruning spurious edges, validated by theory and experiments.

Abstract: Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, PrunE retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [553] [Certified Unlearning for Neural Networks](https://arxiv.org/pdf/2506.06985)
*Anastasia Koloskova, Youssef Allouah, Animesh Jha, Rachid Guerraoui, Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper proposes a certified method for machine unlearning, ensuring provable guarantees without restrictive assumptions, and outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns and regulatory requirements like the 'right to be forgotten' by removing specific training data's influence from models.

Method: Noisy fine-tuning on retain data to leverage privacy amplification by stochastic post-processing, requiring no loss function assumptions.

Result: Achieves formal unlearning guarantees and performs effectively in practice, outperforming existing methods.

Conclusion: The proposed method is broadly applicable, efficient, and accurate, providing a practical solution for certified machine unlearning.

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025

</details>


### [554] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/pdf/2506.06990)
*Mingyi Li, Michael R. Metel, Akiko Takeda*

Main category: cs.LG

TL;DR: The paper analyzes the K-means algorithm's local optimality, proposes modifications to ensure local optimality, and validates improvements through experiments.


<details>
  <summary>Details</summary>
Motivation: There's a lack of rigorous analysis of K-means' local optimality guarantees despite its widespread use.

Method: The authors present conditions for local optimality and propose modified K-means algorithms using Bregman divergence, maintaining original computational complexity.

Result: Numerical experiments show the original K-means sometimes fails to achieve local optimality, while the proposed methods improve solutions with reduced clustering loss.

Conclusion: The modified K-means algorithms ensure local optimality without increasing computational cost, validated by empirical results.

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [555] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/pdf/2506.07088)
*Ilja Kuzborskij, Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [556] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/pdf/2506.07254)
*Kevin Frans, Sergey Levine, Pieter Abbeel*

Main category: cs.LG

TL;DR: The paper proposes SPlus, an improved version of the Shampoo algorithm, addressing divergence, learning rate transfer, and parameter noise issues, achieving faster convergence than Adam.


<details>
  <summary>Details</summary>
Motivation: To address key limitations in the Shampoo optimization algorithm, such as divergence, inefficient learning rate transfer, and parameter noise, leading to more stable and efficient training.

Method: Introduces SPlus with three improvements: bounded updates for stability, shape-aware scaling for learning rate transfer, and iterate-averaging to reduce parameter noise. Evaluated on Transformer tasks.

Result: SPlus achieves Adam's validation performance in 44% of gradient steps and 62% of wallclock time across language modeling, image classification, and diffusion modeling tasks.

Conclusion: SPlus effectively resolves Shampoo's issues, offering faster and more stable optimization, making it a viable alternative to Adam.

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [557] [EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network](https://arxiv.org/pdf/2506.07288)
*Weijie Guan, Haohui Wang, Jian Kang, Lihui Liu, Dawei Zhou*

Main category: cs.LG

TL;DR: EVINET is a framework for open-world graph learning, addressing misclassification and out-of-distribution detection using Beta embedding and subjective logic.


<details>
  <summary>Details</summary>
Motivation: Current graph learning assumes a closed-world, but real-world tasks require handling noisy, open environments with unknown classes.

Method: EVINET integrates Beta embedding and subjective logic, featuring Dissonance Reasoning for misclassification and Vacuity Reasoning for out-of-distribution detection.

Result: EVINET outperforms state-of-the-art methods in classification, misclassification detection, and out-of-distribution detection.

Conclusion: EVINET highlights the importance of uncertainty estimation and logical reasoning for robust open-world graph learning.

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [558] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/pdf/2506.07459)
*Ziwen Wang, Jiajun Fan, Ruihan Guo, Thao Nguyen, Heng Ji, Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero is a novel framework using online reinforcement learning to improve protein generative models, achieving higher success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality protein datasets limits the success rate of protein generative models, prompting the need for scalable self-improvement methods.

Method: ProteinZero employs online reinforcement learning with efficient proxy reward models (ESM-fold and rapid ddG predictor) and includes multi-reward maximization, KL-divergence, and diversity regularization.

Result: ProteinZero outperforms existing methods, improving structural accuracy, designability, stability, and diversity, while reducing failure rates by 36%-48% and achieving >90% success rates.

Conclusion: ProteinZero establishes a continuous self-improvement paradigm for protein design, enabling efficient exploration of the protein design space.

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [559] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/pdf/2506.07584)
*Hao Li, Bowen Deng, Chang Xu, Zhiyuan Feng, Viktor Schlegel, Yu-Hao Huang, Yizheng Sun, Jingyuan Sun, Kailai Yang, Yiyao Yu, Jiang Bian*

Main category: cs.LG

TL;DR: MIRA is a unified foundation model for medical time series forecasting, addressing challenges like irregular intervals and missing values. It improves forecasting accuracy by 10% (out-of-distribution) and 7% (in-distribution) compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing generalist time series models fail to handle medical data due to irregular intervals, heterogeneous sampling, and missing values. MIRA aims to overcome these challenges.

Method: MIRA uses Continuous-Time Rotary Positional Encoding, a frequency-specific mixture-of-experts layer, and a Neural ODE-based Continuous Dynamics Extrapolation Block.

Result: MIRA reduces forecasting errors by 10% (out-of-distribution) and 7% (in-distribution) compared to baselines.

Conclusion: MIRA sets a foundation for future medical time series research, offering improved accuracy and robustness.

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [560] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/pdf/2506.08270)
*Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava*

Main category: cs.LG

TL;DR: A novel method jointly optimizes neural network architecture and weights using a continuous latent space, outperforming manual design and NAS.


<details>
  <summary>Details</summary>
Motivation: Manual neural network design is labor-intensive, while NAS often separates architecture search and weight training. This work aims to unify both processes.

Method: Trains a universal multi-scale autoencoder to embed architectures and weights into a continuous latent space. Gradient descent optimizes a randomly initialized point in this space, incorporating sparsity and compactness penalties.

Result: Effective discovery of sparse, compact neural networks with strong performance on synthetic regression tasks.

Conclusion: The proposed framework successfully unifies architecture and weight optimization, offering a scalable and efficient alternative to traditional methods.

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [561] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/pdf/2506.08473)
*Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan*

Main category: cs.LG

TL;DR: AsFT (Anchoring Safety in Fine-Tuning) is a method to enhance LLM safety during fine-tuning by using alignment direction as an anchor to suppress harmful updates, reducing harmful behavior by 7.60% and improving performance by 3.44%.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to safety risks during fine-tuning, where even small malicious or harmless data can compromise safeguards.

Method: AsFT integrates a regularization term into the training objective, using alignment direction to suppress harmful updates and constrain fine-tuning within a safety basin.

Result: AsFT reduces harmful behavior by 7.60% and improves model performance by 3.44%, outperforming Safe LoRA.

Conclusion: AsFT effectively ensures safety during fine-tuning while maintaining robust performance, framing the parameter space as a narrow safety basin.

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [562] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/pdf/2506.08551)
*Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm is a specialized LLM for automated communication system formulation, trained with a novel dataset (CSFRC) and a two-stage method (SFT + C-ReMax RL), outperforming general LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing general-purpose LLMs lack domain-specific knowledge and reasoning for communication system formulation, necessitating a specialized solution.

Method: Two-stage training: SFT with CoT data for domain knowledge, followed by rule-based RL (C-ReMax) for advanced reasoning.

Result: DeepForm achieves state-of-the-art performance, surpassing larger proprietary LLMs in diverse scenarios.

Conclusion: DeepForm bridges the gap in specialized LLMs for communication systems, with plans to release resources for further research.

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [563] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/pdf/2506.08574)
*Alvise Dei Rossi, Matteo Metaldi, Michal Bechny, Irina Filchenko, Julia van der Meer, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Francesca D. Faraci, Luigi Fiorillo*

Main category: cs.LG

TL;DR: SLEEPYLAND is an open-source framework for sleep staging evaluation, addressing challenges like model bias and dataset variability. It includes extensive datasets and introduces SOMNUS, an ensemble model outperforming individual models and human scorers.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of deep learning for sleep staging is limited by evaluation fairness, generalization issues, and human annotation variability.

Method: The framework includes large ID and OOD datasets, pre-trained models, and SOMNUS, an ensemble model combining architectures via soft voting.

Result: SOMNUS achieves robust performance (68.7%-87.2% macro-F1), outperforms individual models (94.9% cases), and surpasses human scorers in consensus. It also identifies model biases.

Conclusion: SLEEPYLAND and SOMNUS improve sleep staging evaluation, but no architecture consistently minimizes bias. Ensemble disagreement metrics predict human uncertainty.

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 220'000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [564] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/pdf/2506.08681)
*Phuc Minh Nguyen, Ngoc-Hieu Nguyen, Duy H. M. Nguyen, Anji Liu, An Mai, Binh T. Nguyen, Daniel Sonntag, Khoa D. Doan*

Main category: cs.LG

TL;DR: IS-DAAs, an importance-sampling approach, mitigates over-optimization in Direct Alignment Algorithms (DAAs) like DPO, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: DAAs like DPO are prone to over-optimization, causing model drift and degraded performance.

Method: Proposes IS-DAAs, which uses a clipped importance ratio to account for the reference policy distribution.

Result: IS-DAAs effectively reduce over-optimization, especially with low regularization, and outperform alternatives.

Conclusion: IS-DAAs offer a robust solution to over-optimization in DAAs, with public implementation available.

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [565] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/pdf/2506.08837)
*Luca Beurer-Kellner, Beat Buesser Ana-Maria Creţu, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tramèr, Václav Volhejn*

Main category: cs.LG

TL;DR: Proposes design patterns for AI agents to resist prompt injection attacks, balancing utility and security.


<details>
  <summary>Details</summary>
Motivation: Address the critical security challenge of prompt injection attacks in AI agents powered by LLMs, especially when handling sensitive data or tool access.

Method: Introduces principled design patterns for AI agents, analyzes their trade-offs, and validates through case studies.

Result: Demonstrates real-world applicability and effectiveness of the proposed patterns in resisting prompt injection.

Conclusion: The design patterns offer a practical solution to enhance the security of AI agents against prompt injection attacks.

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [566] [On Finetuning Tabular Foundation Models](https://arxiv.org/pdf/2506.08982)
*Ivan Rubachev, Akim Kotelnikov, Nikolay Kartashev, Artem Babenko*

Main category: cs.LG

TL;DR: The paper evaluates finetuning strategies for TabPFNv2, a tabular foundation model, finding full finetuning most effective. It explores how finetuning improves similarity measures and performance, achieving state-of-the-art results on some datasets.


<details>
  <summary>Details</summary>
Motivation: To determine the optimal finetuning approach for TabPFNv2 and understand how it reshapes the model's internal mechanisms, given inconsistent prior findings and the model's unique architecture.

Method: Systematic evaluation of finetuning strategies on diverse datasets, analyzing changes in TabPFNv2's internal mechanisms, particularly similarity measures.

Result: Full finetuning is most practical for TabPFNv2, improving performance on datasets up to 50K objects. It achieves state-of-the-art results on academic datasets but is less stable on temporally shifted or feature-rich datasets.

Conclusion: Finetuning enhances TabPFNv2's performance by refining similarity measures, making it competitive in certain scenarios, though traditional methods remain better for some cases.

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [567] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/pdf/2506.09335)
*Moshi Wei, Sparks Li*

Main category: cs.MA

TL;DR: ISEK is a decentralized network combining AI and human agents on Web3, using blockchain and incentives to foster emergent intelligence.


<details>
  <summary>Details</summary>
Motivation: To create a self-organizing cognitive ecosystem where AI and humans collaborate equally, free from centralized control.

Method: Uses a decentralized multi-agent architecture, symbiotic collaboration, and distributed consensus. Implements a six-phase workflow for task allocation, fault tolerance, and reputation tracking with $ISEK tokens.

Result: Enables large-scale, decentralized cognitive systems where agents evolve beyond centralized constraints.

Conclusion: ISEK shifts the paradigm by facilitating emergent intelligence through decentralized, collaborative systems.

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [568] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/pdf/2506.09434)
*Michael Amir, Matteo Bettini, Amanda Prorok*

Main category: cs.MA

TL;DR: The paper explores when diversity in multi-agent teams outperforms homogeneity, focusing on reward design and using multi-agent reinforcement learning (MARL) to validate theoretical insights.


<details>
  <summary>Details</summary>
Motivation: To understand when and why heterogeneous teams surpass homogeneous ones in multi-agent task allocation, particularly through the lens of reward design.

Method: The study combines theoretical analysis of reward aggregation operators with practical experiments using MARL and a novel algorithm, Heterogeneous Environment Design (HED), to test scenarios favoring heterogeneity.

Result: The curvature of reward operators determines heterogeneity's advantage, and HED successfully identifies reward regimes where diversity benefits teams, validating theoretical predictions.

Conclusion: The research provides a framework for understanding when behavioral diversity is advantageous, bridging theory and practical reward design in MARL.

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


### [569] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/pdf/2506.09600)
*Itay Nakash, George Kour, Koren Lazar, Matan Vetzler, Guy Uziel, Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: The paper addresses the challenge of ensuring task-oriented LLM-based agents adhere to strict policies while maintaining natural interactions. It introduces CRAFT, a red-teaming system, and tau-break benchmark to test agent robustness, and evaluates defense strategies.


<details>
  <summary>Details</summary>
Motivation: To ensure policy-adherent agents remain resilient against adversarial users exploiting them for personal gain.

Method: Proposes CRAFT, a multi-agent red-teaming system using policy-aware persuasive strategies, and introduces tau-break benchmark for robustness testing.

Result: CRAFT outperforms conventional jailbreak methods, and tau-break effectively assesses agent robustness. Defense strategies provide limited protection.

Conclusion: Stronger research-driven safeguards are needed to protect policy-adherent agents from adversarial attacks.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


### [570] [Incentive-based Platoon Formation: Optimizing the Personal Benefit for Drivers](https://arxiv.org/pdf/2411.00570)
*Julian Heinovski, Doğanalp Ergenç, Kirsten Thommes, Falko Dressler*

Main category: cs.MA

TL;DR: A novel platoon formation algorithm optimizes personal benefits for passenger car drivers by balancing fuel savings and travel time, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The impact of platooning for passenger cars is unclear due to trip and driver heterogeneity. The paper aims to optimize personal benefits for individual drivers.

Method: Proposes a new metric combining fuel and travel time costs into a single monetary value, and a platoon formation algorithm prioritizing driver benefits.

Result: The algorithm outperforms adaptive cruise control and similarity-based platooning, balancing fuel savings and travel time.

Conclusion: The approach provides an intuitive way for drivers to evaluate and benefit from platooning, independent of traffic or time cost.

Abstract: Platooning or cooperative adaptive cruise control (CACC) has been
investigated for decades, but debate about its lasting impact is still ongoing.
While the benefits of platooning and the formation of platoons are well
understood for trucks, they are less clear for passenger cars, which have a
higher heterogeneity in trips and drivers' preferences. Most importantly, it
remains unclear how to form platoons of passenger cars in order to optimize the
personal benefit for the individual driver. To this end, in this paper, we
propose a novel platoon formation algorithm that optimizes the personal benefit
for drivers of individual passenger cars. For computing vehicle-to-platoon
assignments, the algorithm utilizes a new metric that we propose to evaluate
the personal benefits of various driving systems, including platooning. By
combining fuel and travel time costs into a single monetary value, drivers can
estimate overall trip costs according to a personal monetary value for time
spent. This provides an intuitive way for drivers to understand and compare the
benefits of driving systems like human driving, adaptive cruise control (ACC),
and, of course, platooning. Unlike previous similarity-based methods, our
proposed algorithm forms platoons only when beneficial for the driver, rather
than solely for platooning. We demonstrate the new metric for the total trip
cost in a numerical analysis and explain its interpretation. Results of a
large-scale simulation study demonstrate that our proposed platoon formation
algorithm outperforms normal ACC as well as previous similarity-based
platooning approaches by balancing fuel savings and travel time, independent of
traffic and drivers' time cost.

</details>


### [571] [Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms](https://arxiv.org/pdf/2502.04388)
*Hepeng Li, Yuhong Liu, Jun Yan, Jie Gao, Xiaoou Yang, Mohamed Naili*

Main category: cs.MA

TL;DR: The paper advocates for dynamic, self-organizing AI systems to address challenges of uncoordinated AI agents in shared environments.


<details>
  <summary>Details</summary>
Motivation: The rise of AI systems with unaligned objectives in critical infrastructure domains poses risks of chaos and safety compromises.

Method: Proposes rethinking multi-agent frameworks to enable dynamic objective adjustment, compromise, coalition formation, and social feedback.

Result: Case studies highlight the need for emergent, context-aware AI systems in critical infrastructure.

Conclusion: A shift toward self-organizing, adaptive AI systems is essential for harmonious coexistence of AI agents.

Abstract: Artificial Intelligence (AI) agents capable of autonomous learning and
independent decision-making hold great promise for addressing complex
challenges across various critical infrastructure domains, including
transportation, energy systems, and manufacturing. However, the surge in the
design and deployment of AI systems, driven by various stakeholders with
distinct and unaligned objectives, introduces a crucial challenge: How can
uncoordinated AI systems coexist and evolve harmoniously in shared environments
without creating chaos or compromising safety? To address this, we advocate for
a fundamental rethinking of existing multi-agent frameworks, such as
multi-agent systems and game theory, which are largely limited to predefined
rules and static objective structures. We posit that AI agents should be
empowered to adjust their objectives dynamically, make compromises, form
coalitions, and safely compete or cooperate through evolving relationships and
social feedback. Through two case studies in critical infrastructure
applications, we call for a shift toward the emergent, self-organizing, and
context-aware nature of these multi-agentic AI systems.

</details>


### [572] [A Replica for our Democracies? On Using Digital Twins to Enhance Deliberative Democracy](https://arxiv.org/pdf/2504.07138)
*Claudio Novelli, Javier Argota Sánchez-Vaquerizo, Dirk Helbing, Antonino Rotolo, Luciano Floridi*

Main category: cs.MA

TL;DR: The paper proposes using Digital Twin (DT) technology as a computational tool to simulate and test deliberative democracy designs, overcoming limitations of real-world and lab experiments.


<details>
  <summary>Details</summary>
Motivation: Identifying optimal institutional designs for deliberative democracy is challenging due to the cost, ethical issues, and scale limitations of traditional methods.

Method: The paper explores DT technology to create dynamic models simulating real-world deliberation, enabling systematic testing of diverse institutional configurations in a virtual environment.

Result: DTs provide a controlled, scalable, and ethical way to assess novel democratic designs using synthetic data, avoiding societal disruption.

Conclusion: While promising, the approach has limitations; future research should address these gaps to refine the methodology.

Abstract: Deliberative democracy depends on carefully designed institutional
frameworks, such as participant selection, facilitation methods, and
decision-making mechanisms, that shape how deliberation performs. However,
identifying optimal institutional designs for specific contexts remains
challenging when relying solely on real-world observations or laboratory
experiments: they can be expensive, ethically and methodologically tricky, or
too limited in scale to give us clear answers. Computational experiments offer
a complementary approach, enabling researchers to conduct large-scale
investigations while systematically analyzing complex dynamics, emergent and
unexpected collective behavior, and risks or opportunities associated with
novel democratic designs. Therefore, this paper explores Digital Twin (DT)
technology as a computational testing ground for deliberative systems (with
potential applicability to broader institutional analysis). By constructing
dynamic models that simulate real-world deliberation, DTs allow researchers and
policymakers to rigorously test "what-if" scenarios across diverse
institutional configurations in a controlled virtual environment. This approach
facilitates evidence-based assessment of novel designs using synthetically
generated data, bypassing the constraints of real-world or lab-based
experimentation, and without societal disruption. The paper also discusses the
limitations of this new methodological approach and suggests where future
research should focus.

</details>


### [573] [Large Language Models Miss the Multi-Agent Mark](https://arxiv.org/pdf/2505.21298)
*Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M. Zhang, Elizabeth Black, Michael Luck, Philip Torr, Michael Wooldridge*

Main category: cs.MA

TL;DR: The paper critiques current MAS LLMs for lacking foundational MAS principles, identifies discrepancies in agency, environment, coordination, and behavior measurement, and advocates for better integration of MAS concepts.


<details>
  <summary>Details</summary>
Motivation: To address the gap between MAS theory and current MAS LLM implementations, highlighting missed opportunities and mischaracterizations.

Method: Systematic analysis of discrepancies in four key areas: agency, environment design, coordination, and emergent behavior measurement.

Result: Identifies oversimplified, LLM-centric architectures lacking MAS characteristics like autonomy and social interaction.

Conclusion: Advocates for integrating established MAS concepts and precise terminology to avoid mischaracterization and leverage research opportunities.

Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)
has led to an increase in frameworks leveraging multiple LLMs to tackle complex
tasks. However, much of this literature appropriates the terminology of MAS
without engaging with its foundational principles. In this position paper, we
highlight critical discrepancies between MAS theory and current MAS LLMs
implementations, focusing on four key areas: the social aspect of agency,
environment design, coordination and communication protocols, and measuring
emergent behaviours. Our position is that many MAS LLMs lack multi-agent
characteristics such as autonomy, social interaction, and structured
environments, and often rely on oversimplified, LLM-centric architectures. The
field may slow down and lose traction by revisiting problems the MAS literature
has already addressed. Therefore, we systematically analyse this issue and
outline associated research opportunities; we advocate for better integrating
established MAS concepts and more precise terminology to avoid
mischaracterisation and missed opportunities.

</details>


### [574] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/pdf/2506.07400)
*Philip R. Liu, Sparsh Bansal, Jimmy Dinh, Aditya Pawar, Ramani Satishkumar, Shail Desai, Neeraj Gupta, Xin Wang, Shu Hu*

Main category: cs.MA

TL;DR: MedChat proposes a multi-agent framework combining vision models and role-specific LLMs to improve glaucoma detection and reporting, addressing limitations of single-agent systems.


<details>
  <summary>Details</summary>
Motivation: To mitigate ophthalmologist shortages and enhance clinical reporting efficiency by overcoming challenges like hallucinations and limited interpretability in general LLMs.

Method: Uses a multi-agent diagnostic framework with specialized vision models and role-specific LLM agents, coordinated by a director agent.

Result: Enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting.

Conclusion: MedChat offers a promising solution for automated glaucoma detection and reporting, improving clinical accuracy and efficiency.

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [575] [Dynamic Sub-region Search in Homogeneous Collections Using CLIP](https://arxiv.org/pdf/2506.09506)
*Bastian Jäckl, Vojtěch Kloda, Daniel A. Keim, Jakub Lokoč*

Main category: cs.MM

TL;DR: The paper explores improving recall in text-image search for homogeneous domains by adding position constraints to queries via dynamic image partitioning, outperforming static methods but being sensitive to position accuracy.


<details>
  <summary>Details</summary>
Motivation: Users struggle with descriptive text queries in homogeneous domains (e.g., underwater imagery), leading to low recall. Position constraints could enhance search effectiveness.

Method: Dynamic image partitioning divides images into semantically meaningful regions, allowing users to specify regions for queries. Position constraints are integrated into semantic search models.

Result: Dynamic partitioning doubles retrieval performance over static methods but is highly sensitive to query position accuracy.

Conclusion: Dynamic sub-region-based search shows promise for improving recall but requires precise position inputs to be effective.

Abstract: Querying with text-image-based search engines in highly homogeneous
domain-specific image collections is challenging for users, as they often
struggle to provide descriptive text queries. For example, in an underwater
domain, users can usually characterize entities only with abstract labels, such
as corals and fish, which leads to low recall rates. Our work investigates
whether recall can be improved by supplementing text queries with position
information. Specifically, we explore dynamic image partitioning approaches
that divide candidates into semantically meaningful regions of interest.
Instead of querying entire images, users can specify regions they recognize.
This enables the use of position constraints while preserving the semantic
capabilities of multimodal models. We introduce and evaluate strategies for
integrating position constraints into semantic search models and compare them
against static partitioning approaches. Our evaluation highlights both the
potential and the limitations of sub-region-based search methods using dynamic
partitioning. Dynamic search models achieve up to double the retrieval
performance compared to static partitioning approaches but are highly sensitive
to perturbations in the specified query positions.

</details>


### [576] [Learning Quality from Complexity and Structure: A Feature-Fused XGBoost Model for Video Quality Assessment](https://arxiv.org/pdf/2506.09795)
*Amritha Premkumar, Prajit T Rajendran, Vignesh V Menon*

Main category: cs.MM

TL;DR: A novel reduced-reference VQA method using low-level complexity and structural features, trained with XGBoost, achieves competitive results with low computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, interpretable, and efficient VQA method without relying on deep neural networks for real-time applications.

Method: Extracts spatio-temporal features (VCA, SSIM), aggregates them via temporal pooling, computes residuals, and trains an XGBoost model.

Result: Competitive correlation with subjective scores and low computational footprint.

Conclusion: The method is suitable for real-time streaming and adaptive encoding due to its lightweight and generalizable design.

Abstract: This paper presents a novel approach for reduced-reference video quality
assessment (VQA), developed as part of the recent VQA Grand Challenge. Our
method leverages low-level complexity and structural information from reference
and test videos to predict perceptual quality scores. Specifically, we extract
spatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM
values from the test video to capture both texture and structural
characteristics. These features are aggregated through temporal pooling, and
residual features are calculated by comparing the original and distorted
feature sets. The combined features are used to train an XGBoost regression
model that estimates the overall video quality. The pipeline is fully
automated, interpretable, and highly scalable, requiring no deep neural
networks or GPU inference. Experimental results on the challenge dataset
demonstrate that our proposed method achieves competitive correlation with
subjective quality scores while maintaining a low computational footprint. The
model's lightweight design and strong generalization performance suit real-time
streaming quality monitoring and adaptive encoding scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [577] [Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality](https://arxiv.org/pdf/2506.09231)
*Saba Tabatabaee, Suzanne Boyce, Liran Oren, Mark Tiede, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The paper compares two speech inversion (SI) training approaches: baseline models (independent estimation of oral tract variables and nasalance) and a synergistic model (combining oral tract variables, source features, and nasalance). The synergistic model outperforms baselines by 5% in oral tract variables and 9% in nasalance estimation.


<details>
  <summary>Details</summary>
Motivation: To improve speech inversion systems by integrating velum movement patterns (nasalance) with oral tract variables and source features for more accurate articulatory mapping.

Method: Two approaches are compared: baseline models (independent estimation) and a synergistic model (combined estimation). Nasalance is used as ground truth for velum movement.

Result: The synergistic model achieves 5% better oral tract variables and 9% better nasalance estimation than baseline models.

Conclusion: Combining oral tract variables, source features, and nasalance in a synergistic model enhances speech inversion accuracy, demonstrating the value of integrated training approaches.

Abstract: Speech is produced through the coordination of vocal tract constricting
organs: lips, tongue, velum, and glottis. Previous works developed Speech
Inversion (SI) systems to recover acoustic-to-articulatory mappings for lip and
tongue constrictions, called oral tract variables (TVs), which were later
enhanced by including source information (periodic and aperiodic energies, and
F0 frequency) as proxies for glottal control. Comparison of the nasometric
measures with high-speed nasopharyngoscopy showed that nasalance can serve as
ground truth, and that an SI system trained with it reliably recovers velum
movement patterns for American English speakers. Here, two SI training
approaches are compared: baseline models that estimate oral TVs and nasalance
independently, and a synergistic model that combines oral TVs and source
features with nasalance. The synergistic model shows relative improvements of
5% in oral TVs estimation and 9% in nasalance estimation compared to the
baseline models.

</details>


### [578] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/pdf/2506.09521)
*Ünal Ege Gaznepoglu, Anna Leschanowsky, Ahmad Aloradi, Prachi Singh, Daniel Tenbrinck, Emanuël A. P. Habets, Nils Peters*

Main category: eess.AS

TL;DR: The paper evaluates speaker anonymization by using BERT as an ASV system, revealing biases in VoicePrivacy datasets due to linguistic content similarity.


<details>
  <summary>Details</summary>
Motivation: To assess the privacy benefits of speaker anonymization systems by examining the impact of intra-speaker linguistic content similarity in ASV attacks.

Method: Adapts BERT as an ASV system to analyze textual content of utterances in VoicePrivacy datasets.

Result: Achieves a mean EER of 35%, with some speakers as low as 2%, highlighting biases from semantically similar keywords.

Conclusion: Suggests reworking VoicePrivacy datasets for fair evaluation and questions reliance on global EER for privacy assessments.

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [579] [A Study on Speech Assessment with Visual Cues](https://arxiv.org/pdf/2506.09549)
*Shafique Ahmed, Ryandhimas E. Zezario, Nasir Saleem, Amir Hussain, Hsin-Min Wang, Yu Tsao*

Main category: eess.AS

TL;DR: A multimodal framework combining audio and visual cues improves non-intrusive speech quality (PESQ) and intelligibility (STOI) prediction, outperforming audio-only methods.


<details>
  <summary>Details</summary>
Motivation: Clean reference signals are often unavailable, necessitating non-intrusive methods for assessing speech quality and intelligibility.

Method: A dual-branch architecture extracts spectral features (STFT) and visual embeddings, fused and processed by a CNN-BLSTM with attention for multi-task learning (PESQ and STOI prediction).

Result: The model outperforms audio-only baselines, improving LCC by 9.61% for PESQ and 11.47% for STOI under seen noise conditions.

Conclusion: Visual cues enhance non-intrusive speech assessment accuracy, as demonstrated on the LRS3-TED dataset.

Abstract: Non-intrusive assessment of speech quality and intelligibility is essential
when clean reference signals are unavailable. In this work, we propose a
multimodal framework that integrates audio features and visual cues to predict
PESQ and STOI scores. It employs a dual-branch architecture, where spectral
features are extracted using STFT, and visual embeddings are obtained via a
visual encoder. These features are then fused and processed by a CNN-BLSTM with
attention, followed by multi-task learning to simultaneously predict PESQ and
STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND
corpus, show that our model outperforms the audio-only baseline. Under seen
noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%
(0.7403->0.8253) for STOI. These results highlight the effectiveness of
incorporating visual cues in enhancing the accuracy of non-intrusive speech
assessment.

</details>


### [580] [Unmasking real-world audio deepfakes: A data-centric approach](https://arxiv.org/pdf/2506.09606)
*David Combei, Adriana Stan, Dan Oneata, Nicolas Müller, Horia Cucu*

Main category: eess.AS

TL;DR: The paper introduces a real-world audio deepfake dataset (AI4T) and uses data-centric methods (curation, pruning, augmentation) to improve detection, achieving significant EER reductions.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection systems struggle with real-world examples, necessitating better datasets and methods.

Method: Data-centric approaches like dataset curation, pruning, and augmentation.

Result: 55% EER reduction on In-the-Wild dataset (1.7% absolute EER) and 63% reduction on AI4T dataset.

Conclusion: Data-centric strategies effectively enhance deepfake detection for real-world applications.

Abstract: The growing prevalence of real-world deepfakes presents a critical challenge
for existing detection systems, which are often evaluated on datasets collected
just for scientific purposes. To address this gap, we introduce a novel dataset
of real-world audio deepfakes. Our analysis reveals that these real-world
examples pose significant challenges, even for the most performant detection
models. Rather than increasing model complexity or exhaustively search for a
better alternative, in this work we focus on a data-centric paradigm, employing
strategies like dataset curation, pruning, and augmentation to improve model
robustness and generalization.
  Through these methods, we achieve a 55% relative reduction in EER on the
In-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on
our newly proposed real-world deepfakes dataset, AI4T. These results highlight
the transformative potential of data-centric approaches in enhancing deepfake
detection for real-world applications. Code and data available at:
https://github.com/davidcombei/AI4T.

</details>


### [581] [Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women](https://arxiv.org/pdf/2506.09653)
*Sakshi Joshi, Eldho Ittan George, Tahir Javed, Kaushal Bhogale, Nikhil Narasimhan, Mitesh M. Khapra*

Main category: eess.AS

TL;DR: The paper addresses poor ASR performance for rural Bhojpuri women by creating SRUTI, a benchmark, and using synthetic speech from minimal audio samples to improve accuracy by 4.7 WER.


<details>
  <summary>Details</summary>
Motivation: Digital inclusion for marginalized rural women in low-resource language regions like Bhojpuri is hindered by untested ASR systems.

Method: Develop SRUTI benchmark, generate synthetic speech from 25-30s audio per speaker (100 women), and augment datasets to improve ASR.

Result: Synthetic data improves ASR performance by 4.7 WER, offering a scalable solution.

Conclusion: Minimally intrusive synthetic data generation enhances ASR for low-resource languages, promoting digital inclusion.

Abstract: Digital inclusion remains a challenge for marginalized communities,
especially rural women in low-resource language regions like Bhojpuri.
Voice-based access to agricultural services, financial transactions, government
schemes, and healthcare is vital for their empowerment, yet existing ASR
systems for this group remain largely untested. To address this gap, we create
SRUTI ,a benchmark consisting of rural Bhojpuri women speakers. Evaluation of
current ASR models on SRUTI shows poor performance due to data scarcity, which
is difficult to overcome due to social and cultural barriers that hinder
large-scale data collection. To overcome this, we propose generating synthetic
speech using just 25-30 seconds of audio per speaker from approximately 100
rural women. Augmenting existing datasets with this synthetic data achieves an
improvement of 4.7 WER, providing a scalable, minimally intrusive solution to
enhance ASR and promote digital inclusion in low-resource language.

</details>


### [582] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/pdf/2506.09707)
*Suhas BN, Andrew M. Sherrill, Jyoti Alaparthi, Dominik Mattioli, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah*

Main category: eess.AS

TL;DR: A method for automatic temporal localization of key PE therapy fidelity elements using audio and transcripts, achieving a mean absolute error of 5.3 seconds.


<details>
  <summary>Details</summary>
Motivation: Manual review of PE therapy sessions for fidelity evaluation is labor-intensive; automation can improve scalability and efficiency.

Method: Fine-tuning Qwen2-Audio with LoRA on 30-second audio-transcript windows, using LLM-based prompting for fidelity labels.

Result: Best configuration (LoRA rank 8, 30s windows) achieves MAE of 5.3 seconds on 313 PE sessions.

Conclusion: The framework offers scalable fidelity tracking for PE therapy, aiding clinician training and quality assurance.

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [583] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/pdf/2506.09804)
*Peter Vieting, Maximilian Kannen, Benedikt Hilmes, Ralf Schlüter, Hermann Ney*

Main category: eess.AS

TL;DR: The paper explores regularization methods to improve neural front-ends for ASR, addressing overfitting and outperforming traditional features.


<details>
  <summary>Details</summary>
Motivation: Neural front-ends for ASR often underperform due to overfitting, prompting investigation into better regularization techniques.

Method: Examines audio perturbation and proposes STFT-domain masking to enhance SpecAugment for learnable front-ends.

Result: Combining these methods closes the performance gap between traditional and learnable features.

Conclusion: Effective regularization can make neural front-ends competitive with classical ASR feature extraction.

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


### [584] [Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience](https://arxiv.org/pdf/2402.03710)
*Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani*

Main category: eess.AS

TL;DR: LCR is a multimodal sound remixer that uses text instructions to control sound sources in a mixture without separation.


<details>
  <summary>Details</summary>
Motivation: Limited control over sound presence and volume in daily life drives the need for a user-friendly, text-based sound remixer.

Method: LCR uses a large language model to interpret text prompts, decomposes mixtures, applies semantic filters, and reassembles components.

Result: Improved signal quality in remixing tasks and robust zero-shot performance with diverse sound sources.

Conclusion: LCR offers a novel, effective approach for sound remixing via text instructions, demonstrated by a large dataset and strong experimental results.

Abstract: In daily life, we encounter a variety of sounds, both desirable and
undesirable, with limited control over their presence and volume. Our work
introduces "Listen, Chat, and Remix" (LCR), a novel multimodal sound remixer
that controls each sound source in a mixture based on user-provided text
instructions. LCR distinguishes itself with a user-friendly text interface and
its unique ability to remix multiple sound sources simultaneously within a
mixture, without needing to separate them. Users input open-vocabulary text
prompts, which are interpreted by a large language model to create a semantic
filter for remixing the sound mixture. The system then decomposes the mixture
into its components, applies the semantic filter, and reassembles filtered
components back to the desired output. We developed a 160-hour dataset with
over 100k mixtures, including speech and various audio sources, along with text
prompts for diverse remixing tasks including extraction, removal, and volume
control of single or multiple sources. Our experiments demonstrate significant
improvements in signal quality across all remixing tasks and robust performance
in zero-shot scenarios with varying numbers and types of sound sources. An
audio demo is available at: https://listenchatremix.github.io/demo.

</details>


### [585] [Channel Adaptation for Speaker Verification Using Optimal Transport with Pseudo Label](https://arxiv.org/pdf/2409.09396)
*Wenhao Yang, Jianguo Wei, Wenhuan Lu, Lei Li, Xugang Lu*

Main category: eess.AS

TL;DR: The paper proposes JPOT-PL, an unsupervised domain adaptation method to address channel mismatch in speaker verification, reducing EER by over 10%.


<details>
  <summary>Details</summary>
Motivation: Domain gap, especially from channel variation, degrades speaker verification performance, but existing methods lack effective alignment and discriminative learning.

Method: JPOT-PL combines optimal transport for distribution alignment with pseudo label-based discriminative learning.

Result: Experiments show a 10%+ reduction in EER compared to state-of-the-art methods.

Conclusion: JPOT-PL effectively mitigates channel mismatch in speaker verification, outperforming existing approaches.

Abstract: Domain gap often degrades the performance of speaker verification (SV)
systems when the statistical distributions of training data and real-world test
speech are mismatched. Channel variation, a primary factor causing this gap, is
less addressed than other issues (e.g., noise). Although various domain
adaptation algorithms could be applied to handle this domain gap problem, most
algorithms could not take the complex distribution structure in domain
alignment with discriminative learning. In this paper, we propose a novel
unsupervised domain adaptation method, i.e., Joint Partial Optimal Transport
with Pseudo Label (JPOT-PL), to alleviate the channel mismatch problem.
Leveraging the geometric-aware distance metric of optimal transport in
distribution alignment, we further design a pseudo label-based discriminative
learning where the pseudo label can be regarded as a new type of soft speaker
label derived from the optimal coupling. With the JPOT-PL, we carry out
experiments on the SV channel adaptation task with VoxCeleb as the basis
corpus. Experiments show our method reduces EER by over 10% compared with
several state-of-the-art channel adaptation algorithms.

</details>


### [586] [ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs](https://arxiv.org/pdf/2410.12359)
*Rui-Chen Zheng, Hui-Peng Du, Xiao-Hang Jiang, Yang Ai, Zhen-Hua Ling*

Main category: eess.AS

TL;DR: ERVQ enhances RVQ in neural audio codecs by optimizing intra- and inter-codebook usage, mitigating codebook collapse and improving performance.


<details>
  <summary>Details</summary>
Motivation: Current RVQ-based audio codecs suffer from codebook collapse, reducing effective codebook size and performance.

Method: ERVQ introduces intra-codebook optimization (online clustering, code balancing loss) and inter-codebook optimization (minimizing similarity between quantizations).

Result: ERVQ boosts codec performance across models, sampling rates, and bitrates, achieving 100% codebook utilization and improving LLM-generated speech naturalness.

Conclusion: ERVQ effectively addresses codebook collapse, enhancing audio codec performance and downstream applications like text-to-speech.

Abstract: Current neural audio codecs typically use residual vector quantization (RVQ)
to discretize speech signals. However, they often experience codebook collapse,
which reduces the effective codebook size and leads to suboptimal performance.
To address this problem, we introduce ERVQ, Enhanced Residual Vector
Quantization, a novel enhancement strategy for the RVQ framework in neural
audio codecs. ERVQ mitigates codebook collapse and boosts codec performance
through both intra- and inter-codebook optimization. Intra-codebook
optimization incorporates an online clustering strategy and a code balancing
loss to ensure balanced and efficient codebook utilization. Inter-codebook
optimization improves the diversity of quantized features by minimizing the
similarity between successive quantizations. Our experiments show that ERVQ
significantly enhances audio codec performance across different models,
sampling rates, and bitrates, achieving superior quality and generalization
capabilities. It also achieves 100% codebook utilization on one of the most
advanced neural audio codecs. Further experiments indicate that audio codecs
improved by the ERVQ strategy can improve unified speech-and-text large
language models (LLMs). Specifically, there is a notable improvement in the
naturalness of generated speech in downstream zero-shot text-to-speech tasks.
Audio samples are available here.

</details>


### [587] [Phonology-Guided Speech-to-Speech Translation for African Languages](https://arxiv.org/pdf/2410.23323)
*Peter Ochieng, Dennis Kaburu*

Main category: eess.AS

TL;DR: A prosody-guided framework for speech-to-speech translation (S2ST) leverages cross-linguistic pause synchrony to align and translate speech without transcripts, improving alignment and translation quality.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the observation that within-phylum language pairs exhibit lower pause variance and higher onset/offset correlation, suggesting prosodic cues can enhance S2ST.

Method: The framework includes SPaDA, a dynamic-programming alignment algorithm integrating silence consistency, rate synchrony, and semantic similarity, and SegUniDiff, a diffusion-based S2ST model guided by external gradients.

Result: SPaDA improves alignment by 3-4 points and reduces spurious matches by 38%. SegUniDiff matches cascade BLEU scores, reduces speaker error rate, and operates efficiently.

Conclusion: Prosodic cues in multilingual speech provide a reliable scaffold for scalable, non-autoregressive S2ST, supported by a new transcript-free BLEU suite for low-resource evaluation.

Abstract: We present a prosody-guided framework for speech-to-speech translation (S2ST)
that aligns and translates speech \emph{without} transcripts by leveraging
cross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news
corpus spanning five languages, we show that \emph{within-phylum} language
pairs exhibit 30--40\% lower pause variance and over 3$\times$ higher
onset/offset correlation compared to cross-phylum pairs. These findings
motivate \textbf{SPaDA}, a dynamic-programming alignment algorithm that
integrates silence consistency, rate synchrony, and semantic similarity. SPaDA
improves alignment $F_1$ by +3--4 points and eliminates up to 38\% of spurious
matches relative to greedy VAD baselines. Using SPaDA-aligned segments, we
train \textbf{SegUniDiff}, a diffusion-based S2ST model guided by
\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff
matches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\ 28.9 for UnitY),
reduces speaker error rate (EER) from 12.5\% to 5.3\%, and runs at an RTF of
1.02. To support evaluation in low-resource settings, we also release a
three-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with
human judgments. Together, our results show that prosodic cues in multilingual
speech provide a reliable scaffold for scalable, non-autoregressive S2ST.

</details>


### [588] [Model Attribution and Detection of Synthetic Speech via Vocoder Fingerprints](https://arxiv.org/pdf/2411.14013)
*Matías Pizarro, Mike Laszkiewicz, Shawkat Hesso, Dorothea Kolossa, Asja Fischer*

Main category: eess.AS

TL;DR: The paper addresses synthetic speech misuse by tackling three tasks: single-model attribution (open-world), model attribution (closed-world), and synthetic vs. real speech distinction, achieving high accuracy (99% AUROC) using standardized residuals as vocoder fingerprints.


<details>
  <summary>Details</summary>
Motivation: Advancements in speech generation raise misuse concerns, necessitating methods to attribute synthetic speech to specific models and distinguish it from real speech.

Method: Uses standardized average residuals from low-pass or EnCodec filtered audio signals as vocoder fingerprints for attribution and detection.

Result: Achieves over 99% AUROC on LJSpeech and JSUT datasets, with robustness to noise up to a certain level.

Conclusion: Standardized residuals are effective fingerprints for synthetic speech attribution and detection, offering high accuracy and noise resilience.

Abstract: As speech generation technology advances, so do the potential threats of
misusing synthetic speech signals. This work tackles three tasks: (1)
single-model attribution in an open-world setting corresponding to the task of
identifying whether synthetic speech signals originate from a specific vocoder
(which requires only target vocoder data), (2) model attribution in a
closed-world setting that corresponds to selecting the specific model that
generated a sample from a given set of models, and (3) distinguishing synthetic
from real speech. We show that standardized average residuals between audio
signals and their low-pass or EnCodec filtered versions serve as powerful
vocoder fingerprints that can be leveraged for all tasks achieving an average
AUROC of over 99% on LJSpeech and JSUT in most settings. The accompanying
robustness study shows that it is also resilient to noise levels up to a
certain degree.

</details>


### [589] [The trajectoRIR Database: Room Acoustic Recordings Along a Trajectory of Moving Microphones](https://arxiv.org/pdf/2503.23004)
*Stefano Damiano, Kathleen MacWilliam, Valerio Lorenzoni, Thomas Dietzen, Toon van Waterschoot*

Main category: eess.AS

TL;DR: The paper introduces trajectoRIR, a database combining dynamic and stationary acoustic recordings for diverse signal processing tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for large, diverse datasets in acoustic signal processing, especially for data-driven approaches.

Method: Recordings include moving microphones and stationary RIRs along an L-shaped trajectory, using various microphone configurations and robotic motion.

Result: The database features 8648 stationary RIRs and dynamic recordings, with Python tools for access.

Conclusion: trajectoRIR is a unique, versatile resource for tasks like sound localization, tracking, and auralization.

Abstract: Data availability is essential to develop acoustic signal processing
algorithms, especially when it comes to data-driven approaches that demand
large and diverse training datasets. For this reason, an increasing number of
databases have been published in recent years, including either room impulse
responses (RIRs) or audio recordings during motion. In this paper we introduce
the trajectoRIR database, an extensive, multi-array collection of both dynamic
and stationary acoustic recordings along a controlled trajectory in a room.
Specifically, the database features recordings using moving microphones and
stationary RIRs spatially sampling the room acoustics along an L-shaped
trajectory. This combination makes trajectoRIR unique and applicable in various
tasks ranging from sound source localization and tracking to spatially dynamic
sound field reconstruction, auralization and system identification. The
recording room has a reverberation time of 0.5 seconds, and the three different
microphone configurations employed include a dummy head, with additional
reference microphones located next to the ears, 3 first-order Ambisonics
microphones, two circular arrays of 16 and 4 channels, and a 12-channel linear
array. The motion of the microphones was achieved using a robotic cart
traversing a 4.62 meter-long rail at three speeds: [0.2, 0.4, 0.8] m/s. Audio
signals were reproduced using two stationary loudspeakers. The collected
database features 8648 stationary RIRs, as well as perfect sweeps, speech,
music, and stationary noise recorded during motion. Python functions are
included to access the recorded audio as well as to retrieve geometrical
information.

</details>


### [590] [The Search for Squawk: Agile Modeling in Bioacoustics](https://arxiv.org/pdf/2505.03071)
*Vincent Dumoulin, Otilia Stretcu, Jenny Hamer, Lauren Harrell, Rob Laber, Hugo Larochelle, Bart van Merriënboer, Amanda Navine, Patrick Hart, Ben Williams, Timothy A. C. Lamont, Tries B. Razak, Mars Coral Restoration Team, Sheryn Brodie, Brendan Doohan, Phil Eichinski, Paul Roe, Lin Schwarzkopf, Tom Denton*

Main category: eess.AS

TL;DR: A scalable system for bioacoustic recognizers reduces data and time requirements, enabling ecologists to address new challenges quickly.


<details>
  <summary>Details</summary>
Motivation: Passive acoustic monitoring (PAM) aids ecosystem health assessment but faces challenges in developing recognizers due to data and expertise needs.

Method: The system uses pre-trained acoustic embeddings, indexed audio search, and active learning to create efficient classifiers.

Result: Tested in real-world case studies (coral reefs, Hawaiian birds, Christmas Island birds) and simulations, the system proved scalable and generalizable.

Conclusion: The system enables rapid development of bioacoustic recognizers, making PAM more accessible for ecological research.

Abstract: Passive acoustic monitoring (PAM) has shown great promise in helping
ecologists understand the health of animal populations and ecosystems. However,
extracting insights from millions of hours of audio recordings requires the
development of specialized recognizers. This is typically a challenging task,
necessitating large amounts of training data and machine learning expertise. In
this work, we introduce a general, scalable and data-efficient system for
developing recognizers for novel bioacoustic problems in under an hour. Our
system consists of several key components that tackle problems in previous
bioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained
for birdsong classification minimize data hunger; 2) indexed audio search
allows the efficient creation of classifier training datasets, and 3)
precomputation of embeddings enables an efficient active learning loop,
improving classifier quality iteratively with minimal wait time. Ecologists
employed our system in three novel case studies: analyzing coral reef health
through unidentified sounds; identifying juvenile Hawaiian bird calls to
quantify breeding success and improve endangered species monitoring; and
Christmas Island bird occupancy modeling. We augment the case studies with
simulated experiments which explore the range of design decisions in a
structured way and help establish best practices. Altogether these experiments
showcase our system's scalability, efficiency, and generalizability, enabling
scientists to quickly address new bioacoustic challenges.

</details>


### [591] [TADA: Training-free Attribution and Out-of-Domain Detection of Audio Deepfakes](https://arxiv.org/pdf/2506.05802)
*Adriana Stan, David Combei, Dan Oneata, Horia Cucu*

Main category: eess.AS

TL;DR: The paper proposes a training-free, green AI method for audio deepfake model attribution using k-Nearest Neighbors (kNN) and a pre-trained SSL model, achieving high F1-scores for in-domain and out-of-domain detection.


<details>
  <summary>Details</summary>
Motivation: While deepfake detection is well-studied, identifying the exact source (e.g., the model behind a deepfake) is less explored. This paper addresses this gap in audio deepfake model attribution.

Method: The approach uses k-Nearest Neighbors (kNN) with a pre-trained self-supervised learning (SSL) model, requiring no additional training.

Result: Achieves an F1-score of 0.93 for in-domain detection and 0.84 for out-of-domain detection across five datasets.

Conclusion: The method is effective for source tracing in audio deepfakes, with strong performance and open-source availability.

Abstract: Deepfake detection has gained significant attention across audio, text, and
image modalities, with high accuracy in distinguishing real from fake. However,
identifying the exact source--such as the system or model behind a
deepfake--remains a less studied problem. In this paper, we take a significant
step forward in audio deepfake model attribution or source tracing by proposing
a training-free, green AI approach based entirely on k-Nearest Neighbors (kNN).
Leveraging a pre-trained self-supervised learning (SSL) model, we show that
grouping samples from the same generator is straightforward--we obtain an 0.93
F1-score across five deepfake datasets. The method also demonstrates strong
out-of-domain (OOD) detection, effectively identifying samples from unseen
models at an F1-score of 0.84.
  We further analyse these results in a multi-dimensional approach and provide
additional insights. All code and data protocols used in this work are
available in our open repository: https://github.com/adrianastan/tada/.

</details>


### [592] [Multi-Distillation from Speech and Music Representation Models](https://arxiv.org/pdf/2506.07237)
*Jui-Chiang Wei, Yi-Cheng Lin, Fabian Ritter-Gutierrez, Hung-yi Lee*

Main category: eess.AS

TL;DR: A multi-teacher distillation framework unifies speech and music models into a single compact model, matching domain-specific performance and excelling in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Real-world audio often mixes speech and music, but existing models handle only one domain, limiting their practicality.

Method: The framework uses domain-specific teachers (HuBERT for speech, MERT for music) and explores strategies to balance both domains.

Result: The unified model matches domain-specific models' performance and outperforms them in few-shot scenarios.

Conclusion: Cross-domain distillation is effective for diverse tasks, especially with limited data, proving the need for general models.

Abstract: Real-world audio often mixes speech and music, yet models typically handle
only one domain. This paper introduces a multi-teacher distillation framework
that unifies speech and music models into a single one while significantly
reducing model size. Our approach leverages the strengths of domain-specific
teacher models, such as HuBERT for speech and MERT for music, and explores
various strategies to balance both domains. Experiments across diverse tasks
demonstrate that our model matches the performance of domain-specific models,
showing the effectiveness of cross-domain distillation. Additionally, we
conduct few-shot learning experiments, highlighting the need for general models
in real-world scenarios where labeled data is limited. Our results show that
our model not only performs on par with specialized models but also outperforms
them in few-shot scenarios, proving that a cross-domain approach is essential
and effective for diverse tasks with limited data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [593] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/pdf/2506.09065)
*Abigail Copiaco, Christian Ritz, Yassine Himeur, Valsamma Eapen, Ammar Albanna, Wathiq Mansoor*

Main category: eess.IV

TL;DR: An AI-powered system using transfer learning and eye gaze data aims to streamline ASD diagnosis, offering in-home convenience and privacy while improving caregiver-therapist communication.


<details>
  <summary>Details</summary>
Motivation: The rising prevalence of ASD and the inefficiency of current diagnostic methods drive the need for a faster, more accessible solution.

Method: The system employs transfer learning and image transforms from eye gaze variables to diagnose ASD.

Result: The approach enables in-home diagnosis, reduces stress, and enhances privacy and communication between caregivers and therapists.

Conclusion: The proposed method provides timely, accessible ASD diagnosis while safeguarding privacy and improving outcomes.

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [594] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/pdf/2506.09095)
*Vivien van Veldhuizen, Vanessa Botha, Chunyao Lu, Melis Erdal Cesur, Kevin Groot Lipman, Edwin D. de Jong, Hugo Horlings, Clárisa Sanchez, Cees Snoek, Ritse Mann, Eric Marcus, Jonas Teuwen*

Main category: eess.IV

TL;DR: Foundation models (FMs) are transforming medical image analysis by leveraging large unlabeled datasets, reducing reliance on manual annotations, and adapting to clinical tasks with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: To explore how FMs are developed and applied in medical imaging (pathology, radiology, ophthalmology) and their potential to improve analysis efficiency and accuracy.

Method: Review of over 150 studies, covering FM pipelines (architectures, self-supervised learning, downstream adaptation) and comparing design choices across domains.

Result: FMs show promise in medical imaging by learning general-purpose features adaptable to specific tasks, with varied applications across domains.

Conclusion: While FMs offer significant potential, challenges remain, and future research should address open questions to optimize their use in medical imaging.

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [595] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/pdf/2506.09100)
*Haonan Zhang, Guoyan Lao, Yuyao Zhang, Hongjiang Wei*

Main category: eess.IV

TL;DR: LoREIN is an unsupervised dual-prior framework for accelerated 3D multi-parametric qMRI reconstruction, combining low-rank and continuity priors for improved fidelity.


<details>
  <summary>Details</summary>
Motivation: Current qMRI reconstruction methods using single priors or physics-informed models yield suboptimal results for highly undersampled data.

Method: LoREIN integrates low-rank prior (via LRR) and continuity prior (via INR) to enhance reconstruction accuracy.

Result: The framework enables high-fidelity reconstruction of weighted images and quantitative parameter maps.

Conclusion: LoREIN advances medical imaging with a zero-shot learning paradigm for complex reconstruction tasks.

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


### [596] [An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation](https://arxiv.org/pdf/2506.09161)
*Rajan Das Gupta, Md Imrul Hasan Showmick, Mushfiqur Rahman Abir, Shanjida Akter, Md. Yeasin Rahat, Md. Jakir Hossen*

Main category: eess.IV

TL;DR: A deep learning system using MobileNet V2 and ResNet-50 detects brain tumors and strokes from MRI images with high accuracy, offering potential for clinical use.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of brain abnormalities like tumors and strokes is crucial for timely intervention and better patient outcomes.

Method: The study employs convolutional neural networks (MobileNet V2 and ResNet-50) optimized via transfer learning to classify MRI scans into five categories, using a curated dataset with dropout and data augmentation to prevent overfitting.

Result: The models achieved 93% training and 88% validation accuracy, with ResNet-50 performing slightly better, while MobileNet V2 is suitable for low-resource settings.

Conclusion: The research provides an AI-driven solution for early brain abnormality detection, with potential for clinical deployment and future improvements.

Abstract: Early and accurate detection of brain abnormalities, such as tumors and
strokes, is essential for timely intervention and improved patient outcomes. In
this study, we present a deep learning-based system capable of identifying both
brain tumors and strokes from MRI images, along with their respective stages.
We have executed two groundbreaking strategies involving convolutional neural
networks, MobileNet V2 and ResNet-50-optimized through transfer learning to
classify MRI scans into five diagnostic categories. Our dataset, aggregated and
augmented from various publicly available MRI sources, was carefully curated to
ensure class balance and image diversity. To enhance model generalization and
prevent overfitting, we applied dropout layers and extensive data augmentation.
The models achieved strong performance, with training accuracy reaching 93\%
and validation accuracy up to 88\%. While ResNet-50 demonstrated slightly
better results, Mobile Net V2 remains a promising option for real-time
diagnosis in low resource settings due to its lightweight architecture. This
research offers a practical AI-driven solution for early brain abnormality
detection, with potential for clinical deployment and future enhancement
through larger datasets and multi modal inputs.

</details>


### [597] [The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset](https://arxiv.org/pdf/2506.09162)
*Tyler J. Richards, Adam E. Flanders, Errol Colak, Luciano M. Prevedello, Robyn L. Ball, Felipe Kitamura, John Mongan, Maryam Vazirabad, Hui-Ming Lin, Anne Kendell, Thanat Kanthawang, Salita Angkurawaranon, Emre Altinmakas, Hakan Dogan, Paulo Eduardo de Aguiar Kuriki, Arjuna Somasundaram, Christopher Ruston, Deniz Bulja, Naida Spahovic, Jennifer Sommer, Sirui Jiang, Eduardo Moreno Judice de Mattos Farina, Eduardo Caminha Nunes, Michael Brassil, Megan McNamara, Johanna Ortiz, Jacob Peoples, Vinson L. Uytana, Anthony Kam, Venkata N. S. Dola, Daniel Murphy, David Vu, Dataset Contributor Group, Dataset Annotator Group, Competition Data Notebook Group, Jason F. Talbott*

Main category: eess.IV

TL;DR: The RSNA LumbarDISC dataset is the largest public MRI lumbar spine dataset, annotated for degenerative changes, used in a 2024 competition to develop deep learning models for grading stenosis.


<details>
  <summary>Details</summary>
Motivation: To advance research in machine learning for lumbar spine imaging, improving patient care and clinical efficiency.

Method: The dataset includes 2,697 patients with 8,593 MRI series, annotated by expert radiologists for degenerative changes.

Result: The dataset is publicly available for non-commercial use, fostering AI development in spine imaging.

Conclusion: LumbarDISC supports AI research to enhance diagnostic accuracy and efficiency in lumbar spine care.

Abstract: The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging
Spine Classification (LumbarDISC) dataset is the largest publicly available
dataset of adult MRI lumbar spine examinations annotated for degenerative
changes. The dataset includes 2,697 patients with a total of 8,593 image series
from 8 institutions across 6 countries and 5 continents. The dataset is
available for free for non-commercial use via Kaggle and RSNA Medical Imaging
Resource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine
Degenerative Classification competition where competitors developed deep
learning models to grade degenerative changes in the lumbar spine. The degree
of spinal canal, subarticular recess, and neural foraminal stenosis was graded
at each intervertebral disc level in the lumbar spine. The images were
annotated by expert volunteer neuroradiologists and musculoskeletal
radiologists from the RSNA, American Society of Neuroradiology, and the
American Society of Spine Radiology. This dataset aims to facilitate research
and development in machine learning and lumbar spine imaging to lead to
improved patient care and clinical efficiency.

</details>


### [598] [An Interpretable Two-Stage Feature Decomposition Method for Deep Learning-based SAR ATR](https://arxiv.org/pdf/2506.09377)
*Chenwei Wang, Renjie Xu, Congwen Wu, Cunyi Yin, Ziyun Liao, Deqing Mao, Sitong Zhang, Hong Yan*

Main category: eess.IV

TL;DR: The paper proposes a physics-based two-stage feature decomposition method for interpretable deep SAR ATR, transforming deep features into interpretable attribute scattering center components (ASCC).


<details>
  <summary>Details</summary>
Motivation: Deep SAR ATR's black-box nature reduces confidence in decision-critical applications, necessitating interpretable reasoning logic.

Method: A two-stage decomposition: (1) feature decoupling and discrimination for global discriminability, and (2) MLO-NMTF for independent physical components.

Result: Experiments on four datasets confirm the method's interpretability, robust recognition, and generalization.

Conclusion: The method provides interpretable reasoning and accurate recognition, addressing the black-box issue in SAR ATR.

Abstract: Synthetic aperture radar automatic target recognition (SAR ATR) has seen
significant performance improvements with deep learning. However, the black-box
nature of deep SAR ATR introduces low confidence and high risks in
decision-critical SAR applications, hindering practical deployment. To address
this issue, deep SAR ATR should provide an interpretable reasoning basis $r_b$
and logic $\lambda_w$, forming the reasoning logic $\sum_{i} {{r_b^i} \times
{\lambda_w^i}} =pred$ behind the decisions. Therefore, this paper proposes a
physics-based two-stage feature decomposition method for interpretable deep SAR
ATR, which transforms uninterpretable deep features into attribute scattering
center components (ASCC) with clear physical meanings. First, ASCCs are
obtained through a clustering algorithm. To extract independent physical
components from deep features, we propose a two-stage decomposition method. In
the first stage, a feature decoupling and discrimination module separates deep
features into approximate ASCCs with global discriminability. In the second
stage, a multilayer orthogonal non-negative matrix tri-factorization (MLO-NMTF)
further decomposes the ASCCs into independent components with distinct physical
meanings. The MLO-NMTF elegantly aligns with the clustering algorithms to
obtain ASCCs. Finally, this method ensures both an interpretable reasoning
process and accurate recognition results. Extensive experiments on four
benchmark datasets confirm its effectiveness, showcasing the method's
interpretability, robust recognition performance, and strong generalization
capability.

</details>


### [599] [A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma](https://arxiv.org/pdf/2506.09661)
*Garima Jain, Sanghamitra Pati, Mona Duggal, Amit Sethi, Abhijeet Patil, Gururaj Malekar, Nilesh Kowe, Jitender Kumar, Jatin Kashyap, Divyajeet Rout, Deepali, Hitesh, Nishi Halduniya, Sharat Kumar, Heena Tabassum, Rupinder Singh Dhaliwal, Sucheta Devi Khuraijam, Sushma Khuraijam, Sharmila Laishram, Simmi Kharb, Sunita Singh, K. Swaminadtan, Ranjana Solanki, Deepika Hemranjani, Shashank Nath Singh, Uma Handa, Manveen Kaur, Surinder Singhal, Shivani Kalhan, Rakesh Kumar Gupta, Ravi. S, D. Pavithra, Sunil Kumar Mahto, Arvind Kumar, Deepali Tirkey, Saurav Banerjee, L. Sreelakshmi*

Main category: eess.IV

TL;DR: A large, multicenter oral cytology dataset is introduced to improve AI-driven early detection of oral squamous cell carcinoma (OSCC), addressing challenges in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Early detection of OSCC improves survival, but traditional histopathology is invasive and resource-intensive. Oral cytology offers a cheaper, less invasive alternative, but needs AI to overcome variability and lack of experts.

Method: A labeled, multi-source dataset of oral cytology slides (PAP and MGG stains) from ten Indian medical centers was created for AI training.

Result: The dataset enables robust AI models for cellular anomaly detection, aiming to reduce diagnostic errors and improve early OSCC diagnosis.

Conclusion: This resource fills a gap in public datasets, potentially lowering mortality and improving outcomes in resource-limited areas.

Abstract: Oral squamous cell carcinoma OSCC is a major global health burden,
particularly in several regions across Asia, Africa, and South America, where
it accounts for a significant proportion of cancer cases. Early detection
dramatically improves outcomes, with stage I cancers achieving up to 90 percent
survival. However, traditional diagnosis based on histopathology has limited
accessibility in low-resource settings because it is invasive,
resource-intensive, and reliant on expert pathologists. On the other hand, oral
cytology of brush biopsy offers a minimally invasive and lower cost
alternative, provided that the remaining challenges, inter observer variability
and unavailability of expert pathologists can be addressed using artificial
intelligence. Development and validation of robust AI solutions requires access
to large, labeled, and multi-source datasets to train high capacity models that
generalize across domain shifts. We introduce the first large and multicenter
oral cytology dataset, comprising annotated slides stained with
Papanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten
tertiary medical centers in India. The dataset is labeled and annotated by
expert pathologists for cellular anomaly classification and detection, is
designed to advance AI driven diagnostic methods. By filling the gap in
publicly available oral cytology datasets, this resource aims to enhance
automated detection, reduce diagnostic errors, and improve early OSCC diagnosis
in resource-constrained settings, ultimately contributing to reduced mortality
and better patient outcomes worldwide.

</details>


### [600] [Sampling Theory for Super-Resolution with Implicit Neural Representations](https://arxiv.org/pdf/2506.09949)
*Mahrokh Najaf, Gregory Ongie*

Main category: eess.IV

TL;DR: The paper explores the sample complexity of estimating images using implicit neural representations (INRs) for solving linear inverse problems, focusing on recovery from low-pass Fourier samples.


<details>
  <summary>Details</summary>
Motivation: To understand the sampling requirements for image recovery using INRs, which are less studied compared to traditional pixel representations.

Method: Uses a single hidden-layer INR with ReLU activation and Fourier features, employing generalized weight decay regularization. Links non-convex optimization to convex penalties in infinite-dimensional spaces.

Result: Identifies sufficient Fourier samples for exact INR-based image recovery and validates theory with empirical tests on low-width INRs and super-resolution tasks.

Conclusion: INRs can achieve exact image recovery under certain conditions, demonstrating their potential for inverse problems in imaging.

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for
solving inverse problems in computer vision and computational imaging. INRs
represent images as continuous domain functions realized by a neural network
taking spatial coordinates as inputs. However, unlike traditional pixel
representations, little is known about the sample complexity of estimating
images using INRs in the context of linear inverse problems. Towards this end,
we study the sampling requirements for recovery of a continuous domain image
from its low-pass Fourier samples by fitting a single hidden-layer INR with
ReLU activation and a Fourier features layer using a generalized form of weight
decay regularization. Our key insight is to relate minimizers of this
non-convex parameter space optimization problem to minimizers of a convex
penalty defined over an infinite-dimensional space of measures. We identify a
sufficient number of Fourier samples for which an image realized by an INR is
exactly recoverable by solving the INR training problem. To validate our
theory, we empirically assess the probability of achieving exact recovery of
images realized by low-width single hidden-layer INRs, and illustrate the
performance of INRs on super-resolution recovery of continuous domain phantom
images.

</details>


### [601] [Coil2Coil: Self-supervised MR image denoising using phased-array coil images](https://arxiv.org/pdf/2208.07552)
*Juhyung Park, Dongwon Park, Sooyeon Ji, Hyeong-Geol Shin, Se Young Chun, Jongho Lee*

Main category: eess.IV

TL;DR: Proposes Coil2Coil (C2C), a self-supervised MRI denoising method using multichannel coil data, eliminating the need for clean or paired training images.


<details>
  <summary>Details</summary>
Motivation: Supervised denoising methods require costly clean image pairs; C2C avoids this by leveraging multichannel data.

Method: Divides multichannel coil images into input-label pairs, processes for noise independence, and trains using Noise2Noise principles.

Result: Outperforms other self-supervised methods, matches supervised results, and denoises real DICOM images effectively.

Conclusion: C2C offers a practical, no-additional-scan solution for clinical MRI denoising.

Abstract: Denoising of magnetic resonance images is beneficial in improving the quality
of low signal-to-noise ratio images. Recently, denoising using deep neural
networks has demonstrated promising results. Most of these networks, however,
utilize supervised learning, which requires large training images of
noise-corrupted and clean image pairs. Obtaining training images, particularly
clean images, is expensive and time-consuming. Hence, methods such as
Noise2Noise (N2N) that require only pairs of noise-corrupted images have been
developed to reduce the burden of obtaining training datasets. In this study,
we propose a new self-supervised denoising method, Coil2Coil (C2C), that does
not require the acquisition of clean images or paired noise-corrupted images
for training. Instead, the method utilizes multichannel data from phased-array
coils to generate training images. First, it divides and combines multichannel
coil images into two images, one for input and the other for label. Then, they
are processed to impose noise independence and sensitivity normalization such
that they can be used for the training images of N2N. For inference, the method
inputs a coil-combined image (e.g., DICOM image), enabling a wide application
of the method. When evaluated using synthetic noise-added images, C2C shows the
best performance against several self-supervised methods, reporting comparable
outcomes to supervised methods. When testing the DICOM images, C2C successfully
denoised real noise without showing structure-dependent residuals in the error
maps. Because of the significant advantage of not requiring additional scans
for clean or paired images, the method can be easily utilized for various
clinical applications.

</details>


### [602] [Plug-and-Play image restoration with Stochastic deNOising REgularization](https://arxiv.org/pdf/2402.01779)
*Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis*

Main category: eess.IV

TL;DR: SNORE introduces a PnP framework using stochastic regularization, applying denoisers only on adequately noisy images, outperforming state-of-the-art methods in deblurring and inpainting.


<details>
  <summary>Details</summary>
Motivation: Address the non-standard use of denoisers in PnP algorithms by aligning with DM principles, ensuring denoisers operate on correctly noised images.

Method: Proposes SNORE, a stochastic regularization framework, using stochastic gradient descent for inverse problems, with convergence analysis.

Result: Competes with state-of-the-art methods in deblurring and inpainting, both quantitatively and qualitatively.

Conclusion: SNORE effectively bridges PnP and DM principles, offering a robust solution for image restoration tasks.

Abstract: Plug-and-Play (PnP) algorithms are a class of iterative algorithms that
address image inverse problems by combining a physical model and a deep neural
network for regularization. Even if they produce impressive image restoration
results, these algorithms rely on a non-standard use of a denoiser on images
that are less and less noisy along the iterations, which contrasts with recent
algorithms based on Diffusion Models (DM), where the denoiser is applied only
on re-noised images. We propose a new PnP framework, called Stochastic
deNOising REgularization (SNORE), which applies the denoiser only on images
with noise of the adequate level. It is based on an explicit stochastic
regularization, which leads to a stochastic gradient descent algorithm to solve
ill-posed inverse problems. A convergence analysis of this algorithm and its
annealing extension is provided. Experimentally, we prove that SNORE is
competitive with respect to state-of-the-art methods on deblurring and
inpainting tasks, both quantitatively and qualitatively.

</details>


### [603] [Towards a Sampling Theory for Implicit Neural Representations](https://arxiv.org/pdf/2405.18410)
*Mahrokh Najaf, Gregory Ongie*

Main category: eess.IV

TL;DR: The paper explores the sample complexity of estimating images using implicit neural representations (INRs) in linear inverse problems, focusing on recovery from low-pass Fourier coefficients with a specific INR architecture.


<details>
  <summary>Details</summary>
Motivation: Little is known about the sample complexity of INRs for image estimation in inverse problems, motivating a study of their recovery capabilities.

Method: The study uses a single hidden-layer INR with ReLU activation and Fourier features, employing generalized weight decay regularization to relate non-convex optimization to convex penalty minimizers.

Result: The paper identifies sufficient sample conditions for exact recovery of images realized by width-1 INRs and conjectures for general width-$W$ cases, supported by empirical validation.

Conclusion: INRs show promise for exact recovery in inverse problems, with theoretical and empirical evidence supporting their effectiveness, particularly in super-resolution tasks.

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for
solving inverse problems in computer vision and computational imaging. INRs
represent images as continuous domain functions realized by a neural network
taking spatial coordinates as inputs. However, unlike traditional pixel
representations, little is known about the sample complexity of estimating
images using INRs in the context of linear inverse problems. Towards this end,
we study the sampling requirements for recovery of a continuous domain image
from its low-pass Fourier coefficients by fitting a single hidden-layer INR
with ReLU activation and a Fourier features layer using a generalized form of
weight decay regularization. Our key insight is to relate minimizers of this
non-convex parameter space optimization problem to minimizers of a convex
penalty defined over an infinite-dimensional space of measures. We identify a
sufficient number of samples for which an image realized by a width-1 INR is
exactly recoverable by solving the INR training problem, and give a conjecture
for the general width-$W$ case. To validate our theory, we empirically assess
the probability of achieving exact recovery of images realized by low-width
single hidden-layer INRs, and illustrate the performance of INR on
super-resolution recovery of more realistic continuous domain phantom images.

</details>


### [604] [UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation](https://arxiv.org/pdf/2408.00273)
*Yanbing Chen, Tianze Tang, Taehyo Kim, Hai Shu*

Main category: eess.IV

TL;DR: UKAN-EP, a 3D extension of U-KAN, improves glioma segmentation in multi-modal MRI by integrating ECA and PFA modules and a dynamic loss strategy, outperforming baselines with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Glioma segmentation is challenging due to tumor heterogeneity and multi-modal MRI variability, requiring efficient and accurate automated methods.

Method: UKAN-EP extends U-KAN with ECA and PFA for better feature fusion and multi-scale representation, plus dynamic loss weighting.

Result: UKAN-EP outperforms U-Net, Attention U-Net, and Swin UNETR on the BraTS-GLI dataset with lower computational costs.

Conclusion: UKAN-EP is effective for glioma segmentation, with ECA and PFA proving crucial, while self-attention alternatives show limited utility.

Abstract: Gliomas are among the most common malignant brain tumors and are
characterized by considerable heterogeneity, which complicates accurate
detection and segmentation. Multi-modal MRI is the clinical standard for glioma
imaging, but variability across modalities and high computational complexity
hinder effective automated segmentation. In this paper, we propose UKAN-EP, a
novel 3D extension of the original 2D U-KAN model for multi-modal MRI brain
tumor segmentation. While U-KAN integrates Kolmogorov-Arnold Network (KAN)
layers into a U-Net backbone, UKAN-EP further incorporates Efficient Channel
Attention (ECA) and Pyramid Feature Aggregation (PFA) modules to enhance
inter-modality feature fusion and multi-scale feature representation. We also
introduce a dynamic loss weighting strategy that adaptively balances the
Cross-Entropy and Dice losses during training. We evaluate UKAN-EP on the 2024
BraTS-GLI dataset and compare it against strong baselines including U-Net,
Attention U-Net, and Swin UNETR. Results show that UKAN-EP achieves superior
segmentation performance while requiring substantially fewer computational
resources. An extensive ablation study further demonstrates the effectiveness
of ECA and PFA, as well as the limited utility of self-attention and spatial
attention alternatives. Code is available at
https://github.com/TianzeTang0504/UKAN-EP.

</details>


### [605] [NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with Extremely Sparse-views](https://arxiv.org/pdf/2408.16355)
*Kirsten W. H. Maas, Danny Ruijters, Anna Vilanova, Nicola Pezzotti*

Main category: eess.IV

TL;DR: NeRF-CA introduces a method for 4D reconstruction of coronary arteries from sparse X-ray angiograms, addressing challenges like sparse views and motion by decoupling static and moving components.


<details>
  <summary>Details</summary>
Motivation: Dynamic 4D reconstruction from 2D X-ray coronary angiography is clinically significant but hindered by sparse views, motion, and vessel complexity. Existing methods require extensive interaction or large datasets.

Method: NeRF-CA decouples the scene into static background and moving coronary arteries, leveraging Neural Radiance Fields (NeRF) for reconstruction from sparse angiograms.

Result: NeRF-CA achieves adequate 4D reconstructions from as few as four angiograms, outperforming state-of-the-art sparse-view X-ray NeRF methods.

Conclusion: NeRF-CA is a promising step toward fully automatic 4D CA reconstruction, validated by phantom datasets and ablation studies, with code made publicly available.

Abstract: Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray
coronary angiography (CA) remains a significant clinical problem. Existing CA
reconstruction methods often require extensive user interaction or large
training datasets. Recently, Neural Radiance Field (NeRF) has successfully
reconstructed high-fidelity scenes in natural and medical contexts without
these requirements. However, challenges such as sparse-views, intra-scan
motion, and complex vessel morphology hinder its direct application to CA data.
We introduce NeRF-CA, a first step toward a fully automatic 4D CA
reconstruction that achieves reconstructions from sparse coronary angiograms.
To the best of our knowledge, we are the first to address the challenges of
sparse-views and cardiac motion by decoupling the scene into the moving
coronary artery and the static background, effectively translating the problem
of motion into a strength. NeRF-CA serves as a first stepping stone for solving
the 4D CA reconstruction problem, achieving adequate 4D reconstructions from as
few as four angiograms, as required by clinical practice, while significantly
outperforming state-of-the-art sparse-view X-ray NeRF. We validate our approach
quantitatively and qualitatively using representative 4D phantom datasets and
ablation studies. To accelerate research in this domain, we made our codebase
public: https://github.com/kirstenmaas/NeRF-CA.

</details>


### [606] [The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound](https://arxiv.org/pdf/2504.07904)
*Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield*

Main category: eess.IV

TL;DR: The study evaluates data augmentation and preprocessing strategies for self-supervised learning in lung ultrasound, finding that semantic-preserving methods excel in COVID-19 classification, while cropping-based methods work best for local tasks like B-line detection.


<details>
  <summary>Details</summary>
Motivation: To determine effective data augmentation and preprocessing strategies for self-supervised learning in medical imaging, specifically lung ultrasound, where natural image methods may not suffice.

Method: Three augmentation pipelines were tested: a baseline, a novel semantic-preserving pipeline for ultrasound, and a distilled set of effective transformations. Pretrained models were evaluated on B-line detection, pleural effusion detection, and COVID-19 classification.

Result: Semantic-preserving augmentation performed best for COVID-19 classification, while cropping-based methods excelled in local tasks like B-line detection. Preprocessing improved performance across tasks.

Conclusion: The study provides practical guidance for data augmentation and preprocessing in self-supervised learning for ultrasound, highlighting task-specific strategies.

Abstract: Data augmentation is a central component of joint embedding self-supervised
learning (SSL). Approaches that work for natural images may not always be
effective in medical imaging tasks. This study systematically investigated the
impact of data augmentation and preprocessing strategies in SSL for lung
ultrasound. Three data augmentation pipelines were assessed: (1) a baseline
pipeline commonly used across imaging domains, (2) a novel semantic-preserving
pipeline designed for ultrasound, and (3) a distilled set of the most effective
transformations from both pipelines. Pretrained models were evaluated on
multiple classification tasks: B-line detection, pleural effusion detection,
and COVID-19 classification. Experiments revealed that semantics-preserving
data augmentation resulted in the greatest performance for COVID-19
classification - a diagnostic task requiring global image context.
Cropping-based methods yielded the greatest performance on the B-line and
pleural effusion object classification tasks, which require strong local
pattern recognition. Lastly, semantics-preserving ultrasound image
preprocessing resulted in increased downstream performance for multiple tasks.
Guidance regarding data augmentation and preprocessing strategies was
synthesized for practitioners working with SSL in ultrasound.

</details>


### [607] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/pdf/2505.22685)
*Marcus J. Vroemen, Yuqian Chen, Yui Lo, Tengfei Xue, Weidong Cai, Fan Zhang, Josien P. W. Pluim, Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: DeepMultiConnectome is a deep-learning model that predicts structural connectomes directly from tractography, bypassing gray matter parcellation and supporting multiple schemes. It is fast, scalable, and highly correlated with traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional connectome generation is time-consuming and requires gray matter parcellation, posing challenges for large-scale studies.

Method: Uses a point-cloud-based neural network with multi-task learning to classify streamlines across two parcellation schemes, sharing a learned representation.

Result: Predicts connectomes in ~40 seconds with high correlation to traditional methods (r=0.992 for 84-region, r=0.986 for 164-region) and preserves network properties.

Conclusion: DeepMultiConnectome offers a scalable, fast solution for generating subject-specific connectomes across multiple parcellation schemes.

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>
