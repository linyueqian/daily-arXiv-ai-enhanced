{"id": "2507.00081", "pdf": "https://arxiv.org/pdf/2507.00081", "abs": "https://arxiv.org/abs/2507.00081", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "AI": {"tldr": "SciBORG is a modular framework for LLM-based agents to autonomously execute complex scientific tasks, leveraging FSA memory and dynamic agent construction for reliability and adaptability.", "motivation": "Address limitations of LLMs in real-world scientific workflows, such as memory, planning, and tool integration, by creating autonomous, context-aware agents.", "method": "Dynamic agent construction from source code docs, augmented with FSA memory for state tracking and context-aware decisions, eliminating manual prompt engineering.", "result": "Reliable execution, adaptive planning, and interpretable state transitions in tasks like bioassay retrieval and hardware integration.", "conclusion": "Memory and state awareness are key for agentic planning, providing a scalable foundation for AI agents in complex environments."}}
{"id": "2507.00491", "pdf": "https://arxiv.org/pdf/2507.00491", "abs": "https://arxiv.org/abs/2507.00491", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "AI": {"tldr": "Twill is a runtime framework for scheduling Compound AI (cAI) systems on mobile edge platforms, reducing latency by 54% while staying within power budgets.", "motivation": "Deploying cAI systems on mobile edge platforms is challenging due to dynamic workloads and the need for concurrent DNN-transformer inference, which existing methods cannot handle.", "method": "Twill uses task affinity-aware cluster mapping, priority-aware task freezing/unfreezing, and DVFS to manage concurrent cAI workloads.", "result": "Twill reduces inference latency by 54% on average compared to state-of-the-art techniques, while adhering to power constraints.", "conclusion": "Twill effectively addresses the scheduling challenges of cAI systems on heterogeneous mobile edge platforms."}}
{"id": "2507.00914", "pdf": "https://arxiv.org/pdf/2507.00914", "abs": "https://arxiv.org/abs/2507.00914", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "AI": {"tldr": "The paper explores Urban LLM Agents, LLM-powered agents for urban decision-making, covering their capabilities, workflows, applications, and challenges.", "motivation": "To leverage LLMs for creating efficient, livable, and sustainable urban environments by deploying them as intelligent agents in hybrid cyber-physical-social spaces.", "method": "Introduces Urban LLM Agents, surveys their workflows (sensing, memory, reasoning, execution, learning), and categorizes applications (urban planning, transportation, environment, public safety, urban society).", "result": "Establishes a foundation for Urban LLM Agents, providing a roadmap for integrating LLMs with urban intelligence.", "conclusion": "Identifies trustworthiness and evaluation challenges, highlights open problems, and aims to advance the field of urban LLM agents."}}
{"id": "2507.00195", "pdf": "https://arxiv.org/pdf/2507.00195", "abs": "https://arxiv.org/abs/2507.00195", "authors": ["Kumar Kshitij Patel"], "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "AI": {"tldr": "The thesis explores Local SGD in distributed/federated optimization under data heterogeneity, showing bounded second-order heterogeneity is key for outperforming centralized methods. It provides tight bounds, a consensus-error analysis framework, and extends to online federated learning.", "motivation": "To understand when and why local update algorithms like Local SGD outperform centralized methods in heterogeneous data settings.", "method": "Uses a fine-grained consensus-error-based analysis framework under third-order smoothness and relaxed heterogeneity assumptions. Establishes tight bounds for various algorithms.", "result": "Shows bounded second-order heterogeneity is necessary and sufficient for local updates to excel. Provides min-max complexity for problem classes and regret bounds for online federated learning.", "conclusion": "Local updates offer provable advantages under specific heterogeneity conditions, with the thesis serving as a guide for analyzing Local SGD in such environments."}}
{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "The paper explores LLMs' efficiency in processing tabular data, comparing text-based and multimodal models across domains and modalities, and introduces the TableEval benchmark.", "motivation": "To assess LLMs' effectiveness in table understanding tasks, particularly across scientific and non-scientific contexts and different table representations.", "method": "Cross-domain and cross-modality evaluation of LLMs, including interpretability analysis, using the TableEval benchmark with 3017 tables in five formats.", "result": "LLMs show robustness across table modalities but struggle with scientific tables.", "conclusion": "The study highlights LLMs' limitations in handling scientific tabular data and introduces a benchmark for future research."}}
{"id": "2507.00155", "pdf": "https://arxiv.org/pdf/2507.00155", "abs": "https://arxiv.org/abs/2507.00155", "authors": ["Richa Namballa", "Agnieszka Roginska", "Magdalena Fuentes"], "title": "Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "6 pages + references, 4 figures, 2 tables, 26th International Society\n  for Music Information Retrieval (ISMIR) Conference", "summary": "Binaural audio remains underexplored within the music information retrieval\ncommunity. Motivated by the rising popularity of virtual and augmented reality\nexperiences as well as potential applications to accessibility, we investigate\nhow well existing music source separation (MSS) models perform on binaural\naudio. Although these models process two-channel inputs, it is unclear how\neffectively they retain spatial information. In this work, we evaluate how\nseveral popular MSS models preserve spatial information on both standard stereo\nand novel binaural datasets. Our binaural data is synthesized using stems from\nMUSDB18-HQ and open-source head-related transfer functions by positioning\ninstrument sources randomly along the horizontal plane. We then assess the\nspatial quality of the separated stems using signal processing and interaural\ncue-based metrics. Our results show that stereo MSS models fail to preserve the\nspatial information critical for maintaining the immersive quality of binaural\naudio, and that the degradation depends on model architecture as well as the\ntarget instrument. Finally, we highlight valuable opportunities for future work\nat the intersection of MSS and immersive audio.", "AI": {"tldr": "Evaluation of music source separation models on binaural audio reveals poor spatial information retention, with degradation varying by model and instrument.", "motivation": "The rise of VR/AR and accessibility needs drives exploration of binaural audio in music information retrieval, focusing on spatial information preservation.", "method": "Assessed spatial quality of MSS models on stereo and synthesized binaural datasets using signal processing and interaural cue metrics.", "result": "Stereo MSS models fail to preserve spatial information, with degradation influenced by model architecture and target instrument.", "conclusion": "Highlights opportunities for future work in MSS and immersive audio to improve spatial retention."}}
{"id": "2507.00051", "pdf": "https://arxiv.org/pdf/2507.00051", "abs": "https://arxiv.org/abs/2507.00051", "authors": ["Tianliang Yao", "Zhiqiang Pei", "Yong Li", "Yixuan Yuan", "Peng Qi"], "title": "Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures", "categories": ["eess.IV", "cs.CV"], "comment": "This paper has been accepted by Advanced Intelligent Systems", "summary": "An ever-growing incorporation of AI solutions into clinical practices\nenhances the efficiency and effectiveness of healthcare services. This paper\nfocuses on guidewire tip tracking tasks during image-guided therapy for\ncardiovascular diseases, aiding physicians in improving diagnostic and\ntherapeutic quality. A novel tracking framework based on a Siamese network with\ndual attention mechanisms combines self- and cross-attention strategies for\nrobust guidewire tip tracking. This design handles visual ambiguities, tissue\ndeformations, and imaging artifacts through enhanced spatial-temporal feature\nlearning. Validation occurred on 3 randomly selected clinical digital\nsubtraction angiography (DSA) sequences from a dataset of 15 sequences,\ncovering multiple interventional scenarios. The results indicate a mean\nlocalization error of 0.421 $\\pm$ 0.138 mm, with a maximum error of 1.736 mm,\nand a mean Intersection over Union (IoU) of 0.782. The framework maintains an\naverage processing speed of 57.2 frames per second, meeting the temporal\ndemands of endovascular imaging. Further validations with robotic platforms for\nautomating diagnostics and therapies in clinical routines yielded tracking\nerrors of 0.708 $\\pm$ 0.695 mm and 0.148 $\\pm$ 0.057 mm in two distinct\nexperimental scenarios.", "AI": {"tldr": "A novel Siamese network with dual attention mechanisms improves guidewire tip tracking in cardiovascular therapy, achieving high accuracy and speed.", "motivation": "To enhance the efficiency and accuracy of guidewire tip tracking in image-guided therapy for cardiovascular diseases, addressing challenges like visual ambiguities and imaging artifacts.", "method": "A Siamese network with self- and cross-attention strategies for robust spatial-temporal feature learning, validated on clinical DSA sequences.", "result": "Mean localization error of 0.421 mm, IoU of 0.782, and processing speed of 57.2 fps. Further robotic validations showed errors of 0.708 mm and 0.148 mm.", "conclusion": "The framework is effective for real-time guidewire tracking, with potential for clinical and robotic applications."}}
{"id": "2507.00926", "pdf": "https://arxiv.org/pdf/2507.00926", "abs": "https://arxiv.org/abs/2507.00926", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction", "categories": ["cs.MM", "cs.LG"], "comment": null, "summary": "Social media popularity prediction plays a crucial role in content\noptimization, marketing strategies, and user engagement enhancement across\ndigital platforms. However, predicting post popularity remains challenging due\nto the complex interplay between visual, textual, temporal, and user behavioral\nfactors. This paper presents HyperFusion, a hierarchical multimodal ensemble\nlearning framework for social media popularity prediction. Our approach employs\na three-tier fusion architecture that progressively integrates features across\nabstraction levels: visual representations from CLIP encoders, textual\nembeddings from transformer models, and temporal-spatial metadata with user\ncharacteristics. The framework implements a hierarchical ensemble strategy\ncombining CatBoost, TabNet, and custom multi-layer perceptrons. To address\nlimited labeled data, we propose a two-stage training methodology with\npseudo-labeling and iterative refinement. We introduce novel cross-modal\nsimilarity measures and hierarchical clustering features that capture\ninter-modal dependencies. Experimental results demonstrate that HyperFusion\nachieves competitive performance on the SMP challenge dataset. Our team\nachieved third place in the SMP Challenge 2025 (Image Track). The source code\nis available at https://anonymous.4open.science/r/SMPDImage.", "AI": {"tldr": "HyperFusion is a hierarchical multimodal ensemble learning framework for predicting social media post popularity, integrating visual, textual, temporal, and user features. It achieved competitive results in the SMP Challenge 2025.", "motivation": "Predicting social media post popularity is challenging due to complex multimodal interactions, motivating the need for advanced frameworks like HyperFusion.", "method": "HyperFusion uses a three-tier fusion architecture with CLIP encoders, transformer models, and metadata. It combines CatBoost, TabNet, and MLPs, with pseudo-labeling for limited data.", "result": "The framework achieved competitive performance, securing third place in the SMP Challenge 2025 (Image Track).", "conclusion": "HyperFusion effectively addresses the challenges of popularity prediction through hierarchical multimodal fusion and ensemble learning."}}
{"id": "2507.00033", "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "Proposes 'moment sampling' for Video LLMs to improve long-form VideoQA by selecting relevant frames using a text-to-video moment retrieval model.", "motivation": "Existing frame sub-sampling methods for Video LLMs lose key frames or include redundant ones, impairing long-range reasoning in VideoQA.", "method": "Uses a lightweight moment retrieval model to guide frame selection, focusing on question-relevant frames.", "result": "Enhances long-form VideoQA performance across four datasets and four Video LLMs.", "conclusion": "Moment sampling is effective for improving Video LLMs' accuracy and efficiency in long-form VideoQA."}}
{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "AI": {"tldr": "DiMo-GUI is a training-free framework for GUI grounding that uses dynamic visual grounding and modality-aware optimization to improve accuracy by focusing on textual and iconic elements separately and refining ambiguous predictions hierarchically.", "motivation": "Grounding natural language queries in GUIs is challenging due to visual diversity, spatial clutter, and language ambiguity. Existing methods often treat GUIs as monolithic images, limiting performance.", "method": "DiMo-GUI splits GUIs into textual and iconic elements, processes each modality independently using vision-language models, and refines ambiguous predictions by dynamically focusing on candidate regions.", "result": "The framework outperforms baseline methods on standard GUI grounding benchmarks, showing improved accuracy without additional training or annotations.", "conclusion": "DiMo-GUI effectively combines modality separation and hierarchical refinement to address GUI grounding challenges, offering a practical and scalable solution."}}
{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "HDRAM introduces a symbolic memory framework for LLMs, addressing precision loss as information spreading, improving associative retrieval using CHQ principles.", "motivation": "LLMs suffer from precision loss, reframed as information spreading, which is addressed by treating transformer latent space as a spread-spectrum channel.", "method": "HDRAM uses hypertokens, integrating ECC, holographic computing, and quantum-inspired search, enabling efficient key-value operations and Grover-style search.", "result": "HDRAM significantly improves associative retrieval without architectural changes.", "conclusion": "CHQ principles can fortify transformer architectures, demonstrating the effectiveness of HDRAM."}}
{"id": "2507.00229", "pdf": "https://arxiv.org/pdf/2507.00229", "abs": "https://arxiv.org/abs/2507.00229", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Rashedul Hasan", "Taieba Athay", "Nursad Mamun", "Anomadarshi Barua"], "title": "A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Speech super-resolution (SSR) enhances low-resolution speech by increasing\nthe sampling rate. While most SSR methods focus on magnitude reconstruction,\nrecent research highlights the importance of phase reconstruction for improved\nperceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency\nTransformation Network that reconstructs both magnitude and phase in complex\ndomains for improved SSR tasks. It incorporates a complex global attention\nblock to model inter-phoneme and inter-frequency dependencies and a complex\nconformer to capture long-range and local features, improving frequency\nreconstruction and noise robustness. CTFT-Net employs time-domain and\nmulti-resolution frequency-domain loss functions for better generalization.\nExperiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,\nWSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling\n(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy\nartifacts.", "AI": {"tldr": "CTFT-Net is a novel speech super-resolution method that reconstructs both magnitude and phase in complex domains, outperforming state-of-the-art models, especially for extreme upsampling.", "motivation": "Most SSR methods neglect phase reconstruction, which is crucial for perceptual quality. CTFT-Net addresses this gap.", "method": "CTFT-Net uses a Complex Time-Frequency Transformation Network with complex global attention and conformer blocks, along with time-domain and multi-resolution frequency-domain loss functions.", "result": "CTFT-Net excels in extreme upsampling (2 kHz to 48 kHz) on the VCTK dataset, outperforming models like NU-Wave and WSRGlow.", "conclusion": "CTFT-Net effectively reconstructs high frequencies without noisy artifacts, setting a new benchmark for SSR tasks."}}
{"id": "2507.00443", "pdf": "https://arxiv.org/pdf/2507.00443", "abs": "https://arxiv.org/abs/2507.00443", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "AI": {"tldr": "A nature-inspired collision-free formation control for multi-UAV systems, combining centralized guidance and distributed control for obstacle avoidance in 2D and 3D environments.", "motivation": "Address the rising demand for multi-UAV systems in urban areas with static and dynamic obstacles, inspired by collective behavior of tilapia fish and pigeons.", "method": "Semi-distributed control approach: centralized guidance for optimal UAV positioning (probabilistic Lloyd's algorithm) and distributed control for collision/obstacle avoidance. Extended to 3D with novel maneuvers.", "result": "Validated in 2D and 3D scenarios, demonstrating effectiveness in dynamic environments with stationary and moving obstacles.", "conclusion": "The framework successfully achieves collision-free formation control for multi-UAV systems in complex environments."}}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "Prompting is a crucial and scientifically valid method for studying and controlling large language models (LLMs), akin to behavioral science.", "motivation": "The paper aims to legitimize prompting as a scientific method for understanding and interacting with LLMs, countering its perception as unreliable or unscientific.", "method": "The authors argue for treating prompting as behavioral science, comparing it to mechanistic interpretability but focusing on language as the native interface for LLMs.", "result": "The paper asserts that prompting is not inferior but essential for unlocking and studying LLM capabilities.", "conclusion": "Prompting should be recognized as a fundamental component of LLM science, not dismissed as alchemy."}}
{"id": "2507.00227", "pdf": "https://arxiv.org/pdf/2507.00227", "abs": "https://arxiv.org/abs/2507.00227", "authors": ["Paul Mayer", "Florian Lux", "Alejandro P\u00e9rez-Gonz\u00e1lez-de-Martos", "Angelina Elizarova", "Lindsey Vanderlyn", "Dirk V\u00e4th", "Ngoc Thang Vu"], "title": "Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "While generative methods have progressed rapidly in recent years, generating\nexpressive prosody for an utterance remains a challenging task in\ntext-to-speech synthesis. This is particularly true for systems that model\nprosody explicitly through parameters such as pitch, energy, and duration,\nwhich is commonly done for the sake of interpretability and controllability. In\nthis work, we investigate the effectiveness of stochastic methods for this\ntask, including Normalizing Flows, Conditional Flow Matching, and Rectified\nFlows. We compare these methods to a traditional deterministic baseline, as\nwell as to real human realizations. Our extensive subjective and objective\nevaluations demonstrate that stochastic methods produce natural prosody on par\nwith human speakers by capturing the variability inherent in human speech.\nFurther, they open up additional controllability options by allowing the\nsampling temperature to be tuned.", "AI": {"tldr": "Stochastic methods like Normalizing Flows outperform deterministic baselines in generating expressive prosody for text-to-speech, matching human variability and offering controllability.", "motivation": "Generating expressive prosody in text-to-speech synthesis is challenging, especially for systems using explicit parameters like pitch, energy, and duration.", "method": "Investigated stochastic methods (Normalizing Flows, Conditional Flow Matching, Rectified Flows) against deterministic baselines and human realizations.", "result": "Stochastic methods produce prosody as natural as humans, capturing speech variability, and allow tuning sampling temperature for controllability.", "conclusion": "Stochastic methods are effective for prosody generation, offering human-like variability and enhanced controllability."}}
{"id": "2507.00185", "pdf": "https://arxiv.org/pdf/2507.00185", "abs": "https://arxiv.org/abs/2507.00185", "authors": ["Yang Zhou", "Chrystie Wan Ning Quek", "Jun Zhou", "Yan Wang", "Yang Bai", "Yuhe Ke", "Jie Yao", "Laura Gutierrez", "Zhen Ling Teo", "Darren Shu Jeng Ting", "Brian T. Soetikno", "Christopher S. Nielsen", "Tobias Elze", "Zengxiang Li", "Linh Le Dinh", "Lionel Tim-Ee Cheng", "Tran Nguyen Tuan Anh", "Chee Leong Cheng", "Tien Yin Wong", "Nan Liu", "Iain Beehuat Tan", "Tony Kiat Hon Lim", "Rick Siow Mong Goh", "Yong Liu", "Daniel Shu Wei Ting"], "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages, 3 composite figures, 4 tables", "summary": "Current artificial intelligence models for medical imaging are predominantly\nsingle modality and single disease. Attempts to create multimodal and\nmulti-disease models have resulted in inconsistent clinical accuracy.\nFurthermore, training these models typically requires large, labour-intensive,\nwell-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,\nmulti-specialty foundation model trained using self-supervised learning and a\nmemory module. MerMED-FM was trained on 3.3 million medical images from over\nten specialties and seven modalities, including computed tomography (CT), chest\nX-rays (CXR), ultrasound (US), pathology patches, color fundus photography\n(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was\nevaluated across multiple diseases and compared against existing foundational\nmodels. Strong performance was achieved across all modalities, with AUROCs of\n0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894\n(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,\nversatile, cross-specialty foundation model that enables robust medical imaging\ninterpretation across diverse medical disciplines.", "AI": {"tldr": "MerMED-FM is a multimodal, multi-specialty foundation model for medical imaging, trained on 3.3M images across 10 specialties and 7 modalities, achieving strong performance with AUROCs ranging from 0.858 to 0.988.", "motivation": "Current AI models for medical imaging are limited to single modality and single disease, with inconsistent accuracy and reliance on large labeled datasets.", "method": "Developed MerMED-FM using self-supervised learning and a memory module, trained on diverse medical images.", "result": "Achieved high AUROCs across modalities (e.g., 0.988 for OCT, 0.858 for CXR), outperforming existing models.", "conclusion": "MerMED-FM is a versatile, adaptable foundation model for robust medical imaging interpretation across specialties."}}
{"id": "2507.00055", "pdf": "https://arxiv.org/pdf/2507.00055", "abs": "https://arxiv.org/abs/2507.00055", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "AI": {"tldr": "LiSER is a knowledge distillation framework for Speech Emotion Recognition (SER) that uses unlabeled audio-visual data and teacher models to train lightweight student models, reducing reliance on labeled datasets.", "motivation": "Voice interfaces can enhance user interaction by recognizing emotions, but labeled data for SER is costly to collect. Multi-modal (audio-visual) cues improve SER, but data scarcity is a challenge.", "method": "Proposes LiSER, a framework using knowledge distillation. Large teacher models (trained on speech and face representations) transfer knowledge to lightweight student models via unlabeled audio-visual data.", "result": "Tested on RAVDESS and CREMA-D datasets, LiSER reduces the need for extensive labeled data while maintaining SER performance.", "conclusion": "LiSER offers a cost-effective solution for SER by leveraging unlabeled data and knowledge distillation, making it practical for real-world applications."}}
{"id": "2507.00042", "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "ER-EMU, an edge model update algorithm, uses adaptive experience replay and a domain distance metric to mitigate catastrophic forgetting in dynamic traffic monitoring, improving object detection performance.", "motivation": "Catastrophic forgetting in cloud-edge collaborative object detection for traffic monitoring hinders model adaptation to dynamic environments with periodic variations (e.g., day/night). Existing methods fail to efficiently prioritize historical data.", "method": "ER-EMU combines a FIFO-managed experience buffer with a Domain Distance Metric-based Experience Selection (DDM-ES) algorithm using MK-MMD to select dissimilar historical data, ensuring diverse training and knowledge retention.", "result": "Experiments on the Bellevue traffic video dataset show ER-EMU consistently enhances performance in cloud-edge collaborative object detection frameworks.", "conclusion": "ER-EMU effectively addresses catastrophic forgetting by intelligently leveraging historical data, improving adaptability and performance in dynamic traffic environments."}}
{"id": "2507.00041", "pdf": "https://arxiv.org/pdf/2507.00041", "abs": "https://arxiv.org/abs/2507.00041", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "AI": {"tldr": "TalentMine is an LLM-enhanced framework that improves semantic understanding of tabular data in talent management, achieving 100% accuracy in query tasks.", "motivation": "Current table extraction methods fail to preserve semantic relationships in tabular data, leading to poor performance in retrieval-augmented systems.", "method": "TalentMine uses multimodal reasoning to create semantically enriched table representations, avoiding CSV or text linearization.", "result": "TalentMine achieves 100% accuracy in query tasks, outperforming AWS Textract (0%) and AWS Textract Visual Q&A (40%).", "conclusion": "TalentMine addresses semantic loss in table extraction, offering a robust solution for talent management systems."}}
{"id": "2507.00003", "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "AI": {"tldr": "NeutroSENSE integrates neutrosophic logic with ensemble models for interpretable IoT intrusion detection, achieving 97% accuracy and using indeterminacy to flag uncertain predictions.", "motivation": "To enhance interpretability and uncertainty quantification in IoT intrusion detection, addressing the need for trustworthy AI in edge deployments.", "method": "Combines Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, decomposing predictions into truth, falsity, and indeterminacy components. Uses adaptive thresholds for review.", "result": "97% accuracy on IoT-CAD dataset; misclassified samples show higher indeterminacy (I=0.62 vs. I=0.24 for correct ones). Indeterminacy correlates with error likelihood.", "conclusion": "NeutroSENSE improves accuracy and explainability, offering a trust-aware AI framework for IoT security, especially in edge deployments."}}
{"id": "2507.00466", "pdf": "https://arxiv.org/pdf/2507.00466", "abs": "https://arxiv.org/abs/2507.00466", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "AI": {"tldr": "The paper proposes a transformer-based model for beat and downbeat tracking in MIDI, outperforming existing methods with novel preprocessing and achieving high F1-scores.", "motivation": "Existing beat tracking methods focus on audio, leaving a gap for MIDI-based approaches, which are crucial for notation-level transcription and rhythm analysis.", "method": "An end-to-end transformer model with encoder-decoder architecture, using dynamic augmentation and optimized tokenization for MIDI input to beat annotations.", "result": "The model outperforms state-of-the-art HMMs and deep learning methods, achieving competitive F1-scores across diverse datasets and musical styles.", "conclusion": "Transformers show promise for symbolic beat tracking, with potential integration into automatic music transcription for improved analysis and score generation."}}
{"id": "2507.00631", "pdf": "https://arxiv.org/pdf/2507.00631", "abs": "https://arxiv.org/abs/2507.00631", "authors": ["David Shi", "Kevin Joo"], "title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "comment": "9 pages, 1 figure", "summary": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "AI": {"tldr": "A protocol ensures correctness in AI systems by using collateralized claims and recursive verification, penalizing incorrect agents and rewarding correct ones.", "motivation": "In dynamic, low-trust environments, ensuring correctness in autonomous AI agents is challenging without upfront specification or centralized oversight.", "method": "Tasks are published as intents; solvers compete to fulfill them, with correctness verified post hoc. Challengers can stake against results to trigger verification, penalizing incorrect agents.", "result": "Correctness becomes the Nash equilibrium when incentives align across solvers, challengers, and verifiers.", "conclusion": "The protocol enforces correctness through decentralized, incentive-aligned mechanisms, making it robust in low-trust environments."}}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "Massimo Caccia", "V\u00e9ronique Eglin", "Alexandre Aussem", "J\u00e9r\u00e9my Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "The paper introduces LineRetriever, a method to optimize retrieval for adaptive planning in web navigation by prioritizing observation lines relevant to future actions, addressing context limits of large language models.", "motivation": "Current retrieval methods lose critical information for adaptive planning in web navigation due to context limits of models, necessitating a solution that retains plan-relevant details.", "method": "LineRetriever uses a language model to identify and retrieve observation lines most relevant to future navigation steps, focusing on planning horizon rather than just semantic similarity.", "result": "LineRetriever reduces observation size while maintaining performance within context limits, demonstrating effectiveness in web navigation tasks.", "conclusion": "LineRetriever offers a viable solution to optimize retrieval for adaptive planning in web navigation, addressing the limitations of current approaches."}}
{"id": "2507.00324", "pdf": "https://arxiv.org/pdf/2507.00324", "abs": "https://arxiv.org/abs/2507.00324", "authors": ["Hashim Ali", "Surya Subramani", "Raksha Varahamurthy", "Nithin Adupa", "Lekha Bollinani", "Hafiz Malik"], "title": "Collecting, Curating, and Annotating Good Quality Speech deepfake dataset for Famous Figures: Process and Challenges", "categories": ["eess.AS"], "comment": null, "summary": "Recent advances in speech synthesis have introduced unprecedented challenges\nin maintaining voice authenticity, particularly concerning public figures who\nare frequent targets of impersonation attacks. This paper presents a\ncomprehensive methodology for collecting, curating, and generating synthetic\nspeech data for political figures and a detailed analysis of challenges\nencountered. We introduce a systematic approach incorporating an automated\npipeline for collecting high-quality bonafide speech samples, featuring\ntranscription-based segmentation that significantly improves synthetic speech\nquality. We experimented with various synthesis approaches; from single-speaker\nto zero-shot synthesis, and documented the evolution of our methodology. The\nresulting dataset comprises bonafide and synthetic speech samples from ten\npublic figures, demonstrating superior quality with a NISQA-TTS naturalness\nscore of 3.69 and the highest human misclassification rate of 61.9\\%.", "AI": {"tldr": "A methodology for collecting and generating synthetic speech data for political figures, addressing challenges in voice authenticity, with high-quality results.", "motivation": "Addressing the challenge of maintaining voice authenticity for public figures amid advances in speech synthesis and impersonation risks.", "method": "Automated pipeline for collecting bonafide speech samples, transcription-based segmentation, and experimenting with single-speaker to zero-shot synthesis.", "result": "Dataset with high-quality synthetic speech (NISQA-TTS score 3.69, 61.9% human misclassification rate).", "conclusion": "The approach effectively improves synthetic speech quality and authenticity for public figures."}}
{"id": "2507.00206", "pdf": "https://arxiv.org/pdf/2507.00206", "abs": "https://arxiv.org/abs/2507.00206", "authors": ["Wenwu Tang", "Khaled Seyam", "Bin Yang"], "title": "Towards 3D Semantic Image Synthesis for Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In the medical domain, acquiring large datasets is challenging due to both\naccessibility issues and stringent privacy regulations. Consequently, data\navailability and privacy protection are major obstacles to applying machine\nlearning in medical imaging. To address this, our study proposes the Med-LSDM\n(Latent Semantic Diffusion Model), which operates directly in the 3D domain and\nleverages de-identified semantic maps to generate synthetic data as a method of\nprivacy preservation and data augmentation. Unlike many existing methods that\nfocus on generating 2D slices, Med-LSDM is designed specifically for 3D\nsemantic image synthesis, making it well-suited for applications requiring full\nvolumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D\nimage generation process by applying a diffusion model within the latent space\nof a pre-trained VQ-GAN. By operating in the compressed latent space, the model\nsignificantly reduces computational complexity while still preserving critical\n3D spatial details. Our approach demonstrates strong performance in 3D semantic\nmedical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional\nDuke Breast dataset and similar Dice scores (0.70964) to those of real images\n(0.71496). These results demonstrate that the synthetic data from our model\nhave a small domain gap with real data and are useful for data augmentation.", "AI": {"tldr": "Proposes Med-LSDM, a 3D semantic diffusion model for generating synthetic medical images, addressing data scarcity and privacy issues while maintaining spatial details.", "motivation": "Challenges in acquiring large medical datasets due to privacy and accessibility issues hinder machine learning applications in medical imaging.", "method": "Med-LSDM uses a diffusion model in the latent space of a pre-trained VQ-GAN to generate 3D synthetic data, reducing computational complexity.", "result": "Achieves a 3D-FID score of 0.0054 and Dice scores (0.70964) close to real images (0.71496), showing minimal domain gap.", "conclusion": "Med-LSDM effectively generates high-quality synthetic 3D medical images, useful for data augmentation and privacy preservation."}}
{"id": "2507.00498", "pdf": "https://arxiv.org/pdf/2507.00498", "abs": "https://arxiv.org/abs/2507.00498", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "title": "MuteSwap: Silent Face-based Voice Conversion", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "comment": null, "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "AI": {"tldr": "The paper introduces MuteSwap, a framework for Silent Face-based Voice Conversion (SFVC), enabling voice conversion using only visual inputs from silent videos and target speaker images.", "motivation": "Traditional voice conversion requires audio inputs from both source and target speakers, which is impractical in scenarios like silent videos or noisy environments. SFVC addresses this limitation by relying solely on visual cues.", "method": "MuteSwap uses contrastive learning to align cross-modality identities and minimizes mutual information to separate shared visual features, enabling intelligible speech generation and identity conversion.", "result": "MuteSwap performs well in speech synthesis and identity conversion, even in noisy conditions where audio-dependent methods fail.", "conclusion": "The study demonstrates the feasibility of SFVC and the effectiveness of MuteSwap's training approach, highlighting its potential for applications where audio inputs are unavailable."}}
{"id": "2507.00043", "pdf": "https://arxiv.org/pdf/2507.00043", "abs": "https://arxiv.org/abs/2507.00043", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "AI": {"tldr": "MR-CLIP is a multimodal contrastive learning framework that aligns MRI images with DICOM metadata to learn contrast-aware representations, addressing challenges of incomplete or noisy metadata.", "motivation": "The lack of reliable and standardized metadata in MRI scans complicates tasks like image interpretation and retrieval, necessitating robust contrast-aware representations for advanced clinical applications.", "method": "MR-CLIP uses contrastive learning to align MR images with their DICOM metadata, trained on diverse clinical datasets without manual labels.", "result": "MR-CLIP effectively captures contrast variations, enabling anatomy-invariant representations and excelling in cross-modal retrieval and contrast classification.", "conclusion": "MR-CLIP offers a scalable solution for contrast-aware representations, with potential for broader clinical applications."}}
{"id": "2507.00048", "pdf": "https://arxiv.org/pdf/2507.00048", "abs": "https://arxiv.org/abs/2507.00048", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "comment": "10 pages, 5 figures", "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "AI": {"tldr": "A distributed self-driving lab (SDL) framework integrates machine learning and FAIR data for collaborative optimization, demonstrated with a frugal twin-inspired food dye experiment.", "motivation": "To accelerate discovery and optimization in science and engineering by enabling geographically dispersed researchers to collaborate effectively using shared FAIR data and ML models.", "method": "Implemented on nanoHUB services, the framework includes a shared database (ResultsDB), automated data processing (Sim2L), and active learning for sequential optimization.", "result": "Researchers can submit data via a web interface, benefit from updated ML models, and collaboratively optimize tasks like food dye combinations.", "conclusion": "The framework is broadly applicable and scalable to other optimization problems, promoting accessible and collaborative research."}}
{"id": "2507.00004", "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "AI": {"tldr": "The paper introduces DS3, a framework for optimizing LLM inference costs by modeling it as stochastic traversal over a skill graph, enabling comparative analysis of strategies like CoT and ToT.", "motivation": "Address the growing resource burden of LLM inference, which current compute-optimality characterizations overlook.", "method": "Propose DS3, a framework for inference as stochastic skill graph traversal, and derive closed-form expressions for task success and compute cost.", "result": "The framework theoretically explains empirical patterns like linear accuracy scaling with logarithmic compute and variation in inference strategies.", "conclusion": "DS3 enhances understanding of training-inference interdependencies, aiding algorithmic design and resource allocation."}}
{"id": "2507.00475", "pdf": "https://arxiv.org/pdf/2507.00475", "abs": "https://arxiv.org/abs/2507.00475", "authors": ["Minoru Kishi", "Ryosuke Sakai", "Shinnosuke Takamichi", "Yusuke Kanamori", "Yuki Okamoto"], "title": "AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "We propose a novel objective evaluation metric for synthesized audio in\ntext-to-audio (TTA), aiming to improve the performance of TTA models. In TTA,\nsubjective evaluation of the synthesized sound is an important, but its\nimplementation requires monetary costs. Therefore, objective evaluation such as\nmel-cepstral distortion are used, but the correlation between these objective\nmetrics and subjective evaluation values is weak. Our proposed objective\nevaluation metric, AudioBERTScore, calculates the similarity between embedding\nof the synthesized and reference sounds. The method is based not only on the\nmax-norm used in conventional BERTScore but also on the $p$-norm to reflect the\nnon-local nature of environmental sounds. Experimental results show that scores\nobtained by the proposed method have a higher correlation with subjective\nevaluation values than conventional metrics.", "AI": {"tldr": "Proposes AudioBERTScore, a novel objective metric for evaluating synthesized audio in TTA, outperforming conventional metrics by better correlating with subjective evaluations.", "motivation": "Subjective evaluation in TTA is costly, and existing objective metrics like mel-cepstral distortion poorly correlate with human judgment.", "method": "AudioBERTScore uses embeddings of synthesized and reference sounds, incorporating max-norm and $p$-norm to account for environmental sound characteristics.", "result": "The proposed metric shows higher correlation with subjective evaluations compared to traditional methods.", "conclusion": "AudioBERTScore is a more effective objective evaluation tool for TTA models, bridging the gap between objective and subjective assessments."}}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw is a multi-agent framework using LLMs for translating Hong Kong legal judgments, outperforming GPT-4 in accuracy but lagging behind humans in nuanced aspects.", "motivation": "The uncertainty of LLMs in translating legal texts due to complex terminology and cultural nuances motivates the development of TransLaw.", "method": "TransLaw employs three specialized agents (Translator, Annotator, Proofreader) for collaborative translation, supporting customizable LLM configurations.", "result": "The framework surpasses GPT-4 in legal semantic accuracy and coherence but trails human experts in nuanced terminology and style.", "conclusion": "TransLaw demonstrates cost-effective, high-accuracy legal translation, though human expertise remains superior for nuanced aspects."}}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "A two-stage approach enhances text classification by using LLM-generated reasoning, improving accuracy by 8.7% over a baseline model.", "motivation": "Standard models lack explicit reasoning, limiting performance and interpretability. This work aims to improve both by incorporating LLM-generated reasoning.", "method": "1. Fine-tune Llama-R-Gen on a reasoning dataset. 2. Use it to create an augmented dataset for a downstream model, trained to output reasoning and labels.", "result": "The model (Q->RA) outperforms the baseline (Q->A) by 8.7% in accuracy on emotion classification.", "conclusion": "LLM-generated reasoning enriches training data, boosting performance and providing interpretability for NLP tasks."}}
{"id": "2507.00458", "pdf": "https://arxiv.org/pdf/2507.00458", "abs": "https://arxiv.org/abs/2507.00458", "authors": ["Zhe Zhang", "Wen-Chin Huang", "Xin Wang", "Xiaoxiao Miao", "Junichi Yamagishi"], "title": "Mitigating Language Mismatch in SSL-Based Speaker Anonymization", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Speaker anonymization aims to protect speaker identity while preserving\ncontent information and the intelligibility of speech. However, most speaker\nanonymization systems (SASs) are developed and evaluated using only English,\nresulting in degraded utility for other languages. This paper investigates\nlanguage mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune\na self-supervised learning (SSL)-based content encoder with Japanese speech to\nverify effective language adaptation. Then, we propose fine-tuning a\nmultilingual SSL model with Japanese speech and evaluating the SAS in Japanese\nand Mandarin. Downstream experiments show that fine-tuning an English-only SSL\nmodel with the target language enhances intelligibility while maintaining\nprivacy and that multilingual SSL further extends SASs' utility across\ndifferent languages. These findings highlight the importance of language\nadaptation and multilingual pre-training of SSLs for robust multilingual\nspeaker anonymization.", "AI": {"tldr": "The paper explores language mismatch in speaker anonymization systems (SASs) for Japanese and Mandarin, showing that fine-tuning SSL models with target languages improves intelligibility and privacy, while multilingual SSL extends utility across languages.", "motivation": "Most SASs are developed for English, degrading utility for other languages. This work investigates language adaptation for Japanese and Mandarin.", "method": "Fine-tune SSL-based content encoder with Japanese speech; propose multilingual SSL fine-tuning for Japanese and Mandarin evaluation.", "result": "Fine-tuning English-only SSL with target language improves intelligibility and privacy; multilingual SSL enhances cross-language utility.", "conclusion": "Language adaptation and multilingual SSL pre-training are crucial for robust multilingual speaker anonymization."}}
{"id": "2507.00209", "pdf": "https://arxiv.org/pdf/2507.00209", "abs": "https://arxiv.org/abs/2507.00209", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "AI": {"tldr": "SurgiSR4K is the first publicly available 4K surgical imaging dataset for robotic-assisted MIS, addressing the lack of high-resolution data and enabling diverse computer vision tasks.", "motivation": "The absence of native 4K datasets for robotic-assisted MIS limits research and development in high-resolution surgical imaging.", "method": "SurgiSR4K is introduced, a dataset captured at native 4K resolution, featuring realistic surgical challenges like reflections, occlusions, and tissue deformations.", "result": "The dataset supports various computer vision tasks, including super-resolution, instrument detection, and 3D reconstruction.", "conclusion": "SurgiSR4K advances high-resolution surgical imaging research and fosters intelligent technologies for safer, more efficient robotic surgeries."}}
{"id": "2507.00950", "pdf": "https://arxiv.org/pdf/2507.00950", "abs": "https://arxiv.org/abs/2507.00950", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo.", "AI": {"tldr": "MVP, a multimodal video popularity predictor, won the SMP Challenge 2025 by integrating deep video features, user metadata, and contextual data, using preprocessing and gradient-boosted regression.", "motivation": "Predicting video popularity on social media aids in content recommendation, trend detection, and audience engagement.", "method": "MVP combines deep video features, user metadata, and contextual info, applies preprocessing (log-transformations, outlier removal), and uses gradient-boosted regression.", "result": "Ranked first in the SMP Challenge 2025 Video Track, proving its effectiveness.", "conclusion": "MVP is a reliable solution for multimodal video popularity prediction on social platforms."}}
{"id": "2507.00044", "pdf": "https://arxiv.org/pdf/2507.00044", "abs": "https://arxiv.org/abs/2507.00044", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "AI": {"tldr": "The paper proposes and compares three artifact detection methods for Whole Slide Imaging (WSI) in cancer diagnostics, with the foundation model-based approach (FMA) outperforming others.", "motivation": "Artifacts in WSIs compromise image analysis, necessitating robust detection methods.", "method": "Three approaches: FMA (fine-tuned UNI architecture), DLA (ResNet50 backbone), and KBA (handcrafted features). Evaluated on 50,000+ patches.", "result": "FMA achieved the highest AUROC (0.995), outperforming DLA (0.977) and KBA (0.940).", "conclusion": "FMA is superior for artifact detection in WSIs, with a quality report scorecard aiding actionable insights."}}
{"id": "2507.00050", "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "AI": {"tldr": "SEZ-HARN is a self-explainable zero-shot HAR model that recognizes unseen activities and provides skeleton videos for transparency, achieving competitive accuracy.", "motivation": "Address limitations in IMU-based HAR datasets and lack of transparency in existing zero-shot HAR models.", "method": "Introduces SEZ-HARN, a model that recognizes untrained activities and explains decisions via skeleton videos. Evaluated on four benchmark datasets.", "result": "SEZ-HARN matches black-box models' accuracy (within 3% on PAMAP2) while offering understandable explanations.", "conclusion": "SEZ-HARN balances accuracy and transparency, advancing practical HAR applications."}}
{"id": "2507.00011", "pdf": "https://arxiv.org/pdf/2507.00011", "abs": "https://arxiv.org/abs/2507.00011", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "title": "Novel RL approach for efficient Elevator Group Control Systems", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 12 figures", "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "AI": {"tldr": "An RL-based elevator control system outperforms traditional methods by handling stochastic and combinatorial challenges with innovative action space encoding and tailored rewards.", "motivation": "Efficient elevator traffic management is crucial for minimizing passenger travel times and energy consumption, but existing heuristic or pattern-based controllers struggle with stochastic and combinatorial complexities.", "method": "The paper models a six-elevator, fifteen-floor system as a Markov Decision Process, using RL with innovations like action space encoding, infra-steps for continuous arrivals, and tailored rewards. Dueling Double Deep Q-learning is explored.", "result": "The RL-based system adapts to fluctuating traffic patterns and outperforms traditional rule-based algorithms in a stochastic environment.", "conclusion": "The proposed RL approach effectively addresses the challenges of elevator dispatching, offering a scalable and adaptive solution for large buildings."}}
{"id": "2507.00693", "pdf": "https://arxiv.org/pdf/2507.00693", "abs": "https://arxiv.org/abs/2507.00693", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "AI": {"tldr": "The paper explores speech as a non-invasive indicator for suicide risk in adolescents, using LLMs for feature extraction and achieving 74% accuracy.", "motivation": "Early identification of suicide risk is critical, and speech offers an accessible mental health indicator.", "method": "Combines large language models (LLMs) with acoustic and semantic features for analysis.", "result": "Achieved 74% accuracy on the test set, ranking first in the SW1 challenge.", "conclusion": "LLM-based methods show promise for suicide risk assessment through speech analysis."}}
{"id": "2502.04388", "pdf": "https://arxiv.org/pdf/2502.04388", "abs": "https://arxiv.org/abs/2502.04388", "authors": ["Hepeng Li", "Yuhong Liu", "Jun Yan", "Jie Gao", "Xiaoou Yang"], "title": "Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) agents capable of autonomous learning and\nindependent decision-making hold great promise for addressing complex\nchallenges across various critical infrastructure domains, including\ntransportation, energy systems, and manufacturing. However, the surge in the\ndesign and deployment of AI systems, driven by various stakeholders with\ndistinct and unaligned objectives, introduces a crucial challenge: How can\nuncoordinated AI systems coexist and evolve harmoniously in shared environments\nwithout creating chaos or compromising safety? To address this, we advocate for\na fundamental rethinking of existing multi-agent frameworks, such as\nmulti-agent systems and game theory, which are largely limited to predefined\nrules and static objective structures. We posit that AI agents should be\nempowered to adjust their objectives dynamically, make compromises, form\ncoalitions, and safely compete or cooperate through evolving relationships and\nsocial feedback. Through two case studies in critical infrastructure\napplications, we call for a shift toward the emergent, self-organizing, and\ncontext-aware nature of these multi-agentic AI systems.", "AI": {"tldr": "The paper advocates for dynamic, self-organizing AI systems in critical infrastructure to ensure harmonious coexistence and safety.", "motivation": "Address the challenge of uncoordinated AI systems with unaligned objectives in shared environments.", "method": "Propose rethinking multi-agent frameworks to allow dynamic objective adjustment, compromise, coalition formation, and social feedback.", "result": "Case studies highlight the need for emergent, context-aware AI systems.", "conclusion": "A shift toward self-organizing, adaptive AI systems is necessary for safe and effective coexistence."}}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "RASTA improves LLM translation by aligning speaker and listener styles, addressing cultural misalignment and bias towards neutrality.", "motivation": "Cultural differences often cause misalignment in communication styles, such as politeness being lost in translation. LLMs exacerbate this by biasing translations towards neutrality and underperforming in non-Western languages.", "method": "RASTA (Retrieval-Augmented STylistic Alignment) uses learned stylistic concepts to align LLM translations with cultural norms.", "result": "RASTA mitigates style misalignment, improving translation accuracy for cultural communication norms.", "conclusion": "RASTA effectively addresses LLM shortcomings in style translation, enhancing cross-cultural communication."}}
{"id": "2507.00755", "pdf": "https://arxiv.org/pdf/2507.00755", "abs": "https://arxiv.org/abs/2507.00755", "authors": ["Jinhai Hu", "Zhongyi Zhang", "Cong Sheng Leow", "Wang Ling Goh", "Yuan Gao"], "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions\n  on Circuits and Systems I: Regular Papers", "summary": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "AI": {"tldr": "A co-design framework for learnable analog front-end (AFE) and backend classifier in audio signal classification, optimizing both jointly for system-level performance.", "motivation": "Separate design of AFE and backend classifiers is common but suboptimal; joint optimization improves performance.", "method": "Jointly optimizes AFE's transfer function (analog BPF bank) and classifier using an SNR-aware training loop and co-design loss function.", "result": "Achieved 90.5%-94.2% accuracy for 10-keyword classification, with 8.7% power and 12.9% capacitor area reduction.", "conclusion": "The co-design framework outperforms conventional approaches in accuracy, power efficiency, and area savings."}}
{"id": "2507.00398", "pdf": "https://arxiv.org/pdf/2507.00398", "abs": "https://arxiv.org/abs/2507.00398", "authors": ["Jian Wang", "Qiongying Ni", "Hongkui Yu", "Ruixuan Yao", "Jinqiao Ying", "Bin Zhang", "Xingyi Yang", "Jin Peng", "Jiongquan Chen", "Junxuan Yu", "Wenlong Shi", "Chaoyu Chen", "Zhongnuo Yan", "Mingyuan Luo", "Gaocheng Cai", "Dong Ni", "Jing Lu", "Xin Yang"], "title": "Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Accurate fetal birth weight (FBW) estimation is essential for optimizing\ndelivery decisions and reducing perinatal mortality. However, clinical methods\nfor FBW estimation are inefficient, operator-dependent, and challenging to\napply in cases of complex fetal anatomy. Existing deep learning methods are\nbased on 2D standard ultrasound (US) images or videos that lack spatial\ninformation, limiting their prediction accuracy. In this study, we propose the\nfirst method for directly estimating FBW from 3D fetal US volumes. Our approach\nintegrates a multi-scale feature fusion network (MFFN) and a synthetic\nsample-based learning framework (SSLF). The MFFN effectively extracts and fuses\nmulti-scale features under sparse supervision by incorporating channel\nattention, spatial attention, and a ranking-based loss function. SSLF generates\nsynthetic samples by simply combining fetal head and abdomen data from\ndifferent fetuses, utilizing semi-supervised learning to improve prediction\nperformance. Experimental results demonstrate that our method achieves superior\nperformance, with a mean absolute error of $166.4\\pm155.9$ $g$ and a mean\nabsolute percentage error of $5.1\\pm4.6$%, outperforming existing methods and\napproaching the accuracy of a senior doctor. Code is available at:\nhttps://github.com/Qioy-i/EFW.", "AI": {"tldr": "A novel method for fetal birth weight (FBW) estimation using 3D ultrasound volumes outperforms existing techniques by integrating multi-scale feature fusion and synthetic sample-based learning.", "motivation": "Current FBW estimation methods are inefficient, operator-dependent, and lack spatial accuracy. Deep learning approaches using 2D images or videos are limited in prediction accuracy.", "method": "Proposes a multi-scale feature fusion network (MFFN) with channel and spatial attention, and a synthetic sample-based learning framework (SSLF) to enhance feature extraction and prediction.", "result": "Achieves a mean absolute error of 166.4\u00b1155.9 g and a mean absolute percentage error of 5.1\u00b14.6%, surpassing existing methods and nearing senior doctor accuracy.", "conclusion": "The method offers a more accurate and efficient FBW estimation from 3D ultrasound volumes, with potential to improve clinical decision-making."}}
{"id": "2307.16579", "pdf": "https://arxiv.org/pdf/2307.16579", "abs": "https://arxiv.org/abs/2307.16579", "authors": ["Yuxin Mao", "Jing Zhang", "Mochu Xiang", "Yunqiu Lv", "Dong Li", "Yiran Zhong", "Yuchao Dai"], "title": "Contrastive Conditional Latent Diffusion for Audio-visual Segmentation", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "We propose a contrastive conditional latent diffusion model for audio-visual\nsegmentation (AVS) to thoroughly investigate the impact of audio, where the\ncorrelation between audio and the final segmentation map is modeled to\nguarantee the strong correlation between them. To achieve semantic-correlated\nrepresentation learning, our framework incorporates a latent diffusion model.\nThe diffusion model learns the conditional generation process of the\nground-truth segmentation map, resulting in ground-truth aware inference during\nthe denoising process at the test stage. As our model is conditional, it is\nvital to ensure that the conditional variable contributes to the model output.\nWe thus extensively model the contribution of the audio signal by minimizing\nthe density ratio between the conditional probability of the multimodal data,\ne.g. conditioned on the audio-visual data, and that of the unimodal data, e.g.\nconditioned on the audio data only. In this way, our latent diffusion model via\ndensity ratio optimization explicitly maximizes the contribution of audio for\nAVS, which can then be achieved with contrastive learning as a constraint,\nwhere the diffusion part serves as the main objective to achieve maximum\nlikelihood estimation, and the density ratio optimization part imposes the\nconstraint. By adopting this latent diffusion model via contrastive learning,\nwe effectively enhance the contribution of audio for AVS. The effectiveness of\nour solution is validated through experimental results on the benchmark\ndataset. Code and results are online via our project page:\nhttps://github.com/OpenNLPLab/DiffusionAVS.", "AI": {"tldr": "A contrastive conditional latent diffusion model for audio-visual segmentation (AVS) enhances audio impact by modeling correlation and optimizing density ratios.", "motivation": "To thoroughly investigate the impact of audio in AVS and ensure strong correlation between audio and segmentation maps.", "method": "Uses a latent diffusion model for conditional generation, optimizing density ratios to maximize audio contribution, with contrastive learning as a constraint.", "result": "Effectively enhances audio contribution for AVS, validated on benchmark datasets.", "conclusion": "The proposed model successfully integrates audio impact in AVS through contrastive learning and density ratio optimization."}}
{"id": "2507.00045", "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "The paper introduces CaughtCheating, a challenging scenario where advanced MLLMs like GPT-o3 fail, highlighting gaps in visual perception and reasoning compared to human detectives.", "motivation": "Existing MLLMs perform well on benchmarks but struggle in expert-level tasks like detecting subtle clues in images, prompting the need for harder tests.", "method": "The study investigates scenarios where GPT-o3 fails, particularly CaughtCheating, inspired by social media requests to detect suspicious clues in photos.", "result": "GPT-o3's performance drops to nearly zero in CaughtCheating, revealing limitations in visual perception and reasoning.", "conclusion": "CaughtCheating offers valuable, practical challenges for MLLMs, aiming to bridge the gap to human-level detective capabilities."}}
{"id": "2507.00054", "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "AdvDistill improves knowledge distillation by using reward-guided dataset distillation, enhancing student model performance in reasoning tasks.", "motivation": "Current knowledge distillation methods limit student models by focusing on copying teacher responses, reducing generalizability and efficiency, especially in reasoning tasks.", "method": "AdvDistill uses multiple teacher responses per prompt, assigns rewards via rule-based verifiers, and trains student models with weighted rewards.", "result": "Significant improvement in student model performance for mathematical and complex reasoning tasks.", "conclusion": "Incorporating a rewarding mechanism in dataset distillation enhances efficacy and benefits for reasoning tasks."}}
{"id": "2507.00012", "pdf": "https://arxiv.org/pdf/2507.00012", "abs": "https://arxiv.org/abs/2507.00012", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "categories": ["cs.LG", "cs.AI", "E.4"], "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "AI": {"tldr": "The paper introduces a method (CMIM) to create undistillable DNNs by minimizing conditional mutual information (CMI) alongside cross-entropy loss, ensuring knockoff students underperform label-smoothed ones.", "motivation": "To protect DNN intellectual property by making models undistillable, preventing effective knowledge distillation.", "method": "Proposes CMIM training, jointly minimizing cross-entropy loss and CMI values of temperature-scaled clusters.", "result": "CMIM models are undistillable by tested KD methods and outperform CE-trained models in accuracy.", "conclusion": "CMIM effectively creates undistillable DNNs while improving prediction accuracy, safeguarding intellectual property."}}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808", "abs": "https://arxiv.org/abs/2507.00808", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "title": "Multi-interaction TTS toward professional recording reproduction", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "AI": {"tldr": "A TTS method with multi-step interaction is proposed to enable iterative style refinement, mimicking voice actor-director feedback.", "motivation": "Current TTS lacks fine-grained style refinement post-synthesis, often deviating from user intent.", "method": "Models interaction between TTS and user, emulating voice actor-director dynamics for iterative refinement.", "result": "Experiments show the model enables iterative style refinements based on user directions.", "conclusion": "The proposed method successfully introduces multi-interaction capability for TTS style refinement."}}
{"id": "2311.17592", "pdf": "https://arxiv.org/pdf/2311.17592", "abs": "https://arxiv.org/abs/2311.17592", "authors": ["Rahul Misra", "Rafa\u0142 Wisniewski", "Carsten Skovmose Kalles\u00f8e", "Manuela L. Bujorianu"], "title": "Robust Correlated Equilibrium: Definition and Computation", "categories": ["eess.SY", "cs.MA", "cs.SY", "stat.ML"], "comment": "Preprint submitted to Automatica", "summary": "We study N-player finite games with costs perturbed due to time-varying\ndisturbances in the underlying system and to that end, we propose the concept\nof Robust Correlated Equilibrium that generalizes the definition of Correlated\nEquilibrium. Conditions under which the Robust Correlated Equilibrium exists\nare specified, and a decentralized algorithm for learning strategies that are\noptimal in the sense of Robust Correlated Equilibrium is proposed. The primary\ncontribution of the paper is the convergence analysis of the algorithm and to\nthat end, we propose a modification of the celebrated Blackwell's\nApproachability theorem to games with costs that are not just time-average, as\nin the original Blackwell's Approachability Theorem, but also include the\ntime-average of previous algorithm iterates. The designed algorithm is applied\nto a practical water distribution network with pumps being the controllers and\ntheir costs being perturbed by uncertain consumption due to the consumers.\nSimulation results show that each controller achieves no regret, and empirical\ndistributions converge to the Robust Correlated Equilibrium.", "AI": {"tldr": "The paper introduces Robust Correlated Equilibrium for N-player games with perturbed costs, proposes a decentralized learning algorithm, and proves its convergence using a modified Blackwell's Approachability theorem.", "motivation": "To address games with time-varying disturbances in costs, generalizing Correlated Equilibrium for robustness.", "method": "Proposes a decentralized algorithm for learning optimal strategies and modifies Blackwell's Approachability theorem for convergence analysis.", "result": "The algorithm ensures no regret for controllers and empirical distributions converge to Robust Correlated Equilibrium.", "conclusion": "The approach is validated in a water distribution network, showing practical applicability."}}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "Linear probes on LM hidden states reveal that refused harmful information remains accessible and influential post-instruction-tuning.", "motivation": "To investigate if harmful information refused by instruction-tuned LMs is still decodable and influential in their behavior.", "method": "Use linear probes trained on LM hidden states to decode initially refused information, testing transferability between base and instruction-tuned models.", "result": "Probes achieve high accuracy (Pearson correlations >0.8), showing refused information is linearly accessible and aligns with suppressed generative behavior.", "conclusion": "Instruction-tuning suppresses but does not eliminate harmful information, leaving it linearly decodable and indirectly influential."}}
{"id": "2507.00874", "pdf": "https://arxiv.org/pdf/2507.00874", "abs": "https://arxiv.org/abs/2507.00874", "authors": ["Jun-Wei Yeow", "Ee-Leng Tan", "Santi Peksi", "Woon-Seng Gan"], "title": "Improving Stereo 3D Sound Event Localization and Detection: Perceptual Features, Stereo-specific Data Augmentation, and Distance Normalization", "categories": ["eess.AS"], "comment": "Technical report for DCASE 2025 Challenge Task 3", "summary": "This technical report presents our submission to Task 3 of the DCASE 2025\nChallenge: Stereo Sound Event Localization and Detection (SELD) in Regular\nVideo Content. We address the audio-only task in this report and introduce\nseveral key contributions. First, we design perceptually-motivated input\nfeatures that improve event detection, sound source localization, and distance\nestimation. Second, we adapt augmentation strategies specifically for the\nintricacies of stereo audio, including channel swapping and time-frequency\nmasking. We also incorporate the recently proposed FilterAugment technique that\nhas yet to be explored for SELD work. Lastly, we apply a distance normalization\napproach during training to stabilize regression targets. Experiments on the\nstereo STARSS23 dataset demonstrate consistent performance gains across all\nSELD metrics. Code to replicate our work is available in this repository:\nhttps://github.com/itsjunwei/NTU_SNTL_Task3", "AI": {"tldr": "The paper presents a submission for the DCASE 2025 Challenge's Task 3, focusing on Stereo Sound Event Localization and Detection (SELD). Key contributions include perceptually-motivated input features, stereo-specific augmentation strategies, FilterAugment, and distance normalization, leading to improved performance on the STARSS23 dataset.", "motivation": "To enhance Stereo SELD performance by addressing event detection, sound source localization, and distance estimation in stereo audio.", "method": "Introduces perceptually-motivated input features, stereo-specific augmentations (channel swapping, time-frequency masking, FilterAugment), and distance normalization during training.", "result": "Experiments on the STARSS23 dataset show consistent performance gains across all SELD metrics.", "conclusion": "The proposed methods effectively improve SELD performance, with code available for replication."}}
{"id": "2507.00511", "pdf": "https://arxiv.org/pdf/2507.00511", "abs": "https://arxiv.org/abs/2507.00511", "authors": ["Sayandeep Kanrar", "Raja Piyush", "Qaiser Razi", "Debanshi Chakraborty", "Vikas Hassija", "GSS Chalapathi"], "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two\ncutting-edge deep learning architectures designed to enhance medical image\nsegmentation. Our approach integrates Squeeze-and-Excitation (SE) and\nConvolutional Block Attention Module (CBAM) techniques into the traditional VM\nU-Net framework, significantly improving segmentation accuracy, feature\nlocalization, and computational efficiency. Both models show superior\nperformance compared to the baseline VM-Unet across multiple datasets. Notably,\nVMSEUnet achieves the highest accuracy, IoU, precision, and recall while\nmaintaining low loss values. It also exhibits exceptional computational\nefficiency with faster inference times and lower memory usage on both GPU and\nCPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a\nvaluable tool for medical image analysis. These findings highlight its\npotential for real-world clinical applications, emphasizing the importance of\nfurther research to optimize accuracy, robustness, and computational\nefficiency.", "AI": {"tldr": "The paper introduces VMSE U-Net and VM-Unet CBAM+, two advanced deep learning models for medical image segmentation, outperforming baseline VM-Unet in accuracy, efficiency, and computational performance.", "motivation": "To enhance medical image segmentation by integrating SE and CBAM techniques into the VM U-Net framework for improved accuracy and efficiency.", "method": "Integration of Squeeze-and-Excitation (SE) and Convolutional Block Attention Module (CBAM) into the VM U-Net architecture.", "result": "VMSE-Unet achieves the highest accuracy, IoU, precision, and recall with low loss and superior computational efficiency.", "conclusion": "The VMSE-Unet is a promising tool for medical image analysis, with potential for clinical applications, warranting further research for optimization."}}
{"id": "2411.13536", "pdf": "https://arxiv.org/pdf/2411.13536", "abs": "https://arxiv.org/abs/2411.13536", "authors": ["Bahri Batuhan Bilecen", "Ahmet Berke Gokmen", "Furkan Guzelant", "Aysegul Dundar"], "title": "Identity Preserving 3D Head Stylization with Multiview Score Distillation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.MM"], "comment": "https://three-bee.github.io/head_stylization", "summary": "3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.", "AI": {"tldr": "The paper introduces a novel framework for 3D head stylization using PanoHead and negative log-likelihood distillation to improve identity preservation and stylization quality.", "motivation": "Existing 3D stylization methods often fail to preserve unique identities and lack diversity, especially in non-frontal views.", "method": "Leverages PanoHead for 360-degree synthesis, integrates multi-view grid score and mirror gradients, and uses score rank weighing in a 3D GAN architecture.", "result": "Achieves qualitative and quantitative improvements in stylization and identity preservation.", "conclusion": "Advances 3D head stylization and provides insights into distillation between diffusion models and GANs."}}
{"id": "2507.00046", "pdf": "https://arxiv.org/pdf/2507.00046", "abs": "https://arxiv.org/abs/2507.00046", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "categories": ["cs.CV", "cs.CE"], "comment": "7 pages, 4 figures", "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "AI": {"tldr": "An evolutionary computing-based image segmentation method using PSO for defect detection in AFSD processes, integrating gradient and distance transforms for enhanced visualization.", "motivation": "To improve defect detection and feature analysis in multilayer AFSD builds by leveraging advanced image segmentation and visualization techniques.", "method": "Uses PSO for optimal thresholding, gradient magnitude analysis, and distance transforms to create attention-weighted visualizations (self-attention maps, multi-channel visualization).", "result": "PSO identified thresholds (156-173) for precise segmentation; multi-channel visualization revealed hidden defects and material transitions.", "conclusion": "Attention-based analysis effectively detects bonding issues and inhomogeneities, offering quantitative metrics for AFSD quality assessment and optimization."}}
{"id": "2507.00079", "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "AI": {"tldr": "The paper introduces VoyagerVision, a multi-modal model that uses visual feedback (screenshots) in Minecraft to enhance open-ended task performance, achieving 2.75 unique structures in 50 iterations.", "motivation": "To extend the open-ended potential of AGI by enabling models to interpret spatial environments through visual inputs, improving task performance.", "method": "Proposes VoyagerVision, a multi-modal model leveraging screenshots for visual feedback in Minecraft, building on the Voyager framework.", "result": "VoyagerVision created 2.75 unique structures in 50 iterations and succeeded in 50% of flat-world building tests, outperforming Voyager.", "conclusion": "Visual feedback enhances spatial interpretation and task performance, demonstrating the potential of multi-modal models in open-ended AGI."}}
{"id": "2507.00013", "pdf": "https://arxiv.org/pdf/2507.00013", "abs": "https://arxiv.org/abs/2507.00013", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by KDD 2025 research track", "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "AI": {"tldr": "ST-MTM improves time-series forecasting by combining masked modeling with seasonal-trend decomposition and contrastive learning, outperforming existing methods.", "motivation": "Current masked time-series modeling ignores inherent semantic structures, leading to spurious patterns. Addressing entangled temporal patterns through decomposition can enhance forecasting.", "method": "ST-MTM uses seasonal-trend decomposition with novel masking strategies: period masking for seasonal components and sub-series masking for trend components, plus contrastive learning for consistency.", "result": "ST-MTM consistently outperforms existing masked modeling, contrastive learning, and supervised forecasting methods.", "conclusion": "Decomposition-based masking and contrastive learning in ST-MTM effectively capture temporal semantics, improving forecasting accuracy."}}
{"id": "2507.00966", "pdf": "https://arxiv.org/pdf/2507.00966", "abs": "https://arxiv.org/abs/2507.00966", "authors": ["Nikolai Lund K\u00fchne", "Jesper Jensen", "Jan \u00d8stergaard", "Zheng-Hua Tan"], "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing for possible publication", "summary": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "AI": {"tldr": "The paper introduces MambAttention, a hybrid model combining Mamba and time-frequency attention modules, outperforming existing models in speech enhancement tasks, especially on out-of-domain datasets.", "motivation": "To address overfitting in sequence models like LSTM and Mamba for speech enhancement, the paper explores hybrid architectures with attention modules for better generalization.", "method": "Proposes MambAttention, integrating Mamba with shared time- and frequency-multi-head attention modules, trained on the challenging VB-DemandEx dataset.", "result": "MambAttention outperforms state-of-the-art models on out-of-domain datasets (DNS 2020 and EARS-WHAM_v2) and matches performance on in-domain data (VB-DemandEx).", "conclusion": "The hybrid MambAttention model demonstrates superior generalization, with ablation studies confirming the importance of weight sharing in attention modules."}}
{"id": "2506.22971", "pdf": "https://arxiv.org/pdf/2506.22971", "abs": "https://arxiv.org/abs/2506.22971", "authors": ["Kesav Kaza", "Ramachandran Anantharaman", "Rahul Meshram"], "title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "comment": "6 pages, 2 figures", "summary": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "AI": {"tldr": "A hierarchical decentralized control architecture for Cyber-Physical Systems is proposed, with global and local controllers operating at different timescales. Two optimization frameworks (COpt and FOpt) for local controllers are introduced, and their relationships are analyzed.", "motivation": "To address control challenges in Cyber-Physical Systems by leveraging a two-timescale hierarchical approach, balancing global optimization with local autonomy.", "method": "The architecture includes global and local controllers modeled as MDPs. COpt optimizes infinite-horizon rewards, while FOpt uses finite-horizon rewards, mimicking a federal structure. Theoretical analysis includes policy existence and value function comparisons.", "result": "Existence of optimal policies for both frameworks is proven. A bound on value function differences is derived, and conditions for identical optimal values are provided.", "conclusion": "The proposed architecture and frameworks offer flexible control solutions, with FOpt enabling greater local autonomy while maintaining theoretical guarantees."}}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "A mathematical model of the morphology-syntax interface is presented, using magma and operad theory to describe word formation and morphosyntactic tree construction without movement in morphology.", "motivation": "To formalize the interface between morphology and syntax under the Strong Minimalist Thesis, addressing how word formation and syntactic structure interact mathematically.", "method": "Uses magma for morphological trees and operad theory for morphosyntactic trees, introducing a coproduct decomposition and transformations inspired by Distributed Morphology.", "result": "A framework where morphological and syntactic data are paired via operadic correspondence, allowing flexible boundary shifts between syntax and morphology.", "conclusion": "The model successfully integrates morphology and syntax mathematically, providing a formal basis for understanding their interface and flexibility."}}
{"id": "2505.14518", "pdf": "https://arxiv.org/pdf/2505.14518", "abs": "https://arxiv.org/abs/2505.14518", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025. Project Website:\n  https://kuan2jiu99.github.io/Balsa", "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.", "AI": {"tldr": "LISTEN is a contrastive-like training method for audio-aware LLMs to reduce hallucinations of non-existent sounds without modifying LLM parameters.", "motivation": "Current audio-aware LLMs hallucinate non-existent sounds, reducing reliability in real-world applications.", "method": "Proposes LISTEN, a contrastive-like training method using synthesized data and a lightweight adapter to integrate audio representations.", "result": "LISTEN mitigates hallucinations while maintaining performance on audio benchmarks and is more efficient in data and computation.", "conclusion": "LISTEN effectively improves the reliability of audio-aware LLMs without extensive modifications."}}
{"id": "2507.00527", "pdf": "https://arxiv.org/pdf/2507.00527", "abs": "https://arxiv.org/abs/2507.00527", "authors": ["Ziyang Liu", "Xingchen Xiao", "Yueyang Xu"], "title": "Anti-aliasing Algorithm Based on Three-dimensional Display Image", "categories": ["eess.IV"], "comment": null, "summary": "3D-display technology has been a promising emerging area with potential to be\nthe core of next-generation display technology. When directly observing\nunprocessed images and text through a naked-eye 3D display device, severe\ndistortion and jaggedness will be displayed, which will make the display effect\nmuch worse. In this work, we try to settle down such degradation with spatial\nand frequency processing, furthermore, we make efforts to extract degenerate\nfunction of columnar lens array thus fundamentally eliminating degradation.", "AI": {"tldr": "The paper addresses distortion and jaggedness in naked-eye 3D displays using spatial and frequency processing and analyzes the degenerate function of columnar lens arrays.", "motivation": "To improve the poor display quality caused by distortion and jaggedness in naked-eye 3D displays.", "method": "Spatial and frequency processing techniques are applied, and the degenerate function of columnar lens arrays is analyzed.", "result": "The proposed methods aim to reduce degradation and improve display quality.", "conclusion": "The approach effectively addresses display degradation in 3D technology by combining processing techniques and lens array analysis."}}
{"id": "2504.04065", "pdf": "https://arxiv.org/pdf/2504.04065", "abs": "https://arxiv.org/abs/2504.04065", "authors": ["Jiaqi Deng", "Kaize Shi", "Zonghan Wu", "Huan Huo", "Dingxian Wang", "Guandong Xu"], "title": "Enabling Collaborative Parametric Knowledge Calibration for Retrieval-Augmented Vision Question Answering", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "10 pages, 5 figures, Under Review", "summary": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions with knowledge retrieved from external knowledge\nbases. The tasks of knowledge retrieval and answer generation tasks both\nnecessitate precise multimodal understanding of question context and external\nknowledge. However, existing methods treat these two stages as separate modules\nwith limited interaction during training, which hinders bi-directional\nparametric knowledge sharing, ultimately leading to suboptimal performance. To\nfully exploit the cross-task synergy in KB-VQA, we propose a unified\nretrieval-augmented VQA framework with collaborative parametric knowledge\ncalibration. The proposed framework can effectively adapt general multimodal\npre-trained models for fine-grained, knowledge-intensive tasks while enabling\nthe retriever and generator to collaboratively enhance and share their\nparametric knowledge during both training and inference. To enhance\nfine-grained understanding of questions and external documents, we also\nintegrate late interaction mechanism into the proposed training framework.\nAdditionally, we introduce a reflective-answering mechanism that allows the\nmodel to explicitly evaluate and refine its knowledge boundary. Our approach\nachieves competitive performance against state-of-the-art models, delivering a\nsignificant 4.7\\% improvement in answering accuracy, and brings an average\n7.5\\% boost in base MLLMs' VQA performance.", "AI": {"tldr": "A unified retrieval-augmented VQA framework improves KB-VQA performance by enabling collaborative knowledge sharing between retrieval and generation tasks, achieving a 4.7% accuracy boost.", "motivation": "Existing KB-VQA methods treat knowledge retrieval and answer generation as separate modules, limiting bi-directional knowledge sharing and performance.", "method": "Proposes a unified framework with collaborative parametric knowledge calibration, late interaction for fine-grained understanding, and a reflective-answering mechanism.", "result": "Achieves a 4.7% improvement in answering accuracy and a 7.5% boost in base MLLMs' VQA performance.", "conclusion": "The framework enhances KB-VQA by integrating retrieval and generation tasks, improving accuracy and knowledge utilization."}}
{"id": "2507.00049", "pdf": "https://arxiv.org/pdf/2507.00049", "abs": "https://arxiv.org/abs/2507.00049", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "AI": {"tldr": "AdaDeDup is a hybrid data pruning method combining density-based and model-informed feedback to efficiently prune redundant data while preserving critical samples, improving data efficiency in large-scale model training.", "motivation": "Large-scale datasets are computationally burdensome and redundant, but existing pruning methods are either task-agnostic or computationally expensive.", "method": "AdaDeDup integrates density-based pruning with model feedback, adaptively adjusting pruning thresholds per cluster based on task-aware signals.", "result": "AdaDeDup outperforms baselines, reduces performance degradation (e.g., 54% vs. random sampling), and achieves near-original performance with 20% data pruned.", "conclusion": "AdaDeDup effectively enhances data efficiency for large-scale training, demonstrated on benchmarks like Waymo, COCO, and nuScenes."}}
{"id": "2507.00092", "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "The paper introduces inverse reasoning, a method for LLMs to decompose and explain their reasoning post-hoc, improving interpretability and performance.", "motivation": "To address the blackbox nature of LLM decision-making by enabling self-reflection and explanation of reasoning chains.", "method": "Uses a metacognitive structure (SAGE-nano) to reverse attention flow and generate explanations for reasoning choices.", "result": "Achieves high reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score), comparable to top models.", "conclusion": "Inverse reasoning enhances LLM transparency and performance, advancing AI safety, education, and scientific discovery."}}
{"id": "2507.00014", "pdf": "https://arxiv.org/pdf/2507.00014", "abs": "https://arxiv.org/abs/2507.00014", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "AI": {"tldr": "SWE-Bench-CL is a continual learning benchmark for evaluating LLMs in evolving software development contexts, featuring chronologically ordered GitHub issues and specialized metrics.", "motivation": "To address the gap in evaluating LLMs' ability to handle continuous, real-world software development tasks, including knowledge transfer and resistance to forgetting.", "method": "Introduces SWE-Bench-CL with chronologically ordered GitHub issues, an interactive LangGraph-based framework, and specialized continual learning metrics.", "result": "Provides a reproducible platform for evaluating memory-enabled and memory-disabled agents in Python repositories.", "conclusion": "SWE-Bench-CL offers a robust tool for developing adaptive AI agents in software engineering, with publicly available code and data."}}
{"id": "2503.04995", "pdf": "https://arxiv.org/pdf/2503.04995", "abs": "https://arxiv.org/abs/2503.04995", "authors": ["Richa Namballa", "Giovana Morais", "Magdalena Fuentes"], "title": "Musical Source Separation of Brazilian Percussion", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "2 pages + references, 1 figure, 1 table, Extended Abstracts for the\n  Late-Breaking Demo Session of the 25th International Society for Music\n  Information Retrieval Conference", "summary": "Musical source separation (MSS) has recently seen a big breakthrough in\nseparating instruments from a mixture in the context of Western music, but\nresearch on non-Western instruments is still limited due to a lack of data. In\nthis demo, we use an existing dataset of Brazilian sama percussion to create\nartificial mixtures for training a U-Net model to separate the surdo drum, a\ntraditional instrument in samba. Despite limited training data, the model\neffectively isolates the surdo, given the drum's repetitive patterns and its\ncharacteristic low-pitched timbre. These results suggest that MSS systems can\nbe successfully harnessed to work in more culturally-inclusive scenarios\nwithout the need of collecting extensive amounts of data.", "AI": {"tldr": "A U-Net model trained on limited Brazilian sama percussion data effectively separates the surdo drum in samba music, suggesting MSS can work in culturally-inclusive scenarios without extensive data.", "motivation": "Research on non-Western instruments in MSS is limited due to lack of data, prompting exploration of Brazilian sama percussion.", "method": "Artificial mixtures from a dataset of Brazilian sama percussion were used to train a U-Net model for separating the surdo drum.", "result": "The model successfully isolates the surdo drum, leveraging its repetitive patterns and low-pitched timbre.", "conclusion": "MSS systems can be adapted for culturally-inclusive scenarios without needing large datasets."}}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "Non-English languages can be more token-efficient for reasoning in language models, preserving accuracy and showing genuine shifts in reasoning behavior.", "motivation": "To investigate if English is the most token-efficient language for reasoning, given that most research focuses on English despite multilingual pretraining.", "method": "Evaluated three open-source RLMs (DeepSeek R1, Qwen 2.5, Qwen 3) across four math datasets and seven diverse languages.", "result": "Non-English reasoning reduces token usage and preserves accuracy, with gains persisting even after translation. Improvement depends on model multilingual strength.", "conclusion": "Multilingual reasoning has potential, and strong multilingual foundations are important for language models."}}
{"id": "2412.01530", "pdf": "https://arxiv.org/pdf/2412.01530", "abs": "https://arxiv.org/abs/2412.01530", "authors": ["Anthony Gibbons", "Emma King", "Ian Donohue", "Andrew Parnell"], "title": "Generative AI-based data augmentation for improved bioacoustic classification in noisy environments", "categories": ["cs.SD", "eess.AS", "stat.AP"], "comment": "20 pages, 3 tables, 6 figures", "summary": "1. Obtaining data to train robust artificial intelligence (AI)-based models\nfor species classification can be challenging, particularly for rare species.\nData augmentation can boost classification accuracy by increasing the diversity\nof training data and is cheaper to obtain than expert-labelled data. However,\nmany classic image-based augmentation techniques are not suitable for audio\nspectrograms. 2. We investigate two generative AI models as data augmentation\ntools to synthesise spectrograms and supplement audio data: Auxiliary\nClassifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion\nProbabilistic Models (DDPMs). The latter performed particularly well in terms\nof both realism of generated spectrograms and accuracy in a resulting\nclassification task. 3. Alongside these new approaches, we present a new audio\ndata set of 640 hours of bird calls from wind farm sites in Ireland,\napproximately 800 samples of which have been labelled by experts. Wind farm\ndata are particularly challenging for classification models given the\nbackground wind and turbine noise. 4. Training an ensemble of classification\nmodels on real and synthetic data combined gave 92.6% accuracy (and 90.5% with\njust the real data) when compared with highly confident BirdNET predictions. 5.\nOur approach can be used to augment acoustic signals for more species and other\nland-use types, and has the potential to bring about a step-change in our\ncapacity to develop reliable AI-based detection of rare species. Our code is\navailable at https://github.com/gibbona1/SpectrogramGenAI.", "AI": {"tldr": "The paper explores using generative AI models (ACGAN and DDPMs) for augmenting audio spectrogram data to improve rare species classification, achieving higher accuracy with synthetic data.", "motivation": "Challenges in obtaining labeled data for rare species classification and the limitations of traditional augmentation methods for audio spectrograms.", "method": "Investigates ACGAN and DDPMs for synthesizing spectrograms, tests their realism and classification accuracy, and introduces a new labeled bird call dataset from wind farms.", "result": "DDPMs outperformed in realism and accuracy, with combined real and synthetic data achieving 92.6% accuracy (vs. 90.5% with real data alone).", "conclusion": "The approach can enhance acoustic signal augmentation for rare species detection, offering potential for reliable AI-based solutions."}}
{"id": "2507.00582", "pdf": "https://arxiv.org/pdf/2507.00582", "abs": "https://arxiv.org/abs/2507.00582", "authors": ["Yi Zhang", "Yidong Zhao", "Qian Tao"], "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted version. Accepted by MICCAI 2025", "summary": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods.", "AI": {"tldr": "DEQReg proposes a novel deformable medical image registration framework using Deep Equilibrium Models (DEQ), addressing memory and convergence issues in learning-based methods.", "motivation": "Traditional optimization-based methods converge iteratively, but learning-based unrolling methods lack convergence guarantees and face high GPU memory usage.", "method": "DEQReg formulates registration as an equilibrium-seeking problem, leveraging DEQ to maintain constant memory usage and enable unlimited iteration steps.", "result": "DEQReg achieves competitive performance on brain MRI and lung CT datasets while reducing memory consumption and ensuring stable convergence.", "conclusion": "DEQReg bridges the gap between classical and learning-based methods, offering a stable, memory-efficient solution for medical image registration."}}
{"id": "2506.12269", "pdf": "https://arxiv.org/pdf/2506.12269", "abs": "https://arxiv.org/abs/2506.12269", "authors": ["Babak Naderi", "Ross Cutler", "Juhee Cho", "Nabakumar Khongbantabam", "Dejan Ivkovic"], "title": "ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Super-Resolution (SR) is a critical task in computer vision, focusing on\nreconstructing high-resolution (HR) images from low-resolution (LR) inputs. The\nfield has seen significant progress through various challenges, particularly in\nsingle-image SR. Video Super-Resolution (VSR) extends this to the temporal\ndomain, aiming to enhance video quality using methods like local, uni-,\nbi-directional propagation, or traditional upscaling followed by restoration.\nThis challenge addresses VSR for conferencing, where LR videos are encoded with\nH.265 at fixed QPs. The goal is to upscale videos by a specific factor,\nproviding HR outputs with enhanced perceptual quality under a low-delay\nscenario using causal models. The challenge included three tracks:\ngeneral-purpose videos, talking head videos, and screen content videos, with\nseparate datasets provided by the organizers for training, validation, and\ntesting. We open-sourced a new screen content dataset for the SR task in this\nchallenge. Submissions were evaluated through subjective tests using a\ncrowdsourced implementation of the ITU-T Rec P.910.", "AI": {"tldr": "The paper discusses Video Super-Resolution (VSR) for conferencing, focusing on enhancing video quality using causal models under low-delay scenarios. It introduces a challenge with three tracks and a new screen content dataset.", "motivation": "To improve video quality in conferencing by addressing the challenges of VSR, particularly for low-resolution videos encoded with H.265.", "method": "The challenge involved three tracks (general-purpose, talking head, screen content) with datasets provided. Causal models were used for upscaling under low-delay constraints.", "result": "A new screen content dataset was open-sourced, and submissions were evaluated via subjective tests using ITU-T Rec P.910.", "conclusion": "The challenge successfully advanced VSR for conferencing, providing a framework and dataset for future research."}}
{"id": "2507.00052", "pdf": "https://arxiv.org/pdf/2507.00052", "abs": "https://arxiv.org/abs/2507.00052", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "AI": {"tldr": "VSF-Med is a framework for evaluating vulnerabilities in medical Vision Language Models (VLMs) using adversarial attacks and a scoring rubric.", "motivation": "To address the lack of systematic security evaluations for VLMs in medical imaging.", "method": "VSF-Med combines text-prompt attack templates, imperceptible visual perturbations, and an eight-dimensional rubric evaluated by LLMs.", "result": "The framework tested 30,000 adversarial variants, revealing vulnerabilities in state-of-the-art VLMs like Llama-3.2-11B-Vision-Instruct and GPT-4o.", "conclusion": "VSF-Med provides a reproducible benchmark for assessing medical VLM security, highlighting significant vulnerabilities."}}
{"id": "2507.00180", "pdf": "https://arxiv.org/pdf/2507.00180", "abs": "https://arxiv.org/abs/2507.00180", "authors": ["Vidhi Rathore"], "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "AI": {"tldr": "A novel pipeline using RL and decision trees extracts interpretable decision logic from legacy systems, tested on dummy systems with promising results.", "motivation": "Legacy systems lack documentation, and traditional methods fail to capture underlying logic.", "method": "Uses RL to explore input space, identify decision boundaries, clusters transitions, and trains decision trees for human-readable rules.", "result": "RL agent focuses on boundary regions; extracted rules accurately reflect system logic.", "conclusion": "The pipeline is effective for generating specifications and test cases in legacy migration."}}
{"id": "2507.00015", "pdf": "https://arxiv.org/pdf/2507.00015", "abs": "https://arxiv.org/abs/2507.00015", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "AI": {"tldr": "A novel vision transformer (ViT) with an adversarial indicator (AdvI) token is proposed to defend against adversarial attacks in modulation classification, combining training and runtime defenses in one model.", "motivation": "Transformers are widely used but vulnerable to adversarial attacks in modulation classification for IoT devices, necessitating a robust defensive strategy.", "method": "Introduces an AdvI token in ViT to detect attacks, integrates adversarial training, and examines attention mechanisms to highlight suspicious features.", "result": "The method outperforms others in handling white-box attacks like FGM, PGD, and BIM, showing effective defense.", "conclusion": "The AdvI token in ViT provides a unified and efficient defense against adversarial attacks in modulation classification."}}
{"id": "2412.19351", "pdf": "https://arxiv.org/pdf/2412.19351", "abs": "https://arxiv.org/abs/2412.19351", "authors": ["Sang-gil Lee", "Zhifeng Kong", "Arushi Goel", "Sungwon Kim", "Rafael Valle", "Bryan Catanzaro"], "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "ICML 2025. Demo: https://research.nvidia.com/labs/adlr/ETTA/ Code:\n  https://github.com/NVIDIA/elucidated-text-to-audio", "summary": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks.", "AI": {"tldr": "The paper explores Text-To-Audio (TTA) synthesis, analyzing data, model architecture, and training strategies. It introduces AF-Synthetic dataset, compares design choices, and proposes ETTA, a model outperforming baselines and handling complex captions well.", "motivation": "To provide a holistic understanding of TTA model design choices, addressing gaps in knowledge about data, architecture, and training effects.", "method": "Conducted large-scale experiments with diffusion and flow matching models, comparing architectures, training objectives, and sampling strategies. Introduced AF-Synthetic dataset and analyzed Pareto curves for sampling methods.", "result": "Proposed ETTA model outperforms baselines on AudioCaps and MusicCaps, competes with proprietary-data models, and excels in generating audio for complex captions.", "conclusion": "ETTA advances TTA synthesis, demonstrating the impact of systematic design choices and offering a robust solution for creative audio generation."}}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "Prompt-based fine-tuning is more privacy-preserving than parameter-based fine-tuning, showing lower vulnerability to membership inference attacks (MIAs).", "motivation": "To address the overlooked privacy risks of memorization during fine-tuning of large language models (LLMs).", "method": "Categorize fine-tuning approaches and evaluate their memorization impact using membership inference attacks (MIAs).", "result": "Prompt-based fine-tuning performs competitively and is less vulnerable to MIAs, regardless of model scale.", "conclusion": "Parameter-based fine-tuning leaks more private information, making prompt-based methods a better privacy-preserving choice."}}
{"id": "2505.04203", "pdf": "https://arxiv.org/pdf/2505.04203", "abs": "https://arxiv.org/abs/2505.04203", "authors": ["Zhiping Qiu", "Yitong Jin", "Yuan Wang", "Yi Shi", "Chongwu Wang", "Chao Tan", "Xiaobing Li", "Feng Yu", "Tao Yu", "Qionghai Dai"], "title": "ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition", "categories": ["cs.GR", "cs.SD", "eess.AS"], "comment": null, "summary": "The art of instrument performance stands as a vivid manifestation of human\ncreativity and emotion. Nonetheless, generating instrument performance motions\nis a highly challenging task, as it requires not only capturing intricate\nmovements but also reconstructing the complex dynamics of the\nperformer-instrument interaction. While existing works primarily focus on\nmodeling partial body motions, we propose Expressive ceLlo performance motion\nGeneration for Audio Rendition (ELGAR), a state-of-the-art diffusion-based\nframework for whole-body fine-grained instrument performance motion generation\nsolely from audio. To emphasize the interactive nature of the instrument\nperformance, we introduce Hand Interactive Contact Loss (HICL) and Bow\nInteractive Contact Loss (BICL), which effectively guarantee the authenticity\nof the interplay. Moreover, to better evaluate whether the generated motions\nalign with the semantic context of the music audio, we design novel metrics\nspecifically for string instrument performance motion generation, including\nfinger-contact distance, bow-string distance, and bowing score. Extensive\nevaluations and ablation studies are conducted to validate the efficacy of the\nproposed methods. In addition, we put forward a motion generation dataset\nSPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated,\nELGAR has shown great potential in generating instrument performance motions\nwith complicated and fast interactions, which will promote further development\nin areas such as animation, music education, interactive art creation, etc.", "AI": {"tldr": "ELGAR is a diffusion-based framework for generating whole-body cello performance motions from audio, using novel interactive contact losses and evaluation metrics.", "motivation": "Existing works focus on partial body motions, but generating realistic instrument performance requires capturing intricate movements and performer-instrument dynamics.", "method": "Proposes ELGAR, a diffusion-based framework, with Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL) to ensure authentic interplay. Introduces new metrics for evaluation.", "result": "ELGAR effectively generates fine-grained motions with complex interactions, validated by extensive evaluations and the SPD-GEN dataset.", "conclusion": "ELGAR advances instrument performance motion generation, with applications in animation, music education, and interactive art."}}
{"id": "2507.00613", "pdf": "https://arxiv.org/pdf/2507.00613", "abs": "https://arxiv.org/abs/2507.00613", "authors": ["Nuno Capit\u00e3o", "Yi Zhang", "Yidong Zhao", "Qian Tao"], "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping", "categories": ["eess.IV", "cs.AI"], "comment": "Submitted version. Accepted at MICCAI 2025", "summary": "Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac\nparametric mapping for characterizing myocardial tissue and diagnosing\ncardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)\nacquires 11 breath-hold baseline images with interleaved rest periods to ensure\nmapping accuracy. However, prolonged scanning can be challenging for patients\nwith poor breathholds, often leading to motion artifacts that degrade image\nquality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a\nsignal recovery model involving an iterative estimation process. Recent studies\nhave proposed deep-learning approaches for rapid $T_1$ mapping using shortened\nsequences to reduce acquisition time for patient comfort. Nevertheless,\nexisting methods overlook important physics constraints, limiting\ninterpretability and generalization. In this work, we present an accelerated,\nend-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary\nDifferential Equations (ODEs) to model temporal dynamics and address these\nchallenges. Our method achieves high-accuracy $T_1$ estimation from a sparse\nsubset of baseline images and ensures efficient null index estimation at test\ntime. Specifically, we develop a continuous-time LSTM-ODE model to enable\nselective Look-Locker (LL) data acquisition with arbitrary time lags.\nExperimental results show superior performance in $T_1$ estimation for both\nnative and post-contrast sequences and demonstrate the strong benefit of our\nphysics-based formulation over direct data-driven $T_1$ priors.", "AI": {"tldr": "A deep-learning framework using Physics-Informed Neural ODEs accelerates $T_1$ mapping, improving accuracy and reducing scan time while incorporating physics constraints.", "motivation": "Prolonged scanning and motion artifacts in conventional MOLLI methods, along with the lack of physics constraints in existing deep-learning approaches, necessitate a more efficient and interpretable solution.", "method": "The proposed framework uses Physics-Informed Neural ODEs, specifically a continuous-time LSTM-ODE model, to enable sparse data acquisition and efficient $T_1$ estimation.", "result": "The method achieves high-accuracy $T_1$ estimation from fewer baseline images and outperforms data-driven priors, demonstrating robustness in native and post-contrast sequences.", "conclusion": "The physics-based deep-learning approach offers a faster, more accurate, and interpretable solution for $T_1$ mapping, addressing limitations of conventional and existing deep-learning methods."}}
{"id": "2507.00068", "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "MANTA unifies visual and auditory inputs into a structured textual space for seamless processing with large language models, improving multi-modal learning by addressing alignment, synchronization, representation, and retrieval challenges.", "motivation": "Current multi-modal learning approaches treat modalities separately, leading to inconsistencies in representation and reasoning.", "method": "MANTA uses information-theoretic optimization for semantic alignment, adaptive temporal synchronization, hierarchical content representation, and context-aware retrieval, formalized in a mathematical framework.", "result": "MANTA improves state-of-the-art models by up to 22.6% in accuracy for Long Video QA, with notable gains in temporal reasoning (23.8%) and cross-modal understanding (25.1%).", "conclusion": "MANTA establishes new foundations for unifying multi-modal representations through structured text, with proven optimality and superior performance."}}
{"id": "2507.00181", "pdf": "https://arxiv.org/pdf/2507.00181", "abs": "https://arxiv.org/abs/2507.00181", "authors": ["Georgios P. Georgiou"], "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "categories": ["cs.AI"], "comment": null, "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "AI": {"tldr": "The study examines how ChatGPT affects students' cognitive engagement in academic writing, finding reduced engagement with AI assistance.", "motivation": "Concerns about LLMs reducing deep thinking and active learning in education prompted this investigation.", "method": "Experimental design with AI-assisted (ChatGPT) and control groups, using a cognitive engagement scale (CES-AI) to measure mental effort, attention, deep processing, and strategic thinking.", "result": "ChatGPT group showed significantly lower cognitive engagement scores than the control group, indicating potential cognitive offloading.", "conclusion": "AI tools like ChatGPT may hinder deep cognitive involvement; pedagogical strategies are needed to promote active engagement with AI-generated content."}}
{"id": "2507.00016", "pdf": "https://arxiv.org/pdf/2507.00016", "abs": "https://arxiv.org/abs/2507.00016", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "AI": {"tldr": "GRFT is an efficient fine-tuning method for large pre-trained models, reducing storage and computational costs by updating selective weight matrix rows/columns and incorporating regularization.", "motivation": "Fine-tuning large models is resource-intensive. Existing methods like GPS reduce parameters but increase resource demands. GRFT aims to address this inefficiency.", "method": "GRFT updates rows/columns of weight matrices with the highest sum of squared gradients, reducing storage and improving selection efficiency. It also uses regularization for better knowledge transfer.", "result": "GRFT outperforms GPS, Adapter Tuning, and LoRA, updating only 1.22% (FGVC) and 0.30% (VTAB) of parameters while achieving state-of-the-art performance.", "conclusion": "GRFT is a highly efficient and effective fine-tuning method, significantly reducing resource usage while maintaining performance."}}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211", "abs": "https://arxiv.org/abs/2505.16211", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "AI": {"tldr": "AudioTrust is a framework for evaluating the trustworthiness of Audio Large Language Models (ALLMs) across six dimensions, using a dataset of 4,420 samples and 9 audio-specific metrics.", "motivation": "Existing evaluation frameworks for ALLMs lack focus on audio-specific risks and scenarios, necessitating a dedicated trustworthiness assessment.", "method": "AudioTrust includes 18 experimental setups, a dataset of real-world audio/text samples, and 9 audio-specific metrics for evaluation.", "result": "The framework identifies trustworthiness boundaries and limitations of current ALLMs in high-risk audio scenarios.", "conclusion": "AudioTrust provides insights for secure deployment of future audio models and is publicly available."}}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "The paper addresses challenges in NLP for low-resource African languages, focusing on data quality, multilingual models, and creating labeled datasets for tasks like named entity recognition and machine translation.", "motivation": "To improve NLP performance for underrepresented African languages by addressing data noise, lack of labeled datasets, and leveraging multilingual models.", "method": "Analyze noise in corpora, curate high-quality data, adapt multilingual PLMs, and create labeled datasets for 21 African languages.", "result": "Demonstrates the importance of data quality over quantity for word embeddings and the effectiveness of multilingual PLMs for low-resource languages.", "conclusion": "The work highlights opportunities for adapting multilingual models to African languages and provides valuable labeled datasets to advance NLP research in underrepresented regions."}}
{"id": "2506.23986", "pdf": "https://arxiv.org/pdf/2506.23986", "abs": "https://arxiv.org/abs/2506.23986", "authors": ["Dake Guo", "Jixun Yao", "Linhan Ma", "He Wang", "Lei Xie"], "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}", "AI": {"tldr": "StreamFlow is a neural architecture for streaming speech generation using diffusion transformers (DiT) with block-wise attention masks, achieving high audio quality and low latency.", "motivation": "Traditional token-to-waveform methods struggle with streaming due to global receptive fields and degraded quality in token-by-token streaming.", "method": "Proposes StreamFlow with local block-wise receptive fields and hierarchical attention masks to manage long-sequence dependencies.", "result": "Matches non-streaming methods in performance, surpasses other streaming methods in quality, and achieves 180 ms first-packet latency.", "conclusion": "StreamFlow effectively addresses streaming challenges in speech generation, balancing quality and latency."}}
{"id": "2507.00660", "pdf": "https://arxiv.org/pdf/2507.00660", "abs": "https://arxiv.org/abs/2507.00660", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.", "AI": {"tldr": "Proposes MTCNet for 4D mitral valve ultrasound segmentation using semi-supervised learning, achieving high accuracy with sparse annotations.", "motivation": "Existing methods lack inter-phase dependency, limiting 4D mitral valve analysis due to motion artifacts and poor imaging quality.", "method": "Uses cross-phase motion-guided consistency learning and topology-guided correlation regularization to leverage structural correspondence.", "result": "Achieves superior performance (Dice: 87.30%, HD: 1.75mm) on a large 4D MV dataset.", "conclusion": "MTCNet effectively addresses challenges in 4D MV analysis and outperforms advanced methods."}}
{"id": "2507.00070", "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zu\u00f1iga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "The paper proposes a system using YOLOv7 and YOLOv8 for early detection of plant diseases, with YOLOv8 showing superior performance in accuracy and efficiency.", "motivation": "Early detection of plant diseases is crucial to prevent widespread damage and improve crop yield, but manual monitoring is inefficient.", "method": "The study employs transfer learning with YOLOv7 and YOLOv8 models, fine-tuned on plant leaf images to detect diseases like Powdery Mildew and Tomato mosaic virus.", "result": "YOLOv8 outperformed other methods with metrics like mAP (91.05), F1-score (89.40), Precision (91.22), and Recall (87.66).", "conclusion": "The system offers a scalable, automated solution for early disease detection, enhancing crop yield and supporting sustainable agriculture."}}
{"id": "2507.00205", "pdf": "https://arxiv.org/pdf/2507.00205", "abs": "https://arxiv.org/abs/2507.00205", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to npj Digital Medicine", "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "AI": {"tldr": "xHAIM enhances HAIM by adding explainability and task-specific data fusion, improving prediction accuracy (AUC from 79.9% to 90.3%) and clinical utility.", "motivation": "Address limitations of HAIM (task-agnostic data use, lack of explainability) to improve AI's clinical applicability.", "method": "Four-step framework: identify task-relevant data, generate patient summaries, improve predictive modeling, and provide clinical explanations.", "result": "xHAIM boosts AUC to 90.3% on chest pathology and operative tasks, outperforming HAIM (79.9%).", "conclusion": "xHAIM transforms AI into an explainable decision support tool, bridging AI advancements with clinical needs."}}
{"id": "2507.00018", "pdf": "https://arxiv.org/pdf/2507.00018", "abs": "https://arxiv.org/abs/2507.00018", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "AI": {"tldr": "The paper presents a unified framework linking Supervised Fine-Tuning (SFT) and preference learning in LLM post-training, showing they operate in the same optimal policy-reward subspace. It identifies a limitation in SFT's KL divergence term and proposes solutions like learning rate reduction and alternative SFT objectives, achieving significant performance gains.", "motivation": "To bridge the gap between SFT and preference learning in LLM post-training and address the limitations of conventional SFT methods.", "method": "Theoretical analysis and mathematical derivation to unify SFT and preference learning, followed by proposing learning rate reduction and alternative SFT objectives based on f-divergence functions.", "result": "Up to 25% relative gain and 6% absolute win rate improvement in instruction-following tasks.", "conclusion": "The unified framework and proposed methods enhance LLM post-training, with theoretical and experimental validation of the relationship between SFT and preference learning."}}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "LMs struggle with syntactic tasks like balanced parentheses. The study identifies faulty and sound mechanisms in LMs and introduces RASteer to boost reliable components, improving accuracy from 0% to ~100%.", "motivation": "To understand and mitigate persistent syntactic errors in LMs despite their advanced coding capabilities.", "method": "Analyze LM components (attention heads, FF neurons) and introduce RASteer to enhance reliable mechanisms.", "result": "RASteer improves balanced parentheses accuracy to ~100% and arithmetic reasoning by ~20%.", "conclusion": "Targeting reliable components in LMs can significantly enhance performance without harming general abilities."}}
{"id": "2507.00670", "pdf": "https://arxiv.org/pdf/2507.00670", "abs": "https://arxiv.org/abs/2507.00670", "authors": ["Jan Nikolas Morshuis", "Christian Schlarmann", "Thomas K\u00fcstner", "Christian F. Baumgartner", "Matthias Hein"], "title": "Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "In recent years, accelerated MRI reconstruction based on deep learning has\nled to significant improvements in image quality with impressive results for\nhigh acceleration factors. However, from a clinical perspective image quality\nis only secondary; much more important is that all clinically relevant\ninformation is preserved in the reconstruction from heavily undersampled data.\nIn this paper, we show that existing techniques, even when considering\nresampling for diffusion-based reconstruction, can fail to reconstruct small\nand rare pathologies, thus leading to potentially wrong diagnosis decisions\n(false negatives). To uncover the potentially missing clinical information we\npropose ``Semantically Diverse Reconstructions'' (\\SDR), a method which, given\nan original reconstruction, generates novel reconstructions with enhanced\nsemantic variability while all of them are fully consistent with the measured\ndata. To evaluate \\SDR automatically we train an object detector on the\nfastMRI+ dataset. We show that \\SDR significantly reduces the chance of\nfalse-negative diagnoses (higher recall) and improves mean average precision\ncompared to the original reconstructions. The code is available on\nhttps://github.com/NikolasMorshuis/SDR", "AI": {"tldr": "The paper introduces Semantically Diverse Reconstructions (SDR) to address false negatives in deep learning-based MRI reconstruction by enhancing semantic variability while maintaining data consistency.", "motivation": "Existing MRI reconstruction techniques may miss small or rare pathologies, leading to false-negative diagnoses. The goal is to ensure clinically relevant information is preserved.", "method": "Proposes SDR, a method generating diverse reconstructions from undersampled data, all consistent with measurements. Evaluated using an object detector on the fastMRI+ dataset.", "result": "SDR reduces false negatives (higher recall) and improves mean average precision compared to original reconstructions.", "conclusion": "SDR enhances diagnostic reliability by preserving clinical information in accelerated MRI reconstructions."}}
{"id": "2507.00153", "pdf": "https://arxiv.org/pdf/2507.00153", "abs": "https://arxiv.org/abs/2507.00153", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "categories": ["cs.CV"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "AI": {"tldr": "The paper proposes diffusion-based image augmentation to improve learning-based perception in underrepresented environments like snow, using foundation models and semantic filtering.", "motivation": "Outdoor robots face performance issues in dynamic environments (e.g., snow) due to underrepresented training data.", "method": "Uses diffusion-based image augmentation with vision foundation models and semantic segmentation to filter hallucinations.", "result": "Enables better representation of deployment environments (e.g., snow) in training data.", "conclusion": "Diffusion-based augmentation can generalize to other underrepresented environments like sand or volcanic terrains."}}
{"id": "2507.00218", "pdf": "https://arxiv.org/pdf/2507.00218", "abs": "https://arxiv.org/abs/2507.00218", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "title": "Learning for routing: A guided review of recent developments and future directions", "categories": ["cs.AI", "math.OC"], "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "AI": {"tldr": "A review of ML applications for NP-hard routing problems like TSP and VRP, proposing a taxonomy for ML-based methods and integrating OR with ML techniques.", "motivation": "Traditional methods for NP-hard routing problems are either computationally expensive (exact algorithms) or lack optimality guarantees (heuristics). ML offers potential improvements.", "method": "Proposes a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, integrating OR and ML techniques.", "result": "A structured framework for applying ML to routing problems, guiding future research and addressing emerging VRP variants.", "conclusion": "ML can enhance traditional OR methods for routing problems, with the proposed taxonomy serving as a foundation for future advancements."}}
{"id": "2507.00019", "pdf": "https://arxiv.org/pdf/2507.00019", "abs": "https://arxiv.org/abs/2507.00019", "authors": ["Minati Rath", "Hema Date"], "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "AI": {"tldr": "The paper proposes and compares three quantum-inspired data encoding strategies (ILS, GDS, CCVS) for classical ML models, focusing on reducing encoding time while maintaining accuracy.", "motivation": "To optimize quantum-inspired data transformations for classical ML by reducing encoding time and analyzing their impact on classification performance.", "method": "Three encoding strategies are evaluated: ILS (instance-level), GDS (global discrete), and CCVS (class-conditional). Their efficiency, correctness, and impact on model accuracy are assessed.", "result": "The study provides insights into trade-offs between encoding time, precision, and predictive performance for each strategy.", "conclusion": "The findings help optimize quantum-inspired data encoding for classical ML workflows."}}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "COLDSELECT improves prompt-based methods by jointly selecting verbalizers and instances, optimizing for diversity and minimal uncertainty, outperforming baselines in cold-start scenarios.", "motivation": "Existing prompt-based methods are sensitive to template, verbalizer, and instance selection, especially in cold-start settings. The dependency between instances and verbalizers is overlooked.", "method": "COLDSELECT maps PLM vocabulary and embeddings into a shared space, uses dimensionality reduction and clustering for efficient, diverse selection, optimizing for minimal uncertainty and maximal diversity.", "result": "Experiments on eight benchmarks show COLDSELECT reduces uncertainty and enhances generalization, outperforming baselines in verbalizer and few-shot instance selection.", "conclusion": "COLDSELECT effectively addresses the limitations of existing methods by modeling data diversity and dependencies, improving performance in cold-start scenarios."}}
{"id": "2507.00673", "pdf": "https://arxiv.org/pdf/2507.00673", "abs": "https://arxiv.org/abs/2507.00673", "authors": ["Abduz Zami", "Shadman Sobhan", "Rounaq Hossain", "Md. Sawran Sorker", "Mohiuddin Ahmed", "Md. Redwan Hossain"], "title": "Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays", "categories": ["eess.IV", "cs.CV"], "comment": "29 Pages", "summary": "Image segmentation plays a vital role in the medical field by isolating\norgans or regions of interest from surrounding areas. Traditionally,\nsegmentation models are trained on a specific organ or a disease, limiting\ntheir ability to handle other organs and diseases. At present, few advanced\nmodels can perform multi-organ or multi-disease segmentation, offering greater\nflexibility. Also, recently, prompt-based image segmentation has gained\nattention as a more flexible approach. It allows models to segment areas based\non user-provided prompts. Despite these advances, there has been no dedicated\nwork on prompt-based interactive multi-organ and multi-disease segmentation,\nespecially for Chest X-rays. This work presents two main contributions: first,\ngenerating doodle prompts by medical experts of a collection of datasets from\nmultiple sources with 23 classes, including 6 organs and 17 diseases,\nspecifically designed for prompt-based Chest X-ray segmentation. Second, we\nintroduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple\norgans and diseases from Chest X-rays. The model incorporates multi-stage\nfeature fusion, enabling it to combine features from various network layers for\nbetter spatial and semantic understanding, enhancing segmentation accuracy.\nCompared to existing pre-trained models for prompt-based image segmentation,\nour model scores well, providing a reliable solution for segmenting Chest\nX-rays based on user prompts.", "AI": {"tldr": "The paper introduces Prompt2SegCXR, a lightweight model for prompt-based multi-organ and multi-disease segmentation in Chest X-rays, using expert-generated doodle prompts and multi-stage feature fusion for improved accuracy.", "motivation": "Traditional segmentation models are limited to specific organs or diseases, lacking flexibility. Prompt-based methods offer adaptability, but no dedicated work exists for multi-organ and multi-disease segmentation in Chest X-rays.", "method": "The work involves creating expert-generated doodle prompts for 23 classes (6 organs, 17 diseases) and developing Prompt2SegCXR, a model with multi-stage feature fusion for enhanced segmentation.", "result": "Prompt2SegCXR outperforms existing pre-trained models in prompt-based segmentation, offering reliable Chest X-ray segmentation based on user prompts.", "conclusion": "The study successfully addresses the gap in prompt-based, interactive multi-organ and multi-disease segmentation for Chest X-rays, presenting a flexible and accurate solution."}}
{"id": "2507.00162", "pdf": "https://arxiv.org/pdf/2507.00162", "abs": "https://arxiv.org/abs/2507.00162", "authors": ["Yu Lu", "Yi Yang"], "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "categories": ["cs.CV"], "comment": "under review", "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "AI": {"tldr": "FreeLong and FreeLong++ are training-free frameworks to improve long video generation by balancing frequency distribution, enhancing temporal consistency and visual fidelity.", "motivation": "Existing video generation models degrade in quality for longer videos due to high-frequency distortion.", "method": "Proposes FreeLong (dual-branch) and FreeLong++ (multi-branch) to blend global low-frequency and local high-frequency features during denoising.", "result": "Outperforms previous methods, enabling longer videos (4x-8x native length) with better consistency and fidelity.", "conclusion": "FreeLong++ enhances existing models without training, supporting coherent multi-prompt and controllable video generation."}}
{"id": "2507.00417", "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO trains language models to reason like search algorithms using self-reflection, backtracking, and exploration, improving reasoning in non-reasoner models like Llama 3.", "motivation": "Enhance reasoning capabilities of non-reasoner models by teaching structured search behavior derived from Monte Carlo Tree Search.", "method": "Uses synthetic datasets from MCTS, converts search traces into natural language chain-of-thoughts, and applies RL with verifiable rewards.", "result": "Achieves significant performance gains (16.0% on MATH-500, 26.9% on AMC 2023, 20.0% on AIME 2024) on challenging problems.", "conclusion": "Search-inspired training is effective for instilling robust reasoning in open LLMs."}}
{"id": "2507.00020", "pdf": "https://arxiv.org/pdf/2507.00020", "abs": "https://arxiv.org/abs/2507.00020", "authors": ["Marcio Borges", "Felipe Pereira", "Michel Tosin"], "title": "Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods", "categories": ["cs.LG", "stat.ML"], "comment": "The main contribution of this work is to show the advantages of using\n  deep generative models like VAE to provide more flexible and versatile prior\n  distributions", "summary": "This study uses a Variational Autoencoder method to enhance the efficiency\nand applicability of Markov Chain Monte Carlo (McMC) methods by generating\nbroader-spectrum prior proposals. Traditional approaches, such as the\nKarhunen-Lo\\`eve Expansion (KLE), require previous knowledge of the covariance\nfunction, often unavailable in practical applications. The VAE framework\nenables a data-driven approach to flexibly capture a broader range of\ncorrelation structures in Bayesian inverse problems, particularly subsurface\nflow modeling. The methodology is tested on a synthetic groundwater flow\ninversion problem, where pressure data is used to estimate permeability fields.\nNumerical experiments demonstrate that the VAE-based parameterization achieves\ncomparable accuracy to KLE when the correlation length is known and outperforms\nKLE when the assumed correlation length deviates from the true value. Moreover,\nthe VAE approach significantly reduces stochastic dimensionality, improving\ncomputational efficiency. The results suggest that leveraging deep generative\nmodels in McMC methods can lead to more adaptable and efficient Bayesian\ninference in high-dimensional problems.", "AI": {"tldr": "A Variational Autoencoder (VAE) is used to improve Markov Chain Monte Carlo (McMC) methods by generating flexible prior proposals, outperforming traditional Karhunen-Lo\u00e8ve Expansion (KLE) in subsurface flow modeling.", "motivation": "Traditional methods like KLE require prior knowledge of covariance functions, which is often unavailable. The VAE offers a data-driven alternative to capture diverse correlation structures.", "method": "The study employs a VAE framework to generate prior proposals for McMC, tested on a synthetic groundwater flow inversion problem.", "result": "VAE matches KLE's accuracy when correlation length is known and outperforms KLE when assumptions are incorrect. It also reduces stochastic dimensionality, enhancing efficiency.", "conclusion": "Deep generative models like VAE can make Bayesian inference in high-dimensional problems more adaptable and efficient."}}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "The paper proposes a RAG pipeline with question decomposition and reranking to improve retrieval and answer accuracy for multi-hop questions.", "motivation": "Standard RAG struggles with multi-hop questions due to distributed facts across documents.", "method": "Decompose queries into sub-questions, retrieve passages for each, merge and rerank the pool.", "result": "Improves retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG.", "conclusion": "Question decomposition and reranking enhance RAG for multi-hop questions without extra training."}}
{"id": "2507.00743", "pdf": "https://arxiv.org/pdf/2507.00743", "abs": "https://arxiv.org/abs/2507.00743", "authors": ["An Le", "Nehal Mehta", "William Freeman", "Ines Nagel", "Melanie Tran", "Anna Heinke", "Akshay Agnihotri", "Lingyun Cheng", "Dirk-Uwe Bartsch", "Hung Nguyen", "Truong Nguyen", "Cheolhong An"], "title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": null, "summary": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "AI": {"tldr": "A deep learning model using ResNet18 and tunable wavelet units (OrthLatt-UwU and PR-Relax-UwU) was developed to classify ERM removal surgery types from postoperative OCT scans, achieving up to 78% accuracy, outperforming human graders.", "motivation": "To improve the accuracy of classifying ERM removal surgery types (ILM removal or ERM-alone removal) from postoperative OCT scans, aiding clinical decision-making.", "method": "The model uses ResNet18 CNN with postoperative OCT scans as inputs, enhanced by tunable wavelet units (OrthLatt-UwU and PR-Relax-UwU) for adaptive filtering during training.", "result": "The model achieved 78% accuracy with PR-Relax-UwU, outperforming human graders (50% accuracy) and original scans (66% accuracy).", "conclusion": "The study demonstrates the potential of CNN-based models with tunable wavelets to enhance clinical decision-making in ERM surgery classification."}}
{"id": "2507.00170", "pdf": "https://arxiv.org/pdf/2507.00170", "abs": "https://arxiv.org/abs/2507.00170", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "M\u00e9lisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Lalibert\u00e9"], "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "comment": null, "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "AI": {"tldr": "SelvaBox is the largest open-access dataset for tropical tree crown detection, with 83,000 labeled crowns, improving detection accuracy and zero-shot performance.", "motivation": "Tropical tree crown detection is vital for ecosystem studies but lacks annotated datasets, hindering robust model development.", "method": "Introduces SelvaBox, a large dataset, and benchmarks models using high-resolution drone imagery and multi-resolution training.", "result": "Higher-resolution inputs boost accuracy, and SelvaBox-trained models achieve competitive zero-shot performance. Joint training yields top-ranking detectors.", "conclusion": "SelvaBox advances tropical tree crown detection, offering public resources for further research."}}
{"id": "2507.00432", "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "Most math-performing LLMs fail to generalize gains to other domains; RL-tuned models outperform SFT-tuned ones in cross-domain tasks.", "motivation": "To determine if improvements in math benchmarks reflect broader problem-solving ability or narrow overfitting.", "method": "Evaluated 20+ reasoning-tuned models across diverse tasks and conducted controlled experiments on Qwen3-14B models with different tuning methods.", "result": "RL-tuned models generalize better, while SFT-tuned models lose general capabilities due to representation drift.", "conclusion": "Suggests rethinking post-training methods, especially reliance on SFT-distilled data, to enhance reasoning models."}}
{"id": "2507.00022", "pdf": "https://arxiv.org/pdf/2507.00022", "abs": "https://arxiv.org/abs/2507.00022", "authors": ["Zehao Wang"], "title": "GLU Attention Improve Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "AI": {"tldr": "GLU Attention, a novel attention mechanism incorporating nonlinearity via Gated Linear Units, enhances model performance and convergence speed without extra parameters or computational costs.", "motivation": "To improve neural network performance by introducing nonlinearity into attention mechanisms, leveraging the success of Gated Linear Units (GLU).", "method": "Introduces GLU Attention, which applies nonlinearity to attention values, and integrates it with existing technologies like Flash Attention and RoPE.", "result": "GLU Attention improves performance and convergence speed in text and vision tasks, with no added parameters or significant computational overhead.", "conclusion": "GLU Attention is a lightweight, effective enhancement for attention mechanisms, compatible with various existing technologies and open-sourced for broader use."}}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojt\u011bch Lanz", "Jan Haji\u010d jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "The paper explores the idea of centonisation in Gregorian chant melodies, using unsupervised segmentation with Pitman-Yor models to achieve state-of-the-art mode classification, but finds the results don't fully align with traditional centonisation theory.", "motivation": "To empirically test the centonisation theory in Gregorian chant by leveraging memory-based segmentation, given its historical memorization and observed melodic re-use.", "method": "Uses nested hierarchical Pitman-Yor language models for unsupervised segmentation of chant melodies, modeling a monk's memorization process.", "result": "Achieves top performance in mode classification and identifies formulaic areas in melodies, but the segmentation doesn't match traditional centonisation.", "conclusion": "Memory-optimal segmentation supports mode classification but doesn't validate centonisation, suggesting the theory may not fully explain chant construction."}}
{"id": "2507.00780", "pdf": "https://arxiv.org/pdf/2507.00780", "abs": "https://arxiv.org/abs/2507.00780", "authors": ["Fei Yuhuan", "Sun Xufei", "Zang Ran", "Wang Gengchen", "Su Meng", "Liu Fenghao"], "title": "Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n", "categories": ["eess.IV", "cs.CV"], "comment": "in Chinese language", "summary": "Early detection and diagnosis of diabetic retinopathy is one of the current\nresearch focuses in ophthalmology. However, due to the subtle features of\nmicro-lesions and their susceptibility to background interference, ex-isting\ndetection methods still face many challenges in terms of accuracy and\nrobustness. To address these issues, a lightweight and high-precision detection\nmodel based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a\nnew dynamic convolution KWConv and C2f-KW module are designed to improve the\nbackbone network, enhancing the model's ability to perceive micro-lesions.\nSecondly, a fea-ture-focused diffusion pyramid network FDPN is designed to\nfully integrate multi-scale context information, further improving the model's\nability to perceive micro-lesions. Finally, a lightweight shared detection head\nGSDHead is designed to reduce the model's parameter count, making it more\ndeployable on re-source-constrained devices. Experimental results show that\ncompared with the base model YOLOv8n, the improved model reduces the parameter\ncount by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by\n7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and\nYOLOv10n, YOLO-KFG demonstrates significant advantages in both detection\naccuracy and efficiency.", "AI": {"tldr": "Proposes YOLO-KFG, a lightweight and high-precision model for diabetic retinopathy detection, improving YOLOv8n with dynamic convolution, feature diffusion, and a shared detection head.", "motivation": "Addresses challenges in detecting diabetic retinopathy due to subtle micro-lesions and background interference, aiming for better accuracy and robustness.", "method": "Enhances YOLOv8n with KWConv, C2f-KW, FDPN for multi-scale context, and GSDHead for reduced parameters.", "result": "Reduces parameters by 20.7%, increases mAP@0.5 by 4.1%, and improves recall by 7.9% over YOLOv8n, outperforming YOLOv5n and YOLOv10n.", "conclusion": "YOLO-KFG achieves superior accuracy and efficiency for diabetic retinopathy detection, suitable for resource-constrained devices."}}
{"id": "2507.00182", "pdf": "https://arxiv.org/pdf/2507.00182", "abs": "https://arxiv.org/abs/2507.00182", "authors": ["J. I. Ru\u00edz", "A. M\u00e9ndez", "E. Rodr\u00edguez"], "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "categories": ["cs.CV"], "comment": null, "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "AI": {"tldr": "A novel GNN-based Deep Learning architecture for identifying plant components in LiDAR 3D Point Cloud data, outperforming existing methods with over 80% IoU accuracy.", "motivation": "Traditional methods using 2D imaging, 3D reconstructions, and CNNs have limitations in processing 3D data and identifying plant components, prompting the need for a more effective solution.", "method": "Proposes a GNN-based architecture using PCA for feature enhancement, KNN for edge establishment, Edge-Conv layers for feature augmentation, and GAT for classifying plant components.", "result": "Achieves segmentation accuracy above 80% in IoU average, surpassing other point cloud-based models.", "conclusion": "The graph-based deep learning approach effectively enhances the identification of individual plant components in 3D point cloud data."}}
{"id": "2507.00557", "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "categories": ["cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "AI": {"tldr": "The paper introduces a two-dimensional cell-jump move and an extended local search framework (2d-LS) for SMT-NRA, integrating MCSAT and sample-cell projection to enhance efficiency. A hybrid framework combining these methods shows improved performance.", "motivation": "To advance local search for SMT-NRA by improving efficiency and effectiveness through novel moves and integration of existing frameworks.", "method": "Proposes a 2d-cell-jump move, extends the LS framework to 2d-LS, integrates MCSAT with sample-cell projection, and designs a hybrid framework combining MCSAT, 2d-LS, and OpenCAD.", "result": "Experimental results show improved local search performance, validating the effectiveness of the proposed methods.", "conclusion": "The introduced techniques and hybrid framework significantly enhance the efficiency of local search for SMT-NRA."}}
{"id": "2507.00024", "pdf": "https://arxiv.org/pdf/2507.00024", "abs": "https://arxiv.org/abs/2507.00024", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "AI": {"tldr": "AIMatDesign, a reinforcement learning framework, addresses challenges in high-dimensional materials design by augmenting data, refining predictions with LLMs, and incorporating expert knowledge, outperforming traditional methods.", "motivation": "The need to reconcile high-dimensional materials composition space with limited experimental data and improve reliability and expert knowledge integration in machine learning-driven inverse design.", "method": "Uses reinforcement learning with difference-based algorithms for data augmentation, LLM-guided refinement for prediction correction, and knowledge-based reward functions.", "result": "Outperforms traditional methods in efficiency, convergence, and success rates; successfully synthesized a high-performance Zr-based alloy matching predictions.", "conclusion": "AIMatDesign demonstrates reliability and potential for closed-loop materials discovery, accurately capturing material property trends."}}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL is a causal prompting framework for Implicit Sentiment Analysis (ISA) that improves accuracy and robustness by integrating front-door adjustment into chain-of-thought reasoning.", "motivation": "Addressing biases and spurious correlations in prompting-based ISA methods by ensuring causal validity in reasoning paths.", "method": "CAPITAL decomposes causal effects into prompt influence and reasoning impact, using encoder-based clustering, NWGM approximation, and contrastive learning.", "result": "Outperforms baselines in accuracy and robustness on ISA datasets, especially under adversarial conditions.", "conclusion": "CAPITAL provides a principled, bias-aware approach for causal inference in LLM prompting, enhancing sentiment reasoning."}}
{"id": "2507.00831", "pdf": "https://arxiv.org/pdf/2507.00831", "abs": "https://arxiv.org/abs/2507.00831", "authors": ["Sachin Maheshwari", "Mike Smart", "Himadri Singh Raghav", "Themis Prodromakis", "Alexander Serb"], "title": "Adiabatic Capacitive Neuron: An Energy-Efficient Functional Unit for Artificial Neural Networks", "categories": ["eess.IV"], "comment": "12 pages, 18 figures, 7 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "This paper introduces a new, highly energy-efficient, Adiabatic Capacitive\nNeuron (ACN) hardware implementation of an Artificial Neuron (AN) with improved\nfunctionality, accuracy, robustness and scalability over previous work. The\npaper describes the implementation of a \\mbox{12-bit} single neuron, with\npositive and negative weight support, in an $\\mathbf{0.18\\mu m}$ CMOS\ntechnology. The paper also presents a new Threshold Logic (TL) design for a\nbinary AN activation function that generates a low symmetrical offset across\nthree process corners and five temperatures between $-55^o$C and $125^o$C.\nPost-layout simulations demonstrate a maximum rising and falling offset voltage\nof 9$mV$ compared to conventional TL, which has rising and falling offset\nvoltages of 27$mV$ and 5$mV$ respectively, across temperature and process.\nMoreover, the proposed TL design shows a decrease in average energy of 1.5$\\%$\nat the SS corner and 2.3$\\%$ at FF corner compared to the conventional TL\ndesign. The total synapse energy saving for the proposed ACN was above 90$\\%$\n(over 12x improvement) when compared to a non-adiabatic CMOS Capacitive Neuron\n(CCN) benchmark for a frequency ranging from 500$kHz$ to 100$MHz$. A\n1000-sample Monte Carlo simulation including process variation and mismatch\nconfirms the worst-case energy savings of $\\>$90$\\%$ compared to CCN in the\nsynapse energy profile. Finally, the impact of supply voltage scaling shows\nconsistent energy savings of above 90$\\%$ (except all zero inputs) without loss\nof functionality.", "AI": {"tldr": "The paper introduces an Adiabatic Capacitive Neuron (ACN) with improved energy efficiency, functionality, and robustness over previous designs, demonstrating significant energy savings and performance enhancements.", "motivation": "To address the limitations of conventional artificial neuron designs, such as high energy consumption and lack of robustness, by proposing a more efficient and scalable hardware implementation.", "method": "The ACN is implemented in 0.18\u03bcm CMOS technology, featuring a 12-bit single neuron with positive/negative weight support and a new Threshold Logic (TL) design for activation. Post-layout simulations and Monte Carlo analysis validate performance.", "result": "The ACN achieves over 90% energy savings (12x improvement) compared to non-adiabatic benchmarks, with consistent performance across process corners, temperatures, and supply voltage scaling.", "conclusion": "The ACN design offers a highly energy-efficient, robust, and scalable solution for artificial neurons, with significant improvements over existing methods."}}
{"id": "2507.00224", "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "AI": {"tldr": "The paper introduces FiboSB, a challenging 6D pose video dataset for collaborative K-12 education, evaluates existing methods, and improves performance by fine-tuning YOLO11-x.", "motivation": "Existing systems struggle to capture real-world interactions in collaborative educational settings, which 6D pose estimation could address.", "method": "The authors created FiboSB, a dataset of groups solving tasks with small cubes and a scale, tested four 6D pose methods, and fine-tuned YOLO11-x for better performance.", "result": "Current 6D pose methods failed due to object detection issues; fine-tuned YOLO11-x achieved an mAP_50 of 0.898.", "conclusion": "FiboSB and the improved method lay the foundation for better 6D pose estimation in collaborative contexts."}}
{"id": "2507.00726", "pdf": "https://arxiv.org/pdf/2507.00726", "abs": "https://arxiv.org/abs/2507.00726", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "categories": ["cs.AI", "cs.LG"], "comment": "27 pages", "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "AI": {"tldr": "RL for LLMs in chess shows promise with dense rewards but plateaus below expert levels due to pretrained model limitations.", "motivation": "Explore if LLMs can develop strategic reasoning in chess using RL, leveraging dense rewards from a chess-pretrained network.", "method": "Use a chess-pretrained action-value network for dense rewards (knowledge distillation) and compare with sparse binary rewards.", "result": "Dense rewards outperform sparse ones, but models plateau below expert levels, likely due to pretrained model deficits.", "conclusion": "RL alone may not fully overcome pretrained LLMs' chess understanding deficits; further research is needed."}}
{"id": "2507.00025", "pdf": "https://arxiv.org/pdf/2507.00025", "abs": "https://arxiv.org/abs/2507.00025", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by TPAMI 2025", "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "AI": {"tldr": "FNSDA is a parameter-efficient method for generalizing to new dynamics by adapting in Fourier space, outperforming existing methods with fewer parameters.", "motivation": "Current deep learning approaches for modeling dynamics struggle with generalization to unseen systems governed by the same dynamics but differing in environmental characteristics.", "method": "FNSDA identifies shareable dynamics via automatic partition in Fourier modes and adjusts modes for new environments using low-dimensional latent parameters.", "result": "FNSDA achieves superior or competitive generalization performance on four dynamic systems with reduced parameter cost.", "conclusion": "FNSDA offers an efficient and effective solution for generalizing to new dynamics, with potential applications in complex physical systems."}}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "Simple supervision improves language model alignment with diverse population groups, evaluated across datasets and models.", "motivation": "Accurate prediction of subjective question responses across diverse groups is valuable.", "method": "Use of simple supervision to align language models with diverse groups, evaluated over multiple datasets and models.", "result": "Improved alignment with diverse groups, with findings on when to use the approach.", "conclusion": "The approach is simple, general, and provides a benchmark for future research."}}
{"id": "2507.00832", "pdf": "https://arxiv.org/pdf/2507.00832", "abs": "https://arxiv.org/abs/2507.00832", "authors": ["Jisoo Kim", "Chu-Hsuan Lin", "Alberto Ceballos-Arroyo", "Ping Liu", "Huaizu Jiang", "Shrikanth Yadav", "Qi Wan", "Lei Qin", "Geoffrey S Young"], "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Introduction: Deep learning (DL) models can help detect intracranial\naneurysms on CTA, but high false positive (FP) rates remain a barrier to\nclinical translation, despite improvement in model architectures and strategies\nlike detection threshold tuning. We employed an automated, anatomy-based,\nheuristic-learning hybrid artery-vein segmentation post-processing method to\nfurther reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D\nconvolutional neural network-transformer hybrid (3D-CNN-TR), were trained with\n1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143\nheld-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and\ncavernous venous sinus (CVS) segmentation masks were applied to remove possible\nFPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)\nvein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more\nthan artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79\nfalse-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were\ncommonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;\n3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular\n(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing\nCPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without\nreducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from\n1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable\npost-processing can improve DL-based aneurysm detection model performance. More\nbroadly, automated, domain-informed, hybrid heuristic-learning processing holds\npromise for improving the performance and clinical acceptance of aneurysm\ndetection models.", "AI": {"tldr": "An anatomy-based post-processing method reduces false positives in DL models for intracranial aneurysm detection without losing true positives.", "motivation": "High false positive rates in DL models for aneurysm detection hinder clinical translation, despite advancements in model architectures and tuning.", "method": "Used hybrid artery-vein segmentation post-processing on outputs from two DL models (CPM-Net and 3D-CNN-TR) trained on open-source CTAs. Applied five masking methods to remove FPs.", "result": "Method 5 reduced FPs by 70.6% for CPM-Net and 51.6% for 3D-CNN-TR without reducing TPs, improving FP/case rates significantly.", "conclusion": "Anatomy-based post-processing enhances DL model performance, suggesting hybrid heuristic-learning methods can boost clinical acceptance of aneurysm detection models."}}
{"id": "2507.00243", "pdf": "https://arxiv.org/pdf/2507.00243", "abs": "https://arxiv.org/abs/2507.00243", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "title": "VOCAL: Visual Odometry via ContrAstive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "AI": {"tldr": "VOCAL is a novel visual odometry framework using contrastive learning and Bayesian inference to improve interpretability and flexibility in camera state estimation.", "motivation": "Existing learning-based VO methods rely on rigid geometric assumptions, lacking interpretability and theoretical grounding in data-driven frameworks.", "method": "VOCAL reimagines VO as a label ranking challenge, integrating Bayesian inference and representation learning to align visual features with camera states.", "result": "Evaluations on the KITTI dataset show VOCAL enhances interpretability and flexibility in spatial intelligence.", "conclusion": "VOCAL advances VO by providing more general and explainable solutions, addressing limitations of current methods."}}
{"id": "2507.00810", "pdf": "https://arxiv.org/pdf/2507.00810", "abs": "https://arxiv.org/abs/2507.00810", "authors": ["Qing Xu", "Xiaohua Xuan"], "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "AI": {"tldr": "An improved numerical algorithm for solving minimax problems using nonsmooth optimization, quadratic programming, and iterative methods, with proven convergence under mild assumptions.", "motivation": "Address the need for efficient solutions to minimax problems in fields like robust optimization and imbalanced learning.", "method": "Combines nonsmooth optimization, quadratic programming, and iterative processes, with assumptions like gradient continuity and boundedness.", "result": "Provides a rigorous proof of convergence for the proposed algorithm.", "conclusion": "The algorithm is versatile and applicable in various domains, offering a reliable solution for minimax problems."}}
{"id": "2507.00026", "pdf": "https://arxiv.org/pdf/2507.00026", "abs": "https://arxiv.org/abs/2507.00026", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "AI": {"tldr": "ROSE is a framework using multi-objective reinforcement learning to generate diverse and context-rich adversarial prompts for evaluating LLM safety, outperforming existing methods.", "motivation": "Current safety evaluations for LLMs are static and labor-intensive, failing to adapt to rapid advancements. Automated methods lack topic diversity and real-world alignment.", "method": "ROSE employs multi-objective reinforcement learning to fine-tune an adversarial LLM, generating diverse and contextually rich prompts.", "result": "ROSE outperforms existing methods in uncovering safety vulnerabilities in LLMs, improving integrated evaluation metrics.", "conclusion": "ROSE advances practical, reality-oriented safety evaluation for LLMs, addressing limitations of current approaches."}}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "Open LLM benchmarks like HELM and BIG-bench enable fair LM comparisons but are vulnerable to exploitation, as shown by 'cheating' models that rank highly without real-world utility.", "motivation": "To highlight the pitfalls of open benchmarks, which can be gamed by models fine-tuned on public test sets, undermining their reliability.", "method": "Constructed smaller variants of BART, T5, and GPT-2 fine-tuned on public test sets to exploit open benchmarks like HELM.", "result": "These 'cheating' models achieved top rankings on HELM despite poor generalization, revealing flaws in open benchmarks.", "conclusion": "Open benchmarks alone are insufficient; private/dynamic benchmarks and reevaluated practices are needed for trustworthy LM assessments."}}
{"id": "2507.00903", "pdf": "https://arxiv.org/pdf/2507.00903", "abs": "https://arxiv.org/abs/2507.00903", "authors": ["Andreea Bianca Popescu", "Andreas Seitz", "Heiko Mahrholdt", "Jens Wetzl", "Athira Jacob", "Lucian Mihai Itu", "Constantin Suciu", "Teodora Chitiboi"], "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "This work has been submitted for consideration at European Radiology\n  (Springer). Upon acceptance, this preprint will be updated with the journal\n  reference", "summary": "Objectives Parametric tissue mapping enables quantitative cardiac tissue\ncharacterization but is limited by inter-observer variability during manual\ndelineation. Traditional approaches relying on average relaxation values and\nsingle cutoffs may oversimplify myocardial complexity. This study evaluates\nwhether deep learning (DL) can achieve segmentation accuracy comparable to\ninter-observer variability, explores the utility of statistical features beyond\nmean T1/T2 values, and assesses whether machine learning (ML) combining\nmultiple features enhances disease detection. Materials & Methods T1 and T2\nmaps were manually segmented. The test subset was independently annotated by\ntwo observers, and inter-observer variability was assessed. A DL model was\ntrained to segment left ventricle blood pool and myocardium. Average (A), lower\nquartile (LQ), median (M), and upper quartile (UQ) were computed for the\nmyocardial pixels and employed in classification by applying cutoffs or in ML.\nDice similarity coefficient (DICE) and mean absolute percentage error evaluated\nsegmentation performance. Bland-Altman plots assessed inter-user and\nmodel-observer agreement. Receiver operating characteristic analysis determined\noptimal cutoffs. Pearson correlation compared features from model and manual\nsegmentations. F1-score, precision, and recall evaluated classification\nperformance. Wilcoxon test assessed differences between classification methods,\nwith p < 0.05 considered statistically significant. Results 144 subjects were\nsplit into training (100), validation (15) and evaluation (29) subsets.\nSegmentation model achieved a DICE of 85.4%, surpassing inter-observer\nagreement. Random forest applied to all features increased F1-score (92.7%, p <\n0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining\nmultiple features with ML improves disease detection.", "AI": {"tldr": "Deep learning improves cardiac tissue segmentation accuracy, surpassing inter-observer variability, and combining multiple features with machine learning enhances disease detection.", "motivation": "To address limitations of manual delineation and oversimplification in traditional parametric tissue mapping by leveraging deep learning and machine learning for better accuracy and disease detection.", "method": "T1 and T2 maps were manually segmented, and a DL model was trained. Statistical features (average, quartiles) were used for classification. Performance was evaluated using DICE, Bland-Altman plots, and ROC analysis.", "result": "DL achieved 85.4% DICE, surpassing inter-observer agreement. Random forest with multiple features increased F1-score to 92.7%.", "conclusion": "DL enables accurate T1/T2 map segmentation, and ML with multiple features improves disease detection."}}
{"id": "2507.00248", "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "A lightweight DNN framework for real-time sign language recognition, addressing data scarcity and computational costs, achieving 92% accuracy with low latency.", "motivation": "To overcome challenges in sign language recognition like data scarcity, high computational costs, and frame rate discrepancies.", "method": "Encodes sign-specific parameters into vectorized inputs, uses MediaPipe for landmark extraction, and employs a lightweight DNN optimized for edge devices.", "result": "92% accuracy in isolated sign recognition with <10ms latency on edge devices, integrated into 'slait ai' web app.", "conclusion": "The framework is effective for real-time sign language recognition, offering high accuracy and low latency."}}
{"id": "2507.00841", "pdf": "https://arxiv.org/pdf/2507.00841", "abs": "https://arxiv.org/abs/2507.00841", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "categories": ["cs.AI", "cs.CR"], "comment": "12 pages", "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "AI": {"tldr": "The paper addresses security risks in multimodal foundation model-driven agents, proposing a risk discrimination mechanism and automated assessment to mitigate jailbreak vulnerabilities.", "motivation": "Multimodal agents are increasingly used in sensitive scenarios but face jailbreak risks, where attackers bypass constraints to trigger unauthorized actions. Existing security measures lack effectiveness in complex interactions.", "method": "The study constructs a risk discrimination mechanism using behavioral sequence information and designs an automated assessment scheme based on a large language model.", "result": "Preliminary validation shows improved recognition of risky behaviors and reduced jailbreak probability in high-risk tasks.", "conclusion": "The work provides insights for security risk modeling and protection in multimodal intelligent agent systems."}}
{"id": "2507.00028", "pdf": "https://arxiv.org/pdf/2507.00028", "abs": "https://arxiv.org/abs/2507.00028", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "AI": {"tldr": "HiT-JEPA is a hierarchical framework for multi-scale urban trajectory representation, combining fine-grained details and high-level semantics.", "motivation": "Existing methods fail to integrate fine-grained details and high-level summaries in trajectory data, limiting their ability to capture both local and global patterns.", "method": "HiT-JEPA uses a three-layer hierarchy to capture point-level details, intermediate patterns, and high-level abstractions in trajectories.", "result": "Experiments show HiT-JEPA produces richer, multi-scale representations, improving trajectory similarity computation.", "conclusion": "HiT-JEPA effectively unifies local and global trajectory semantics, advancing urban trajectory analysis."}}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "Jo\u00e3o Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "The paper proposes a modular pipeline for managing ads in RAG-based conversational systems, focusing on seamless integration and detection using synthetic data and classifier-guided strategies.", "motivation": "The integration of ads in generative search systems blurs boundaries between content and promotions, raising transparency and trust concerns.", "method": "A pipeline with an ad-rewriter and ad-classifier is developed, using synthetic data for training and two ad-integration strategies: fine-tuning and best-of-N sampling.", "result": "The ad-classifier shows robust detection performance, and classifier-guided optimization improves ad stealth.", "conclusion": "The work contributes a framework for ad-aware generative systems and robust ad classifiers, balancing commercial and user experience needs."}}
{"id": "2507.00983", "pdf": "https://arxiv.org/pdf/2507.00983", "abs": "https://arxiv.org/abs/2507.00983", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "title": "DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of brain tumors in MRI scans is essential for reliable\nclinical diagnosis and effective treatment planning. Recently, diffusion models\nhave demonstrated remarkable effectiveness in image generation and segmentation\ntasks. This paper introduces a novel approach to corrective segmentation based\non diffusion models. We propose DMCIE (Diffusion Model with Concatenation of\nInputs and Errors), a novel framework for accurate brain tumor segmentation in\nmulti-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation\nmask, from which an error map is generated by identifying the differences\nbetween the prediction and the ground truth. The error map, concatenated with\nthe original MRI images, are used to guide a diffusion model. Using multimodal\nMRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation\naccuracy by focusing on misclassified regions, guided by the original inputs.\nEvaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art\ndiffusion-based segmentation methods, achieving a Dice Score of 93.46 and an\nHD95 of 5.94 mm. These results highlight the effectiveness of error-guided\ndiffusion in producing precise and reliable brain tumor segmentations.", "AI": {"tldr": "DMCIE, a diffusion model framework, improves brain tumor segmentation in MRI by using error maps and original inputs, achieving high accuracy on BraTS2020.", "motivation": "Accurate brain tumor segmentation in MRI is crucial for diagnosis and treatment. Diffusion models show promise for such tasks.", "method": "DMCIE combines a 3D U-Net for initial segmentation with a diffusion model guided by error maps and original MRI inputs.", "result": "DMCIE achieves a Dice Score of 93.46 and HD95 of 5.94 mm on BraTS2020, outperforming other methods.", "conclusion": "Error-guided diffusion models like DMCIE enhance segmentation accuracy, proving effective for brain tumor analysis."}}
{"id": "2507.00253", "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "AI": {"tldr": "GazeTarget360 is a novel system for 360-degree gaze target estimation in images, addressing limitations of prior methods by integrating conditional inference engines for accurate predictions in unseen scenarios.", "motivation": "Understanding human gaze targets is critical for tasks like attention estimation and movement anticipation in human-robot interactions, but existing methods fail in out-of-frame or non-camera-facing scenarios.", "method": "The system combines an eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion decoder to estimate gaze targets in generalized visual scenes.", "result": "Cross-validation shows GazeTarget360 provides accurate and reliable gaze target predictions, outperforming prior methods in efficiency and deployability.", "conclusion": "GazeTarget360 is a pioneering, efficient system for gaze target prediction from realistic camera footage, with publicly available source code."}}
{"id": "2507.00951", "pdf": "https://arxiv.org/pdf/2507.00951", "abs": "https://arxiv.org/abs/2507.00951", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "categories": ["cs.AI"], "comment": null, "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "AI": {"tldr": "The paper explores the limitations of current AI models in achieving AGI, emphasizing the need for modular reasoning, memory, and multi-agent coordination. It introduces Agentic RAG frameworks and discusses generalization strategies, VLMs, and the integration of memory and reasoning as key to AGI.", "motivation": "To address the gap between current AI capabilities and AGI by synthesizing insights from AI, cognitive neuroscience, and psychology, and proposing pathways for more adaptive and domain-agnostic intelligence.", "method": "Cross-disciplinary synthesis of AGI development, analyzing architectural and cognitive foundations, and exploring frameworks like Agentic RAG, VLMs, and neurosymbolic systems.", "result": "Identifies modular reasoning, memory integration, and multi-agent coordination as critical for AGI, along with the potential of Agentic RAG and VLMs for adaptive behavior.", "conclusion": "True AGI requires integration of memory, reasoning, and modular components, with ongoing challenges in scientific, technical, and ethical domains."}}
{"id": "2507.00029", "pdf": "https://arxiv.org/pdf/2507.00029", "abs": "https://arxiv.org/abs/2507.00029", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "AI": {"tldr": "LoRA-Mixer integrates LoRA experts into MoE for efficient multi-task adaptation in LLMs, improving performance with fewer parameters.", "motivation": "Existing methods for combining LoRA with MoE in LLMs are inefficient, either swapping entire layers or adding parallel branches, reducing parameter efficiency and task fidelity.", "method": "LoRA-Mixer replaces projection matrices in attention modules with dynamically routed, task-specific LoRA experts, supporting joint optimization or frozen pre-trained modules. It uses a hard-soft routing strategy and a Specialization Balance Loss for robust training.", "result": "LoRA-Mixer improves performance by up to 7.61% on benchmarks like GSM8K, HumanEval, and MedQA, outperforming state-of-the-art methods with 48% fewer parameters.", "conclusion": "LoRA-Mixer is a lightweight, efficient framework for multi-task adaptation in LLMs, achieving strong performance with reduced computational overhead."}}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar is a framework for evaluating continual learning in multilingual and multi-domain ASR, using real-world data from 22 languages and 208 districts in India. It introduces novel scenarios like LIDIL and highlights the inconsistency of current CL methods.", "motivation": "To address the lack of realistic evaluation frameworks for continual learning in multilingual and multi-domain ASR, Nirantar leverages real-world data to reflect dynamic, non-uniform shifts.", "method": "Nirantar uses 3250 hours of human-transcribed speech, including 1720 newly introduced hours, to evaluate CL methods across LIL, DIL, and LIDIL scenarios.", "result": "Existing CL methods perform inconsistently, showing no single approach is robust across all scenarios.", "conclusion": "Nirantar provides a realistic testbed for CL research, emphasizing the need for more robust strategies."}}
{"id": "2507.00993", "pdf": "https://arxiv.org/pdf/2507.00993", "abs": "https://arxiv.org/abs/2507.00993", "authors": ["Qingqiu Li", "Runtian Yuan", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "title": "Advancing Lung Disease Diagnosis in 3D CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "To enable more accurate diagnosis of lung disease in chest CT scans, we\npropose a straightforward yet effective model. Firstly, we analyze the\ncharacteristics of 3D CT scans and remove non-lung regions, which helps the\nmodel focus on lesion-related areas and reduces computational cost. We adopt\nResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss\nto mitigate class imbalance, especially for the underrepresented squamous cell\ncarcinoma category. Our model achieves a Macro F1 Score of 0.80 on the\nvalidation set of the Fair Disease Diagnosis Challenge, demonstrating its\nstrong performance in distinguishing between different lung conditions.", "AI": {"tldr": "A model for accurate lung disease diagnosis in CT scans by focusing on lung regions, using ResNeSt50 and weighted loss to address class imbalance, achieving a Macro F1 Score of 0.80.", "motivation": "Improve lung disease diagnosis accuracy in CT scans by reducing computational cost and focusing on lesion-related areas.", "method": "Analyze 3D CT scans, remove non-lung regions, use ResNeSt50 for feature extraction, and apply weighted cross-entropy loss for class imbalance.", "result": "Achieves a Macro F1 Score of 0.80 on the validation set, showing strong performance in distinguishing lung conditions.", "conclusion": "The proposed model is effective for accurate lung disease diagnosis, particularly in handling class imbalance and computational efficiency."}}
{"id": "2507.00261", "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "VirtualFencer is a system that extracts 3D fencing motion and strategy from video without supervision, generating realistic fencing behavior for self-play, real fencer interaction, and professional fencing.", "motivation": "Fencing involves diverse motions and strategic responses to opponents, making it ideal for data-driven modeling.", "method": "VirtualFencer extracts 3D fencing motion and strategy from in-the-wild video unsupervisedly.", "result": "The system can fence against itself, real fencers from video, and interactively against professionals.", "conclusion": "VirtualFencer demonstrates versatile capabilities in modeling and generating realistic fencing behavior."}}
{"id": "2507.00979", "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "CIP uses causal influence diagrams (CIDs) to improve safety in LLM-powered autonomous agents by structuring decision-making and mitigating risks.", "motivation": "Ensuring safe and reliable behavior in LLM-powered agents to prevent unintended consequences.", "method": "Three-step approach: initialize CID for task specifications, guide agent interactions using CID, and iteratively refine CID based on outcomes.", "result": "Enhanced safety in code execution and mobile device control tasks.", "conclusion": "CIP effectively improves agent safety by leveraging CIDs for structured decision-making."}}
{"id": "2507.00030", "pdf": "https://arxiv.org/pdf/2507.00030", "abs": "https://arxiv.org/abs/2507.00030", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "AI": {"tldr": "A novel DRL approach combines contextual bandits with DQN to adaptively select action durations, improving performance and efficiency in tasks like Atari games.", "motivation": "The temporal scale of action execution in DRL is underexplored, limiting policy flexibility and computational efficiency.", "method": "Integrates contextual bandits with DQN to learn optimal action repetition rates based on state contexts.", "result": "Experiments on Atari 2600 games show significant performance gains over static duration baselines.", "conclusion": "The paradigm enhances DRL scalability for real-time applications like gaming and robotics by enabling dynamic action durations."}}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "A Capsule Network-based algorithm improves intent recognition accuracy in human-computer interaction by modeling semantic features hierarchically and using dynamic routing.", "motivation": "Addressing insufficient accuracy in intent recognition for human-computer interaction.", "method": "Uses vectorized capsule structures and dynamic routing to capture hierarchical semantic relationships, with a convolutional feature extraction module and margin-based loss function.", "result": "Outperforms traditional methods and deep learning structures in accuracy, F1-score, and intent detection rate.", "conclusion": "The proposed method effectively improves intent recognition under complex semantic conditions."}}
{"id": "2507.00006", "pdf": "https://arxiv.org/pdf/2507.00006", "abs": "https://arxiv.org/abs/2507.00006", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "categories": ["cs.GR", "cs.LG", "eess.IV"], "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released.", "AI": {"tldr": "MVGBench is a benchmark for multi-view image generation models, evaluating 3D consistency, image quality, and semantics without relying on ground truth. It introduces a 3D self-consistency metric and tests 12 MVGs on real and synthetic datasets, identifying limitations and critical design choices. The findings lead to ViFiGen, a superior method.", "motivation": "Existing metrics for MVGs rely on ground truth comparisons, which are unsuitable for generative tasks with multiple valid solutions. Robustness and generalization to real data are also under-evaluated.", "method": "MVGBench evaluates MVGs on best setup performance, generalization, and robustness using a novel 3D self-consistency metric. It tests 12 MVGs on 4 datasets.", "result": "The benchmark reveals limitations in robustness and generalization of existing MVGs and identifies key design choices. ViFiGen, developed using these insights, outperforms others in 3D consistency.", "conclusion": "MVGBench provides a rigorous evaluation framework for MVGs, highlighting critical improvements and introducing ViFiGen as a state-of-the-art solution."}}
{"id": "2507.00263", "pdf": "https://arxiv.org/pdf/2507.00263", "abs": "https://arxiv.org/abs/2507.00263", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "AI": {"tldr": "The paper introduces a machine learning pipeline to categorize and group vacation rental property images by room type and bed configuration, improving traveler understanding of spatial layouts.", "motivation": "The lack of structured categorization in vacation rental property images makes it difficult for travelers to understand spatial layouts, especially with multiple rooms of the same type.", "method": "A computationally efficient pipeline combines supervised room-type detection, overlap detection, clustering, and a Multi-modal Large Language Model (MLLM) to group images and identify bed types.", "result": "The pipeline outperforms existing methods like contrastive learning and clustering with pretrained embeddings.", "conclusion": "The proposed approach effectively organizes property images, aiding travelers in comprehending spatial layouts and sleeping configurations."}}
{"id": "2507.00007", "pdf": "https://arxiv.org/pdf/2507.00007", "abs": "https://arxiv.org/abs/2507.00007", "authors": ["Vasiliy Znamenskiy", "Rafael Niyazov", "Joel Hernandez"], "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T50, 68U20, 97U50, 97D40", "I.2.7; K.3.1; K.3.2; H.5.3"], "comment": "http://doi.org/10.5121/ijci.2025.140302", "summary": "This paper presents a new educational framework for integrating generative\nartificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini\ninto laboratory activities aimed at developing critical thinking and digital\nliteracy among undergraduate students. Recognizing the limitations and risks of\nuncritical reliance on large language models (LLMs), the proposed pedagogical\nmodel reframes GenAI as a research subject and cognitive tool. Students\nformulate discipline-specific prompts and evaluate GenAI-generated responses in\ntext, image, and video modalities. A pilot implementation in a general\nastronomy course for non-science majors demonstrated high levels of engagement\nand critical reflection, with many students continuing the activity after class\nand presenting results at a research symposium. The results highlight the\nimportance of structured AI interactions in education and suggest that GenAI\ncan improve learning outcomes when combined with reflective assessment methods.\nThe study proposes a replicable model for interdisciplinary AI-integrated lab\nwork, adaptable to scientific disciplines. See the guide to learning activities\nbased on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802", "AI": {"tldr": "A new educational framework uses GenAI platforms like ChatGPT to enhance critical thinking and digital literacy in undergraduates by treating AI as a research tool and subject.", "motivation": "To address the risks of uncritical reliance on LLMs and improve learning outcomes through structured AI interactions.", "method": "Students create discipline-specific prompts and evaluate GenAI responses in text, image, and video formats. Piloted in an astronomy course.", "result": "High engagement and critical reflection observed, with students continuing activities post-class and presenting at a symposium.", "conclusion": "GenAI can enhance learning when paired with reflective assessment, offering a replicable model for interdisciplinary labs."}}
{"id": "2507.00031", "pdf": "https://arxiv.org/pdf/2507.00031", "abs": "https://arxiv.org/abs/2507.00031", "authors": ["Chuan Li", "Jiang You", "Hassine Moungla", "Vincent Gauthier", "Miguel Nunez-del-Prado", "Hugo Alatrista-Salas"], "title": "Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru", "categories": ["cs.LG"], "comment": null, "summary": "Accurate modeling of human mobility is critical for understanding epidemic\nspread and deploying timely interventions. In this work, we leverage a\nlarge-scale spatio-temporal dataset collected from Peru's national Digital\nContact Tracing (DCT) application during the COVID-19 pandemic to forecast\nmobility flows across urban regions. A key challenge lies in the spatial\nsparsity of hourly mobility counts across hexagonal grid cells, which limits\nthe predictive power of conventional time series models. To address this, we\npropose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)\ntechnique that augments each cell's features with aggregated signals from its\nimmediate H3 neighbors. We evaluate this strategy on three forecasting\nbackbones: NLinear, PatchTST, and K-U-Net, under various historical input\nlengths. Experimental results show that SPN consistently improves forecasting\nperformance, achieving up to 9.85 percent reduction in test MSE. Our findings\ndemonstrate that spatial smoothing of sparse mobility signals provides a simple\nyet effective path toward robust spatio-temporal forecasting during public\nhealth crises.", "AI": {"tldr": "The paper proposes a Spatial Neighbourhood Fusion (SPN) technique to improve mobility flow forecasting in sparse data, showing a 9.85% reduction in test MSE.", "motivation": "Accurate human mobility modeling is crucial for epidemic control, but sparse spatial data limits conventional models.", "method": "SPN aggregates signals from neighboring hexagonal grid cells to augment features, tested on NLinear, PatchTST, and K-U-Net models.", "result": "SPN consistently enhances forecasting, reducing test MSE by up to 9.85%.", "conclusion": "Spatial smoothing of sparse mobility data is a simple, effective method for robust forecasting during health crises."}}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "The paper addresses methodological challenges in computationally intensive research, focusing on topic modelling, and provides guidelines to ensure rigour.", "motivation": "The opacity and lack of transparency in advanced computational algorithms undermine trust in research, necessitating methodological guidance.", "method": "The author illustrates the structural topic modelling algorithm and presents guidelines for ensuring rigour in its application.", "result": "The guidelines are useful for novice researchers, editors, and reviewers, and can be adapted for other algorithms.", "conclusion": "The paper contributes to the literature on topic modelling and methodological rigour in computationally intensive research."}}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316", "abs": "https://arxiv.org/abs/2507.00316", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "title": "$\u03bc^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.00287", "pdf": "https://arxiv.org/pdf/2507.00287", "abs": "https://arxiv.org/abs/2507.00287", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "title": "Self-Supervised Multiview Xray Matching", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "AI": {"tldr": "A self-supervised pipeline for multi-view X-ray correspondence, using synthetic data and transformers, improves fracture detection.", "motivation": "Current AI methods lack robust multi-view X-ray correspondence, which is vital for clinical diagnosis.", "method": "Uses synthetic X-ray views (DRRs) from CT volumes and a transformer-based approach to learn correspondences without manual annotation.", "result": "Improves multi-view fracture classification performance on both synthetic and real datasets.", "conclusion": "Learning correspondences from synthetic data enhances real-world multi-view fracture detection."}}
{"id": "2507.00034", "pdf": "https://arxiv.org/pdf/2507.00034", "abs": "https://arxiv.org/abs/2507.00034", "authors": ["Reece Bourisaw", "Reid McCants", "Jean-Marie Le Corre", "Anna Iskhakova", "Arsen S. Iskhakov"], "title": "Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark", "categories": ["cs.LG", "cs.CE"], "comment": null, "summary": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water\nreactors, defining safe thermal-hydraulic operating limits. To support Phase II\nof the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power\nprofiles, this work compiles and digitizes a broad CHF dataset covering both\nuniform and non-uniform axial heating conditions. Heating profiles were\nextracted from technical reports, interpolated onto a consistent axial mesh,\nvalidated via energy-balance checks, and encoded in machine-readable formats\nfor benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating\nand degrade markedly when applied to non-uniform profiles, while modern tabular\nmethods offer improved but still imperfect predictions. A neural network\ntrained solely on uniform data performs well in that regime but fails to\ngeneralize to spatially varying scenarios, underscoring the need for models\nthat explicitly incorporate axial power distributions. By providing these\ncurated datasets and baseline modeling results, this study lays the groundwork\nfor advanced transfer-learning strategies, rigorous uncertainty quantification,\nand design-optimization efforts in the next phase of the CHF benchmark.", "AI": {"tldr": "This paper compiles and digitizes a CHF dataset for AI/ML benchmarking, highlighting the limitations of classical correlations and neural networks under non-uniform heating conditions.", "motivation": "To support Phase II of the OECD/NEA AI/ML CHF benchmark by providing curated datasets and baseline results for advanced modeling.", "method": "Heating profiles were extracted, interpolated, validated, and encoded in machine-readable formats. Classical correlations and neural networks were evaluated.", "result": "Classical correlations and neural networks trained on uniform data fail under non-uniform heating, emphasizing the need for models incorporating axial power distributions.", "conclusion": "The study provides foundational data for future transfer-learning, uncertainty quantification, and design-optimization in CHF benchmarking."}}
{"id": "2507.00579", "pdf": "https://arxiv.org/pdf/2507.00579", "abs": "https://arxiv.org/abs/2507.00579", "authors": ["Miriam Ansch\u00fctz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "A two-part pipeline combining retrieval-based fact verification and BERT-based fine-tuning addresses multilingual LLM hallucinations, achieving top-10 results in eight languages.", "motivation": "Hallucinations in LLMs hinder trustworthiness, especially in multilingual contexts, which are often overlooked in research.", "method": "Proposes a pipeline with retrieval-based fact verification against Wikipedia and a BERT-based system fine-tuned to detect hallucination patterns.", "result": "Competitive performance across languages, top-10 in eight languages, and support for languages beyond the task's scope.", "conclusion": "The multilingual hallucination identifier enhances LLM outputs and their future utility."}}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333", "abs": "https://arxiv.org/abs/2507.00333", "authors": ["Emin Zerman", "Jonas Carlsson", "M\u00e5rten Sj\u00f6str\u00f6m"], "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "AI": {"tldr": "A shooting visualization system was developed to enhance marksmanship training by combining first-person video with overlaid metrics. Evaluated with novices and experts, a dashboard-style view was preferred and effective.", "motivation": "Current marksmanship training lacks real-time, first-person insights, relying on post-session analysis. The study aims to improve coaching by integrating visual analytics.", "method": "Five composite visualizations were created using enriched first-person shooting videos. Evaluated with 10 participants (5 experts, 5 novices) through tasks, comparisons, and interviews.", "result": "A dashboard-style view (raw video + polar plot + graphs) was preferred by 9/10 participants and improved understanding across skill levels.", "conclusion": "Integrating first-person video with visual analytics is valuable for coaching marksmanship and can extend to other precision sports."}}
{"id": "2507.00292", "pdf": "https://arxiv.org/pdf/2507.00292", "abs": "https://arxiv.org/abs/2507.00292", "authors": ["Ali Mammadov", "Lo\u00efc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "AI": {"tldr": "A Multi-Fidelity, Model Fusion strategy is introduced to reduce performance variability in Multiple Instance Learning (MIL) methods for WSI classification.", "motivation": "High variability in performance of MIL methods due to factors like weight initialization, batch ordering, and learning rate makes reliable comparison difficult.", "method": "Train multiple models for a few epochs, average the most stable ones based on validation scores, and apply this fusion strategy to any MIL model.", "result": "The approach reduces variability, simplifies hyperparameter tuning, improves reproducibility, and maintains computational efficiency.", "conclusion": "The proposed strategy effectively addresses performance variability in MIL methods for WSI classification, validated through extensive experiments."}}
{"id": "2507.00036", "pdf": "https://arxiv.org/pdf/2507.00036", "abs": "https://arxiv.org/abs/2507.00036", "authors": ["Rohan Putatunda", "Sanjay Purushotham", "Ratnaksha Lele", "Vandana P. Janeja"], "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting", "categories": ["cs.LG", "physics.ao-ph"], "comment": "16 pages, 4 figures", "summary": "Drifting icebergs in the polar oceans play a key role in the Earth's climate\nsystem, impacting freshwater fluxes into the ocean and regional ecosystems\nwhile also posing a challenge to polar navigation. However, accurately\nforecasting iceberg trajectories remains a formidable challenge, primarily due\nto the scarcity of spatiotemporal data and the complex, nonlinear nature of\niceberg motion, which is also impacted by environmental variables. The iceberg\nmotion is influenced by multiple dynamic environmental factors, creating a\nhighly variable system that makes trajectory identification complex. These\nlimitations hinder the ability of deep learning models to effectively capture\nthe underlying dynamics and provide reliable predictive outcomes. To address\nthese challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep\nlearning model that combines an analytical formulation of iceberg drift\nphysics, with an augmented residual learning model. The model learns the\npattern of mismatch between the analytical solution and ground-truth\nobservations, which is combined with a rotate-augmented spectral neural network\nthat captures both global and local patterns from the data to forecast future\niceberg drift positions. We compare IDRIFTNET model performance with\nstate-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings\ndemonstrate that IDRIFTNET outperforms other models by achieving a lower Final\nDisplacement Error (FDE) and Average Displacement Error (ADE) across a variety\nof time points. These results highlight IDRIFTNET's effectiveness in capturing\nthe complex, nonlinear drift of icebergs for forecasting iceberg trajectories\nunder limited data and dynamic environmental conditions.", "AI": {"tldr": "IDRIFTNET, a hybrid physics-driven deep learning model, outperforms state-of-the-art models in forecasting iceberg trajectories by combining analytical drift physics with residual learning and spectral neural networks.", "motivation": "Accurate iceberg trajectory forecasting is challenging due to sparse data and complex environmental influences, limiting deep learning models' effectiveness.", "method": "Proposes IDRIFTNET, integrating analytical drift physics with a residual learning model and rotate-augmented spectral neural network to capture global and local patterns.", "result": "IDRIFTNET achieves lower Final Displacement Error (FDE) and Average Displacement Error (ADE) compared to other models on Antarctic icebergs A23A and B22A.", "conclusion": "IDRIFTNET effectively forecasts iceberg trajectories under limited data and dynamic conditions, demonstrating superior performance over existing models."}}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601", "abs": "https://arxiv.org/abs/2507.00601", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "A unified framework combining knowledge transfer and parameter-efficient fine-tuning improves adaptation of large language models in low-resource scenarios.", "motivation": "Addressing limited transfer and adaptation capabilities of large language models in low-resource language settings.", "method": "Uses knowledge alignment loss, soft prompt tuning, lightweight adaptation modules, freezing strategies, and prompt injection.", "result": "Outperforms existing models and methods in cross-lingual tasks like MLQA, XQuAD, and PAWS-X, especially in data-scarce conditions.", "conclusion": "The method enhances adaptability and preserves general capabilities, making it suitable for multilingual and complex semantic tasks."}}
{"id": "2507.00365", "pdf": "https://arxiv.org/pdf/2507.00365", "abs": "https://arxiv.org/abs/2507.00365", "authors": ["Wanghui Xiao"], "title": "An Improved U-Net Model for Offline handwriting signature denoising", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "AI": {"tldr": "A study introduces a U-net-based model with wavelet and PCA transforms to denoise offline handwriting signatures, improving clarity and recognition robustness.", "motivation": "Handwriting signatures are crucial for identity recognition but suffer from noise in historical samples, complicating forensic analysis.", "method": "The proposed model uses an improved U-net structure with discrete wavelet and PCA transforms to enhance noise suppression.", "result": "The model outperforms traditional methods, improving image clarity and readability for signature analysis.", "conclusion": "The model provides reliable technical support for signature recognition, addressing noise challenges in forensic appraisals."}}
{"id": "2507.00327", "pdf": "https://arxiv.org/pdf/2507.00327", "abs": "https://arxiv.org/abs/2507.00327", "authors": ["Chuyan Zhang", "Kefan Wang", "Yun Gu"], "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational\ncosts while maintaining performance comparable to fully fine-tuned foundation\nmodels across various tasks. However, its fixed low-rank structure restricts\nits adaptability in scenarios with substantial domain gaps, where higher ranks\nare often required to capture domain-specific complexities. Current adaptive\nLoRA methods attempt to overcome this limitation by dynamically expanding or\nselectively allocating ranks, but these approaches frequently depend on\ncomputationally intensive techniques such as iterative pruning, rank searches,\nor additional regularization. To address these challenges, we introduce Stable\nRank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the\nstable rank of pre-trained weight matrices as a natural prior for layer-wise\nrank allocation. By leveraging the stable rank, which reflects the intrinsic\ndimensionality of the weights, SR-LoRA enables a principled and efficient\nredistribution of ranks across layers, enhancing adaptability without incurring\nadditional search costs. Empirical evaluations on few-shot tasks with\nsignificant domain gaps show that SR-LoRA consistently outperforms recent\nadaptive LoRA variants, achieving a superior trade-off between performance and\nefficiency. Our code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.", "AI": {"tldr": "SR-LoRA improves LoRA by using stable rank for efficient rank allocation, outperforming adaptive LoRA variants in domain-gap tasks.", "motivation": "Fixed low-rank in LoRA limits adaptability in domain gaps; current adaptive methods are computationally expensive.", "method": "Uses stable rank of pre-trained weights for layer-wise rank allocation, avoiding search costs.", "result": "Outperforms adaptive LoRA variants in few-shot tasks with domain gaps, balancing performance and efficiency.", "conclusion": "SR-LoRA offers a principled, efficient solution for rank allocation in LoRA, enhancing adaptability."}}
{"id": "2507.00032", "pdf": "https://arxiv.org/pdf/2507.00032", "abs": "https://arxiv.org/abs/2507.00032", "authors": ["Grey Kuling", "Marinka Zitnik"], "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "AI": {"tldr": "KUL-KT is a biologically inspired knowledge tracing architecture combining Hebbian memory and gradient-based consolidation, enabling scalable, input-agnostic learning with graceful forgetting and continual adaptation.", "motivation": "The paper aims to improve knowledge tracing by mimicking neural memory consolidation, addressing challenges like forgetting, continual learning, and scalability in student modeling.", "method": "KUL-KT uses a fast Hebbian memory for associative updates and a slower linear network for gradient-based consolidation, with innovations like time-decaying memory and Loss-aligned Internal Target (LIT) for continual learning.", "result": "KUL-KT outperforms baselines on ten benchmarks, improves learner-perceived helpfulness, trains 1.75x faster, and uses 99.01% less memory than a graph-based model.", "conclusion": "KUL-KT is a biologically grounded, efficient, and flexible framework for personalized learning at scale."}}
{"id": "2507.00037", "pdf": "https://arxiv.org/pdf/2507.00037", "abs": "https://arxiv.org/abs/2507.00037", "authors": ["Phoomraphee Luenam", "Andreas Spanopoulos", "Amit Sant", "Thomas Hofmann", "Sotiris Anagnostidis", "Sidak Pal Singh"], "title": "Model Fusion via Neuron Interpolation", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1"], "comment": "5 figures, 15 tables, 23 pages", "summary": "Model fusion aims to combine the knowledge of multiple models by creating one\nrepresentative model that captures the strengths of all of its parents.\nHowever, this process is non-trivial due to differences in internal\nrepresentations, which can stem from permutation invariance, random\ninitialization, or differently distributed training data. We present a novel,\nneuron-centric family of model fusion algorithms designed to integrate multiple\ntrained neural networks into a single network effectively regardless of\ntraining data distribution. Our algorithms group intermediate neurons of parent\nmodels to create target representations that the fused model approximates with\nits corresponding sub-network. Unlike prior approaches, our approach\nincorporates neuron attribution scores into the fusion process. Furthermore,\nour algorithms can generalize to arbitrary layer types. Experimental results on\nvarious benchmark datasets demonstrate that our algorithms consistently\noutperform previous fusion techniques, particularly in zero-shot and non-IID\nfusion scenarios. The code is available at\nhttps://github.com/AndrewSpano/neuron-interpolation-model-fusion.", "AI": {"tldr": "A novel neuron-centric model fusion algorithm integrates multiple neural networks into one, outperforming prior methods, especially in zero-shot and non-IID scenarios.", "motivation": "Model fusion is challenging due to differences in internal representations like permutation invariance or varied training data. The goal is to create a unified model capturing all parent models' strengths.", "method": "The approach groups intermediate neurons of parent models using neuron attribution scores, creating target representations for the fused model. It works with arbitrary layer types.", "result": "The algorithm consistently outperforms previous fusion techniques, particularly in zero-shot and non-IID scenarios.", "conclusion": "The neuron-centric fusion method effectively integrates diverse neural networks, offering superior performance and generalization."}}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606", "abs": "https://arxiv.org/abs/2507.00606", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "Mixture of Reasoning (MoR) trains LLMs to autonomously adapt reasoning strategies, eliminating manual prompt engineering and improving performance.", "motivation": "Current LLMs rely on task-specific prompts, limiting adaptability and efficiency. MoR aims to embed diverse reasoning strategies for autonomous task-adaptive reasoning.", "method": "MoR involves two phases: Thought Generation (creating reasoning chain templates) and SFT Dataset Construction (pairing templates with datasets for supervised fine-tuning).", "result": "MoR improves performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT and 0.734 (13.5% improvement) over baselines.", "conclusion": "MoR provides a generalizable solution for robust reasoning across tasks without task-specific prompts."}}
{"id": "2507.00372", "pdf": "https://arxiv.org/pdf/2507.00372", "abs": "https://arxiv.org/abs/2507.00372", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "AI": {"tldr": "Proposes a scalable dataset synthesis method for depth-dependent defocus and optical aberrations, enabling effective generalization to real-world images without real-world fine-tuning.", "motivation": "Addresses the challenge of shallow depth of field in fixed-focus cameras and domain gaps in deep learning models due to unmatched optical aberrations and defocus properties.", "method": "An efficient and scalable dataset synthesis approach that models depth-dependent defocus and spatially varying optical aberrations without relying on real-world data.", "result": "A network trained on low-resolution synthetic images generalizes effectively to high-resolution (12MP) real-world images across diverse scenes.", "conclusion": "The proposed method successfully bridges the domain gap and offers a practical solution for fixed-focus camera systems."}}
{"id": "2507.00328", "pdf": "https://arxiv.org/pdf/2507.00328", "abs": "https://arxiv.org/abs/2507.00328", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D. Ryser", "Lars J. Grimm", "Joseph Y. Lo"], "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "categories": ["cs.CV"], "comment": null, "summary": "Accurate lesion tracking in temporal mammograms is essential for monitoring\nbreast cancer progression and facilitating early diagnosis. However, automated\nlesion correspondence across exams remains a challenges in computer-aided\ndiagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,\na mask-guided lesion tracking framework that automates lesion localization\nacross consecutively exams. Our approach follows a coarse-to-fine strategy\nincorporating three key modules: global search, local search, and score\nrefinement. To support large-scale training and evaluation, we introduce a new\ndataset with curated prior-exam annotations for 730 mass and calcification\ncases from the public EMBED mammogram dataset, yielding over 20000 lesion\npairs, making it the largest known resource for temporal lesion tracking in\nmammograms. Experimental results demonstrate that MammoTracker achieves 0.455\naverage overlap and 0.509 accuracy, surpassing baseline models by 8%,\nhighlighting its potential to enhance CAD-based lesion progression analysis.\nOur dataset will be available at\nhttps://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.", "AI": {"tldr": "MammoTracker, a mask-guided lesion tracking framework, improves automated lesion correspondence in mammograms, achieving 0.455 average overlap and 0.509 accuracy, outperforming baselines by 8%.", "motivation": "Automated lesion tracking in temporal mammograms is challenging but crucial for monitoring breast cancer progression and early diagnosis.", "method": "MammoTracker uses a coarse-to-fine strategy with three modules: global search, local search, and score refinement. A new dataset with 730 cases and 20000 lesion pairs supports training and evaluation.", "result": "The framework achieves 0.455 average overlap and 0.509 accuracy, outperforming baseline models by 8%.", "conclusion": "MammoTracker shows promise for enhancing CAD-based lesion progression analysis, and the dataset will be publicly available."}}
{"id": "2507.00038", "pdf": "https://arxiv.org/pdf/2507.00038", "abs": "https://arxiv.org/abs/2507.00038", "authors": ["Fei Chen", "Wenchi Zhou"], "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data reduction plays a vital role in data-centric AI by identifying the most\ninformative instance within large-scale datasets to enhance model training\nefficiency. The core challenge lies in how to select the optimal\ninstances-rather than the entire datasets-to improve data quality and training\nefficiency. In this paper, we propose an effective data reduction strategy\nbased on Pointwise V-information(PVI). First, we quantify instance difficulty\nusing PVI and filter out low-difficulty instances enabling a static approach.\nExperiments demonstrate that removing 10%-30% of the data preserves the\nclassifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we\nuse a progressive learning approach to training the classifiers on instances\nsorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy\ngain over conventional training. Our results suggest that with the effective\ndata reduction strategy, training a classifier on the selected optimal subset\ncould enhance the model performance and boost training efficiency. Moreover, we\nhave transferred the PVI framework, which previously applied only to English\ndatasets, to diverse Chinese NLP tasks and base models, leading to valuable\ninsights for cross-lingual data reduction and faster training. The codes are\nreleased at https://github.com/zhouwenchi/DatasetReductionStrategy.", "AI": {"tldr": "The paper proposes a data reduction strategy using Pointwise V-information (PVI) to enhance model training efficiency by selecting optimal instances, preserving performance while reducing data volume.", "motivation": "To address the challenge of improving data quality and training efficiency by selecting the most informative instances rather than using entire datasets.", "method": "Quantifies instance difficulty with PVI, filters low-difficulty instances statically, and uses progressive learning on instances sorted by ascending PVI.", "result": "Removing 10%-30% of data preserves classifier performance with minimal accuracy loss (0.0001%-0.76%). Progressive learning achieves a 0.8% accuracy gain.", "conclusion": "The PVI-based strategy enhances model performance and training efficiency, with successful cross-lingual application to Chinese NLP tasks."}}
{"id": "2507.00039", "pdf": "https://arxiv.org/pdf/2507.00039", "abs": "https://arxiv.org/abs/2507.00039", "authors": ["Lucas Potin", "Rosa Figueiredo", "Vincent Labatut", "Christine Largeron"], "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph classification aims to categorize graphs based on their structural and\nattribute features, with applications in diverse fields such as social network\nanalysis and bioinformatics. Among the methods proposed to solve this task,\nthose relying on patterns (i.e. subgraphs) provide good explainability, as the\npatterns used for classification can be directly interpreted. To identify\nmeaningful patterns, a standard approach is to use a quality measure, i.e. a\nfunction that evaluates the discriminative power of each pattern. However, the\nliterature provides tens of such measures, making it difficult to select the\nmost appropriate for a given application. Only a handful of surveys try to\nprovide some insight by comparing these measures, and none of them specifically\nfocuses on graphs. This typically results in the systematic use of the most\nwidespread measures, without thorough evaluation. To address this issue, we\npresent a comparative analysis of 38 quality measures from the literature. We\ncharacterize them theoretically, based on four mathematical properties. We\nleverage publicly available datasets to constitute a benchmark, and propose a\nmethod to elaborate a gold standard ranking of the patterns. We exploit these\nresources to perform an empirical comparison of the measures, both in terms of\npattern ranking and classification performance. Moreover, we propose a\nclustering-based preprocessing step, which groups patterns appearing in the\nsame graphs to enhance classification performance. Our experimental results\ndemonstrate the effectiveness of this step, reducing the number of patterns to\nbe processed while achieving comparable performance. Additionally, we show that\nsome popular measures widely used in the literature are not associated with the\nbest results.", "AI": {"tldr": "The paper compares 38 quality measures for graph classification, evaluates their theoretical properties and empirical performance, and introduces a clustering-based preprocessing step to improve efficiency.", "motivation": "The lack of a focused comparison of quality measures for graph classification leads to the arbitrary use of popular measures without thorough evaluation.", "method": "The study theoretically characterizes 38 quality measures, benchmarks them using public datasets, and introduces a clustering-based preprocessing step to group patterns.", "result": "Empirical analysis shows that some widely used measures underperform, while the proposed preprocessing step reduces pattern count without sacrificing performance.", "conclusion": "The study provides insights for selecting quality measures in graph classification and demonstrates the effectiveness of clustering-based preprocessing."}}
{"id": "2507.00665", "pdf": "https://arxiv.org/pdf/2507.00665", "abs": "https://arxiv.org/abs/2507.00665", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "SAFER is a framework using Sparse Autoencoders to interpret and improve reward models in RLHF, enabling insights into safety-relevant decisions and targeted data manipulation.", "motivation": "Reward models in RLHF are opaque; SAFER aims to interpret and refine them for better alignment with human values.", "method": "Uses Sparse Autoencoders to analyze reward model activations, identify safety-relevant features, and design data poisoning/denoising strategies.", "result": "SAFER precisely degrades or enhances safety alignment with minimal data changes, without affecting general chat performance.", "conclusion": "SAFER advances reward model interpretation, auditing, and refinement for high-stakes LLM alignment."}}
{"id": "2507.00373", "pdf": "https://arxiv.org/pdf/2507.00373", "abs": "https://arxiv.org/abs/2507.00373", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "title": "Customizable ROI-Based Deep Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "AI": {"tldr": "A customizable ROI-based deep image compression method is proposed, allowing users to define ROI via text and adjust quality trade-offs between ROI and non-ROI.", "motivation": "Existing ROI-based compression lacks flexibility for diverse user preferences and fixed ROI definitions.", "method": "Introduces Text-controlled Mask Acquisition (TMA), Customizable Value Assign (CVA), and Latent Mask Attention (LMA) modules for flexible ROI customization and quality management.", "result": "Effectively supports customizable ROI definition and quality trade-offs, improving user adaptability.", "conclusion": "The proposed paradigm successfully addresses customization needs in ROI-based image compression."}}
{"id": "2507.00334", "pdf": "https://arxiv.org/pdf/2507.00334", "abs": "https://arxiv.org/abs/2507.00334", "authors": ["Mengyi Shan", "Zecheng He", "Haoyu Ma", "Felix Juefei-Xu", "Peizhao Zhang", "Tingbo Hou", "Ching-Yao Chuang"], "title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://shanmy.github.io/Populate-A-Scene", "summary": "Can a video generation model be repurposed as an interactive world simulator?\nWe explore the affordance perception potential of text-to-video models by\nteaching them to predict human-environment interaction. Given a scene image and\na prompt describing human actions, we fine-tune the model to insert a person\ninto the scene, while ensuring coherent behavior, appearance, harmonization,\nand scene affordance. Unlike prior work, we infer human affordance for video\ngeneration (i.e., where to insert a person and how they should behave) from a\nsingle scene image, without explicit conditions like bounding boxes or body\nposes. An in-depth study of cross-attention heatmaps demonstrates that we can\nuncover the inherent affordance perception of a pre-trained video model without\nlabeled affordance datasets.", "AI": {"tldr": "A study repurposes a text-to-video model to simulate human-environment interactions by inferring affordances from a single scene image, without explicit conditions.", "motivation": "To explore the potential of video generation models as interactive world simulators by predicting human-environment interactions.", "method": "Fine-tune a text-to-video model to insert a person into a scene based on a prompt, ensuring coherence and affordance, using cross-attention heatmaps for analysis.", "result": "The model successfully infers human affordance for video generation without labeled datasets or explicit conditions.", "conclusion": "Pre-trained video models inherently perceive affordances, enabling interactive world simulation without additional labeled data."}}
{"id": "2507.00057", "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel B\u00f6hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "AI": {"tldr": "The paper introduces 'incoherence,' a measure to quantify incorrectness in LLM-generated code without needing an oracle, showing strong agreement with oracle-based evaluations.", "motivation": "LLMs often generate factually incorrect code from natural language specs, and there's a need to quantify correctness without an oracle.", "method": "Proposes incoherence as a measure to estimate incorrectness efficiently, providing a lower bound on error probability.", "result": "Incoherence identifies ~2/3 of incorrect programs without false positives and aligns closely with oracle-based rankings.", "conclusion": "Incoherence can reliably replace oracle-based evaluations for ranking LLMs in code generation tasks."}}
{"id": "2507.00061", "pdf": "https://arxiv.org/pdf/2507.00061", "abs": "https://arxiv.org/abs/2507.00061", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "AI": {"tldr": "Smooth-Distill is a self-distillation framework for HAR and sensor placement detection using a unified CNN, MTL-net, reducing computational overhead while maintaining performance.", "motivation": "To address the computational inefficiency of traditional distillation methods and improve multitask learning for HAR and sensor placement detection.", "method": "Uses MTL-net, a CNN-based architecture, with a smoothed historical model as the teacher for self-distillation, tested on a new dataset and two public ones.", "result": "Outperforms alternatives in HAR and placement detection, showing stable convergence and reduced overfitting.", "conclusion": "Smooth-Distill offers an efficient, accurate solution for multitask learning with accelerometer data, reducing training costs for resource-constrained scenarios."}}
{"id": "2507.00700", "pdf": "https://arxiv.org/pdf/2507.00700", "abs": "https://arxiv.org/abs/2507.00700", "authors": ["Ahmed Sabir", "Azinovi\u010d Gasper", "Mengsay Loem", "Rajesh Sharma"], "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "categories": ["cs.CL"], "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "VLMs trained on Japanese and English exhibit culturally grounded attentional patterns, mirroring holistic (East Asian) and analytic (Western) tendencies.", "motivation": "To explore if VLMs reflect cultural differences in visual information processing, similar to humans.", "method": "Comparative analysis of image descriptions generated by VLMs trained on Japanese (holistic) and English (analytic).", "result": "VLMs reproduce cultural behaviors, showing holistic vs. analytic tendencies based on training language.", "conclusion": "Cultural cognition implicitly shapes VLM outputs, suggesting training data influences model behavior beyond language structure."}}
{"id": "2507.00447", "pdf": "https://arxiv.org/pdf/2507.00447", "abs": "https://arxiv.org/abs/2507.00447", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "categories": ["cs.CV", "eess.IV"], "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "AI": {"tldr": "Latent-PMRF improves face restoration by reformulating PMRF in VAE latent space, aligning better with human perception and achieving faster convergence.", "motivation": "The pixel-space approach of PMRF limits alignment with human perception, prompting the need for a latent-space solution.", "method": "Latent-PMRF uses a VAE's latent space to define source distributions, bounding distortion by reconstruction error and optimizing for perception.", "result": "Latent-PMRF outperforms PMRF with a 5.79X speedup in FID and better perceptual-distortion tradeoff.", "conclusion": "Latent-PMRF offers superior face restoration by leveraging latent space and efficient VAE design, advancing the PD-tradeoff."}}
{"id": "2507.00339", "pdf": "https://arxiv.org/pdf/2507.00339", "abs": "https://arxiv.org/abs/2507.00339", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "comment": "9 pages, 2 figures", "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "AI": {"tldr": "MOVi-MC-AC is a new dataset for amodal segmentation and content completion, featuring multi-camera views and ground-truth amodal content, with 5.8 million labeled object instances.", "motivation": "Existing datasets lack multi-camera perspectives and ground-truth amodal content, limiting research in object detection, tracking, and segmentation.", "method": "The dataset simulates cluttered household scenes in multi-camera video, providing consistent object IDs across frames and cameras, and includes ground-truth amodal content labels.", "result": "MOVi-MC-AC is the largest amodal segmentation dataset and the first to offer ground-truth amodal content, with 5.8 million labeled instances.", "conclusion": "The dataset advances research in amodal segmentation and content completion by addressing gaps in multi-camera settings and providing high-quality labels."}}
{"id": "2507.00066", "pdf": "https://arxiv.org/pdf/2507.00066", "abs": "https://arxiv.org/abs/2507.00066", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies.", "AI": {"tldr": "The paper proposes InSight-R, a framework for automated human failure event identification and interface-induced risk assessment, addressing limitations of traditional HRA methods.", "motivation": "Human reliability in safety-critical domains like nuclear power is often compromised by subjective and non-scalable HRA methods, lacking integration of interface-level data.", "method": "The InSight-R framework uses AutoGraph to link empirical behavioral data to an interface-embedded knowledge graph (IE-KG), enabling automated HFE identification.", "result": "InSight-R improves objectivity and interpretability of HFE identification and supports dynamic, real-time human reliability assessment.", "conclusion": "The framework advances mechanism-driven HRA methodologies and provides insights for interface design optimization."}}
{"id": "2507.00073", "pdf": "https://arxiv.org/pdf/2507.00073", "abs": "https://arxiv.org/abs/2507.00073", "authors": ["Urvi Pawar", "Kunal Telangi"], "title": "Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8"], "comment": "Submitted to Journal of Machine Learning Research (JMLR), June 2025.\n  24 pages, 3 figures. Under review", "summary": "We propose Fractional Policy Gradients (FPG), a reinforcement learning\nframework incorporating fractional calculus for long-term temporal modeling in\npolicy optimization. Standard policy gradient approaches face limitations from\nMarkovian assumptions, exhibiting high variance and inefficient sampling. By\nreformulating gradients using Caputo fractional derivatives, FPG establishes\npower-law temporal correlations between state transitions. We develop an\nefficient recursive computation technique for fractional temporal-difference\nerrors with constant time and memory requirements. Theoretical analysis shows\nFPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus\nstandard policy gradients while preserving convergence. Empirical validation\ndemonstrates 35-68% sample efficiency gains and 24-52% variance reduction\nversus state-of-the-art baselines. This framework provides a mathematically\ngrounded approach for leveraging long-range dependencies without computational\noverhead.", "AI": {"tldr": "FPG introduces fractional calculus into policy gradients for long-term temporal modeling, reducing variance and improving sample efficiency.", "motivation": "Standard policy gradients suffer from high variance and inefficiency due to Markovian assumptions. FPG addresses this by leveraging long-range temporal dependencies.", "method": "FPG uses Caputo fractional derivatives to reformulate gradients, enabling power-law temporal correlations. It includes an efficient recursive computation technique for fractional temporal-difference errors.", "result": "FPG achieves asymptotic variance reduction (O(t^(-alpha))) and shows 35-68% sample efficiency gains and 24-52% variance reduction over baselines.", "conclusion": "FPG provides a mathematically grounded, computationally efficient framework for leveraging long-term dependencies in reinforcement learning."}}
{"id": "2507.00718", "pdf": "https://arxiv.org/pdf/2507.00718", "abs": "https://arxiv.org/abs/2507.00718", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "The paper presents a framework for using LLMs to generate financial reports from time series data, emphasizing prompt engineering, model selection, and evaluation, with an automated highlighting system for categorizing insights.", "motivation": "To leverage LLMs for generating coherent and informative financial reports from time series data, while evaluating their factual grounding and reasoning capabilities.", "method": "Proposes a framework involving prompt engineering, model selection, and evaluation, with an automated highlighting system to categorize insights (data-derived, financial reasoning, or external knowledge).", "result": "Experiments show LLMs can produce coherent and informative financial reports using real and synthetic time series data.", "conclusion": "LLMs are capable of generating useful financial reports, with the proposed framework aiding in evaluating their reasoning and factual grounding."}}
{"id": "2507.00490", "pdf": "https://arxiv.org/pdf/2507.00490", "abs": "https://arxiv.org/abs/2507.00490", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "title": "Just Noticeable Difference for Large Multimodal Models", "categories": ["cs.CV", "eess.IV"], "comment": "19 pages, 19 figures", "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "AI": {"tldr": "The paper introduces LMM-JND, a concept to quantify visual blind spots in large multimodal models (LMMs), and presents VPA-JND, a dataset to study these limitations. It highlights gaps in LMMs' visual perception compared to humans.", "motivation": "To address the lack of systematic study on perceptual boundaries in LMMs and uncover their visual blind spots, which could impact security and efficiency.", "method": "Proposes LMM-JND and its determination pipeline, constructs the VPA-JND dataset (21.5k images, 489k stimuli), and evaluates LMMs like GPT-4o and InternVL2.5.", "result": "Reveals significant visual perception gaps in LMMs compared to humans, with notable struggles in basic comparison tasks. Also identifies correlations between vision/language backbones and performance.", "conclusion": "LMM-JND is a valuable metric for studying LMMs, and its predictability is crucial for security. The work provides tools and insights for future LMM refinement."}}
{"id": "2507.00356", "pdf": "https://arxiv.org/pdf/2507.00356", "abs": "https://arxiv.org/abs/2507.00356", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "categories": ["cs.CV", "cs.AI"], "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "AI": {"tldr": "The paper introduces CGEarthEye, a high-resolution remote sensing vision foundation model (RSVFM) for the Jilin-1 satellite, leveraging a 15-million-scale SSL dataset and achieving SOTA performance across multiple tasks.", "motivation": "The limited accessibility of ultra-high-resolution RS imagery hinders RSVFM progress. The Jilin-1 constellation's abundant data offers an opportunity to address this gap.", "method": "Developed CGEarthEye with five backbones (2.1B parameters) and JLSSD dataset (15M samples). Used seasonal, augmentation, and masked patch token contrastive strategies for pre-training.", "result": "Achieved SOTA performance on 10 benchmark datasets across four RS tasks, with superior feature visualization, convergence, and efficiency.", "conclusion": "CGEarthEye's capabilities can enhance Jilin-1 data applications in Earth Observation, demonstrating its potential for broader RS advancements."}}
{"id": "2507.00075", "pdf": "https://arxiv.org/pdf/2507.00075", "abs": "https://arxiv.org/abs/2507.00075", "authors": ["Yifan Sun", "Yushan Liang", "Zhen Zhang", "Jiaye Teng"], "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages", "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.", "AI": {"tldr": "The paper explores the dynamics of self-improvement in LLMs, introducing the solver-verifier gap concept to model performance evolution and predict outcomes early in training. It also examines the impact of external data.", "motivation": "To understand how LLM performance evolves during self-improvement and the role of the solver-verifier gap, as well as the influence of external data.", "method": "Theoretical modeling of self-improvement dynamics using the solver-verifier gap, followed by empirical validation on various LLMs and datasets.", "result": "The solver-verifier gap effectively explains performance enhancement, and early training epochs can predict ultimate self-improvement power. External data's impact is minimal in limited regimes.", "conclusion": "The solver-verifier gap is key to understanding self-improvement in LLMs, and external data can be flexibly used without major performance trade-offs."}}
{"id": "2507.00078", "pdf": "https://arxiv.org/pdf/2507.00078", "abs": "https://arxiv.org/abs/2507.00078", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "title": "The language of time: a language model perspective on time-series foundation models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "AI": {"tldr": "The paper explores how patch-based time series foundation models achieve cross-domain success despite the intuitive implausibility, linking their performance to latent probabilistic representations akin to language models.", "motivation": "To resolve the paradox of time series models' empirical success in cross-domain transfer despite data reflecting distinct dynamical systems.", "method": "Investigates representation learning and generalization through theoretical and experimental analysis of patch-based time series models.", "result": "Demonstrates that time series patches can be quantized into a discrete vocabulary with statistical properties similar to natural language, enabling robust representation and transfer.", "conclusion": "Provides a theoretical foundation for understanding and improving the safety and reliability of time series foundation models."}}
{"id": "2507.00769", "pdf": "https://arxiv.org/pdf/2507.00769", "abs": "https://arxiv.org/abs/2507.00769", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench is introduced as a standardized benchmark for evaluating creative writing by LLMs, identifying Claude-3.7-Sonnet as the best OTS judge and trained reward models outperforming OTS judges.", "motivation": "The lack of ground truths in creative writing evaluation makes it challenging to assess LLM-generated content reliably.", "method": "LitBench includes a dataset of human-labeled story comparisons, benchmarking zero-shot LLM judges, training reward models, and validating rankings via a human study.", "result": "Claude-3.7-Sonnet achieves 73% agreement with humans; trained reward models reach 78% accuracy.", "conclusion": "LitBench provides a reliable resource for automated evaluation and optimization of creative writing systems."}}
{"id": "2507.00739", "pdf": "https://arxiv.org/pdf/2507.00739", "abs": "https://arxiv.org/abs/2507.00739", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": null, "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "AI": {"tldr": "A novel biorthogonal tunable wavelet unit using a lifting scheme improves CNN operations, boosting accuracy in image classification and anomaly detection.", "motivation": "To enhance flexibility in filter design and improve CNN performance by relaxing orthogonality and equal filter length constraints.", "method": "Constructs a biorthogonal tunable wavelet unit via a lifting scheme, integrating it into ResNet architectures for tasks like classification and anomaly detection.", "result": "Improved CIFAR-10 accuracy by 2.12%, DTD by 9.73%, and achieved competitive anomaly detection performance in the MVTec dataset.", "conclusion": "The proposed wavelet unit effectively enhances CNN performance, demonstrating robustness and accuracy in diverse tasks."}}
{"id": "2507.00363", "pdf": "https://arxiv.org/pdf/2507.00363", "abs": "https://arxiv.org/abs/2507.00363", "authors": ["Xingjun Wang", "Lianlei Shan"], "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control", "categories": ["cs.CV"], "comment": null, "summary": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023},\naddressing challenges in initialization, optimization, and density control.\nGaussian Splatting is an alternative for rendering realistic images while\nsupporting real-time performance, and it has gained popularity due to its\nexplicit 3D Gaussian representation. However, 3DGS heavily depends on accurate\ninitialization and faces difficulties in optimizing unstructured Gaussian\ndistributions into ordered surfaces, with limited adaptive density control\nmechanism proposed so far. Our first key contribution is a geometry-guided\ninitialization to predict Gaussian parameters, ensuring precise placement and\nfaster convergence. We then introduce a surface-aligned optimization strategy\nto refine Gaussian placement, improving geometric accuracy and aligning with\nthe surface normals of the scene. Finally, we present a dynamic adaptive\ndensity control mechanism that adjusts Gaussian density based on regional\ncomplexity, for visual fidelity. These innovations enable our method to achieve\nhigh-fidelity real-time rendering and significant improvements in visual\nquality, even in complex scenes. Our method demonstrates comparable or superior\nresults to state-of-the-art methods, rendering high-fidelity images in real\ntime.", "AI": {"tldr": "A method to improve 3D Gaussian Splatting (3DGS) with better initialization, optimization, and density control for high-fidelity real-time rendering.", "motivation": "3DGS relies on accurate initialization and struggles with optimizing unstructured Gaussian distributions and adaptive density control.", "method": "Proposes geometry-guided initialization, surface-aligned optimization, and dynamic adaptive density control.", "result": "Achieves high-fidelity real-time rendering with superior visual quality in complex scenes.", "conclusion": "The method outperforms or matches state-of-the-art techniques in real-time high-fidelity rendering."}}
{"id": "2507.00082", "pdf": "https://arxiv.org/pdf/2507.00082", "abs": "https://arxiv.org/abs/2507.00082", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 16 figures, IEEE Internet of Things", "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "AI": {"tldr": "FedHLM is a hybrid language model framework combining SLMs and LLMs, using federated learning to optimize token-level uncertainty thresholds, reducing LLM transmissions by over 95% with minimal accuracy loss.", "motivation": "Traditional HLMs face high communication overhead due to frequent offloading to LLMs for uncertain predictions. FedHLM aims to reduce this overhead while maintaining accuracy.", "method": "FedHLM integrates uncertainty-aware inference with federated learning to collaboratively learn dynamic thresholds for LLM assistance. It also uses P2P token resolution and hierarchical model aggregation.", "result": "FedHLM reduces LLM transmissions by over 95% with negligible accuracy loss in large-scale news classification tasks.", "conclusion": "FedHLM is scalable and efficient for edge-AI applications, significantly cutting communication costs while preserving accuracy."}}
{"id": "2507.00080", "pdf": "https://arxiv.org/pdf/2507.00080", "abs": "https://arxiv.org/abs/2507.00080", "authors": ["Ali Tavasoli", "Heman Shakeri"], "title": "Online Meal Detection Based on CGM Data Dynamics", "categories": ["cs.LG", "nlin.AO", "stat.AP"], "comment": null, "summary": "We utilize dynamical modes as features derived from Continuous Glucose\nMonitoring (CGM) data to detect meal events. By leveraging the inherent\nproperties of underlying dynamics, these modes capture key aspects of glucose\nvariability, enabling the identification of patterns and anomalies associated\nwith meal consumption. This approach not only improves the accuracy of meal\ndetection but also enhances the interpretability of the underlying glucose\ndynamics. By focusing on dynamical features, our method provides a robust\nframework for feature extraction, facilitating generalization across diverse\ndatasets and ensuring reliable performance in real-world applications. The\nproposed technique offers significant advantages over traditional approaches,\nimproving detection accuracy,", "AI": {"tldr": "Using dynamical modes from CGM data improves meal event detection by capturing glucose variability patterns.", "motivation": "To enhance meal detection accuracy and interpretability of glucose dynamics.", "method": "Extract dynamical modes from CGM data to identify meal-related patterns and anomalies.", "result": "Improved detection accuracy and robustness across diverse datasets.", "conclusion": "The technique outperforms traditional methods, offering reliable real-world performance."}}
{"id": "2507.00782", "pdf": "https://arxiv.org/pdf/2507.00782", "abs": "https://arxiv.org/abs/2507.00782", "authors": ["Matthieu Pierre Boyer"], "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "A functional programming approach enhances natural language semantics by formalizing a category-based type and effect system and using a diagrammatic calculus for efficient denotation computation.", "motivation": "To improve the expressivity of traditional denotation-style semantics in natural language processing.", "method": "Formalize a category-based type and effect system and construct a diagrammatic calculus for modeling parsing and effect handling.", "result": "Efficient computation of sentence denotations.", "conclusion": "The approach successfully combines functional programming with natural language semantics for enhanced expressivity and efficiency."}}
{"id": "2401.03302", "pdf": "https://arxiv.org/pdf/2401.03302", "abs": "https://arxiv.org/abs/2401.03302", "authors": ["Seyed Mohammad Hossein Hashemi", "Leila Safari", "Mohsen Hooshmand", "Amirhossein Dadashzadeh Taromi"], "title": "Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Reliable diagnosis of brain tumors remains challenging due to low clinical\nincidence rates of such cases. However, this low rate is neglected in most of\nproposed methods. We propose a clinically inspired framework for\nanomaly-resilient tumor detection and classification. Detection leverages\nYOLOv8n fine-tuned on a realistically imbalanced dataset (1:9 tumor-to-normal\nratio; 30,000 MRI slices from 81 patients). In addition, we propose a novel\nPatient-to-Patient (PTP) metric that evaluates diagnostic reliability at the\npatient level. Classification employs knowledge distillation: a Data Efficient\nImage Transformer (DeiT) student model is distilled from a ResNet152 teacher.\nThe distilled ViT achieves an F1-score of 0.92 within 20 epochs, matching near\nteacher performance (F1=0.97) with significantly reduced computational\nresources. This end-to-end framework demonstrates high robustness in clinically\nrepresentative anomaly-distributed data, offering a viable tool that adheres to\nrealistic situations in clinics.", "AI": {"tldr": "A framework for brain tumor detection and classification using YOLOv8n and knowledge distillation with a novel Patient-to-Patient (PTP) metric, achieving high robustness in imbalanced datasets.", "motivation": "Addressing the challenge of reliable brain tumor diagnosis due to low clinical incidence rates, often neglected in existing methods.", "method": "Detection uses YOLOv8n on an imbalanced dataset (1:9 tumor-to-normal ratio). Classification employs knowledge distillation (DeiT student from ResNet152 teacher). Introduces PTP metric for patient-level reliability.", "result": "Distilled ViT achieves F1-score of 0.92 (teacher: 0.97) with reduced resources. Framework shows robustness in anomaly-distributed data.", "conclusion": "The framework is clinically viable, adhering to realistic scenarios and offering reliable tumor diagnosis."}}
{"id": "2507.00368", "pdf": "https://arxiv.org/pdf/2507.00368", "abs": "https://arxiv.org/abs/2507.00368", "authors": ["Hikaru Shijo", "Yutaka Yoshihama", "Kenichi Yadani", "Norifumi Murata"], "title": "Out-of-Distribution Detection with Adaptive Top-K Logits Integration", "categories": ["cs.CV"], "comment": null, "summary": "Neural networks often make overconfident predictions from out-of-distribution\n(OOD) samples. Detection of OOD data is therefore crucial to improve the safety\nof machine learning. The simplest and most powerful method for OOD detection is\nMaxLogit, which uses the model's maximum logit to provide an OOD score. We have\ndiscovered that, in addition to the maximum logit, some other logits are also\nuseful for OOD detection. Based on this finding, we propose a new method called\nATLI (Adaptive Top-k Logits Integration), which adaptively determines effective\ntop-k logits that are specific to each model and combines the maximum logit\nwith the other top-k logits. In this study we evaluate our proposed method\nusing ImageNet-1K benchmark. Extensive experiments showed our proposed method\nto reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit\napproach, and decreased FPR95 by an additional 2.67% compared to other\nstate-of-the-art methods.", "AI": {"tldr": "The paper introduces ATLI, a method for OOD detection that combines the maximum logit with adaptive top-k logits, outperforming MaxLogit and other state-of-the-art methods.", "motivation": "Neural networks often overconfidently predict OOD samples, necessitating better detection methods for safety.", "method": "Proposes ATLI, which adaptively selects and integrates top-k logits with the maximum logit for OOD detection.", "result": "ATLI reduces FPR95 by 6.73% over MaxLogit and 2.67% over other methods on ImageNet-1K.", "conclusion": "ATLI is a powerful and adaptive approach for improving OOD detection in neural networks."}}
{"id": "2507.00083", "pdf": "https://arxiv.org/pdf/2507.00083", "abs": "https://arxiv.org/abs/2507.00083", "authors": ["Wei Meng"], "title": "Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks", "categories": ["cs.LG", "cs.AI", "91A80, 91B62, 68T07", "I.2.6; J.7; K.4.1; C.2.4"], "comment": "This paper proposes the first closed-loop causal modeling framework\n  (IA-STGNN) that links tactical strike variables to strategic delay outcomes\n  via graph neural networks with counterfactual reasoning", "summary": "This study addresses the lack of structured causal modeling between tactical\nstrike behavior and strategic delay in current strategic-level simulations,\nparticularly the structural bottlenecks in capturing intermediate variables\nwithin the \"resilience - nodal suppression - negotiation window\" chain. We\npropose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),\na novel framework that closes the causal loop from tactical input to strategic\ndelay output. The model integrates graph attention mechanisms, counterfactual\nsimulation units, and spatial intervention node reconstruction to enable\ndynamic simulations of strike configurations and synchronization strategies.\nTraining data are generated from a multi-physics simulation platform (GEANT4 +\nCOMSOL) under NIST SP 800-160 standards, ensuring structural traceability and\npolicy-level validation. Experimental results demonstrate that IA-STGNN\nsignificantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),\nachieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5\npercent accuracy, while improving causal path consistency and intervention\nstability. IA-STGNN enables interpretable prediction of strategic delay and\nsupports applications such as nuclear deterrence simulation, diplomatic window\nassessment, and multi-strategy optimization, providing a structured and\ntransparent AI decision-support mechanism for high-level policy modeling.", "AI": {"tldr": "The paper introduces IA-STGNN, a model for structured causal modeling between tactical strike behavior and strategic delay, outperforming baselines with improved accuracy and interpretability.", "motivation": "Addressing the gap in capturing intermediate variables in strategic-level simulations, particularly in the 'resilience - nodal suppression - negotiation window' chain.", "method": "Proposes IA-STGNN, integrating graph attention, counterfactual simulation, and spatial intervention node reconstruction, trained on multi-physics simulation data under NIST SP 800-160 standards.", "result": "IA-STGNN reduces MAE by 12.8% and increases Top-5% accuracy by 18.4%, enhancing causal path consistency and intervention stability.", "conclusion": "IA-STGNN provides a transparent AI decision-support tool for high-level policy modeling, applicable in nuclear deterrence, diplomatic assessment, and strategy optimization."}}
{"id": "2507.00085", "pdf": "https://arxiv.org/pdf/2507.00085", "abs": "https://arxiv.org/abs/2507.00085", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "AI": {"tldr": "GFEN is a novel framework for traffic speed prediction, combining topological spatiotemporal graph fusion and adaptive smoothing to outperform existing methods.", "motivation": "Current traffic prediction methods struggle with complexity, non-linearity, and static techniques for non-stationary data, limiting adaptability.", "method": "GFEN uses topological spatiotemporal graph fusion and a hybrid approach (k-th order difference and attention-based deep learning) to model multi-scale features and smooth data.", "result": "GFEN improves prediction accuracy by 6.3% and converges twice as fast as hybrid models.", "conclusion": "GFEN's superior performance and efficiency make it a promising solution for enhancing traffic prediction systems."}}
{"id": "2507.00783", "pdf": "https://arxiv.org/pdf/2507.00783", "abs": "https://arxiv.org/abs/2507.00783", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "title": "Generative AI and the future of scientometrics: current topics and future questions", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "The paper reviews GenAI's use in scientometrics, discussing its potential and limitations, and calls for systematic comparisons and empirical work to adapt to its impact.", "motivation": "To explore GenAI's role in scientometrics and its broader implications, including its ability to mimic human reasoning and its impact on measuring science.", "method": "The paper reviews GenAI's generative nature, critically engages with recent experiments in scientometrics, and discusses its potential impact on textual characteristics in science.", "result": "GenAI excels in language generation tasks like labelling but struggles with stable semantics or domain knowledge. Its rapid evolution necessitates systematic model comparisons.", "conclusion": "Empirical work and theoretical reflection are crucial to interpret GenAI's evolving impact on scientometrics and knowledge production."}}
{"id": "2407.14153", "pdf": "https://arxiv.org/pdf/2407.14153", "abs": "https://arxiv.org/abs/2407.14153", "authors": ["Qing Xu", "Jiaxuan Li", "Xiangjian He", "Chenxin Li", "Fiseha B. Tesem", "Wenting Duan", "Zhen Chen", "Rong Qu", "Jonathan M. Garibaldi", "Chang Wen Chen"], "title": "De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable Medical Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "The universality of deep neural networks across different modalities and\ntheir generalization capabilities to unseen domains play an essential role in\nmedical image segmentation. The recent segment anything model (SAM) has\ndemonstrated strong adaptability across diverse natural scenarios. However, the\nhuge computational costs, demand for manual annotations as prompts and\nconflict-prone decoding process of SAM degrade its generalization capabilities\nin medical scenarios. To address these limitations, we propose a\nmodality-decoupled lightweight SAM for domain-generalized medical image\nsegmentation, named De-LightSAM. Specifically, we first devise a lightweight\ndomain-controllable image encoder (DC-Encoder) that produces discriminative\nvisual features for diverse modalities. Further, we introduce the self-patch\nprompt generator (SP-Generator) to automatically generate high-quality dense\nprompt embeddings for guiding segmentation decoding. Finally, we design the\nquery-decoupled modality decoder (QM-Decoder) that leverages a one-to-one\nstrategy to provide an independent decoding channel for every modality,\npreventing mutual knowledge interference of different modalities. Moreover, we\ndesign a multi-modal decoupled knowledge distillation (MDKD) strategy to\nleverage robust common knowledge to complement domain-specific medical feature\nrepresentations. Extensive experiments indicate that De-LightSAM outperforms\nstate-of-the-arts in diverse medical imaging segmentation tasks, displaying\nsuperior modality universality and generalization capabilities. Especially,\nDe-LightSAM uses only 2.0% parameters compared to SAM-H. The source code is\navailable at https://github.com/xq141839/De-LightSAM.", "AI": {"tldr": "De-LightSAM is a lightweight, modality-decoupled SAM for medical image segmentation, addressing SAM's computational cost, manual prompt needs, and decoding issues. It outperforms state-of-the-art methods with only 2.0% of SAM-H's parameters.", "motivation": "SAM's limitations in medical scenarios\u2014high computational costs, manual prompt dependency, and decoding conflicts\u2014motivate a lightweight, domain-generalized solution.", "method": "De-LightSAM uses a DC-Encoder for modality-specific features, SP-Generator for automatic prompts, QM-Decoder for independent modality decoding, and MDKD for knowledge distillation.", "result": "De-LightSAM excels in medical segmentation tasks, showing superior universality and generalization while using minimal parameters.", "conclusion": "De-LightSAM effectively addresses SAM's drawbacks for medical use, offering a lightweight, high-performance alternative."}}
{"id": "2507.00371", "pdf": "https://arxiv.org/pdf/2507.00371", "abs": "https://arxiv.org/abs/2507.00371", "authors": ["Xin Yang", "Ruiming Du", "Hanyang Huang", "Jiayang Xie", "Pengyao Xie", "Leisen Fang", "Ziyue Guo", "Nanjun Jiang", "Yu Jiang", "Haiyan Cen"], "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching", "categories": ["cs.CV"], "comment": null, "summary": "Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex datasets. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant datasets, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.", "AI": {"tldr": "PlantSegNeRF is a novel method for high-precision organ segmentation of plant point clouds from multi-view RGB images, outperforming existing techniques in accuracy and generalizability.", "motivation": "Existing techniques for plant organ segmentation lack resolution, accuracy, and generalizability across species.", "method": "PlantSegNeRF uses 2D instance segmentation, instance matching, and an implicit scene (NeRF) to generate high-precision instance point clouds.", "result": "Outperforms common methods by 16.1-24.2% in semantic segmentation metrics and 11.7-38.2% in instance segmentation metrics.", "conclusion": "Provides a high-throughput solution for organ-level plant phenotyping and supports large-scale plant science models."}}
{"id": "2507.00087", "pdf": "https://arxiv.org/pdf/2507.00087", "abs": "https://arxiv.org/abs/2507.00087", "authors": ["Jiale Zhao", "Pengzhi Mao", "Kaifei Wang", "Yiming Li", "Yaping Peng", "Ranfei Chen", "Shuqi Lu", "Xiaohong Ji", "Jiaxiang Ding", "Xin Zhang", "Yucheng Liao", "Weinan E", "Weijie Zhang", "Han Wen", "Hao Chi"], "title": "pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning has advanced mass spectrometry data interpretation, yet most\nmodels remain feature extractors rather than unified scoring frameworks. We\npresent pUniFind, the first large-scale multimodal pre-trained model in\nproteomics that integrates end-to-end peptide-spectrum scoring with open,\nzero-shot de novo sequencing. Trained on over 100 million open search-derived\nspectra, pUniFind aligns spectral and peptide modalities via cross modality\nprediction and outperforms traditional engines across diverse datasets,\nparticularly achieving a 42.6 percent increase in the number of identified\npeptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind\nidentifies 60 percent more PSMs than existing de novo methods despite a\n300-fold larger search space. A deep learning based quality control module\nfurther recovers 38.5 percent additional peptides including 1,891 mapped to the\ngenome but absent from reference proteomes while preserving full fragment ion\ncoverage. These results establish a unified, scalable deep learning framework\nfor proteomic analysis, offering improved sensitivity, modification coverage,\nand interpretability.", "AI": {"tldr": "pUniFind is a multimodal pre-trained model for proteomics, integrating peptide-spectrum scoring and de novo sequencing, outperforming traditional methods with improved sensitivity and modification coverage.", "motivation": "Current deep learning models in mass spectrometry are limited to feature extraction, lacking unified scoring frameworks for proteomic analysis.", "method": "pUniFind is trained on 100M spectra, using cross-modality prediction to align spectral and peptide data, and includes a quality control module.", "result": "Achieves 42.6% more peptide identifications, 60% more PSMs than de novo methods, and recovers 38.5% additional peptides with full fragment coverage.", "conclusion": "pUniFind establishes a scalable, unified deep learning framework for proteomics, enhancing sensitivity, modification coverage, and interpretability."}}
{"id": "2507.00089", "pdf": "https://arxiv.org/pdf/2507.00089", "abs": "https://arxiv.org/abs/2507.00089", "authors": ["Aho Yapi", "Pierre Latouche", "Arnaud Guillin", "Yan Bailly"], "title": "A new machine learning framework for occupational accidents forecasting with safety inspections integration", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "We propose a generic framework for short-term occupational accident\nforecasting that leverages safety inspections and models accident occurrences\nas binary time series. The approach generates daily predictions, which are then\naggregated into weekly safety assessments to better inform decision making. To\nensure the reliability and operational applicability of the forecasts, we apply\na sliding-window cross-validation procedure specifically designed for time\nseries data, combined with an evaluation based on aggregated period-level\nmetrics. Several machine learning algorithms, including logistic regression,\ntree-based models, and neural networks, are trained and systematically compared\nwithin this framework. Unlike the other approaches, the long short-term memory\n(LSTM) network outperforms the other approaches and detects the upcoming\nhigh-risk periods with a balanced accuracy of 0.86, confirming the robustness\nof our methodology and demonstrating that a binary time series model can\nanticipate these critical periods based on safety inspections. The proposed\nmethodology converts routine safety inspection data into clear weekly risk\nscores, detecting the periods when accidents are most likely. Decision-makers\ncan integrate these scores into their planning tools to classify inspection\npriorities, schedule targeted interventions, and funnel resources to the sites\nor shifts classified as highest risk, stepping in before incidents occur and\ngetting the greatest return on safety investments.", "AI": {"tldr": "A framework for short-term occupational accident forecasting using safety inspections and binary time series models, with LSTM outperforming other methods (0.86 balanced accuracy).", "motivation": "To improve decision-making by converting routine safety inspections into actionable weekly risk scores for proactive accident prevention.", "method": "Uses binary time series modeling, sliding-window cross-validation, and compares machine learning algorithms (logistic regression, tree-based models, neural networks). LSTM is identified as the best performer.", "result": "LSTM achieves 0.86 balanced accuracy in detecting high-risk periods, demonstrating the framework's robustness.", "conclusion": "The methodology effectively transforms inspection data into weekly risk scores, aiding proactive safety planning and resource allocation."}}
{"id": "2507.00814", "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "LLMs in groups show a utilitarian boost in moral dilemmas, similar to humans, but the underlying mechanisms differ.", "motivation": "To understand if LLMs exhibit collective utilitarian reasoning like humans in moral dilemmas.", "method": "Tested six LLM models in solo and group conditions on moral dilemmas, comparing responses.", "result": "LLMs in groups found moral violations more acceptable, but the reasoning differed from humans.", "conclusion": "LLM collective behavior mimics human group reasoning superficially, but the drivers are distinct, impacting AI alignment."}}
{"id": "2408.16553", "pdf": "https://arxiv.org/pdf/2408.16553", "abs": "https://arxiv.org/abs/2408.16553", "authors": ["Zhi-Song Liu", "Markus Buttner", "Vadym Aizinger", "Andreas Rupp"], "title": "Downscaling Neural Network for Coastal Simulations", "categories": ["eess.IV", "cs.LG"], "comment": "13 pages, 12 figures", "summary": "Learning the fine-scale details of a coastal ocean simulation from a coarse\nrepresentation is a challenging task. For real-world applications,\nhigh-resolution simulations are necessary to advance understanding of many\ncoastal processes, specifically, to predict flooding resulting from tsunamis\nand storm surges. We propose a Downscaling Neural Network for Coastal\nSimulation (DNNCS) for spatiotemporal enhancement to efficiently learn the\nhigh-resolution numerical solution. Given images of coastal simulations\nproduced on low-resolution computational meshes using low polynomial order\ndiscontinuous Galerkin discretizations and a coarse temporal resolution, the\nproposed DNNCS learns to produce high-resolution free surface elevation and\nvelocity visualizations in both time and space. To efficiently model the\ndynamic changes over time and space, we propose grid-aware spatiotemporal\nattention to project the temporal features to the spatial domain for non-local\nfeature matching. The coordinate information is also utilized via positional\nencoding. For the final reconstruction, we use the spatiotemporal bilinear\noperation to interpolate the missing frames and then expand the feature maps to\nthe frequency domain for residual mapping. Besides data-driven losses, the\nproposed physics-informed loss guarantees gradient consistency and momentum\nchanges. Their combination contributes to the overall 24% improvements in Root\nMean Square Error (RMSE). To train the proposed model, we propose a novel\ncoastal simulation dataset and use it for model optimization and evaluation.\nOur method shows superior downscaling quality and fast computation compared to\nthe state-of-the-art methods.", "AI": {"tldr": "A Downscaling Neural Network for Coastal Simulation (DNNCS) is proposed to enhance coarse coastal ocean simulations into high-resolution spatiotemporal outputs, improving accuracy by 24% in RMSE.", "motivation": "High-resolution simulations are crucial for understanding coastal processes like flooding from tsunamis and storm surges, but are computationally expensive.", "method": "DNNCS uses grid-aware spatiotemporal attention, positional encoding, and physics-informed loss to enhance coarse inputs into high-resolution outputs.", "result": "The method achieves a 24% improvement in RMSE and outperforms state-of-the-art methods in downscaling quality and speed.", "conclusion": "DNNCS effectively bridges the gap between coarse and high-resolution coastal simulations, offering computational efficiency and accuracy."}}
{"id": "2507.00377", "pdf": "https://arxiv.org/pdf/2507.00377", "abs": "https://arxiv.org/abs/2507.00377", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis", "categories": ["cs.CV"], "comment": "11 pages,3 figures", "summary": "Recent advancements in deep learning for medical image segmentation are often\nlimited by the scarcity of high-quality training data.While diffusion models\nprovide a potential solution by generating synthetic images, their\neffectiveness in medical imaging remains constrained due to their reliance on\nlarge-scale medical datasets and the need for higher image quality. To address\nthese challenges, we present MedDiff-FT, a controllable medical image\ngeneration method that fine-tunes a diffusion foundation model to produce\nmedical images with structural dependency and domain specificity in a\ndata-efficient manner. During inference, a dynamic adaptive guiding mask\nenforces spatial constraints to ensure anatomically coherent synthesis, while a\nlightweight stochastic mask generator enhances diversity through hierarchical\nrandomness injection. Additionally, an automated quality assessment protocol\nfilters suboptimal outputs using feature-space metrics, followed by mask\ncorrosion to refine fidelity. Evaluated on five medical segmentation\ndatasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's\nsegmentation performance by an average of 1% in Dice score. The framework\neffectively balances generation quality, diversity, and computational\nefficiency, offering a practical solution for medical data augmentation. The\ncode is available at https://github.com/JianhaoXie1/MedDiff-FT.", "AI": {"tldr": "MedDiff-FT fine-tunes a diffusion model for efficient medical image generation, ensuring anatomical coherence and diversity, improving segmentation performance by 1% Dice score.", "motivation": "Addressing the scarcity of high-quality medical training data and limitations of existing diffusion models in medical imaging.", "method": "Fine-tunes a diffusion foundation model with dynamic adaptive guiding masks, stochastic mask generation, and automated quality assessment.", "result": "Improves segmentation performance by 1% Dice score on five medical datasets.", "conclusion": "MedDiff-FT offers a practical, efficient solution for medical data augmentation, balancing quality, diversity, and computational efficiency."}}
{"id": "2507.00088", "pdf": "https://arxiv.org/pdf/2507.00088", "abs": "https://arxiv.org/abs/2507.00088", "authors": ["Alexandre S. Pires", "Laurens Samson", "Sennay Ghebreab", "Fernando P. Santos"], "title": "How large language models judge and influence human cooperation", "categories": ["physics.soc-ph", "cs.AI", "cs.SI"], "comment": null, "summary": "Humans increasingly rely on large language models (LLMs) to support decisions\nin social settings. Previous work suggests that such tools shape people's moral\nand political judgements. However, the long-term implications of LLM-based\nsocial decision-making remain unknown. How will human cooperation be affected\nwhen the assessment of social interactions relies on language models? This is a\npressing question, as human cooperation is often driven by indirect\nreciprocity, reputations, and the capacity to judge interactions of others.\nHere, we assess how state-of-the-art LLMs judge cooperative actions. We provide\n21 different LLMs with an extensive set of examples where individuals cooperate\n-- or refuse cooperating -- in a range of social contexts, and ask how these\ninteractions should be judged. Furthermore, through an evolutionary\ngame-theoretical model, we evaluate cooperation dynamics in populations where\nthe extracted LLM-driven judgements prevail, assessing the long-term impact of\nLLMs on human prosociality. We observe a remarkable agreement in evaluating\ncooperation against good opponents. On the other hand, we notice within- and\nbetween-model variance when judging cooperation with ill-reputed individuals.\nWe show that the differences revealed between models can significantly impact\nthe prevalence of cooperation. Finally, we test prompts to steer LLM norms,\nshowing that such interventions can shape LLM judgements, particularly through\ngoal-oriented prompts. Our research connects LLM-based advices and long-term\nsocial dynamics, and highlights the need to carefully align LLM norms in order\nto preserve human cooperation.", "AI": {"tldr": "The paper investigates how LLMs influence human cooperation by analyzing their judgments of cooperative actions and simulating long-term social dynamics.", "motivation": "Understanding the long-term implications of LLM-based social decision-making on human cooperation, driven by indirect reciprocity and reputations.", "method": "Evaluated 21 LLMs on cooperative actions in various social contexts and used an evolutionary game-theoretical model to assess cooperation dynamics.", "result": "LLMs agree on judging cooperation with good opponents but vary for ill-reputed individuals, impacting cooperation prevalence. Prompt interventions can steer LLM norms.", "conclusion": "Aligning LLM norms is crucial to preserving human cooperation, highlighting the need for careful integration of LLM advice in social settings."}}
{"id": "2507.00090", "pdf": "https://arxiv.org/pdf/2507.00090", "abs": "https://arxiv.org/abs/2507.00090", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zarat\u00e9 Pascale"], "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "categories": ["cs.LG", "cs.AI"], "comment": "accepted at IEEE SMC 2025 Vienna", "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "AI": {"tldr": "The paper compares data generation methods for optimizing firefighter resource allocation, evaluating their effectiveness using domain-specific and standard metrics.", "motivation": "To improve firefighter response optimization by generating high-quality synthetic data for scenario simulations.", "method": "Comparison of Random Sampling, Tabular Variational Autoencoders, standard GANs, Conditional Tabular GANs, and Diffusion Probabilistic Models, evaluated with domain-specific and Wasserstein distance metrics.", "result": "The study highlights the challenges of generating synthetic data for highly unbalanced, non-Gaussian distributions and assesses methods' efficacy in capturing real-world intricacies.", "conclusion": "Domain-specific metrics are crucial for evaluating synthetic data quality in firefighting scenarios, as traditional metrics may not suffice."}}
{"id": "2507.00828", "pdf": "https://arxiv.org/pdf/2507.00828", "abs": "https://arxiv.org/abs/2507.00828", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolom\u00e9", "Jordan Boyd-Graber", "Philip Resnik"], "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "A scalable human evaluation protocol and LLM-based proxy for topic model and document-clustering evaluations, validated as statistically indistinguishable from human annotators.", "motivation": "Current evaluation methods for topic models and document clustering either misalign with human preferences or are impractical to scale.", "method": "Design a human evaluation protocol where annotators (or an LLM proxy) infer categories for groups of documents and apply them to others. Validate proxies using crowdworker annotations.", "result": "LLM-based proxies perform comparably to human annotators, making them suitable for automated evaluations.", "conclusion": "The proposed protocol and LLM proxies offer a scalable and effective alternative to traditional evaluation methods."}}
{"id": "2506.10230", "pdf": "https://arxiv.org/pdf/2506.10230", "abs": "https://arxiv.org/abs/2506.10230", "authors": ["Emerson P. Grabke", "Masoom A. Haider", "Babak Taati"], "title": "Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation", "categories": ["eess.IV", "cs.CV"], "comment": "MAH and BT are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "summary": "Objective: Latent diffusion models (LDM) could alleviate data scarcity\nchallenges affecting machine learning development for medical imaging. However,\nmedical LDM strategies typically rely on short-prompt text encoders,\nnon-medical LDMs, or large data volumes. These strategies can limit performance\nand scientific accessibility. We propose a novel LDM conditioning approach to\naddress these limitations. Methods: We propose Class-Conditioned Efficient\nLarge Language model Adapter (CCELLA), a novel dual-head conditioning approach\nthat simultaneously conditions the LDM U-Net with free-text clinical reports\nand radiology classification. We also propose a data-efficient LDM framework\ncentered around CCELLA and a proposed joint loss function. We first evaluate\nour method on 3D prostate MRI against state-of-the-art. We then augment a\ndownstream classifier model training dataset with synthetic images from our\nmethod. Results: Our method achieves a 3D FID score of 0.025 on a size-limited\n3D prostate MRI dataset, significantly outperforming a recent foundation model\nwith FID 0.071. When training a classifier for prostate cancer prediction,\nadding synthetic images generated by our method during training improves\nclassifier accuracy from 69% to 74%. Training a classifier solely on our\nmethod's synthetic images achieved comparable performance to training on real\nimages alone. Conclusion: We show that our method improved both synthetic image\nquality and downstream classifier performance using limited data and minimal\nhuman annotation. Significance: The proposed CCELLA-centric framework enables\nradiology report and class-conditioned LDM training for high-quality medical\nimage synthesis given limited data volume and human data annotation, improving\nLDM performance and scientific accessibility. Code from this study will be\navailable at https://github.com/grabkeem/CCELLA", "AI": {"tldr": "The paper proposes CCELLA, a dual-head conditioning approach for latent diffusion models (LDM) to improve medical image synthesis with limited data and minimal human annotation, enhancing both synthetic image quality and downstream classifier performance.", "motivation": "Address limitations of current medical LDMs, which rely on short-prompt text encoders, non-medical LDMs, or large data volumes, hindering performance and accessibility.", "method": "Introduces CCELLA, a dual-head conditioning approach combining free-text clinical reports and radiology classification, alongside a data-efficient LDM framework and joint loss function.", "result": "Achieves a 3D FID score of 0.025, outperforming a foundation model (FID 0.071). Synthetic images improve classifier accuracy from 69% to 74%.", "conclusion": "CCELLA improves synthetic image quality and downstream tasks with limited data, enhancing LDM performance and accessibility. Code will be publicly available."}}
{"id": "2507.00392", "pdf": "https://arxiv.org/pdf/2507.00392", "abs": "https://arxiv.org/abs/2507.00392", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching plays a fundamental role in many computer vision tasks, yet\nexisting methods heavily rely on scarce and clean multi-view image collections,\nwhich constrains their generalization to diverse and challenging scenarios.\nMoreover, conventional feature encoders are typically trained on single-view 2D\nimages, limiting their capacity to capture 3D-aware correspondences. In this\npaper, we propose a novel two-stage framework that lifts 2D images to 3D space,\nnamed as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and\ndiverse single-view images. To be specific, in the first stage, we learn a\n3D-aware feature encoder using a combination of multi-view image synthesis and\n3D feature Gaussian representation, which injects 3D geometry knowledge into\nthe encoder. In the second stage, a novel-view rendering strategy, combined\nwith large-scale synthetic data generation from single-view images, is employed\nto learn a feature decoder for robust feature matching, thus achieving\ngeneralization across diverse domains. Extensive experiments demonstrate that\nour method achieves superior generalization across zero-shot evaluation\nbenchmarks, highlighting the effectiveness of the proposed framework for robust\nfeature matching.", "AI": {"tldr": "A two-stage framework, Lift to Match (L2M), improves feature matching by leveraging 3D-aware features from single-view images, enhancing generalization across diverse scenarios.", "motivation": "Existing feature matching methods rely on scarce multi-view datasets and lack 3D-awareness, limiting their robustness in diverse scenarios.", "method": "L2M uses a 3D-aware feature encoder (stage 1) and a feature decoder with novel-view rendering (stage 2) to learn from single-view images.", "result": "The method achieves superior generalization in zero-shot benchmarks, demonstrating robust feature matching.", "conclusion": "L2M effectively addresses limitations of conventional methods by incorporating 3D geometry and large-scale synthetic data."}}
{"id": "2507.00093", "pdf": "https://arxiv.org/pdf/2507.00093", "abs": "https://arxiv.org/abs/2507.00093", "authors": ["Binghua Yao", "Joris M. Mooij"], "title": "$\u03c3$-Maximal Ancestral Graphs", "categories": ["cs.DM", "cs.AI", "cs.DS", "math.ST", "stat.TH"], "comment": "It has beee accepted by the 41st Conference on Uncertainty in\n  Artificial Intelligence (UAI)", "summary": "Maximal Ancestral Graphs (MAGs) provide an abstract representation of\nDirected Acyclic Graphs (DAGs) with latent (selection) variables. These\ngraphical objects encode information about ancestral relations and\nd-separations of the DAGs they represent. This abstract representation has been\nused amongst others to prove the soundness and completeness of the FCI\nalgorithm for causal discovery, and to derive a do-calculus for its output. One\nsignificant inherent limitation of MAGs is that they rule out the possibility\nof cyclic causal relationships. In this work, we address that limitation. We\nintroduce and study a class of graphical objects that we coin\n''$\\sigma$-Maximal Ancestral Graphs'' (''$\\sigma$-MAGs''). We show how these\ngraphs provide an abstract representation of (possibly cyclic) Directed Graphs\n(DGs) with latent (selection) variables, analogously to how MAGs represent\nDAGs. We study the properties of these objects and provide a characterization\nof their Markov equivalence classes.", "AI": {"tldr": "The paper introduces \u03c3-MAGs, an extension of MAGs, to represent cyclic causal relationships in directed graphs with latent variables, addressing a key limitation of MAGs.", "motivation": "MAGs cannot represent cyclic causal relationships, limiting their applicability. The work aims to generalize MAGs to include such cases.", "method": "The authors propose \u03c3-MAGs, a new class of graphical objects, and study their properties, including Markov equivalence classes.", "result": "\u03c3-MAGs successfully represent cyclic directed graphs with latent variables, extending the capabilities of MAGs.", "conclusion": "The work provides a theoretical foundation for \u03c3-MAGs, enabling broader applications in causal discovery."}}
{"id": "2507.00101", "pdf": "https://arxiv.org/pdf/2507.00101", "abs": "https://arxiv.org/abs/2507.00101", "authors": ["Giovanni Ruggieri"], "title": "DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "We introduce DFReg, a physics-inspired regularization method for deep neural\nnetworks that operates on the global distribution of weights. Drawing from\nDensity Functional Theory (DFT), DFReg applies a functional penalty to\nencourage smooth, diverse, and well-distributed weight configurations. Unlike\ntraditional techniques such as Dropout or L2 decay, DFReg imposes global\nstructural regularity without architectural changes or stochastic\nperturbations.", "AI": {"tldr": "DFReg is a physics-inspired regularization method for deep neural networks, leveraging Density Functional Theory to globally optimize weight distributions without architectural changes.", "motivation": "Traditional regularization methods like Dropout or L2 decay lack global structural regularity. DFReg aims to address this by ensuring smooth, diverse, and well-distributed weight configurations.", "method": "DFReg applies a functional penalty derived from Density Functional Theory (DFT) to the global distribution of weights, promoting structural regularity.", "result": "The method achieves regularization without requiring architectural modifications or stochastic perturbations.", "conclusion": "DFReg offers a novel, physics-based approach to regularization, enhancing weight distribution globally in deep neural networks."}}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838", "abs": "https://arxiv.org/abs/2507.00838", "authors": ["Karol Przystalski", "Jan K. Argasi\u0144ski", "Iwona Grabska-Gradzi\u0144ska", "Jeremi K. Ochab"], "title": "Stylometry recognizes human and LLM-generated texts in short samples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "The paper uses stylometry to differentiate between LLM-generated and human-written texts, achieving high accuracy with tree-based models and stylometric features.", "motivation": "Addressing model attribution, intellectual property, and ethical AI use by identifying emergent writing patterns in LLM-generated texts.", "method": "Created a benchmark dataset with human and LLM-generated texts, applied tree-based models (decision trees, LightGBM) using stylometric features (lexical, grammatical, syntactic, punctuation).", "result": "Achieved up to .87 Matthews correlation coefficient in multiclass and .79-1. accuracy in binary classification, with GPT-4 vs. Wikipedia reaching .98 accuracy.", "conclusion": "Stylometry can effectively distinguish machine-generated from human-written texts for well-defined text types, even with sophisticated LLMs."}}
{"id": "2506.20206", "pdf": "https://arxiv.org/pdf/2506.20206", "abs": "https://arxiv.org/abs/2506.20206", "authors": ["Yang Li"], "title": "Volumetric segmentation of muscle compartments using in vivo imaging and architectural validation in human finger flexors", "categories": ["eess.IV"], "comment": "19 pages, 13 figures", "summary": "Segmenting muscle compartments and measuring their architecture can\nfacilitate movement function assessment, accurate musculoskeletal modeling, and\nsynergy-based electromyogram simulation. Here, we presented a novel method for\nvolumetric segmentation of muscle compartments using in vivo imaging, focusing\non the independent compartments for finger control of flexor digitorum\nsuperficialis (FDS). Besides, we measured the architectural properties of FDS\ncompartments and validated the segmentation. Specifically, ultrasound and\nmagnetic resonance imaging (MRI) from 10 healthy subjects were used for\nsegmentation and measurement, while electromyography was utilized for\nvalidation. A two-step piecewise segmentation was proposed, first annotating\ncompartment regions in the cross-sectional ultrasound image based on\ncompartment movement, and then performing minimum energy matching to register\nthe ultrasound data to the three-dimensional MRI coordinate system.\nAdditionally, the architectural properties were measured in the compartment\nmasks from the segmentation using MRI tractography. Anatomical correctness was\nverified by comparing known anatomy with reconstructed fiber tracts and\nmeasured properties, while segmentation accuracy was quantified as the\npercentage of finger electromyogram centers falling within their corresponding\ncompartments. Results demonstrated agreement for the fiber orientation between\nthe tractography and cadaveric photographs. Significant differences in\narchitectural properties (P < 0.001) were observed between compartments. The\nproperties of FDS and its compartments were within the physiological ranges (P\n< 0.01). 95% (38/40) of the electromyogram centers were located within\nrespective compartments, with 2 errors occurring in the index and little\nfingers. The validated segmentation method and derived architectural properties\nmay advance biomedical applications.", "AI": {"tldr": "A novel method for volumetric segmentation of muscle compartments using in vivo imaging was presented, focusing on the flexor digitorum superficialis (FDS) for finger control. Ultrasound and MRI were used for segmentation, while electromyography validated the results. The method showed high accuracy and physiological relevance.", "motivation": "To improve movement function assessment, musculoskeletal modeling, and electromyogram simulation by accurately segmenting muscle compartments and measuring their architecture.", "method": "A two-step piecewise segmentation: annotating compartment regions in ultrasound images based on movement, then registering to MRI. Architectural properties were measured using MRI tractography. Validation involved comparing fiber tracts and electromyogram centers.", "result": "Fiber orientation matched cadaveric data. Significant differences in architectural properties between compartments were found. 95% of electromyogram centers were correctly located.", "conclusion": "The validated method and derived properties can advance biomedical applications, demonstrating high accuracy and physiological relevance."}}
{"id": "2507.00401", "pdf": "https://arxiv.org/pdf/2507.00401", "abs": "https://arxiv.org/abs/2507.00401", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "AI": {"tldr": "The paper introduces the MIV-head, a novel approach for cross-domain few-shot learning without fine-tuning backbones, achieving competitive accuracy with lower adaptation costs.", "motivation": "Addressing the challenge of few-shot learning with frozen backbones, which is common in practical scenarios, by improving low-quality static embeddings.", "method": "Proposes the MIV-head, a backbone-agnostic and efficient classification head, trained on few-shot target domain data for meta-testing.", "result": "MIV-head outperforms adapter methods and traditional classification heads in accuracy while being more cost-effective.", "conclusion": "The MIV-head is a viable solution for cross-domain few-shot learning without backbone fine-tuning, validated by extensive experiments."}}
{"id": "2507.00094", "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications.", "AI": {"tldr": "The paper introduces a novel algorithm for computing data-aware optimal alignments in declarative process models, combining A* search and SMT solving for efficiency and expressiveness.", "motivation": "Existing alignment-based conformance checking for declarative process models lacks support for general data types and conditions, limiting its applicability.", "method": "The approach combines A* search and SMT solving, using repair actions to resolve constraint violations incrementally.", "result": "The algorithm efficiently computes optimal alignments, outperforming state-of-the-art methods while supporting more expressive data dependencies.", "conclusion": "The proposed method advances conformance checking by enabling efficient and expressive data-aware alignment computation, with potential for real-world applications."}}
{"id": "2507.00102", "pdf": "https://arxiv.org/pdf/2507.00102", "abs": "https://arxiv.org/abs/2507.00102", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "AI": {"tldr": "The paper presents a data-driven, interpretable fault detection method for industrial quality control, combining machine learning, Shapley Additive Explanations, and domain-specific visualization, achieving high accuracy and expert-validated interpretability.", "motivation": "To address the limitations of conventional quality control (lack of adaptability) and black-box machine learning models (lack of interpretability) in industrial settings.", "method": "Integrates supervised machine learning for fault classification, Shapley Additive Explanations for interpretability, and domain-specific visualization. Evaluates explanations via perturbation analysis and expert assessment.", "result": "Achieves 95.9% fault detection accuracy; explanations are validated as relevant and interpretable.", "conclusion": "The human-centric approach enhances trust in data-driven fault detection, benefiting industrial quality control."}}
{"id": "2507.00883", "pdf": "https://arxiv.org/pdf/2507.00883", "abs": "https://arxiv.org/abs/2507.00883", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "categories": ["cs.CL"], "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "The paper examines cultural bias in math problem presentation, adapting GSM8K for five regions and testing LLMs. Results show models perform best on US-centric data but reasoning-capable models handle cultural shifts better.", "motivation": "To address cultural neutrality in math problem presentation, as existing benchmarks like GSM8K are Western-centric.", "method": "Created culturally adapted GSM8K variants for five regions, evaluated six LLMs with five prompting strategies.", "result": "Models perform best on US-centric data; reasoning-capable models are more resilient to cultural shifts.", "conclusion": "Deeper reasoning helps mitigate cultural presentation gaps in math tasks."}}
{"id": "2506.22397", "pdf": "https://arxiv.org/pdf/2506.22397", "abs": "https://arxiv.org/abs/2506.22397", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "4 figures, 10 pages + refs, 40 pages total (including supplement), 24\n  supplementary figures", "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "AI": {"tldr": "HazeMatching is a novel iterative method for dehazing microscopy images, balancing fidelity and realism by adapting conditional flow matching.", "motivation": "Cheaper microscopy modalities like widefield produce hazy images; existing dehazing methods prioritize either fidelity or realism, not both.", "method": "HazeMatching uses conditional flow matching, guiding the generative process with hazy observations in the conditional velocity field.", "result": "Evaluated on 5 datasets, HazeMatching outperforms 7 baselines, balancing fidelity and realism, and produces well-calibrated predictions.", "conclusion": "HazeMatching effectively balances fidelity and realism in dehazing microscopy images without needing an explicit degradation operator."}}
{"id": "2507.00429", "pdf": "https://arxiv.org/pdf/2507.00429", "abs": "https://arxiv.org/abs/2507.00429", "authors": ["Jingyi Pan", "Dan Xu", "Qiong Luo"], "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting", "categories": ["cs.CV"], "comment": "ICCV 2025, Project page: https://rorisis.github.io/DiGA3D/", "summary": "Developing a unified pipeline that enables users to remove, re-texture, or\nreplace objects in a versatile manner is crucial for text-guided 3D inpainting.\nHowever, there are still challenges in performing multiple 3D inpainting tasks\nwithin a unified framework: 1) Single reference inpainting methods lack\nrobustness when dealing with views that are far from the reference view. 2)\nAppearance inconsistency arises when independently inpainting multi-view images\nwith 2D diffusion priors; 3) Geometry inconsistency limits performance when\nthere are significant geometric changes in the inpainting regions. To tackle\nthese challenges, we introduce DiGA3D, a novel and versatile 3D inpainting\npipeline that leverages diffusion models to propagate consistent appearance and\ngeometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy\nfor selecting multiple reference views to reduce errors during propagation.\nNext, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that\npropagates attention features from the selected reference views to other views\nvia diffusion models to maintain appearance consistency. Furthermore, DiGA3D\nintroduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to\nfurther improve the geometric consistency of inpainted 3D scenes. Extensive\nexperiments on multiple 3D inpainting tasks demonstrate the effectiveness of\nour method. The project page is available at https://rorisis.github.io/DiGA3D/.", "AI": {"tldr": "DiGA3D is a unified 3D inpainting pipeline using diffusion models for consistent appearance and geometry, addressing challenges like view robustness and inconsistency.", "motivation": "Challenges in unified 3D inpainting include lack of robustness in single-reference methods, appearance inconsistency, and geometry inconsistency.", "method": "DiGA3D uses multiple reference views, Attention Feature Propagation (AFP), and Texture-Geometry Score Distillation Sampling (TG-SDS) loss for consistency.", "result": "Extensive experiments show DiGA3D's effectiveness in various 3D inpainting tasks.", "conclusion": "DiGA3D provides a robust and versatile solution for 3D inpainting with improved consistency."}}
{"id": "2507.00096", "pdf": "https://arxiv.org/pdf/2507.00096", "abs": "https://arxiv.org/abs/2507.00096", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "title": "AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets", "categories": ["cs.CR", "cs.AI"], "comment": "8 Pages, 1 figure", "summary": "Alternative Assets tokenization is transforming non-traditional financial\ninstruments are represented and traded on the web. However, ensuring\ntrustworthiness in web-based tokenized ecosystems poses significant challenges,\nfrom verifying off-chain asset data to enforcing regulatory compliance. This\npaper proposes an AI-governed agent architecture that integrates intelligent\nagents with blockchain to achieve web-trustworthy tokenization of alternative\nassets. In the proposed architecture, autonomous agents orchestrate the\ntokenization process (asset verification, valuation, compliance checking, and\nlifecycle management), while an AI-driven governance layer monitors agent\nbehavior and enforces trust through adaptive policies and cryptoeconomic\nincentives. We demonstrate that this approach enhances transparency, security,\nand compliance in asset tokenization, addressing key concerns around data\nauthenticity and fraud. A case study on tokenizing real estate assets\nillustrates how the architecture mitigates risks (e.g., fraudulent listings and\nmoney laundering) through real-time AI anomaly detection and on-chain\nenforcement. Our evaluation and analysis suggest that combining AI governance\nwith multi-agent systems and blockchain can significantly bolster trust in\ntokenized asset ecosystems. This work offers a novel framework for trustworthy\nasset tokenization on the web and provides insights for practitioners aiming to\ndeploy secure, compliant tokenization platforms.", "AI": {"tldr": "The paper proposes an AI-governed agent architecture combining intelligent agents and blockchain to ensure trustworthy tokenization of alternative assets, enhancing transparency, security, and compliance.", "motivation": "Addressing challenges in trustworthiness for web-based tokenized ecosystems, such as verifying off-chain asset data and regulatory compliance.", "method": "An AI-governed agent architecture with autonomous agents handling tokenization processes and an AI-driven governance layer enforcing trust through adaptive policies and incentives.", "result": "The approach improves transparency, security, and compliance, demonstrated via a real estate tokenization case study.", "conclusion": "Combining AI governance, multi-agent systems, and blockchain significantly bolsters trust in tokenized asset ecosystems, offering a novel framework for practitioners."}}
{"id": "2507.00105", "pdf": "https://arxiv.org/pdf/2507.00105", "abs": "https://arxiv.org/abs/2507.00105", "authors": ["Javier Castellano", "Ignacio Villanueva"], "title": "Graph Neural Networks in Wind Power Forecasting", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "We study the applicability of GNNs to the problem of wind energy forecasting.\nWe find that certain architectures achieve performance comparable to our best\nCNN-based benchmark. The study is conducted on three wind power facilities\nusing five years of historical data. Numerical Weather Prediction (NWP)\nvariables were used as predictors, and models were evaluated on a 24 to 36 hour\nahead test horizon.", "AI": {"tldr": "GNNs show comparable performance to CNNs in wind energy forecasting using NWP variables over 24-36 hours.", "motivation": "To explore the effectiveness of GNNs in wind energy forecasting compared to traditional CNN-based methods.", "method": "Evaluated GNN architectures against CNN benchmarks using five years of historical data from three wind facilities, with NWP variables as predictors.", "result": "Certain GNN architectures matched the performance of the best CNN benchmarks.", "conclusion": "GNNs are a viable alternative to CNNs for wind energy forecasting, offering comparable accuracy."}}
{"id": "2507.00885", "pdf": "https://arxiv.org/pdf/2507.00885", "abs": "https://arxiv.org/abs/2507.00885", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "Downstream scaling laws predict task performance from pretraining losses, but linear trends are rare (39% cases) and sensitive to experimental changes. Understanding conditions for scaling laws is crucial.", "motivation": "To clarify whether downstream scaling laws can reliably predict task performance, given conflicting evidence about linear trends and challenges like emergence.", "method": "Meta-analysis of existing data on downstream scaling laws, examining fit to linear trends and impact of experimental changes.", "result": "Linear scaling trends occur only 39% of the time, and minor experimental changes can alter scaling behavior.", "conclusion": "Scaling laws require deeper understanding of conditions for success, including cases deviating from linear trends."}}
{"id": "2506.23298", "pdf": "https://arxiv.org/pdf/2506.23298", "abs": "https://arxiv.org/abs/2506.23298", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "categories": ["eess.IV"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference", "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off.", "AI": {"tldr": "CALIN is an inference-time calibration method addressing calibration biases and demographic unfairness in MLLMs for medical image classification, improving accuracy and fairness.", "motivation": "Ensuring safe deployment of MLLMs in clinical practice requires analyzing prediction accuracies and calibration errors across demographic subgroups.", "method": "CALIN uses a bi-level procedure to estimate calibration matrices (population to subgroup level) and applies them to calibrate confidence scores during inference.", "result": "CALIN improves prediction accuracy and ensures fair confidence calibration across three medical imaging datasets with minimal fairness-utility trade-off.", "conclusion": "CALIN effectively mitigates biases in MLLMs, enhancing their reliability for medical image classification in diverse populations."}}
{"id": "2507.00430", "pdf": "https://arxiv.org/pdf/2507.00430", "abs": "https://arxiv.org/abs/2507.00430", "authors": ["Huanxin Yang", "Qiwen Wang"], "title": "MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten mathematical expression recognition (HMER) suffers from complex\nformula structures and character layouts in sequence prediction. In this paper,\nwe incorporate frequency domain analysis into HMER and propose a method that\nmarries frequency domain with HMER (MFH), leveraging the discrete cosine\ntransform (DCT). We emphasize the structural analysis assistance of frequency\ninformation for recognizing mathematical formulas. When implemented on various\nbaseline models, our network exhibits a consistent performance enhancement,\ndemonstrating the efficacy of frequency domain information. Experiments show\nthat our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on\nthe CROHME 2014/2016/2019 test sets. The source code is available at\nhttps://github.com/Hryxyhe/MFH.", "AI": {"tldr": "The paper introduces MFH, a method combining frequency domain analysis with HMER using DCT, improving recognition accuracy on CROHME datasets.", "motivation": "HMER struggles with complex formula structures and layouts; frequency domain analysis is proposed to assist structural recognition.", "method": "Incorporates DCT-based frequency domain analysis into HMER, creating the MFH method.", "result": "MFH-CoMER achieves 61.66%/62.07%/63.72% accuracy on CROHME 2014/2016/2019 test sets.", "conclusion": "Frequency domain information enhances HMER performance, as demonstrated by MFH's consistent improvements."}}
{"id": "2507.00108", "pdf": "https://arxiv.org/pdf/2507.00108", "abs": "https://arxiv.org/abs/2507.00108", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "comment": null, "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.", "AI": {"tldr": "The paper discusses the impact of AI-driven code generation tools on programming education, advocating for a shift towards teaching code comprehension and execution using visual tools.", "motivation": "The transformation in programming education due to AI tools necessitates reevaluating teaching methods to ensure deeper student understanding.", "method": "The article reviews existing studies and proposes integrating visual representations and simulations of code execution into teaching methodologies.", "result": "Visual tools are highlighted as effective for teaching and assessing programming, supported by student feedback from an object-oriented programming course.", "conclusion": "The paper concludes that visual simulations enhance learning and should be incorporated into programming education."}}
{"id": "2507.00184", "pdf": "https://arxiv.org/pdf/2507.00184", "abs": "https://arxiv.org/abs/2507.00184", "authors": ["Jacob Schrum", "Olivia Kilday", "Emilio Salas", "Bess Hagan", "Reid Williams"], "title": "Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent research shows how diffusion models can unconditionally generate\ntile-based game levels, but use of diffusion models for text-to-level\ngeneration is underexplored. There are practical considerations for creating a\nusable model: caption/level pairs are needed, as is a text embedding model, and\na way of generating entire playable levels, rather than individual scenes. We\npresent strategies to automatically assign descriptive captions to an existing\nlevel dataset, and train diffusion models using both pretrained text encoders\nand simple transformer models trained from scratch. Captions are automatically\nassigned to generated levels so that the degree of overlap between input and\noutput captions can be compared. We also assess the diversity and playability\nof the resulting levels. Results are compared with an unconditional diffusion\nmodel and a generative adversarial network, as well as the text-to-level\napproaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model\nuses a simple transformer model for text embedding, and takes less time to\ntrain than diffusion models employing more complex text encoders, indicating\nthat reliance on larger language models is not necessary. We also present a GUI\nallowing designers to construct long levels from model-generated scenes.", "AI": {"tldr": "The paper explores text-to-level generation using diffusion models, addressing practical needs like caption/level pairs and playable level generation. It compares results with other models and introduces a GUI for designers.", "motivation": "To advance text-to-level generation in tile-based games, addressing gaps in using diffusion models for this purpose.", "method": "Automatically assigns captions to levels, trains diffusion models with pretrained and simple transformer text encoders, and evaluates diversity and playability.", "result": "The best diffusion model uses a simple transformer, trains faster, and performs comparably to models with complex encoders.", "conclusion": "Simpler text encoders can be effective for text-to-level generation, and a GUI aids in practical level design."}}
{"id": "2507.00891", "pdf": "https://arxiv.org/pdf/2507.00891", "abs": "https://arxiv.org/abs/2507.00891", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "MemeCMD is a Chinese multi-turn dialogue dataset with contextually retrieved memes, addressing the lack of multimodal expressiveness in existing datasets.", "motivation": "Existing dialogue datasets lack multimodal expressiveness and contextual nuance, limiting their effectiveness for modern social interactions.", "method": "Combines a large-scale, MLLM-annotated meme library with auto-generated dialogues by dual agents, using a retrieval framework and adaptive threshold for contextual relevance.", "result": "The approach generates contextually appropriate and diverse meme-incorporated dialogues effectively.", "conclusion": "MemeCMD provides a scalable, privacy-preserving resource for advancing multimodal conversational AI."}}
{"id": "2506.23309", "pdf": "https://arxiv.org/pdf/2506.23309", "abs": "https://arxiv.org/abs/2506.23309", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarak I. Hoque", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "summary": "In contemporary surgical research and practice, accurately comprehending 3D\nsurgical scenes with text-promptable capabilities is particularly crucial for\nsurgical planning and real-time intra-operative guidance, where precisely\nidentifying and interacting with surgical tools and anatomical structures is\nparamount. However, existing works focus on surgical vision-language model\n(VLM), 3D reconstruction, and segmentation separately, lacking support for\nreal-time text-promptable 3D queries. In this paper, we present SurgTPGS, a\nnovel text-promptable Gaussian Splatting method to fill this gap. We introduce\na 3D semantics feature learning strategy incorporating the Segment Anything\nmodel and state-of-the-art vision-language models. We extract the segmented\nlanguage features for 3D surgical scene reconstruction, enabling a more\nin-depth understanding of the complex surgical environment. We also propose\nsemantic-aware deformation tracking to capture the seamless deformation of\nsemantic features, providing a more precise reconstruction for both texture and\nsemantic features. Furthermore, we present semantic region-aware optimization,\nwhich utilizes regional-based semantic information to supervise the training,\nparticularly promoting the reconstruction quality and semantic smoothness. We\nconduct comprehensive experiments on two real-world surgical datasets to\ndemonstrate the superiority of SurgTPGS over state-of-the-art methods,\nhighlighting its potential to revolutionize surgical practices. SurgTPGS paves\nthe way for developing next-generation intelligent surgical systems by\nenhancing surgical precision and safety. Our code is available at:\nhttps://github.com/lastbasket/SurgTPGS.", "AI": {"tldr": "SurgTPGS introduces a text-promptable Gaussian Splatting method for 3D surgical scene understanding, combining segmentation and vision-language models for real-time queries and precise reconstruction.", "motivation": "Accurate 3D surgical scene comprehension with text-promptable capabilities is critical for surgical planning and intra-operative guidance, but existing methods lack real-time text-promptable 3D queries.", "method": "Uses a 3D semantics feature learning strategy with Segment Anything and vision-language models, semantic-aware deformation tracking, and semantic region-aware optimization.", "result": "Outperforms state-of-the-art methods on real-world surgical datasets, improving reconstruction quality and semantic smoothness.", "conclusion": "SurgTPGS enhances surgical precision and safety, paving the way for next-generation intelligent surgical systems."}}
{"id": "2507.00454", "pdf": "https://arxiv.org/pdf/2507.00454", "abs": "https://arxiv.org/abs/2507.00454", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "AI": {"tldr": "ATSTrack addresses misalignment in Visual-Language Tracking by aligning temporal and spatial scales of inputs, improving feature modification.", "motivation": "Misalignment between visual and language inputs due to target movement and inherent scale differences hinders tracking performance.", "method": "Decomposes language descriptions into phrases with temporal/spatial attributes, modifies features finely, and uses a Visual-Language token for guidance.", "result": "ATSTrack achieves performance comparable to existing methods.", "conclusion": "The proposed method effectively reduces misalignment and improves tracking by addressing scale differences."}}
{"id": "2507.00145", "pdf": "https://arxiv.org/pdf/2507.00145", "abs": "https://arxiv.org/abs/2507.00145", "authors": ["Hasan Yi\u011fit"], "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "AI": {"tldr": "AI-Hybrid TRNG is a lightweight, deep-learning framework that generates high-entropy random numbers using low-cost hardware, eliminating the need for expensive quantum devices.", "motivation": "To provide a cost-effective and scalable solution for generating high-quality random numbers without relying on bulky or expensive hardware.", "method": "Uses a dynamic inner-outer network combining adaptive natural sources (RF front end and CPU-timing jitter) and reseeding to produce unpredictable sequences.", "result": "Generates 32-bit high-entropy streams passing NIST SP 800-22 and bespoke statistical tests, meeting cryptographic standards with no exploitable biases.", "conclusion": "AI-Hybrid TRNG is deployable on resource-constrained platforms, expanding access to high-integrity random numbers for secure systems and applications."}}
{"id": "2507.00191", "pdf": "https://arxiv.org/pdf/2507.00191", "abs": "https://arxiv.org/abs/2507.00191", "authors": ["Eray Erturk", "Fahad Kamran", "Salar Abbaspourazad", "Sean Jewell", "Harsh Sharma", "Yujie Li", "Sinead Williamson", "Nicholas J Foti", "Joseph Futoma"], "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Wearable devices record physiological and behavioral signals that can improve\nhealth predictions. While foundation models are increasingly used for such\npredictions, they have been primarily applied to low-level sensor data, despite\nbehavioral data often being more informative due to their alignment with\nphysiologically relevant timescales and quantities. We develop foundation\nmodels of such behavioral signals using over 2.5B hours of wearable data from\n162K individuals, systematically optimizing architectures and tokenization\nstrategies for this unique dataset. Evaluated on 57 health-related tasks, our\nmodel shows strong performance across diverse real-world applications including\nindividual-level classification and time-varying health state prediction. The\nmodel excels in behavior-driven tasks like sleep prediction, and improves\nfurther when combined with representations of raw sensor data. These results\nunderscore the importance of tailoring foundation model design to wearables and\ndemonstrate the potential to enable new health applications.", "AI": {"tldr": "A foundation model for behavioral signals from wearables improves health predictions, outperforming in tasks like sleep prediction and combining well with raw sensor data.", "motivation": "Behavioral data from wearables is more informative for health predictions than low-level sensor data, but foundation models have not been tailored for it.", "method": "Developed foundation models using 2.5B hours of wearable data from 162K individuals, optimizing architectures and tokenization strategies.", "result": "Strong performance on 57 health tasks, excelling in behavior-driven tasks like sleep prediction, with further improvement when combined with raw sensor data.", "conclusion": "Tailoring foundation models to wearables enhances health predictions and enables new applications."}}
{"id": "2507.00911", "pdf": "https://arxiv.org/pdf/2507.00911", "abs": "https://arxiv.org/abs/2507.00911", "authors": ["Luise H\u00e4user", "Alexandros Stamatakis"], "title": "The Cognate Data Bottleneck in Language Phylogenetics", "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "The paper highlights the challenge of applying computational phylogenetic methods to cognate data due to insufficient dataset sizes and demonstrates the limitations of automated dataset generation from multilingual resources like BabelNet.", "motivation": "To explore the feasibility of leveraging computational phylogenetic methods for cognate data, given the lack of large manually collected datasets.", "method": "Automatically extracted cognate datasets from BabelNet and evaluated phylogenetic inferences against gold standard trees.", "result": "Phylogenetic trees from BabelNet data were inconsistent with established gold standards, suggesting limitations in automated dataset generation.", "conclusion": "Current computational approaches requiring large datasets are not viable for cognate data, leaving their application in historical linguistics uncertain."}}
{"id": "2507.00462", "pdf": "https://arxiv.org/pdf/2507.00462", "abs": "https://arxiv.org/abs/2507.00462", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Xinyuan Gao", "Yihong Gong"], "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.", "AI": {"tldr": "MS-TTA improves CLIP's test-time adaptation by refining all test samples with kNN Mean-Shift, outperforming existing methods without additional training.", "motivation": "VLMs like CLIP struggle with distribution shifts at test time, and existing TTA methods ignore low-confidence samples.", "method": "Proposes MS-TTA, a training-free approach using kNN Mean-Shift to refine features beyond CLIP's space, enhancing compactness and separability.", "result": "Outperforms state-of-the-art TTA methods on OOD and cross-dataset benchmarks, achieving robust adaptation.", "conclusion": "MS-TTA offers a simple yet effective training-free solution for improving CLIP's test-time adaptation."}}
{"id": "2507.00161", "pdf": "https://arxiv.org/pdf/2507.00161", "abs": "https://arxiv.org/abs/2507.00161", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments.", "AI": {"tldr": "AI-mediated storytelling reduces political polarization by adapting narratives to sustain emotional engagement and promote perspective-taking.", "motivation": "Political polarization hinders civic education; AI can mitigate this by fostering open-mindedness through emotionally-responsive narratives.", "method": "Developed an AI-DCS platform using facial emotion recognition, attention tracking, and GPT-4 for adaptive storytelling.", "result": "The platform personalizes narratives to sustain engagement and encourage perspective-taking toward political out-groups.", "conclusion": "AI-supported emotionally-sensitive strategies can address polarization while preserving learner autonomy, with implications for civic education and HCI."}}
{"id": "2507.00230", "pdf": "https://arxiv.org/pdf/2507.00230", "abs": "https://arxiv.org/abs/2507.00230", "authors": ["Peilin He", "James Joshi"], "title": "PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction", "categories": ["cs.LG", "cs.CR"], "comment": "This paper is under review; do not distribute", "summary": "Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging, particularly in\ncollaborative scenarios where centralized training poses significant privacy\nrisks, including data leakage and inference attacks, as well as high\ncomputational costs. We propose a novel Privacy-Preserving Federated\nLearning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image\nreconstruction. PPFL-RDSN integrates Federated Learning (FL), local\ndifferential privacy, and robust model watermarking techniques, ensuring data\nremains secure on local devices, safeguarding sensitive information, and\nmaintaining model authenticity without revealing underlying data. Empirical\nevaluations show that PPFL-RDSN achieves comparable performance to the\nstate-of-the-art centralized methods while reducing computational burdens, and\neffectively mitigates security and privacy vulnerabilities, making it a\npractical solution for secure and privacy-preserving collaborative computer\nvision applications.", "AI": {"tldr": "PPFL-RDSN combines Federated Learning, local differential privacy, and model watermarking for secure, privacy-preserving image reconstruction, matching centralized methods' performance.", "motivation": "Centralized training for image reconstruction poses privacy risks (data leakage, inference attacks) and high computational costs.", "method": "Integrates Federated Learning, local differential privacy, and robust model watermarking to secure data on local devices.", "result": "Achieves comparable performance to centralized methods, reduces computational burden, and mitigates security/privacy risks.", "conclusion": "PPFL-RDSN is a practical solution for secure, privacy-preserving collaborative computer vision."}}
{"id": "2507.00985", "pdf": "https://arxiv.org/pdf/2507.00985", "abs": "https://arxiv.org/abs/2507.00985", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "categories": ["cs.CL"], "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "The paper explores moral self-correction in LLMs, identifying paradoxes in its effectiveness and proposing solutions based on heuristic analysis of discourse constructions.", "motivation": "To address the paradoxes in moral self-correction in LLMs, where superficial effectiveness and inconsistency in diagnosing moral issues hinder alignment with human values.", "method": "Analyzes discourse constructions in fine-tuning corpora to uncover heuristics, demonstrating their role in moral self-correction and their limitations.", "result": "Reveals that heuristic shortcuts in discourse constructions cause inconsistency in self-correction and self-diagnosis, proposing a solution using curated datasets.", "conclusion": "Highlights the need for improved moral self-correction techniques and addresses generalization challenges, particularly in context learning and model scalability."}}
{"id": "2507.00469", "pdf": "https://arxiv.org/pdf/2507.00469", "abs": "https://arxiv.org/abs/2507.00469", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "categories": ["cs.CV", "cs.LG"], "comment": "23 pages, 12 figures, 10 tables", "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "AI": {"tldr": "Bisecle is a video-language continual learning method inspired by hippocampal memory mechanisms, addressing catastrophic forgetting and update conflicts in VLMs through multi-directional supervision and contrastive prompt learning.", "motivation": "Real-world videos evolve continuously, requiring VLMs to adapt without prohibitive computational costs, but existing methods struggle with forgetting and conflicts in parameter-efficient updates.", "method": "Bisecle uses multi-directional supervision for cross-modal relationships and contrastive prompt learning to isolate task-specific knowledge, mimicking hippocampal memory processes.", "result": "Bisecle mitigates forgetting and improves cross-task generalization on VideoQA benchmarks.", "conclusion": "Bisecle offers a robust and efficient solution for continual learning in video understanding tasks by leveraging hippocampal-inspired mechanisms."}}
{"id": "2507.00225", "pdf": "https://arxiv.org/pdf/2507.00225", "abs": "https://arxiv.org/abs/2507.00225", "authors": ["S. V. Chekanov", "H. Kjellerstrand"], "title": "Discovering the underlying analytic structure within Standard Model constants using artificial intelligence", "categories": ["hep-ph", "cs.AI", "physics.data-an"], "comment": "42 pages, 10 tables", "summary": "This paper presents a search for underlying analytic structures among the\nfundamental parameters of the Standard Model (SM) using symbolic regression and\ngenetic programming. We identify the simplest analytic relationships connecting\npairs of these constants and report several notable observations based on about\na thousand expressions with relative precision better than 1%. These results\nmay serve as valuable inputs for model builders and artificial intelligence\nmethods aimed at uncovering hidden patterns among the SM constants, or\npotentially used as building blocks for a deeper underlying law that connects\nall parameters of the SM through a small set of fundamental constants.", "AI": {"tldr": "The paper uses symbolic regression and genetic programming to find simple analytic relationships among Standard Model (SM) parameters, identifying precise expressions for potential deeper laws.", "motivation": "To uncover hidden patterns or underlying structures connecting the fundamental constants of the SM.", "method": "Symbolic regression and genetic programming are applied to search for analytic relationships among SM parameters.", "result": "About a thousand precise expressions (better than 1% relative precision) connecting pairs of SM constants are identified.", "conclusion": "The findings could aid model builders and AI methods in discovering deeper laws linking all SM parameters."}}
{"id": "2507.00234", "pdf": "https://arxiv.org/pdf/2507.00234", "abs": "https://arxiv.org/abs/2507.00234", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "AI": {"tldr": "A novel framework combines ResNet and Transformer heatmaps for improved interpretability, addressing spatial-temporal misalignment. It achieves high accuracy in clinical and industrial datasets and includes an NLP module for narrative explanations.", "motivation": "Existing interpretability methods suffer from spatial-temporal misalignment, limiting actionable insights in critical domains like healthcare and industrial monitoring.", "method": "Integrates gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization, ensuring spatial-temporal alignment. Includes an NLP module for narrative explanations.", "result": "Achieves 94.1% accuracy on PhysioNet and RMSE = 0.28 kWh on UCI Energy dataset, outperforming baselines by 3.8-12.4%. NLP module scores BLEU-4 (0.586) and ROUGE-L (0.650).", "conclusion": "The framework bridges technical outputs and stakeholder understanding, offering scalable, transparent decision-making with causal fidelity and alignment."}}
{"id": "2507.00994", "pdf": "https://arxiv.org/pdf/2507.00994", "abs": "https://arxiv.org/abs/2507.00994", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "Andr\u00e9 F. T. Martins", "C\u00e9line Hudelot", "Pierre Colombo"], "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "The paper investigates whether CLM-trained models outperform MLM-trained ones in text representation tasks, finding MLM generally better but CLM more data-efficient. A biphasic CLM-MLM training strategy is optimal.", "motivation": "To determine if CLM's advantages in text representation tasks are inherent or due to confounding factors like model/data scale.", "method": "Large-scale pretraining ablations with 30 models (210M-1B parameters) and 15,000+ fine-tuning runs, comparing MLM and CLM.", "result": "MLM performs better overall, but CLM is more data-efficient and stable. A biphasic CLM-MLM strategy yields optimal performance.", "conclusion": "Combining CLM and MLM training is optimal, especially when leveraging existing CLM models, reducing computational costs for top-tier encoders."}}
{"id": "2507.00472", "pdf": "https://arxiv.org/pdf/2507.00472", "abs": "https://arxiv.org/abs/2507.00472", "authors": ["Ying Guo", "Xi Liu", "Cheng Zhen", "Pengfei Yan", "Xiaoming Wei"], "title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations", "categories": ["cs.CV"], "comment": "ICCV 2025. Homepage: https://jinyugy21.github.io/ARIG/", "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.", "AI": {"tldr": "The paper proposes ARIG, an autoregressive frame-wise framework for real-time interactive head generation, addressing limitations of clip-wise methods by using diffusion-based motion prediction and emphasizing interactive and conversational state understanding.", "motivation": "Face-to-face communication drives the need for realistic virtual agents with listening and speaking capabilities, but existing methods struggle with real-time performance and realism due to signal acquisition and behavioral understanding issues.", "method": "ARIG uses a non-vector-quantized autoregressive process for motion prediction, leveraging diffusion for continuous space accuracy. It incorporates interactive behavior understanding (IBU) and conversational state understanding (CSU) for realism.", "result": "Experiments confirm ARIG's effectiveness in real-time generation with improved interaction realism.", "conclusion": "ARIG advances interactive head generation by combining autoregressive prediction with diffusion and contextual understanding, achieving real-time and realistic results."}}
{"id": "2507.00257", "pdf": "https://arxiv.org/pdf/2507.00257", "abs": "https://arxiv.org/abs/2507.00257", "authors": ["Davide Salaorni", "Vincenzo De Paola", "Samuele Delpero", "Giovanni Dispoto", "Paolo Bonetti", "Alessio Russo", "Giuseppe Calcagno", "Francesco Trov\u00f2", "Matteo Papini", "Alberto Maria Metelli", "Marco Mussi", "Marcello Restelli"], "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages", "summary": "In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable\nprogress, achieving superhuman performance in a wide range of simulated\nenvironments. As research moves toward deploying RL in real-world applications,\nthe field faces a new set of challenges inherent to real-world settings, such\nas large state-action spaces, non-stationarity, and partial observability.\nDespite their importance, these challenges are often underexplored in current\nbenchmarks, which tend to focus on idealized, fully observable, and stationary\nenvironments, often neglecting to incorporate real-world complexities\nexplicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive\nsuite of realistic environments designed to support the development and\nevaluation of RL algorithms that can operate in real-world scenarios. The suite\nincludes a diverse set of tasks that expose algorithms to a variety of\npractical challenges. Our experimental results show that, in these settings,\nstandard RL algorithms confirm their competitiveness against rule-based\nbenchmarks, motivating the development of new methods to fully exploit the\npotential of RL to tackle the complexities of real-world tasks.", "AI": {"tldr": "The paper introduces Gym4ReaL, a suite of realistic environments for RL, addressing real-world challenges like large state-action spaces and partial observability, and shows standard RL algorithms perform competitively.", "motivation": "Current RL benchmarks lack real-world complexities like non-stationarity and partial observability, limiting practical deployment.", "method": "Developed Gym4ReaL, a suite of diverse tasks simulating real-world challenges for RL evaluation.", "result": "Standard RL algorithms perform competitively in Gym4ReaL, highlighting their potential for real-world applications.", "conclusion": "Gym4ReaL bridges the gap between idealized benchmarks and real-world RL needs, encouraging further method development."}}
{"id": "2507.00259", "pdf": "https://arxiv.org/pdf/2507.00259", "abs": "https://arxiv.org/abs/2507.00259", "authors": ["Amr Abourayya", "Jens Kleesiek", "Bharat Rao", "Michael Kamp"], "title": "Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Data heterogeneity is a central challenge in federated learning, and\npersonalized federated learning (PFL) aims to address it by tailoring models to\neach client's distribution. Yet many PFL methods fail to outperform local or\ncentralized baselines, suggesting a mismatch between the collaboration they\nenforce and the structure of the data. We propose an approach based on adaptive\ncollaboration, where clients decide adaptively not only how much to rely on\nothers, but also whom to trust at the level of individual examples. We\ninstantiate this principle in FEDMOSAIC, a federated co-training method in\nwhich clients exchange predictions over a shared unlabeled dataset. This\nenables fine-grained trust decisions that are difficult to achieve with\nparameter sharing alone. Each client adjusts its loss weighting based on the\nagreement between private and public data, and contributes to global\npseudo-labels in proportion to its estimated per-example confidence.\nEmpirically, FEDMOSAIC improves upon state-of-the-art PFL methods across\ndiverse non-IID settings, and we provide convergence guarantees under standard\nassumptions. Our results demonstrate the potential of data-aware collaboration\nfor robust and effective personalization.", "AI": {"tldr": "FEDMOSAIC improves personalized federated learning by enabling adaptive collaboration among clients, outperforming state-of-the-art methods in non-IID settings.", "motivation": "Addressing data heterogeneity in federated learning, where many PFL methods fail to surpass local or centralized baselines.", "method": "Proposes FEDMOSAIC, a federated co-training method where clients exchange predictions on shared unlabeled data, making fine-grained trust decisions.", "result": "Empirically outperforms state-of-the-art PFL methods in diverse non-IID settings with convergence guarantees.", "conclusion": "Demonstrates the effectiveness of data-aware collaboration for robust personalization in federated learning."}}
{"id": "2507.00999", "pdf": "https://arxiv.org/pdf/2507.00999", "abs": "https://arxiv.org/abs/2507.00999", "authors": ["Mar\u00eda Grandury", "Javier Aula-Blasco", "J\u00falia Falc\u00e3o", "Cl\u00e9mentine Fourrier", "Miguel Gonz\u00e1lez", "Gonzalo Mart\u00ednez", "Gonzalo Santamar\u00eda", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena G\u00f3mez", "Marta Guerrero", "Guido Ivetta", "Natalia L\u00f3pez", "Flor Miriam Plaza-del-Arco", "Mar\u00eda Teresa Mart\u00edn-Valdivia", "Helena Montoro", "Carmen Mu\u00f1oz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "Mar\u00eda Estrella Vallecillo-Rodr\u00edguez", "Jorge Vallego", "Irune Zubiaga"], "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "La Leaderboard is the first open-source leaderboard for evaluating generative LLMs in Spanish and its varieties, aiming to promote linguistic and cultural diversity.", "motivation": "To encourage the development of LLMs that represent the Spanish-speaking community's diversity and establish an evaluation standard.", "method": "Combines 66 datasets in Basque, Catalan, Galician, and Spanish varieties, evaluating 50 models with a focus on fewer few-shot examples to reduce environmental impact.", "result": "Showcases evaluation results of 50 models and provides a methodology for community-driven leaderboard development.", "conclusion": "La Leaderboard sets a precedent for evaluating LLMs in diverse languages and encourages broader, reproducible research."}}
{"id": "2507.00474", "pdf": "https://arxiv.org/pdf/2507.00474", "abs": "https://arxiv.org/abs/2507.00474", "authors": ["Yaofei Duan", "Yuhao Huang", "Xin Yang", "Luyi Han", "Xinyu Xie", "Zhiyuan Zhu", "Ping He", "Ka-Hou Chan", "Ligang Cui", "Sio-Kei Im", "Dong Ni", "Tao Tan"], "title": "ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis", "categories": ["cs.CV"], "comment": "11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025", "summary": "Deep learning-based diagnostic models often suffer performance drops due to\ndistribution shifts between training (source) and test (target) domains.\nCollecting and labeling sufficient target domain data for model retraining\nrepresents an optimal solution, yet is limited by time and scarce resources.\nActive learning (AL) offers an efficient approach to reduce annotation costs\nwhile maintaining performance, but struggles to handle the challenge posed by\ndistribution variations across different datasets. In this study, we propose a\nnovel unsupervised Active learning framework for Domain Adaptation, named\nADAptation, which efficiently selects informative samples from multi-domain\ndata pools under limited annotation budget. As a fundamental step, our method\nfirst utilizes the distribution homogenization capabilities of diffusion models\nto bridge cross-dataset gaps by translating target images into source-domain\nstyle. We then introduce two key innovations: (a) a hypersphere-constrained\ncontrastive learning network for compact feature clustering, and (b) a\ndual-scoring mechanism that quantifies and balances sample uncertainty and\nrepresentativeness. Extensive experiments on four breast ultrasound datasets\n(three public and one in-house/multi-center) across five common deep\nclassifiers demonstrate that our method surpasses existing strong AL-based\ncompetitors, validating its effectiveness and generalization for clinical\ndomain adaptation. The code is available at the anonymized link:\nhttps://github.com/miccai25-966/ADAptation.", "AI": {"tldr": "Proposes ADAptation, an unsupervised Active Learning framework for domain adaptation, using diffusion models and dual-scoring to handle distribution shifts in medical imaging.", "motivation": "Address performance drops in deep learning models due to distribution shifts between training and test domains, reducing annotation costs.", "method": "Uses diffusion models for domain homogenization, hypersphere-constrained contrastive learning, and dual-scoring for sample selection.", "result": "Outperforms existing AL methods on breast ultrasound datasets, showing effectiveness in clinical domain adaptation.", "conclusion": "ADAptation is a robust solution for domain adaptation with limited annotation budgets, validated across multiple datasets."}}
{"id": "2507.00268", "pdf": "https://arxiv.org/pdf/2507.00268", "abs": "https://arxiv.org/abs/2507.00268", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "27 pages, 10 figures", "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "AI": {"tldr": "A novel DRL framework addresses action execution mismatches by modeling and compensating for uncertainties, enhancing robustness in real-world applications.", "motivation": "Traditional DRL assumes perfect action execution, ignoring real-world uncertainties like system dynamics and hardware constraints, which degrade performance.", "method": "The framework uses a two-stage process: determining desired actions and selecting control signals to correct execution mismatches, training the agent to account for errors.", "result": "The approach improves robustness in real-world conditions, validated in five mechanical simulation environments.", "conclusion": "This method bridges the gap between idealized learning and practical implementation, offering a practical solution for control-oriented applications."}}
{"id": "2507.00265", "pdf": "https://arxiv.org/pdf/2507.00265", "abs": "https://arxiv.org/abs/2507.00265", "authors": ["Alexis Carrillo", "Asieh Abolpour Mofrad", "Anis Yazidi", "Moises Betancort"], "title": "Examining Reject Relations in Stimulus Equivalence Simulations", "categories": ["cs.LG", "q-bio.NC", "I.2.0; J.4; I.6.5"], "comment": "18 pages, 6 figures", "summary": "Simulations offer a valuable tool for exploring stimulus equivalence (SE),\nyet the potential of reject relations to disrupt the assessment of equivalence\nclass formation is contentious. This study investigates the role of reject\nrelations in the acquisition of stimulus equivalence using computational\nmodels. We examined feedforward neural networks (FFNs), bidirectional encoder\nrepresentations from transformers (BERT), and generative pre-trained\ntransformers (GPT) across 18 conditions in matching-to-sample (MTS)\nsimulations. Conditions varied in training structure (linear series,\none-to-many, and many-to-one), relation type (select-only, reject-only, and\nselect-reject), and negative comparison selection (standard and biased). A\nprobabilistic agent served as a benchmark, embodying purely associative\nlearning. The primary goal was to determine whether artificial neural networks\ncould demonstrate equivalence class formation or whether their performance\nreflected associative learning. Results showed that reject relations influenced\nagent performance. While some agents achieved high accuracy on equivalence\ntests, particularly with reject relations and biased negative comparisons, this\nperformance was comparable to the probabilistic agent. These findings suggest\nthat artificial neural networks, including transformer models, may rely on\nassociative strategies rather than SE. This underscores the need for careful\nconsideration of reject relations and more stringent criteria in computational\nmodels of equivalence.", "AI": {"tldr": "The study explores how reject relations affect stimulus equivalence (SE) in computational models, finding that artificial neural networks may rely on associative learning rather than SE.", "motivation": "To investigate the contentious role of reject relations in SE and assess whether computational models truly demonstrate equivalence class formation.", "method": "Used feedforward neural networks (FFNs), BERT, and GPT across 18 conditions in matching-to-sample simulations, varying training structures, relation types, and negative comparison selection. A probabilistic agent served as a benchmark.", "result": "Reject relations influenced performance, with some models achieving high accuracy but resembling associative learning. Transformer models showed similar behavior.", "conclusion": "Artificial neural networks may not truly demonstrate SE, highlighting the need for stricter criteria and careful handling of reject relations in computational models."}}
{"id": "2507.01001", "pdf": "https://arxiv.org/pdf/2507.01001", "abs": "https://arxiv.org/abs/2507.01001", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena is a collaborative platform for evaluating foundation models on scientific literature tasks using community voting, with over 13,000 votes collected. It supports 23 models and includes a meta-evaluation benchmark, SciArena-Eval, to assess automated evaluation methods.", "motivation": "Traditional benchmarks for scientific literature tasks lack community engagement and open-ended evaluation. SciArena aims to address this by involving researchers directly in model comparisons.", "method": "SciArena uses community voting (similar to Chatbot Arena) to evaluate models on literature-grounded, long-form tasks. It supports 23 models and analyzes diversity, real-world alignment, and annotator consistency.", "result": "Over 13,000 votes show diverse, real-world-aligned questions and strong annotator agreement. The leaderboard provides model rankings, and SciArena-Eval highlights challenges in automated evaluation.", "conclusion": "SciArena successfully engages the community for model evaluation and reveals gaps in automated methods, promoting further research in reliable evaluation systems."}}
{"id": "2507.00493", "pdf": "https://arxiv.org/pdf/2507.00493", "abs": "https://arxiv.org/abs/2507.00493", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://www.fenildoshi.com/configural-shape/", "summary": "Humans are able to recognize objects based on both local texture cues and the\nconfiguration of object parts, yet contemporary vision models primarily harvest\nlocal texture cues, yielding brittle, non-compositional features. Work on\nshape-vs-texture bias has pitted shape and texture representations in\nopposition, measuring shape relative to texture, ignoring the possibility that\nmodels (and humans) can simultaneously rely on both types of cues, and\nobscuring the absolute quality of both types of representation. We therefore\nrecast shape evaluation as a matter of absolute configural competence,\noperationalized by the Configural Shape Score (CSS), which (i) measures the\nability to recognize both images in Object-Anagram pairs that preserve local\ntexture while permuting global part arrangement to depict different object\ncategories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)\nuncovers a broad spectrum of configural sensitivity with fully self-supervised\nand language-aligned transformers -- exemplified by DINOv2, SigLIP2 and\nEVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes\nreveal that (iii) high-CSS networks depend on long-range interactions:\nradius-controlled attention masks abolish performance showing a distinctive\nU-shaped integration profile, and representational-similarity analyses expose a\nmid-depth transition from local to global coding. A BagNet control remains at\nchance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that\nconfigural shape score also predicts other shape-dependent evals. Overall, we\npropose that the path toward truly robust, generalizable, and human-like vision\nsystems may not lie in forcing an artificial choice between shape and texture,\nbut rather in architectural and learning frameworks that seamlessly integrate\nboth local-texture and global configural shape.", "AI": {"tldr": "The paper introduces the Configural Shape Score (CSS) to evaluate models' ability to recognize objects based on global part arrangement, showing top performance in self-supervised and language-aligned transformers.", "motivation": "Current vision models focus on local texture cues, neglecting global shape configurations, which limits robustness and human-like recognition.", "method": "CSS measures recognition of Object-Anagram pairs, testing global part arrangement. Evaluated 86 models, including convolutional, transformer, and hybrid architectures.", "result": "High-CSS models (e.g., DINOv2, SigLIP2, EVA-CLIP) rely on long-range interactions, with a U-shaped integration profile and mid-depth transition from local to global coding.", "conclusion": "Robust vision systems should integrate both local-texture and global configural shape, avoiding artificial trade-offs between shape and texture."}}
{"id": "2507.00269", "pdf": "https://arxiv.org/pdf/2507.00269", "abs": "https://arxiv.org/abs/2507.00269", "authors": ["Omar Claflin"], "title": "Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Current sparse autoencoder (SAE) approaches to neural network\ninterpretability assume that activations can be decomposed through linear\nsuperposition into sparse, interpretable features. Despite high reconstruction\nfidelity, SAEs consistently fail to eliminate polysemanticity and exhibit\npathological behavioral errors. We propose that neural networks encode\ninformation in two complementary spaces compressed into the same substrate:\nfeature identity and feature integration. To test this dual encoding\nhypothesis, we develop sequential and joint-training architectures to capture\nidentity and integration patterns simultaneously. Joint training achieves 41.3%\nreconstruction improvement and 51.6% reduction in KL divergence errors. This\narchitecture spontaneously develops bimodal feature organization: low squared\nnorm features contributing to integration pathways and the rest contributing\ndirectly to the residual. Small nonlinear components (3% of parameters) achieve\n16.5% standalone improvements, demonstrating parameter-efficient capture of\ncomputational relationships crucial for behavior. Additionally, intervention\nexperiments using 2x2 factorial stimulus designs demonstrated that integration\nfeatures exhibit selective sensitivity to experimental manipulations and\nproduce systematic behavioral effects on model outputs, including significant\ninteraction effects across semantic dimensions. This work provides systematic\nevidence for (1) dual encoding in neural representations, (2) meaningful\nnonlinearly encoded feature interactions, and (3) introduces an architectural\nparadigm shift from post-hoc feature analysis to integrated computational\ndesign, establishing foundations for next-generation SAEs.", "AI": {"tldr": "The paper proposes a dual encoding hypothesis for neural networks, introducing architectures to capture feature identity and integration simultaneously, achieving significant improvements in reconstruction and error reduction.", "motivation": "Current sparse autoencoder (SAE) approaches fail to eliminate polysemanticity and exhibit behavioral errors, prompting the need for a new encoding paradigm.", "method": "Developed sequential and joint-training architectures to simultaneously capture feature identity and integration, with small nonlinear components for efficiency.", "result": "Joint training improved reconstruction by 41.3% and reduced KL divergence errors by 51.6%. Intervention experiments confirmed selective sensitivity and systematic behavioral effects.", "conclusion": "The work provides evidence for dual encoding and meaningful nonlinear feature interactions, shifting the paradigm from post-hoc analysis to integrated computational design for next-gen SAEs."}}
{"id": "2507.00275", "pdf": "https://arxiv.org/pdf/2507.00275", "abs": "https://arxiv.org/abs/2507.00275", "authors": ["Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "title": "Double Q-learning for Value-based Deep Reinforcement Learning, Revisited", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages", "summary": "Overestimation is pervasive in reinforcement learning (RL), including in\nQ-learning, which forms the algorithmic basis for many value-based deep RL\nalgorithms. Double Q-learning is an algorithm introduced to address\nQ-learning's overestimation by training two Q-functions and using both to\nde-correlate action-selection and action-evaluation in bootstrap targets.\nShortly after Q-learning was adapted to deep RL in the form of deep Q-networks\n(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.\nHowever, Double DQN only loosely adapts Double Q-learning, forgoing the\ntraining of two different Q-functions that bootstrap off one another. In this\npaper, we study algorithms that adapt this core idea of Double Q-learning for\nvalue-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our\naim is to understand whether DDQL exhibits less overestimation than Double DQN\nand whether performant instantiations of DDQL exist. We answer both questions\naffirmatively, demonstrating that DDQL reduces overestimation and outperforms\nDouble DQN in aggregate across 57 Atari 2600 games, without requiring\nadditional hyperparameters. We also study several aspects of DDQL, including\nits network architecture, replay ratio, and minibatch sampling strategy.", "AI": {"tldr": "The paper introduces Deep Double Q-learning (DDQL) to address overestimation in deep RL, showing it outperforms Double DQN in reducing overestimation and improving performance across 57 Atari games.", "motivation": "Overestimation is a common issue in RL, particularly in Q-learning and its deep variants like DQN. While Double Q-learning mitigates this, its deep adaptation (Double DQN) lacks the full benefits of training two Q-functions. This paper explores DDQL to better address overestimation.", "method": "The study adapts Double Q-learning's core idea for deep RL, training two Q-functions that bootstrap off each other. It evaluates DDQL's performance, network architecture, replay ratio, and minibatch sampling.", "result": "DDQL reduces overestimation and outperforms Double DQN across 57 Atari 2600 games without extra hyperparameters.", "conclusion": "DDQL effectively addresses overestimation in deep RL, offering a performant alternative to Double DQN."}}
{"id": "2507.00501", "pdf": "https://arxiv.org/pdf/2507.00501", "abs": "https://arxiv.org/abs/2507.00501", "authors": ["Yongzhen Wang", "Liangliang Chen", "Bingwen Hu", "Heng Liu", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing", "categories": ["cs.CV"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Recent progress in image restoration has underscored Spatial State Models\n(SSMs) as powerful tools for modeling long-range dependencies, owing to their\nappealing linear complexity and computational efficiency. However, SSM-based\napproaches exhibit limitations in reconstructing localized structures and tend\nto be less effective when handling high-dimensional data, frequently resulting\nin suboptimal recovery of fine image features. To tackle these challenges, we\nintroduce Laplace-Mamba, a novel framework that integrates Laplace frequency\nprior with a hybrid Mamba-CNN architecture for efficient image dehazing.\nLeveraging the Laplace decomposition, the image is disentangled into\nlow-frequency components capturing global texture and high-frequency components\nrepresenting edges and fine details. This decomposition enables specialized\nprocessing via dual parallel pathways: the low-frequency branch employs SSMs\nfor global context modeling, while the high-frequency branch utilizes CNNs to\nrefine local structural details, effectively addressing diverse haze scenarios.\nNotably, the Laplace transformation facilitates information-preserving\ndownsampling of low-frequency components in accordance with the Nyquist theory,\nthereby significantly improving computational efficiency. Extensive evaluations\nacross multiple benchmarks demonstrate that our method outperforms\nstate-of-the-art approaches in both restoration quality and efficiency. The\nsource code and pretrained models are available at\nhttps://github.com/yz-wang/Laplace-Mamba.", "AI": {"tldr": "Laplace-Mamba integrates Laplace frequency prior with a hybrid Mamba-CNN architecture for efficient image dehazing, outperforming state-of-the-art methods in quality and efficiency.", "motivation": "SSM-based approaches struggle with localized structures and high-dimensional data, leading to poor recovery of fine image features.", "method": "Uses Laplace decomposition to split the image into low and high-frequency components, processed via dual pathways: SSMs for global context and CNNs for local details.", "result": "Outperforms state-of-the-art methods in restoration quality and efficiency across benchmarks.", "conclusion": "Laplace-Mamba effectively addresses SSM limitations, offering superior performance in image dehazing."}}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286", "abs": "https://arxiv.org/abs/2507.00286", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "AI": {"tldr": "BLV individuals use GenAI for visual content but face privacy challenges. Interviews with 21 participants reveal current practices and design preferences for balancing privacy, efficiency, and emotional agency.", "motivation": "To explore how BLV individuals use GenAI tools for visual content and address the privacy challenges they face.", "method": "Conducted an interview study with 21 BLV participants to investigate practices and design preferences.", "result": "Identified current practices balancing privacy, efficiency, and emotional agency, along with design preferences like on-device processing and sensitive content redaction.", "conclusion": "Provides actionable design recommendations for user-centered visual privacy in GenAI, emphasizing responsible data handling."}}
{"id": "2507.00301", "pdf": "https://arxiv.org/pdf/2507.00301", "abs": "https://arxiv.org/abs/2507.00301", "authors": ["Harsh Sharma", "Juan Diego Draxl Giannoni", "Boris Kramer"], "title": "Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.02273", "summary": "This work presents structure-preserving Lift & Learn, a scientific machine\nlearning method that employs lifting variable transformations to learn\nstructure-preserving reduced-order models for nonlinear partial differential\nequations (PDEs) with conservation laws. We propose a hybrid learning approach\nbased on a recently developed energy-quadratization strategy that uses\nknowledge of the nonlinearity at the PDE level to derive an equivalent\nquadratic lifted system with quadratic system energy. The lifted dynamics\nobtained via energy quadratization are linear in the old variables, making\nmodel learning very effective in the lifted setting. Based on the lifted\nquadratic PDE model form, the proposed method derives quadratic reduced terms\nanalytically and then uses those derived terms to formulate a constrained\noptimization problem to learn the remaining linear reduced operators in a\nstructure-preserving way. The proposed hybrid learning approach yields\ncomputationally efficient quadratic reduced-order models that respect the\nunderlying physics of the high-dimensional problem. We demonstrate the\ngeneralizability of quadratic models learned via the proposed\nstructure-preserving Lift & Learn method through three numerical examples: the\none-dimensional wave equation with exponential nonlinearity, the\ntwo-dimensional sine-Gordon equation, and the two-dimensional\nKlein-Gordon-Zakharov equations. The numerical results show that the proposed\nlearning approach is competitive with the state-of-the-art structure-preserving\ndata-driven model reduction method in terms of both accuracy and computational\nefficiency.", "AI": {"tldr": "The paper introduces 'structure-preserving Lift & Learn,' a method for learning reduced-order models of nonlinear PDEs with conservation laws, combining energy-quadratization and hybrid learning to preserve structure and improve efficiency.", "motivation": "To develop a method for learning reduced-order models of nonlinear PDEs that preserves their structure and underlying physics, addressing challenges in computational efficiency and accuracy.", "method": "Uses energy-quadratization to transform PDEs into quadratic lifted systems, derives quadratic reduced terms analytically, and formulates a constrained optimization problem to learn linear reduced operators.", "result": "Demonstrates the method's effectiveness through numerical examples, showing competitive accuracy and efficiency compared to state-of-the-art approaches.", "conclusion": "The proposed Lift & Learn method successfully preserves structure and physics in reduced-order models, offering a computationally efficient and accurate solution for nonlinear PDEs."}}
{"id": "2507.00502", "pdf": "https://arxiv.org/pdf/2507.00502", "abs": "https://arxiv.org/abs/2507.00502", "authors": ["JianChao Zhao", "Songlin Dong"], "title": "ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to enable models to adapt\non-the-fly to a stream of unlabeled data under evolving distribution shifts.\nHowever, existing CTTA methods typically rely on shared model parameters across\nall domains, making them vulnerable to feature entanglement and catastrophic\nforgetting in the presence of large or non-stationary domain shifts. To address\nthis limitation, we propose \\textbf{ExPaMoE}, a novel framework based on an\n\\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples\ndomain-general and domain-specific knowledge via a dual-branch expert design\nwith token-guided feature separation, and dynamically expands its expert pool\nbased on a \\emph{Spectral-Aware Online Domain Discriminator} (SODD) that\ndetects distribution changes in real-time using frequency-domain cues.\nExtensive experiments demonstrate the superiority of ExPaMoE across diverse\nCTTA scenarios. We evaluate our method on standard benchmarks including\nCIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic\nsegmentation. Additionally, we introduce \\textbf{ImageNet++}, a large-scale and\nrealistic CTTA benchmark built from multiple ImageNet-derived datasets, to\nbetter reflect long-term adaptation under complex domain evolution. ExPaMoE\nconsistently outperforms prior arts, showing strong robustness, scalability,\nand resistance to forgetting.", "AI": {"tldr": "ExPaMoE introduces an expandable parallel Mixture-of-Experts framework to tackle feature entanglement and catastrophic forgetting in Continual Test-Time Adaptation (CTTA), outperforming existing methods on multiple benchmarks.", "motivation": "Existing CTTA methods struggle with feature entanglement and catastrophic forgetting due to shared model parameters across domains, especially under large or non-stationary shifts.", "method": "ExPaMoE uses a dual-branch expert design for domain-general and domain-specific knowledge separation, dynamically expanding experts via a Spectral-Aware Online Domain Discriminator (SODD).", "result": "ExPaMoE outperforms prior methods on benchmarks like CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC, and introduces ImageNet++ for realistic CTTA evaluation.", "conclusion": "ExPaMoE demonstrates robustness, scalability, and resistance to forgetting, making it a superior solution for CTTA challenges."}}
{"id": "2507.00288", "pdf": "https://arxiv.org/pdf/2507.00288", "abs": "https://arxiv.org/abs/2507.00288", "authors": ["Claire Li", "David Freeborn"], "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "categories": ["econ.TH", "cs.AI", "cs.ET"], "comment": "22 pages", "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain.", "AI": {"tldr": "AI-powered digital innovations are disrupting traditional accountability mechanisms in transnational governance. The study integrates TAM, ANT, and institutional theory to analyze AI adoption, emphasizing compliance, legitimacy, and relational accountability.", "motivation": "To understand how AI reshapes accountability in transnational governance, addressing regulatory, ethical, and cultural pressures.", "method": "Integrates Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and institutional theory to examine AI adoption.", "result": "Accountability is co-constructed in socio-technical networks, influenced by governance logics and normative expectations. Two strategies are proposed: internal governance reconfiguration and external actor-network engagement.", "conclusion": "The study highlights the need for relational accountability and strategic approaches to ensure responsible and globally accepted AI adoption in accounting."}}
{"id": "2507.00304", "pdf": "https://arxiv.org/pdf/2507.00304", "abs": "https://arxiv.org/abs/2507.00304", "authors": ["Yujun Zhang", "Runlong Li", "Xiaoxiang Liang", "Xinhao Yang", "Tian Su", "Bo Liu", "Yan Zhou"], "title": "MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic", "categories": ["cs.LG", "cs.NI"], "comment": "16 pages", "summary": "The abnormal fluctuations in network traffic may indicate potential security\nthreats or system failures. Therefore, efficient network traffic prediction and\nanomaly detection methods are crucial for network security and traffic\nmanagement. This paper proposes a novel network traffic prediction and anomaly\ndetection model, MamNet, which integrates time-domain modeling and\nfrequency-domain feature extraction. The model first captures the long-term\ndependencies of network traffic through the Mamba module (time-domain\nmodeling), and then identifies periodic fluctuations in the traffic using\nFourier Transform (frequency-domain feature extraction). In the feature fusion\nlayer, multi-scale information is integrated to enhance the model's ability to\ndetect network traffic anomalies. Experiments conducted on the UNSW-NB15 and\nCAIDA datasets demonstrate that MamNet outperforms several recent mainstream\nmodels in terms of accuracy, recall, and F1-Score. Specifically, it achieves an\nimprovement of approximately 2% to 4% in detection performance for complex\ntraffic patterns and long-term trend detection. The results indicate that\nMamNet effectively captures anomalies in network traffic across different time\nscales and is suitable for anomaly detection tasks in network security and\ntraffic management. Future work could further optimize the model structure by\nincorporating external network event information, thereby improving the model's\nadaptability and stability in complex network environments.", "AI": {"tldr": "MamNet, a novel model combining time-domain and frequency-domain analysis, improves network traffic anomaly detection by 2-4% over existing methods.", "motivation": "Abnormal network traffic fluctuations can signal security threats or failures, necessitating robust prediction and detection methods.", "method": "MamNet uses Mamba for time-domain modeling and Fourier Transform for frequency-domain features, integrating multi-scale information for anomaly detection.", "result": "Outperforms recent models on UNSW-NB15 and CAIDA datasets, with 2-4% better accuracy, recall, and F1-Score.", "conclusion": "MamNet effectively detects anomalies across time scales; future work includes optimizing with external event data for better adaptability."}}
{"id": "2507.00310", "pdf": "https://arxiv.org/pdf/2507.00310", "abs": "https://arxiv.org/abs/2507.00310", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "title": "Open-ended Scientific Discovery via Bayesian Surprise", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "AI": {"tldr": "AutoDS introduces a method for open-ended autonomous scientific discovery (ASD) using Bayesian surprise and Monte Carlo tree search, outperforming competitors by generating more surprising discoveries.", "motivation": "Current ASD methods rely on human-specified questions or flawed heuristics, limiting exploration. AutoDS aims to accelerate discovery by letting AI drive exploration using Bayesian surprise.", "method": "AutoDS quantifies epistemic shifts (Bayesian surprise) and uses Monte Carlo tree search with surprisal as a reward to explore nested hypotheses.", "result": "AutoDS outperforms competitors, producing 5-29% more surprising discoveries across 21 datasets. Two-thirds of its findings surprised domain experts.", "conclusion": "AutoDS advances open-ended ASD by effectively navigating hypothesis spaces and generating meaningful, surprising discoveries."}}
{"id": "2507.00505", "pdf": "https://arxiv.org/pdf/2507.00505", "abs": "https://arxiv.org/abs/2507.00505", "authors": ["Haoran Lou", "Chunxiao Fan", "Ziyan Liu", "Yuexin Wu", "Xinxiang Wang"], "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs", "categories": ["cs.CV"], "comment": "ICCV", "summary": "The architecture of multimodal large language models (MLLMs) commonly\nconnects a vision encoder, often based on CLIP-ViT, to a large language model.\nWhile CLIP-ViT works well for capturing global image features, it struggles to\nmodel local relationships between adjacent patches, leading to weaker visual\nrepresentation, which in turn affects the detailed understanding ability of\nMLLMs. To solve this, we propose LLaVA-SP, which \\textbf{ only adds six spatial\nvisual tokens} to the original visual tokens to enhance the visual\nrepresentation. Our approach offers three key advantages: 1)We propose a novel\nProjector, which uses convolutional kernels to derive visual spatial tokens\nfrom ViT patch features, simulating two visual spatial ordering approaches:\n``from central region to global\" and ``from abstract to specific\". Then, a\ncross-attention mechanism is applied to fuse fine-grained visual information,\nenriching the overall visual representation. 2) We present two model variants:\nLLaVA-SP-Cropping, which focuses on detail features through progressive\ncropping, and LLaVA-SP-Pooling, which captures global semantics through\nadaptive pooling, enabling the model to handle diverse visual understanding\ntasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,\nachieves significant performance improvements across various multimodal\nbenchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple\ntasks with nearly identical inference latency. The code and models are\navailable at\n\\href{https://github.com/CnFaker/LLaVA-SP}{\\texttt{https://github.com/CnFaker/LLaVA-SP}}.", "AI": {"tldr": "LLaVA-SP enhances multimodal large language models by adding six spatial visual tokens to improve local feature representation, outperforming LLaVA-1.5 with minimal latency increase.", "motivation": "CLIP-ViT struggles with local patch relationships, weakening visual representation in MLLMs. LLaVA-SP addresses this by enriching visual features with spatial tokens.", "method": "Proposes a novel Projector using convolutional kernels to derive spatial tokens, simulating two ordering approaches. Introduces two variants (Cropping and Pooling) for diverse visual tasks.", "result": "LLaVA-SP outperforms LLaVA-1.5 in multimodal benchmarks with nearly identical inference latency.", "conclusion": "LLaVA-SP effectively enhances visual representation in MLLMs, offering improved performance without significant computational overhead."}}
{"id": "2507.00347", "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "AI": {"tldr": "VTS-AI integrates Visual Thinking Strategies into AI to extract insights from unstructured business reports, offering richer outputs than ChatGPT and maintaining human oversight.", "motivation": "Modern firms struggle with dense, unstructured reports, needing agile solutions for quick insights.", "method": "VTS-AI uses a three-tier system (micro, meso, macro) to tag issues, link them to source pages, and store actionable insights in a searchable YAML file.", "result": "In tests, VTS-AI matched ChatGPT's speed but provided richer outputs like page locations, verbatim excerpts, severity scores, and causal links.", "conclusion": "VTS-AI shows promise for rapid business analysis, with planned upgrades to enhance financial mapping, language models, and data security."}}
{"id": "2507.00320", "pdf": "https://arxiv.org/pdf/2507.00320", "abs": "https://arxiv.org/abs/2507.00320", "authors": ["Christiana Westlin", "Ashutosh Singh", "Deniz Erdogmus", "Georgios Stratis", "Lisa Feldman Barrett"], "title": "Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "comment": null, "summary": "In the science of emotion, it is widely assumed that folk emotion categories\nform a biological and psychological typology, and studies are routinely\ndesigned and analyzed to identify emotion-specific patterns. This approach\nshapes the observations that studies report, ultimately reinforcing the\nassumption that guided the investigation. Here, we reanalyzed data from one\nsuch typologically-guided study that reported mappings between individual brain\npatterns and group-averaged ratings of 34 emotion categories. Our reanalysis\nwas guided by an alternative view of emotion categories as populations of\nvariable, situated instances, and which predicts a priori that there will be\nsignificant variation in brain patterns within a category across instances.\nCorrespondingly, our analysis made minimal assumptions about the structure of\nthe variance present in the data. As predicted, we did not observe the original\nmappings and instead observed significant variation across individuals. These\nfindings demonstrate how starting assumptions can ultimately impact scientific\nconclusions and suggest that a hypothesis must be supported using multiple\nanalytic methods before it is taken seriously.", "AI": {"tldr": "The paper challenges the assumption that emotion categories are biologically and psychologically distinct by reanalyzing data from a study that mapped brain patterns to emotion categories. The reanalysis, guided by a view of emotions as variable instances, found significant individual variation, undermining the original findings.", "motivation": "To question the widely held assumption that emotion categories are biologically and psychologically distinct, and to demonstrate how starting assumptions can influence scientific conclusions.", "method": "Reanalyzed data from a typologically-guided study using a minimal-assumption approach, treating emotion categories as populations of variable instances.", "result": "The reanalysis did not replicate the original mappings and instead revealed significant variation in brain patterns across individuals within emotion categories.", "conclusion": "Scientific conclusions are shaped by starting assumptions, and hypotheses should be tested with multiple methods before being accepted."}}
{"id": "2507.00425", "pdf": "https://arxiv.org/pdf/2507.00425", "abs": "https://arxiv.org/abs/2507.00425", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "AI": {"tldr": "The paper introduces TarFlowLM, a novel framework for language modeling in continuous latent spaces using autoregressive normalizing flows, offering flexibility in context capture, generation, and hierarchical processes.", "motivation": "To explore alternatives to traditional discrete token-based autoregressive models by leveraging continuous latent spaces for greater modeling flexibility.", "method": "Proposes TarFlowLM, which uses transformer-based autoregressive normalizing flows to model continuous representations, enabling bi-directional context, block-wise generation, and hierarchical multi-pass processes.", "result": "Demonstrates strong performance on language modeling benchmarks and highlights the framework's flexible capabilities.", "conclusion": "TarFlowLM provides a promising alternative to discrete autoregressive models, offering enhanced flexibility and performance in language modeling."}}
{"id": "2507.00506", "pdf": "https://arxiv.org/pdf/2507.00506", "abs": "https://arxiv.org/abs/2507.00506", "authors": ["Yunfei Xie", "Yuxuan Cheng", "Juncheng Wu", "Haoyu Zhang", "Yuyin Zhou", "Shoudong Han"], "title": "SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in adapting vision-language pre-training models like CLIP\nfor person re-identification (ReID) tasks often rely on complex adapter design\nor modality-specific tuning while neglecting cross-modal interaction, leading\nto high computational costs or suboptimal alignment. To address these\nlimitations, we propose a simple yet effective framework named Selective\nCross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and\nrobustness against real-world perturbations. Our method introduces two key\ninnovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a\nlightweight module that dynamically injects discriminative visual features into\ntext prompts via a cross-modal gating mechanism. Moreover, the proposed\nPerturbation-Driven Consistency Alignment (PDCA) is a dual-path training\nstrategy that enforces invariant feature alignment under random image\nperturbations by regularizing consistency between original and augmented\ncross-modal embeddings. Extensive experiments are conducted on several popular\nbenchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,\nand P-DukeMTMC, which demonstrate the impressive performance of the proposed\nmethod. Notably, our framework eliminates heavy adapters while maintaining\nefficient inference, achieving an optimal trade-off between performance and\ncomputational overhead. The code will be released upon acceptance.", "AI": {"tldr": "A new framework, SCING, improves cross-modal alignment in person ReID by introducing lightweight modules and a dual-path training strategy, achieving efficient performance without heavy adapters.", "motivation": "Existing methods for adapting vision-language models in ReID tasks are complex or ignore cross-modal interaction, leading to inefficiency or poor alignment.", "method": "SCING uses Selective Visual Prompt Fusion (SVIP) for dynamic feature injection and Perturbation-Driven Consistency Alignment (PDCA) for robust feature alignment under perturbations.", "result": "The method outperforms on benchmarks like Market1501 and DukeMTMC-ReID, balancing performance and computational efficiency.", "conclusion": "SCING offers a simple, effective solution for cross-modal alignment in ReID, eliminating heavy adapters while maintaining high performance."}}
{"id": "2507.00352", "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "AI": {"tldr": "A novel methodology combining AST embedding and RAG improves SVRF code synthesis, achieving 40% better accuracy in semiconductor design rule applications.", "motivation": "Addressing the inefficiency and expertise gap in traditional SVRF development due to complex design rules in advancing semiconductor nodes.", "method": "Integrates AST for structural validation and RAG for domain-specific knowledge, evaluated with T5-based models and a custom SVRF scoring framework.", "result": "40% improvement in code generation accuracy on 740 DRC rule benchmarks compared to basic fine-tuning.", "conclusion": "The approach enhances SVRF development, reduces errors, and boosts productivity in semiconductor design cycles."}}
{"id": "2507.00358", "pdf": "https://arxiv.org/pdf/2507.00358", "abs": "https://arxiv.org/abs/2507.00358", "authors": ["Yilie Huang", "Xun Yu Zhou"], "title": "Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": "36 pages, 10 figures", "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "AI": {"tldr": "The paper introduces an adaptive exploration mechanism for RL in continuous-time stochastic LQ control, improving efficiency and matching sublinear regret bounds.", "motivation": "To address the inefficiency and tuning challenges of fixed exploration schedules in RL for LQ control problems.", "method": "Proposes a model-free, data-driven approach with adaptive entropy regularization and policy variance adjustments.", "result": "Achieves sublinear regret matching best-known bounds, with numerical experiments showing faster convergence and better performance.", "conclusion": "Adaptive exploration enhances learning efficiency and regret performance in RL for LQ control."}}
{"id": "2507.00449", "pdf": "https://arxiv.org/pdf/2507.00449", "abs": "https://arxiv.org/abs/2507.00449", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "AI": {"tldr": "The paper addresses the challenge of long-context modeling in NLP by improving state-space models (SSMs) with a novel synthetic task (joint recall) and proposing a solution integrating SSMs with Context-Dependent Sparse Attention (CDSA).", "motivation": "Efficient long-context modeling is critical for NLP, but current methods like Transformers and SSMs face scalability or effectiveness issues.", "method": "The authors introduce joint recall as a more complex synthetic task, prove SSMs' limitations, and propose HAX, a solution combining SSMs with CDSA.", "result": "HAX outperforms SSM baselines and SSMs with context-independent sparse attention (CISA) on synthetic and real-world benchmarks.", "conclusion": "The proposed HAX method effectively bridges the gap between theoretical analysis and practical long-context modeling, offering a scalable and efficient solution."}}
{"id": "2507.00519", "pdf": "https://arxiv.org/pdf/2507.00519", "abs": "https://arxiv.org/abs/2507.00519", "authors": ["Ruize Cui", "Jiaan Zhang", "Jialun Pei", "Kai Wang", "Pheng-Ann Heng", "Jing Qin"], "title": "Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection", "categories": ["cs.CV"], "comment": "This paper has been accepted by MICCAI 2025", "summary": "Liver landmarks provide crucial anatomical guidance to the surgeon during\nlaparoscopic liver surgery to minimize surgical risk. However, the tubular\nstructural properties of landmarks and dynamic intraoperative deformations pose\nsignificant challenges for automatic landmark detection. In this study, we\nintroduce TopoNet, a novel topology-constrained learning framework for\nlaparoscopic liver landmark detection. Our framework adopts a snake-CNN\ndual-path encoder to simultaneously capture detailed RGB texture information\nand depth-informed topological structures. Meanwhile, we propose a\nboundary-aware topology fusion (BTF) module, which adaptively merges RGB-D\nfeatures to enhance edge perception while preserving global topology.\nAdditionally, a topological constraint loss function is embedded, which\ncontains a center-line constraint loss and a topological persistence loss to\nensure homotopy equivalence between predictions and labels. Extensive\nexperiments on L3D and P2ILF datasets demonstrate that TopoNet achieves\noutstanding accuracy and computational complexity, highlighting the potential\nfor clinical applications in laparoscopic liver surgery. Our code will be\navailable at https://github.com/cuiruize/TopoNet.", "AI": {"tldr": "TopoNet is a topology-constrained learning framework for laparoscopic liver landmark detection, combining RGB texture and depth-informed topological structures with adaptive feature fusion and topological constraints for high accuracy.", "motivation": "Liver landmarks are vital for surgical guidance but challenging to detect due to tubular structures and intraoperative deformations.", "method": "Uses a snake-CNN dual-path encoder for RGB and depth data, a boundary-aware topology fusion module, and topological constraint losses (center-line and persistence).", "result": "Achieves high accuracy and computational efficiency on L3D and P2ILF datasets.", "conclusion": "TopoNet shows strong potential for clinical use in laparoscopic liver surgery."}}
{"id": "2507.00378", "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "AI": {"tldr": "iPanda is an end-to-end framework using LLMs to automate protocol conformance testing, improving efficiency and accuracy over traditional methods.", "motivation": "Traditional protocol conformance testing is labor-intensive and inefficient, while LLMs offer automation potential.", "method": "iPanda automates test case generation, interprets implementations, and refines test scripts iteratively.", "result": "iPanda outperforms pure LLM-based methods, improving test-code generation success rates significantly.", "conclusion": "iPanda effectively automates and enhances protocol conformance testing using LLMs."}}
{"id": "2507.00390", "pdf": "https://arxiv.org/pdf/2507.00390", "abs": "https://arxiv.org/abs/2507.00390", "authors": ["Geng Zhang", "Yuxuan Han", "Yuxuan Lou", "Wangbo Zhao", "Yiqi Zhang", "Yang You"], "title": "MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE", "categories": ["cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\nby activating only a subset of experts per input token. However, deploying\nMoE-based models incurs significant memory overhead due to the need to retain\nall experts in memory. While structured pruning is promising to reduce memory\ncosts, existing methods often show suboptimal performance and unstable\ndegradation in three dimensions: model architectures, calibration data sources,\nand calibration sample sizes. This paper proposes\nMixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that\nreplaces redundant experts with lightweight novices to achieve effective and\nrobust model compression. MoNE evaluates expert redundancy based on two\nmetrics: access frequency and output variance. Experts exhibiting low usage and\nstable outputs are pruned and replaced with lightweight novices-unbiased\nestimations of their original outputs-minimizing performance degradation.\nExtensive experiments demonstrate that MoNE consistently outperforms baseline\nmethods with minimal accuracy degradation across the three dimensions,\nconfirming its effectiveness and robustness. Notably, it improves the average\nzero shot accuracy across nine downstream tasks by up to 2.71 under 25\\%\npruning ratio and 3.61 under 50\\% pruning. The code is available at\nhttps://github.com/zxgx/mode-pd.", "AI": {"tldr": "MoNE is a novel expert pruning method for Mixture-of-Experts models, replacing redundant experts with lightweight novices to reduce memory overhead while maintaining performance.", "motivation": "Deploying MoE-based models incurs high memory costs, and existing pruning methods show suboptimal performance and instability.", "method": "MoNE evaluates expert redundancy using access frequency and output variance, pruning low-usage experts and replacing them with lightweight novices.", "result": "MoNE outperforms baselines with minimal accuracy loss, improving zero-shot accuracy by up to 3.61 under 50% pruning.", "conclusion": "MoNE is an effective and robust method for compressing MoE models, balancing memory efficiency and performance."}}
{"id": "2507.00487", "pdf": "https://arxiv.org/pdf/2507.00487", "abs": "https://arxiv.org/abs/2507.00487", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "AI": {"tldr": "MassTool is a multi-task framework for improving tool retrieval in LLMs by enhancing query representation and retrieval accuracy using a two-tower architecture and search-based intent modeling.", "motivation": "Existing tool retrieval methods focus on tool representations but overlook precise query comprehension, leading to suboptimal performance.", "method": "MassTool uses a two-tower architecture (tool usage detection and retrieval towers), QC-GCN for query-tool matching, SUIM for intent modeling, and AdaKT for multi-task learning.", "result": "Extensive experiments show MassTool improves retrieval accuracy significantly.", "conclusion": "MassTool addresses the gap in query comprehension for tool retrieval, offering a robust solution for LLM-tool interaction."}}
{"id": "2507.00525", "pdf": "https://arxiv.org/pdf/2507.00525", "abs": "https://arxiv.org/abs/2507.00525", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "AI": {"tldr": "Box-QAymo introduces a dataset for evaluating VLMs on spatial and temporal reasoning in autonomous driving, addressing gaps in localized user-driven queries.", "motivation": "Current VLMs lack interpretability for real-world driving scenarios, especially for localized user intent.", "method": "A box-referring dataset with hierarchical evaluation (binary checks, attribute prediction, motion understanding, spatiotemporal reasoning) and crowd-sourced annotations.", "result": "Reveals limitations in VLMs for perception tasks, emphasizing the gap in real-world performance.", "conclusion": "Box-QAymo lays groundwork for more robust, interpretable autonomous driving communication."}}
{"id": "2507.00407", "pdf": "https://arxiv.org/pdf/2507.00407", "abs": "https://arxiv.org/abs/2507.00407", "authors": ["Cong Fu", "Yuchao Lin", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials", "categories": ["physics.chem-ph", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Accurate molecular property predictions require 3D geometries, which are\ntypically obtained using expensive methods such as density functional theory\n(DFT). Here, we attempt to obtain molecular geometries by relying solely on\nmachine learning interatomic potential (MLIP) models. To this end, we first\ncurate a large-scale molecular relaxation dataset comprising 3.5 million\nmolecules and 300 million snapshots. Then MLIP foundation models are trained\nwith supervised learning to predict energy and forces given 3D molecular\nstructures. Once trained, we show that the foundation models can be used in\ndifferent ways to obtain geometries either explicitly or implicitly. First, it\ncan be used to obtain low-energy 3D geometries via geometry optimization,\nproviding relaxed 3D geometries for downstream molecular property predictions.\nTo mitigate potential biases and enhance downstream predictions, we introduce\ngeometry fine-tuning based on the relaxed 3D geometries. Second, the foundation\nmodels can be directly fine-tuned for property prediction when ground truth 3D\ngeometries are available. Our results demonstrate that MLIP foundation models\ntrained on relaxation data can provide valuable molecular geometries that\nbenefit property predictions.", "AI": {"tldr": "The paper explores using machine learning interatomic potential (MLIP) models to predict molecular geometries, avoiding expensive DFT methods. It trains foundation models on a large relaxation dataset and demonstrates their utility for geometry optimization and property prediction.", "motivation": "Traditional methods like DFT for obtaining 3D molecular geometries are costly. The study aims to leverage MLIP models to provide accurate geometries efficiently.", "method": "A large-scale dataset of 3.5M molecules and 300M snapshots is curated. MLIP foundation models are trained with supervised learning to predict energy and forces. These models are then used for geometry optimization and fine-tuning for property prediction.", "result": "The MLIP models successfully provide low-energy 3D geometries via optimization and enhance downstream property predictions through fine-tuning.", "conclusion": "MLIP foundation models trained on relaxation data offer a viable alternative to DFT for obtaining molecular geometries, benefiting property predictions."}}
{"id": "2507.00394", "pdf": "https://arxiv.org/pdf/2507.00394", "abs": "https://arxiv.org/abs/2507.00394", "authors": ["Geng Zhang", "Shenggan Cheng", "Xuanlei Zhao", "Ziming Liu", "Yang You"], "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev.", "AI": {"tldr": "HelixPipe is a pipeline parallelism method for long-sequence transformer training, addressing performance issues with quadratic attention and memory overhead. It introduces attention parallel partition and a two-fold micro batch schedule, achieving better throughput and scalability.", "motivation": "Existing pipeline parallelisms perform poorly with growing transformer sequence lengths due to quadratic attention computation and high memory overhead.", "method": "HelixPipe uses attention parallel partition, a two-fold micro batch schedule, recomputation without attention, and chunked MLP to optimize performance.", "result": "HelixPipe outperforms baselines, achieving a 26% speedup for a 7B model with 128k sequence length on 64 GPUs.", "conclusion": "HelixPipe is effective for long-sequence transformer training, offering superior throughput and scalability."}}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740", "abs": "https://arxiv.org/abs/2507.00740", "authors": ["Craig S Wright"], "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "AI": {"tldr": "The paper provides a formal specification and proof for Simplified Payment Verification (SPV), clarifying its security and optimality for scalable digital cash systems, debunking common misconceptions.", "motivation": "To correct misrepresentations of SPV in popular implementations and demonstrate its security and efficiency under adversarial conditions.", "method": "Reconstructs SPV from first principles using symbolic automata, Merkle membership relations, and probabilistic/game-theoretic analysis.", "result": "Proves SPV's security and optimality, with low-bandwidth optimizations like adaptive polling and compressed header sync.", "conclusion": "The paper serves as a secure SPV implementation guide and rebuts misconceptions about non-validating clients."}}
{"id": "2507.00537", "pdf": "https://arxiv.org/pdf/2507.00537", "abs": "https://arxiv.org/abs/2507.00537", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "21 pages, 7 figures", "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "AI": {"tldr": "The paper introduces Attention Ablation Technique (AAT) to improve CLIP's performance by identifying and suppressing detrimental attention heads in its image encoder.", "motivation": "CLIP's robust performance is sometimes hindered by certain attention heads, and ablating them could enhance downstream task performance.", "method": "Proposes AAT, a method to manipulate attention weights and suppress harmful heads, using two tailored strategies for different scenarios.", "result": "AAT improves downstream tasks, achieving up to 11.1% higher recall in cross-modal retrieval with no added inference cost.", "conclusion": "AAT effectively refines vision-language models like CLIP by optimizing attention heads, demonstrating practical benefits."}}
{"id": "2507.00418", "pdf": "https://arxiv.org/pdf/2507.00418", "abs": "https://arxiv.org/abs/2507.00418", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank W\u00fcrthwein"], "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "categories": ["cs.DC", "cs.AI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "AI": {"tldr": "Benchmarking of Qualcomm Cloud AI 100 Ultra (QAic) for LLM inference shows competitive energy efficiency against NVIDIA and AMD GPUs.", "motivation": "Evaluate the QAic accelerator's energy efficiency and performance for LLM inference in HPC applications.", "method": "Tested 15 LLMs (117M to 90B parameters) using vLLM framework, comparing QAic with NVIDIA A100, H200, and AMD MI300A GPUs.", "result": "QAic is energy-efficient and performs well in throughput per watt, often outperforming competitors.", "conclusion": "QAic shows promise for HPC applications within the NRP ecosystem due to its energy efficiency."}}
{"id": "2507.00411", "pdf": "https://arxiv.org/pdf/2507.00411", "abs": "https://arxiv.org/abs/2507.00411", "authors": ["Jinfu Fan", "Xiaohui Zhong", "Kangrui Ren", "Jiangnan Li", "Linqing Huang"], "title": "Diffusion Disambiguation Models for Partial Label Learning", "categories": ["cs.LG"], "comment": null, "summary": "Learning from ambiguous labels is a long-standing problem in practical\nmachine learning applications. The purpose of \\emph{partial label learning}\n(PLL) is to identify the ground-truth label from a set of candidate labels\nassociated with a given instance. Inspired by the remarkable performance of\ndiffusion models in various generation tasks, this paper explores their\npotential to denoise ambiguous labels through the reverse denoising process.\nTherefore, this paper reformulates the label disambiguation problem from the\nperspective of generative models, where labels are generated by iteratively\nrefining initial random guesses. This perspective enables the diffusion model\nto learn how label information is generated stochastically. By modeling the\ngeneration uncertainty, we can use the maximum likelihood estimate of the label\nfor classification inference. However, such ambiguous labels lead to a mismatch\nbetween instance and label, which reduces the quality of generated data. To\naddress this issue, this paper proposes a \\emph{diffusion disambiguation model\nfor PLL} (DDMP), which first uses the potential complementary information\nbetween instances and labels to construct pseudo-clean labels for initial\ndiffusion training. Furthermore, a transition-aware matrix is introduced to\nestimate the potential ground-truth labels, which are dynamically updated\nduring the diffusion generation. During training, the ground-truth label is\nprogressively refined, improving the classifier. Experiments show the advantage\nof the DDMP and its suitability for PLL.", "AI": {"tldr": "The paper proposes DDMP, a diffusion disambiguation model for partial label learning (PLL), leveraging diffusion models to denoise ambiguous labels and improve classification accuracy.", "motivation": "Ambiguous labels in PLL reduce data quality and hinder accurate classification. The paper aims to address this by using diffusion models to denoise and refine labels.", "method": "The DDMP model constructs pseudo-clean labels for initial diffusion training and uses a transition-aware matrix to estimate ground-truth labels, dynamically updating them during diffusion generation.", "result": "Experiments demonstrate DDMP's effectiveness in improving label disambiguation and classification performance in PLL tasks.", "conclusion": "DDMP successfully leverages diffusion models for PLL, enhancing label disambiguation and classifier accuracy."}}
{"id": "2507.00877", "pdf": "https://arxiv.org/pdf/2507.00877", "abs": "https://arxiv.org/abs/2507.00877", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "AI": {"tldr": "The paper introduces VLTL-Bench, a benchmark for evaluating NL-to-TL translation systems, focusing on verifiability and grounding, addressing gaps in current benchmarks.", "motivation": "Current NL-to-TL benchmarks ignore grounding and verifiability, leading to inflated performance metrics. A unifying benchmark is needed for domain-general systems.", "method": "VLTL-Bench includes diverse NL specifications, formal TL, and sample traces, supporting end-to-end evaluation and substep analysis (lifting, grounding, translation, verification).", "result": "The benchmark provides ground truths for each substep, enabling detailed evaluation and improvement of NL-to-TL translation frameworks.", "conclusion": "VLTL-Bench addresses critical gaps in existing benchmarks, promoting methodologically sound advances in verifiable NL-to-TL translation."}}
{"id": "2507.00554", "pdf": "https://arxiv.org/pdf/2507.00554", "abs": "https://arxiv.org/abs/2507.00554", "authors": ["Zhenya Yang", "Bingchen Gong", "Kai Chen", "Qi Dou"], "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing", "categories": ["cs.CV"], "comment": null, "summary": "Despite the advancements in quality and efficiency achieved by 3D Gaussian\nSplatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent\nchallenge. Existing approaches primarily rely on low-pass filtering to mitigate\naliasing. However, these methods are not sensitive to the sampling rate, often\nresulting in under-filtering and over-smoothing renderings. To address this\nlimitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework\nfor Gaussian Splatting, which dynamically predicts the optimal filtering\nstrength for each 3D Gaussian primitive. Specifically, we introduce a set of\nbasis functions to each Gaussian, which take the sampling rate as input to\nmodel appearance variations, enabling sampling-rate-sensitive filtering. These\nbasis function parameters are jointly optimized with the 3D Gaussian in an\nend-to-end manner. The sampling rate is influenced by both focal length and\ncamera distance. However, existing methods and datasets rely solely on\ndown-sampling to simulate focal length changes for anti-aliasing evaluation,\noverlooking the impact of camera distance. To enable a more comprehensive\nassessment, we introduce a new synthetic dataset featuring objects rendered at\nvarying camera distances. Extensive experiments on both public datasets and our\nnewly collected dataset demonstrate that our method achieves SOTA rendering\nquality while effectively eliminating aliasing. The code and dataset have been\nopen-sourced.", "AI": {"tldr": "LOD-GS is a Level-of-Detail-sensitive filtering framework for 3D Gaussian Splatting that dynamically predicts optimal filtering strength, addressing aliasing artifacts while avoiding under-filtering and over-smoothing.", "motivation": "Aliasing artifacts in 3D Gaussian Splatting persist despite existing low-pass filtering methods, which are insensitive to sampling rates.", "method": "LOD-GS introduces basis functions to model appearance variations based on sampling rate, optimizing parameters end-to-end with 3D Gaussians.", "result": "The method achieves state-of-the-art rendering quality and effectively eliminates aliasing, validated on public and new synthetic datasets.", "conclusion": "LOD-GS outperforms existing methods by dynamically adapting filtering strength, and the open-sourced code and dataset support further research."}}
{"id": "2507.00419", "pdf": "https://arxiv.org/pdf/2507.00419", "abs": "https://arxiv.org/abs/2507.00419", "authors": ["Yimin Dou", "Xinming Wu", "Nathan L Bangs", "Harpreet Singh Sethi", "Jintao Li", "Hang Gao", "Zhixiang Guo"], "title": "Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Understanding Earth's subsurface is critical for energy transition, natural\nhazard mitigation, and planetary science. Yet subsurface analysis remains\nfragmented, with separate models required for structural interpretation,\nstratigraphic analysis, geobody segmentation, and property modeling-each\ntightly coupled to specific data distributions and task formulations. We\nintroduce the Geological Everything Model 3D (GEM), a unified generative\narchitecture that reformulates all these tasks as prompt-conditioned inference\nalong latent structural frameworks derived from subsurface imaging. This\nformulation moves beyond task-specific models by enabling a shared inference\nmechanism, where GEM propagates human-provided prompts-such as well logs,\nmasks, or structural sketches-along inferred structural frameworks to produce\ngeologically coherent outputs. Through this mechanism, GEM achieves zero-shot\ngeneralization across tasks with heterogeneous prompt types, without retraining\nfor new tasks or data sources. This capability emerges from a two-stage\ntraining process that combines self-supervised representation learning on\nlarge-scale field seismic data with adversarial fine-tuning using mixed prompts\nand labels across diverse subsurface tasks. GEM demonstrates broad\napplicability across surveys and tasks, including Martian radar stratigraphy\nanalysis, structural interpretation in subduction zones, full seismic\nstratigraphic interpretation, geobody delineation, and property modeling. By\nbridging expert knowledge with generative reasoning in a structurally aware\nmanner, GEM lays the foundation for scalable, human-in-the-loop geophysical\nAI-transitioning from fragmented pipelines to a vertically integrated,\npromptable reasoning system. Project page: https://douyimin.github.io/GEM", "AI": {"tldr": "GEM is a unified generative model for subsurface analysis, enabling zero-shot generalization across tasks using prompts without retraining.", "motivation": "To address fragmentation in subsurface analysis by unifying structural interpretation, stratigraphic analysis, and other tasks into a single model.", "method": "GEM uses a two-stage training process: self-supervised learning on seismic data and adversarial fine-tuning with mixed prompts. It propagates human-provided prompts along inferred structural frameworks.", "result": "GEM achieves zero-shot generalization across diverse tasks like Martian radar stratigraphy, seismic interpretation, and geobody delineation.", "conclusion": "GEM bridges expert knowledge and generative reasoning, offering a scalable, human-in-the-loop AI system for geophysical tasks."}}
{"id": "2507.00440", "pdf": "https://arxiv.org/pdf/2507.00440", "abs": "https://arxiv.org/abs/2507.00440", "authors": ["Yujia Yin", "Tianyi Qu", "Zihao Wang", "Yifan Chen"], "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": "ICML 2025 accepted", "summary": "Through recognizing causal subgraphs, causal graph learning (CGL) has risen\nto be a promising approach for improving the generalizability of graph neural\nnetworks under out-of-distribution (OOD) scenarios. However, the empirical\nsuccesses of CGL techniques are mostly exemplified in classification settings,\nwhile regression tasks, a more challenging setting in graph learning, are\noverlooked. We thus devote this work to tackling causal graph regression (CGR);\nto this end we reshape the processing of confounding effects in existing CGL\nstudies, which mainly deal with classification. Specifically, we reflect on the\npredictive power of confounders in graph-level regression, and generalize\nclassification-specific causal intervention techniques to regression through a\nlens of contrastive learning. Extensive experiments on graph OOD benchmarks\nvalidate the efficacy of our proposals for CGR. The model implementation and\nthe code are provided on https://github.com/causal-graph/CGR.", "AI": {"tldr": "The paper introduces causal graph regression (CGR) to address the overlooked challenge of regression tasks in causal graph learning, adapting causal intervention techniques for regression via contrastive learning.", "motivation": "Regression tasks in graph learning are more challenging and underexplored compared to classification, despite the success of causal graph learning (CGL) in improving generalizability under OOD scenarios.", "method": "The authors reshape the handling of confounding effects in CGL for classification, generalize causal intervention techniques to regression using contrastive learning, and validate their approach on graph OOD benchmarks.", "result": "Extensive experiments demonstrate the efficacy of the proposed CGR method.", "conclusion": "The work successfully extends causal graph learning to regression tasks, providing a validated framework and open-source implementation."}}
{"id": "2507.00898", "pdf": "https://arxiv.org/pdf/2507.00898", "abs": "https://arxiv.org/abs/2507.00898", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "AI": {"tldr": "ONLY is a training-free decoding method for LVLMs that reduces hallucination by amplifying key textual information using a text-to-visual entropy ratio, enabling efficient real-time deployment with minimal computational cost.", "motivation": "LVLMs suffer from hallucination, limiting reliable real-world deployment. Existing contrastive decoding methods are slow due to multiple queries.", "method": "ONLY uses a single query and one-layer intervention, selectively amplifying crucial textual information via a text-to-visual entropy ratio.", "result": "ONLY outperforms state-of-the-art methods across benchmarks with minimal effort and cost.", "conclusion": "ONLY offers an efficient, real-time solution to LVLM hallucination, balancing performance and computational efficiency."}}
{"id": "2507.00566", "pdf": "https://arxiv.org/pdf/2507.00566", "abs": "https://arxiv.org/abs/2507.00566", "authors": ["Kai Zhou", "Shuhai Zhang", "Zeng You", "Jinwu Hu", "Mingkui Tan", "Fei Liu"], "title": "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment", "categories": ["cs.CV"], "comment": "This paper is accepted by IEEE TIP 2025. Code is publicly available\n  at https://github.com/kaai520/PGFA", "summary": "Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.", "AI": {"tldr": "The paper introduces PGFA, a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, addressing limitations in existing methods and achieving significant accuracy improvements.", "motivation": "The challenge lies in generalizing from known to unseen skeleton-based human actions, with prior methods suffering from insufficient skeleton feature discrimination and alignment bias.", "method": "PGFA employs an end-to-end cross-modal contrastive training framework for better skeleton-text alignment and a prototype-guided text feature alignment strategy to mitigate distribution discrepancies.", "result": "PGFA outperforms the top competitor (SMIE) with absolute accuracy gains of 22.96%, 12.53%, and 18.54% on NTU-60, NTU-120, and PKU-MMD datasets.", "conclusion": "PGFA effectively addresses the limitations of previous methods, demonstrating superior performance in zero-shot skeleton-based action recognition."}}
{"id": "2507.00435", "pdf": "https://arxiv.org/pdf/2507.00435", "abs": "https://arxiv.org/abs/2507.00435", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://robo-eval.github.io", "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "AI": {"tldr": "RoboEval is a benchmark for evaluating bimanual manipulation policies, revealing hidden weaknesses beyond binary success metrics.", "motivation": "Current benchmarks only report binary task success, masking critical policy flaws like poor coordination or slipping.", "method": "RoboEval uses tiered tasks with skill-specific stages, diagnostic metrics, and 3000+ human demonstrations.", "result": "Policies with similar success rates show divergent execution issues, and behavioral metrics correlate with success.", "conclusion": "RoboEval provides deeper insights into policy failures, advocating for evaluation beyond binary success."}}
{"id": "2507.00445", "pdf": "https://arxiv.org/pdf/2507.00445", "abs": "https://arxiv.org/abs/2507.00445", "authors": ["Xingyu Su", "Xiner Li", "Masatoshi Uehara", "Sunwoo Kim", "Yulai Zhao", "Gabriele Scalia", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Degui Zhi", "Shuiwang Ji"], "title": "Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "We address the problem of fine-tuning diffusion models for reward-guided\ngeneration in biomolecular design. While diffusion models have proven highly\neffective in modeling complex, high-dimensional data distributions, real-world\napplications often demand more than high-fidelity generation, requiring\noptimization with respect to potentially non-differentiable reward functions\nsuch as physics-based simulation or rewards based on scientific knowledge.\nAlthough RL methods have been explored to fine-tune diffusion models for such\nobjectives, they often suffer from instability, low sample efficiency, and mode\ncollapse due to their on-policy nature. In this work, we propose an iterative\ndistillation-based fine-tuning framework that enables diffusion models to\noptimize for arbitrary reward functions. Our method casts the problem as policy\ndistillation: it collects off-policy data during the roll-in phase, simulates\nreward-based soft-optimal policies during roll-out, and updates the model by\nminimizing the KL divergence between the simulated soft-optimal policy and the\ncurrent model policy. Our off-policy formulation, combined with KL divergence\nminimization, enhances training stability and sample efficiency compared to\nexisting RL-based methods. Empirical results demonstrate the effectiveness and\nsuperior reward optimization of our approach across diverse tasks in protein,\nsmall molecule, and regulatory DNA design.", "AI": {"tldr": "A framework for fine-tuning diffusion models in biomolecular design using iterative distillation to optimize for non-differentiable rewards, improving stability and efficiency over RL methods.", "motivation": "Real-world applications require diffusion models to optimize for non-differentiable rewards (e.g., physics-based simulation), but RL methods face instability and inefficiency.", "method": "Proposes an iterative distillation framework: collects off-policy data, simulates reward-based soft-optimal policies, and updates the model via KL divergence minimization.", "result": "Outperforms RL-based methods in stability, sample efficiency, and reward optimization across protein, small molecule, and DNA design tasks.", "conclusion": "The method effectively fine-tunes diffusion models for reward-guided generation, addressing limitations of RL approaches."}}
{"id": "2210.06230", "pdf": "https://arxiv.org/pdf/2210.06230", "abs": "https://arxiv.org/abs/2210.06230", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder", "categories": ["cs.CL", "cs.AI"], "comment": "CoNLL2025 (Best Paper nomination)", "summary": "Formal/symbolic semantics can provide canonical, rigid controllability and\ninterpretability to sentence representations due to their \\textit{localisation}\nor \\textit{composition} property. How can we deliver such property to the\ncurrent distributional sentence representations to control and interpret the\ngeneration of language models (LMs)? In this work, we theoretically frame the\nsentence semantics as the composition of \\textit{semantic role - word content}\nfeatures and propose the formal semantic geometry. To inject such geometry into\nTransformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational\nAutoEncoder with a supervision approach, where the sentence generation can be\nmanipulated and explained over low-dimensional latent Gaussian space. In\naddition, we propose a new probing algorithm to guide the movement of sentence\nvectors over such geometry. Experimental results reveal that the formal\nsemantic geometry can potentially deliver better control and interpretation to\nsentence generation.", "AI": {"tldr": "The paper proposes a method to integrate formal semantic properties into distributional sentence representations for better control and interpretability in language models.", "motivation": "To enhance the controllability and interpretability of sentence representations in language models by incorporating formal semantic properties.", "method": "Theoretical framing of sentence semantics as composition of semantic role-word content features, using Transformer-based Variational AutoEncoder with supervision and a probing algorithm.", "result": "Formal semantic geometry improves control and interpretation of sentence generation in Transformer-based LMs.", "conclusion": "The proposed method successfully injects formal semantic properties into language models, enhancing their controllability and interpretability."}}
{"id": "2507.00570", "pdf": "https://arxiv.org/pdf/2507.00570", "abs": "https://arxiv.org/abs/2507.00570", "authors": ["Zizhao Li", "Xueyang Kang", "Joseph West", "Kourosh Khoshelham"], "title": "Out-of-distribution detection in 3D applications: a review", "categories": ["cs.CV"], "comment": null, "summary": "The ability to detect objects that are not prevalent in the training set is a\ncritical capability in many 3D applications, including autonomous driving.\nMachine learning methods for object recognition often assume that all object\ncategories encountered during inference belong to a closed set of classes\npresent in the training data. This assumption limits generalization to the real\nworld, as objects not seen during training may be misclassified or entirely\nignored. As part of reliable AI, OOD detection identifies inputs that deviate\nsignificantly from the training distribution. This paper provides a\ncomprehensive overview of OOD detection within the broader scope of trustworthy\nand uncertain AI. We begin with key use cases across diverse domains, introduce\nbenchmark datasets spanning multiple modalities, and discuss evaluation\nmetrics. Next, we present a comparative analysis of OOD detection\nmethodologies, exploring model structures, uncertainty indicators, and\ndistributional distance taxonomies, alongside uncertainty calibration\ntechniques. Finally, we highlight promising research directions, including\nadversarially robust OOD detection and failure identification, particularly\nrelevant to 3D applications. The paper offers both theoretical and practical\ninsights into OOD detection, showcasing emerging research opportunities such as\n3D vision integration. These insights help new researchers navigate the field\nmore effectively, contributing to the development of reliable, safe, and robust\nAI systems.", "AI": {"tldr": "The paper provides a comprehensive overview of Out-of-Distribution (OOD) detection in AI, focusing on its importance for reliable systems like autonomous driving. It covers use cases, datasets, methodologies, and future research directions.", "motivation": "The need to detect objects not seen during training is critical for real-world applications, as traditional methods assume a closed set of classes, limiting generalization.", "method": "The paper reviews benchmark datasets, evaluation metrics, and methodologies for OOD detection, including model structures, uncertainty indicators, and calibration techniques.", "result": "A comparative analysis of OOD detection approaches is presented, highlighting strengths and limitations. Future directions like adversarial robustness and 3D integration are identified.", "conclusion": "The paper serves as a guide for researchers, offering insights into OOD detection to advance reliable and robust AI systems, with a focus on emerging 3D applications."}}
{"id": "2507.00451", "pdf": "https://arxiv.org/pdf/2507.00451", "abs": "https://arxiv.org/abs/2507.00451", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "title": "Best Agent Identification for General Game Playing", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "AI": {"tldr": "The paper introduces Optimistic-WS, a method for identifying the best-performing algorithm in multi-task domains using multi-armed bandits, showing improved performance over existing methods.", "motivation": "To accurately and efficiently identify the best algorithm for each sub-task in multi-problem domains, particularly in general game frameworks like GVGAI and Ludii.", "method": "Treats the problem as best arm identification in multi-armed bandits, using an optimistic selection process based on the Wilson score interval (Optimistic-WS) to rank arms by potential regret reduction.", "result": "Demonstrates substantial improvement in average simple regret compared to previous best arm identification algorithms.", "conclusion": "Optimistic-WS enhances agent evaluation in general game frameworks and other high-runtime multi-task domains."}}
{"id": "2507.00453", "pdf": "https://arxiv.org/pdf/2507.00453", "abs": "https://arxiv.org/abs/2507.00453", "authors": ["Ankit Kashyap"], "title": "Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling", "categories": ["cs.LG", "F.2.2; I.2.6; I.2.7"], "comment": "19 pages, 9 figures, 1 table; implemented entirely from scratch in\n  PyTorch", "summary": "We present a Transformer architecture for long-context language modeling that\ncombines global attention with two biologically inspired components: chunked\nlocal attention and a gated FIFO memory mechanism. This unified attention block\nallows the model to efficiently handle both short-range and long-range\ndependencies without increasing attention cost quadratically. The memory module\npersistently stores past token representations using a gated update mechanism\ninspired by recurrent networks. Rotary positional encoding is applied per\nattention head to enable directionally disentangled, scale-invariant positional\nsignals. The architecture is implemented entirely from scratch in PyTorch, with\nno reliance on high-level libraries, enabling transparent and modular\nexperimentation. Our model offers a lightweight and extensible design for tasks\nsuch as dialogue modeling, code completion, and document understanding.", "AI": {"tldr": "A Transformer model with global attention, chunked local attention, and a gated FIFO memory for efficient long-context language modeling.", "motivation": "To handle both short-range and long-range dependencies efficiently without quadratic attention cost.", "method": "Combines global attention, chunked local attention, and a gated FIFO memory with rotary positional encoding.", "result": "Efficient and lightweight model for tasks like dialogue modeling, code completion, and document understanding.", "conclusion": "The architecture provides a scalable and modular solution for long-context language tasks."}}
{"id": "2401.14640", "pdf": "https://arxiv.org/pdf/2401.14640", "abs": "https://arxiv.org/abs/2401.14640", "authors": ["Nan Hu", "Jiaoyan Chen", "Yike Wu", "Guilin Qi", "Hongru Wang", "Sheng Bi", "Yongrui Chen", "Tongtong Wu", "Jeff Z. Pan"], "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark.", "AI": {"tldr": "The paper introduces Complex Attributed Question Answering (CAQA), a benchmark addressing limitations in AQA evaluation, featuring fine-grained attribution categories, automatic KG-based generation, and complex scenarios.", "motivation": "To overcome limitations in evaluating attributions in AQA, such as lack of fine-grained categories, reliance on manual annotations, and inability to compare subtle differences.", "method": "Developed CAQA using Knowledge Graphs (KGs) for automatic generation of comprehensive attribution categories and complex scenarios. Evaluated 25 automatic evaluators, compared them with human evaluators, and tested fine-tuned LLM evaluators.", "result": "CAQA proved effective, with experiments revealing important findings for future AQA research.", "conclusion": "CAQA serves as a valuable benchmark for AQA, with publicly available codes and data."}}
{"id": "2507.00583", "pdf": "https://arxiv.org/pdf/2507.00583", "abs": "https://arxiv.org/abs/2507.00583", "authors": ["Christian Intern\u00f2", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "title": "AI-Generated Video Detection via Perceptual Straightening", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "AI": {"tldr": "ReStraV detects AI-generated videos by analyzing temporal curvature and stepwise distance in neural representations, achieving high accuracy and efficiency.", "motivation": "The rise of realistic synthetic videos challenges content authentication, requiring better detection methods.", "method": "Uses a pre-trained vision transformer (DINOv2) to quantify temporal curvature and stepwise distance, training a classifier on these metrics.", "result": "Achieves 97.17% accuracy and 98.63% AUROC, outperforming existing methods.", "conclusion": "ReStraV offers an efficient, effective solution for AI-generated video detection, leveraging neural representation geometry."}}
{"id": "2507.00459", "pdf": "https://arxiv.org/pdf/2507.00459", "abs": "https://arxiv.org/abs/2507.00459", "authors": ["Hoang Cuong Phan", "Minh Tien Tran", "Chihun Lee", "Hoheok Kim", "Sehyok Oh", "Dong-Kyu Kim", "Ho Won Lee"], "title": "Process-aware and high-fidelity microstructure generation using stable diffusion", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": "46 pages, 13 figures, 5 tables, 3rd Word Congress on Artificial\n  Intelligence in Materials & Manufacturing 2025", "summary": "Synthesizing realistic microstructure images conditioned on processing\nparameters is crucial for understanding process-structure relationships in\nmaterials design. However, this task remains challenging due to limited\ntraining micrographs and the continuous nature of processing variables. To\novercome these challenges, we present a novel process-aware generative modeling\napproach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art\ntext-to-image diffusion model adapted for microstructure generation. Our method\nintroduces numeric-aware embeddings that encode continuous variables (annealing\ntemperature, time, and magnification) directly into the model's conditioning,\nenabling controlled image generation under specified process conditions and\ncapturing process-driven microstructural variations. To address data scarcity\nand computational constraints, we fine-tune only a small fraction of the\nmodel's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently\ntransferring the pre-trained model to the materials domain. We validate realism\nusing a semantic segmentation model based on a fine-tuned U-Net with a VGG16\nencoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean\nIoU, outperforming previous methods. Quantitative analyses using physical\ndescriptors and spatial statistics show strong agreement between synthetic and\nreal microstructures. Specifically, two-point correlation and lineal-path\nerrors remain below 2.1% and 0.6%, respectively. Our method represents the\nfirst adaptation of SD3.5-Large for process-aware microstructure generation,\noffering a scalable approach for data-driven materials design.", "AI": {"tldr": "A novel process-aware generative model using SD3.5-Large for microstructure synthesis, addressing data scarcity and computational constraints, validated by high accuracy and agreement with real microstructures.", "motivation": "Understanding process-structure relationships in materials design is hindered by limited training data and continuous processing variables.", "method": "Adapts SD3.5-Large with numeric-aware embeddings for process conditions, fine-tuned via DreamBooth and LoRA, and validated using a semantic segmentation model.", "result": "Achieves 97.1% accuracy and 85.7% mean IoU, with synthetic microstructures closely matching real ones (errors below 2.1% and 0.6%).", "conclusion": "First adaptation of SD3.5-Large for microstructure generation, offering a scalable solution for materials design."}}
{"id": "2507.00467", "pdf": "https://arxiv.org/pdf/2507.00467", "abs": "https://arxiv.org/abs/2507.00467", "authors": ["Sijan Bhattarai", "Saurav Bhandari", "Girija Bhusal", "Saroj Shakya", "Tapendra Pandey"], "title": "Diversity Conscious Refined Random Forest", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Random Forest (RF) is a widely used ensemble learning technique known for its\nrobust classification performance across diverse domains. However, it often\nrelies on hundreds of trees and all input features, leading to high inference\ncost and model redundancy. In this work, our goal is to grow trees dynamically\nonly on informative features and then enforce maximal diversity by clustering\nand retaining uncorrelated trees. Therefore, we propose a Refined Random Forest\nClassifier that iteratively refines itself by first removing the least\ninformative features and then analytically determines how many new trees should\nbe grown, followed by correlation-based clustering to remove redundant trees.\nThe classification accuracy of our model was compared against the standard RF\non the same number of trees. Experiments on 8 multiple benchmark datasets,\nincluding binary and multiclass datasets, demonstrate that the proposed model\nachieves improved accuracy compared to standard RF.", "AI": {"tldr": "A refined Random Forest classifier dynamically grows trees on informative features, removes redundancy, and improves accuracy over standard RF.", "motivation": "Standard Random Forest uses many trees and all features, leading to high costs and redundancy. The goal is to optimize by focusing on informative features and diverse trees.", "method": "The model iteratively removes less informative features, grows new trees as needed, and uses correlation-based clustering to eliminate redundant trees.", "result": "Experiments on 8 benchmark datasets show improved accuracy compared to standard RF with the same number of trees.", "conclusion": "The refined RF classifier is more efficient and accurate by reducing redundancy and focusing on informative features."}}
{"id": "2406.04370", "pdf": "https://arxiv.org/pdf/2406.04370", "abs": "https://arxiv.org/abs/2406.04370", "authors": ["Tejaswini Pedapati", "Amit Dhurandhar", "Soumya Ghosh", "Soham Dan", "Prasanna Sattigeri"], "title": "Large Language Model Confidence Estimation via Black-Box Access", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to TMLR 2025", "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.", "AI": {"tldr": "A framework for estimating confidence in LLM responses using interpretable features and logistic regression, showing effectiveness across multiple models and tasks.", "motivation": "To evaluate trust in model responses by estimating confidence, especially with only black-box access to LLMs.", "method": "Engineer novel features and train a logistic regression model to predict confidence in responses.", "result": "Outperforms baselines by over 10% in some cases, with models generalizing zero-shot across LLMs.", "conclusion": "Simple, interpretable framework effectively estimates confidence and provides insights into predictive features."}}
{"id": "2507.00585", "pdf": "https://arxiv.org/pdf/2507.00585", "abs": "https://arxiv.org/abs/2507.00585", "authors": ["Tang Hao", "Guo ZhiQing", "Wang LieJun", "Liu Chao"], "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, it has been found that \"grandmother cells\" in the primary\nvisual cortex (V1) of macaques can directly recognize visual input with complex\nshapes. This inspires us to examine the value of these cells in promoting the\nresearch of medical image segmentation. In this paper, we design a Similarity\nMemory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,\nwe propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and\nremembers the category features of specific lesions or organs in medical images\nthrough the similarity memory prior in the prototype memory bank, thus helping\nthe network to learn subtle texture changes between categories. DMW-LA also\ndynamically updates the similarity memory prior in reverse through Weight-Loss\nDynamic (W-LD) update strategy, effectively assisting the network directly\nextract category features. In addition, we propose the Double-Similarity Global\nInternal Enhancement Module (DS-GIM) to deeply explore the internal differences\nin the feature distribution of input data through cosine similarity and\neuclidean distance. Extensive experiments on four public datasets show that\nSim-MPNet has better segmentation performance than other state-of-the-art\nmethods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.", "AI": {"tldr": "The paper introduces Sim-MPNet, a network for medical image segmentation, leveraging grandmother cells' recognition capabilities. It includes DMW-LA for dynamic feature matching and DS-GIM for feature exploration, outperforming state-of-the-art methods.", "motivation": "Inspired by grandmother cells in macaque V1, the study aims to enhance medical image segmentation by leveraging complex shape recognition.", "method": "Proposes Sim-MPNet with DMW-LA for dynamic feature matching and DS-GIM for feature distribution analysis using cosine similarity and Euclidean distance.", "result": "Sim-MPNet achieves superior segmentation performance on four public datasets.", "conclusion": "The method effectively improves segmentation by dynamically learning and updating category features, validated by extensive experiments."}}
{"id": "2507.00461", "pdf": "https://arxiv.org/pdf/2507.00461", "abs": "https://arxiv.org/abs/2507.00461", "authors": ["Garimella Ramamurthy", "Marcos Eduardo Valle", "Tata Jagannadha Swamy"], "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization", "categories": ["cs.NE", "cs.AI"], "comment": "Paper submitted to the Fifth International Conference on Emerging\n  Techniques in Computational Intelligence (ICETCI 2025)", "summary": "This research paper introduces two novel complex-valued Hopfield neural\nnetworks (CvHNNs) that incorporate phase and magnitude quantization. The first\nCvHNN employs a ceiling-type activation function that operates on the\nrectangular coordinate representation of the complex net contribution. The\nsecond CvHNN similarly incorporates phase and magnitude quantization but\nutilizes a ceiling-type activation function based on the polar coordinate\nrepresentation of the complex net contribution. The proposed CvHNNs, with their\nphase and magnitude quantization, significantly increase the number of states\ncompared to existing models in the literature, thereby expanding the range of\npotential applications for CvHNNs.", "AI": {"tldr": "Two new complex-valued Hopfield neural networks (CvHNNs) with phase and magnitude quantization are introduced, increasing state capacity and application potential.", "motivation": "To enhance the capabilities of CvHNNs by incorporating phase and magnitude quantization, expanding their usability.", "method": "Two CvHNNs are proposed: one uses rectangular coordinate-based activation, the other polar coordinate-based activation, both with quantization.", "result": "The new CvHNNs significantly increase the number of states compared to existing models.", "conclusion": "The proposed CvHNNs offer expanded application potential due to their increased state capacity."}}
{"id": "2507.00480", "pdf": "https://arxiv.org/pdf/2507.00480", "abs": "https://arxiv.org/abs/2507.00480", "authors": ["Kiyoung Om", "Kyuil Sim", "Taeyoung Yun", "Hyeongyu Kang", "Jinkyoo Park"], "title": "Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization", "categories": ["cs.LG", "stat.ML"], "comment": "25 pages, 11 figures, 5 tables. Equal contribution by Kiyoung Om,\n  Kyuil Sim, and Taeyoung Yun", "summary": "Optimizing high-dimensional black-box functions under black-box constraints\nis a pervasive task in a wide range of scientific and engineering problems.\nThese problems are typically harder than unconstrained problems due to\nhard-to-find feasible regions. While Bayesian optimization (BO) methods have\nbeen developed to solve such problems, they often struggle with the curse of\ndimensionality. Recently, generative model-based approaches have emerged as a\npromising alternative for constrained optimization. However, they suffer from\npoor scalability and are vulnerable to mode collapse, particularly when the\ntarget distribution is highly multi-modal. In this paper, we propose a new\nframework to overcome these challenges. Our method iterates through two stages.\nFirst, we train flow-based models to capture the data distribution and\nsurrogate models that predict both function values and constraint violations\nwith uncertainty quantification. Second, we cast the candidate selection\nproblem as a posterior inference problem to effectively search for promising\ncandidates that have high objective values while not violating the constraints.\nDuring posterior inference, we find that the posterior distribution is highly\nmulti-modal and has a large plateau due to constraints, especially when\nconstraint feedback is given as binary indicators of feasibility. To mitigate\nthis issue, we amortize the sampling from the posterior distribution in the\nlatent space of flow-based models, which is much smoother than that in the data\nspace. We empirically demonstrate that our method achieves superior performance\non various synthetic and real-world constrained black-box optimization tasks.\nOur code is publicly available \\href{https://github.com/umkiyoung/CiBO}{here}.", "AI": {"tldr": "A new framework for high-dimensional constrained black-box optimization using flow-based models and posterior inference, outperforming existing methods.", "motivation": "High-dimensional constrained optimization is challenging due to hard-to-find feasible regions and limitations of current methods like BO and generative models.", "method": "Proposes a two-stage approach: training flow-based models for data distribution and surrogate models, then using posterior inference for candidate selection.", "result": "Empirically superior performance on synthetic and real-world tasks, addressing multi-modality and scalability issues.", "conclusion": "The framework effectively handles constrained optimization, leveraging smooth latent spaces for improved performance."}}
{"id": "2410.01141", "pdf": "https://arxiv.org/pdf/2410.01141", "abs": "https://arxiv.org/abs/2410.01141", "authors": ["Doohee You", "S Fraiberger"], "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics.", "AI": {"tldr": "The study evaluates deduplication methods for NLP datasets using pairing techniques, distance measures, and sBERT, finding low duplicate prevalence. Human-annotated validation supports NLP-based metrics.", "motivation": "To assess the efficiency of deduplication techniques for large NLP datasets, particularly economic research paper titles, to ensure data quality.", "method": "Explored pairing methods with Levenshtein distance, cosine similarity, and sBERT for semantic evaluation, followed by human-annotated validation.", "result": "Low prevalence of duplicates observed across methods, with NLP and LLM-based metrics validated by human annotation.", "conclusion": "The study confirms the effectiveness of NLP-based deduplication, suggesting further refinement with human validation for conclusive results."}}
{"id": "2507.00586", "pdf": "https://arxiv.org/pdf/2507.00586", "abs": "https://arxiv.org/abs/2507.00586", "authors": ["Luming Zhao", "Jingwen Xuan", "Jiamin Lou", "Yonghui Yu", "Wenwu Yang"], "title": "Context-Aware Academic Emotion Dataset and Benchmark", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Academic emotion analysis plays a crucial role in evaluating students'\nengagement and cognitive states during the learning process. This paper\naddresses the challenge of automatically recognizing academic emotions through\nfacial expressions in real-world learning environments. While significant\nprogress has been made in facial expression recognition for basic emotions,\nacademic emotion recognition remains underexplored, largely due to the scarcity\nof publicly available datasets. To bridge this gap, we introduce RAER, a novel\ndataset comprising approximately 2,700 video clips collected from around 140\nstudents in diverse, natural learning contexts such as classrooms, libraries,\nlaboratories, and dormitories, covering both classroom sessions and individual\nstudy. Each clip was annotated independently by approximately ten annotators\nusing two distinct sets of academic emotion labels with varying granularity,\nenhancing annotation consistency and reliability. To our knowledge, RAER is the\nfirst dataset capturing diverse natural learning scenarios. Observing that\nannotators naturally consider context cues-such as whether a student is looking\nat a phone or reading a book-alongside facial expressions, we propose CLIP-CAER\n(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes\nlearnable text prompts within the vision-language model CLIP to effectively\nintegrate facial expression and context cues from videos. Experimental results\ndemonstrate that CLIP-CAER substantially outperforms state-of-the-art\nvideo-based facial expression recognition methods, which are primarily designed\nfor basic emotions, emphasizing the crucial role of context in accurately\nrecognizing academic emotions. Project page: https://zgsfer.github.io/CAER", "AI": {"tldr": "The paper introduces RAER, a novel dataset for academic emotion recognition, and CLIP-CAER, a method combining facial and context cues using CLIP, outperforming existing methods.", "motivation": "Academic emotion recognition is underexplored due to lack of datasets. The paper aims to address this gap by creating a diverse dataset and improving recognition accuracy.", "method": "The authors introduce RAER, a dataset of 2,700 video clips from natural learning contexts, annotated by multiple annotators. They propose CLIP-CAER, leveraging CLIP to integrate facial and context cues.", "result": "CLIP-CAER outperforms state-of-the-art methods, highlighting the importance of context in academic emotion recognition.", "conclusion": "The study advances academic emotion recognition by providing a robust dataset and a context-aware method, demonstrating the significance of contextual cues."}}
{"id": "2507.00482", "pdf": "https://arxiv.org/pdf/2507.00482", "abs": "https://arxiv.org/abs/2507.00482", "authors": ["Chanseok Lee", "Fakhriyya Mammadova", "Jiseong Barg", "Mooseok Jang"], "title": "Physics-Aware Style Transfer for Adaptive Holographic Reconstruction", "categories": ["physics.optics", "cs.AI", "cs.LG"], "comment": "Keywords: holographic imaging, style transfer, phase retrieval, deep\n  learning", "summary": "Inline holographic imaging presents an ill-posed inverse problem of\nreconstructing objects' complex amplitude from recorded diffraction patterns.\nAlthough recent deep learning approaches have shown promise over classical\nphase retrieval algorithms, they often require high-quality ground truth\ndatasets of complex amplitude maps to achieve a statistical inverse mapping\noperation between the two domains. Here, we present a physics-aware style\ntransfer approach that interprets the object-to-sensor distance as an implicit\nstyle within diffraction patterns. Using the style domain as the intermediate\ndomain to construct cyclic image translation, we show that the inverse mapping\noperation can be learned in an adaptive manner only with datasets composed of\nintensity measurements. We further demonstrate its biomedical applicability by\nreconstructing the morphology of dynamically flowing red blood cells,\nhighlighting its potential for real-time, label-free imaging. As a framework\nthat leverages physical cues inherently embedded in measurements, the presented\nmethod offers a practical learning strategy for imaging applications where\nground truth is difficult or impossible to obtain.", "AI": {"tldr": "A physics-aware style transfer method is introduced to solve the ill-posed inverse problem in inline holographic imaging, enabling reconstruction without high-quality ground truth datasets.", "motivation": "Traditional deep learning approaches require high-quality ground truth datasets for complex amplitude reconstruction, which are often difficult or impossible to obtain.", "method": "The method interprets object-to-sensor distance as an implicit style in diffraction patterns, using cyclic image translation to learn the inverse mapping adaptively with intensity-only datasets.", "result": "The approach successfully reconstructs the morphology of dynamically flowing red blood cells, demonstrating real-time, label-free imaging potential.", "conclusion": "The framework leverages inherent physical cues in measurements, offering a practical learning strategy for imaging applications lacking ground truth data."}}
{"id": "2507.00485", "pdf": "https://arxiv.org/pdf/2507.00485", "abs": "https://arxiv.org/abs/2507.00485", "authors": ["Weiran Guo", "Guanjun Liu", "Ziyuan Zhou", "Ling Wang"], "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) is widely used in tasks where agents interact\nwith an environment to maximize rewards. Building on this foundation, Safe\nReinforcement Learning (Safe RL) incorporates a cost metric alongside the\nreward metric, ensuring that agents adhere to safety constraints during\ndecision-making. In this paper, we identify that Safe RL is vulnerable to\nbackdoor attacks, which can manipulate agents into performing unsafe actions.\nFirst, we introduce the relevant concepts and evaluation metrics for backdoor\nattacks in Safe RL. It is the first attack framework in the Safe RL field that\ninvolves both Positive and Negative Action sample (PNAct) is to implant\nbackdoors, where positive action samples provide reference actions and negative\naction samples indicate actions to be avoided. We theoretically point out the\nproperties of PNAct and design an attack algorithm. Finally, we conduct\nexperiments to evaluate the effectiveness of our proposed backdoor attack\nframework, evaluating it with the established metrics. This paper highlights\nthe potential risks associated with Safe RL and underscores the feasibility of\nsuch attacks. Our code and supplementary material are available at\nhttps://github.com/azure-123/PNAct.", "AI": {"tldr": "The paper introduces a backdoor attack framework (PNAct) for Safe Reinforcement Learning, demonstrating its feasibility and risks.", "motivation": "To expose vulnerabilities in Safe RL by showing how backdoor attacks can manipulate agents into unsafe actions.", "method": "Introduces PNAct, a framework using Positive and Negative Action samples to implant backdoors, with a designed attack algorithm.", "result": "Experiments confirm the effectiveness of PNAct in manipulating Safe RL agents.", "conclusion": "Highlights risks in Safe RL and proves the feasibility of backdoor attacks, urging further research into defenses."}}
{"id": "2410.14405", "pdf": "https://arxiv.org/pdf/2410.14405", "abs": "https://arxiv.org/abs/2410.14405", "authors": ["Denitsa Saynova", "Lovisa Hagstr\u00f6m", "Moa Johansson", "Richard Johansson", "Marco Kuhlmann"], "title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion", "categories": ["cs.CL"], "comment": "accepted to ACL Findings 2025", "summary": "Language models (LMs) can make a correct prediction based on many possible\nsignals in a prompt, not all corresponding to recall of factual associations.\nHowever, current interpretations of LMs fail to take this into account. For\nexample, given the query \"Astrid Lindgren was born in\" with the corresponding\ncompletion \"Sweden\", no difference is made between whether the prediction was\nbased on knowing where the author was born or assuming that a person with a\nSwedish-sounding name was born in Sweden. In this paper, we present a\nmodel-specific recipe - PrISM - for constructing datasets with examples of four\ndifferent prediction scenarios: generic language modeling, guesswork,\nheuristics recall and exact fact recall. We apply two popular interpretability\nmethods to the scenarios: causal tracing (CT) and information flow analysis. We\nfind that both yield distinct results for each scenario. Results for exact fact\nrecall and generic language modeling scenarios confirm previous conclusions\nabout the importance of mid-range MLP sublayers for fact recall, while results\nfor guesswork and heuristics indicate a critical role of late last token\nposition MLP sublayers. In summary, we contribute resources for a more\nextensive and granular study of fact completion in LMs, together with analyses\nthat provide a more nuanced understanding of how LMs process fact-related\nqueries.", "AI": {"tldr": "The paper introduces PrISM, a method to analyze how language models (LMs) predict facts, distinguishing between recall, guesswork, and heuristics. It uses causal tracing and information flow analysis to reveal distinct processing patterns for each scenario.", "motivation": "Current interpretations of LMs overlook the diverse signals they use for predictions, such as factual recall or name-based assumptions. This limits understanding of LM behavior.", "method": "The authors propose PrISM to create datasets for four prediction scenarios (generic LM, guesswork, heuristics, exact recall) and apply causal tracing and information flow analysis.", "result": "Exact fact recall and generic LM scenarios emphasize mid-range MLP sublayers, while guesswork and heuristics rely on late last token MLP sublayers.", "conclusion": "The study provides tools and insights for a nuanced analysis of how LMs process fact-related queries, improving interpretability."}}
{"id": "2507.00593", "pdf": "https://arxiv.org/pdf/2507.00593", "abs": "https://arxiv.org/abs/2507.00593", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "categories": ["cs.CV"], "comment": "Under review at ESWA", "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "AI": {"tldr": "The study evaluates overtake detection in trucks using CAN bus data, testing ANN, RF, and SVM classifiers. Data diversity and multi-vehicle training improve performance, with score-level fusion achieving TNR=93% and TPR=86.5%.", "motivation": "Safe overtaking in trucks is critical for accident prevention and traffic efficiency, requiring accurate prediction for ADAS.", "method": "Used CAN bus data from five Volvo trucks, evaluated ANN, RF, and SVM classifiers, and applied score-level fusion for improved performance.", "result": "Variability in traffic conditions affects classification; multi-vehicle training enhances generalization. Fusion achieved TNR=93% and TPR=86.5%.", "conclusion": "Score-level fusion and diverse training data improve overtake detection accuracy, supporting ADAS applications."}}
{"id": "2507.00513", "pdf": "https://arxiv.org/pdf/2507.00513", "abs": "https://arxiv.org/abs/2507.00513", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "ACM CSCW Poster 2025", "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "AI": {"tldr": "AI assistance in customer service reduces traditional burdens but adds new ones like compliance and psychological stress.", "motivation": "To understand how CSRs perceive AI tools in customer interactions.", "method": "Field visits and semi-structured interviews with 13 CSRs.", "result": "AI eases tasks like typing but introduces new burdens (e.g., compliance, psychological stress).", "conclusion": "AI integration in organizations requires balancing benefits and new challenges for employees."}}
{"id": "2507.00518", "pdf": "https://arxiv.org/pdf/2507.00518", "abs": "https://arxiv.org/abs/2507.00518", "authors": ["Walid Bendada", "Guillaume Salha-Galvan", "Romain Hennequin", "Th\u00e9o Bontempelli", "Thomas Bouab\u00e7a", "Tristan Cazenave"], "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "categories": ["cs.LG", "cs.IR"], "comment": "42nd International Conference on Machine Learning (ICML 2025)", "summary": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable\nmethod for exploring large action sets in reinforcement learning problems where\nhyperspherical embedding vectors represent these actions. vMF-exp involves\ninitially sampling a state embedding representation using a von Mises-Fisher\ndistribution, then exploring this representation's nearest neighbors, which\nscales to virtually unlimited numbers of candidate actions. We show that, under\ntheoretical assumptions, vMF-exp asymptotically maintains the same probability\nof exploring each action as Boltzmann Exploration (B-exp), a popular\nalternative that, nonetheless, suffers from scalability issues as it requires\ncomputing softmax values for each action. Consequently, vMF-exp serves as a\nscalable alternative to B-exp for exploring large action sets with\nhyperspherical embeddings. Experiments on simulated data, real-world public\ndata, and the successful large-scale deployment of vMF-exp on the recommender\nsystem of a global music streaming service empirically validate the key\nproperties of the proposed method.", "AI": {"tldr": "vMF-exp is a scalable exploration method for large action sets in RL using hyperspherical embeddings, outperforming Boltzmann Exploration in scalability.", "motivation": "Address scalability issues in exploring large action sets, particularly where Boltzmann Exploration fails due to computational inefficiency.", "method": "Sample state embeddings using von Mises-Fisher distribution and explore nearest neighbors, enabling scalability to unlimited actions.", "result": "Theoretically matches Boltzmann Exploration's exploration probability while being scalable; validated in simulations, real-world data, and a music streaming recommender system.", "conclusion": "vMF-exp is a practical, scalable alternative to Boltzmann Exploration for large action sets with hyperspherical embeddings."}}
{"id": "2412.14373", "pdf": "https://arxiv.org/pdf/2412.14373", "abs": "https://arxiv.org/abs/2412.14373", "authors": ["William Han", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling", "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "comment": "38 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods.", "AI": {"tldr": "ECG-Byte is a novel BPE tokenizer pipeline for direct end-to-end LLM training of ECG signals, improving efficiency and interpretability over traditional two-stage methods.", "motivation": "Existing methods for generating text from ECG signals are inefficient due to multi-stage training and lack interpretability of encoder-generated features.", "method": "Proposed ECG-Byte, a BPE tokenizer pipeline that compresses and encodes ECG signals into tokens for direct LLM training, combining ECG and text tokens.", "result": "Achieves competitive NLG performance, trains 3 times faster, and uses only 48% of the data compared to traditional methods.", "conclusion": "ECG-Byte offers a more efficient and interpretable solution for ECG-to-text generation, outperforming existing approaches."}}
{"id": "2507.00603", "pdf": "https://arxiv.org/pdf/2507.00603", "abs": "https://arxiv.org/abs/2507.00603", "authors": ["Yupeng Zheng", "Pengxuan Yang", "Zebin Xing", "Qichao Zhang", "Yuhang Zheng", "Yinfeng Gao", "Pengfei Li", "Teng Zhang", "Zhongpu Xia", "Peng Jia", "Dongbin Zhao"], "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "categories": ["cs.CV"], "comment": "ICCV 2025, first version", "summary": "End-to-end autonomous driving directly generates planning trajectories from\nraw sensor data, yet it typically relies on costly perception supervision to\nextract scene information. A critical research challenge arises: constructing\nan informative driving world model to enable perception annotation-free,\nend-to-end planning via self-supervised learning. In this paper, we present\nWorld4Drive, an end-to-end autonomous driving framework that employs vision\nfoundation models to build latent world models for generating and evaluating\nmulti-modal planning trajectories. Specifically, World4Drive first extracts\nscene features, including driving intention and world latent representations\nenriched with spatial-semantic priors provided by vision foundation models. It\nthen generates multi-modal planning trajectories based on current scene\nfeatures and driving intentions and predicts multiple intention-driven future\nstates within the latent space. Finally, it introduces a world model selector\nmodule to evaluate and select the best trajectory. We achieve perception\nannotation-free, end-to-end planning through self-supervised alignment between\nactual future observations and predicted observations reconstructed from the\nlatent space. World4Drive achieves state-of-the-art performance without manual\nperception annotations on both the open-loop nuScenes and closed-loop NavSim\nbenchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower\ncollision rate, and 3.75 faster training convergence. Codes will be accessed at\nhttps://github.com/ucaszyp/World4Drive.", "AI": {"tldr": "World4Drive is an end-to-end autonomous driving framework using vision foundation models for self-supervised trajectory planning, eliminating costly perception annotations.", "motivation": "To enable perception annotation-free, end-to-end planning by constructing an informative driving world model via self-supervised learning.", "method": "Extracts scene features using vision foundation models, generates multi-modal trajectories, predicts future states, and selects the best trajectory via a world model selector.", "result": "Achieves state-of-the-art performance with 18.1% lower L2 error, 46.7% fewer collisions, and 3.75x faster training.", "conclusion": "World4Drive demonstrates effective self-supervised planning without manual perception annotations, outperforming benchmarks."}}
{"id": "2507.00535", "pdf": "https://arxiv.org/pdf/2507.00535", "abs": "https://arxiv.org/abs/2507.00535", "authors": ["Dietmar Jannach", "Amra Deli\u0107", "Francesco Ricci", "Markus Zanker"], "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "categories": ["cs.IR", "cs.AI"], "comment": "Submitted for publication", "summary": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "AI": {"tldr": "The paper critiques the lack of real-world adoption of group recommender systems despite extensive research, suggesting a shift toward AI-assisted, interactive designs like ChatGPT for more natural decision-making.", "motivation": "Despite decades of research on group recommender systems, practical implementations are rare, prompting a reevaluation of academic assumptions and system designs.", "method": "Proposes leveraging modern Generative AI (e.g., ChatGPT) to create interactive, AI-assisted group recommendation agents that support natural decision-making in chats.", "result": "Highlights the gap between academic research and real-world needs, advocating for AI-driven solutions to improve adoption.", "conclusion": "Calls for a reorientation in group recommender research, focusing on AI-assisted, interactive systems to enhance practicality and user adoption."}}
{"id": "2507.00574", "pdf": "https://arxiv.org/pdf/2507.00574", "abs": "https://arxiv.org/abs/2507.00574", "authors": ["Haresh Rengaraj Rajamohan", "Xiang Gao", "Weicheng Zhu", "Shih-Lun Huang", "Long Chen", "Kyunghyun Cho", "Cem M. Deniz", "Narges Razavian"], "title": "Foundation Models for Clinical Records at Health System Scale", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025 Workshop on Foundation Models for Structured\n  Data", "summary": "Large-scale pretraining has transformed modeling of language and other data\ntypes, but its potential remains underexplored in healthcare with structured\nelectronic health records (EHRs). We present a novel generative pretraining\nstrategy for sequential EHR data using next-visit event prediction. Our model\nlearns to autoregressively generate various tokenized clinical events for the\nnext visit based on patient history and inherently handles the joint prediction\nof heterogeneous data types. Additionally, we introduce regularization on\npredicting repeated events and highlight a key pitfall in EHR-based foundation\nmodel evaluations: repeated event tokens can inflate performance metrics when\nnew onsets are not distinguished from subsequent occurrences. Our model is\nevaluated via zero-shot prediction for forecasting dementia and knee\nosteoarthritis incidence within 2 and 5 years, and the model performance rivals\na fully fine-tuned masked pretrained Transformer baseline, demonstrating that\nour approach captures complex clinical dependencies without requiring costly\ntask-specific fine-tuning.", "AI": {"tldr": "The paper introduces a generative pretraining strategy for EHR data using next-visit event prediction, addressing pitfalls in evaluation and demonstrating competitive performance in zero-shot prediction tasks.", "motivation": "To explore the potential of large-scale pretraining in healthcare, particularly for structured EHR data, which remains underexplored.", "method": "A novel generative pretraining strategy for sequential EHR data using next-visit event prediction, with regularization for repeated events.", "result": "The model rivals a fully fine-tuned baseline in zero-shot prediction tasks for dementia and knee osteoarthritis incidence.", "conclusion": "The approach captures complex clinical dependencies without costly task-specific fine-tuning, highlighting the promise of generative pretraining in healthcare."}}
{"id": "2501.01144", "pdf": "https://arxiv.org/pdf/2501.01144", "abs": "https://arxiv.org/abs/2501.01144", "authors": ["Wonsuk Jang", "Thierry Tambe"], "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference", "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.", "AI": {"tldr": "BlockDialect is a block-wise fine-grained mixed format technique for quantizing LLMs, using DialectFP4 for better data representation and energy efficiency.", "motivation": "Addressing memory and computational challenges in large language models (LLMs) by improving quantization methods for weights and activations.", "method": "Proposes BlockDialect for per-block optimal number format assignment and DialectFP4, a formatbook of FP4 variants. Introduces a two-stage approach for online DialectFP4 activation quantization.", "result": "Achieves 10.78% (7.48%) accuracy gain on LLaMA3-8B (LLaMA2-7B) compared to MXFP4, with lower bit usage and near-full precision performance.", "conclusion": "BlockDialect offers an energy-efficient solution for LLM inference by focusing on optimal data representation."}}
{"id": "2507.00608", "pdf": "https://arxiv.org/pdf/2507.00608", "abs": "https://arxiv.org/abs/2507.00608", "authors": ["Zehua Fu", "Chenguang Liu", "Yuyu Chen", "Jiaqi Zhou", "Qingjie Liu", "Yunhong Wang"], "title": "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  15 pages, 10 figures", "summary": "Despite its significant success, object detection in traffic and\ntransportation scenarios requires time-consuming and laborious efforts in\nacquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation\n(UDA) for object detection has recently gained increasing research attention.\nUDA for object detection has been dominated by domain alignment methods, which\nachieve top performance. Recently, self-labeling methods have gained popularity\ndue to their simplicity and efficiency. In this paper, we investigate the\nlimitations that prevent self-labeling detectors from achieving commensurate\nperformance with domain alignment methods. Specifically, we identify the high\nproportion of simple samples during training, i.e., the simple-label bias, as\nthe central cause. We propose a novel approach called De-Simplifying Pseudo\nLabels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level\nmemory bank to implement an innovative pseudo label updating strategy. Then,\nadversarial samples are introduced during training to enhance the proportion.\nFurthermore, we propose an adaptive weighted loss to avoid the model suffering\nfrom an abundance of false positive pseudo labels in the late training period.\nExperimental results demonstrate that DeSimPL effectively reduces the\nproportion of simple samples during training, leading to a significant\nperformance improvement for self-labeling detectors. Extensive experiments\nconducted on four benchmarks validate our analysis and conclusions.", "AI": {"tldr": "The paper addresses the performance gap between self-labeling and domain alignment methods in UDA for object detection, attributing it to simple-label bias, and proposes DeSimPL to mitigate this issue.", "motivation": "The need to reduce reliance on labeled data in traffic and transportation object detection, and the underperformance of self-labeling methods compared to domain alignment due to simple-label bias.", "method": "Proposes DeSimPL, which uses an instance-level memory bank for pseudo label updating, introduces adversarial samples, and employs an adaptive weighted loss to handle false positives.", "result": "DeSimPL reduces simple samples during training, improving self-labeling detector performance, validated on four benchmarks.", "conclusion": "DeSimPL effectively addresses simple-label bias, enhancing self-labeling methods in UDA for object detection."}}
{"id": "2507.00546", "pdf": "https://arxiv.org/pdf/2507.00546", "abs": "https://arxiv.org/abs/2507.00546", "authors": ["Reza Marzban", "Ali Adibi", "Raphael Pestourie"], "title": "Inverse Design in Nanophotonics via Representation Learning", "categories": ["physics.app-ph", "cs.AI", "cs.LG", "physics.optics"], "comment": null, "summary": "Inverse design in nanophotonics, the computational discovery of structures\nachieving targeted electromagnetic (EM) responses, has become a key tool for\nrecent optical advances. Traditional intuition-driven or iterative optimization\nmethods struggle with the inherently high-dimensional, non-convex design spaces\nand the substantial computational demands of EM simulations. Recently, machine\nlearning (ML) has emerged to address these bottlenecks effectively. This review\nframes ML-enhanced inverse design methodologies through the lens of\nrepresentation learning, classifying them into two categories: output-side and\ninput-side approaches. Output-side methods use ML to learn a representation in\nthe solution space to create a differentiable solver that accelerates\noptimization. Conversely, input-side techniques employ ML to learn compact,\nlatent-space representations of feasible device geometries, enabling efficient\nglobal exploration through generative models. Each strategy presents unique\ntrade-offs in data requirements, generalization capacity, and novel design\ndiscovery potentials. Hybrid frameworks that combine physics-based optimization\nwith data-driven representations help escape poor local optima, improve\nscalability, and facilitate knowledge transfer. We conclude by highlighting\nopen challenges and opportunities, emphasizing complexity management,\ngeometry-independent representations, integration of fabrication constraints,\nand advancements in multiphysics co-designs.", "AI": {"tldr": "The paper reviews machine learning (ML) approaches for inverse design in nanophotonics, categorizing them into output-side and input-side methods to address challenges in high-dimensional design spaces and computational demands.", "motivation": "Traditional intuition-driven or iterative optimization methods struggle with high-dimensional, non-convex design spaces and computational costs of EM simulations, prompting the need for ML-enhanced solutions.", "method": "The review classifies ML-enhanced inverse design into output-side (differentiable solver acceleration) and input-side (latent-space geometry representation) approaches, with hybrid frameworks combining physics-based optimization and data-driven methods.", "result": "ML methods improve optimization efficiency, scalability, and knowledge transfer, though trade-offs exist in data needs, generalization, and novel design discovery.", "conclusion": "Open challenges include managing complexity, geometry-independent representations, fabrication constraints, and advancing multiphysics co-designs."}}
{"id": "2507.00589", "pdf": "https://arxiv.org/pdf/2507.00589", "abs": "https://arxiv.org/abs/2507.00589", "authors": ["Seok Bin Son", "Joongheon Kim"], "title": "Quantum Circuit Structure Optimization for Quantum Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) enables agents to learn optimal policies through\nenvironmental interaction. However, RL suffers from reduced learning efficiency\ndue to the curse of dimensionality in high-dimensional spaces. Quantum\nreinforcement learning (QRL) addresses this issue by leveraging superposition\nand entanglement in quantum computing, allowing efficient handling of\nhigh-dimensional problems with fewer resources. QRL combines quantum neural\nnetworks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as\nthe core computational module. The PQC performs linear and nonlinear\ntransformations through gate operations, similar to hidden layers in classical\nneural networks. Previous QRL studies, however, have used fixed PQC structures\nbased on empirical intuition without verifying their optimality. This paper\nproposes a QRL-NAS algorithm that integrates quantum neural architecture search\n(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that\nQRL-NAS achieves higher rewards than QRL with fixed circuits, validating its\neffectiveness and practical utility.", "AI": {"tldr": "QRL-NAS algorithm optimizes PQC structures in quantum reinforcement learning, outperforming fixed-circuit QRL.", "motivation": "Traditional QRL uses fixed PQC structures without verifying optimality, limiting efficiency.", "method": "Integrates quantum neural architecture search (QNAS) to optimize PQC structures in QRL.", "result": "QRL-NAS achieves higher rewards than fixed-circuit QRL, proving its effectiveness.", "conclusion": "QRL-NAS enhances QRL by optimizing PQC structures, offering practical utility."}}
{"id": "2501.09310", "pdf": "https://arxiv.org/pdf/2501.09310", "abs": "https://arxiv.org/abs/2501.09310", "authors": ["Jiawei Shen", "Chengcheng Wan", "Ruoyi Qiao", "Jiazhen Zou", "Hang Xu", "Yuchen Shao", "Yueling Zhang", "Weikai Miao", "Geguang Pu"], "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.", "AI": {"tldr": "The paper studies text-to-SQL errors in LLMs, identifies 29 error types, evaluates existing repair methods, and introduces MapleRepair, a framework that improves correctness with lower overhead.", "motivation": "Address the widespread correctness issues in LLM-based text-to-SQL tasks and the inefficiency of existing repair solutions.", "method": "Comprehensive study of four ICL-based techniques, five repair methods, two benchmarks, and two LLM settings, leading to the development of MapleRepair.", "result": "Identified 29 error types; MapleRepair repairs 13.8% more queries with negligible mis-repairs and 67.4% less overhead.", "conclusion": "MapleRepair is an effective solution for text-to-SQL error detection and repair, outperforming existing methods."}}
{"id": "2507.00648", "pdf": "https://arxiv.org/pdf/2507.00648", "abs": "https://arxiv.org/abs/2507.00648", "authors": ["Siyuan Yao", "Rui Zhu", "Ziqi Wang", "Wenqi Ren", "Yanyang Yan", "Xiaochun Cao"], "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Visual object tracking has gained promising progress in past decades. Most of\nthe existing approaches focus on learning target representation in\nwell-conditioned daytime data, while for the unconstrained real-world scenarios\nwith adverse weather conditions, e.g. nighttime or foggy environment, the\ntremendous domain shift leads to significant performance degradation. In this\npaper, we propose UMDATrack, which is capable of maintaining high-quality\ntarget state prediction under various adverse weather conditions within a\nunified domain adaptation framework. Specifically, we first use a controllable\nscenario generator to synthesize a small amount of unlabeled videos (less than\n2% frames in source daytime datasets) in multiple weather conditions under the\nguidance of different text prompts. Afterwards, we design a simple yet\neffective domain-customized adapter (DCA), allowing the target objects'\nrepresentation to rapidly adapt to various weather conditions without redundant\nmodel updating. Furthermore, to enhance the localization consistency between\nsource and target domains, we propose a target-aware confidence alignment\nmodule (TCA) following optimal transport theorem. Extensive experiments\ndemonstrate that UMDATrack can surpass existing advanced visual trackers and\nlead new state-of-the-art performance by a significant margin. Our code is\navailable at https://github.com/Z-Z188/UMDATrack.", "AI": {"tldr": "UMDATrack improves visual object tracking in adverse weather conditions using domain adaptation, a scenario generator, and custom adapters.", "motivation": "Existing tracking methods degrade in adverse weather due to domain shifts. UMDATrack aims to maintain performance in such conditions.", "method": "Uses a controllable scenario generator for synthetic videos, a domain-customized adapter (DCA), and a target-aware confidence alignment module (TCA).", "result": "Outperforms existing trackers significantly in adverse weather.", "conclusion": "UMDATrack sets a new state-of-the-art for robust visual tracking in diverse weather conditions."}}
{"id": "2507.00577", "pdf": "https://arxiv.org/pdf/2507.00577", "abs": "https://arxiv.org/abs/2507.00577", "authors": ["Yinghao Wu", "Liyan Zhang"], "title": "BadViM: Backdoor Attack against Vision Mamba", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks.", "AI": {"tldr": "BadViM introduces a stealthy backdoor attack framework for Vision Mamba (ViM), exploiting frequency sensitivity and hidden state alignment to achieve high attack success while evading defenses.", "motivation": "To explore the vulnerability of Vision Mamba (ViM) to backdoor attacks, a critical gap in security research for this emerging architecture.", "method": "BadViM uses a Resonant Frequency Trigger (RFT) and Hidden State Alignment loss to manipulate model behavior without compromising clean data accuracy.", "result": "BadViM achieves high attack success rates and resists common defenses like PatchDrop and JPEG compression.", "conclusion": "Vision Mamba is vulnerable to sophisticated backdoor attacks, highlighting the need for robust security measures in SSM-based architectures."}}
{"id": "2507.00611", "pdf": "https://arxiv.org/pdf/2507.00611", "abs": "https://arxiv.org/abs/2507.00611", "authors": ["Chenyang Cao", "Miguel Rogel-Garc\u00eda", "Mohamed Nabail", "Xueqian Wang", "Nicholas Rhinehart"], "title": "Residual Reward Models for Preference-based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "26 pages, 22 figures", "summary": "Preference-based Reinforcement Learning (PbRL) provides a way to learn\nhigh-performance policies in environments where the reward signal is hard to\nspecify, avoiding heuristic and time-consuming reward design. However, PbRL can\nsuffer from slow convergence speed since it requires training in a reward\nmodel. Prior work has proposed learning a reward model from demonstrations and\nfine-tuning it using preferences. However, when the model is a neural network,\nusing different loss functions for pre-training and fine-tuning can pose\nchallenges to reliable optimization. In this paper, we propose a method to\neffectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM\nassumes that the true reward of the environment can be split into a sum of two\nparts: a prior reward and a learned reward. The prior reward is a term\navailable before training, for example, a user's ``best guess'' reward\nfunction, or a reward function learned from inverse reinforcement learning\n(IRL), and the learned reward is trained with preferences. We introduce\nstate-based and image-based versions of RRM and evaluate them on several tasks\nin the Meta-World environment suite. Experimental results show that our method\nsubstantially improves the performance of a common PbRL method. Our method\nachieves performance improvements for a variety of different types of prior\nrewards, including proxy rewards, a reward obtained from IRL, and even a\nnegated version of the proxy reward. We also conduct experiments with a Franka\nPanda to show that our method leads to superior performance on a real robot. It\nsignificantly accelerates policy learning for different tasks, achieving\nsuccess in fewer steps than the baseline. The videos are presented at\nhttps://sunlighted.github.io/RRM-web/.", "AI": {"tldr": "The paper proposes a Residual Reward Model (RRM) to improve Preference-based Reinforcement Learning (PbRL) by combining prior knowledge with learned rewards, enhancing performance and convergence speed.", "motivation": "PbRL suffers from slow convergence due to reward model training challenges. Existing methods struggle with optimization when using different loss functions for pre-training and fine-tuning.", "method": "The RRM splits the true reward into a prior reward (pre-defined or from IRL) and a learned reward (trained with preferences). State-based and image-based RRM versions are introduced and tested on Meta-World tasks and a real robot.", "result": "RRM substantially improves PbRL performance, works with various prior rewards, and accelerates policy learning, achieving success in fewer steps than baselines.", "conclusion": "RRM effectively leverages prior knowledge to enhance PbRL, demonstrating superior performance in simulated and real-world tasks."}}
{"id": "2502.14051", "pdf": "https://arxiv.org/pdf/2502.14051", "abs": "https://arxiv.org/abs/2502.14051", "authors": ["Payman Behnam", "Yaosheng Fu", "Ritchie Zhao", "Po-An Tsai", "Zhiding Yu", "Alexey Tumanov"], "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.", "AI": {"tldr": "RocketKV is a training-free KV cache compression method for Transformer-based models, achieving high compression ratios and speedups with minimal accuracy loss.", "motivation": "The KV cache in large language models grows with input length, straining memory and bandwidth. RocketKV aims to mitigate this.", "method": "RocketKV uses two stages: coarse-grain permanent KV cache eviction and fine-grain top-k sparse attention with dimensionality reduction.", "result": "Achieves up to 400x compression, 3.7x speedup, and 32.6% memory reduction with negligible accuracy loss.", "conclusion": "RocketKV effectively optimizes KV cache usage, outperforming existing methods in multi-turn scenarios."}}
{"id": "2507.00659", "pdf": "https://arxiv.org/pdf/2507.00659", "abs": "https://arxiv.org/abs/2507.00659", "authors": ["Juelin Zhu", "Shuaibang Peng", "Long Wang", "Hanlin Tan", "Yu Liu", "Maojun Zhang", "Shen Yan"], "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "We propose a novel method for aerial visual localization over low\nLevel-of-Detail (LoD) city models. Previous wireframe-alignment-based method\nLoD-Loc has shown promising localization results leveraging LoD models.\nHowever, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the\nmajority of available models and those many countries plan to construct\nnationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD\ncity models could unlock drones' potential for global urban localization. To\naddress these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine\nstrategy using explicit silhouette alignment to achieve accurate localization\nover low-LoD city models in the air. Specifically, given a query image, LoD-Loc\nv2 first applies a building segmentation network to shape building silhouettes.\nThen, in the coarse pose selection stage, we construct a pose cost volume by\nuniformly sampling pose hypotheses around a prior pose to represent the pose\nprobability distribution. Each cost of the volume measures the degree of\nalignment between the projected and predicted silhouettes. We select the pose\nwith maximum value as the coarse pose. In the fine pose estimation stage, a\nparticle filtering method incorporating a multi-beam tracking approach is used\nto efficiently explore the hypothesis space and obtain the final pose\nestimation. To further facilitate research in this field, we release two\ndatasets with LoD1 city models covering 10.7 km , along with real RGB queries\nand ground-truth pose annotations. Experimental results show that LoD-Loc v2\nimproves estimation accuracy with high-LoD models and enables localization with\nlow-LoD models for the first time. Moreover, it outperforms state-of-the-art\nbaselines by large margins, even surpassing texture-model-based methods, and\nbroadens the convergence basin to accommodate larger prior errors.", "AI": {"tldr": "LoD-Loc v2 introduces a coarse-to-fine strategy for aerial visual localization over low-LoD city models, improving accuracy and enabling localization where previous methods failed.", "motivation": "Existing methods rely on high-LoD city models, but low-LoD models are more common and planned for nationwide use. Enabling localization with low-LoD models could expand drone applications in urban areas.", "method": "LoD-Loc v2 uses building segmentation, a pose cost volume for coarse pose selection, and particle filtering for fine pose estimation.", "result": "The method improves accuracy with high-LoD models and enables localization with low-LoD models, outperforming state-of-the-art baselines.", "conclusion": "LoD-Loc v2 advances aerial localization by supporting low-LoD models, broadening convergence, and outperforming existing methods."}}
{"id": "2507.00598", "pdf": "https://arxiv.org/pdf/2507.00598", "abs": "https://arxiv.org/abs/2507.00598", "authors": ["Madison Cotteret", "Christopher J. Kymn", "Hugh Greatorex", "Martin Ziegler", "Elisabetta Chicca", "Friedrich T. Sommer"], "title": "High-resolution spatial memory requires grid-cell-like neural codes", "categories": ["cs.NE", "cs.AI", "cs.SC"], "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures", "summary": "Continuous attractor networks (CANs) are widely used to model how the brain\ntemporarily retains continuous behavioural variables via persistent recurrent\nactivity, such as an animal's position in an environment. However, this memory\nmechanism is very sensitive to even small imperfections, such as noise or\nheterogeneity, which are both common in biological systems. Previous work has\nshown that discretising the continuum into a finite set of discrete attractor\nstates provides robustness to these imperfections, but necessarily reduces the\nresolution of the represented variable, creating a dilemma between stability\nand resolution. We show that this stability-resolution dilemma is most severe\nfor CANs using unimodal bump-like codes, as in traditional models. To overcome\nthis, we investigate sparse binary distributed codes based on random feature\nembeddings, in which neurons have spatially-periodic receptive fields. We\ndemonstrate theoretically and with simulations that such grid-cell-like codes\nenable CANs to achieve both high stability and high resolution simultaneously.\nThe model extends to embedding arbitrary nonlinear manifolds into a CAN, such\nas spheres or tori, and generalises linear path integration to integration\nalong freely-programmable on-manifold vector fields. Together, this work\nprovides a theory of how the brain could robustly represent continuous\nvariables with high resolution and perform flexible computations over\ntask-relevant manifolds.", "AI": {"tldr": "The paper explores how sparse binary distributed codes, inspired by grid-cell-like patterns, can enhance the stability and resolution of continuous attractor networks (CANs) in representing continuous variables, overcoming the traditional stability-resolution dilemma.", "motivation": "The brain's ability to retain continuous variables via CANs is hindered by sensitivity to noise and heterogeneity. Traditional models face a trade-off between stability and resolution.", "method": "The study investigates sparse binary distributed codes with spatially-periodic receptive fields, similar to grid cells, to improve CAN performance. Theoretical analysis and simulations are used.", "result": "Grid-cell-like codes allow CANs to achieve high stability and resolution simultaneously, even for nonlinear manifolds like spheres or tori.", "conclusion": "This work offers a theoretical framework for robust high-resolution representation of continuous variables in the brain and flexible computations on task-relevant manifolds."}}
{"id": "2507.00647", "pdf": "https://arxiv.org/pdf/2507.00647", "abs": "https://arxiv.org/abs/2507.00647", "authors": ["Andr\u00e9 Ribeiro", "Ana Luiza Ten\u00f3rio", "Juan Belieni", "Amauri H. Souza", "Diego Mesquita"], "title": "Cooperative Sheaf Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Sheaf diffusion has recently emerged as a promising design pattern for graph\nrepresentation learning due to its inherent ability to handle heterophilic data\nand avoid oversmoothing. Meanwhile, cooperative message passing has also been\nproposed as a way to enhance the flexibility of information diffusion by\nallowing nodes to independently choose whether to propagate/gather information\nfrom/to neighbors. A natural question ensues: is sheaf diffusion capable of\nexhibiting this cooperative behavior? Here, we provide a negative answer to\nthis question. In particular, we show that existing sheaf diffusion methods\nfail to achieve cooperative behavior due to the lack of message directionality.\nTo circumvent this limitation, we introduce the notion of cellular sheaves over\ndirected graphs and characterize their in- and out-degree Laplacians. We\nleverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).\nTheoretically, we characterize the receptive field of CSNN and show it allows\nnodes to selectively attend (listen) to arbitrarily far nodes while ignoring\nall others in their path, potentially mitigating oversquashing. Our experiments\nshow that CSNN presents overall better performance compared to prior art on\nsheaf diffusion as well as cooperative graph neural networks.", "AI": {"tldr": "Sheaf diffusion lacks cooperative behavior due to missing message directionality. The paper introduces directed cellular sheaves and proposes Cooperative Sheaf Neural Networks (CSNNs), which outperform existing methods.", "motivation": "To address the limitation of sheaf diffusion in achieving cooperative behavior, which is crucial for flexible information diffusion in graph representation learning.", "method": "Introduces cellular sheaves over directed graphs, characterizes their Laplacians, and proposes CSNNs to enable cooperative behavior.", "result": "CSNNs outperform prior methods in sheaf diffusion and cooperative graph neural networks, with theoretical support for selective attention and mitigation of oversquashing.", "conclusion": "Directed cellular sheaves and CSNNs successfully enable cooperative behavior in sheaf diffusion, improving performance and addressing key limitations."}}
{"id": "2503.03040", "pdf": "https://arxiv.org/pdf/2503.03040", "abs": "https://arxiv.org/abs/2503.03040", "authors": ["Yizhe Zhang", "Navdeep Jaitly"], "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages main text", "summary": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen", "AI": {"tldr": "SAGE introduces latent variables (State-Action Chain) to control emotional states and strategies in chatbots, improving emotional intelligence while maintaining LLM performance.", "motivation": "Building emotionally intelligent chatbots capable of natural, strategic conversations remains a challenge despite advances in large language models.", "method": "Uses latent variables (SAC) to augment fine-tuning, enabling control over dialogue progression. Includes a self-improvement pipeline with tree search, reward modeling, and fine-tuning.", "result": "Improved emotional intelligence metrics while maintaining strong LLM benchmark performance.", "conclusion": "SAGE's discrete latent variables enable search-based strategies and future RL applications for dialogue systems."}}
{"id": "2507.00676", "pdf": "https://arxiv.org/pdf/2507.00676", "abs": "https://arxiv.org/abs/2507.00676", "authors": ["Edward Effendy", "Kuan-Wei Tseng", "Rei Kawakami"], "title": "A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Accepted in the ICIP 2025\n  We present a novel transformer-based framework for whole-body grasping that\naddresses both pose generation and motion infilling, enabling realistic and\nstable object interactions. Our pipeline comprises three stages: Grasp Pose\nGeneration for full-body grasp generation, Temporal Infilling for smooth motion\ncontinuity, and a LiftUp Transformer that refines downsampled joints back to\nhigh-resolution markers. To overcome the scarcity of hand-object interaction\ndata, we introduce a data-efficient Generalized Pretraining stage on large,\ndiverse motion datasets, yielding robust spatio-temporal representations\ntransferable to grasping tasks. Experiments on the GRAB dataset show that our\nmethod outperforms state-of-the-art baselines in terms of coherence, stability,\nand visual realism. The modular design also supports easy adaptation to other\nhuman-motion applications.", "AI": {"tldr": "A transformer-based framework for whole-body grasping, combining pose generation and motion infilling, outperforms baselines in coherence, stability, and realism.", "motivation": "Addressing the challenges of realistic and stable whole-body grasping, especially due to limited hand-object interaction data.", "method": "Three-stage pipeline: Grasp Pose Generation, Temporal Infilling, and LiftUp Transformer. Includes data-efficient Generalized Pretraining for robust representations.", "result": "Outperforms state-of-the-art baselines on the GRAB dataset in coherence, stability, and visual realism.", "conclusion": "The modular framework is effective for whole-body grasping and adaptable to other human-motion tasks."}}
{"id": "2507.00653", "pdf": "https://arxiv.org/pdf/2507.00653", "abs": "https://arxiv.org/abs/2507.00653", "authors": ["Yilun Zhang"], "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "The escalating computational costs of Large Language Model (LLM) inference\nhave become a critical barrier to their widespread and sustainable deployment.\nWhile existing optimization strategies are effective, they are predominantly\nbased on statistical heuristics or architectural modifications, lacking a\nguiding cognitive theory to manage the inference process itself. This paper\naims to bridge this gap by introducing a novel paradigm: the Cognitive\nLoad-Aware Inference (CLAI) framework, which operationalizes principles from\nCognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize\nthe concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and\nGermane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,\nand $GCL_{LLM}$), thereby reframing the inference process as a cognitive\neconomics optimization problem: based on the intrinsic complexity of a problem\n($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically\nallocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two\nimplementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM\nthrough cognitive control steps via a structured meta-prompt, and CLAI-Tune, a\nfine-tuned model that internalizes these principles for spontaneous cognitive\neconomy. Across a range of benchmarks in complex reasoning, long-context\nquestion answering, and code generation, our methods achieve significant\nreductions in token consumption (up to 45\\%) without sacrificing accuracy.\nFurthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose\ndifficult problems, a key characteristic of human expert cognition. This work\ndemonstrates that by emulating the brain's resource management strategies, we\ncan build more efficient, robust, and capable artificial intelligence systems.", "AI": {"tldr": "The paper introduces the Cognitive Load-Aware Inference (CLAI) framework, applying Cognitive Load Theory to optimize LLM inference, reducing token usage by up to 45% without accuracy loss.", "motivation": "Address the high computational costs of LLM inference by leveraging cognitive theory for optimization, moving beyond heuristic-based methods.", "method": "Develops CLAI framework with quantifiable metrics ($ICL_{LLM}$, $ECL_{LLM}$, $GCL_{LLM}$) and two implementations: CLAI-Prompt (zero-shot) and CLAI-Tune (fine-tuned).", "result": "Achieves up to 45% reduction in token consumption while maintaining accuracy; CLAI-Tune shows emergent problem decomposition skills.", "conclusion": "Emulating human cognitive strategies can enhance LLM efficiency and capability, offering a sustainable path for AI deployment."}}
{"id": "2507.00651", "pdf": "https://arxiv.org/pdf/2507.00651", "abs": "https://arxiv.org/abs/2507.00651", "authors": ["Maurizio Filippone", "Marius P. Linhard"], "title": "GANs Secretly Perform Approximate Bayesian Model Selection", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "AI": {"tldr": "The paper interprets GANs as probabilistic generative models, linking them to Bayesian neural networks, and proposes regularization strategies based on marginal likelihood optimization for improved performance.", "motivation": "To address the challenges of GAN optimization and overfitting by reinterpreting GANs as probabilistic models and leveraging Bayesian principles.", "method": "Interprets GANs as Bayesian neural networks with partial stochasticity, establishes universal approximation conditions, and optimizes marginal likelihood proxies for regularization.", "result": "Proposed strategies improve GAN performance and provide insights into regularization techniques.", "conclusion": "The probabilistic interpretation and Bayesian-inspired strategies enhance GAN optimization and generalization, offering a deeper understanding of regularization."}}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044", "abs": "https://arxiv.org/abs/2503.15044", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "title": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "ACL LLMSEC", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.", "AI": {"tldr": "SPADE is a framework for detecting synthetic dialogues using prompt-based samples, improving MGT detection with new datasets and better generalization.", "motivation": "Addressing the lack of high-quality synthetic datasets for training MGT detection models due to misuse concerns of LLMs.", "method": "Proposes SPADE, a structured framework using prompt-based positive and negative samples, creating 14 new dialogue datasets and benchmarking against eight MGT detection models.", "result": "Improved generalization performance with mixed datasets and practical enhancement of LLM security. Examines chat history length's impact on detection accuracy.", "conclusion": "SPADE offers a robust solution for synthetic dialogue detection, with open-source resources for further research."}}
{"id": "2507.00690", "pdf": "https://arxiv.org/pdf/2507.00690", "abs": "https://arxiv.org/abs/2507.00690", "authors": ["Keke Tang", "Ziyong Du", "Weilong Peng", "Xiaofei Wang", "Peican Zhu", "Ligang Liu", "Zhihong Tian"], "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance.", "AI": {"tldr": "CageAttack introduces a cage-based deformation framework for generating natural adversarial point clouds, balancing transferability, undefendability, and plausibility.", "motivation": "Existing adversarial attacks on point clouds either limit transferability or introduce unnatural distortions, undermining plausibility.", "method": "The framework constructs a cage around the target object, applies perturbations to cage vertices, and propagates these smoothly to the point cloud.", "result": "CageAttack outperforms state-of-the-art methods on seven 3D classifiers across three datasets.", "conclusion": "The proposed method achieves superior balance in adversarial point cloud generation, ensuring natural deformations and high plausibility."}}
{"id": "2507.00657", "pdf": "https://arxiv.org/pdf/2507.00657", "abs": "https://arxiv.org/abs/2507.00657", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.", "AI": {"tldr": "LLMs simulate political discourse on social media, revealing biases and polarization when contextualized, challenging their reliability for social applications.", "motivation": "To understand how LLMs behave in simulating political discourse and their potential biases when used as social proxies.", "method": "Constructed LLM agents based on real users' interactions during the 2024 U.S. election, comparing human replies under controlled conditions with different initialization methods (Zero Shot and Few Shot).", "result": "Richer contextualization improves consistency but increases polarization and toxicity, with LLMs amplifying traits beyond empirical baselines.", "conclusion": "LLMs reconstruct rather than emulate users, introducing structural biases that undermine their reliability for social applications like content moderation and policy modeling."}}
{"id": "2507.00654", "pdf": "https://arxiv.org/pdf/2507.00654", "abs": "https://arxiv.org/abs/2507.00654", "authors": ["Hans van Gorp", "Davide Belli", "Amir Jalalirad", "Bence Major"], "title": "Neural Augmented Kalman Filters for Road Network assisted GNSS positioning", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": "Accepted to ICML 2025 workshop ML4Wireless", "summary": "The Global Navigation Satellite System (GNSS) provides critical positioning\ninformation globally, but its accuracy in dense urban environments is often\ncompromised by multipath and non-line-of-sight errors. Road network data can be\nused to reduce the impact of these errors and enhance the accuracy of a\npositioning system. Previous works employing road network data are either\nlimited to offline applications, or rely on Kalman Filter (KF) heuristics with\nlittle flexibility and robustness. We instead propose training a Temporal Graph\nNeural Network (TGNN) to integrate road network information into a KF. The TGNN\nis designed to predict the correct road segment and its associated uncertainty\nto be used in the measurement update step of the KF. We validate our approach\nwith real-world GNSS data and open-source road networks, observing a 29%\ndecrease in positioning error for challenging scenarios compared to a GNSS-only\nKF. To the best of our knowledge, ours is the first deep learning-based\napproach jointly employing road network data and GNSS measurements to determine\nthe user position on Earth.", "AI": {"tldr": "A Temporal Graph Neural Network (TGNN) is proposed to integrate road network data with GNSS measurements, reducing positioning errors by 29% in dense urban areas.", "motivation": "GNSS accuracy is compromised in dense urban environments due to multipath and non-line-of-sight errors. Road network data can mitigate these issues, but existing methods lack flexibility and robustness.", "method": "A TGNN is trained to predict the correct road segment and its uncertainty, integrating this into a Kalman Filter (KF) for improved positioning.", "result": "The approach reduces positioning error by 29% in challenging scenarios compared to GNSS-only KF.", "conclusion": "This is the first deep learning-based method combining road network data and GNSS measurements for accurate user positioning."}}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248", "abs": "https://arxiv.org/abs/2503.21248", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.", "AI": {"tldr": "The paper introduces a benchmark to evaluate LLMs' ability in scientific hypothesis discovery, covering inspiration retrieval, hypothesis composition, and ranking. It uses a 2024-only dataset to avoid contamination, showing LLMs excel in inspiration retrieval.", "motivation": "To assess LLMs' potential in generating high-quality research hypotheses, a capability not yet benchmarked, by creating a dedicated evaluation framework.", "method": "Developed an automated framework extracting key components (e.g., research questions, hypotheses) from 2024 scientific papers across 12 disciplines, validated by experts.", "result": "LLMs perform well in inspiration retrieval, an out-of-distribution task, indicating their ability to uncover novel knowledge associations.", "conclusion": "LLMs can act as 'research hypothesis mines,' enabling scalable, automated scientific discovery with minimal human input."}}
{"id": "2507.00698", "pdf": "https://arxiv.org/pdf/2507.00698", "abs": "https://arxiv.org/abs/2507.00698", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "title": "Rectifying Magnitude Neglect in Linear Attention", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "AI": {"tldr": "The paper introduces Magnitude-Aware Linear Attention (MALA) to address the performance gap between Linear Attention and Softmax Attention by incorporating Query magnitude, achieving efficient global modeling with linear complexity.", "motivation": "Linear Attention's linear complexity is advantageous for vision tasks but suffers performance degradation compared to Softmax Attention due to ignoring Query magnitude.", "method": "Analyzes Linear Attention's formulation, identifies the Query magnitude issue, and proposes MALA to dynamically adapt attention scores by incorporating Query magnitude.", "result": "MALA achieves strong performance across tasks like image classification, object detection, NLP, and more, closely resembling Softmax Attention's distribution.", "conclusion": "MALA effectively bridges the performance gap between Linear and Softmax Attention while maintaining linear complexity, validated across diverse tasks."}}
{"id": "2507.00669", "pdf": "https://arxiv.org/pdf/2507.00669", "abs": "https://arxiv.org/abs/2507.00669", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Work in progress, 42 pages", "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "AI": {"tldr": "Audio-3DVG integrates audio and spatial info for 3D visual grounding, outperforming prior methods by decomposing the task into object mention detection and audio-guided attention.", "motivation": "Prior work lacks exploration of spoken language in 3DVG. Advances in ASR and speech representation learning motivate leveraging audio for enhanced grounding.", "method": "Decomposes task into Object Mention Detection (multi-label classification) and Audio-Guided Attention (captures object-speech interactions). Uses synthesized audio from standard datasets.", "result": "Achieves state-of-the-art in audio-based grounding and competes with text-based methods.", "conclusion": "Audio-3DVG shows promise for integrating spoken language into 3D vision tasks."}}
{"id": "2507.00687", "pdf": "https://arxiv.org/pdf/2507.00687", "abs": "https://arxiv.org/abs/2507.00687", "authors": ["Philipp Vaeth", "Dibyanshu Kumar", "Benjamin Paassen", "Magda Gregorov\u00e1"], "title": "Diffusion Classifier Guidance for Non-robust Classifiers", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ECML 2025", "summary": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers.", "AI": {"tldr": "Extending classifier guidance to non-robust classifiers by addressing noise sensitivity and stabilizing gradients, improving conditional sampling in generative models.", "motivation": "Current classifier guidance methods are limited to robust classifiers trained on diffusion noise, restricting their applicability. This work aims to enable general classifiers for guidance.", "method": "Analyzes noise sensitivity of classifiers, proposes using one-step denoised predictions and stabilization techniques (e.g., exponential moving averages) to improve guidance stability.", "result": "Non-robust classifiers degrade under noise, but the proposed method stabilizes guidance while preserving sample diversity and visual quality.", "conclusion": "The approach broadens the use of classifiers in guidance, advancing conditional sampling techniques in generative models."}}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393", "abs": "https://arxiv.org/abs/2503.21393", "authors": ["Rohitash Chandra", "Aryan Chaudhari", "Yeshwanth Rayavarapu"], "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models.", "AI": {"tldr": "The study evaluates LLMs (Gemini, GPT, Google Translate) for translating Indian languages (Sanskrit, Telugu, Hindi) into English, comparing them to expert translations. GPT outperforms in sentiment and semantic preservation, though challenges remain in complex contexts like the Bhagavad Gita.", "motivation": "To assess the quality of LLM translations for Indian languages, focusing on semantic and sentiment accuracy compared to expert translations.", "method": "Semantic and sentiment analysis of LLM-generated translations (Bhagavad Gita, Tamas, Maha Prasthanam) compared to expert translations.", "result": "GPT models excel in sentiment and semantic preservation, outperforming Google Translate, but struggle with metaphorical/philosophical contexts.", "conclusion": "GPT shows promise for culturally sensitive translations, but further refinement is needed for complex texts."}}
{"id": "2507.00707", "pdf": "https://arxiv.org/pdf/2507.00707", "abs": "https://arxiv.org/abs/2507.00707", "authors": ["Zeming Chen", "Hang Zhao"], "title": "BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view image generation in autonomous driving demands consistent 3D scene\nunderstanding across camera views. Most existing methods treat this problem as\na 2D image set generation task, lacking explicit 3D modeling. However, we argue\nthat a structured representation is crucial for scene generation, especially\nfor autonomous driving applications. This paper proposes BEV-VAE for consistent\nand controllable view synthesis. BEV-VAE first trains a multi-view image\nvariational autoencoder for a compact and unified BEV latent space and then\ngenerates the scene with a latent diffusion transformer. BEV-VAE supports\narbitrary view generation given camera configurations, and optionally 3D\nlayouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance\nin both 3D consistent reconstruction and generation. The code is available at:\nhttps://github.com/Czm369/bev-vae.", "AI": {"tldr": "BEV-VAE proposes a method for multi-view image generation in autonomous driving using a structured 3D representation, combining a multi-view image VAE and a latent diffusion transformer for consistent and controllable synthesis.", "motivation": "Existing methods treat multi-view image generation as a 2D task, lacking explicit 3D modeling, which is crucial for autonomous driving applications.", "method": "BEV-VAE trains a multi-view image variational autoencoder (VAE) for a compact BEV latent space and uses a latent diffusion transformer for scene generation, supporting arbitrary views and optional 3D layouts.", "result": "Experiments on nuScenes and Argoverse 2 show strong performance in 3D consistent reconstruction and generation.", "conclusion": "BEV-VAE provides a structured and effective approach for multi-view image generation in autonomous driving, with code publicly available."}}
{"id": "2507.00709", "pdf": "https://arxiv.org/pdf/2507.00709", "abs": "https://arxiv.org/abs/2507.00709", "authors": ["Yiming Yang", "Yueru Luo", "Bingkun He", "Hongbin Lin", "Suzhong Fu", "Chao Yan", "Kun Tang", "Xinrui Yan", "Chao Zheng", "Shuguang Cui", "Zhen Li"], "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.4%\nmAP in lane segment perception and +2.1% OLS in centerline perception tasks.", "AI": {"tldr": "TopoStreamer improves lane segment topology reasoning with streaming attribute constraints, dynamic positional encoding, and denoising, outperforming existing methods by +3.4% mAP and +2.1% OLS.", "motivation": "Existing methods lack consistent positional embedding and temporal attribute learning, hindering accurate road network reconstruction for autonomous driving.", "method": "TopoStreamer introduces streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising.", "result": "Achieves +3.4% mAP in lane segment perception and +2.1% OLS in centerline perception on OpenLane-V2.", "conclusion": "TopoStreamer enhances lane topology reasoning, benefiting autonomous driving maneuvers like turning and lane changing."}}
{"id": "2507.00695", "pdf": "https://arxiv.org/pdf/2507.00695", "abs": "https://arxiv.org/abs/2507.00695", "authors": ["Daniel Pfrommer", "Max Simchowitz", "Ali Jadbabaie"], "title": "A Test-Function Approach to Incremental Stability", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages", "summary": "This paper presents a novel framework for analyzing\nIncremental-Input-to-State Stability ($\\delta$ISS) based on the idea of using\nrewards as \"test functions.\" Whereas control theory traditionally deals with\nLyapunov functions that satisfy a time-decrease condition, reinforcement\nlearning (RL) value functions are constructed by exponentially decaying a\nLipschitz reward function that may be non-smooth and unbounded on both sides.\nThus, these RL-style value functions cannot be directly understood as Lyapunov\ncertificates. We develop a new equivalence between a variant of incremental\ninput-to-state stability of a closed-loop system under given a policy, and the\nregularity of RL-style value functions under adversarial selection of a\nH\\\"older-continuous reward function. This result highlights that the regularity\nof value functions, and their connection to incremental stability, can be\nunderstood in a way that is distinct from the traditional Lyapunov-based\napproach to certifying stability in control theory.", "AI": {"tldr": "The paper introduces a framework linking RL-style value functions to incremental stability, distinct from traditional Lyapunov methods.", "motivation": "To bridge the gap between RL value functions and control theory stability analysis, which traditionally relies on Lyapunov functions.", "method": "Develops an equivalence between incremental input-to-state stability (\u03b4ISS) and the regularity of RL-style value functions using H\u00f6lder-continuous rewards.", "result": "Shows RL-style value functions can certify stability without relying on Lyapunov functions.", "conclusion": "The regularity of RL value functions offers a new perspective on stability, diverging from classical Lyapunov-based approaches."}}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": "accepted to TSD 2025", "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g., masked\nlanguage modeling (MLM), when common domain adaptation via LM fine-tuning is\nnot possible due to a lack of labeled task data. Although popular, MLM requires\na significant corpus of domain-related data, which is difficult to obtain for\nspecific domains in languages other than English, such as the process industry\nin the German language. This paper introduces an efficient approach called\nICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL)\nand k-nearest neighbors (kNN) to augment target data with domain-related and\nin-domain texts, significantly reducing GPU time while maintaining strong model\nperformance. Our results show that the best configuration of ICL-APT performed\nbetter than the state-of-the-art DAPT by 28.7% (7.87 points) and requires\nalmost 4 times less GPU-computing time, providing a cost-effective solution for\nindustries with limited computational capacity. The findings highlight the\nbroader applicability of this framework to other low-resource industries,\nmaking NLP-based solutions more accessible and feasible in production\nenvironments.", "AI": {"tldr": "ICL-APT is an efficient domain-adaptive pretraining method using in-context learning and kNN, outperforming DAPT by 28.7% with 4x less GPU time.", "motivation": "Addresses the challenge of limited labeled task data and domain-related corpora in non-English languages, like German, for NLP applications.", "method": "Uses in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data, reducing GPU time while maintaining performance.", "result": "ICL-APT outperforms DAPT by 28.7% (7.87 points) and requires 4x less GPU time.", "conclusion": "Provides a cost-effective solution for low-resource industries, enhancing accessibility of NLP solutions in production environments."}}
{"id": "2507.00721", "pdf": "https://arxiv.org/pdf/2507.00721", "abs": "https://arxiv.org/abs/2507.00721", "authors": ["Xiao Zhang", "Fei Wei", "Yong Wang", "Wenda Zhao", "Feiyi Li", "Xiangxiang Chu"], "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the\nlack of images in the target domain. Previous approaches leverage\nVision-Language Models (VLMs) to tackle this challenge, exploiting their\nzero-shot learning capabilities. However, these methods primarily address\ndomain distribution shifts and overlook the misalignment between the detection\ntask and VLMs, which rely on manually crafted prompts. To overcome these\nlimitations, we propose the unified prompt and representation enhancement\n(UPRE) framework, which jointly optimizes both textual prompts and visual\nrepresentations. Specifically, our approach introduces a multi-view domain\nprompt that combines linguistic domain priors with detection-specific\nknowledge, and a visual representation enhancement module that produces domain\nstyle variations. Furthermore, we introduce multi-level enhancement strategies,\nincluding relative domain distance and positive-negative separation, which\nalign multi-modal representations at the image level and capture diverse visual\nrepresentations at the instance level, respectively. Extensive experiments\nconducted on nine benchmark datasets demonstrate the superior performance of\nour framework in ZSDA detection scenarios. Code is available at\nhttps://github.com/AMAP-ML/UPRE.", "AI": {"tldr": "The paper proposes UPRE, a framework for zero-shot domain adaptation (ZSDA) that jointly optimizes prompts and visual representations to address misalignment between detection tasks and Vision-Language Models (VLMs).", "motivation": "Existing ZSDA methods using VLMs overlook misalignment between detection tasks and VLMs, relying on manual prompts. UPRE aims to overcome this by enhancing both textual prompts and visual representations.", "method": "UPRE introduces a multi-view domain prompt combining linguistic priors with detection knowledge, and a visual enhancement module for domain style variations. Multi-level strategies align representations at image and instance levels.", "result": "Experiments on nine datasets show UPRE's superior performance in ZSDA detection.", "conclusion": "UPRE effectively addresses ZSDA challenges by optimizing prompts and visual representations, outperforming prior methods."}}
{"id": "2507.00724", "pdf": "https://arxiv.org/pdf/2507.00724", "abs": "https://arxiv.org/abs/2507.00724", "authors": ["Linghui Zhu", "Yiming Li", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Shu-Tao Xia", "Zhi Wang"], "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision models achieve remarkable performance in various downstream\ntasks, primarily by personalizing pre-trained models through fine-tuning with\nprivate and valuable local data, which makes the personalized model a valuable\nintellectual property for its owner. Similar to the era of traditional DNNs,\nmodel stealing attacks also pose significant risks to these personalized\nmodels. However, in this paper, we reveal that most existing defense methods\n(developed for traditional DNNs), typically designed for models trained from\nscratch, either introduce additional security risks, are prone to misjudgment,\nor are even ineffective for fine-tuned models. To alleviate these problems,\nthis paper proposes a harmless model ownership verification method for\npersonalized models by decoupling similar common features. In general, our\nmethod consists of three main stages. In the first stage, we create shadow\nmodels that retain common features of the victim model while disrupting\ndataset-specific features. We represent the dataset-specific features of the\nvictim model by the output differences between the shadow and victim models.\nAfter that, a meta-classifier is trained to identify stolen models by\ndetermining whether suspicious models contain the dataset-specific features of\nthe victim. In the third stage, we conduct model ownership verification by\nhypothesis test to mitigate randomness and enhance robustness. Extensive\nexperiments on benchmark datasets verify the effectiveness of the proposed\nmethod in detecting different types of model stealing simultaneously.", "AI": {"tldr": "Proposes a harmless model ownership verification method for personalized vision models by decoupling common and dataset-specific features, addressing vulnerabilities in existing defenses.", "motivation": "Existing defense methods for traditional DNNs are ineffective or risky for fine-tuned personalized models, necessitating a robust verification method.", "method": "Uses shadow models to isolate common features, represents dataset-specific features via output differences, trains a meta-classifier, and verifies ownership via hypothesis testing.", "result": "Effective in detecting various model stealing attacks on benchmark datasets.", "conclusion": "The method provides a secure and robust solution for verifying ownership of personalized models."}}
{"id": "2507.00701", "pdf": "https://arxiv.org/pdf/2507.00701", "abs": "https://arxiv.org/abs/2507.00701", "authors": ["Chong Zhang", "Xichao Liu", "Yibing Zhan", "Dapeng Tao", "Jun Ni"], "title": "SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval", "categories": ["cs.LG"], "comment": "16 pages,6 tables,11 figures", "summary": "Recent advancements in spaceborne GNSS missions have produced extensive\nglobal datasets, providing a robust basis for deep learning-based significant\nwave height (SWH) retrieval. While existing deep learning models predominantly\nutilize CYGNSS data with four-channel information, they often adopt\nsingle-channel inputs or simple channel concatenation without leveraging the\nbenefits of cross-channel information interaction during training. To address\nthis limitation, a novel spatial-channel attention-based network, namely\nSCAWaveNet, is proposed for SWH retrieval. Specifically, features from each\nchannel of the DDMs are modeled as independent attention heads, enabling the\nfusion of spatial and channel-wise information. For auxiliary parameters, a\nlightweight attention mechanism is designed to assign weights along the spatial\nand channel dimensions. The final feature integrates both spatial and\nchannel-level characteristics. Model performance is evaluated using\nfour-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves\nan average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE\nreaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the\naverage RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC\nbuoy observations. The code is available at\nhttps://github.com/Clifx9908/SCAWaveNet.", "AI": {"tldr": "SCAWaveNet, a novel spatial-channel attention-based network, improves significant wave height (SWH) retrieval by leveraging cross-channel interactions, outperforming existing models with reduced RMSE.", "motivation": "Existing deep learning models for SWH retrieval lack effective cross-channel information interaction, limiting performance.", "method": "SCAWaveNet models features from each channel as independent attention heads, fusing spatial and channel-wise information, and uses a lightweight attention mechanism for auxiliary parameters.", "result": "SCAWaveNet achieves RMSE of 0.438 m (ERA5) and 0.432 m (NDBC), reducing RMSE by 3.52% and 5.47% respectively compared to state-of-the-art models.", "conclusion": "SCAWaveNet demonstrates superior performance in SWH retrieval by effectively utilizing cross-channel interactions, offering a promising approach for future applications."}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Prasoon Varshney", "Makesh Narsimhan", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Shaona Ghosh", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Chris Alexiuk", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "The Llama-Nemotron series offers open, heterogeneous reasoning models with competitive performance, superior efficiency, and a dynamic reasoning toggle. Available in three sizes, they include training details and open resources.", "motivation": "To provide open-source, high-performance reasoning models with enterprise-friendly licensing and advanced features like dynamic reasoning toggling.", "method": "Training involves neural architecture search, knowledge distillation, continued pretraining, supervised fine-tuning, and large-scale reinforcement learning.", "result": "Models (Nano, Super, Ultra) compete with state-of-the-art reasoning models while excelling in inference throughput and memory efficiency.", "conclusion": "Llama-Nemotron models advance open-source reasoning capabilities with practical features and released resources for broader adoption and research."}}
{"id": "2507.00748", "pdf": "https://arxiv.org/pdf/2507.00748", "abs": "https://arxiv.org/abs/2507.00748", "authors": ["Bob Zhang", "Haoran Li", "Tao Zhang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yanbin Hao"], "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications involving complex multi-image\ncompositions and multimodal instructions, which reveals limitations in\ncross-image reasoning and generalization. To address these challenges, we adopt\na Reinforcement Learning (RL) based post-training strategy to improve the\nreasoning performance of MLLMs in multi-image grounding tasks. Our approach\nbegins with synthesizing high-quality chain-of-thought (CoT) data for\ncold-start initialization, followed by supervised fine-tuning (SFT) using\nlow-rank adaptation (LoRA). The cold-start training stage enables the model to\nidentify correct solutions. Subsequently, we perform rejection sampling using\nthe merged SFT model to curate high-quality RL data and leverage rule-based RL\nto guide the model toward optimal reasoning paths. Extensive experimental\nresults demonstrate the effectiveness of our approach, achieving +9.04\\%\nimprovements on MIG-Bench and +4.98\\% improvements on several out-of-domain\nreasoning grounding benchmarks over the SFT baseline. Furthermore, our approach\nexhibits strong generalization in multi-image perception, with gains of +3.1\\%\nand +2.4\\% over the base model on subsets of the BLINK and MMIU benchmarks,\nrespectively.", "AI": {"tldr": "The paper introduces a Reinforcement Learning (RL) post-training strategy to enhance Multimodal Large Language Models (MLLMs) for multi-image grounding tasks, achieving significant performance improvements.", "motivation": "MLLMs struggle with complex multi-image compositions and multimodal instructions, highlighting limitations in cross-image reasoning and generalization.", "method": "The approach involves synthesizing chain-of-thought data for cold-start initialization, supervised fine-tuning (SFT) with LoRA, and rule-based RL for optimal reasoning paths.", "result": "The method improves performance by +9.04% on MIG-Bench and +4.98% on out-of-domain benchmarks, with additional gains in multi-image perception.", "conclusion": "The RL-based post-training strategy effectively enhances MLLMs for multi-image grounding, demonstrating strong generalization and reasoning improvements."}}
{"id": "2507.00788", "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma S\u00f6derberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "AI": {"tldr": "AI assistants improve development speed but have mixed effects on maintainability, with slight benefits for habitual users.", "motivation": "To investigate the impact of AI assistants on software maintainability, focusing on how easily other developers can evolve AI-assisted code.", "method": "A two-phase controlled experiment with 151 participants (95% professional developers). Phase 1 involved feature addition with/without AI, while Phase 2 evaluated code evolution without AI.", "result": "AI-assisted development led to faster evolution and slightly better CodeHealth, significant for habitual users. Productivity improved by 30.7% (median) and 55.9% (mean for habitual users).", "conclusion": "AI assistants accelerate development without degrading maintainability. Future research should address risks like code bloat and cognitive debt."}}
{"id": "2507.00711", "pdf": "https://arxiv.org/pdf/2507.00711", "abs": "https://arxiv.org/abs/2507.00711", "authors": ["Jhouben Cuesta-Ramirez", "Samuel Beaussant", "Mehdi Mounsif"], "title": "Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories", "categories": ["cs.LG"], "comment": "Accepted to KONVENS 2025", "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nrecently achieved impressive results on reasoning benchmarks. Yet, growing\nevidence shows that these models often generate longer but ineffective chains\nof thought (CoTs), calling into question whether benchmark gains reflect real\nreasoning improvements. We present new evidence of overthinking, where models\ndisregard correct solutions even when explicitly provided, instead continuing\nto generate unnecessary reasoning steps that often lead to incorrect\nconclusions. Experiments on three state-of-the-art models using the AIME2024\nmath benchmark reveal critical limitations in these models ability to integrate\ncorrective information, posing new challenges for achieving robust and\ninterpretable reasoning.", "AI": {"tldr": "LLMs trained via RL show overthinking, generating unnecessary reasoning steps and ignoring correct solutions, raising doubts about their real reasoning improvements.", "motivation": "To investigate whether benchmark gains in LLMs reflect actual reasoning improvements or just longer, ineffective chains of thought.", "method": "Experiments on three state-of-the-art models using the AIME2024 math benchmark to analyze their reasoning behavior.", "result": "Models often disregard correct solutions, generate unnecessary steps, and reach incorrect conclusions, highlighting limitations in integrating corrective information.", "conclusion": "The findings reveal critical challenges in achieving robust and interpretable reasoning in LLMs, questioning their current effectiveness."}}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722", "abs": "https://arxiv.org/abs/2505.16722", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "AI": {"tldr": "The paper introduces \"Cross-lingual Detoxification\" to reduce toxicity in LLMs across languages, evaluating its effectiveness in 392 settings and balancing safety with performance.", "motivation": "Ensuring toxicity-free LLMs in diverse linguistic contexts is a critical challenge, especially for high and low-resource languages.", "method": "A cross-lingual paradigm is explored to transfer detoxification capabilities between languages, tested in 392 settings with limited data.", "result": "The study evaluates toxicity reduction and its impact on model performance, revealing trade-offs between safety and knowledge preservation.", "conclusion": "The approach is effective but highlights the need to balance detoxification with maintaining model performance on non-toxic tasks."}}
{"id": "2507.00752", "pdf": "https://arxiv.org/pdf/2507.00752", "abs": "https://arxiv.org/abs/2507.00752", "authors": ["Hao Xing", "Kai Zhe Boey", "Yuankai Wu", "Darius Burschka", "Gordon Cheng"], "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 4 figures, accepted in IROS25, Hangzhou, China", "summary": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "AI": {"tldr": "Proposes a Multi-Modal Graph Convolutional Network (MMGCN) to improve human action segmentation by integrating low-frame-rate visual data with high-frame-rate motion data, reducing over-segmentation errors.", "motivation": "Accurate temporal segmentation of human actions is critical for intelligent robots, but noise in pose estimation and object detection causes over-segmentation errors.", "method": "Uses MMGCN with sinusoidal encoding for 3D skeletons, temporal graph fusion for multi-modal alignment, and SmoothLabelMix for data augmentation.", "result": "Outperforms state-of-the-art methods on the Bimanual Actions Dataset, achieving F1@10: 94.5% and F1@25: 92.8%.", "conclusion": "The MMGCN framework effectively mitigates fragmentation and improves action segmentation accuracy."}}
{"id": "2507.00790", "pdf": "https://arxiv.org/pdf/2507.00790", "abs": "https://arxiv.org/abs/2507.00790", "authors": ["Huaqiu Li", "Yong Wang", "Tongwen Huang", "Hailang Huang", "Haoqian Wang", "Xiangxiang Chu"], "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified image restoration is a significantly challenging task in low-level\nvision. Existing methods either make tailored designs for specific tasks,\nlimiting their generalizability across various types of degradation, or rely on\ntraining with paired datasets, thereby suffering from closed-set constraints.\nTo address these issues, we propose a novel, dataset-free, and unified approach\nthrough recurrent posterior sampling utilizing a pretrained latent diffusion\nmodel. Our method incorporates the multimodal understanding model to provide\nsematic priors for the generative model under a task-blind condition.\nFurthermore, it utilizes a lightweight module to align the degraded input with\nthe generated preference of the diffusion model, and employs recurrent\nrefinement for posterior sampling. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods, validating its effectiveness and\nrobustness. Our code and data will be available at\nhttps://github.com/AMAP-ML/LD-RPS.", "AI": {"tldr": "A novel, dataset-free, unified image restoration method using a pretrained latent diffusion model with multimodal understanding and recurrent refinement.", "motivation": "Existing methods are either task-specific or rely on paired datasets, limiting generalizability and suffering from closed-set constraints.", "method": "Utilizes a pretrained latent diffusion model with multimodal understanding for semantic priors, a lightweight module for alignment, and recurrent refinement for posterior sampling.", "result": "Outperforms state-of-the-art methods in experiments, showing effectiveness and robustness.", "conclusion": "The proposed approach is a promising solution for unified image restoration without dataset dependency."}}
{"id": "2507.00733", "pdf": "https://arxiv.org/pdf/2507.00733", "abs": "https://arxiv.org/abs/2507.00733", "authors": ["Stefan Haas", "Eyke H\u00fcllermeier"], "title": "Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction", "categories": ["cs.LG"], "comment": null, "summary": "Ordinal classification problems, where labels exhibit a natural order, are\nprevalent in high-stakes fields such as medicine and finance. Accurate\nuncertainty quantification, including the decomposition into aleatoric\n(inherent variability) and epistemic (lack of knowledge) components, is crucial\nfor reliable decision-making. However, existing research has primarily focused\non nominal classification and regression. In this paper, we introduce a novel\nclass of measures of aleatoric and epistemic uncertainty in ordinal\nclassification, which is based on a suitable reduction to (entropy- and\nvariance-based) measures for the binary case. These measures effectively\ncapture the trade-off in ordinal classification between exact hit-rate and\nminimial error distances. We demonstrate the effectiveness of our approach on\nvarious tabular ordinal benchmark datasets using ensembles of gradient-boosted\ntrees and multi-layer perceptrons for approximate Bayesian inference. Our\nmethod significantly outperforms standard and label-wise entropy and\nvariance-based measures in error detection, as indicated by misclassification\nrates and mean absolute error. Additionally, the ordinal measures show\ncompetitive performance in out-of-distribution (OOD) detection. Our findings\nhighlight the importance of considering the ordinal nature of classification\nproblems when assessing uncertainty.", "AI": {"tldr": "The paper introduces new measures for aleatoric and epistemic uncertainty in ordinal classification, outperforming existing methods in error and OOD detection.", "motivation": "Ordinal classification is common in critical fields like medicine and finance, but existing uncertainty measures focus on nominal classification and regression, leaving a gap for ordinal cases.", "method": "The authors propose entropy- and variance-based measures for ordinal classification by reducing them to binary cases, tested on benchmark datasets using gradient-boosted trees and multi-layer perceptrons.", "result": "The new measures outperform standard methods in error detection (misclassification rates, mean absolute error) and show competitive OOD detection performance.", "conclusion": "The ordinal nature of classification problems is crucial for accurate uncertainty assessment, and the proposed measures effectively address this need."}}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080", "abs": "https://arxiv.org/abs/2505.17080", "authors": ["Davide Picca"], "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.", "AI": {"tldr": "The paper argues for a semiotic view of LLMs, rejecting the cognitive framing, and highlights their role in sign manipulation and meaning-making.", "motivation": "To challenge the anthropomorphic view of LLMs as cognitive systems and propose a semiotic framework for understanding their function in cultural processes.", "method": "Theoretical analysis and practical examples demonstrate LLMs as semiotic agents generating texts open to interpretation.", "result": "LLMs are reframed as tools for creativity, dialogue, and critical inquiry, altering how meaning is produced and interpreted.", "conclusion": "A semiotic approach provides a rigorous, ethically aware framework for studying LLMs, emphasizing their role in the ecology of signs."}}
{"id": "2507.00754", "pdf": "https://arxiv.org/pdf/2507.00754", "abs": "https://arxiv.org/abs/2507.00754", "authors": ["Selim Kuzucu", "Muhammad Ferjad Naeem", "Anna Kukleva", "Federico Tombari", "Bernt Schiele"], "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs", "categories": ["cs.CV"], "comment": "26 pages, 6 figures", "summary": "The integration of Large Language Model (LLMs) blocks with Vision\nTransformers (ViTs) holds immense promise for vision-only tasks by leveraging\nthe rich semantic knowledge and reasoning capabilities of LLMs. However, a\nfundamental challenge lies in the inherent modality mismatch between\ntext-centric pretraining of LLMs and vision-centric training of ViTs. Direct\nfusion often fails to fully exploit the LLM's potential and suffers from\nunstable finetuning. As a result, LLM blocks are kept frozen while only the\nvision components are learned. As a remedy to these challenges, we introduce\nLanguage-Unlocked Vision Transformers (LUViT), a novel approach that bridges\nthis modality mismatch through a synergistic pre-training strategy. LUViT\nco-adapts a ViT backbone and an LLM fusion block by (1) employing Masked\nAuto-Encoding (MAE) to pre-train the ViT for richer visual representations, and\n(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM\nblock using the MAE objective. This joint optimization guides the ViT to\nproduce LLM-aligned features and the LLM to effectively interpret visual\ninformation. We demonstrate through extensive experiments that LUViT\nsignificantly improves performance on various downstream vision tasks,\nshowcasing a more effective and efficient pathway to harness LLM knowledge for\nvisual understanding.", "AI": {"tldr": "LUViT bridges the modality gap between LLMs and ViTs via joint pre-training with MAE and LoRA, enhancing vision task performance.", "motivation": "The mismatch between text-centric LLMs and vision-centric ViTs limits their combined potential, leading to unstable finetuning.", "method": "LUViT uses MAE to pre-train ViT and LoRA layers in the LLM block, jointly optimizing for alignment.", "result": "LUViT improves performance on downstream vision tasks, effectively leveraging LLM knowledge.", "conclusion": "LUViT offers a synergistic approach to integrate LLMs and ViTs for better visual understanding."}}
{"id": "2507.00816", "pdf": "https://arxiv.org/pdf/2507.00816", "abs": "https://arxiv.org/abs/2507.00816", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "AI": {"tldr": "PI-WAN combines physics and data-driven methods for robust quadrotor dynamics modeling, improving tracking in unknown environments.", "motivation": "Traditional methods fail in unknown environments, while data-driven ones lack generalization. PI-WAN addresses these gaps.", "method": "Uses a Temporal Convolutional Network with physics-informed loss for dynamics learning, integrated into MPC for control.", "result": "Outperforms baselines in accuracy, tracking, and robustness in simulations and real-world tests.", "conclusion": "PI-WAN effectively bridges knowledge and data-driven approaches for better quadrotor performance in unpredictable conditions."}}
{"id": "2507.00736", "pdf": "https://arxiv.org/pdf/2507.00736", "abs": "https://arxiv.org/abs/2507.00736", "authors": ["Arthur Thuy", "Ekaterina Loginova", "Dries F. Benoit"], "title": "Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN", "categories": ["cs.LG", "stat.ML"], "comment": "Published in the EvalLAC'25 workshop at AIED 2025", "summary": "Recent years have seen growing interest in Question Difficulty Estimation\n(QDE) using natural language processing techniques. Question difficulty is\noften represented using discrete levels, framing the task as ordinal regression\ndue to the inherent ordering from easiest to hardest. However, the literature\nhas neglected the ordinal nature of the task, relying on classification or\ndiscretized regression models, with specialized ordinal regression methods\nremaining unexplored. Furthermore, evaluation metrics are tightly coupled to\nthe modeling paradigm, hindering cross-study comparability. While some metrics\nfail to account for the ordinal structure of difficulty levels, none adequately\naddress class imbalance, resulting in biased performance assessments. This\nstudy addresses these limitations by benchmarking three types of model outputs\n-- discretized regression, classification, and ordinal regression -- using the\nbalanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly\ncaptures ordinality and class imbalance. In addition to using popular ordinal\nregression methods, we propose OrderedLogitNN, extending the ordered logit\nmodel from econometrics to neural networks. We fine-tune BERT on the RACE++ and\nARC datasets and find that OrderedLogitNN performs considerably better on\ncomplex tasks. The balanced DRPS offers a robust and fair evaluation metric for\ndiscrete-level QDE, providing a principled foundation for future research.", "AI": {"tldr": "The paper benchmarks QDE methods, introduces OrderedLogitNN, and proposes balanced DRPS for fair evaluation.", "motivation": "Addressing neglect of ordinality and class imbalance in QDE, and improving evaluation metrics.", "method": "Benchmarks discretized regression, classification, and ordinal regression; introduces OrderedLogitNN and balanced DRPS.", "result": "OrderedLogitNN outperforms on complex tasks; balanced DRPS proves robust.", "conclusion": "Balanced DRPS and OrderedLogitNN provide a principled foundation for future QDE research."}}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117", "abs": "https://arxiv.org/abs/2505.17117", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "AI": {"tldr": "The paper compares how humans and LLMs organize knowledge, finding that while LLMs align broadly with human categories, they lack fine-grained distinctions and favor aggressive compression over nuanced human-like representations.", "motivation": "To understand whether LLMs achieve human-like semantic compression and fidelity in knowledge organization.", "method": "An information-theoretic framework based on Rate-Distortion Theory and the Information Bottleneck principle, analyzing LLM token embeddings against human categorization benchmarks.", "result": "LLMs form broad categories aligned with humans but miss fine-grained distinctions and prioritize aggressive compression over nuanced human-like representations.", "conclusion": "The study highlights key differences between AI and human cognition, suggesting pathways for developing LLMs with more human-aligned conceptual representations."}}
{"id": "2507.00756", "pdf": "https://arxiv.org/pdf/2507.00756", "abs": "https://arxiv.org/abs/2507.00756", "authors": ["Hao Xing", "Kai Zhe Boey", "Gordon Cheng"], "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 3 figures, accepted in IROS25, Hangzhou, China", "summary": "Human-object interaction segmentation is a fundamental task of daily activity\nunderstanding, which plays a crucial role in applications such as assistive\nrobotics, healthcare, and autonomous systems. Most existing learning-based\nmethods excel in closed-world action segmentation, they struggle to generalize\nto open-world scenarios where novel actions emerge. Collecting exhaustive\naction categories for training is impractical due to the dynamic diversity of\nhuman activities, necessitating models that detect and segment\nout-of-distribution actions without manual annotation. To address this issue,\nwe formally define the open-world action segmentation problem and propose a\nstructured framework for detecting and segmenting unseen actions. Our framework\nintroduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional\nNetwork (EPGCN) with a novel decoder module for robust spatiotemporal feature\nupsampling. 2) Mixup-based training to synthesize out-of-distribution data,\neliminating reliance on manual annotations. 3) A novel Temporal Clustering loss\nthat groups in-distribution actions while distancing out-of-distribution\nsamples.\n  We evaluate our framework on two challenging human-object interaction\nrecognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.\nExperimental results demonstrate significant improvements over state-of-the-art\naction segmentation models across multiple open-set evaluation metrics,\nachieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and\nout-of-distribution detection performances (AUROC), respectively. Additionally,\nwe conduct an in-depth ablation study to assess the impact of each proposed\ncomponent, identifying the optimal framework configuration for open-world\naction segmentation.", "AI": {"tldr": "The paper addresses the challenge of open-world human-object interaction segmentation, proposing a framework with innovations like EPGCN, Mixup-based training, and Temporal Clustering loss, achieving significant performance gains.", "motivation": "Existing methods struggle with open-world scenarios where novel actions emerge, requiring models to handle out-of-distribution actions without manual annotation.", "method": "Proposes a structured framework with EPGCN for feature upsampling, Mixup-based training for synthetic data, and Temporal Clustering loss for action grouping.", "result": "Achieves 16.9% and 34.6% relative gains in open-set segmentation and out-of-distribution detection on Bimanual Actions and H2O datasets.", "conclusion": "The framework effectively addresses open-world action segmentation, with ablation studies confirming the impact of each component."}}
{"id": "2507.00817", "pdf": "https://arxiv.org/pdf/2507.00817", "abs": "https://arxiv.org/abs/2507.00817", "authors": ["Jiaming Zhang", "Rui Hu", "Qing Guo", "Wei Yang Bryan Lim"], "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.", "AI": {"tldr": "CAVALRY-V is a novel adversarial attack framework targeting V-MLLMs, improving attack performance by 22.8% on benchmarks.", "motivation": "V-MLLMs are vulnerable to adversarial attacks, but existing methods lack efficiency and effectiveness due to cross-modal and temporal challenges.", "method": "Introduces a dual-objective loss function and a two-stage generator framework for efficient adversarial attacks.", "result": "Outperforms baselines by 22.8% on video benchmarks and shows 34.4% improvement on image tasks.", "conclusion": "CAVALRY-V is a flexible and effective approach for adversarial research in multimodal systems."}}
{"id": "2507.00742", "pdf": "https://arxiv.org/pdf/2507.00742", "abs": "https://arxiv.org/abs/2507.00742", "authors": ["Carlos Caminha", "Maria de Lourdes M. Silva", "Iago C. Chaves", "Felipe T. Brito", "Victor A. E. Farias", "Javam C. Machado"], "title": "Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports", "categories": ["cs.LG"], "comment": "To be published in the Proceedings of the Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "summary": "Computer manufacturers offer platforms for users to describe device faults\nusing textual reports such as \"My screen is flickering\". Identifying the faulty\ncomponent from the report is essential for automating tests and improving user\nexperience. However, such reports are often ambiguous and lack detail, making\nthis task challenging. Large Language Models (LLMs) have shown promise in\naddressing such issues. This study evaluates 27 open-source models (1B-72B\nparameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,\nFew-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted\n98,948 inferences, processing over 51 million input tokens and generating 13\nmillion output tokens. We achieve f1-score up to 0.76. Results show that three\nmodels offer the best balance between size and performance:\nmistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and\ngemma-2-2b-it, that offer competitive performance with lower VRAM usage,\nenabling efficient inference on end-user devices as modern laptops or\nsmartphones with NPUs.", "AI": {"tldr": "The study evaluates 27 open-source and 2 proprietary LLMs to identify faulty components from ambiguous user reports, achieving an F1-score of 0.76. Three models (mistral-small-24b-instruct, llama-3.2-1b-instruct, gemma-2-2b-it) balance performance and efficiency for end-user devices.", "motivation": "Automating fault identification from ambiguous user reports to improve testing and user experience.", "method": "Evaluated 27 open-source and 2 proprietary LLMs using four prompting strategies (Zero-Shot, Few-Shot, Chain-of-Thought, CoT+Few-Shot) across 98,948 inferences.", "result": "Achieved F1-score up to 0.76; identified three efficient models suitable for end-user devices.", "conclusion": "Smaller models like llama-3.2-1b-instruct and gemma-2-2b-it offer competitive performance with lower resource usage, making them practical for deployment."}}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778", "abs": "https://arxiv.org/abs/2505.24778", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.", "AI": {"tldr": "The paper investigates whether LLMs use epistemic markers (e.g., \"fairly confident\") to reflect their intrinsic confidence, finding inconsistencies in out-of-distribution settings.", "motivation": "Accurately assessing LLM confidence is crucial for high-stakes applications, but it's unclear if epistemic markers reliably reflect intrinsic confidence.", "method": "Defines marker confidence as observed accuracy when a model uses an epistemic marker and evaluates it across question-answering datasets in in-distribution and out-of-distribution settings.", "result": "Markers generalize well within the same distribution but show inconsistent confidence in out-of-distribution scenarios.", "conclusion": "Epistemic markers are unreliable for confidence estimation, highlighting the need for better alignment between markers and actual model uncertainty."}}
{"id": "2507.00789", "pdf": "https://arxiv.org/pdf/2507.00789", "abs": "https://arxiv.org/abs/2507.00789", "authors": ["Ziji Lu"], "title": "OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models often struggle to achieve accurate semantic\nalignment between generated images and text prompts while maintaining\nefficiency for deployment on resource-constrained hardware. Existing approaches\neither incur substantial computational overhead through noise optimization or\ncompromise semantic fidelity by aggressively pruning tokens. In this work, we\npropose OptiPrune, a unified framework that combines distribution-aware initial\nnoise optimization with similarity-based token pruning to address both\nchallenges simultaneously. Specifically, (1) we introduce a distribution-aware\nnoise optimization module guided by attention scores to steer the initial\nlatent noise toward semantically meaningful regions, mitigating issues such as\nsubject neglect and feature entanglement; (2) we design a hardware-efficient\ntoken pruning strategy that selects representative base tokens via patch-wise\nsimilarity, injects randomness to enhance generalization, and recovers pruned\ntokens using maximum similarity copying before attention operations. Our method\npreserves the Gaussian prior during noise optimization and enables efficient\ninference without sacrificing alignment quality. Experiments on benchmark\ndatasets, including Animal-Animal, demonstrate that OptiPrune achieves\nstate-of-the-art prompt-image consistency with significantly reduced\ncomputational cost.", "AI": {"tldr": "OptiPrune combines noise optimization and token pruning to improve semantic alignment in text-to-image diffusion models while maintaining efficiency.", "motivation": "Existing methods either increase computational overhead or reduce semantic fidelity, creating a need for a balanced solution.", "method": "OptiPrune uses distribution-aware noise optimization and similarity-based token pruning to enhance alignment and efficiency.", "result": "Achieves state-of-the-art prompt-image consistency with reduced computational cost on benchmark datasets.", "conclusion": "OptiPrune effectively balances semantic alignment and efficiency in text-to-image generation."}}
{"id": "2507.00833", "pdf": "https://arxiv.org/pdf/2507.00833", "abs": "https://arxiv.org/abs/2507.00833", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page: https://openhumanoidgen.github.io", "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.", "AI": {"tldr": "HumanoidGen automates task creation and demonstration collection for bimanual dexterous manipulation in humanoid robots using atomic operations and LLM reasoning, improving performance with scalable datasets.", "motivation": "Existing datasets and benchmarks lack support for humanoid robots with dual arms and dexterous hands, making bimanual manipulation tasks challenging.", "method": "The framework uses atomic dexterous operations and LLM reasoning to generate spatial constraints, enhanced by Monte Carlo tree search for long-horizon tasks.", "result": "A novel benchmark shows that 2D and 3D diffusion policies scale well with the generated dataset.", "conclusion": "HumanoidGen addresses the gap in humanoid robot manipulation data, demonstrating effective automation and scalability."}}
{"id": "2507.00761", "pdf": "https://arxiv.org/pdf/2507.00761", "abs": "https://arxiv.org/abs/2507.00761", "authors": ["Wenbo Yu", "Anirbit Ghosh", "Tobias Sebastian Finn", "Rossella Arcucci", "Marc Bocquet", "Sibo Cheng"], "title": "A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model", "categories": ["cs.LG"], "comment": null, "summary": "Thanks to recent advances in generative AI, computers can now simulate\nrealistic and complex natural processes. We apply this capability to predict\nhow wildfires spread, a task made difficult by the unpredictable nature of fire\nand the variety of environmental conditions it depends on. In this study, We\npresent the first denoising diffusion model for predicting wildfire spread, a\nnew kind of AI framework that learns to simulate fires not just as one fixed\noutcome, but as a range of possible scenarios. By doing so, it accounts for the\ninherent uncertainty of wildfire dynamics, a feature that traditional models\ntypically fail to represent. Unlike deterministic approaches that generate a\nsingle prediction, our model produces ensembles of forecasts that reflect\nphysically meaningful distributions of where fire might go next. This\ntechnology could help us develop smarter, faster, and more reliable tools for\nanticipating wildfire behavior, aiding decision-makers in fire risk assessment\nand response planning.", "AI": {"tldr": "A denoising diffusion model is introduced to predict wildfire spread, generating multiple possible scenarios to account for uncertainty, unlike traditional deterministic models.", "motivation": "Wildfire spread prediction is challenging due to fire's unpredictability and environmental variability. Existing models often fail to represent uncertainty.", "method": "The study uses a denoising diffusion model, an AI framework that simulates fire spread as a range of possible outcomes, producing ensemble forecasts.", "result": "The model generates physically meaningful distributions of fire spread scenarios, improving reliability over single-outcome predictions.", "conclusion": "This approach enhances wildfire behavior prediction, aiding risk assessment and response planning with smarter, faster tools."}}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710", "abs": "https://arxiv.org/abs/2506.18710", "authors": ["Maxime Leli\u00e8vre", "Amy Waldock", "Meng Liu", "Natalia Vald\u00e9s Aspillaga", "Alasdair Mackintosh", "Mar\u00eda Jos\u00e9 Ogando Portela", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.", "AI": {"tldr": "The paper introduces The Pedagogy Benchmark to evaluate AI models' pedagogical knowledge, addressing a gap in existing benchmarks focused on content knowledge. It reports results for 97 models and provides an online leaderboard for interactive exploration.", "motivation": "Existing benchmarks focus on content knowledge, neglecting pedagogical understanding, which is critical for AI's role in education.", "method": "The benchmark uses questions from professional teacher exams, covering pedagogical subdomains like teaching strategies and assessment methods. Results for 97 models are analyzed, with accuracy ranging from 28% to 89%.", "result": "Model accuracies vary widely (28%-89%), and the paper explores cost-accuracy trade-offs. An online leaderboard allows interactive exploration of model performance.", "conclusion": "Pedagogical benchmarks are essential for responsible AI deployment in education, guiding development and policy decisions."}}
{"id": "2507.00792", "pdf": "https://arxiv.org/pdf/2507.00792", "abs": "https://arxiv.org/abs/2507.00792", "authors": ["Hendric Voss", "Stefan Kopp"], "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK", "AI": {"tldr": "A novel real-time inverse kinematics solver for realistic human-like movement, leveraging TensorFlow for efficiency and addressing challenges like error accumulation and joint limits.", "motivation": "To improve virtual human movement accuracy for applications in computer graphics, virtual environments, robotics, and biomechanics.", "method": "Uses TensorFlow's automatic differentiation and just-in-time compilation to handle complex human skeletons, treating forward and inverse kinematics as differentiable operations.", "result": "Achieves real-time performance with rapid convergence, minimal overhead, and higher success rates compared to CCD, FABRIK, and IPOPT.", "conclusion": "The solver is effective for realistic human motion modeling, outperforming existing methods in real-time scenarios."}}
{"id": "2507.00880", "pdf": "https://arxiv.org/pdf/2507.00880", "abs": "https://arxiv.org/abs/2507.00880", "authors": ["Ruihan Xu", "Haokui Zhang", "Yaowei Wang", "Wei Zeng", "Shiliang Zhang"], "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to CVPR 2025. Code is avaiable at\n  https://github.com/XuRuihan/NNFormer", "summary": "The growing use of deep learning necessitates efficient network design and\ndeployment, making neural predictors vital for estimating attributes such as\naccuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers\nhave shown promising performance in representing neural architectures. However,\neach of both methods has its disadvantages. GNNs lack the capabilities to\nrepresent complicated features, while transformers face poor generalization\nwhen the depth of architecture grows. To mitigate the above issues, we rethink\nneural architecture topology and show that sibling nodes are pivotal while\noverlooked in previous research. We thus propose a novel predictor leveraging\nthe strengths of GNNs and transformers to learn the enhanced topology. We\nintroduce a novel token mixer that considers siblings, and a new channel mixer\nnamed bidirectional graph isomorphism feed-forward network. Our approach\nconsistently achieves promising performance in both accuracy and latency\nprediction, providing valuable insights for learning Directed Acyclic Graph\n(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.", "AI": {"tldr": "A novel neural predictor combining GNNs and transformers addresses limitations of both by focusing on sibling nodes in neural architecture topology, achieving strong accuracy and latency prediction.", "motivation": "The need for efficient neural network design and deployment drives the development of better predictors for attributes like accuracy and latency, overcoming the shortcomings of existing GNNs and transformers.", "method": "Proposes a hybrid predictor using GNNs and transformers, introducing a sibling-aware token mixer and a bidirectional graph isomorphism feed-forward network for enhanced topology learning.", "result": "The approach consistently delivers strong performance in accuracy and latency prediction, offering insights for DAG topology learning.", "conclusion": "The novel predictor effectively leverages sibling nodes and hybrid techniques, providing a robust solution for neural architecture attribute prediction."}}
{"id": "2507.00762", "pdf": "https://arxiv.org/pdf/2507.00762", "abs": "https://arxiv.org/abs/2507.00762", "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"], "title": "Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments", "categories": ["cs.LG"], "comment": "This article has been submitted to and accepted for presentation at\n  the 11th International Conference on Machine Learning, Optimization, and Data\n  Science (LOD 2025). After publication, it will appear in the official LOD\n  2025 proceedings", "summary": "Reinforcement Learning (RL) has demonstrated significant potential in certain\nreal-world industrial applications, yet its broader deployment remains limited\nby inherent challenges such as sample inefficiency and unstable learning\ndynamics. This study investigates the utilization of Genetic Algorithms (GAs)\nas a mechanism for improving RL performance in an industrially inspired sorting\nenvironment. We propose a novel approach in which GA-generated expert\ndemonstrations are used to enhance policy learning. These demonstrations are\nincorporated into a Deep Q-Network (DQN) replay buffer for experience-based\nlearning and utilized as warm-start trajectories for Proximal Policy\nOptimization (PPO) agents to accelerate training convergence. Our experiments\ncompare standard RL training with rule-based heuristics, brute-force\noptimization, and demonstration data, revealing that GA-derived demonstrations\nsignificantly improve RL performance. Notably, PPO agents initialized with\nGA-generated data achieved superior cumulative rewards, highlighting the\npotential of hybrid learning paradigms, where heuristic search methods\ncomplement data-driven RL. The utilized framework is publicly available and\nenables further research into adaptive RL strategies for real-world\napplications.", "AI": {"tldr": "The paper explores using Genetic Algorithms (GAs) to enhance Reinforcement Learning (RL) performance in industrial sorting tasks by generating expert demonstrations for RL agents.", "motivation": "RL's broader industrial use is limited by sample inefficiency and unstable learning. This study aims to improve RL performance by integrating GA-generated demonstrations.", "method": "GA-generated expert demonstrations are fed into a DQN replay buffer and used as warm-start trajectories for PPO agents. Comparisons are made with rule-based heuristics and brute-force optimization.", "result": "GA-derived demonstrations significantly boost RL performance, with PPO agents achieving superior cumulative rewards when initialized with GA data.", "conclusion": "Hybrid learning paradigms combining heuristic search (GAs) and data-driven RL show promise for real-world applications, with the framework made publicly available for further research."}}
{"id": "2506.19089", "pdf": "https://arxiv.org/pdf/2506.19089", "abs": "https://arxiv.org/abs/2506.19089", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available.", "AI": {"tldr": "StorySim is a framework for generating synthetic stories to evaluate LLMs' theory of mind and world modeling, revealing performance gaps and heuristic behaviors.", "motivation": "To address contamination in pretraining data and enable precise evaluation of ToM and WM in LLMs.", "method": "Uses a controllable Storyboard to generate novel, compositional story prompts for ToM and WM tasks.", "result": "LLMs perform better on WM than ToM tasks and show heuristic behaviors like recency bias.", "conclusion": "StorySim provides a robust tool for evaluating LLMs, uncovering limitations and biases in reasoning."}}
{"id": "2507.00802", "pdf": "https://arxiv.org/pdf/2507.00802", "abs": "https://arxiv.org/abs/2507.00802", "authors": ["Minye Shao", "Xingyu Miao", "Haoran Duan", "Zeyu Wang", "Jingkun Chen", "Yawen Huang", "Xian Wu", "Jingjing Deng", "Yang Long", "Yefeng Zheng"], "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  preprint version). MICCAI proceedings DOI will appear here", "summary": "3D medical image generation is essential for data augmentation and patient\nprivacy, calling for reliable and efficient models suited for clinical\npractice. However, current methods suffer from limited anatomical fidelity,\nrestricted axial length, and substantial computational cost, placing them\nbeyond reach for regions with limited resources and infrastructure. We\nintroduce TRACE, a framework that generates 3D medical images with\nspatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.\nTRACE models sequential 2D slices as video frame pairs, combining segmentation\npriors and radiology reports for anatomical alignment, incorporating optical\nflow to sustain temporal coherence. During inference, an overlapping-frame\nstrategy links frame pairs into a flexible length sequence, reconstructed into\na spatiotemporally and anatomically aligned 3D volume. Experimental results\ndemonstrate that TRACE effectively balances computational efficiency with\npreserving anatomical fidelity and spatiotemporal consistency. Code is\navailable at: https://github.com/VinyehShaw/TRACE.", "AI": {"tldr": "TRACE is a framework for generating 3D medical images using a 2D diffusion approach, addressing anatomical fidelity and computational efficiency.", "motivation": "Current 3D medical image generation methods lack anatomical fidelity, axial length flexibility, and are computationally expensive, limiting accessibility in resource-limited regions.", "method": "TRACE uses a 2D multimodal-conditioned diffusion approach, modeling slices as video frames with segmentation priors, radiology reports, and optical flow for alignment and coherence.", "result": "TRACE generates spatiotemporally and anatomically aligned 3D volumes efficiently, balancing computational cost with fidelity.", "conclusion": "TRACE offers a practical solution for 3D medical image generation, suitable for clinical practice and resource-limited settings."}}
{"id": "2507.00902", "pdf": "https://arxiv.org/pdf/2507.00902", "abs": "https://arxiv.org/abs/2507.00902", "authors": ["Feng Wang", "Shengyu Zhang", "Een-Kee Hong", "Tony Q. S. Quek"], "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "comment": "To appear in IEEE Communications Magazine", "summary": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "AI": {"tldr": "The paper proposes a Constellation as a Service (CaaS) framework to optimize DS2D connectivity in multi-constellation environments by dynamically forming sub-constellations and using predictive beamforming and pre-configured handover paths.", "motivation": "The challenge of managing DS2D connectivity in multi-constellation scenarios, including interference and frequent handovers, motivates the need for a more efficient solution.", "method": "The CaaS framework treats multi-constellation infrastructure as a shared resource pool, forming optimal sub-constellations with predictive satellite beamforming (GenAI) and pre-configured handover paths.", "result": "Simulations show CaaS improves satellite service rates and reduces handover overhead.", "conclusion": "CaaS is an efficient and sustainable solution for DS2D connectivity in multi-constellation environments."}}
{"id": "2507.00846", "pdf": "https://arxiv.org/pdf/2507.00846", "abs": "https://arxiv.org/abs/2507.00846", "authors": ["Rishal Aggrwal", "Jacky Chen", "Nicholas M. Boffi", "David Ryan Koes"], "title": "BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation", "categories": ["cs.LG", "physics.bio-ph"], "comment": "19 pages, 25 figures, submitted to NeurIPS 2025", "summary": "Efficient sampling from the Boltzmann distribution defined by an energy\nfunction is a key challenge in modeling physical systems such as molecules.\nBoltzmann Generators tackle this by leveraging Continuous Normalizing Flows\nthat transform a simple prior into a distribution that can be reweighted to\nmatch the Boltzmann distribution using sample likelihoods. However, obtaining\nlikelihoods requires computing costly Jacobians during integration, making it\nimpractical for large molecular systems. To overcome this, we propose learning\nthe likelihood of the generated distribution via an energy-based model trained\nwith noise contrastive estimation and score matching. By using stochastic\ninterpolants to anneal between the prior and generated distributions, we\ncombine both the objective functions to efficiently learn the density function.\nOn the alanine dipeptide system, we demonstrate that our method yields free\nenergy profiles and energy distributions comparable to those obtained with\nexact likelihoods. Additionally, we show that free energy differences between\nmetastable states can be estimated accurately with orders-of-magnitude speedup.", "AI": {"tldr": "The paper proposes a method to efficiently sample from the Boltzmann distribution using energy-based models and stochastic interpolants, avoiding costly Jacobian computations.", "motivation": "Sampling from the Boltzmann distribution is computationally expensive for large molecular systems due to the need for costly Jacobian calculations.", "method": "The method learns the likelihood of the generated distribution using an energy-based model trained with noise contrastive estimation and score matching, combined with stochastic interpolants for annealing.", "result": "The approach achieves comparable free energy profiles and distributions to exact likelihood methods, with significant speedup in estimating free energy differences.", "conclusion": "The proposed method efficiently approximates the Boltzmann distribution, offering a practical solution for large molecular systems."}}
{"id": "2506.21096", "pdf": "https://arxiv.org/pdf/2506.21096", "abs": "https://arxiv.org/abs/2506.21096", "authors": ["Kang He", "Yuzhe Ding", "Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "AI": {"tldr": "DALR improves multimodal sentence representation by addressing cross-modal misalignment and intra-modal divergence through dual-level alignment learning.", "motivation": "Existing methods struggle with cross-modal misalignment and intra-modal semantic divergence, degrading representation quality.", "method": "DALR uses consistency learning for fine-grained cross-modal alignment and ranking distillation for intra-modal alignment.", "result": "Outperforms state-of-the-art baselines on STS and TR tasks.", "conclusion": "DALR effectively enhances multimodal sentence representation quality."}}
{"id": "2507.00822", "pdf": "https://arxiv.org/pdf/2507.00822", "abs": "https://arxiv.org/abs/2507.00822", "authors": ["Yasser El Jarida", "Youssef Iraqi", "Loubna Mekouar"], "title": "Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data", "categories": ["cs.CV"], "comment": "Accepted at the Synthetic Data for Computer Vision Workshop @ CVPR\n  2025. 10 pages, 5 figures. Code available at\n  https://github.com/YasserElj/Synthetic-Granular-Gen", "summary": "Accurate particle size distribution (PSD) measurement is important in\nindustries such as mining, pharmaceuticals, and fertilizer manufacturing,\nsignificantly influencing product quality and operational efficiency.\nTraditional PSD methods like sieve analysis and laser diffraction are manual,\ntime-consuming, and limited by particle overlap. Recent developments in\nconvolutional neural networks (CNNs) enable automated, real-time PSD estimation\ndirectly from particle images. In this work, we present a CNN-based methodology\ntrained on realistic synthetic particle imagery generated using Blender's\nadvanced rendering capabilities. Synthetic data sets using this method can\nreplicate various industrial scenarios by systematically varying particle\nshapes, textures, lighting, and spatial arrangements that closely resemble the\nactual configurations. We evaluated three CNN-based architectures, ResNet-50,\nInceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,\nd50, d90). Results demonstrated comparable accuracy across models, with\nEfficientNet-B0 achieving the best computational efficiency suitable for\nreal-time industrial deployment. This approach shows the effectiveness of\nrealistic synthetic data for robust CNN training, which offers significant\npotential for automated industrial PSD monitoring. The code is released at :\nhttps://github.com/YasserElj/Synthetic-Granular-Gen", "AI": {"tldr": "A CNN-based method using synthetic particle images for real-time PSD measurement, outperforming traditional methods in efficiency and accuracy.", "motivation": "Accurate PSD measurement is crucial in industries like mining and pharmaceuticals, but traditional methods are manual and slow.", "method": "Uses synthetic particle images generated with Blender to train CNNs (ResNet-50, InceptionV3, EfficientNet-B0) for PSD estimation.", "result": "EfficientNet-B0 showed the best computational efficiency with comparable accuracy, suitable for real-time industrial use.", "conclusion": "Synthetic data enables robust CNN training, offering potential for automated industrial PSD monitoring."}}
{"id": "2507.00907", "pdf": "https://arxiv.org/pdf/2507.00907", "abs": "https://arxiv.org/abs/2507.00907", "authors": ["Fabio Correa Xavier"], "title": "The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses", "categories": ["cs.CR", "cs.AI", "68T07, 68T45, 94A60", "K.6.5; D.4.6; I.2.6"], "comment": "14 pages", "summary": "In a world where deepfakes and cloned voices are emerging as sophisticated\nattack vectors, organizations require a new security mindset: Sensorial Zero\nTrust [9]. This article presents a scientific analysis of the need to\nsystematically doubt information perceived through the senses, establishing\nrigorous verification protocols to mitigate the risks of fraud based on\ngenerative artificial intelligence. Key concepts, such as Out-of-Band\nverification, Vision-Language Models (VLMs) as forensic collaborators,\ncryptographic provenance, and human training, are integrated into a framework\nthat extends Zero Trust principles to human sensory information. The approach\nis grounded in empirical findings and academic research, emphasizing that in an\nera of AI-generated realities, even our eyes and ears can no longer be\nimplicitly trusted without verification. Leaders are called to foster a culture\nof methodological skepticism to protect organizational integrity in this new\nthreat landscape.", "AI": {"tldr": "The paper advocates for Sensorial Zero Trust to combat deepfake and voice cloning threats by doubting sensory information and using verification protocols.", "motivation": "The rise of deepfakes and cloned voices as attack vectors necessitates a new security mindset to protect organizational integrity.", "method": "Proposes a framework integrating Out-of-Band verification, Vision-Language Models, cryptographic provenance, and human training to extend Zero Trust to sensory data.", "result": "Empirical findings highlight the need for rigorous verification of sensory information in the age of AI-generated realities.", "conclusion": "Leaders must cultivate methodological skepticism to safeguard against AI-driven fraud."}}
{"id": "2507.00848", "pdf": "https://arxiv.org/pdf/2507.00848", "abs": "https://arxiv.org/abs/2507.00848", "authors": ["Don Roosan", "Saif Nirzhor", "Rubayat Khan", "Fahmida Hai", "Mohammad Rifat Haidar"], "title": "Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters", "categories": ["cs.LG", "q-bio.MN"], "comment": "Conference details can be found here:\n  https://www.insticc.org/node/technicalprogram/DATA/2025", "summary": "HIV epidemiological data is increasingly complex, requiring advanced\ncomputation for accurate cluster detection and forecasting. We employed\nquantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code\nlevel using AIDSVu and synthetic SDoH data for 2022. Our approach compared\nclassical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization\nalgorithm (QAOA), developed a hybrid quantum-classical neural network for HIV\nprevalence forecasting, and used quantum Bayesian networks to explore causal\nlinks between SDoH factors and HIV incidence. The QAOA-based method achieved\n92% accuracy in cluster detection within 1.6 seconds, outperforming classical\nalgorithms. Meanwhile, the hybrid quantum-classical neural network predicted\nHIV prevalence with 94% accuracy, surpassing a purely classical counterpart.\nQuantum Bayesian analysis identified housing instability as a key driver of HIV\ncluster emergence and expansion, with stigma exerting a geographically variable\ninfluence. These quantum-enhanced methods deliver greater precision and\nefficiency in HIV surveillance while illuminating critical causal pathways.\nThis work can guide targeted interventions, optimize resource allocation for\nPrEP, and address structural inequities fueling HIV transmission.", "AI": {"tldr": "The paper uses quantum-accelerated machine learning to improve HIV cluster detection and forecasting, outperforming classical methods in accuracy and speed.", "motivation": "Complex HIV epidemiological data requires advanced computational methods for accurate analysis and intervention planning.", "method": "Combined quantum (QAOA, hybrid neural networks, Bayesian networks) and classical (DBSCAN, HDBSCAN) techniques to analyze HIV data.", "result": "Quantum methods achieved 92-94% accuracy, identified housing instability as a key factor, and outperformed classical approaches.", "conclusion": "Quantum-enhanced methods offer precise, efficient HIV surveillance and insights for targeted interventions and resource allocation."}}
{"id": "2506.21098", "pdf": "https://arxiv.org/pdf/2506.21098", "abs": "https://arxiv.org/abs/2506.21098", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "AI": {"tldr": "ComRAG is a retrieval-augmented generation framework for real-time industrial CQA, integrating static and dynamic knowledge with efficient memory mechanisms, outperforming baselines in performance and efficiency.", "motivation": "Existing methods underutilize external knowledge and lack dynamic historical context or efficient memory mechanisms for industrial CQA.", "method": "ComRAG combines static knowledge with dynamic historical QA pairs using a centroid-based memory mechanism for retrieval, generation, and storage.", "result": "ComRAG achieves up to 25.9% improvement in vector similarity, reduces latency by 8.7%-23.3%, and lowers chunk growth from 20.23% to 2.06%.", "conclusion": "ComRAG effectively addresses challenges in industrial CQA, outperforming existing methods in performance and efficiency."}}
{"id": "2507.00825", "pdf": "https://arxiv.org/pdf/2507.00825", "abs": "https://arxiv.org/abs/2507.00825", "authors": ["Hongxing Peng", "Lide Chen", "Hui Zhu", "Yan Chen"], "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "comment": "14 pages, 9 figures, to appear in KBS", "summary": "Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial\nchallenges, including small target sizes, high-density distributions, and\ncluttered backgrounds in UAV imagery. Current algorithms often depend on\nhand-crafted components like anchor boxes, which demand fine-tuning and exhibit\nlimited generalization, and Non-Maximum Suppression (NMS), which is\nthreshold-sensitive and prone to misclassifying dense objects. These generic\narchitectures thus struggle to adapt to aerial imaging characteristics,\nresulting in performance limitations. Moreover, emerging end-to-end frameworks\nhave yet to effectively mitigate these aerial-specific challenges.To address\nthese issues, we propose HEGS-DETR, a comprehensively enhanced, real-time\nDetection Transformer framework tailored for UAVs. First, we introduce the\nHigh-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.\nHFESNet preserves critical high-frequency spatial details to extract robust\nsemantic features, thereby improving discriminative capability for small and\noccluded targets in complex backgrounds. Second, our Efficient Small Object\nPyramid (ESOP) strategy strategically fuses high-resolution feature maps with\nminimal computational overhead, significantly boosting small object detection.\nFinally, the proposed Selective Query Recollection (SQR) and Geometry-Aware\nPositional Encoding (GAPE) modules enhance the detector's decoder stability and\nlocalization accuracy, effectively optimizing bounding boxes and providing\nexplicit spatial priors for dense scenes. Experiments on the VisDrone dataset\ndemonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase\nover the baseline, while maintaining real-time speed and reducing parameter\ncount by 4M.", "AI": {"tldr": "HEGS-DETR is an enhanced Detection Transformer framework for UAV-based object detection, addressing challenges like small targets and cluttered backgrounds with novel components like HFESNet, ESOP, SQR, and GAPE, achieving improved performance on the VisDrone dataset.", "motivation": "Current UAV-OD algorithms struggle with small targets, high-density distributions, and cluttered backgrounds due to reliance on hand-crafted components and lack of aerial-specific optimizations.", "method": "Proposes HEGS-DETR with HFESNet for high-frequency detail preservation, ESOP for small object detection, and SQR and GAPE for decoder stability and localization accuracy.", "result": "Achieves a 5.1% AP$_{50}$ and 3.8% AP increase over baseline on VisDrone, with real-time speed and reduced parameters.", "conclusion": "HEGS-DETR effectively addresses UAV-OD challenges, outperforming existing methods while maintaining efficiency."}}
{"id": "2507.00909", "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "AI": {"tldr": "Emerald Conductor, a software-only solution, reduces AI data center power usage by 25% during peak grid events while maintaining QoS, enhancing grid reliability and affordability.", "motivation": "AI's growing electricity demand threatens grid reliability, raises costs, and delays innovation due to infrastructure constraints.", "method": "A field demonstration at a 256-GPU cluster in a Phoenix data center tested Emerald Conductor, which optimizes AI workloads using real-time grid signals without hardware changes.", "result": "Achieved a 25% power reduction during peak grid events for three hours without compromising AI performance.", "conclusion": "Emerald Conductor transforms data centers into grid-interactive assets, improving reliability, affordability, and AI development."}}
{"id": "2507.00851", "pdf": "https://arxiv.org/pdf/2507.00851", "abs": "https://arxiv.org/abs/2507.00851", "authors": ["Rares Cristian", "Pavithra Harsha", "Georgia Perakis", "Brian Quanz"], "title": "Aligning Learning and Endogenous Decision-Making", "categories": ["cs.LG"], "comment": null, "summary": "Many of the observations we make are biased by our decisions. For instance,\nthe demand of items is impacted by the prices set, and online checkout choices\nare influenced by the assortments presented. The challenge in decision-making\nunder this setting is the lack of counterfactual information, and the need to\nlearn it instead. We introduce an end-to-end method under endogenous\nuncertainty to train ML models to be aware of their downstream, enabling their\neffective use in the decision-making stage. We further introduce a robust\noptimization variant that accounts for uncertainty in ML models -- specifically\nby constructing uncertainty sets over the space of ML models and optimizing\nactions to protect against worst-case predictions. We prove guarantees that\nthis robust approach can capture near-optimal decisions with high probability\nas a function of data. Besides this, we also introduce a new class of two-stage\nstochastic optimization problems to the end-to-end learning framework that can\nnow be addressed through our framework. Here, the first stage is an\ninformation-gathering problem to decide which random variable to poll and gain\ninformation about before making a second-stage decision based off of it. We\npresent several computational experiments for pricing and inventory\nassortment/recommendation problems. We compare against existing methods in\nonline learning/bandits/offline reinforcement learning and show our approach\nhas consistent improved performance over these. Just as in the endogenous\nsetting, the model's prediction also depends on the first-stage decision made.\nWhile this decision does not affect the random variable in this setting, it\ndoes affect the correct point forecast that should be made.", "AI": {"tldr": "The paper introduces an end-to-end ML method for decision-making under endogenous uncertainty, including a robust optimization variant and a new two-stage stochastic optimization framework, outperforming existing methods in experiments.", "motivation": "The challenge of biased observations in decision-making due to endogenous factors (e.g., prices affecting demand) and the lack of counterfactual information necessitates learning-aware ML models.", "method": "Proposes an end-to-end ML training method under endogenous uncertainty, a robust optimization variant with uncertainty sets, and a two-stage stochastic optimization framework for information-gathering and decision-making.", "result": "The robust approach guarantees near-optimal decisions with high probability, and computational experiments show improved performance over existing methods in pricing and inventory problems.", "conclusion": "The framework effectively addresses endogenous uncertainty and introduces novel optimization problems, demonstrating superior performance in real-world applications."}}
{"id": "2506.22403", "pdf": "https://arxiv.org/pdf/2506.22403", "abs": "https://arxiv.org/abs/2506.22403", "authors": ["NAVER Cloud HyperCLOVA X Team"], "title": "HyperCLOVA X THINK Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": "50 pages, 13 figures; fixed figures in the appendix", "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.", "AI": {"tldr": "HyperCLOVA X THINK is a reasoning-focused large language model, pre-trained on 6 trillion Korean and English tokens, with competitive performance on Korean benchmarks and bilingual consistency. It includes a vision-augmented variant and aims for open-source release.", "motivation": "To create a robust, bilingual (Korean-English) reasoning-focused model with efficient training compute and strong performance on Korean benchmarks.", "method": "Pre-trained on 6 trillion tokens using a compute-memory-balanced Peri-LN Transformer, scaled with \u03bcP, and post-trained via supervised fine-tuning with reinforcement learning. Includes a vision-augmented variant.", "result": "Competitive performance on Korean benchmarks (KMMLU, CSAT, etc.), bilingual consistency, and vision-augmented variant matching GPT-4.1 on KCSAT STEM. Achieved with lower training compute.", "conclusion": "HyperCLOVA X THINK is a strong foundation for Korean AI innovation and a valuable resource globally, with plans for open-source release."}}
{"id": "2507.00845", "pdf": "https://arxiv.org/pdf/2507.00845", "abs": "https://arxiv.org/abs/2507.00845", "authors": ["Peter Pavl\u00edk", "Marc Schleiss", "Anna Bou Ezzeddine", "Viera Rozinajov\u00e1"], "title": "Do Echo Top Heights Improve Deep Learning Nowcasts?", "categories": ["cs.CV", "cs.LG"], "comment": "Pre-review version of an article accepted at Transactions on\n  Large-Scale Data and Knowledge-Centered Systems", "summary": "Precipitation nowcasting -- the short-term prediction of rainfall using\nrecent radar observations -- is critical for weather-sensitive sectors such as\ntransportation, agriculture, and disaster mitigation. While recent deep\nlearning models have shown promise in improving nowcasting skill, most\napproaches rely solely on 2D radar reflectivity fields, discarding valuable\nvertical information available in the full 3D radar volume. In this work, we\nexplore the use of Echo Top Height (ETH), a 2D projection indicating the\nmaximum altitude of radar reflectivity above a given threshold, as an auxiliary\ninput variable for deep learning-based nowcasting. We examine the relationship\nbetween ETH and radar reflectivity, confirming its relevance for predicting\nrainfall intensity. We implement a single-pass 3D U-Net that processes both the\nradar reflectivity and ETH as separate input channels. While our models are\nable to leverage ETH to improve skill at low rain-rate thresholds, results are\ninconsistent at higher intensities and the models with ETH systematically\nunderestimate precipitation intensity. Three case studies are used to\nillustrate how ETH can help in some cases, but also confuse the models and\nincrease the error variance. Nonetheless, the study serves as a foundation for\ncritically assessing the potential contribution of additional variables to\nnowcasting performance.", "AI": {"tldr": "The paper explores using Echo Top Height (ETH) as an auxiliary input for deep learning-based precipitation nowcasting, finding mixed results with improvements at low rain rates but inconsistencies at higher intensities.", "motivation": "Improve short-term rainfall prediction by incorporating vertical radar information (ETH) alongside traditional 2D reflectivity data.", "method": "Implemented a 3D U-Net to process radar reflectivity and ETH as separate input channels, analyzing their relationship and impact on nowcasting.", "result": "ETH improved skill at low rain rates but led to underestimation and inconsistency at higher intensities, with case studies showing varied effects.", "conclusion": "While ETH shows potential, its inconsistent performance highlights the need for further research on integrating additional variables into nowcasting models."}}
{"id": "2507.00938", "pdf": "https://arxiv.org/pdf/2507.00938", "abs": "https://arxiv.org/abs/2507.00938", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "comment": "10 pages, 9 figures, 4 tables", "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "AI": {"tldr": "WebArXiv is introduced as a static benchmark for evaluating web agents, addressing instability in existing benchmarks. It includes 275 tasks on arXiv, with a proposed reflection mechanism to improve agent performance.", "motivation": "Existing benchmarks for web agents are unstable and inconsistent, relying on dynamic content or oversimplified simulations, making evaluation unreliable.", "method": "WebArXiv uses fixed web snapshots with deterministic ground truths and standardized action trajectories. A dynamic reflection mechanism is proposed to mitigate over-reliance on fixed interaction histories.", "result": "Evaluation of ten state-of-the-art web agents on WebArXiv shows performance differences and validates the effectiveness of the reflection strategy.", "conclusion": "WebArXiv provides a reproducible benchmark for web agents, and the dynamic reflection mechanism improves agent decision-making."}}
{"id": "2507.00862", "pdf": "https://arxiv.org/pdf/2507.00862", "abs": "https://arxiv.org/abs/2507.00862", "authors": ["Davide Andreoletti", "Aris Marcolongo", "Natasa Sarafijanovic Djukic", "Julien Roulet", "Stefano Billeter", "Andrzej Kurenda", "Margot Visse-Mansiaux", "Brice Dupuis", "Carrol Annette Plummer", "Beatrice Paoli", "Omran Ayoub"], "title": "Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals", "categories": ["cs.LG"], "comment": "8 pages, 7 figures", "summary": "Accurately predicting potato sprouting before the emergence of any visual\nsigns is critical for effective storage management, as sprouting degrades both\nthe commercial and nutritional value of tubers. Effective forecasting allows\nfor the precise application of anti-sprouting chemicals (ASCs), minimizing\nwaste and reducing costs. This need has become even more pressing following the\nban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to\nhealth and environmental concerns, which has led to the adoption of\nsignificantly more expensive alternative ASCs. Existing approaches primarily\nrely on visual identification, which only detects sprouting after morphological\nchanges have occurred, limiting their effectiveness for proactive management. A\nreliable early prediction method is therefore essential to enable timely\nintervention and improve the efficiency of post-harvest storage strategies,\nwhere early refers to detecting sprouting before any visible signs appear. In\nthis work, we address the problem of early prediction of potato sprouting. To\nthis end, we propose a novel machine learning (ML)-based approach that enables\nearly prediction of potato sprouting using electrophysiological signals\nrecorded from tubers using proprietary sensors. Our approach preprocesses the\nrecorded signals, extracts relevant features from the wavelet domain, and\ntrains supervised ML models for early sprouting detection. Additionally, we\nincorporate uncertainty quantification techniques to enhance predictions.\nExperimental results demonstrate promising performance in the early detection\nof potato sprouting by accurately predicting the exact day of sprouting for a\nsubset of potatoes and while showing acceptable average error across all\npotatoes. Despite promising results, further refinements are necessary to\nminimize prediction errors, particularly in reducing the maximum observed\ndeviations.", "AI": {"tldr": "A machine learning approach using electrophysiological signals for early prediction of potato sprouting, addressing the need for proactive storage management post-CIPC ban.", "motivation": "The ban on CIPC and the limitations of visual sprouting detection necessitate a reliable early prediction method to optimize storage and reduce costs.", "method": "Proposes an ML-based approach using electrophysiological signals, preprocessing, wavelet domain feature extraction, and uncertainty quantification.", "result": "Demonstrates promising performance in early sprouting detection, with accurate day predictions for some potatoes and acceptable average error.", "conclusion": "While results are promising, further refinements are needed to minimize prediction errors, especially in reducing maximum deviations."}}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "The report summarizes a workshop on AI language models and human cognition, highlighting their interplay, potential, and limitations.", "motivation": "To explore the relationship between AI language models and human cognitive processes in text comprehension and production.", "method": "Interdisciplinary collaboration among experts in cognitive psychology, linguistics, and AI, analyzing human and AI language processes.", "result": "Findings include LLMs' potential to mimic human language processing, benefits of human feedback, and challenges in human-AI collaboration.", "conclusion": "The report guides future research and emphasizes ethical AI use to enhance human-AI collaboration in language tasks."}}
{"id": "2507.00849", "pdf": "https://arxiv.org/pdf/2507.00849", "abs": "https://arxiv.org/abs/2507.00849", "authors": ["Wei Li", "Jiaman Tang", "Yang Li", "Beihao Xia", "Ligang Tan", "Hongmao Qin"], "title": "UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection", "categories": ["cs.CV"], "comment": "The paper was accepted by the 36th IEEE Intelligent Vehicles\n  Symposium (IEEE IV 2025)", "summary": "Unmanned Aerial Vehicle (UAV) object detection has been widely used in\ntraffic management, agriculture, emergency rescue, etc. However, it faces\nsignificant challenges, including occlusions, small object sizes, and irregular\nshapes. These challenges highlight the necessity for a robust and efficient\nmultimodal UAV object detection method. Mamba has demonstrated considerable\npotential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a\nmultimodal UAV object detection framework based on Mamba architectures. To\nimprove geometric adaptability, we propose the Deformable Token Mamba Block\n(DTMB) to generate deformable tokens by incorporating adaptive patches from\ndeformable convolutions alongside normal patches from normal convolutions,\nwhich serve as the inputs to the Mamba Block. To optimize the multimodal\nfeature complementarity, we design two separate DTMBs for the RGB and infrared\n(IR) modalities, with the outputs from both DTMBs integrated into the Mamba\nBlock for feature extraction and into the Fusion Mamba Block for feature\nfusion. Additionally, to improve multiscale object detection, especially for\nsmall objects, we stack four DTMBs at different scales to produce multiscale\nfeature representations, which are then sent to the Detection Neck for Mamba\n(DNM). The DNM module, inspired by the YOLO series, includes modifications to\nthe SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In\nparticular, we employ cross-enhanced spatial attention before the DTMB and\ncross-channel attention after the Fusion Mamba Block to extract more\ndiscriminative features. Experimental results on the DroneVehicle dataset show\nthat our method outperforms the baseline OAFA method by 3.6% in the mAP metric.\nCodes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.", "AI": {"tldr": "UAVD-Mamba, a multimodal UAV object detection framework, improves detection by using deformable tokens, multimodal feature fusion, and multiscale processing, outperforming baseline methods by 3.6% mAP.", "motivation": "Challenges like occlusions, small object sizes, and irregular shapes in UAV object detection necessitate a robust multimodal method.", "method": "Proposes UAVD-Mamba with Deformable Token Mamba Blocks (DTMBs) for RGB and IR modalities, multiscale feature stacking, and a Detection Neck for Mamba (DNM) inspired by YOLO.", "result": "Outperforms baseline OAFA by 3.6% mAP on the DroneVehicle dataset.", "conclusion": "UAVD-Mamba effectively addresses UAV object detection challenges through innovative multimodal and multiscale feature handling."}}
{"id": "2507.00953", "pdf": "https://arxiv.org/pdf/2507.00953", "abs": "https://arxiv.org/abs/2507.00953", "authors": ["Ke Liu", "Shuanke Shen", "Hao Chen"], "title": "From Sentences to Sequences: Rethinking Languages in Biological System", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "The paradigm of large language models in natural language processing (NLP)\nhas also shown promise in modeling biological languages, including proteins,\nRNA, and DNA. Both the auto-regressive generation paradigm and evaluation\nmetrics have been transferred from NLP to biological sequence modeling.\nHowever, the intrinsic structural correlations in natural and biological\nlanguages differ fundamentally. Therefore, we revisit the notion of language in\nbiological systems to better understand how NLP successes can be effectively\ntranslated to biological domains. By treating the 3D structure of biomolecules\nas the semantic content of a sentence and accounting for the strong\ncorrelations between residues or bases, we highlight the importance of\nstructural evaluation and demonstrate the applicability of the auto-regressive\nparadigm in biological language modeling. Code can be found at\n\\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}", "AI": {"tldr": "The paper explores adapting NLP language models to biological sequences, emphasizing structural correlations and evaluation.", "motivation": "To understand how NLP successes can be effectively applied to biological domains by addressing intrinsic structural differences.", "method": "Treats 3D biomolecular structure as semantic content and uses auto-regressive generation, accounting for residue/base correlations.", "result": "Demonstrates the applicability of the auto-regressive paradigm in biological language modeling with structural evaluation.", "conclusion": "Highlights the need for structural evaluation and shows potential for NLP-inspired models in biological sequence analysis."}}
{"id": "2507.00899", "pdf": "https://arxiv.org/pdf/2507.00899", "abs": "https://arxiv.org/abs/2507.00899", "authors": ["Carlos Vonessen", "Charles Harris", "Miruna Cretu", "Pietro Li\u00f2"], "title": "TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality", "categories": ["cs.LG"], "comment": null, "summary": "State-of-the-art models for 3D molecular generation are based on significant\ninductive biases, SE(3), permutation equivariance to respect symmetry and graph\nmessage-passing networks to capture local chemistry, yet the generated\nmolecules still struggle with physical plausibility. We introduce TABASCO which\nrelaxes these assumptions: The model has a standard non-equivariant transformer\narchitecture, treats atoms in a molecule as sequences and reconstructs bonds\ndeterministically after generation. The absence of equivariant layers and\nmessage passing allows us to significantly simplify the model architecture and\nscale data throughput. On the GEOM-Drugs benchmark TABASCO achieves\nstate-of-the-art PoseBusters validity and delivers inference roughly 10x faster\nthan the strongest baseline, while exhibiting emergent rotational equivariance\ndespite symmetry not being hard-coded. Our work offers a blueprint for training\nminimalist, high-throughput generative models suited to specialised tasks such\nas structure- and pharmacophore-based drug design. We provide a link to our\nimplementation at github.com/carlosinator/tabasco.", "AI": {"tldr": "TABASCO is a non-equivariant transformer model for 3D molecular generation that simplifies architecture, improves speed, and achieves state-of-the-art validity without hard-coded symmetry.", "motivation": "Current 3D molecular generation models struggle with physical plausibility despite strong inductive biases. TABASCO aims to relax these assumptions for better performance and simplicity.", "method": "TABASCO uses a standard non-equivariant transformer, treats atoms as sequences, and reconstructs bonds deterministically post-generation.", "result": "Achieves state-of-the-art validity on GEOM-Drugs, 10x faster inference than baselines, and exhibits emergent rotational equivariance.", "conclusion": "TABASCO provides a minimalist, high-throughput approach for specialized tasks like drug design, with potential for broader applications."}}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137", "abs": "https://arxiv.org/abs/2506.23137", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "AI": {"tldr": "The paper introduces Flow-Modulated Scoring (FMS) for Knowledge Graph Completion, combining static and dynamic modeling to improve relational semantics.", "motivation": "Existing KGC methods rely on static embeddings, lacking contextual and dynamic relational understanding.", "method": "FMS uses semantic context learning and conditional flow-matching to dynamically refine static scores.", "result": "FMS outperforms state-of-the-art methods on standard benchmarks.", "conclusion": "FMS enhances KGC by integrating context-aware static and dynamic relational modeling."}}
{"id": "2507.00852", "pdf": "https://arxiv.org/pdf/2507.00852", "abs": "https://arxiv.org/abs/2507.00852", "authors": ["Fatemeh Sadat Daneshmand"], "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting", "categories": ["cs.CV"], "comment": null, "summary": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment.", "AI": {"tldr": "A computer vision system using Mask R-CNN enables robots to detect and grasp pen components in any orientation under varying lighting, improving flexibility and reducing setup time.", "motivation": "Industry 4.0 demands robots that can handle objects in unstructured environments without rigid positioning constraints.", "method": "A Mask R-CNN-based approach is implemented and tested on a pen manufacturing line, addressing detection without positional constraints, lighting robustness, and cost-effective cameras.", "result": "The system achieves 95% detection accuracy, reduces setup time by 30%, and enhances manufacturing flexibility.", "conclusion": "The validated system is practical for real-world industrial deployment, addressing key challenges in flexible manufacturing."}}
{"id": "2507.00969", "pdf": "https://arxiv.org/pdf/2507.00969", "abs": "https://arxiv.org/abs/2507.00969", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "title": "Surgical Neural Radiance Fields from One Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "AI": {"tldr": "Single-image NeRF training for surgical settings using preoperative MRI and neural style transfer, achieving high reconstruction fidelity.", "motivation": "NeRF's reliance on extensive multi-view data is impractical in surgical intraoperative settings due to time constraints. This work aims to enable NeRF training with limited intraoperative data.", "method": "Leverage preoperative MRI for camera viewpoints, transfer intraoperative image appearance via neural style transfer (WTC2 and STROTSS) to create a training dataset for single-image NeRF.", "result": "Evaluated on four neurosurgical cases, showing high reconstruction fidelity and stylistic alignment compared to ground truth and multi-view NeRF.", "conclusion": "Feasibility of single-image NeRF training in surgery is demonstrated, overcoming multi-view data limitations."}}
{"id": "2507.00920", "pdf": "https://arxiv.org/pdf/2507.00920", "abs": "https://arxiv.org/abs/2507.00920", "authors": ["Dang Qua Nguyen", "Morteza Hashemi", "Erik Perrins", "Sergiy A. Vorobyov", "David J. Love", "Taejoon Kim"], "title": "Privacy-Preserving Quantized Federated Learning with Diverse Precision", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.", "AI": {"tldr": "The paper proposes a novel stochastic quantizer (SQ) for federated learning (FL) to address privacy risks and quantization heterogeneity, ensuring differential privacy and minimal quantization error while improving learning utility.", "motivation": "FL faces challenges like privacy risks from unprotected model updates and reduced utility due to quantization heterogeneity. Existing solutions address only one issue, so the paper aims to tackle both simultaneously.", "method": "Introduces a stochastic quantizer (SQ) for differential privacy and minimal quantization error, alongside a cluster size optimization and linear fusion technique for accurate model aggregation.", "result": "Numerical simulations show the approach outperforms the conventional LaplaceSQ-FL in privacy protection and learning utility.", "conclusion": "The proposed SQ and optimization techniques effectively enhance FL by balancing privacy and utility under quantization heterogeneity."}}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146", "abs": "https://arxiv.org/abs/2506.23146", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "AI": {"tldr": "The paper introduces the Learning-to-Context Slope (LCS) metric to evaluate in-context learning (ICL) effectiveness in LLMs, addressing reliability, attribution, and data scarcity issues.", "motivation": "Current ICL evaluation methods are unreliable, poorly attributed, and impractical in data-insufficient scenarios, necessitating a better metric.", "method": "Proposes LCS, which models the slope between learning gain (loss decrease) and contextual relevance (demonstration-input relevance) to quantify ICL effectiveness.", "result": "LCS reliably correlates with performance improvements, works in biased/data-scarce scenarios, and identifies actionable thresholds and critical model capabilities.", "conclusion": "LCS is a robust metric for evaluating ICL effectiveness, offering practical insights for practitioners."}}
{"id": "2507.00861", "pdf": "https://arxiv.org/pdf/2507.00861", "abs": "https://arxiv.org/abs/2507.00861", "authors": ["Xiaoshuai Hao", "Lingdong Kong", "Rong Yin", "Pengwei Wang", "Jing Zhang", "Yunfeng Diao", "Shu Zhao"], "title": "SafeMap: Robust HD Map Construction from Incomplete Observations", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability.", "AI": {"tldr": "SafeMap is a novel framework for robust HD map construction in autonomous driving, addressing incomplete camera data with Gaussian-based reconstruction and distillation-based correction.", "motivation": "Existing methods struggle with incomplete multi-view camera data, necessitating a robust solution for accurate HD map construction.", "method": "SafeMap integrates Gaussian-based Perspective View Reconstruction (G-PVR) and Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) to dynamically prioritize informative regions and correct BEV representations.", "result": "SafeMap outperforms previous methods in both complete and incomplete scenarios, demonstrating superior performance and reliability.", "conclusion": "SafeMap provides a plug-and-play solution for robust HD map generation, enhancing accuracy even with missing camera views."}}
{"id": "2507.00971", "pdf": "https://arxiv.org/pdf/2507.00971", "abs": "https://arxiv.org/abs/2507.00971", "authors": ["Taeyoun Kim", "Fahim Tajwar", "Aditi Raghunathan", "Aviral Kumar"], "title": "Reasoning as an Adaptive Defense for Safety", "categories": ["cs.LG", "cs.AI"], "comment": "42 pages, 11 Figures, 7 Tables", "summary": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt.", "AI": {"tldr": "The paper introduces TARS, a reinforcement learning method to train LLMs for adaptive reasoning on safety, improving robustness against harmful requests and jailbreaks.", "motivation": "To enhance LLM robustness against safety vulnerabilities by adaptively allocating compute during reasoning, addressing issues like refusal shortcuts and reasoning degeneration.", "method": "TARS uses RL with chain-of-thought traces and a balanced reward signal, incorporating a warmstart SFT stage, diverse prompts, and a specific reward function.", "result": "TARS-trained models show adaptive compute use, better safety-refusal trade-offs, and improved robustness against attacks like GCG and PAIR.", "conclusion": "TARS provides an effective, open recipe for training LLMs to handle harmful requests by adaptive reasoning per prompt."}}
{"id": "2507.00927", "pdf": "https://arxiv.org/pdf/2507.00927", "abs": "https://arxiv.org/abs/2507.00927", "authors": ["Antonis Vasileiou", "Timo Stoll", "Christopher Morris"], "title": "Understanding Generalization in Node and Link Prediction", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.07106", "summary": "Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.", "AI": {"tldr": "The paper introduces a unified framework to analyze the generalization of MPNNs in node and link prediction, addressing gaps in understanding and unrealistic assumptions in prior work.", "motivation": "Current understanding of MPNNs' generalization in node- and link-level tasks is limited, with existing studies relying on unrealistic assumptions and neglecting graph structure.", "method": "A unified framework is proposed to analyze MPNN generalization, incorporating diverse architectural parameters, loss functions, and graph structure influence.", "result": "Empirical studies support theoretical insights, enhancing understanding of MPNNs' generalization in inductive and transductive settings.", "conclusion": "The framework advances MPNN generalization analysis, applicable beyond graphs to other classification tasks, and provides deeper insights into their capabilities."}}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431", "abs": "https://arxiv.org/abs/2506.23431", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "AI": {"tldr": "A new pipelined decoder architecture enables parallel text generation, improving speed without compromising quality or memory usage.", "motivation": "Autoregressive models generate tokens sequentially, creating a speed bottleneck. This paper aims to enable parallel generation while maintaining quality.", "method": "Proposes a pipelined decoder that generates multiple subsequences simultaneously, producing new tokens for each subsequence in parallel.", "result": "Experiments show significant speed improvements in tasks like question answering and summarization, with minimal quality loss.", "conclusion": "The pipelined decoder effectively addresses the speed bottleneck of autoregressive models without sacrificing quality or memory efficiency."}}
{"id": "2507.00868", "pdf": "https://arxiv.org/pdf/2507.00868", "abs": "https://arxiv.org/abs/2507.00868", "authors": ["Simon Rei\u00df", "Zdravko Marinov", "Alexander Jaus", "Constantin Seibold", "M. Saquib Sarfraz", "Erik Rodner", "Rainer Stiefelhagen"], "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "In this paper, we explore the potential of visual in-context learning to\nenable a single model to handle multiple tasks and adapt to new tasks during\ntest time without re-training. Unlike previous approaches, our focus is on\ntraining in-context learners to adapt to sequences of tasks, rather than\nindividual tasks. Our goal is to solve complex tasks that involve multiple\nintermediate steps using a single model, allowing users to define entire vision\npipelines flexibly at test time. To achieve this, we first examine the\nproperties and limitations of visual in-context learning architectures, with a\nparticular focus on the role of codebooks. We then introduce a novel method for\ntraining in-context learners using a synthetic compositional task generation\nengine. This engine bootstraps task sequences from arbitrary segmentation\ndatasets, enabling the training of visual in-context learners for compositional\ntasks. Additionally, we investigate different masking-based training objectives\nto gather insights into how to train models better for solving complex,\ncompositional tasks. Our exploration not only provides important insights\nespecially for multi-modal medical task sequences but also highlights\nchallenges that need to be addressed.", "AI": {"tldr": "The paper explores visual in-context learning for multi-task adaptation without retraining, focusing on sequences of tasks and introducing a synthetic task generation engine for training.", "motivation": "To enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training, particularly for complex, compositional tasks.", "method": "Examines visual in-context learning architectures, introduces a synthetic compositional task generation engine, and investigates masking-based training objectives.", "result": "Provides insights for multi-modal medical task sequences and identifies challenges in training models for complex tasks.", "conclusion": "The approach offers a flexible way to define vision pipelines at test time but highlights unresolved challenges."}}
{"id": "2507.00990", "pdf": "https://arxiv.org/pdf/2507.00990", "abs": "https://arxiv.org/abs/2507.00990", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project Page: https://rigvid-robot.github.io/", "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "AI": {"tldr": "RIGVid enables robots to perform complex tasks by imitating AI-generated videos, eliminating the need for physical demonstrations or robot-specific training.", "motivation": "To simplify robotic manipulation by leveraging AI-generated videos as a supervision source, avoiding costly real-world demonstrations.", "method": "Uses a video diffusion model to generate potential demonstrations, filters them with a vision-language model, extracts object trajectories via 6D pose tracking, and retargets them to the robot.", "result": "Filtered generated videos are as effective as real demonstrations, with performance improving with video quality. Generated videos outperform keypoint prediction, and 6D pose tracking is superior to other trajectory extraction methods.", "conclusion": "AI-generated videos can effectively supervise robotic manipulation, offering a scalable and efficient alternative to real demonstrations."}}
{"id": "2507.00945", "pdf": "https://arxiv.org/pdf/2507.00945", "abs": "https://arxiv.org/abs/2507.00945", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri"], "title": "Time Series Foundation Models are Flow Predictors", "categories": ["cs.LG", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2203.07372", "summary": "We investigate the effectiveness of time series foundation models (TSFMs) for\ncrowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three\nreal-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD\nflows-these models are deployed in a strict zero-shot setting, using only the\ntemporal evolution of each OD flow and no explicit spatial information. Moirai\nand TimesFM outperform both statistical and deep learning baselines, achieving\nup to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to\nstate-of-the-art competitors. Our results highlight the practical value of\nTSFMs for accurate, scalable flow prediction, even in scenarios with limited\nannotated data or missing spatial context.", "AI": {"tldr": "Time series foundation models (TSFMs) like Moirai and TimesFM outperform traditional methods in crowd flow prediction, achieving significant accuracy improvements without spatial data.", "motivation": "To evaluate the effectiveness of TSFMs in crowd flow prediction, especially in zero-shot settings with limited data.", "method": "Deployed Moirai and TimesFM on three mobility datasets (Bike NYC, Taxi Beijing, Spanish OD flows) in a zero-shot setting, using only temporal data.", "result": "TSFMs achieved up to 33% lower RMSE, 39% lower MAE, and 49% higher CPC compared to state-of-the-art methods.", "conclusion": "TSFMs are highly effective for scalable and accurate flow prediction, even without spatial context or abundant annotated data."}}
{"id": "2506.23743", "pdf": "https://arxiv.org/pdf/2506.23743", "abs": "https://arxiv.org/abs/2506.23743", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.", "AI": {"tldr": "The study quantifies positional bias in binary question answering across five large language models, showing it grows exponentially with increased uncertainty.", "motivation": "To analyze how positional bias in models varies with answer uncertainty, using adapted datasets and benchmarks.", "method": "Adapted SQuAD-it dataset with incorrect options, created versions with varying uncertainty, and evaluated benchmarks (WebGPT, Winning Arguments). Flipped correct option order to measure bias.", "result": "Positional bias is minimal in low-uncertainty conditions but increases exponentially with higher uncertainty.", "conclusion": "Positional bias in models is strongly influenced by answer uncertainty, highlighting the need for bias mitigation in uncertain scenarios."}}
{"id": "2507.00886", "pdf": "https://arxiv.org/pdf/2507.00886", "abs": "https://arxiv.org/abs/2507.00886", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Xi Wang", "Danda Pani Paudel", "Luc Van Gool"], "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As multimodal language models advance, their application to 3D scene\nunderstanding is a fast-growing frontier, driving the development of 3D\nVision-Language Models (VLMs). Current methods show strong dependence on object\ndetectors, introducing processing bottlenecks and limitations in taxonomic\nflexibility. To address these limitations, we propose a scene-centric 3D VLM\nfor 3D Gaussian splat scenes that employs language- and task-aware scene\nrepresentations. Our approach directly embeds rich linguistic features into the\n3D scene representation by associating language with each Gaussian primitive,\nachieving early modality alignment. To process the resulting dense\nrepresentations, we introduce a dual sparsifier that distills them into\ncompact, task-relevant tokens via task-guided and location-guided pathways,\nproducing sparse, task-aware global and local scene tokens. Notably, we present\nthe first Gaussian splatting-based VLM, leveraging photorealistic 3D\nrepresentations derived from standard RGB images, demonstrating strong\ngeneralization: it improves performance of prior 3D VLM five folds, in\nout-of-the-domain settings.", "AI": {"tldr": "A scene-centric 3D Vision-Language Model (VLM) using Gaussian splatting embeds linguistic features directly into 3D scenes, improving flexibility and performance over detector-dependent methods.", "motivation": "Current 3D VLMs rely heavily on object detectors, causing bottlenecks and limited taxonomic flexibility.", "method": "Proposes a scene-centric 3D VLM with language- and task-aware representations, embedding linguistic features into Gaussian primitives and using a dual sparsifier for compact tokens.", "result": "Achieves five-fold performance improvement over prior 3D VLMs in out-of-domain settings.", "conclusion": "The method demonstrates strong generalization and advances 3D scene understanding by leveraging photorealistic representations."}}
{"id": "2507.01003", "pdf": "https://arxiv.org/pdf/2507.01003", "abs": "https://arxiv.org/abs/2507.01003", "authors": ["Eun-Ji Park", "Sangwon Yun"], "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 figures", "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation we present a unified framework\nfor understanding and accelerating the training of deep neural networks via\nstochastic gradient descent. By analyzing the geometric landscape of the\nobjective function we introduce a practical diagnostic, the running estimate of\nthe largest Lyapunov exponent, which provably distinguishes genuine convergence\ntoward stable minimizers from mere statistical stabilization near saddle\npoints. We then propose a ghost category extension for standard classifiers\nthat adds auxiliary ghost output nodes so the model gains extra descent\ndirections that open a lateral corridor around narrow loss barriers and enable\nthe optimizer to bypass poor basins during the early training phase. We show\nthat this extension strictly reduces approximation error and that after\nsufficient convergence the ghost dimensions collapse and the extended model's\ninvariant law coincides with that of the original and there exists a path in\nthe enlarged parameter space along which the total loss does not increase while\nthe original loss decreases by an arbitrary margin. Taken together these\nresults provide a principled architecture level intervention that accelerates\nearly stage trainability while preserving asymptotic behavior.", "AI": {"tldr": "A unified framework for accelerating deep neural network training via stochastic gradient descent, using Lyapunov exponents to distinguish convergence from stabilization, and introducing ghost output nodes to bypass poor basins early on.", "motivation": "To improve the training of deep neural networks by distinguishing genuine convergence from stabilization near saddle points and enabling the optimizer to bypass poor basins.", "method": "Analyze the geometric landscape of the objective function, introduce a diagnostic using Lyapunov exponents, and extend classifiers with ghost output nodes for extra descent directions.", "result": "The ghost extension reduces approximation error, collapses after convergence, and preserves the original model's behavior while accelerating early training.", "conclusion": "The proposed framework provides a principled way to enhance early-stage trainability without compromising asymptotic performance."}}
{"id": "2507.00964", "pdf": "https://arxiv.org/pdf/2507.00964", "abs": "https://arxiv.org/abs/2507.00964", "authors": ["Jack Foxabbott", "Arush Tagade", "Andrew Cusick", "Robbie McCorkell", "Leo McKee-Reid", "Jugal Patel", "Jamie Rumbelow", "Jessica Rumbelow", "Zohreh Shams"], "title": "Benchmarking the Discovery Engine", "categories": ["cs.LG", "I.2.6; I.2.3; I.5.1; H.2.8; J.2; J.3; J.4"], "comment": "16 pages, 8 figures, benchmarks Discovery Engine on five scientific\n  datasets (medicine, materials science, climate, air quality, social science)", "summary": "The Discovery Engine is a general purpose automated system for scientific\ndiscovery, which combines machine learning with state-of-the-art ML\ninterpretability to enable rapid and robust scientific insight across diverse\ndatasets. In this paper, we benchmark the Discovery Engine against five recent\npeer-reviewed scientific publications applying machine learning across\nmedicine, materials science, social science, and environmental science. In each\ncase, the Discovery Engine matches or exceeds prior predictive performance\nwhile also generating deeper, more actionable insights through rich\ninterpretability artefacts. These results demonstrate its potential as a new\nstandard for automated, interpretable scientific modelling that enables complex\nknowledge discovery from data.", "AI": {"tldr": "The Discovery Engine is an automated system for scientific discovery, outperforming or matching five peer-reviewed studies in predictive performance while providing deeper insights through interpretability.", "motivation": "To demonstrate the Discovery Engine's capability as a general-purpose tool for automated, interpretable scientific modeling across diverse fields.", "method": "Benchmarked the Discovery Engine against five peer-reviewed studies in medicine, materials science, social science, and environmental science, using machine learning and interpretability techniques.", "result": "The Discovery Engine matched or exceeded prior predictive performance and generated more actionable insights with interpretability artefacts.", "conclusion": "The Discovery Engine shows potential as a new standard for automated, interpretable scientific modeling, enabling complex knowledge discovery."}}
{"id": "2506.23940", "pdf": "https://arxiv.org/pdf/2506.23940", "abs": "https://arxiv.org/abs/2506.23940", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.", "AI": {"tldr": "A framework for unifying domain-specific MLLMs via parameter integration, using CAPS for efficient fusion and domain compatibility scoring for alignment.", "motivation": "Addressing the fragmentation of knowledge in domain-specific MLLMs and enabling modular composition of expert capabilities.", "method": "Proposes Compatibility-Aware Parameter Splicing (CAPS) for selective parameter fusion, extended to low-rank adaptation layers, and introduces domain compatibility scoring.", "result": "Effective integration of heterogeneous expertise validated across diverse multimodal benchmarks.", "conclusion": "Offers a scalable solution for compositional, domain-adaptive MLLMs."}}
{"id": "2507.00916", "pdf": "https://arxiv.org/pdf/2507.00916", "abs": "https://arxiv.org/abs/2507.00916", "authors": ["Tianshi Cao", "Marie-Julie Rakotosaona", "Ben Poole", "Federico Tombari", "Michael Niemeyer"], "title": "Masks make discriminative models great again!", "categories": ["cs.CV"], "comment": null, "summary": "We present Image2GS, a novel approach that addresses the challenging problem\nof reconstructing photorealistic 3D scenes from a single image by focusing\nspecifically on the image-to-3D lifting component of the reconstruction\nprocess. By decoupling the lifting problem (converting an image to a 3D model\nrepresenting what is visible) from the completion problem (hallucinating\ncontent not present in the input), we create a more deterministic task suitable\nfor discriminative models. Our method employs visibility masks derived from\noptimized 3D Gaussian splats to exclude areas not visible from the source view\nduring training. This masked training strategy significantly improves\nreconstruction quality in visible regions compared to strong baselines.\nNotably, despite being trained only on masked regions, Image2GS remains\ncompetitive with state-of-the-art discriminative models trained on full target\nimages when evaluated on complete scenes. Our findings highlight the\nfundamental struggle discriminative models face when fitting unseen regions and\ndemonstrate the advantages of addressing image-to-3D lifting as a distinct\nproblem with specialized techniques.", "AI": {"tldr": "Image2GS improves 3D scene reconstruction from a single image by focusing on visible regions using masked training, outperforming baselines.", "motivation": "Addressing the challenge of reconstructing photorealistic 3D scenes from a single image by separating the lifting (visible regions) and completion (unseen regions) tasks.", "method": "Uses visibility masks from optimized 3D Gaussian splats to train discriminative models only on visible regions, improving reconstruction quality.", "result": "Outperforms baselines in visible regions and remains competitive with state-of-the-art models trained on full images.", "conclusion": "Specializing in image-to-3D lifting with masked training yields better results, highlighting the difficulty of fitting unseen regions."}}
{"id": "2507.01006", "pdf": "https://arxiv.org/pdf/2507.01006", "abs": "https://arxiv.org/abs/2507.01006", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "AI": {"tldr": "GLM-4.1V-Thinking is a vision-language model (VLM) that excels in multimodal reasoning, outperforming comparable models and even larger ones like Qwen2.5-VL-72B. It uses a reasoning-centric training framework and Reinforcement Learning with Curriculum Sampling (RLCS) to enhance capabilities across diverse tasks.", "motivation": "To advance general-purpose multimodal reasoning by developing a capable vision-language model with state-of-the-art performance.", "method": "Large-scale pre-training for the vision foundation model, followed by Reinforcement Learning with Curriculum Sampling (RLCS) to unlock full potential.", "result": "Achieves SOTA performance among comparable models, outperforming Qwen2.5-VL-7B on nearly all tasks and matching/surpassing Qwen2.5-VL-72B on 18 benchmarks. Also competes with closed-source models like GPT-4o on challenging tasks.", "conclusion": "GLM-4.1V-Thinking demonstrates strong capabilities in multimodal reasoning, making it a competitive open-source model for research and practical applications."}}
{"id": "2507.00965", "pdf": "https://arxiv.org/pdf/2507.00965", "abs": "https://arxiv.org/abs/2507.00965", "authors": ["F\u00e9lix Lefebvre", "Ga\u00ebl Varoquaux"], "title": "Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Many machine learning tasks can benefit from external knowledge. Large\nknowledge graphs store such knowledge, and embedding methods can be used to\ndistill it into ready-to-use vector representations for downstream\napplications. For this purpose, current models have however two limitations:\nthey are primarily optimized for link prediction, via local contrastive\nlearning, and they struggle to scale to the largest graphs due to GPU memory\nlimits. To address these, we introduce SEPAL: a Scalable Embedding Propagation\nALgorithm for large knowledge graphs designed to produce high-quality\nembeddings for downstream tasks at scale. The key idea of SEPAL is to enforce\nglobal embedding alignment by optimizing embeddings only on a small core of\nentities, and then propagating them to the rest of the graph via message\npassing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream\nmachine learning tasks. Our results show that SEPAL significantly outperforms\nprevious methods on downstream tasks. In addition, SEPAL scales up its base\nembedding model, enabling fitting huge knowledge graphs on commodity hardware.", "AI": {"tldr": "SEPAL is a scalable embedding propagation algorithm for large knowledge graphs, designed to improve downstream task performance by optimizing embeddings on a core set of entities and propagating them globally.", "motivation": "Current embedding methods for knowledge graphs are limited by their focus on link prediction and scalability issues with large graphs.", "method": "SEPAL optimizes embeddings on a small core of entities and propagates them globally via message passing, addressing scalability and downstream task performance.", "result": "SEPAL outperforms previous methods on 46 downstream tasks and scales efficiently on large graphs using commodity hardware.", "conclusion": "SEPAL provides a scalable and effective solution for embedding large knowledge graphs, enhancing performance in downstream applications."}}
{"id": "2506.24117", "pdf": "https://arxiv.org/pdf/2506.24117", "abs": "https://arxiv.org/abs/2506.24117", "authors": ["David M. Smiley"], "title": "Intertextual Parallel Detection in Biblical Hebrew: A Transformer-Based Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Identifying parallel passages in biblical Hebrew (BH) is central to biblical\nscholarship for understanding intertextual relationships. Traditional methods\nrely on manual comparison, a labor-intensive process prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between Samuel/Kings and\nChronicles, I assessed each model's capability to generate word embeddings\ndistinguishing parallel from non-parallel passages. Using cosine similarity and\nWasserstein Distance measures, I found that E5 and AlephBERT show promise; E5\nexcels in parallel detection, while AlephBERT demonstrates stronger\nnon-parallel differentiation. These findings indicate that pre-trained models\ncan enhance the efficiency and accuracy of detecting intertextual parallels in\nancient texts, suggesting broader applications for ancient language studies.", "AI": {"tldr": "The study evaluates transformer-based models (E5, AlephBERT, MPNet, LaBSE) for detecting parallel passages in biblical Hebrew, finding E5 and AlephBERT effective for parallel and non-parallel differentiation, respectively.", "motivation": "Traditional manual comparison of biblical Hebrew texts is labor-intensive and error-prone, prompting the need for automated methods.", "method": "Pre-trained transformer models (E5, AlephBERT, MPNet, LaBSE) were tested using cosine similarity and Wasserstein Distance to detect parallels in Samuel/Kings and Chronicles.", "result": "E5 excels in detecting parallels, while AlephBERT better differentiates non-parallel passages.", "conclusion": "Pre-trained models can improve efficiency and accuracy in detecting intertextual parallels, with potential applications in ancient language studies."}}
{"id": "2507.00980", "pdf": "https://arxiv.org/pdf/2507.00980", "abs": "https://arxiv.org/abs/2507.00980", "authors": ["Yuheng Du", "Sheng Yang", "Lingxuan Wang", "Zhenghua Hou", "Chengying Cai", "Zhitao Tan", "Mingxia Chen", "Shi-Sheng Huang", "Qiang Li"], "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization", "categories": ["cs.CV"], "comment": null, "summary": "While recent online HD mapping methods relieve burdened offline pipelines and\nsolve map freshness, they remain limited by perceptual inaccuracies, occlusion\nin dense traffic, and an inability to fuse multi-agent observations. We propose\nRTMap to enhance these single-traversal methods by persistently crowdsourcing a\nmulti-traversal HD map as a self-evolutional memory. On onboard agents, RTMap\nsimultaneously addresses three core challenges in an end-to-end fashion: (1)\nUncertainty-aware positional modeling for HD map elements, (2)\nprobabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)\nreal-time detection for possible road structural changes. Experiments on\nseveral public autonomous driving datasets demonstrate our solid performance on\nboth the prior-aided map quality and the localization accuracy, demonstrating\nour effectiveness of robustly serving downstream prediction and planning\nmodules while gradually improving the accuracy and freshness of the\ncrowdsourced prior-map asynchronously. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RTMap (Camera ready version\nincorporating reviewer suggestions will be updated soon).", "AI": {"tldr": "RTMap enhances single-traversal HD mapping by crowdsourcing a multi-traversal map, addressing uncertainty, localization, and real-time updates for improved accuracy and freshness.", "motivation": "Current online HD mapping methods suffer from perceptual inaccuracies, occlusion, and lack of multi-agent fusion, limiting their effectiveness.", "method": "RTMap uses uncertainty-aware modeling, probabilistic localization, and real-time detection to crowdsource and update HD maps.", "result": "Experiments show improved map quality and localization accuracy, benefiting downstream tasks like prediction and planning.", "conclusion": "RTMap effectively improves HD map accuracy and freshness while supporting robust autonomous driving applications."}}
{"id": "2203.09952", "pdf": "https://arxiv.org/pdf/2203.09952", "abs": "https://arxiv.org/abs/2203.09952", "authors": ["Kefan Jin", "Xingyao Han"], "title": "Conquering Ghosts: Relation Learning for Information Reliability Representation and End-to-End Robust Navigation", "categories": ["cs.AI"], "comment": null, "summary": "Environmental disturbances, such as sensor data noises, various lighting\nconditions, challenging weathers and external adversarial perturbations, are\ninevitable in real self-driving applications. Existing researches and testings\nhave shown that they can severely influence the vehicles perception ability and\nperformance, one of the main issue is the false positive detection, i.e., the\nghost object which is not real existed or occurs in the wrong position (such as\na non-existent vehicle). Traditional navigation methods tend to avoid every\ndetected objects for safety, however, avoiding a ghost object may lead the\nvehicle into a even more dangerous situation, such as a sudden break on the\nhighway. Considering the various disturbance types, it is difficult to address\nthis issue at the perceptual aspect. A potential solution is to detect the\nghost through relation learning among the whole scenario and develop an\nintegrated end-to-end navigation system. Our underlying logic is that the\nbehavior of all vehicles in the scene is influenced by their neighbors, and\nnormal vehicles behave in a logical way, while ghost vehicles do not. By\nlearning the spatio-temporal relation among surrounding vehicles, an\ninformation reliability representation is learned for each detected vehicle and\nthen a robot navigation network is developed. In contrast to existing works, we\nencourage the network to learn how to represent the reliability and how to\naggregate all the information with uncertainties by itself, thus increasing the\nefficiency and generalizability. To the best of the authors knowledge, this\npaper provides the first work on using graph relation learning to achieve\nend-to-end robust navigation in the presence of ghost vehicles. Simulation\nresults in the CARLA platform demonstrate the feasibility and effectiveness of\nthe proposed method in various scenarios.", "AI": {"tldr": "The paper proposes a graph relation learning method to detect ghost objects in self-driving scenarios and integrates it into an end-to-end navigation system for robust performance.", "motivation": "Environmental disturbances like sensor noise and adversarial perturbations cause false positives (ghost objects), which traditional navigation methods fail to handle safely.", "method": "The approach learns spatio-temporal relations among vehicles to assess information reliability and integrates this into a navigation network.", "result": "Simulations in CARLA show the method effectively handles ghost objects in diverse scenarios.", "conclusion": "The work is the first to use graph relation learning for end-to-end robust navigation against ghost vehicles, improving safety and generalizability."}}
{"id": "2507.01004", "pdf": "https://arxiv.org/pdf/2507.01004", "abs": "https://arxiv.org/abs/2507.01004", "authors": ["Yuhong Chou", "Zehao Liu", "Ruijie Zhu", "Xinyi Wan", "Tianjian Li", "Congying Chu", "Qian Liu", "Jibin Wu", "Zejun Ma"], "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention", "categories": ["cs.LG"], "comment": null, "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.", "AI": {"tldr": "ZeCO introduces a Zero Communication Overhead sequence parallelism method for linear attention models, achieving near-linear scalability and significant speedups for long sequence training.", "motivation": "Existing Sequence Parallelism (SP) methods cause substantial communication overhead, becoming a bottleneck for efficient processing of ultra-long sequences in LLMs.", "method": "ZeCO uses All-Scan, a new collective communication primitive, to eliminate communication overhead by providing each SP rank with the required initial operator state.", "result": "ZeCO achieves a 60% speedup on 256 GPUs with an 8M sequence length compared to the current SOTA SP method.", "conclusion": "ZeCO enables efficient training of next-generation LLMs on previously intractable sequence lengths."}}
{"id": "2407.19342", "pdf": "https://arxiv.org/pdf/2407.19342", "abs": "https://arxiv.org/abs/2407.19342", "authors": ["Aochuan Chen", "Jiashun Cheng", "Zijing Liu", "Ziqi Gao", "Fugee Tsung", "Yu Li", "Jia Li"], "title": "Parameter-Efficient Fine-Tuning via Circular Convolution", "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025", "summary": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose Circular Convolution Adaptation (C$^3$A), which not only\nachieves high-rank adaptation with enhanced performance but also excels in both\ncomputational power and memory utilization. Extensive experiments demonstrate\nthat C$^3$A consistently outperforms LoRA and its variants across various\nfine-tuning tasks.", "AI": {"tldr": "C\u00b3A (Circular Convolution Adaptation) outperforms LoRA in fine-tuning large models by achieving high-rank adaptation while maintaining computational and memory efficiency.", "motivation": "LoRA's low-rank characteristic may limit performance, and existing variants often sacrifice efficiency. C\u00b3A aims to enhance performance without compromising efficiency.", "method": "Proposes C\u00b3A, which uses circular convolution for high-rank adaptation, improving performance while retaining computational and memory efficiency.", "result": "C\u00b3A consistently outperforms LoRA and its variants in various fine-tuning tasks.", "conclusion": "C\u00b3A offers a superior alternative to LoRA, balancing high-rank adaptation with efficiency."}}
{"id": "2507.00981", "pdf": "https://arxiv.org/pdf/2507.00981", "abs": "https://arxiv.org/abs/2507.00981", "authors": ["Jack Nugent", "Siyang Wu", "Zeyu Ma", "Beining Han", "Meenal Parakh", "Abhishek Joshi", "Lingjie Mei", "Alexander Raistrick", "Xinyuan Li", "Jia Deng"], "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval.", "AI": {"tldr": "The paper introduces PDE, a new benchmark for evaluating robustness in monocular depth estimation, addressing gaps in standard benchmarks.", "motivation": "Standard benchmarks for monocular depth estimation focus on accuracy but lack robustness evaluation, prompting the need for a more comprehensive assessment tool.", "method": "PDE uses procedural generation to create 3D scenes with controlled perturbations (e.g., object, camera, material, lighting changes) to test model robustness.", "result": "The analysis identifies challenging perturbations for state-of-the-art depth models, providing insights for future research.", "conclusion": "PDE offers a systematic way to evaluate robustness in depth estimation, with findings that can guide further advancements in the field."}}
{"id": "2408.10774", "pdf": "https://arxiv.org/pdf/2408.10774", "abs": "https://arxiv.org/abs/2408.10774", "authors": ["Chenxing Wei", "Yao Shu", "Ying Tiffany He", "Fei Richard Yu"], "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "40 pages, 15 figures", "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.", "AI": {"tldr": "The paper introduces Flexora, a method to improve LoRA by automatically selecting key layers for fine-tuning, enhancing performance on downstream tasks.", "motivation": "Large Language Models (LLMs) face performance limitations in specific tasks due to knowledge boundaries. Fine-tuning methods like LoRA can overfit, reducing effectiveness.", "method": "Flexora frames layer selection as a hyperparameter optimization problem, solved using unrolled differentiation, and selects optimal layers for fine-tuning.", "result": "Experiments show Flexora consistently outperforms baselines across pretrained models and tasks.", "conclusion": "Flexora effectively addresses LoRA's overfitting, improving task performance, with theoretical and empirical support."}}
{"id": "2507.00005", "pdf": "https://arxiv.org/pdf/2507.00005", "abs": "https://arxiv.org/abs/2507.00005", "authors": ["Vasavi Lankipalle"], "title": "SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning", "categories": ["cs.NE", "cs.LG"], "comment": "6", "summary": "Disaster response requires rapid, adaptive decision-making in chaotic\nenvironments. SwarmFusion, a novel hybrid framework, integrates particle swarm\noptimization with convolutional neural networks to optimize real-time resource\nallocation and path planning. By processing live satellite, drone, and sensor\ndata, SwarmFusion enhances situational awareness and operational efficiency in\nflood and wildfire scenarios. Simulations using the DisasterSim2025 dataset\ndemonstrate up to 40 percentage faster response times and 90 percentage\nsurvivor coverage compared to baseline methods. This scalable, data-driven\napproach offers a transformative solution for time-critical disaster\nmanagement, with potential applications across diverse crisis scenarios.", "AI": {"tldr": "SwarmFusion combines particle swarm optimization and CNNs for faster disaster response, showing 40% quicker times and 90% survivor coverage.", "motivation": "Disaster response needs rapid, adaptive decision-making in chaotic environments.", "method": "Hybrid framework integrating particle swarm optimization with CNNs, using real-time data from satellites, drones, and sensors.", "result": "40% faster response times and 90% survivor coverage in simulations.", "conclusion": "Scalable, data-driven solution for time-critical disaster management with broad applications."}}
{"id": "2409.20302", "pdf": "https://arxiv.org/pdf/2409.20302", "abs": "https://arxiv.org/abs/2409.20302", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang"], "title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "15 pages, 8 figures, 1 table", "summary": "Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose a fresh approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurements for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in an OV testbed originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some apparent false mappings\ndetected by OV systems are not actually untrue.", "AI": {"tldr": "The paper proposes a new ontology versioning (OV) approach using ontology matching (OM) techniques, introducing the OM4OV pipeline and a cross-reference mechanism to improve performance.", "motivation": "The dynamic nature of the Semantic Web and growing ontology sizes overwhelm current OV methods, necessitating a more efficient solution.", "method": "The paper introduces the OM4OV pipeline, reformulates OV tasks using OM techniques, and proposes a cross-reference mechanism for optimization.", "result": "Experimental validation on OAEI datasets shows improved OV performance, with insights into false mappings in OV tasks.", "conclusion": "The OM4OV pipeline and cross-reference mechanism effectively enhance OV, addressing scalability and manual error challenges."}}
{"id": "2507.00992", "pdf": "https://arxiv.org/pdf/2507.00992", "abs": "https://arxiv.org/abs/2507.00992", "authors": ["Yuanrui Wang", "Cong Han", "YafeiLi", "Zhipeng Jin", "Xiawei Li", "SiNan Du", "Wen Tao", "Yi Yang", "shuanglong li", "Chun Yuan", "Liu Lin"], "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness.", "AI": {"tldr": "A novel segmentation-guided framework for text-to-image generation improves visual text rendering by using pixel-level text masks, achieving state-of-the-art performance on benchmarks.", "motivation": "Accurate visual text rendering in text-to-image generation is hindered by issues like blurred glyphs and semantic drift, with existing methods being inflexible and complex.", "method": "Proposes a framework with a fine-tuned bilingual segmentation model for text mask extraction and a streamlined diffusion model with adaptive glyph conditioning and region-specific loss.", "result": "Outperforms prior methods on the AnyText benchmark and introduces new benchmarks (GlyphMM-benchmark and MiniText-benchmark) for rigorous evaluation.", "conclusion": "The approach excels in small text rendering and complex layouts, demonstrating strong generalization and readiness for deployment."}}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952", "abs": "https://arxiv.org/abs/2505.02952", "authors": ["Fabrizio Marozzo"], "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output.", "AI": {"tldr": "The paper introduces an iterative method to refine ambiguous natural language prompts in generative AI, improving accuracy and user satisfaction over one-shot solutions.", "motivation": "Natural language ambiguity in generative AI leads to imprecise outputs, requiring users to manually iterate corrections.", "method": "The approach uses structured clarification questions and alternative solutions, supported by input/output examples, to resolve ambiguities systematically.", "result": "The method outperforms one-shot solutions in accuracy, resolution time, and user satisfaction across coding, data analysis, and creative writing tasks.", "conclusion": "The iterative approach effectively reduces ambiguity in generative AI interactions, enhancing precision and user experience."}}
{"id": "2411.19906", "pdf": "https://arxiv.org/pdf/2411.19906", "abs": "https://arxiv.org/abs/2411.19906", "authors": ["Ali Lotfi", "Ian McQuillan", "Steven Rayan"], "title": "A Graph-Based Classical and Quantum Approach to Deterministic L-System Inference", "categories": ["quant-ph", "cs.CL", "cs.DS", "cs.FL", "cs.LG"], "comment": "17 pages, 1 figure", "summary": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a massively time-consuming process.\nIt would be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-systems) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem.", "AI": {"tldr": "Automated inference of deterministic context-free L-systems (D0L-systems) from string sequences using characteristic graphs, translating the problem into MIS and SAT, with classical and quantum solutions.", "motivation": "Manual L-system modeling is time-consuming; automating it from data (e.g., string sequences) would be impactful.", "method": "Introduce characteristic graphs to translate D0L-system inference into MIS and SAT problems, solved via classical exact and approximate quantum algorithms.", "result": "Proposes polynomial-time translation to MIS/SAT and algorithms for solving the problem.", "conclusion": "Automated D0L-system inference is feasible with graph-based methods and quantum approaches."}}
{"id": "2507.01009", "pdf": "https://arxiv.org/pdf/2507.01009", "abs": "https://arxiv.org/abs/2507.01009", "authors": ["Anna Foix Romero", "Craig Russell", "Alexander Krull", "Virginie Uhlmann"], "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The shape of objects is an important source of visual information in a wide\nrange of applications. One of the core challenges of shape quantification is to\nensure that the extracted measurements remain invariant to transformations that\npreserve an object's intrinsic geometry, such as changing its size,\norientation, and position in the image. In this work, we introduce ShapeEmbed,\na self-supervised representation learning framework designed to encode the\ncontour of objects in 2D images, represented as a Euclidean distance matrix,\ninto a shape descriptor that is invariant to translation, scaling, rotation,\nreflection, and point indexing. Our approach overcomes the limitations of\ntraditional shape descriptors while improving upon existing state-of-the-art\nautoencoder-based approaches. We demonstrate that the descriptors learned by\nour framework outperform their competitors in shape classification tasks on\nnatural and biological images. We envision our approach to be of particular\nrelevance to biological imaging applications.", "AI": {"tldr": "ShapeEmbed is a self-supervised framework for learning invariant shape descriptors from 2D object contours, outperforming traditional and autoencoder-based methods in classification tasks.", "motivation": "Shape quantification often requires invariance to geometric transformations (e.g., translation, scaling). Existing methods have limitations, prompting the need for a robust solution.", "method": "ShapeEmbed uses a Euclidean distance matrix to encode object contours into invariant descriptors via self-supervised learning.", "result": "The framework outperforms competitors in shape classification tasks on natural and biological images.", "conclusion": "ShapeEmbed is a promising tool, especially for biological imaging, due to its robustness and superior performance."}}
{"id": "2505.06096", "pdf": "https://arxiv.org/pdf/2505.06096", "abs": "https://arxiv.org/abs/2505.06096", "authors": ["Sam Bush", "Matthew DeLorenzo", "Phat Tieu", "Jeyavijayan Rajendran"], "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs", "categories": ["cs.AI"], "comment": "Accepted at DAC 2025", "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.", "AI": {"tldr": "The paper addresses copyright risks in LLM-generated Verilog codes by introducing FreeSet, an open-source dataset, and FreeV, a fine-tuned Llama model, reducing violation rates to 3% and improving Verilog generation.", "motivation": "Existing hardware datasets for LLM fine-tuning are limited and pose copyright risks, prompting the need for a safer, open-source alternative.", "method": "Proposes an evaluation benchmark for copyright risk, curates the FreeSet dataset, and fine-tunes the Llama model (FreeV) using continual pre-training.", "result": "FreeV achieves a 3% copyright violation rate and improves VerilogEval pass@10 rates by over 10%.", "conclusion": "The FreeSet dataset and FreeV model mitigate copyright risks while enhancing Verilog generation performance."}}
{"id": "2412.03704", "pdf": "https://arxiv.org/pdf/2412.03704", "abs": "https://arxiv.org/abs/2412.03704", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Linjie Li", "Hongjin Lu", "Yuancheng Xu", "Chung-Ching Lin", "Kevin Lin", "Furong Huang", "Lijuan Wang"], "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite significant advancements in vision-language models (VLMs), there\nlacks effective approaches to enhance response quality by scaling\ninference-time computation. This capability is known to be a core step towards\nthe self-improving models in recent large language model studies. In this\npaper, we present Vision Value Model (VisVM) that can guide VLM inference-time\nsearch to generate responses with better visual comprehension. Specifically,\nVisVM not only evaluates the generated sentence quality in the current search\nstep, but also anticipates the quality of subsequent sentences that may result\nfrom the current step, thus providing a long-term value. In this way, VisVM\nsteers VLMs away from generating sentences prone to hallucinations or\ninsufficient detail, thereby producing higher quality responses. Experimental\nresults demonstrate that VisVM-guided search significantly enhances VLMs'\nability to generate descriptive captions with richer visual details and fewer\nhallucinations, compared with greedy decoding and search methods with other\nvisual reward signals. Furthermore, we find that self-training the model with\nthe VisVM-guided captions improve VLM's performance across a wide range of\nmultimodal benchmarks, indicating the potential for developing self-improving\nVLMs. Our value model and code are available at\nhttps://github.com/si0wang/VisVM.", "AI": {"tldr": "VisVM enhances vision-language models by guiding inference-time search for better visual comprehension, reducing hallucinations and improving detail.", "motivation": "Address the lack of effective methods to scale inference-time computation for improving response quality in VLMs.", "method": "Introduces Vision Value Model (VisVM) to evaluate and anticipate sentence quality during inference, providing long-term value.", "result": "VisVM significantly improves caption quality with richer details and fewer hallucinations, and boosts VLM performance via self-training.", "conclusion": "VisVM shows potential for self-improving VLMs, with code and model available for further development."}}
{"id": "2507.01012", "pdf": "https://arxiv.org/pdf/2507.01012", "abs": "https://arxiv.org/abs/2507.01012", "authors": ["Zhe Kong", "Le Li", "Yong Zhang", "Feng Gao", "Shaoshu Yang", "Tao Wang", "Kaihao Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Guanying Chen", "Wenhan Luo"], "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by ACM SIGGRAPH 2025, Homepage:\n  https://kongzhecn.github.io/projects/dam-vsr/ Github:\n  https://github.com/kongzhecn/DAM-VSR", "summary": "Real-world video super-resolution (VSR) presents significant challenges due\nto complex and unpredictable degradations. Although some recent methods utilize\nimage diffusion models for VSR and have shown improved detail generation\ncapabilities, they still struggle to produce temporally consistent frames. We\nattempt to use Stable Video Diffusion (SVD) combined with ControlNet to address\nthis issue. However, due to the intrinsic image-animation characteristics of\nSVD, it is challenging to generate fine details using only low-quality videos.\nTo tackle this problem, we propose DAM-VSR, an appearance and motion\ndisentanglement framework for VSR. This framework disentangles VSR into\nappearance enhancement and motion control problems. Specifically, appearance\nenhancement is achieved through reference image super-resolution, while motion\ncontrol is achieved through video ControlNet. This disentanglement fully\nleverages the generative prior of video diffusion models and the detail\ngeneration capabilities of image super-resolution models. Furthermore, equipped\nwith the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can\nconduct VSR on longer input videos. DAM-VSR achieves state-of-the-art\nperformance on real-world data and AIGC data, demonstrating its powerful detail\ngeneration capabilities.", "AI": {"tldr": "DAM-VSR is a framework for video super-resolution (VSR) that disentangles appearance enhancement and motion control, leveraging video diffusion and image super-resolution models for improved performance.", "motivation": "Addressing the challenge of generating temporally consistent frames in VSR due to complex degradations and the limitations of existing methods.", "method": "Proposes DAM-VSR, which separates VSR into appearance enhancement (using reference image super-resolution) and motion control (using video ControlNet), with a motion-aligned bidirectional sampling strategy.", "result": "Achieves state-of-the-art performance on real-world and AIGC data, demonstrating strong detail generation capabilities.", "conclusion": "DAM-VSR effectively combines generative priors and super-resolution models to enhance VSR performance, particularly in handling longer videos."}}
{"id": "2505.16459", "pdf": "https://arxiv.org/pdf/2505.16459", "abs": "https://arxiv.org/abs/2505.16459", "authors": ["Guiyao Tie", "Xueyang Zhou", "Tianhe Gu", "Ruihang Zhang", "Chaoran Hu", "Sizhe Zhang", "Mengqu Sun", "Yan Zhang", "Pan Zhou", "Lichao Sun"], "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks", "categories": ["cs.AI"], "comment": "39 pages, 28 figures, 4 tables", "summary": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled\nunified processing of language, vision, and structured inputs, opening the door\nto complex tasks such as logical deduction, spatial reasoning, and scientific\nanalysis. Despite their promise, the reasoning capabilities of MLLMs,\nparticularly those augmented with intermediate thinking traces (MLLMs-T),\nremain poorly understood and lack standardized evaluation benchmarks. Existing\nwork focuses primarily on perception or final answer correctness, offering\nlimited insight into how models reason or fail across modalities. To address\nthis gap, we introduce the MMMR, a new benchmark designed to rigorously\nevaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a\nhigh-difficulty dataset of 1,083 questions spanning six diverse reasoning types\nwith symbolic depth and multi-hop demands and 2) a modular Reasoning Trace\nEvaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy\nthrough metrics like relevance, consistency, and structured error annotations.\nEmpirical results show that MLLMs-T overall outperform non-thinking\ncounterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro\nsuffer from reasoning pathologies such as inconsistency and overthinking. This\nbenchmark reveals persistent gaps between accuracy and reasoning quality and\nprovides an actionable evaluation pipeline for future model development.\nOverall, the MMMR offers a scalable foundation for evaluating, comparing, and\nimproving the next generation of multi-modal reasoning systems.", "AI": {"tldr": "The paper introduces MMMR, a benchmark to evaluate multi-modal reasoning in MLLMs, highlighting gaps in reasoning quality despite improved accuracy.", "motivation": "Existing benchmarks lack focus on reasoning quality in MLLMs, particularly those with intermediate thinking traces (MLLMs-T).", "method": "MMMR includes a high-difficulty dataset (1,083 questions) and a Reasoning Trace Evaluation Pipeline (RTEP) to assess reasoning beyond accuracy.", "result": "MLLMs-T outperform non-thinking models but exhibit reasoning flaws like inconsistency. Top models (e.g., Claude-3.7-Sonnet) still struggle with reasoning quality.", "conclusion": "MMMR provides a scalable tool for evaluating and improving multi-modal reasoning, revealing gaps between accuracy and reasoning quality."}}
{"id": "2507.00260", "pdf": "https://arxiv.org/pdf/2507.00260", "abs": "https://arxiv.org/abs/2507.00260", "authors": ["Jin-Hong Du", "Kathryn Roeder", "Larry Wasserman"], "title": "Disentangled Feature Importance", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "26 main and 29 supplementary pages", "summary": "Feature importance quantification faces a fundamental challenge: when\npredictors are correlated, standard methods systematically underestimate their\ncontributions. We prove that major existing approaches target identical\npopulation functionals under squared-error loss, revealing why they share this\ncorrelation-induced bias.\n  To address this limitation, we introduce \\emph{Disentangled Feature\nImportance (DFI)}, a nonparametric generalization of the classical $R^2$\ndecomposition via optimal transport. DFI transforms correlated features into\nindependent latent variables using a transport map, eliminating correlation\ndistortion. Importance is computed in this disentangled space and attributed\nback through the transport map's sensitivity. DFI provides a principled\ndecomposition of importance scores that sum to the total predictive variability\nfor latent additive models and to interaction-weighted functional ANOVA\nvariances more generally, under arbitrary feature dependencies.\n  We develop a comprehensive semiparametric theory for DFI. For general\ntransport maps, we establish root-$n$ consistency and asymptotic normality of\nimportance estimators in the latent space, which extends to the original\nfeature space for the Bures-Wasserstein map. Notably, our estimators achieve\nsecond-order estimation error, which vanishes if both regression function and\ntransport map estimation errors are $o_{\\mathbb{P}}(n^{-1/4})$. By design, DFI\navoids the computational burden of repeated submodel refitting and the\nchallenges of conditional covariate distribution estimation, thereby achieving\ncomputational efficiency.", "AI": {"tldr": "The paper introduces Disentangled Feature Importance (DFI) to address bias in feature importance quantification caused by correlated predictors. DFI uses optimal transport to transform features into independent latent variables, providing unbiased importance scores.", "motivation": "Standard methods underestimate feature contributions due to correlations among predictors. This bias limits accurate importance quantification.", "method": "DFI employs optimal transport to disentangle correlated features into independent latent variables, computing importance in this space and attributing it back to original features. Theoretical guarantees include root-n consistency and asymptotic normality.", "result": "DFI provides unbiased importance scores, summing to total predictive variability for latent additive models and interaction-weighted functional ANOVA variances. It avoids computational burdens of submodel refitting.", "conclusion": "DFI offers a principled, efficient solution to correlation-induced bias in feature importance quantification, with strong theoretical and computational advantages."}}
{"id": "2412.06432", "pdf": "https://arxiv.org/pdf/2412.06432", "abs": "https://arxiv.org/abs/2412.06432", "authors": ["Marco Wrzalik", "Adrian Ulges", "Anne Uersfeld", "Florian Faust", "Viola Campos"], "title": "Integrating Expert Labels into LLM-based Emission Goal Detection: Example Selection vs Automatic Prompt Design", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": null, "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task.", "AI": {"tldr": "The paper compares dynamic few-shot example selection and automatic prompt optimization for detecting emission reduction goals in corporate reports, finding the latter superior.", "motivation": "Monitoring companies' climate change progress by detecting emission reduction goals in reports is crucial.", "method": "Compares dynamic few-shot example selection and automatic prompt optimization using a dataset of 769 climate-related passages.", "result": "Automatic prompt optimization outperforms dynamic few-shot selection, with combined methods offering limited benefits.", "conclusion": "Optimized prompts effectively capture the complexities of emission goal extraction, making them the preferred approach."}}
{"id": "2507.00190", "pdf": "https://arxiv.org/pdf/2507.00190", "abs": "https://arxiv.org/abs/2507.00190", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "title": "Rethink 3D Object Detection from Physical World", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages, 10 figures", "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics.", "AI": {"tldr": "The paper introduces new metrics (L-AP and P-AP) for evaluating 3D object detection in autonomous driving, addressing speed-accuracy trade-offs and real-world impact on motion planning.", "motivation": "Existing metrics like mAP and latency fail to address trade-offs between speed, accuracy, hardware differences, and their impact on collision avoidance in motion planning.", "method": "Proposes L-AP (latency-aware AP) and P-AP (planning-aware AP) as new metrics, validated using the nuPlan dataset. Also introduces L-HPO (latency-aware hyperparameter optimization) for model performance.", "result": "Demonstrates effectiveness of L-AP and P-AP, optimizes hardware and model selection, and debunks the assumption that more point clouds always improve recognition in real-time applications.", "conclusion": "The new metrics provide a more comprehensive evaluation for real-time 3D object detection, improving system performance and safety in autonomous driving."}}
{"id": "2505.20094", "pdf": "https://arxiv.org/pdf/2505.20094", "abs": "https://arxiv.org/abs/2505.20094", "authors": ["Qi Li", "Kun Li", "Haozhi Han", "Honghui Shang", "Xinfu He", "Yunquan Zhang", "Hong An", "Ting Cao", "Mao Yang"], "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale", "categories": ["cs.AI"], "comment": null, "summary": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence.", "AI": {"tldr": "SwarmThinkers is a reinforcement learning framework for atomic-scale simulations, combining physical consistency, interpretability, and scalability by modeling particles as decision-making agents.", "motivation": "Address the challenge of achieving physical consistency, interpretability, and scalability simultaneously in scientific simulations, which classical and learning-based methods fail to deliver.", "method": "Uses a reinforcement learning framework where particles act as agents with a shared policy network, trained under thermodynamic constraints, and employs a reweighting mechanism for statistical fidelity.", "result": "Achieves full-scale, physically consistent simulation on a single GPU, with up to 4963x faster computation and 485x lower memory usage compared to supercomputer-based methods.", "conclusion": "SwarmThinkers represents a paradigm shift by unifying physical consistency, interpretability, and scalability through agent-driven intelligence in scientific simulations."}}
{"id": "2507.00298", "pdf": "https://arxiv.org/pdf/2507.00298", "abs": "https://arxiv.org/abs/2507.00298", "authors": ["Arkaprabha Ganguli", "Nesar Ramachandra", "Julie Bessac", "Emil Constantinescu"], "title": "Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This study addresses the challenge of statistically extracting generative\nfactors from complex, high-dimensional datasets in unsupervised or\nsemi-supervised settings. We investigate encoder-decoder-based generative\nmodels for nonlinear dimensionality reduction, focusing on disentangling\nlow-dimensional latent variables corresponding to independent physical factors.\nIntroducing Aux-VAE, a novel architecture within the classical Variational\nAutoencoder framework, we achieve disentanglement with minimal modifications to\nthe standard VAE loss function by leveraging prior statistical knowledge\nthrough auxiliary variables. These variables guide the shaping of the latent\nspace by aligning latent factors with learned auxiliary variables. We validate\nthe efficacy of Aux-VAE through comparative assessments on multiple datasets,\nincluding astronomical simulations.", "AI": {"tldr": "Aux-VAE, a modified Variational Autoencoder, disentangles latent variables in high-dimensional data using auxiliary variables, validated on datasets like astronomical simulations.", "motivation": "To extract generative factors from complex datasets in unsupervised/semi-supervised settings, focusing on disentangling latent variables tied to physical factors.", "method": "Introduces Aux-VAE, which modifies the VAE framework by using auxiliary variables to guide latent space alignment without major changes to the loss function.", "result": "Aux-VAE effectively disentangles latent variables, as demonstrated on various datasets, including astronomical simulations.", "conclusion": "Aux-VAE offers a simple yet effective approach for disentangling latent factors in high-dimensional data, leveraging auxiliary variables."}}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288", "abs": "https://arxiv.org/abs/2504.05288", "authors": ["Mingyang Fu", "Yuyang Peng", "Dongping Chen", "Zetong Zhou", "Benlin Liu", "Yao Wan", "Zhou Zhao", "Philip S. Yu", "Ranjay Krishna"], "title": "Seeking and Updating with Live Visual Knowledge", "categories": ["cs.CV", "cs.CL"], "comment": "Preprint. Under Review", "summary": "The visual world around us constantly evolves, from real-time news and social\nmedia trends to global infrastructure changes visible through satellite imagery\nand augmented reality enhancements. However, Multimodal Large Language Models\n(MLLMs), which automate many tasks, struggle to stay current, limited by the\ncutoff dates in their fixed training datasets. To quantify this stagnation, we\nintroduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and\n12 categories data specifically designed to support research in both seeking\nand updating with live visual knowledge. Drawing from recent news articles,\nvideo platforms, and academic publications in April 2024-May 2025, LiveVQA\nenables evaluation of how models handle latest visual information beyond their\nknowledge boundaries and how current methods help to update them. Our\ncomprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant\nperformance gaps on content beyond knowledge cutoff, and tool-use or agentic\nvisual seeking framework drastically gain an average of 327% improvement.\nFurthermore, we explore parameter-efficient fine-tuning (PEFT) methods to\nupdate MLLMs with new visual knowledge. We dive deeply to the critical balance\nbetween adapter capacity and model capability when updating MLLMs with new\nvisual knowledge. All the experimental dataset and source code are publicly\navailable at: https://livevqa.github.io.", "AI": {"tldr": "LiveVQA dataset introduces 107,143 samples to evaluate MLLMs' ability to handle live visual knowledge, revealing performance gaps and improvements with tool-use frameworks and PEFT methods.", "motivation": "MLLMs struggle with outdated training data, limiting their ability to process current visual information. LiveVQA addresses this by providing a dataset for evaluating and updating MLLMs with live visual knowledge.", "method": "The study benchmarks 17 MLLMs using LiveVQA, explores tool-use frameworks for visual seeking, and investigates PEFT methods for updating models with new visual knowledge.", "result": "Tool-use frameworks improve performance by 327% on content beyond knowledge cutoff. PEFT methods are explored for efficient model updates.", "conclusion": "LiveVQA highlights the need for updating MLLMs with live visual knowledge and demonstrates effective methods for improvement, with publicly available resources."}}
{"id": "2507.00416", "pdf": "https://arxiv.org/pdf/2507.00416", "abs": "https://arxiv.org/abs/2507.00416", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios.", "AI": {"tldr": "A plug-and-play module enhances Vision-Language-Action (VLA) models by implicitly adding 3D geometry features, improving spatial understanding without extra sensors.", "motivation": "VLMs lack precise spatial understanding due to 2D training. Explicit 3D inputs require additional sensors, so an implicit solution is needed.", "method": "Leverages an off-the-shelf visual geometry foundation model to inject 3D features into VLA models. Evaluated on five spatially challenging tasks.", "result": "Significantly improves performance of state-of-the-art VLA models in diverse scenarios.", "conclusion": "The module effectively enhances VLA models' spatial capabilities without hardware dependencies."}}
{"id": "2505.20170", "pdf": "https://arxiv.org/pdf/2505.20170", "abs": "https://arxiv.org/abs/2505.20170", "authors": ["Yunze Lin"], "title": "Program of Equations Thoughts to Solve Algebra Word Problems", "categories": ["cs.AI"], "comment": "Withdrawn pending institutional authorization and core revisions to\n  address methodological inconsistencies in Sections 3-4", "summary": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset.", "AI": {"tldr": "POET improves AWP solving by offloading computations to Python, achieving SOTA results.", "motivation": "Address calculation errors in LLMs when solving algebraic word problems.", "method": "Two-stage approach: predict equations and generate code (POET), and one-step code generation (Zero-shot POET).", "result": "95.3% on PEN, 98.0% on ALG514, and 95.5% on DRAW-1K.", "conclusion": "POET effectively mitigates LLM calculation errors and sets new SOTA benchmarks."}}
{"id": "2507.00353", "pdf": "https://arxiv.org/pdf/2507.00353", "abs": "https://arxiv.org/abs/2507.00353", "authors": ["Samuel Filgueira da Silva", "Mehmet Fatih Ozkan", "Faissal El Idrissi", "Marcello Canova"], "title": "Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Accurate electrochemical models are essential for the safe and efficient\noperation of lithium-ion batteries in real-world applications such as\nelectrified vehicles and grid storage. Reduced-order models (ROM) offer a\nbalance between fidelity and computational efficiency but often struggle to\ncapture complex and nonlinear behaviors, such as the dynamics in the cell\nvoltage response under high C-rate conditions. To address these limitations,\nthis study proposes an Adaptive Ensemble Sparse Identification (AESI) framework\nthat enhances the accuracy of reduced-order li-ion battery models by\ncompensating for unpredictable dynamics. The approach integrates an Extended\nSingle Particle Model (ESPM) with an evolutionary ensemble sparse learning\nstrategy to construct a robust hybrid model. In addition, the AESI framework\nincorporates a conformal prediction method to provide theoretically guaranteed\nuncertainty quantification for voltage error dynamics, thereby improving the\nreliability of the model's predictions. Evaluation across diverse operating\nconditions shows that the hybrid model (ESPM + AESI) improves the voltage\nprediction accuracy, achieving mean squared error reductions of up to 46% on\nunseen data. Prediction reliability is further supported by conformal\nprediction, yielding statistically valid prediction intervals with coverage\nratios of 96.85% and 97.41% for the ensemble models based on bagging and\nstability selection, respectively.", "AI": {"tldr": "The paper introduces an Adaptive Ensemble Sparse Identification (AESI) framework to improve the accuracy of reduced-order lithium-ion battery models, addressing unpredictable dynamics and providing reliable uncertainty quantification.", "motivation": "Accurate electrochemical models are crucial for lithium-ion battery applications, but reduced-order models often fail to capture complex behaviors, especially under high C-rate conditions.", "method": "The AESI framework integrates an Extended Single Particle Model (ESPM) with an evolutionary ensemble sparse learning strategy and uses conformal prediction for uncertainty quantification.", "result": "The hybrid model (ESPM + AESI) improves voltage prediction accuracy by up to 46% and provides reliable prediction intervals with coverage ratios of 96.85% and 97.41%.", "conclusion": "The AESI framework enhances the reliability and accuracy of reduced-order battery models, making them more suitable for real-world applications."}}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416", "abs": "https://arxiv.org/abs/2504.07416", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language (VL) alignment in radiology. However, existing approaches\nstruggle to effectively utilize complex radiology reports for learning and\noffer limited interpretability through attention probability visualizations. To\naddress these challenges, we introduce RadZero, a novel framework for VL\nalignment in radiology with zero-shot multi-task capability. A key component of\nour approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity),\nwhich aligns text embeddings with local image features for interpretable,\nfine-grained VL reasoning. RadZero leverages large language models to extract\nconcise semantic sentences from radiology reports and employs multi-positive\ncontrastive training to effectively capture relationships between images and\nmultiple relevant textual descriptions. It uses a pre-trained vision encoder\nwith additional trainable Transformer layers, allowing efficient\nhigh-resolution image processing. By computing similarity between text\nembeddings and local image patch features, VL-CABS enables zero-shot inference\nwith similarity probability for classification, and pixel-level VL similarity\nmaps for grounding and segmentation. Experimental results on public chest\nradiograph benchmarks show that RadZero outperforms state-of-the-art methods in\nzero-shot classification, grounding, and segmentation. Furthermore, VL\nsimilarity map analysis highlights the potential of VL-CABS for improving\nexplainability in VL alignment. Additionally, qualitative evaluation\ndemonstrates RadZero's capability for open-vocabulary semantic segmentation,\nfurther validating its effectiveness in medical imaging.", "AI": {"tldr": "RadZero introduces VL-CABS for fine-grained vision-language alignment in radiology, outperforming state-of-the-art methods in zero-shot tasks and improving interpretability.", "motivation": "Existing multi-modal models struggle with complex radiology reports and lack interpretability.", "method": "RadZero uses VL-CABS for alignment, multi-positive contrastive training, and a pre-trained vision encoder with Transformer layers.", "result": "Outperforms state-of-the-art in zero-shot classification, grounding, and segmentation on chest radiograph benchmarks.", "conclusion": "RadZero enhances VL alignment in radiology with zero-shot capability and improved explainability."}}
{"id": "2507.00476", "pdf": "https://arxiv.org/pdf/2507.00476", "abs": "https://arxiv.org/abs/2507.00476", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.", "AI": {"tldr": "FreNBRDF introduces a frequency-rectified neural material representation using spherical harmonics to improve BRDF modeling, enhancing fidelity and adaptability.", "motivation": "Traditional BRDF models lack frequency-domain understanding, limiting photorealism in rendering.", "method": "Uses spherical harmonics and a novel frequency-rectified loss for neural BRDF modeling, integrated into a reconstruction and editing pipeline.", "result": "Improves accuracy and robustness in material appearance reconstruction and editing over state-of-the-art baselines.", "conclusion": "FreNBRDF enables more structured and interpretable material modeling for photorealistic rendering."}}
{"id": "2505.23153", "pdf": "https://arxiv.org/pdf/2505.23153", "abs": "https://arxiv.org/abs/2505.23153", "authors": ["Fan Wang", "Shaoshan Liu"], "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Collective Adaptive Intelligence (CAI) represent a transformative approach in\nembodied AI, wherein numerous autonomous agents collaborate, adapt, and\nself-organize to navigate complex, dynamic environments. By enabling systems to\nreconfigure themselves in response to unforeseen challenges, CAI facilitate\nrobust performance in real-world scenarios. This article introduces a\nconceptual framework for designing and analyzing CAI. It delineates key\nattributes including task generalization, resilience, scalability, and\nself-assembly, aiming to bridge theoretical foundations with practical\nmethodologies for engineering adaptive, emergent intelligence. By providing a\nstructured foundation for understanding and implementing CAI, this work seeks\nto guide researchers and practitioners in developing more resilient, scalable,\nand adaptable AI systems across various domains.", "AI": {"tldr": "The paper introduces Collective Adaptive Intelligence (CAI), a framework for designing AI systems where autonomous agents collaborate and adapt in dynamic environments.", "motivation": "To address the need for robust, scalable, and adaptable AI systems in complex real-world scenarios.", "method": "Proposes a conceptual framework with key attributes like task generalization, resilience, scalability, and self-assembly.", "result": "A structured foundation for understanding and implementing CAI, bridging theory and practice.", "conclusion": "The work aims to guide researchers in developing resilient and adaptable AI systems across domains."}}
{"id": "2507.00402", "pdf": "https://arxiv.org/pdf/2507.00402", "abs": "https://arxiv.org/abs/2507.00402", "authors": ["Suqing Liu", "Xuan Bi", "Tianxi Li"], "title": "GRAND: Graph Release with Assured Node Differential Privacy", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.", "AI": {"tldr": "GRAND ensures node-level differential privacy for network data, preserving structural properties and matching original network distribution asymptotically.", "motivation": "Existing methods for node-level privacy in networks are limited to query-based approaches or fail to preserve structural properties.", "method": "Proposes GRAND, a mechanism releasing entire networks with node-level differential privacy under latent space models.", "result": "Released networks asymptotically follow the same distribution as the original, validated via synthetic and real-world datasets.", "conclusion": "GRAND is the first method to ensure node-level differential privacy while preserving network structure."}}
{"id": "2505.00703", "pdf": "https://arxiv.org/pdf/2505.00703", "abs": "https://arxiv.org/abs/2505.00703", "authors": ["Dongzhi Jiang", "Ziyu Guo", "Renrui Zhang", "Zhuofan Zong", "Hao Li", "Le Zhuo", "Shilin Yan", "Pheng-Ann Heng", "Hongsheng Li"], "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://github.com/CaraJ7/T2I-R1", "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1", "AI": {"tldr": "T2I-R1 is a text-to-image model using RL and bi-level CoT reasoning (semantic and token levels) to enhance generation, outperforming benchmarks by 13-19%.", "motivation": "To explore and improve reasoning strategies (CoT and RL) in visual generation, an underexplored area.", "method": "Introduces BiCoT-GRPO with two CoT levels (semantic for prompt planning, token for pixel processing) and ensemble rewards for joint optimization.", "result": "13% improvement on T2I-CompBench, 19% on WISE, surpassing FLUX.", "conclusion": "T2I-R1 demonstrates the effectiveness of bi-level CoT and RL in enhancing text-to-image generation."}}
{"id": "2507.00635", "pdf": "https://arxiv.org/pdf/2507.00635", "abs": "https://arxiv.org/abs/2507.00635", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "AI": {"tldr": "A new method for eye localization and tracking in ophthalmic surgery robots combines machine learning and traditional algorithms, improving precision and reducing reliance on manual navigation.", "motivation": "Current eye gaze estimation techniques in surgery face challenges like sensor dependence, occlusion, and facial detection requirements, limiting consistency and increasing uncertainty.", "method": "The study proposes a hybrid approach using machine learning and traditional algorithms for stable iris detection and gaze estimation without landmarks, adaptable to varying lighting.", "result": "The method achieves an average eye orientation estimation error of 0.58 degrees and a 2.08-degree control error for robotic arm movement.", "conclusion": "The proposed method enhances precision and reliability in ophthalmic surgical robots, addressing key limitations of existing techniques."}}
{"id": "2506.20702", "pdf": "https://arxiv.org/pdf/2506.20702", "abs": "https://arxiv.org/abs/2506.20702", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "S\u00f6ren Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Sim\u00e9on Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Johannes Heidecke", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Mo\u00ebs", "Simon M\u00f6ller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Se\u00e1n O h\u00c9igeartaigh", "Alejandro Ortega", "Pierre Peign\u00e9", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Mart\u00edn Soto", "Cheston Tan", "Dong Ting", "William Tjhi", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje \u017dikeli\u0107"], "title": "The Singapore Consensus on Global AI Safety Research Priorities", "categories": ["cs.AI", "cs.CY"], "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "AI": {"tldr": "The paper discusses the importance of AI safety, proposing a framework to address challenges in development, assessment, and control of AI systems.", "motivation": "The rapid advancement of AI necessitates ensuring its safety, trustworthiness, and reliability to foster public confidence and innovation.", "method": "The report adopts a defence-in-depth model, categorizing AI safety research into Development, Assessment, and Control domains.", "result": "The framework identifies key challenges in creating, evaluating, and managing AI systems to ensure safety.", "conclusion": "A structured approach to AI safety research is essential for building a trusted ecosystem and mitigating risks."}}
{"id": "2507.00423", "pdf": "https://arxiv.org/pdf/2507.00423", "abs": "https://arxiv.org/abs/2507.00423", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ICCV 2025", "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree.", "AI": {"tldr": "FedPoisonMIA is a poisoning attack in federated learning (FL) that infers membership information, with a proposed defense mechanism to mitigate its impact.", "motivation": "FL's distributed nature makes it vulnerable to poisoning attacks, but existing attacks focus on degrading model integrity, not privacy. This study addresses the gap by introducing a privacy-focused poisoning attack.", "method": "FedPoisonMIA involves malicious clients crafting local model updates to infer membership information. A defense mechanism is also proposed.", "result": "Experiments show FedPoisonMIA's effectiveness, while the defense reduces its impact.", "conclusion": "The study highlights privacy risks in FL from poisoning attacks and offers a defense, though further improvements are needed."}}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232", "abs": "https://arxiv.org/abs/2505.18232", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "title": "Two-Stage Regularization-Based Structured Pruning for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment.", "AI": {"tldr": "TRSP is a novel two-stage regularization-based pruning method for LLMs that avoids knowledge loss and retraining, outperforming traditional structured pruning methods.", "motivation": "Large language models (LLMs) face deployment challenges due to their size. Traditional pruning methods cause knowledge loss and require retraining.", "method": "TRSP uses two-stage regularization: first, learnable weights with \u21131-norm regularization; second, regularization on layer output differences to shift knowledge.", "result": "TRSP retains more knowledge, preserves performance, and outperforms other pruning methods without retraining, enabling efficient LLM deployment.", "conclusion": "TRSP is an effective pruning solution for LLMs, offering performance preservation and acceleration without retraining."}}
{"id": "2506.21329", "pdf": "https://arxiv.org/pdf/2506.21329", "abs": "https://arxiv.org/abs/2506.21329", "authors": ["Karthik Duraisamy"], "title": "Active Inference AI Systems for Scientific Discovery", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "comment": null, "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "AI": {"tldr": "The paper proposes an AI architecture for scientific discovery, emphasizing closing abstraction, reasoning, and reality gaps over scaling models. It advocates for active inference systems with causal models, Bayesian planners, knowledge graphs, and closed-loop interaction with simulations and labs, while highlighting the enduring role of human judgment.", "motivation": "Current AI systems are limited in scientific discovery due to brittle reasoning, abstraction gaps, and separation from experimental reality. The paper aims to address these limitations by proposing a more integrated and adaptive AI architecture.", "method": "The paper outlines an active inference AI system with: (i) causal self-supervised models, (ii) Bayesian-guarded planners, (iii) dynamic knowledge graphs, and (iv) closed-loop interaction with simulations and labs. Human judgment is integrated as a permanent component.", "result": "The proposed architecture enables AI systems to simulate actions, distinguish causation from correlation, and refine understanding through empirical validation, fostering more robust scientific discovery.", "conclusion": "Scientific discovery AI must bridge abstraction, reasoning, and reality gaps through active inference, grounded in causal models and human judgment, rather than relying solely on scale or compute."}}
{"id": "2507.00514", "pdf": "https://arxiv.org/pdf/2507.00514", "abs": "https://arxiv.org/abs/2507.00514", "authors": ["Leander Thiele", "Adrian E. Bayer", "Naoya Takeishi"], "title": "Simulation-Efficient Cosmological Inference with Multi-Fidelity SBI", "categories": ["astro-ph.CO", "cs.LG"], "comment": "5 pages, 4 figures; accepted at ICML-colocated ML4Astro 2025 workshop", "summary": "The simulation cost for cosmological simulation-based inference can be\ndecreased by combining simulation sets of varying fidelity. We propose an\napproach to such multi-fidelity inference based on feature matching and\nknowledge distillation. Our method results in improved posterior quality,\nparticularly for small simulation budgets and difficult inference problems.", "AI": {"tldr": "A method combining multi-fidelity simulations with feature matching and knowledge distillation reduces costs and improves posterior quality in cosmological inference.", "motivation": "To decrease the high simulation costs in cosmological simulation-based inference while maintaining or improving posterior quality.", "method": "Uses feature matching and knowledge distillation to combine simulation sets of varying fidelity.", "result": "Improved posterior quality, especially for small simulation budgets and challenging inference problems.", "conclusion": "The proposed approach effectively reduces costs and enhances inference quality in cosmological simulations."}}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955", "abs": "https://arxiv.org/abs/2505.19955", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "42 pages, 9 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.", "AI": {"tldr": "MLR-Bench is a benchmark for evaluating AI agents in open-ended ML research, featuring tasks, an automated evaluation framework (MLR-Judge), and a modular agent (MLR-Agent). It highlights LLMs' strengths in idea generation and paper writing but reveals coding agents' reliability issues.", "motivation": "To address the lack of benchmarks for assessing AI agents' capabilities in scientific research, particularly in open-ended ML tasks.", "method": "MLR-Bench includes 201 research tasks, MLR-Judge for automated evaluation, and MLR-Agent for task completion. It evaluates agents across idea generation, proposal formulation, experimentation, and paper writing.", "result": "LLMs excel in idea generation and paper writing, but coding agents often produce unreliable results (80% failure rate). MLR-Judge aligns well with human reviewers.", "conclusion": "MLR-Bench is a scalable tool for improving AI research agents, emphasizing the need for reliability in scientific discovery."}}
{"id": "2507.00937", "pdf": "https://arxiv.org/pdf/2507.00937", "abs": "https://arxiv.org/abs/2507.00937", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "comment": "8 pages, accepted by IROS 2025", "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "AI": {"tldr": "RaGNNarok is a lightweight GNN-based framework for enhancing radar point clouds in indoor mobile robots, offering real-time performance and reliability in complex environments.", "motivation": "Existing lidar and camera-based solutions for indoor robots have limitations like poor performance in obscured environments and high costs. Radar sensors are cost-effective but suffer from sparse point clouds and noise.", "method": "RaGNNarok uses a graph neural network (GNN) to enhance radar point clouds, achieving real-time inference (7.3 ms) on low-cost devices like Raspberry Pi 5.", "result": "Evaluated in three environments, RaGNNarok shows strong reliability and generalizability in tasks like localization, SLAM, and autonomous navigation.", "conclusion": "RaGNNarok is a robust, efficient solution for low-cost indoor mobile robots, addressing limitations of existing methods."}}
{"id": "2506.22419", "pdf": "https://arxiv.org/pdf/2506.22419", "abs": "https://arxiv.org/abs/2506.22419", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "AI": {"tldr": "The paper introduces the Automated LLM Speedrunning Benchmark to evaluate AI agents' ability to reproduce scientific work, focusing on training GPT-2 models quickly. Despite detailed hints, current LLMs struggle to reimplement known innovations, highlighting limitations in automating scientific reproduction.", "motivation": "To assess AI agents' capability in reproducing scientific results, a critical skill for autonomous research, using the NanoGPT speedrun as a test case.", "method": "The benchmark includes 19 tasks where agents use previous records' training scripts and hints (from pseudocode to paper-like descriptions) to improve GPT-2 training speed.", "result": "Recent reasoning LLMs with state-of-the-art scaffolds fail to reimplement known innovations, even with detailed hints.", "conclusion": "The benchmark offers a simple, non-saturated measure of LLMs' ability to automate scientific reproduction, revealing current limitations."}}
{"id": "2507.00600", "pdf": "https://arxiv.org/pdf/2507.00600", "abs": "https://arxiv.org/abs/2507.00600", "authors": ["Christian Franssen", "Iman van Lelyveld", "Bernd Heidergott"], "title": "A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Understanding the functional roles of financial institutions within\ninterconnected markets is critical for effective supervision, systemic risk\nassessment, and resolution planning. We propose an interpretable role-based\nclustering approach for multi-layer financial networks, designed to identify\nthe functional positions of institutions across different market segments. Our\nmethod follows a general clustering framework defined by proximity measures,\ncluster evaluation criteria, and algorithm selection. We construct explainable\nnode embeddings based on egonet features that capture both direct and indirect\ntrading relationships within and across market layers. Using transaction-level\ndata from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate\nhow the approach uncovers heterogeneous institutional roles such as market\nintermediaries, cross-segment connectors, and peripheral lenders or borrowers.\nThe results highlight the flexibility and practical value of role-based\nclustering in analyzing financial networks and understanding institutional\nbehavior in complex market structures.", "AI": {"tldr": "The paper introduces an interpretable role-based clustering method for analyzing multi-layer financial networks to identify institutional roles, using ECB's MMSR data to reveal diverse roles like intermediaries and connectors.", "motivation": "Understanding the functional roles of financial institutions in interconnected markets is crucial for supervision, systemic risk assessment, and resolution planning.", "method": "Proposes a clustering framework with proximity measures, evaluation criteria, and algorithm selection, using explainable node embeddings based on egonet features to capture trading relationships.", "result": "The method identifies heterogeneous institutional roles (e.g., intermediaries, cross-segment connectors) in financial networks using ECB's MMSR data.", "conclusion": "Role-based clustering is flexible and valuable for analyzing financial networks and institutional behavior in complex market structures."}}
{"id": "2506.11999", "pdf": "https://arxiv.org/pdf/2506.11999", "abs": "https://arxiv.org/abs/2506.11999", "authors": ["Zheli Zhou", "Chenxu Zhu", "Jianghao Lin", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "title": "Generative Representational Learning of Foundation Models for Recommendation", "categories": ["cs.IR", "cs.CL"], "comment": "Project page is available at https://junkfood436.github.io/RecFound/", "summary": "Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.", "AI": {"tldr": "RecFound is a generative representational learning framework for recommendation foundation models, addressing multi-task learning challenges like knowledge sharing and convergence inconsistencies.", "motivation": "The goal is to develop a single foundation model capable of excelling across diverse recommendation tasks, overcoming limitations of existing models that overlook embedding tasks and struggle with multi-task complexities.", "method": "RecFound introduces a multi-task training scheme with Task-wise Mixture of Low-rank Experts (TMoLE), Step-wise Convergence-oriented Sample Scheduler (S2Sched), and a Model Merge module. It uses a comprehensive dataset for both generative and embedding tasks.", "result": "RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.", "conclusion": "RecFound successfully addresses key challenges in recommendation foundation models, demonstrating superior performance and versatility."}}
{"id": "2507.00984", "pdf": "https://arxiv.org/pdf/2507.00984", "abs": "https://arxiv.org/abs/2507.00984", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline.", "AI": {"tldr": "A self-supervised domain adaptation pipeline improves box pose and shape estimation in warehouse automation, outperforming simulation-only and zero-shot baselines.", "motivation": "Warehouse automation generates vast unannotated data; leveraging this for perception models without manual annotations is crucial.", "method": "Develops a correct-and-certify pipeline for self-supervised box pose and shape estimation, using real-world unlabeled data.", "result": "The model outperforms simulation-only training and zero-shot baselines, validated on 50,000 real-world images.", "conclusion": "Self-supervised domain adaptation effectively enhances perception models in industrial settings without manual annotations."}}
{"id": "2506.22774", "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "The paper proposes a method to assess AI trustworthiness by combining ethical guidelines with algorithmic processes (PageRank and TrustRank), aiming for a balanced, quantitative, and holistic evaluation.", "motivation": "AI's pervasive societal impact and complexity necessitate reliable trustworthiness assessments, but current methods lack either quantification or holistic perspectives.", "method": "The paper integrates ethical components of Trustworthy AI with algorithmic processes like PageRank and TrustRank to create a quantitative yet holistic assessment framework.", "result": "The proposed method successfully provides quantitative insights into AI trustworthiness while incorporating theoretical guidelines, reducing subjectivity in evaluations.", "conclusion": "The framework bridges the gap between ethical guidelines and algorithmic quantification, offering a more objective and comprehensive approach to assessing AI trustworthiness."}}
{"id": "2507.00616", "pdf": "https://arxiv.org/pdf/2507.00616", "abs": "https://arxiv.org/abs/2507.00616", "authors": ["Natha\u00ebl Da Costa", "B\u00e1lint Mucs\u00e1nyi", "Philipp Hennig"], "title": "Geometric Gaussian Approximations of Probability Distributions", "categories": ["math.DG", "cs.LG", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "Approximating complex probability distributions, such as Bayesian posterior\ndistributions, is of central interest in many applications. We study the\nexpressivity of geometric Gaussian approximations. These consist of\napproximations by Gaussian pushforwards through diffeomorphisms or Riemannian\nexponential maps. We first review these two different kinds of geometric\nGaussian approximations. Then we explore their relationship to one another. We\nfurther provide a constructive proof that such geometric Gaussian\napproximations are universal, in that they can capture any probability\ndistribution. Finally, we discuss whether, given a family of probability\ndistributions, a common diffeomorphism can be found to obtain uniformly\nhigh-quality geometric Gaussian approximations for that family.", "AI": {"tldr": "The paper explores geometric Gaussian approximations for complex probability distributions, showing their universality and discussing uniform approximations for distribution families.", "motivation": "To approximate complex probability distributions like Bayesian posteriors using geometric Gaussian methods.", "method": "Review and compare Gaussian pushforwards via diffeomorphisms and Riemannian exponential maps, then prove their universality.", "result": "Geometric Gaussian approximations are universal and can approximate any distribution.", "conclusion": "The study highlights the potential of geometric Gaussian approximations and raises questions about uniform approximations for distribution families."}}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571", "abs": "https://arxiv.org/abs/2506.16571", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Capturing Visualization Design Rationale", "categories": ["cs.HC", "cs.CL"], "comment": "To be presented at IEEE VIS 2025", "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "AI": {"tldr": "A new dataset and methodology for probing visualization design rationale using real-world student-created notebooks and LLMs to generate and validate question-answer-rationale triples.", "motivation": "Existing datasets focus on interpreting visualizations, not understanding their design rationale. This work aims to fill that gap using real-world data.", "method": "Leverages student-created visualization notebooks and LLMs to generate and validate question-answer-rationale triples.", "result": "A curated dataset capturing visualization design choices and rationales.", "conclusion": "The dataset and methodology provide a novel way to study visualization design rationale, moving beyond interpretation to understanding encoding decisions."}}
{"id": "2507.01016", "pdf": "https://arxiv.org/pdf/2507.01016", "abs": "https://arxiv.org/abs/2507.01016", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io", "AI": {"tldr": "The paper introduces a vector quantization-based action tokenizer using a large-scale dataset, improving inference speed and action coherence, with zero-shot adaptability to various tasks.", "motivation": "To address the limitations of previous approaches by leveraging a much larger dataset to capture spatiotemporal dynamics and bridge the synthetic-real data gap.", "method": "Develops an action tokenizer using extensive synthetic data, validated in simulated and real robotic environments.", "result": "Achieves up to 30% higher success rate in real-world tasks, showing performance improves with more synthetic data.", "conclusion": "The tokenizer is a scalable solution for real-time robotic control, enhancing efficiency and reliability across applications."}}
{"id": "2506.22919", "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto introduces a lightweight MoE architecture combining GRU and FFNN experts for diverse reasoning tasks, achieving specialization and interpretability.", "motivation": "Current MoE models lack representational diversity due to identical inductive biases, limiting efficiency and interpretability.", "method": "Hecto combines a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism.", "result": "Hecto matches or closely trails homogeneous baselines, achieves clear expert specialization, and improves performance at larger batch sizes.", "conclusion": "Hecto sets a new benchmark for conditional computation, offering specialized reasoning in low-resource regimes."}}
{"id": "2507.00629", "pdf": "https://arxiv.org/pdf/2507.00629", "abs": "https://arxiv.org/abs/2507.00629", "authors": ["Jean Barbier", "Federica Gerace", "Alessandro Ingrosso", "Clarissa Lauditi", "Enrico M. Malatesta", "Gibbs Nwemadji", "Rodrigo P\u00e9rez Ortiz"], "title": "Generalization performance of narrow one-hidden layer networks in the teacher-student setting", "categories": ["cond-mat.dis-nn", "cs.LG", "math.PR", "math.ST", "stat.TH"], "comment": "34 pages, figures", "summary": "Understanding the generalization abilities of neural networks for simple\ninput-output distributions is crucial to account for their learning performance\non real datasets. The classical teacher-student setting, where a network is\ntrained from data obtained thanks to a label-generating teacher model, serves\nas a perfect theoretical test bed. In this context, a complete theoretical\naccount of the performance of fully connected one-hidden layer networks in the\npresence of generic activation functions is lacking. In this work, we develop\nsuch a general theory for narrow networks, i.e. networks with a large number of\nhidden units, yet much smaller than the input dimension. Using methods from\nstatistical physics, we provide closed-form expressions for the typical\nperformance of both finite temperature (Bayesian) and empirical risk\nminimization estimators, in terms of a small number of weight statistics. In\ndoing so, we highlight the presence of a transition where hidden neurons\nspecialize when the number of samples is sufficiently large and proportional to\nthe number of parameters of the network. Our theory accurately predicts the\ngeneralization error of neural networks trained on regression or classification\ntasks with either noisy full-batch gradient descent (Langevin dynamics) or\nfull-batch gradient descent.", "AI": {"tldr": "The paper develops a general theory for the performance of narrow neural networks with generic activation functions, using statistical physics methods to predict generalization error in teacher-student settings.", "motivation": "To understand the generalization abilities of neural networks for simple input-output distributions, filling a gap in theoretical accounts for fully connected one-hidden layer networks.", "method": "Uses statistical physics to derive closed-form expressions for network performance, analyzing both Bayesian and empirical risk minimization estimators.", "result": "Identifies a transition where hidden neurons specialize with sufficient samples, accurately predicting generalization error for regression and classification tasks.", "conclusion": "The theory provides a comprehensive framework for understanding neural network performance in simple settings, validated by empirical results."}}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119", "abs": "https://arxiv.org/abs/2506.24119", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "AI": {"tldr": "SPIRAL is a self-play framework for language models to learn reasoning through zero-sum games, eliminating human supervision and achieving transferable reasoning improvements.", "motivation": "To develop reasoning in language models without relying on human-curated data or domain-specific rewards.", "method": "Uses self-play in zero-sum games with role-conditioned advantage estimation (RAE) for stable multi-agent training.", "result": "Improves reasoning (8.6% on math, 8.4% on general reasoning) and transfers skills across domains.", "conclusion": "Zero-sum games naturally foster transferable reasoning, offering a path for autonomous reasoning development."}}
{"id": "2302.14368", "pdf": "https://arxiv.org/pdf/2302.14368", "abs": "https://arxiv.org/abs/2302.14368", "authors": ["Wonwoong Cho", "Hareesh Ravi", "Midhun Harikumar", "Vinh Khuc", "Krishna Kumar Singh", "Jingwan Lu", "David I. Inouye", "Ajinkya Kale"], "title": "Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "ECCV 2024; Code is available at\n  https://github.com/WonwoongCho/Towards-Enhanced-Controllability-of-Diffusion-Models", "summary": "As Diffusion Models have shown promising performance, a lot of efforts have\nbeen made to improve the controllability of Diffusion Models. However, how to\ntrain Diffusion Models to have the disentangled latent spaces and how to\nnaturally incorporate the disentangled conditions during the sampling process\nhave been underexplored. In this paper, we present a training framework for\nfeature disentanglement of Diffusion Models (FDiff). We further propose two\nsampling methods that can boost the realism of our Diffusion Models and also\nenhance the controllability. Concisely, we train Diffusion Models conditioned\non two latent features, a spatial content mask, and a flattened style\nembedding. We rely on the inductive bias of the denoising process of Diffusion\nModels to encode pose/layout information in the content feature and\nsemantic/style information in the style feature. Regarding the sampling\nmethods, we first generalize Composable Diffusion Models (GCDM) by breaking the\nconditional independence assumption to allow for some dependence between\nconditional inputs, which is shown to be effective in realistic generation in\nour experiments. Second, we propose timestep-dependent weight scheduling for\ncontent and style features to further improve the performance. We also observe\nbetter controllability of our proposed methods compared to existing methods in\nimage manipulation and image translation.", "AI": {"tldr": "A framework (FDiff) for disentangling features in Diffusion Models is proposed, along with two sampling methods to enhance realism and controllability.", "motivation": "Improving controllability and disentanglement in Diffusion Models, which are underexplored areas.", "method": "Training Diffusion Models with spatial content masks and style embeddings, and proposing two sampling methods: generalized GCDM and timestep-dependent weight scheduling.", "result": "Enhanced realism and controllability in image manipulation and translation compared to existing methods.", "conclusion": "The proposed FDiff framework and sampling methods effectively improve disentanglement and controllability in Diffusion Models."}}
{"id": "2506.23520", "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "categories": ["cs.AI"], "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "ChemActor, a fine-tuned LLM, converts chemical procedures into structured actions, using a sequential data framework and multi-round review for state-of-the-art performance.", "motivation": "Automating chemical procedure extraction is challenging due to ambiguous language and high annotation costs.", "method": "Uses a fine-tuned LLM with a data selection module and multi-round review to generate structured actions from unstructured inputs.", "result": "Achieves 10% better performance than baselines in R2D and D2A tasks.", "conclusion": "ChemActor demonstrates advanced understanding and effectiveness in automating chemical procedure extraction."}}
{"id": "2507.00640", "pdf": "https://arxiv.org/pdf/2507.00640", "abs": "https://arxiv.org/abs/2507.00640", "authors": ["Denis Belomestny", "John. Schoenmakers"], "title": "Forward Reverse Kernel Regression for the Schr\u00f6dinger bridge problem", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "90C40, 65C05, 62G08"], "comment": null, "summary": "In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is\ncentral to entropic optimal transport. For general reference processes and\nbegin--endpoint distributions, we propose a forward-reverse iterative Monte\nCarlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric\nway. In particular, we use kernel based Monte Carlo regression in the context\nof Picard iteration of a corresponding fixed point problem. By preserving in\nthe iteration positivity and contractivity in a Hilbert metric sense, we\ndevelop a provably convergent algorithm. Furthermore, we provide convergence\nrates for the potential estimates and prove their optimality. Finally, as an\napplication, we propose a non-nested Monte Carlo procedure for the final\ndimensional distributions of the Schr\\\"odinger Bridge process, based on the\nconstructed potentials and the forward-reverse simulation method for\nconditional diffusions.", "AI": {"tldr": "The paper proposes a forward-reverse iterative Monte Carlo method to approximate Schr\u00f6dinger potentials for the Schr\u00f6dinger Bridge Problem, ensuring convergence and optimality, with applications in simulating conditional diffusions.", "motivation": "The Schr\u00f6dinger Bridge Problem is central to entropic optimal transport, but general solutions for arbitrary reference processes and distributions are challenging. This work aims to provide a nonparametric, convergent approach.", "method": "A forward-reverse iterative Monte Carlo procedure using kernel-based regression and Picard iteration, preserving positivity and contractivity in a Hilbert metric sense.", "result": "The algorithm is provably convergent with optimal convergence rates for potential estimates.", "conclusion": "The method is effective for approximating Schr\u00f6dinger potentials and has practical applications in simulating conditional diffusions."}}
{"id": "2404.09158", "pdf": "https://arxiv.org/pdf/2404.09158", "abs": "https://arxiv.org/abs/2404.09158", "authors": ["Xuelong Li", "Hongjun An", "Haofei Zhao", "Guangying Li", "Bo Liu", "Xing Wang", "Guanghua Cheng", "Guojun Wu", "Zhe Sun"], "title": "StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by IEEE Transactions on Image Processing (T-IP)", "summary": "In this paper, we introduce StreakNet-Arch, a real-time, end-to-end\nbinary-classification framework based on our self-developed Underwater Carrier\nLiDAR-Radar (UCLR) that embeds Self-Attention and our novel Double Branch Cross\nAttention (DBC-Attention) to enhance scatter suppression. Under controlled\nwater tank validation conditions, StreakNet-Arch with Self-Attention or\nDBC-Attention outperforms traditional bandpass filtering and achieves higher\n$F_1$ scores than learning-based MP networks and CNNs at comparable model size\nand complexity. Real-time benchmarks on an NVIDIA RTX 3060 show a constant\nAverage Imaging Time (54 to 84 ms) regardless of frame count, versus a linear\nincrease (58 to 1,257 ms) for conventional methods. To facilitate further\nresearch, we contribute a publicly available streak-tube camera image dataset\ncontains 2,695,168 real-world underwater 3D point cloud data. More importantly,\nwe validate our UCLR system in a South China Sea trial, reaching an error of\n46mm for 3D target at 1,000 m depth and 20 m range. Source code and data are\navailable at https://github.com/BestAnHongjun/StreakNet .", "AI": {"tldr": "StreakNet-Arch is a real-time binary-classification framework using UCLR with Self-Attention and DBC-Attention, outperforming traditional methods in scatter suppression and achieving high F1 scores. It maintains constant imaging time and was validated in real-world trials.", "motivation": "To enhance scatter suppression in underwater imaging and improve real-time performance for binary classification tasks.", "method": "Uses UCLR with Self-Attention and novel DBC-Attention for scatter suppression, validated in controlled and real-world conditions.", "result": "Outperforms traditional methods in F1 scores, maintains constant imaging time (54-84 ms), and achieves 46mm error in real-world trials.", "conclusion": "StreakNet-Arch is effective for real-time underwater imaging, with publicly available code and data for further research."}}
{"id": "2310.02277", "pdf": "https://arxiv.org/pdf/2310.02277", "abs": "https://arxiv.org/abs/2310.02277", "authors": ["Lu Yin", "Ajay Jaiswal", "Shiwei Liu", "Souvik Kundu", "Zhangyang Wang"], "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Published at ICML 2024", "summary": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git.", "AI": {"tldr": "The paper challenges the belief that small-magnitude weights in LLMs are redundant, showing they encode vital knowledge for difficult tasks, and pruning them causes performance drops. Quantization doesn't show the same effect.", "motivation": "To counter the common assumption that pruning small-magnitude weights in LLMs is harmless, revealing their importance for difficult tasks.", "method": "Analyzes pruning effects on downstream tasks, introduces metrics for task difficulty, and compares with quantization.", "result": "Pruning small weights leads to performance drops in difficult tasks, while quantization doesn't show the same effect.", "conclusion": "Small-magnitude weights are crucial for difficult tasks, supporting the 'Junk DNA Hypothesis' that they aren't redundant."}}
{"id": "2507.00641", "pdf": "https://arxiv.org/pdf/2507.00641", "abs": "https://arxiv.org/abs/2507.00641", "authors": ["Gunjan Auti", "Hirofumi Daiguji", "Gouhei Tanaka"], "title": "Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws", "categories": ["nlin.AO", "cs.LG", "stat.CO", "stat.ME"], "comment": "6 pages, 2 figures, 2 supplementary videos", "summary": "Traditional machine learning approaches in physics rely on global\noptimization, limiting interpretability and enforcing physical constraints\nexternally. We introduce the Hebbian Physics Network (HPN), a self-organizing\ncomputational framework in which learning emerges from local Hebbian updates\ndriven by violations of conservation laws. Grounded in non-equilibrium\nthermodynamics and inspired by Prigogine/'s theory of dissipative structures,\nHPNs eliminate the need for global loss functions by encoding physical laws\ndirectly into the system/'s local dynamics. Residuals - quantified imbalances\nin continuity, momentum, or energy - serve as thermodynamic signals that drive\nweight adaptation through generalized Hebbian plasticity. We demonstrate this\napproach on incompressible fluid flow and continuum diffusion, where physically\nconsistent structures emerge from random initial conditions without\nsupervision. HPNs reframe computation as a residual-driven thermodynamic\nprocess, offering an interpretable, scalable, and physically grounded\nalternative for modeling complex dynamical systems.", "AI": {"tldr": "The paper introduces the Hebbian Physics Network (HPN), a self-organizing framework that learns through local Hebbian updates driven by conservation law violations, eliminating the need for global optimization.", "motivation": "Traditional machine learning in physics lacks interpretability and relies on external enforcement of physical constraints. HPNs address this by embedding physical laws directly into local dynamics.", "method": "HPNs use residuals (imbalances in continuity, momentum, or energy) as thermodynamic signals to drive weight adaptation via Hebbian plasticity, inspired by non-equilibrium thermodynamics and Prigogine's dissipative structures.", "result": "HPNs successfully model incompressible fluid flow and continuum diffusion, with physically consistent structures emerging from random initial conditions without supervision.", "conclusion": "HPNs offer an interpretable, scalable, and physically grounded alternative for modeling complex dynamical systems, reframing computation as a residual-driven thermodynamic process."}}
{"id": "2405.05769", "pdf": "https://arxiv.org/pdf/2405.05769", "abs": "https://arxiv.org/abs/2405.05769", "authors": ["Fangzhou Han", "Lingyu Si", "Zhizhuo Jiang", "Hongwei Dong", "Lamei Zhang", "Yu Liu", "Hao Chen", "Bo Du"], "title": "Exploring Text-Guided Single Image Editing for Remote Sensing Images", "categories": ["cs.CV"], "comment": "17 pages, 18 figures, Accepted by IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing", "summary": "Artificial intelligence generative content (AIGC) has significantly impacted\nimage generation in the field of remote sensing. However, the equally important\narea of remote sensing image (RSI) editing has not received sufficient\nattention. Deep learning based editing methods generally involve two sequential\nstages: generation and editing. For natural images, these stages primarily rely\non generative backbones pre-trained on large-scale benchmark datasets and text\nguidance facilitated by vision-language models (VLMs). However, it become less\nviable for RSIs: First, existing generative RSI benchmark datasets do not fully\ncapture the diversity of RSIs, and is often inadequate for universal editing\ntasks. Second, the single text semantic corresponds to multiple image\nsemantics, leading to the introduction of incorrect semantics. To solve above\nproblems, this paper proposes a text-guided RSI editing method and can be\ntrained using only a single image. A multi-scale training approach is adopted\nto preserve consistency without the need for training on extensive benchmarks,\nwhile leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure\naccuracy and controllability. Experimental results on multiple RSI editing\ntasks show that the proposed method offers significant advantages in both CLIP\nscores and subjective evaluations compared to existing methods. Additionally,\nwe explore the ability of the edited RSIs to support disaster assessment tasks\nin order to validate their practicality. Codes will be released at\nhttps://github.com/HIT-PhilipHan/remote_sensing_image_editing.", "AI": {"tldr": "The paper proposes a text-guided remote sensing image (RSI) editing method using multi-scale training and pre-trained vision-language models (VLMs) to address limitations in existing generative RSI datasets and text-semantic mismatches.", "motivation": "Existing RSI editing methods lack diversity in datasets and suffer from incorrect semantics due to text-image mismatches, limiting their effectiveness.", "method": "A multi-scale training approach with RSI pre-trained VLMs and prompt ensembling (PE) is introduced, requiring only a single image for training.", "result": "The method outperforms existing techniques in CLIP scores and subjective evaluations, and demonstrates practicality in disaster assessment tasks.", "conclusion": "The proposed method effectively addresses RSI editing challenges, offering improved accuracy and controllability without extensive benchmark training."}}
{"id": "2311.10248", "pdf": "https://arxiv.org/pdf/2311.10248", "abs": "https://arxiv.org/abs/2311.10248", "authors": ["Sheldon C. Ebron", "Meiying Zhang", "Kan Yang"], "title": "Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "Accepted to ACISP 2025. This is the full version", "summary": "Federated Learning (FL) enables multiple parties to train machine learning\nmodels collaboratively without sharing the raw training data. However, the\nfederated nature of FL enables malicious clients to influence a trained model\nby injecting error model updates via Byzantine or backdoor attacks. To detect\nmalicious model updates, a typical approach is to measure the distance between\neach model update and a \\textit{ground-truth model update}. To find such\n\\textit{ground-truth model updates}, existing defenses either require a benign\nroot dataset on the server (e.g., FLTrust) or simply use trimmed mean or median\nas the threshold for clipping (e.g., FLAME). However, such benign root datasets\nare impractical, and the trimmed mean or median may also eliminate\ncontributions from these underrepresented datasets.\n  In this paper, we propose a generic solution, namely FedTruth, to defend\nagainst model poisoning attacks in FL, where the \\textit{ground-truth model\nupdate} (i.e., the global model update) will be estimated among all the model\nupdates with dynamic aggregation weights. Specifically, FedTruth does not have\nspecific assumptions on the benign or malicious data distribution or access to\na benign root dataset. Moreover, FedTruth considers the potential contributions\nfrom all benign clients. Our empirical results show that FedTruth can reduce\nthe impacts of poisoned model updates against both Byzantine and backdoor\nattacks, and is also efficient in large-scale FL systems.", "AI": {"tldr": "FedTruth is a solution for detecting malicious model updates in Federated Learning without needing a benign root dataset or assuming data distributions.", "motivation": "Existing defenses against model poisoning attacks in FL rely on impractical assumptions like benign root datasets or may exclude underrepresented data contributions.", "method": "FedTruth dynamically estimates the ground-truth model update (global model update) among all updates using dynamic aggregation weights, without specific assumptions on data distribution.", "result": "FedTruth effectively reduces the impact of poisoned model updates in both Byzantine and backdoor attacks and is efficient in large-scale FL systems.", "conclusion": "FedTruth provides a practical and efficient defense against model poisoning attacks in FL, accommodating all benign clients' contributions."}}
{"id": "2507.00671", "pdf": "https://arxiv.org/pdf/2507.00671", "abs": "https://arxiv.org/abs/2507.00671", "authors": ["Congye Wang", "Matthew A. Fisher", "Heishiro Kanagawa", "Wilson Chen", "Chris. J. Oates"], "title": "Harnessing the Power of Reinforcement Learning for Adaptive MCMC", "categories": ["stat.CO", "cs.LG", "stat.ML"], "comment": null, "summary": "Sampling algorithms drive probabilistic machine learning, and recent years\nhave seen an explosion in the diversity of tools for this task. However, the\nincreasing sophistication of sampling algorithms is correlated with an increase\nin the tuning burden. There is now a greater need than ever to treat the tuning\nof samplers as a learning task in its own right. In a conceptual breakthrough,\nWang et al (2025) formulated Metropolis-Hastings as a Markov decision process,\nopening up the possibility for adaptive tuning using Reinforcement Learning\n(RL). Their emphasis was on theoretical foundations; realising the practical\nbenefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for\nsubsequent work. The purpose of this paper is twofold: First, we observe the\nsurprising result that natural choices of reward, such as the acceptance rate,\nor the expected squared jump distance, provide insufficient signal for training\nRLMH. Instead, we propose a novel reward based on the contrastive divergence,\nwhose superior performance in the context of RLMH is demonstrated. Second, we\nexplore the potential of RLMH and present adaptive gradient-based samplers that\nbalance flexibility of the Markov transition kernel with learnability of the\nassociated RL task. A comprehensive simulation study using the posteriordb\nbenchmark supports the practical effectiveness of RLMH.", "AI": {"tldr": "The paper addresses the tuning burden in advanced sampling algorithms by proposing a novel reward for Reinforcement Learning Metropolis-Hastings (RLMH) and demonstrating its practical effectiveness.", "motivation": "The increasing sophistication of sampling algorithms has led to a higher tuning burden, necessitating adaptive tuning methods like RL.", "method": "The authors formulate Metropolis-Hastings as a Markov decision process, propose a novel reward based on contrastive divergence, and develop adaptive gradient-based samplers.", "result": "The proposed reward outperforms traditional choices, and RLMH shows practical effectiveness in simulations.", "conclusion": "RLMH with the novel reward offers a promising solution for adaptive tuning in probabilistic machine learning."}}
{"id": "2406.00772", "pdf": "https://arxiv.org/pdf/2406.00772", "abs": "https://arxiv.org/abs/2406.00772", "authors": ["Cristiano Patr\u00edcio", "Carlo Alberto Barbano", "Attilio Fiandrotti", "Riccardo Renzulli", "Marco Grangetto", "Luis F. Teixeira", "Jo\u00e3o C. Neves"], "title": "Unsupervised contrastive analysis for anomaly detection in brain MRIs via conditional diffusion models", "categories": ["cs.CV"], "comment": "Under consideration at Pattern Recognition Letters", "summary": "Contrastive Analysis (CA) detects anomalies by contrasting patterns unique to\na target group (e.g., unhealthy subjects) from those in a background group\n(e.g., healthy subjects). In the context of brain MRIs, existing CA approaches\nrely on supervised contrastive learning or variational autoencoders (VAEs)\nusing both healthy and unhealthy data, but such reliance on target samples is\nchallenging in clinical settings. Unsupervised Anomaly Detection (UAD) offers\nan alternative by learning a reference representation of healthy anatomy\nwithout the need for target samples. Deviations from this reference\ndistribution can indicate potential anomalies. In this context, diffusion\nmodels have been increasingly adopted in UAD due to their superior performance\nin image generation compared to VAEs. Nonetheless, precisely reconstructing the\nanatomy of the brain remains a challenge. In this work, we propose an\nunsupervised framework to improve the reconstruction quality by training a\nself-supervised contrastive encoder on healthy images to extract meaningful\nanatomical features. These features are used to condition a diffusion model to\nreconstruct the healthy appearance of a given image, enabling interpretable\nanomaly localization via pixel-wise comparison. We validate our approach\nthrough a proof-of-concept on a facial image dataset and further demonstrate\nits effectiveness on four brain MRI datasets, achieving state-of-the-art\nanomaly localization performance on the NOVA benchmark.", "AI": {"tldr": "Proposes an unsupervised framework using a self-supervised contrastive encoder and diffusion model to improve brain MRI reconstruction for anomaly detection.", "motivation": "Existing methods rely on target samples (unhealthy data), which are hard to obtain in clinical settings. Unsupervised anomaly detection (UAD) avoids this but struggles with precise brain anatomy reconstruction.", "method": "Trains a self-supervised contrastive encoder on healthy images to extract features, then conditions a diffusion model for reconstruction. Anomalies are localized via pixel-wise comparison.", "result": "Achieves state-of-the-art anomaly localization on the NOVA benchmark and demonstrates effectiveness on four brain MRI datasets.", "conclusion": "The framework improves reconstruction quality and anomaly detection without needing unhealthy samples, making it clinically viable."}}
{"id": "2402.01020", "pdf": "https://arxiv.org/pdf/2402.01020", "abs": "https://arxiv.org/abs/2402.01020", "authors": ["Jason Lo"], "title": "Quantifying analogy of concepts via ologs and wiring diagrams", "categories": ["cs.LO", "cs.AI", "cs.DM", "math.CO", "math.CT", "68T30 (Primary) 68T20, 68P05, 68T40 (Secondary)", "I.2.4; I.2.8"], "comment": "30 pages. Minor updates to Section 5", "summary": "We build on the theory of ontology logs (ologs) created by Spivak and Kent,\nand define a notion of wiring diagrams. In this article, a wiring diagram is a\nfinite directed labelled graph. The labels correspond to types in an olog; they\ncan also be interpreted as readings of sensors in an autonomous system. As\nsuch, wiring diagrams can be used as a framework for an autonomous system to\nform abstract concepts. We show that the graphs underlying skeleton wiring\ndiagrams form a category. This allows skeleton wiring diagrams to be compared\nand manipulated using techniques from both graph theory and category theory. We\nalso extend the usual definition of graph edit distance to the case of wiring\ndiagrams by using operations only available to wiring diagrams, leading to a\nmetric on the set of all skeleton wiring diagrams. In the end, we give an\nextended example on calculating the distance between two concepts represented\nby wiring diagrams, and explain how to apply our framework to any application\ndomain.", "AI": {"tldr": "The paper extends ologs with wiring diagrams, forming a category for abstract concept representation, and introduces a metric for comparing diagrams.", "motivation": "To provide a framework for autonomous systems to form abstract concepts using wiring diagrams derived from ologs.", "method": "Defines wiring diagrams as finite directed labeled graphs, forms a category from their underlying graphs, and extends graph edit distance to wiring diagrams.", "result": "Shows that skeleton wiring diagrams form a category and introduces a metric for comparing them.", "conclusion": "Demonstrates practical application with an example and suggests broader applicability across domains."}}
{"id": "2507.00683", "pdf": "https://arxiv.org/pdf/2507.00683", "abs": "https://arxiv.org/abs/2507.00683", "authors": ["Satadeep Bhattacharjee", "Seung-Cheol Lee"], "title": "Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians we obtain analytic\n\\textit{phase boundaries} logit gap criteria that predict which token should\ndominate the next-token distribution for a given context. A systematic\nevaluation on 144 heads across 20 factual-recall prompts reveals a strong\nnegative correlation between the theoretical logit gaps and the model's\nempirical token rankings ($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations\nfurther show that suppressing the heads most aligned with the spin-bath\npredictions induces the anticipated shifts in output probabilities, confirming\na causal link rather than a coincidental association. Taken together, our\nfindings provide the first strong empirical evidence for the spin-bath analogy\nin a production-grade model. This validation not only furnishes a tractable,\nphysics-inspired lens for interpretability but also provides the groundwork for\nnovel generative models, bridging the gap between theoretical condensed matter\nphysics and AI.", "AI": {"tldr": "The paper validates the spin-bath analogy for LLM attention mechanisms using GPT-2, showing strong empirical evidence and causal links.", "motivation": "To empirically test the physics-based spin-bath analogy for LLM attention mechanisms and bridge condensed matter physics with AI.", "method": "Extracted Query-Key weight matrices from GPT-2, derived effective Hamiltonians, and evaluated phase boundaries against empirical token rankings.", "result": "Found strong negative correlation (r\u2248-0.70, p<10^-3) between theoretical logit gaps and empirical rankings, with causal links confirmed via ablations.", "conclusion": "Provides first strong empirical validation of the spin-bath analogy, offering a physics-inspired interpretability lens and groundwork for new generative models."}}
{"id": "2406.04814", "pdf": "https://arxiv.org/pdf/2406.04814", "abs": "https://arxiv.org/abs/2406.04814", "authors": ["Jason Yoo", "Yingchen He", "Saeid Naderiparizi", "Dylan Green", "Gido M. van de Ven", "Geoff Pleiss", "Frank Wood"], "title": "Lifelong Learning of Video Diffusion Models From a Single Video Stream", "categories": ["cs.CV", "cs.LG"], "comment": "Video samples are available here:\n  https://drive.google.com/drive/folders/1CsmWqug-CS7I6NwGDvHsEN9FqN2QzspN", "summary": "This work demonstrates that training autoregressive video diffusion models\nfrom a single video stream$\\unicode{x2013}$resembling the experience of\nembodied agents$\\unicode{x2013}$is not only possible, but can also be as\neffective as standard offline training given the same number of gradient steps.\nOur work further reveals that this main result can be achieved using experience\nreplay methods that only retain a subset of the preceding video stream. To\nsupport training and evaluation in this setting, we introduce four new datasets\nfor streaming lifelong generative video modeling: Lifelong Bouncing Balls,\nLifelong 3D Maze, Lifelong Drive, and Lifelong PLAICraft, each consisting of\none million consecutive frames from environments of increasing complexity.", "AI": {"tldr": "Training autoregressive video diffusion models from a single video stream is effective and comparable to offline training, with experience replay methods retaining only a subset of the stream.", "motivation": "To explore the feasibility and effectiveness of training video diffusion models from a single video stream, resembling embodied agents' experience.", "method": "Uses autoregressive video diffusion models and experience replay methods, retaining a subset of the preceding video stream. Introduces four new datasets for evaluation.", "result": "Demonstrates that training from a single video stream is as effective as offline training with the same gradient steps.", "conclusion": "The approach is viable and efficient, supported by new datasets for streaming lifelong generative video modeling."}}
{"id": "2402.10747", "pdf": "https://arxiv.org/pdf/2402.10747", "abs": "https://arxiv.org/abs/2402.10747", "authors": ["Peter Pavl\u00edk", "Martin V\u00fdboh", "Anna Bou Ezzeddine", "Viera Rozinajov\u00e1"], "title": "Fully Differentiable Lagrangian Convolutional Neural Network for Physics-Informed Precipitation Nowcasting", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.1; J.2"], "comment": "Submitted to Applied Computing and Geosciences", "summary": "This paper presents a convolutional neural network model for precipitation\nnowcasting that combines data-driven learning with physics-informed domain\nknowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed\nNowcasting, that draws from existing extrapolation-based nowcasting methods. It\nconsists of a U-Net that dynamically produces mesoscale advection motion\nfields, a differentiable semi-Lagrangian extrapolation operator, and an\nadvection-free U-Net capturing the growth and decay of precipitation over time.\nUsing our approach, we successfully implement the Lagrangian convolutional\nneural network for precipitation nowcasting in a fully differentiable and\nGPU-accelerated manner. This allows for end-to-end training and inference,\nincluding the data-driven Lagrangian coordinate system transformation of the\ndata at runtime. We evaluate the model and compare it with other related\nAI-based models both quantitatively and qualitatively in an extreme event case\nstudy. Based on our evaluation, LUPIN matches and even exceeds the performance\nof the chosen benchmarks, opening the door for other Lagrangian machine\nlearning models.", "AI": {"tldr": "LUPIN, a Lagrangian Double U-Net model, combines physics-informed domain knowledge with data-driven learning for precipitation nowcasting, outperforming benchmarks.", "motivation": "To improve precipitation nowcasting by integrating physics-informed methods with deep learning for better accuracy and adaptability.", "method": "LUPIN uses a U-Net for advection motion fields, a semi-Lagrangian extrapolation operator, and an advection-free U-Net for precipitation dynamics, enabling end-to-end training.", "result": "LUPIN matches or exceeds benchmark performance in extreme event case studies, demonstrating effectiveness.", "conclusion": "The success of LUPIN paves the way for Lagrangian machine learning models in nowcasting and related fields."}}
{"id": "2507.00719", "pdf": "https://arxiv.org/pdf/2507.00719", "abs": "https://arxiv.org/abs/2507.00719", "authors": ["Anantha Narayanan Suresh Babu", "Akhil Sadam", "Pierre F. J. Lermusiaux"], "title": "Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence", "categories": ["physics.flu-dyn", "cs.LG", "physics.ao-ph", "physics.geo-ph"], "comment": "56 pages, 23 figures, 7 tables", "summary": "Typically, numerical simulations of the ocean, weather, and climate are\ncoarse, and observations are sparse and gappy. In this work, we apply four\ngenerative diffusion modeling approaches to super-resolution and inference of\nforced two-dimensional quasi-geostrophic turbulence on the beta-plane from\ncoarse, sparse, and gappy observations. Two guided approaches minimally adapt a\npre-trained unconditional model: SDEdit modifies the initial condition, and\nDiffusion Posterior Sampling (DPS) modifies the reverse diffusion process\nscore. The other two conditional approaches, a vanilla variant and\nclassifier-free guidance, require training with paired high-resolution and\nobservation data. We consider eight test cases spanning: two regimes, eddy and\nanisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two\nobservation types, 4x coarse-resolution fields and coarse, sparse and gappy\nobservations. Our comprehensive skill metrics include norms of the\nreconstructed vorticity fields, turbulence statistical quantities, and\nquantification of the super-resolved probabilistic ensembles and their errors.\nWe also study the sensitivity to tuning parameters such as guidance strength.\nResults show that SDEdit generates unphysical fields, while DPS generates\nreasonable reconstructions at low computational cost but with smoothed\nfine-scale features. Both conditional approaches require re-training, but they\nreconstruct missing fine-scale features, are cycle-consistent with\nobservations, and possess the correct statistics such as energy spectra.\nFurther, their mean model errors are highly correlated with and predictable\nfrom their ensemble standard deviations. Results highlight the trade-offs\nbetween ease of implementation, fidelity (sharpness), and cycle-consistency of\nthe diffusion models, and offer practical guidance for deployment in\ngeophysical inverse problems.", "AI": {"tldr": "The paper explores four generative diffusion models for super-resolution and inference in 2D quasi-geostrophic turbulence, comparing their performance, computational cost, and fidelity.", "motivation": "To address the challenges of coarse, sparse, and gappy observations in numerical simulations of ocean, weather, and climate.", "method": "Four approaches are tested: SDEdit, DPS, a vanilla conditional model, and classifier-free guidance, evaluated across eight test cases with varying turbulence regimes, Reynolds numbers, and observation types.", "result": "SDEdit produces unphysical fields; DPS is cost-effective but smooths fine features. Conditional models reconstruct fine details and maintain statistical accuracy but require retraining.", "conclusion": "The study highlights trade-offs between implementation ease, fidelity, and cycle-consistency, offering practical insights for geophysical inverse problems."}}
{"id": "2409.07995", "pdf": "https://arxiv.org/pdf/2409.07995", "abs": "https://arxiv.org/abs/2409.07995", "authors": ["Siyu Chen", "Ting Han", "Changshe Zhang", "Weiquan Liu", "Jinhe Su", "Zongyue Wang", "Guorong Cai"], "title": "Depth Matters: Exploring Deep Interactions of RGB-D for Semantic Segmentation in Traffic Scenes", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "RGB-D has gradually become a crucial data source for understanding complex\nscenes in assisted driving. However, existing studies have paid insufficient\nattention to the intrinsic spatial properties of depth maps. This oversight\nsignificantly impacts the attention representation, leading to prediction\nerrors caused by attention shift issues. To this end, we propose a novel\nlearnable Depth interaction Pyramid Transformer (DiPFormer) to explore the\neffectiveness of depth. Firstly, we introduce Depth Spatial-Aware Optimization\n(Depth SAO) as offset to represent real-world spatial relationships. Secondly,\nthe similarity in the feature space of RGB-D is learned by Depth Linear\nCross-Attention (Depth LCA) to clarify spatial differences at the pixel level.\nFinally, an MLP Decoder is utilized to effectively fuse multi-scale features\nfor meeting real-time requirements. Comprehensive experiments demonstrate that\nthe proposed DiPFormer significantly addresses the issue of attention\nmisalignment in both road detection (+7.5%) and semantic segmentation (+4.9% /\n+1.5%) tasks. DiPFormer achieves state-of-the-art performance on the KITTI\n(97.57% F-score on KITTI road and 68.74% mIoU on KITTI-360) and Cityscapes\n(83.4% mIoU) datasets.", "AI": {"tldr": "DiPFormer improves RGB-D scene understanding by addressing depth map spatial properties, enhancing attention representation, and achieving state-of-the-art results in road detection and semantic segmentation.", "motivation": "Existing studies overlook intrinsic spatial properties of depth maps, causing attention shift issues and prediction errors.", "method": "Proposes DiPFormer with Depth SAO for spatial relationships, Depth LCA for feature similarity, and an MLP Decoder for multi-scale feature fusion.", "result": "Significant improvements in road detection (+7.5%) and semantic segmentation (+4.9% / +1.5%), with top performance on KITTI and Cityscapes datasets.", "conclusion": "DiPFormer effectively addresses attention misalignment and sets new benchmarks for RGB-D scene understanding."}}
{"id": "2409.09111", "pdf": "https://arxiv.org/pdf/2409.09111", "abs": "https://arxiv.org/abs/2409.09111", "authors": ["Qitian Wu", "David Wipf", "Junchi Yan"], "title": "Transformers from Diffusion: A Unified Framework for Neural Message Passing", "categories": ["cs.LG", "cs.AI"], "comment": "Published in Journal of Machine Learning Research (JMLR). Extended\n  from DIFFormer in ICLR 2023", "summary": "Learning representations for structured data with certain geometries (e.g.,\nobserved or unobserved) is a fundamental challenge, wherein message passing\nneural networks (MPNNs) have become a de facto class of model solutions. In\nthis paper, inspired by physical systems, we propose an energy-constrained\ndiffusion model, which integrates the inductive bias of diffusion on manifolds\nwith layer-wise constraints of energy minimization. We identify that the\ndiffusion operators have a one-to-one correspondence with the energy functions\nimplicitly descended by the diffusion process, and the finite-difference\niteration for solving the energy-constrained diffusion system induces the\npropagation layers of various types of MPNNs operating on observed or latent\nstructures. This leads to a unified mathematical framework for common neural\narchitectures whose computational flows can be cast as message passing (or its\nspecial case), including MLPs, GNNs, and Transformers. Building on these\ninsights, we devise a new class of neural message passing models, dubbed\ndiffusion-inspired Transformers (DIFFormer), whose global attention layers are\nderived from the principled energy-constrained diffusion framework. Across\ndiverse datasets ranging from real-world networks to images, texts, and\nphysical particles, we demonstrate that the new model achieves promising\nperformance in scenarios where the data structures are observed (as a graph),\npartially observed, or entirely unobserved.", "AI": {"tldr": "The paper proposes an energy-constrained diffusion model inspired by physical systems, unifying MPNNs under a single framework and introducing DIFFormer, a new model with promising performance across diverse datasets.", "motivation": "To address the challenge of learning representations for structured data with geometries, leveraging physical system-inspired diffusion models to unify and improve neural architectures like MPNNs.", "method": "Develops an energy-constrained diffusion model, linking diffusion operators to energy functions, and derives propagation layers for MPNNs. Introduces DIFFormer, a diffusion-inspired Transformer.", "result": "Demonstrates strong performance on datasets with observed, partially observed, or unobserved structures, validating the framework's versatility.", "conclusion": "The proposed framework unifies MPNNs and introduces DIFFormer, offering a principled approach for learning structured data representations with broad applicability."}}
{"id": "2507.00747", "pdf": "https://arxiv.org/pdf/2507.00747", "abs": "https://arxiv.org/abs/2507.00747", "authors": ["Diemen Delgado-Cano", "Erick Kracht", "Urban Fasel", "Benjamin Herrmann"], "title": "SINDy on slow manifolds", "categories": ["math.DS", "cs.LG", "physics.comp-ph"], "comment": "18 pages, 6 figures, to be submitted to Nonlinear Dynamics (Springer)", "summary": "The sparse identification of nonlinear dynamics (SINDy) has been established\nas an effective method to learn interpretable models of dynamical systems from\ndata. However, for high-dimensional slow-fast dynamical systems, the regression\nproblem becomes simultaneously computationally intractable and ill-conditioned.\nAlthough, in principle, modeling only the dynamics evolving on the underlying\nslow manifold addresses both of these challenges, the truncated fast variables\nhave to be compensated by including higher-order nonlinearities as candidate\nterms for the model, leading to an explosive growth in the size of the SINDy\nlibrary. In this work, we develop a SINDy variant that is able to robustly and\nefficiently identify slow-fast dynamics in two steps: (i) identify the slow\nmanifold, that is, an algebraic equation for the fast variables as functions of\nthe slow ones, and (ii) learn a model for the dynamics of the slow variables\nrestricted to the manifold. Critically, the equation learned in (i) is\nleveraged to build a manifold-informed function library for (ii) that contains\nonly essential higher-order nonlinearites as candidate terms. Rather than\ncontaining all monomials of up to a certain degree, the resulting custom\nlibrary is a sparse subset of the latter that is tailored to the specific\nproblem at hand. The approach is demonstrated on numerical examples of a\nsnap-through buckling beam and the flow over a NACA 0012 airfoil. We find that\nour method significantly reduces both the condition number and the size of the\nSINDy library, thus enabling accurate identification of the dynamics on slow\nmanifolds.", "AI": {"tldr": "A SINDy variant is developed to efficiently identify slow-fast dynamics by first identifying the slow manifold and then learning the slow variables' dynamics, reducing computational complexity and improving accuracy.", "motivation": "High-dimensional slow-fast dynamical systems make regression computationally intractable and ill-conditioned, requiring a more efficient method.", "method": "Two-step approach: (i) identify the slow manifold (fast variables as functions of slow ones), (ii) learn slow variables' dynamics using a custom, sparse library of essential nonlinearities.", "result": "Reduces condition number and library size, enabling accurate identification of slow manifold dynamics, demonstrated on a buckling beam and airfoil flow.", "conclusion": "The method effectively addresses computational and conditioning challenges in SINDy for slow-fast systems."}}
{"id": "2410.05255", "pdf": "https://arxiv.org/pdf/2410.05255", "abs": "https://arxiv.org/abs/2410.05255", "authors": ["Daoan Zhang", "Guangchen Lan", "Dong-Jun Han", "Wenlin Yao", "Xiaoman Pan", "Hongming Zhang", "Mingxiao Li", "Pengcheng Chen", "Yu Dong", "Christopher Brinton", "Jiebo Luo"], "title": "Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization", "categories": ["cs.CV", "cs.LG", "I.2.6; I.2.10; I.4.0; I.5.0"], "comment": null, "summary": "Existing post-training techniques are broadly categorized into supervised\nfine-tuning (SFT) and reinforcement learning (RL) methods; the former is stable\nduring training but suffers from limited generalization, while the latter,\ndespite its stronger generalization capability, relies on additional preference\ndata or reward models and carries the risk of reward exploitation. In order to\npreserve the advantages of both SFT and RL -- namely, eliminating the need for\npaired data and reward models while retaining the training stability of SFT and\nthe generalization ability of RL -- a new alignment method, Self-Sampling\nPreference Optimization (SSPO), is proposed in this paper. SSPO introduces a\nRandom Checkpoint Replay (RCR) strategy that utilizes historical checkpoints to\nconstruct paired data, thereby effectively mitigating overfitting.\nSimultaneously, a Self-Sampling Regularization (SSR) strategy is employed to\ndynamically evaluate the quality of generated samples; when the generated\nsamples are more likely to be winning samples, the approach automatically\nswitches from DPO (Direct Preference Optimization) to SFT, ensuring that the\ntraining process accurately reflects the quality of the samples. Experimental\nresults demonstrate that SSPO not only outperforms existing methods on\ntext-to-image benchmarks, but its effectiveness has also been validated in\ntext-to-video tasks. We validate SSPO across both text-to-image and\ntext-to-video benchmarks. SSPO surpasses all previous approaches on the\ntext-to-image benchmarks and demonstrates outstanding performance on the\ntext-to-video benchmarks.", "AI": {"tldr": "SSPO combines SFT and RL advantages, using RCR and SSR to improve generalization and stability, outperforming existing methods in text-to-image and text-to-video tasks.", "motivation": "To address the limitations of SFT (limited generalization) and RL (dependency on preference data/reward models, risk of reward exploitation) by combining their strengths.", "method": "Proposes SSPO with RCR (uses historical checkpoints for paired data) and SSR (dynamically evaluates sample quality, switches between DPO and SFT).", "result": "Outperforms existing methods on text-to-image benchmarks and shows strong performance in text-to-video tasks.", "conclusion": "SSPO effectively balances stability and generalization, proving superior in multimodal generation tasks."}}
{"id": "2409.12446", "pdf": "https://arxiv.org/pdf/2409.12446", "abs": "https://arxiv.org/abs/2409.12446", "authors": ["Sourav Chatterjee", "Timothy Sudijono"], "title": "Neural Networks Generalize on Low Complexity Data", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": "37 pages. V4: sharpened results and typos fixed", "summary": "We show that feedforward neural networks with ReLU activation generalize on\nlow complexity data, suitably defined. Given i.i.d.~data generated from a\nsimple programming language, the minimum description length (MDL) feedforward\nneural network which interpolates the data generalizes with high probability.\nWe define this simple programming language, along with a notion of description\nlength of such networks. We provide several examples on basic computational\ntasks, such as checking primality of a natural number. For primality testing,\nour theorem shows the following and more. Suppose that we draw an i.i.d.~sample\nof $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let\n$y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL\nnetwork accurately answers, with error probability $1- O((\\ln N)/n)$, whether a\nnewly drawn number between $1$ and $N$ is a prime or not. Note that the network\nis not designed to detect primes; minimum description learning discovers a\nnetwork which does so. Extensions to noisy data are also discussed, suggesting\nthat MDL neural network interpolators can demonstrate tempered overfitting.", "AI": {"tldr": "Feedforward neural networks with ReLU activation generalize on low-complexity data, demonstrated via a simple programming language and MDL principles.", "motivation": "To understand how neural networks generalize on structured, low-complexity data and whether MDL-based networks can discover underlying patterns (e.g., primality) without explicit design.", "method": "Define a simple programming language and MDL for networks. Train MDL networks to interpolate i.i.d. data (e.g., primality labels) and evaluate generalization.", "result": "MDL networks generalize well, e.g., accurately predict primality with error probability $1-O((\\ln N)/n)$. Also handles noisy data, showing tempered overfitting.", "conclusion": "MDL-based neural networks can effectively generalize on low-complexity tasks, even discovering patterns like primality without explicit training."}}
{"id": "2507.00866", "pdf": "https://arxiv.org/pdf/2507.00866", "abs": "https://arxiv.org/abs/2507.00866", "authors": ["Jonas Chris Ferrao", "Dickson Dias", "Pranav Naik", "Glory D'Cruz", "Anish Naik", "Siya Khandeparkar", "Manisha Gokuldas Fal Dessai"], "title": "Template-Fitting Meets Deep Learning: Redshift Estimation Using Physics-Guided Neural Networks", "categories": ["astro-ph.IM", "cs.LG"], "comment": null, "summary": "Accurate photometric redshift estimation is critical for observational\ncosmology, especially in large-scale surveys where spectroscopic measurements\nare impractical. Traditional approaches include template fitting and machine\nlearning, each with distinct strengths and limitations. We present a hybrid\nmethod that integrates template fitting with deep learning using physics-guided\nneural networks. By embedding spectral energy distribution templates into the\nnetwork architecture, our model encodes physical priors into the training\nprocess. The system employs a multimodal design, incorporating cross-attention\nmechanisms to fuse photometric and image data, along with Bayesian layers for\nuncertainty estimation. We evaluate our model on the publicly available PREML\ndataset, which includes approximately 400,000 galaxies from the Hyper\nSuprime-Cam PDR3 release, with 5-band photometry, multi-band imaging, and\nspectroscopic redshifts. Our approach achieves an RMS error of 0.0507, a\n3-sigma catastrophic outlier rate of 0.13%, and a bias of 0.0028. The model\nsatisfies two of the three LSST photometric redshift requirements for redshifts\nbelow 3. These results highlight the potential of combining physically\nmotivated templates with data-driven models for robust redshift estimation in\nupcoming cosmological surveys.", "AI": {"tldr": "A hybrid method combining template fitting and deep learning for photometric redshift estimation achieves high accuracy and low bias, meeting some LSST survey requirements.", "motivation": "Accurate photometric redshift estimation is crucial for cosmology, especially in large surveys where spectroscopy is impractical. Traditional methods have limitations, prompting the need for a hybrid approach.", "method": "The method integrates template fitting with deep learning using physics-guided neural networks, embedding spectral templates and employing multimodal design with cross-attention and Bayesian layers.", "result": "Tested on the PREML dataset (400,000 galaxies), the model achieves RMS error of 0.0507, 0.13% catastrophic outliers, and bias of 0.0028, meeting two LSST requirements for redshifts below 3.", "conclusion": "Combining physical templates with data-driven models shows promise for robust redshift estimation in future cosmological surveys."}}
{"id": "2410.11281", "pdf": "https://arxiv.org/pdf/2410.11281", "abs": "https://arxiv.org/abs/2410.11281", "authors": ["Eduardo Hirata-Miyasaki", "Soorya Pradeep", "Ziwen Liu", "Alishba Imran", "Taylla Milena Theodoro", "Ivan E. Ivanov", "Sudip Khadka", "See-Chi Lee", "Michelle Grunberg", "Hunter Woosley", "Madhura Bhave", "Carolina Arias", "Shalin B. Mehta"], "title": "DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal Regularization", "categories": ["cs.CV", "q-bio.QM", "I.2.6; J.3"], "comment": "30 pages, 6 figures, 13 appendix figures, 5 videos (ancillary files)", "summary": "We report DynaCLR, a self-supervised method for embedding cell and organelle\nDynamics via Contrastive Learning of Representations of time-lapse images.\nDynaCLR integrates single-cell tracking and time-aware contrastive sampling to\nlearn robust, temporally regularized representations of cell dynamics. DynaCLR\nembeddings generalize effectively to in-distribution and out-of-distribution\ndatasets, and can be used for several downstream tasks with sparse human\nannotations. We demonstrate efficient annotations of cell states with a\nhuman-in-the-loop using fluorescence and label-free imaging channels. DynaCLR\nmethod enables diverse downstream biological analyses: classification of cell\ndivision and infection, clustering heterogeneous cell migration patterns,\ncross-modal distillation of cell states from fluorescence to label-free\nchannel, alignment of asynchronous cellular responses and broken cell tracks,\nand discovering organelle response due to infection. DynaCLR is a flexible\nmethod for comparative analyses of dynamic cellular responses to\npharmacological, microbial, and genetic perturbations. We provide PyTorch-based\nimplementations of the model training and inference pipeline\n(https://github.com/mehta-lab/viscy) and a GUI\n(https://github.com/czbiohub-sf/napari-iohub) for the visualization and\nannotation of trajectories of cells in the real space and the embedding space.", "AI": {"tldr": "DynaCLR is a self-supervised method for embedding cell and organelle dynamics using contrastive learning on time-lapse images. It integrates single-cell tracking and time-aware sampling for robust representations, generalizing well to various datasets and enabling diverse downstream biological tasks with minimal human annotations.", "motivation": "The motivation is to develop a flexible, self-supervised method for analyzing dynamic cellular responses to perturbations, reducing reliance on extensive human annotations.", "method": "DynaCLR combines single-cell tracking and time-aware contrastive sampling to learn temporally regularized representations of cell dynamics.", "result": "The method generalizes effectively to in-distribution and out-of-distribution datasets, enabling tasks like cell state classification, clustering migration patterns, and cross-modal distillation.", "conclusion": "DynaCLR is a versatile tool for comparative analyses of cellular dynamics, with applications in pharmacological, microbial, and genetic studies, supported by open-source implementations."}}
{"id": "2411.04403", "pdf": "https://arxiv.org/pdf/2411.04403", "abs": "https://arxiv.org/abs/2411.04403", "authors": ["Zhichao Geng", "Yiwen Wang", "Dongyu Ru", "Yang Yang"], "title": "Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Learned sparse retrieval, which can efficiently perform retrieval through\nmature inverted-index engines, has garnered growing attention in recent years.\nParticularly, the inference-free sparse retrievers are attractive as they\neliminate online model inference in the retrieval phase thereby avoids huge\ncomputational cost, offering reasonable throughput and latency. However, even\nthe state-of-the-art (SOTA) inference-free sparse models lag far behind in\nterms of search relevance when compared to both sparse and dense siamese\nmodels. Towards competitive search relevance for inference-free sparse\nretrievers, we argue that they deserve dedicated training methods other than\nusing same ones with siamese encoders. In this paper, we propose two different\napproaches for performance improvement. First, we propose an IDF-aware penalty\nfor the matching function that suppresses the contribution of low-IDF tokens\nand increases the model's focus on informative terms. Moreover, we propose a\nheterogeneous ensemble knowledge distillation framework that combines siamese\ndense and sparse retrievers to generate supervisory signals during the\npre-training phase. The ensemble framework of dense and sparse retriever\ncapitalizes on their strengths respectively, providing a strong upper bound for\nknowledge distillation. To concur the diverse feedback from heterogeneous\nsupervisors, we normalize and then aggregate the outputs of the teacher models\nto eliminate score scale differences. On the BEIR benchmark, our model\noutperforms existing SOTA inference-free sparse model by \\textbf{3.3 NDCG@10\nscore}. It exhibits search relevance comparable to siamese sparse retrievers\nand client-side latency only \\textbf{1.1x that of BM25}.", "AI": {"tldr": "The paper proposes two methods to improve search relevance in inference-free sparse retrievers: an IDF-aware penalty and a heterogeneous ensemble knowledge distillation framework, achieving a 3.3 NDCG@10 score boost on BEIR.", "motivation": "Current inference-free sparse retrievers lag behind in search relevance compared to siamese models, necessitating dedicated training methods.", "method": "1. IDF-aware penalty for matching function to focus on informative terms. 2. Heterogeneous ensemble knowledge distillation combining dense and sparse retrievers.", "result": "Outperforms SOTA inference-free sparse models by 3.3 NDCG@10, matching siamese sparse retrievers with low latency.", "conclusion": "The proposed methods significantly enhance search relevance for inference-free sparse retrievers, bridging the gap with siamese models."}}
{"id": "2507.00894", "pdf": "https://arxiv.org/pdf/2507.00894", "abs": "https://arxiv.org/abs/2507.00894", "authors": ["Davide Adamo", "Marco Corneli", "Manon Vuillien", "Emmanuelle Vila"], "title": "An in depth look at the Procrustes-Wasserstein distance: properties and barycenters", "categories": ["stat.ML", "cs.LG"], "comment": "16 pages", "summary": "Due to its invariance to rigid transformations such as rotations and\nreflections, Procrustes-Wasserstein (PW) was introduced in the literature as an\noptimal transport (OT) distance, alternative to Wasserstein and more suited to\ntasks such as the alignment and comparison of point clouds. Having that\napplication in mind, we carefully build a space of discrete probability\nmeasures and show that over that space PW actually is a distance. Algorithms to\nsolve the PW problems already exist, however we extend the PW framework by\ndiscussing and testing several initialization strategies. We then introduce the\nnotion of PW barycenter and detail an algorithm to estimate it from the data.\nThe result is a new method to compute representative shapes from a collection\nof point clouds. We benchmark our method against existing OT approaches,\ndemonstrating superior performance in scenarios requiring precise alignment and\nshape preservation. We finally show the usefulness of the PW barycenters in an\narchaeological context. Our results highlight the potential of PW in boosting\n2D and 3D point cloud analysis for machine learning and computational geometry\napplications.", "AI": {"tldr": "The paper introduces Procrustes-Wasserstein (PW) as an optimal transport distance for point cloud alignment and comparison, proves it's a distance, extends the framework with initialization strategies, and introduces PW barycenters for shape representation.", "motivation": "To address the need for a distance metric invariant to rigid transformations (rotations/reflections) for tasks like point cloud alignment and comparison.", "method": "Builds a space of discrete probability measures to prove PW is a distance, extends PW with initialization strategies, and introduces PW barycenters with a detailed algorithm.", "result": "Demonstrates superior performance in alignment and shape preservation compared to existing OT methods, with practical applications in archaeology.", "conclusion": "PW enhances 2D/3D point cloud analysis, showing promise for machine learning and computational geometry."}}
{"id": "2411.09105", "pdf": "https://arxiv.org/pdf/2411.09105", "abs": "https://arxiv.org/abs/2411.09105", "authors": ["Chenglin Li", "Qianglong Chen", "Zhi Li", "Feng Tao", "Yin Zhang"], "title": "VideoCogQA: A Controllable Benchmark for Evaluating Cognitive Abilities in Video-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Video-Language Models (LVLMs) have led to\npromising results in multimodal video understanding. However, it remains\nunclear whether these models possess the cognitive capabilities required for\nhigh-level tasks, particularly those involving symbolic and abstract\nperception. Existing benchmarks typically rely on real-world, annotated videos,\nwhich lack control over video content and inherent difficulty, limiting their\ndiagnostic power. To bridge this gap, we propose VideoCogQA, a scalable and\nfully controllable benchmark inspired by game-world environments, designed to\nevaluate the cognitive abilities of LVLMs. By generating synthetic videos via a\nprogrammatic engine, VideoCogQA allows fine-grained control over visual\nelements, temporal dynamics, and task difficulty. This approach enables a\nfocused evaluation of video cognitive abilities, independent of prior knowledge\nfrom visual scene semantics. The dataset includes 800 videos and 3,280\nquestion-answer pairs, featuring tasks related to abstract concepts, symbolic\nelements, and multimodal integration, with varying levels of difficulty.\nExperimental results show that even state-of-the-art (SOTA) models, such as\nGPT-4o, achieve an average performance of 48.8% on tasks involving abstract\nconcepts. Additionally, performance drops by 15% as task complexity increases,\nhighlighting the challenges LVLMs face in maintaining consistent performance.\nThrough this work, we hope to show the limitations of current LVLMs and offer\ninsights into how they can more effectively emulate human cognitive processes\nin the future.", "AI": {"tldr": "VideoCogQA is a synthetic benchmark to evaluate cognitive abilities of LVLMs, revealing limitations in abstract and symbolic tasks.", "motivation": "Existing benchmarks lack control and diagnostic power for evaluating high-level cognitive abilities in LVLMs.", "method": "Proposed VideoCogQA, a scalable benchmark using synthetic videos with fine-grained control over content and difficulty.", "result": "SOTA models like GPT-4o score 48.8% on abstract tasks, with performance dropping 15% as complexity increases.", "conclusion": "Current LVLMs struggle with abstract cognition; VideoCogQA highlights gaps and future improvement directions."}}
{"id": "2411.04946", "pdf": "https://arxiv.org/pdf/2411.04946", "abs": "https://arxiv.org/abs/2411.04946", "authors": ["Amir M. Vahedi", "Horea T. Ilies"], "title": "SPGD: Steepest Perturbed Gradient Descent Optimization", "categories": ["math.OC", "cs.AI", "cs.CE", "cs.LG", "math-ph", "math.MP"], "comment": "28 pages, 26 figures, submitted to Journal of Mechanical Design", "summary": "Optimization algorithms are pivotal in advancing various scientific and\nindustrial fields but often encounter obstacles such as trapping in local\nminima, saddle points, and plateaus (flat regions), which makes the convergence\nto reasonable or near-optimal solutions particularly challenging. This paper\npresents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that\ninnovatively combines the principles of the gradient descent method with\nperiodic uniform perturbation sampling to effectively circumvent these\nimpediments and lead to better solutions whenever possible. SPGD is\ndistinctively designed to generate a set of candidate solutions and select the\none exhibiting the steepest loss difference relative to the current solution.\nIt enhances the traditional gradient descent approach by integrating a\nstrategic exploration mechanism that significantly increases the likelihood of\nescaping sub-optimal local minima and navigating complex optimization\nlandscapes effectively. Our approach not only retains the directed efficiency\nof gradient descent but also leverages the exploratory benefits of stochastic\nperturbations, thus enabling a more comprehensive search for global optima\nacross diverse problem spaces. We demonstrate the efficacy of SPGD in solving\nthe 3D component packing problem, an NP-hard challenge. Preliminary results\nshow a substantial improvement over four established methods, particularly on\nresponse surfaces with complex topographies and in multidimensional non-convex\ncontinuous optimization problems. Comparative analyses with established 2D\nbenchmark functions highlight SPGD's superior performance, showcasing its\nability to navigate complex optimization landscapes. These results emphasize\nSPGD's potential as a versatile tool for a wide range of optimization problems.", "AI": {"tldr": "SPGD combines gradient descent with periodic perturbations to escape local optima, outperforming traditional methods in complex optimization problems like 3D packing.", "motivation": "Overcoming challenges like local minima and saddle points in optimization to achieve better solutions.", "method": "SPGD integrates gradient descent with uniform perturbation sampling, generating candidate solutions and selecting the steepest loss difference.", "result": "SPGD shows substantial improvement over four established methods, especially in complex landscapes and NP-hard problems.", "conclusion": "SPGD is a versatile and effective tool for diverse optimization problems, outperforming traditional approaches."}}
{"id": "2507.00957", "pdf": "https://arxiv.org/pdf/2507.00957", "abs": "https://arxiv.org/abs/2507.00957", "authors": ["Ankit Biswas"], "title": "Atmospheric model-trained machine learning selection and classification of ultracool TY dwarfs", "categories": ["astro-ph.SR", "astro-ph.EP", "astro-ph.IM", "cs.LG"], "comment": "12 pages, 9 figures, to be published in Monthly Notices of the Royal\n  Astronomical Society", "summary": "The T and Y spectral classes represent the coolest and lowest-mass population\nof brown dwarfs, yet their census remains incomplete due to limited statistics.\nExisting detection frameworks are often constrained to identifying M, L, and\nearly T dwarfs, owing to the sparse observational sample of ultracool dwarfs\n(UCDs) at later types. This paper presents a novel machine learning framework\ncapable of detecting and classifying late-T and Y dwarfs, trained entirely on\nsynthetic photometry from atmospheric models. Utilizing grids from the ATMO\n2020 and Sonora Bobcat models, I produce a training dataset over two orders of\nmagnitude larger than any empirical set of >T6 UCDs. Polynomial color relations\nfitted to the model photometry are used to assign spectral types to these\nsynthetic models, which in turn train an ensemble of classifiers to identify\nand classify the spectral type of late UCDs. The model is highly performant\nwhen validating on both synthetic and empirical datasets, verifying catalogs of\nknown UCDs with object classification metrics >99% and an average spectral type\nprecision within 0.35 +/- 0.37 subtypes. Application of the model to a 1.5\ndegree region around Pisces and the UKIDSS UDS field results in the discovery\nof one previously uncatalogued T8.2 candidate, demonstrating the ability of\nthis model-trained approach in discovering faint, late-type UCDs from\nphotometric catalogs.", "AI": {"tldr": "A novel machine learning framework is introduced to detect and classify late-T and Y dwarfs using synthetic photometry, achieving high accuracy and discovering a new T8.2 candidate.", "motivation": "The census of late-T and Y dwarfs is incomplete due to limited observational data, and existing frameworks struggle with these ultracool dwarfs.", "method": "The framework uses synthetic photometry from atmospheric models (ATMO 2020 and Sonora Bobcat) to train an ensemble of classifiers, validated on synthetic and empirical datasets.", "result": "The model achieves >99% classification accuracy and 0.35 +/- 0.37 subtype precision, successfully identifying a new T8.2 candidate.", "conclusion": "The model demonstrates effectiveness in discovering faint, late-type ultracool dwarfs, addressing gaps in current detection methods."}}
{"id": "2411.12787", "pdf": "https://arxiv.org/pdf/2411.12787", "abs": "https://arxiv.org/abs/2411.12787", "authors": ["Pengkun Jiao", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo", "Yu-Gang Jiang"], "title": "From Holistic to Localized: Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal\nLarge Language Models (MLLMs) to downstream tasks with minimal computational\noverhead. However, as task diversity and complexity increase, EVIT faces\nsignificant challenges in resolving data conflicts. To address this limitation,\nwe propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local\nframework that enhances the adapter's capacity to address data conflict through\ndual structural optimization. Specifically, we utilize two subspaces: a skill\nspace for stable, holistic knowledge retention, and a rank-rectified task space\nthat locally activates the holistic knowledge. Additionally, we introduce\nVisual Cue Enhancement (VCE), a multi-level local feature aggregation module\ndesigned to enrich the vision-language projection with local details. Our\napproach is both memory- and time-efficient, requiring only 1.16$\\times$ the\ninference time of the standard LoRA method (with injection into the query and\nvalue projection layers), and just 73\\% of the inference time of a 4-expert\nLoRA-MoE. Extensive experiments on various downstream tasks and general MLLM\nbenchmarks validate the effectiveness of our proposed methods.", "AI": {"tldr": "EVIT introduces Dual-LoRA and VCE to efficiently adapt MLLMs for diverse tasks, addressing data conflicts with minimal overhead.", "motivation": "To resolve data conflicts in EVIT as task diversity grows, enhancing adapter capacity.", "method": "Dual-LoRA (holistic-to-local framework with skill and task spaces) and VCE (multi-level feature aggregation).", "result": "Memory- and time-efficient, outperforming standard LoRA and LoRA-MoE in inference time.", "conclusion": "Dual-LoRA and VCE effectively enhance MLLM adaptation, validated by extensive experiments."}}
{"id": "2411.13949", "pdf": "https://arxiv.org/pdf/2411.13949", "abs": "https://arxiv.org/abs/2411.13949", "authors": ["Ziqi Wang", "Chang Che", "Qi Wang", "Yangyang Li", "Zenglin Shi", "Meng Wang"], "title": "SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual instruction tuning (VIT) enables multimodal large language models\n(MLLMs) to effectively handle a wide range of vision tasks by framing them as\nlanguage-based instructions. Building on this, continual visual instruction\ntuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks,\naccommodating evolving functionalities. While prior work has advanced CVIT\nthrough the development of new benchmarks and approaches to mitigate\ncatastrophic forgetting, these efforts largely follow traditional continual\nlearning paradigms, neglecting the unique challenges specific to CVIT. We\nidentify a dual form of catastrophic forgetting in CVIT, where MLLMs not only\nforget previously learned visual understanding but also experience a decline in\ninstruction following abilities as they acquire new tasks. To address this, we\nintroduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework,\nwhich employs separable routing through two distinct modules-one for visual\nunderstanding and another for instruction following. This dual-routing design\nenables specialized adaptation in both domains, preventing forgetting while\nimproving performance. Furthermore, we propose a new CVIT benchmark that goes\nbeyond existing benchmarks by additionally evaluating a model's ability to\ngeneralize to unseen tasks and handle diverse instructions across various\ntasks. Extensive experiments demonstrate that SMoLoRA outperforms existing\nmethods in mitigating dual forgetting, improving generalization to unseen\ntasks, and ensuring robustness in following diverse instructions. Code is\navailable at https://github.com/Minato-Zackie/SMoLoRA.", "AI": {"tldr": "The paper introduces SMoLoRA, a framework for continual visual instruction tuning (CVIT) to address dual catastrophic forgetting in multimodal large language models (MLLMs), improving performance and generalization.", "motivation": "Prior CVIT efforts overlook unique challenges like dual forgetting (visual understanding and instruction following). The goal is to enhance MLLMs' adaptability to new tasks without losing prior capabilities.", "method": "Proposes SMoLoRA, a dual-routing framework with separate modules for visual understanding and instruction following, using low-rank adaptation to prevent forgetting. Introduces a new CVIT benchmark for evaluation.", "result": "SMoLoRA outperforms existing methods in mitigating forgetting, generalizing to unseen tasks, and handling diverse instructions.", "conclusion": "SMoLoRA effectively addresses dual forgetting in CVIT, offering a robust solution for evolving MLLM functionalities."}}
{"id": "2310.05175", "pdf": "https://arxiv.org/pdf/2310.05175", "abs": "https://arxiv.org/abs/2310.05175", "authors": ["Lu Yin", "You Wu", "Zhenyu Zhang", "Cheng-Yu Hsieh", "Yaqing Wang", "Yiling Jia", "Gen Li", "Ajay Jaiswal", "Mykola Pechenizkiy", "Yi Liang", "Michael Bendersky", "Zhangyang Wang", "Shiwei Liu"], "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity", "categories": ["cs.LG"], "comment": "Published at ICML 2024", "summary": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2.6x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL.", "AI": {"tldr": "The paper introduces OWL, a novel pruning method for LLMs that uses non-uniform layerwise sparsity based on activation outliers, outperforming existing methods in performance and speed.", "motivation": "Large Language Models (LLMs) are difficult to deploy due to their size, and uniform pruning strategies may not be optimal. The study aims to understand why non-uniform sparsity works better in vision models and adapt it for LLMs.", "method": "The authors propose Outlier Weighed Layerwise sparsity (OWL), where sparsity ratios are tailored to each layer's outlier ratio. This aligns weight sparsity with outlier patterns.", "result": "OWL outperforms Wanda and SparseGPT, achieving significant perplexity gains (61.22 and 6.80) at 70% sparsity and a 2.6x speed-up in inference.", "conclusion": "OWL demonstrates that non-uniform sparsity based on outlier ratios is effective for LLM pruning, offering better performance and efficiency than uniform methods."}}
{"id": "2411.19278", "pdf": "https://arxiv.org/pdf/2411.19278", "abs": "https://arxiv.org/abs/2411.19278", "authors": ["Yiming Zuo", "Willow Yang", "Zeyu Ma", "Jia Deng"], "title": "OMNI-DC: Highly Robust Depth Completion with Multiresolution Depth Integration", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025. Added additional results and ablations", "summary": "Depth completion (DC) aims to predict a dense depth map from an RGB image and\na sparse depth map. Existing DC methods generalize poorly to new datasets or\nunseen sparse depth patterns, limiting their real-world applications. We\npropose OMNI-DC, a highly robust DC model that generalizes well zero-shot to\nvarious datasets. The key design is a novel Multi-resolution Depth Integrator,\nallowing our model to deal with very sparse depth inputs. We also introduce a\nnovel Laplacian loss to model the ambiguity in the training process. Moreover,\nwe train OMNI-DC on a mixture of high-quality datasets with a scale\nnormalization technique and synthetic depth patterns. Extensive experiments on\n7 datasets show consistent improvements over baselines, reducing errors by as\nmuch as 43%. Codes and checkpoints are available at\nhttps://github.com/princeton-vl/OMNI-DC.", "AI": {"tldr": "OMNI-DC is a robust depth completion model that generalizes zero-shot to various datasets, improving accuracy by up to 43% over baselines.", "motivation": "Existing depth completion methods struggle with generalization to new datasets or unseen sparse depth patterns, limiting real-world applications.", "method": "OMNI-DC uses a Multi-resolution Depth Integrator for handling sparse inputs, a Laplacian loss for training ambiguity, and is trained on mixed datasets with scale normalization and synthetic depth patterns.", "result": "Extensive experiments on 7 datasets show consistent improvements, reducing errors by up to 43%.", "conclusion": "OMNI-DC demonstrates strong generalization and robustness, making it suitable for real-world depth completion tasks."}}
{"id": "2412.18241", "pdf": "https://arxiv.org/pdf/2412.18241", "abs": "https://arxiv.org/abs/2412.18241", "authors": ["Rong Shan", "Jianghao Lin", "Chenxu Zhu", "Bo Chen", "Menghui Zhu", "Kangning Zhang", "Jieming Zhu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "An Automatic Graph Construction Framework based on Large Language Models for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.", "AI": {"tldr": "AutoGraph is an LLM-based automatic graph construction framework for recommendation, addressing global-view invisibility and inefficiency in existing methods, and improving performance in real-world applications.", "motivation": "Existing GNN-based recommendation methods neglect graph construction, relying on simplistic or labor-intensive approaches. Recent LLM-based methods lack global view and efficiency.", "method": "AutoGraph uses LLMs to infer user preferences and item knowledge, encodes them as semantic vectors, extracts latent factors via vector quantization, and incorporates these as extra nodes in the graph. Metapath-based message aggregation is used to combine semantic and collaborative information.", "result": "AutoGraph outperforms baselines on three datasets and achieves a 2.69% RPM and 7.31% eCPM improvement in Huawei's advertising platform, serving millions.", "conclusion": "AutoGraph effectively automates graph construction, enhances recommendation performance, and is scalable for real-world deployment."}}
