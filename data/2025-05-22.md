<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 237]
- [cs.CV](#cs.CV) [Total: 176]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.SD](#cs.SD) [Total: 17]
- [cs.LG](#cs.LG) [Total: 231]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 16]
- [eess.IV](#eess.IV) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/pdf/2505.14848)
*Xi Wang, Jiaqian Hu, Safinah Ali*

Main category: cs.CL

TL;DR: MAATS is a Multi Agent Automated Translation System using MQM for error detection, outperforming single-agent methods in accuracy and fluency.


<details>
  <summary>Details</summary>
Motivation: To improve translation quality by leveraging specialized AI agents for distinct error categories, addressing limitations of single-agent systems.

Method: Uses multiple AI agents, each focused on a specific MQM category (e.g., Accuracy, Fluency), with a synthesis agent for iterative refinement.

Result: Outperforms zero-shot and single-agent baselines in automatic metrics and human assessments, excelling in semantic accuracy and locale adaptation.

Conclusion: MAATS bridges the gap between black-box LLMs and human workflows, enhancing semantic and contextual fidelity in translations.

Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.

</details>


### [2] [Addressing the Challenges of Planning Language Generation](https://arxiv.org/pdf/2505.14763)
*Prabhu Prakash Kagitha, Andrew Zhu, Li Zhang*

Main category: cs.CL

TL;DR: Open-source LLMs under 50B parameters can generate PDDL for planning when using solver feedback, outperforming intuitive methods like constrained decoding.


<details>
  <summary>Details</summary>
Motivation: To explore if smaller open-source LLMs can generate PDDL effectively, despite prior limitations, and identify the best pipeline.

Method: Tested 8 PDDL generation pipelines with open-source LLMs under 50B parameters, including intuitive and scaling approaches.

Result: Solver feedback and plan validation doubled performance, while intuitive methods like constrained decoding reduced it.

Conclusion: Inference-time scaling with solver feedback is key for effective PDDL generation in smaller open-source LLMs.

Abstract: Using LLMs to generate formal planning languages such as PDDL that invokes
symbolic solvers to deterministically derive plans has been shown to outperform
generating plans directly. While this success has been limited to
closed-sourced models or particular LLM pipelines, we design and evaluate 8
different PDDL generation pipelines with open-source models under 50 billion
parameters previously shown to be incapable of this task. We find that
intuitive approaches such as using a high-resource language wrapper or
constrained decoding with grammar decrease performance, yet inference-time
scaling approaches such as revision with feedback from the solver and plan
validator more than double the performance.

</details>


### [3] [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/pdf/2505.14804)
*Richard Khoury, Maxence Verhaverbeke, Julie A. Gramaccia*

Main category: cs.CL

TL;DR: The paper introduces an automated pipeline for extracting 5W1H (who, what, when, where, why, how) information from French news articles and evaluates its performance against GPT-4o.


<details>
  <summary>Details</summary>
Motivation: 5W1H questions are essential for tasks like summarization and news aggregation, but automated extraction for French news is lacking.

Method: The authors designed an extraction pipeline and created a corpus of 250 Quebec news articles annotated by humans for evaluation.

Result: The pipeline performs comparably to GPT-4o in extracting 5W1H information.

Conclusion: The proposed pipeline is effective for automated 5W1H extraction from French news, matching state-of-the-art performance.

Abstract: The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algo- rithm, we also create a
corpus of 250 Quebec news articles with 5W1H answers marked by four human
annotators. Our results demonstrate that our pipeline performs as well in this
task as the large language model GPT-4o.

</details>


### [4] [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/pdf/2505.14810)
*Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, Yu Cheng*

Main category: cs.CL

TL;DR: The paper introduces MathIF, a benchmark for evaluating instruction-following in mathematical reasoning tasks, revealing a trade-off between reasoning capacity and controllability in LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored ability of reasoning-oriented LLMs to adhere to natural language instructions, especially in mathematical tasks.

Method: Introduces MathIF benchmark and conducts empirical analysis on models tuned with reasoning-oriented methods, observing their instruction adherence.

Result: Models with enhanced reasoning often struggle with instruction compliance, especially with longer generation. Simple interventions can partially recover obedience but reduce reasoning performance.

Conclusion: Highlights a tension in LLM training between reasoning and instruction-following, calling for more instruction-aware models.

Abstract: Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.

</details>


### [5] [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/pdf/2505.14815)
*Mingyang Wang, Lukas Lange, Heike Adel, Yunpu Ma, Jannik Strötgen, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper systematically studies language mixing in reasoning language models (RLMs), analyzing its patterns, impact, and causes, and shows how task difficulty, subject area, and language choice influence it. Forcing specific reasoning languages improves accuracy, and language mixing aligns with internal model representations.


<details>
  <summary>Details</summary>
Motivation: To understand and address the phenomenon of language mixing in RLMs, which affects performance but lacks systematic study.

Method: Examined language mixing across 15 languages, 7 task difficulty levels, and 18 subject areas, and tested constrained decoding to control reasoning language.

Result: Language mixing is influenced by task, subject, and language. Forcing reasoning in Latin or Han scripts improves accuracy. Mixing aligns with internal model representations.

Conclusion: The study provides insights for optimizing multilingual reasoning and suggests controlling reasoning languages for more interpretable and adaptable RLMs.

Abstract: Reasoning language models (RLMs) excel at complex tasks by leveraging a
chain-of-thought process to generate structured intermediate steps. However,
language mixing, i.e., reasoning steps containing tokens from languages other
than the prompt, has been observed in their outputs and shown to affect
performance, though its impact remains debated. We present the first systematic
study of language mixing in RLMs, examining its patterns, impact, and internal
causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and
show how all three factors influence language mixing. Moreover, we demonstrate
that the choice of reasoning language significantly affects performance:
forcing models to reason in Latin or Han scripts via constrained decoding
notably improves accuracy. Finally, we show that the script composition of
reasoning traces closely aligns with that of the model's internal
representations, indicating that language mixing reflects latent processing
preferences in RLMs. Our findings provide actionable insights for optimizing
multilingual reasoning and open new directions for controlling reasoning
languages to build more interpretable and adaptable RLMs.

</details>


### [6] [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/pdf/2505.14818)
*Leon Lin, Jun Zheng, Haidong Wang*

Main category: cs.CL

TL;DR: WebNovelBench is a new benchmark for evaluating long-form storytelling in LLMs using a dataset of 4,000 Chinese web novels and a multi-faceted framework for narrative quality.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack scale, diversity, or objective measures for evaluating long-form storytelling in LLMs.

Method: Uses a synopsis-to-story task, assesses eight narrative dimensions via LLM-as-Judge, and aggregates scores with PCA for percentile ranking.

Result: Effectively differentiates human-written and LLM-generated content, ranks 24 LLMs' storytelling abilities.

Conclusion: Provides a scalable, replicable, and data-driven method for advancing LLM narrative generation.

Abstract: Robustly evaluating the long-form storytelling capabilities of Large Language
Models (LLMs) remains a significant challenge, as existing benchmarks often
lack the necessary scale, diversity, or objective measures. To address this, we
introduce WebNovelBench, a novel benchmark specifically designed for evaluating
long-form novel generation. WebNovelBench leverages a large-scale dataset of
over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story
generation task. We propose a multi-faceted framework encompassing eight
narrative quality dimensions, assessed automatically via an LLM-as-Judge
approach. Scores are aggregated using Principal Component Analysis and mapped
to a percentile rank against human-authored works. Our experiments demonstrate
that WebNovelBench effectively differentiates between human-written
masterpieces, popular web novels, and LLM-generated content. We provide a
comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling
abilities and offering insights for future development. This benchmark provides
a scalable, replicable, and data-driven methodology for assessing and advancing
LLM-driven narrative generation.

</details>


### [7] [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/pdf/2505.15154)
*Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, Han Wang, Can Huang*

Main category: cs.CL

TL;DR: CAR dynamically switches between short answers and long-form reasoning based on model perplexity, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Excessive reliance on chain-of-thought reasoning can impair performance and reduce efficiency, especially on simpler tasks.

Method: Proposes Certainty-based Adaptive Reasoning (CAR), which evaluates perplexity to decide between short answers or long-form reasoning.

Result: CAR outperforms both short-answer and long-form reasoning approaches in accuracy and efficiency across diverse benchmarks.

Conclusion: CAR optimally balances accuracy and efficiency by adapting reasoning based on model confidence.

Abstract: Recent advancements in reasoning have significantly enhanced the capabilities
of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
across diverse tasks. However, excessive reliance on chain-of-thought (CoT)
reasoning can impair model performance and brings unnecessarily lengthened
outputs, reducing efficiency. Our work reveals that prolonged reasoning does
not universally improve accuracy and even degrade performance on simpler tasks.
To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel
framework that dynamically switches between short answers and long-form
reasoning based on the model perplexity. CAR first generates a short answer and
evaluates its perplexity, triggering reasoning only when the model exhibits low
confidence (i.e., high perplexity). Experiments across diverse multimodal
VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both
short-answer and long-form reasoning approaches, striking an optimal balance
between accuracy and efficiency.

</details>


### [8] [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/pdf/2505.14824)
*Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper explores how factual recall and crosslingual consistency evolve during pretraining in LLMs, using OLMo-7B as a case study. It identifies two pathways for multilingual knowledge acquisition: frequency-driven learning and crosslingual transfer.


<details>
  <summary>Details</summary>
Motivation: Most studies evaluate only final LLM models, leaving the development of factual recall and crosslingual consistency during pretraining unexplored.

Method: The study traces factual recall and crosslingual consistency during pretraining, analyzing OLMo-7B's performance across languages.

Result: Accuracy and consistency improve over time, driven by fact frequency in the pretraining corpus. Crosslingual transfer aids low-frequency facts in non-English languages.

Conclusion: Multilingual factual knowledge acquisition occurs via frequency-driven learning (dominant) and crosslingual transfer (limited). The findings are shared to encourage further research.

Abstract: Large Language Models (LLMs) are capable of recalling multilingual factual
knowledge present in their pretraining data. However, most studies evaluate
only the final model, leaving the development of factual recall and
crosslingual consistency throughout pretraining largely unexplored. In this
work, we trace how factual recall and crosslingual consistency evolve during
pretraining, focusing on OLMo-7B as a case study. We find that both accuracy
and consistency improve over time for most languages. We show that this
improvement is primarily driven by the fact frequency in the pretraining
corpus: more frequent facts are more likely to be recalled correctly,
regardless of language. Yet, some low-frequency facts in non-English languages
can still be correctly recalled. Our analysis reveals that these instances
largely benefit from crosslingual transfer of their English counterparts -- an
effect that emerges predominantly in the early stages of pretraining. We
pinpoint two distinct pathways through which multilingual factual knowledge
acquisition occurs: (1) frequency-driven learning, which is dominant and
language-agnostic, and (2) crosslingual transfer, which is limited in scale and
typically constrained to relation types involving named entities. We release
our code and data to facilitate further research at
https://github.com/cisnlp/multilingual-fact-tracing.

</details>


### [9] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/pdf/2505.14827)
*Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao*

Main category: cs.CL

TL;DR: MoI (Mixture of Inputs) is a training-free method for autoregressive generation that blends sampled tokens with discarded token distributions, improving text quality and reasoning.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive generation discards the next-token distribution after sampling, losing valuable information. MoI aims to preserve this distribution for richer representations.

Method: MoI constructs new inputs by blending discrete tokens with the token distribution using Bayesian estimation, replacing one-hot vectors with continuous posterior expectations.

Result: MoI improves performance on mathematical reasoning, code generation, and PhD-level QA tasks across multiple models without additional training or significant overhead.

Conclusion: MoI enhances autoregressive generation by leveraging discarded distributions, offering a simple yet effective way to boost model performance.

Abstract: In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.

</details>


### [10] [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/pdf/2505.14832)
*Wonje Jeung, Sangyeon Yoon, Albert No*

Main category: cs.CL

TL;DR: SEPS is a new framework for evaluating machine unlearning in LLMs, focusing on mixed-query scenarios. It identifies flaws in existing methods and proposes Mixed Prompt (MP) unlearning for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning metrics don't account for real-world scenarios where forget and retain queries coexist in prompts, limiting their practical applicability.

Method: Introduces SEPS for mixed-query evaluation and proposes Mixed Prompt (MP) unlearning to address failure modes in current methods.

Result: MP unlearning significantly improves effectiveness, handling up to eight mixed queries robustly.

Conclusion: SEPS and MP unlearning advance machine unlearning by addressing real-world mixed-query challenges.

Abstract: Machine unlearning aims to selectively remove targeted knowledge from Large
Language Models (LLMs), ensuring they forget specified content while retaining
essential information. Existing unlearning metrics assess whether a model
correctly answers retain queries and rejects forget queries, but they fail to
capture real-world scenarios where forget queries rarely appear in isolation.
In fact, forget and retain queries often coexist within the same prompt, making
mixed-query evaluation crucial.
  We introduce SEPS, an evaluation framework that explicitly measures a model's
ability to both forget and retain information within a single prompt. Through
extensive experiments across three benchmarks, we identify two key failure
modes in existing unlearning methods: (1) untargeted unlearning
indiscriminately erases both forget and retain content once a forget query
appears, and (2) targeted unlearning overfits to single-query scenarios,
leading to catastrophic failures when handling multiple queries. To address
these issues, we propose Mixed Prompt (MP) unlearning, a strategy that
integrates both forget and retain queries into a unified training objective.
Our approach significantly improves unlearning effectiveness, demonstrating
robustness even in complex settings with up to eight mixed forget and retain
queries in a single prompt.

</details>


### [11] [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/pdf/2505.14845)
*Wang Jiaqi, Wang bo, Guo fa, Cheng cheng, Yang li*

Main category: cs.CL

TL;DR: LLMs exhibit dynamic, input-driven personality traits, differing from human traits in stability and consistency, suggesting the need for LLM-specific frameworks.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs display personality-like traits and how they compare to human personality, using conventional assessment tools.

Method: A behavior-based approach across three studies: test-retest stability, cross-variant consistency, and role-playing retention.

Result: LLMs show higher variability, input sensitivity, and low internal consistency compared to humans, with traits shaped by prompts and settings.

Conclusion: LLMs express fluid, externally dependent personality patterns, highlighting the need for tailored frameworks and advancing human-AI interaction.

Abstract: Large Language Models (LLMs) have demonstrated human-like capabilities in
language comprehension and generation, becoming active participants in social
and cognitive domains. This study investigates whether LLMs exhibit
personality-like traits and how these traits compare with human personality,
focusing on the applicability of conventional personality assessment tools. A
behavior-based approach was used across three empirical studies. Study 1
examined test-retest stability and found that LLMs show higher variability and
are more input-sensitive than humans, lacking long-term stability. Based on
this, we propose the Distributed Personality Framework, conceptualizing LLM
traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency
in personality measures and found LLMs' responses were highly sensitive to item
wording, showing low internal consistency compared to humans. Study 3 explored
personality retention during role-playing, showing LLM traits are shaped by
prompt and parameter settings. These findings suggest that LLMs express fluid,
externally dependent personality patterns, offering insights for constructing
LLM-specific personality frameworks and advancing human-AI interaction. This
work contributes to responsible AI development and extends the boundaries of
personality psychology in the age of intelligent systems.

</details>


### [12] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/pdf/2505.14874)
*Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen*

Main category: cs.CL

TL;DR: Fine-tuning a voice conversion model on English dysarthric speech to generate non-English dysarthric-like speech improves multilingual ASR performance for dysarthric speech.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in non-English dysarthric speech for ASR by leveraging English dysarthric data.

Method: Fine-tune a voice conversion model on English dysarthric speech, apply it to convert healthy non-English speech into dysarthric-like speech, and use this to fine-tune a multilingual ASR model.

Result: VC with speaker and prosody conversion outperforms off-the-shelf MMS and conventional augmentation techniques.

Conclusion: Generated dysarthric-like speech improves ASR performance and simulates dysarthric characteristics effectively.

Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.

</details>


### [13] [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/pdf/2505.14852)
*Drishya Karki, Michiel Kamphuis, Angelecia Frey*

Main category: cs.CL

TL;DR: EasyMath is a benchmark for math reasoning in small language models, tested on 23 models (14M to 4B parameters) with accuracy improving with size and training.


<details>
  <summary>Details</summary>
Motivation: To provide a practical benchmark for evaluating math reasoning in small language models, covering diverse categories like arithmetic, algebra, and word problems.

Method: Tested 23 models using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting, with chain-of-thought prompting.

Result: Accuracy increases with model size and training; chain-of-thought offers modest gains, and consistency improves at scale.

Conclusion: EasyMath effectively benchmarks math reasoning in small models, showing performance trends tied to size and training.

Abstract: EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.

</details>


### [14] [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/pdf/2505.14887)
*Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky*

Main category: cs.CL

TL;DR: A scalable framework for in-context learning in Phi-4 Multimodal improves ASR robustness with minimal adaptation data, showing human-like adaptation but gaps in certain language varieties.


<details>
  <summary>Details</summary>
Motivation: To explore if state-of-the-art spoken language models can adapt to unfamiliar speakers and language varieties like humans, and improve ASR robustness.

Method: Introduces a framework for in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts and audio-text pairs, tested with as few as 12 example utterances.

Result: Reduces word error rates by 19.7% (1.2 pp.) on average, with notable improvements in low-resource varieties and matching context-speaker conditions.

Conclusion: The ICL adaptation scheme mirrors human adaptability but reveals gaps in certain varieties, highlighting areas for model improvement.

Abstract: Human listeners readily adjust to unfamiliar speakers and language varieties
through exposure, but do these adaptation benefits extend to state-of-the-art
spoken language models? We introduce a scalable framework that allows for
in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts
and audio-text pairs, and find that as few as 12 example utterances (~50
seconds) at inference time reduce word error rates by a relative 19.7% (1.2
pp.) on average across diverse English corpora. These improvements are most
pronounced in low-resource varieties, when the context and target speaker
match, and when more examples are provided--though scaling our procedure yields
diminishing marginal returns to context length. Overall, we find that our novel
ICL adaptation scheme (1) reveals a similar performance profile to human
listeners, and (2) demonstrates consistent improvements to automatic speech
recognition (ASR) robustness across diverse speakers and language backgrounds.
While adaptation succeeds broadly, significant gaps remain for certain
varieties, revealing where current models still fall short of human
flexibility. We release our prompts and code on GitHub.

</details>


### [15] [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/pdf/2505.14871)
*Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang*

Main category: cs.CL

TL;DR: Saten, a sparse augmented tensor network, improves accuracy and compression efficiency in tensorized LLMs during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Efficient deployment of LLMs on resource-constrained devices is challenging due to high-rank nature and lack of pretraining data access.

Method: Proposes Saten, a framework for low-rank tensorized LLMs during fine-tuning, enabling full model compression.

Result: Saten achieves state-of-the-art performance in accuracy and compression efficiency.

Conclusion: Saten effectively enhances tensorized LLMs, making them more practical for deployment.

Abstract: The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.

</details>


### [16] [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/pdf/2505.15333)
*Yuhao Zhang, Xiangnan Ma, Kaiqi Kou, Peizhuo Liu, Weiqiao Shan, Benyou Wang, Tong Xiao, Yuxin Huang, Zhengtao Yu, Jingbo Zhu*

Main category: cs.CL

TL;DR: The paper proposes a unit language method to address challenges in textless speech-to-speech translation (S2ST), improving performance and aligning with text-trained models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in S2ST: extracting linguistic features (cross-modal) and aligning languages (cross-lingual).

Method: Introduces unit language, a text-like representation using $n$-gram modeling, with multi-task learning and task prompt modeling to resolve conflicts.

Result: Significant improvements over baselines on the Voxpupil dataset, matching text-trained model performance.

Conclusion: Unit language effectively tackles S2ST challenges, offering a viable alternative to text-based methods.

Abstract: The success of building textless speech-to-speech translation (S2ST) models
has attracted much attention. However, S2ST still faces two main challenges: 1)
extracting linguistic features for various speech signals, called cross-modal
(CM), and 2) learning alignment of difference languages in long sequences,
called cross-lingual (CL). We propose the unit language to overcome the two
modeling challenges. The unit language can be considered a text-like
representation format, constructed using $n$-gram language modeling. We
implement multi-task learning to utilize the unit language in guiding the
speech modeling process. Our initial results reveal a conflict when applying
source and target unit languages simultaneously. We propose task prompt
modeling to mitigate this conflict. We conduct experiments on four languages of
the Voxpupil dataset. Our method demonstrates significant improvements over a
strong baseline and achieves performance comparable to models trained with
text.

</details>


### [17] [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/pdf/2502.12562)
*Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng*

Main category: cs.CL

TL;DR: SEA optimizes embeddings for multimodal safety alignment using textual data, improving MLLM security against threats from additional modalities like images, videos, and audio.


<details>
  <summary>Details</summary>
Motivation: Existing low-resource security alignment methods struggle with risks from additional modalities, and constructing multimodal datasets is costly.

Method: Proposes SEA, which augments textual datasets by optimizing embeddings of additional modalities through gradient updates.

Result: SEA synthesizes high-quality embeddings quickly (24s on RTX3090) and significantly enhances MLLM security. A new benchmark, VA-SafetyBench, validates the challenge of video/audio threats.

Conclusion: SEA is an efficient solution for multimodal safety alignment, addressing security risks without requiring costly multimodal datasets.

Abstract: Multimodal Large Language Models (MLLMs) have serious security
vulnerabilities.While safety alignment using multimodal datasets consisting of
text and data of additional modalities can effectively enhance MLLM's security,
it is costly to construct these datasets. Existing low-resource security
alignment methods, including textual alignment, have been found to struggle
with the security risks posed by additional modalities. To address this, we
propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes
embeddings of additional modality through gradient updates to expand textual
datasets. This enables multimodal safety alignment training even when only
textual data is available. Extensive experiments on image, video, and
audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding
on a single RTX3090 GPU within 24 seconds. SEA significantly improves the
security of MLLMs when faced with threats from additional modalities. To assess
the security risks introduced by video and audio, we also introduced a new
benchmark called VA-SafetyBench. High attack success rates across multiple
MLLMs validate its challenge. Our code and data will be available at
https://github.com/ZeroNLP/SEA.

</details>


### [18] [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/pdf/2505.14880)
*Chris Sypherd, Sergei Petrov, Sonny George, Vaishak Belle*

Main category: cs.CL

TL;DR: The paper introduces Big-$O_{tok}$, a framework for analyzing token efficiency in prompting strategies for large language models, emphasizing the diminishing returns of increased token usage.


<details>
  <summary>Details</summary>
Motivation: Current prompting strategies for large language models focus on performance but overlook token efficiency, which is crucial for real-world utility.

Method: Proposes Big-$O_{tok}$ (a theoretical framework for token usage growth) and Token Cost (an empirical measure of tokens per performance), applied to common prompting strategies.

Result: Increased token usage leads to drastically diminishing performance returns, validating Big-$O_{tok}$ analyses.

Conclusion: Efficiency-aware evaluations, balancing performance and token usage, are essential for practical applications of prompting strategies.

Abstract: In recent years, large language models have demonstrated remarkable
performance across diverse tasks. However, their task effectiveness is heavily
dependent on the prompting strategy used to elicit output, which can vary
widely in both performance and token usage. While task performance is often
used to determine prompting strategy success, we argue that
efficiency--balancing performance and token usage--can be a more practical
metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a
theoretical framework for describing the token usage growth of prompting
strategies, and analyze Token Cost, an empirical measure of tokens per
performance. We apply these to several common prompting strategies and find
that increased token usage leads to drastically diminishing performance
returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need
for efficiency-aware evaluations.

</details>


### [19] [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/pdf/2505.15355)
*Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Mathieu Bourguignon, Nicola Molinaro*

Main category: cs.CL

TL;DR: The study decoded phonetic information from MEG signals during speech production and perception, finding higher accuracy in production tasks. Elastic Net outperformed neural networks, and low-frequency oscillations were key.


<details>
  <summary>Details</summary>
Motivation: To understand neural mechanisms of speech production for cognitive neuroscience and communication technologies.

Method: Analyzed MEG signals from 17 participants during speech tasks, using machine learning (Elastic Net, neural networks) and frequency band analysis.

Result: Higher decoding accuracy in speech production (76.6%) vs. perception (~51%). Elastic Net was best, and Delta/Theta bands were most informative.

Conclusion: Overt speech production provides richer neural data, aiding BCI development for speech impairments, but methodological refinements are needed.

Abstract: Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.

</details>


### [20] [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/pdf/2505.14886)
*Danqing Wang, Zhuorui Ye, Xinran Zhao, Fei Fang, Lei Li*

Main category: cs.CL

TL;DR: TreeDebater is a novel debate framework using Rehearsal and Debate Flow Trees to optimize argument strategy and time allocation, outperforming state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in competitive debates, such as time constraints and dynamic argument interactions, which existing systems fail to evaluate effectively.

Method: Introduces Rehearsal Tree for claim strength evaluation and Debate Flow Tree for tracking active actions, with time allocation and audience feedback.

Result: Outperforms state-of-the-art multi-agent debate systems in human evaluations, aligning with expert debate strategies.

Conclusion: TreeDebater effectively addresses debate challenges, demonstrating superior performance and strategic alignment with human experts.

Abstract: Winning competitive debates requires sophisticated reasoning and argument
skills. There are unique challenges in the competitive debate: (1) The time
constraints force debaters to make strategic choices about which points to
pursue rather than covering all possible arguments; (2) The persuasiveness of
the debate relies on the back-and-forth interaction between arguments, which a
single final game status cannot evaluate. To address these challenges, we
propose TreeDebater, a novel debate framework that excels in competitive
debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow
Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the
strength of the claim, while the Debate Flow Tree tracks the debate status to
identify the active actions. TreeDebater allocates its time budget among
candidate actions and uses the speech time controller and feedback from the
simulated audience to revise its statement. The human evaluation on both the
stage-level and the debate-level comparison shows that our TreeDebater
outperforms the state-of-the-art multi-agent debate system. Further
investigation shows that TreeDebater shows better strategies in limiting time
to important debate actions, aligning with the strategies of human debate
experts.

</details>


### [21] [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/pdf/2505.14892)
*Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin*

Main category: cs.CL

TL;DR: LLMs struggle with state tracking as complexity increases, showing degraded accuracy in deterministic state dynamics tasks. Key attention heads propagate state info but fail in joint state-action reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' ability to model state transition dynamics in finite-state systems, given their growing use in tasks requiring state tracking.

Method: Evaluated LLMs (GPT-2 XL, Pythia-1B) on deterministic state dynamics tasks (Box Tracking, Abstract DFA Sequences, Complex Text Games) with varying state-space sizes and transition sparsity. Used activation patching to identify key attention heads.

Result: Accuracy drops with larger state spaces (e.g., GPT-2 XL falls below 30% for >5 boxes/states). Pythia-1B fails to exceed 50% accuracy for >10 states and <30 transitions. Key attention heads propagate state info but lack joint state-action reasoning.

Conclusion: State tracking in LLMs relies on distributed next-token interactions, not explicit symbolic computation, limiting performance in complex state dynamics.

Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.

</details>


### [22] [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/pdf/2505.15646)
*Ke Hu, Krishna Puvvada, Elena Rastorgueva, Zhehuai Chen, He Huang, Shuoyang Ding, Kunal Dhawan, Hainan Xu, Jagadeesh Balam, Boris Ginsburg*

Main category: cs.CL

TL;DR: A data-driven method for word-level timestamp prediction in the Canary model, eliminating external alignment modules by using NeMo Forced Aligner as a teacher model. Achieves 80-90% precision/recall with 20-120 ms errors.


<details>
  <summary>Details</summary>
Motivation: Accurate word-level timestamps are vital for tasks like speech retrieval and subtitles, but traditional systems rely on external alignment modules.

Method: Uses NeMo Forced Aligner to generate timestamps and trains Canary model with a new <|timestamp|> token for direct prediction.

Result: 80-90% precision/recall, 20-120 ms errors in four languages, minimal WER degradation. Extends to AST with ~200 ms errors.

Conclusion: The approach effectively integrates timestamp prediction into the Canary model, reducing reliance on external alignment and maintaining accuracy.

Abstract: We introduce a data-driven approach for enabling word-level timestamp
prediction in the Canary model. Accurate timestamp information is crucial for a
variety of downstream tasks such as speech content retrieval and timed
subtitles. While traditional hybrid systems and end-to-end (E2E) models may
employ external modules for timestamp prediction, our approach eliminates the
need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner
(NFA) as a teacher model, we generate word-level timestamps and train the
Canary model to predict timestamps directly. We introduce a new <|timestamp|>
token, enabling the Canary model to predict start and end timestamps for each
word. Our method demonstrates precision and recall rates between 80% and 90%,
with timestamp prediction errors ranging from 20 to 120 ms across four
languages, with minimal WER degradation. Additionally, we extend our system to
automatic speech translation (AST) tasks, achieving timestamp prediction errors
around 200 milliseconds.

</details>


### [23] [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/pdf/2505.14905)
*Xiaoyan Bai, Ike Peng, Aditya Singh, Chenhao Tan*

Main category: cs.CL

TL;DR: The paper introduces 'concept incongruence' to analyze how LLMs handle conflicting or mis-specified prompts, like drawing a unicorn with two horns. It proposes metrics to measure model behavior under incongruence, focusing on temporal boundaries in Role-Play settings, and identifies causes for model failures.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how LLMs behave when faced with prompts that clash with defined concepts (e.g., a unicorn with two horns) or temporal boundaries (e.g., a role's death).

Method: The study defines concept incongruence and proposes three metrics (abstention rate, conditional accuracy, answer rate) to analyze model behavior. It focuses on temporal incongruence in Role-Play settings and conducts probing experiments.

Result: Models fail to abstain after a role's death and show accuracy drops. Causes include unreliable encoding of the 'death' state and shifts in temporal representations due to role-playing.

Conclusion: Concept incongruence leads to unexpected model behaviors. Insights from this work can guide improvements in model consistency under such conditions.

Abstract: Consider this prompt "Draw a unicorn with two horns". Should large language
models (LLMs) recognize that a unicorn has only one horn by definition and ask
users for clarifications, or proceed to generate something anyway? We introduce
concept incongruence to capture such phenomena where concept boundaries clash
with each other, either in user prompts or in model representations, often
leading to under-specified or mis-specified behaviors. In this work, we take
the first step towards defining and analyzing model behavior under concept
incongruence. Focusing on temporal boundaries in the Role-Play setting, we
propose three behavioral metrics--abstention rate, conditional accuracy, and
answer rate--to quantify model behavior under incongruence due to the role's
death. We show that models fail to abstain after death and suffer from an
accuracy drop compared to the Non-Role-Play setting. Through probing
experiments, we identify two main causes: (i) unreliable encoding of the
"death" state across different years, leading to unsatisfactory abstention
behavior, and (ii) role playing causes shifts in the model's temporal
representations, resulting in accuracy drops. We leverage these insights to
improve consistency in the model's abstention and answer behaviors. Our
findings suggest that concept incongruence leads to unexpected model behaviors
and point to future directions on improving model behavior under concept
incongruence.

</details>


### [24] [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/pdf/2505.15670)
*Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg*

Main category: cs.CL

TL;DR: A novel duplex speech-to-speech (S2S) architecture enables real-time adaptability, outperforming previous models in reasoning, turn-taking, and barge-in abilities while reducing bitrate and simplifying training.


<details>
  <summary>Details</summary>
Motivation: Current speech models lack real-time adaptability like user barge-in, limiting intuitive human-computer interaction.

Method: Proposes a duplex S2S architecture with continuous user inputs and codec agent outputs, using separate architectures for user and agent modeling and a pretrained streaming encoder.

Result: Outperforms previous duplex models, halves bitrate (0.6 kbps), and requires less speech data by skipping pretraining.

Conclusion: The model simplifies duplex S2S model development, is openly available, and fosters reproducibility.

Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet
current speech language models often remain constrained to turn-based
exchanges, lacking real-time adaptability such as user barge-in. We propose a
novel duplex speech to speech (S2S) architecture featuring continuous user
inputs and codec agent outputs with channel fusion that directly models
simultaneous user and agent streams. Using a pretrained streaming encoder for
user input enables the first duplex S2S model without requiring speech
pretrain. Separate architectures for agent and user modeling facilitate codec
fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared
to previous works. Experimental results show that the proposed model
outperforms previous duplex models in reasoning, turn-taking, and barge-in
abilities. The model requires significantly less speech data, as speech
pretrain is skipped, which markedly simplifies the process of building a duplex
S2S model from any LLMs. Finally, it is the first openly available duplex S2S
model with training and inference code to foster reproducibility.

</details>


### [25] ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/pdf/2505.15700)
*Alkis Koudounas, Claudio Savelli, Flavio Giobergia, Elena Baralis*

Main category: cs.CL

TL;DR: The paper introduces UnSLU-BENCH, a benchmark for evaluating machine unlearning in spoken language understanding (SLU) across four languages, assessing eight techniques and proposing a novel metric.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on machine unlearning effectiveness in complex tasks like SLU and evaluate the quality of 'right to be forgotten' requests.

Method: Developed UnSLU-BENCH, a benchmark for SLU unlearning, tested eight techniques, and introduced a new metric for efficacy, utility, and efficiency.

Result: Revealed significant differences in effectiveness and computational feasibility of unlearning techniques in SLU.

Conclusion: UnSLU-BENCH establishes a foundation for SLU unlearning research and highlights the variability in technique performance.

Abstract: Machine unlearning, the process of efficiently removing specific information
from machine learning models, is a growing area of interest for responsible AI.
However, few studies have explored the effectiveness of unlearning methods on
complex tasks, particularly speech-related ones. This paper introduces
UnSLU-BENCH, the first benchmark for machine unlearning in spoken language
understanding (SLU), focusing on four datasets spanning four languages. We
address the unlearning of data from specific speakers as a way to evaluate the
quality of potential "right to be forgotten" requests. We assess eight
unlearning techniques and propose a novel metric to simultaneously better
capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation
for unlearning in SLU and reveals significant differences in the effectiveness
and computational feasibility of various techniques.

</details>


### [26] [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/pdf/2505.14906)
*Ye Yuan, Haolun Wu, Hao Zhou, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang*

Main category: cs.CL

TL;DR: A novel language model-based technique, TeleSEE, is proposed for extracting structured telecom entities, improving accuracy and speed in 6G networks.


<details>
  <summary>Details</summary>
Motivation: To advance AI-native 6G networks by transforming fragmented telecom knowledge into structured formats for better AI model understanding.

Method: TeleSEE uses token-efficient representation and hierarchical parallel decoding to predict entity types and attribute keys, enhancing encoder-decoder architecture.

Result: TeleSEE outperforms baselines in accuracy and achieves 5-9x faster sample processing, validated on the 6GTech dataset.

Conclusion: TeleSEE effectively extracts telecom entities, supporting AI-driven network intelligence in 6G.

Abstract: Knowledge understanding is a foundational part of envisioned 6G networks to
advance network intelligence and AI-native network architectures. In this
paradigm, information extraction plays a pivotal role in transforming
fragmented telecom knowledge into well-structured formats, empowering diverse
AI models to better understand network terminologies. This work proposes a
novel language model-based information extraction technique, aiming to extract
structured entities from the telecom context. The proposed telecom structured
entity extraction (TeleSEE) technique applies a token-efficient representation
method to predict entity types and attribute keys, aiming to save the number of
output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a
hierarchical parallel decoding method, improving the standard encoder-decoder
architecture by integrating additional prompting and decoding strategies into
entity extraction tasks. In addition, to better evaluate the performance of the
proposed technique in the telecom domain, we further designed a dataset named
6GTech, including 2390 sentences and 23747 words from more than 100 6G-related
technical publications. Finally, the experiment shows that the proposed TeleSEE
method achieves higher accuracy than other baseline techniques, and also
presents 5 to 9 times higher sample processing speed.

</details>


### [27] [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/pdf/2505.14917)
*Zhiwei Liu, Paul Thompson, Jiaqi Rong, Sophia Ananiadou*

Main category: cs.CL

TL;DR: The paper addresses the challenge of detecting LLM-generated conspiracy theories, which can evade traditional detection methods by altering emotional tones. It introduces ConDID-v2, an augmented dataset with LLM-rewritten tweets, and ConspEmoLLM-v2, an improved detection model that outperforms baselines on disguised content.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate or disguise conspiracy theories, evading detection methods trained on human-authored text. This poses risks like misinformation spread.

Method: Developed ConDID-v2 by augmenting human-authored conspiracy tweets with LLM-rewritten versions to reduce negativity. Trained ConspEmoLLM-v2 on this dataset.

Result: ConspEmoLLM-v2 matches or exceeds performance on human-authored content and significantly outperforms baselines on sentiment-transformed tweets.

Conclusion: The enhanced dataset and model improve detection of disguised conspiracy content, mitigating risks of LLM-generated misinformation.

Abstract: Despite the many benefits of large language models (LLMs), they can also
cause harm, e.g., through automatic generation of misinformation, including
conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories
by altering characteristic textual features, e.g., by transforming their
typically strong negative emotions into a more positive tone. Although several
studies have proposed automated conspiracy theory detection methods, they are
usually trained using human-authored text, whose features can vary from
LLM-generated text. Furthermore, several conspiracy detection models, including
the previously proposed ConspEmoLLM, rely heavily on the typical emotional
features of human-authored conspiracy content. As such, intentionally disguised
content may evade detection. To combat such issues, we firstly developed an
augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which
supplements human-authored conspiracy tweets with versions rewritten by an LLM
to reduce the negativity of their original sentiment. The quality of the
rewritten tweets was verified by combining human and LLM-based assessment. We
subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of
ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or
exceeds the performance of ConspEmoLLM on the original human-authored content
in ConDID, and considerably outperforms both ConspEmoLLM and several other
baselines when applied to sentiment-transformed tweets in ConDID-v2. The
project will be available at https://github.com/lzw108/ConspEmoLLM.

</details>


### [28] [Streaming Sequence Transduction through Dynamic Compression](https://arxiv.org/pdf/2402.01172)
*Weiting Tan, Yunmo Chen, Tongfei Chen, Guanghui Qin, Haoran Xu, Heidi C. Zhang, Benjamin Van Durme, Philipp Koehn*

Main category: cs.CL

TL;DR: STAR is a Transformer-based model for efficient sequence-to-sequence transduction over streams, achieving high compression and superior performance in ASR and speech-to-text tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and high-quality sequence-to-sequence transduction over streaming data, particularly in ASR and speech-to-text applications.

Method: STAR dynamically segments input streams into compressed anchor representations using a Transformer-based architecture.

Result: Achieves nearly lossless 12x compression in ASR, outperforms existing methods, and optimizes latency, memory, and quality in speech-to-text tasks.

Conclusion: STAR is an effective solution for streaming sequence transduction, offering significant improvements in efficiency and performance.

Abstract: We introduce STAR (Stream Transduction with Anchor Representations), a novel
Transformer-based model designed for efficient sequence-to-sequence
transduction over streams. STAR dynamically segments input streams to create
compressed anchor representations, achieving nearly lossless compression (12x)
in Automatic Speech Recognition (ASR) and outperforming existing methods.
Moreover, STAR demonstrates superior segmentation and latency-quality
trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory
footprint, and quality.

</details>


### [29] [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/pdf/2505.14918)
*Fadel M. Megahed, Ying-Ju Chen, L. Allision Jones-Farmer, Younghwa Lee, Jiawei Brooke Wang, Inez M. Zwetsloot*

Main category: cs.CL

TL;DR: The study introduces a framework for assessing consistency in LLM binary text classification, using psychometric principles to evaluate reliability. A case study on financial news sentiment classification across 14 LLMs shows high intra-rater consistency and strong performance against benchmarks, though models failed at predicting market movements.


<details>
  <summary>Details</summary>
Motivation: Address the lack of established methods for evaluating reliability in LLM binary text classification.

Method: Adapt psychometric principles to determine sample sizes, develop metrics for invalid responses, and assess intra- and inter-rater reliability. A case study evaluates 14 LLMs on financial news sentiment classification.

Result: High intra-rater consistency (90-98% agreement), strong accuracy (0.76-0.88) against benchmarks, but chance performance in predicting market movements. Smaller models outperformed larger ones.

Conclusion: The framework aids LLM selection, sample size planning, and reliability assessment, helping organizations optimize resources for classification tasks.

Abstract: This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.

</details>


### [30] [dMel: Speech Tokenization made Simple](https://arxiv.org/pdf/2407.15835)
*Richard He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, Navdeep Jaitly*

Main category: cs.CL

TL;DR: The paper introduces dmel, a novel speech representation method that discretizes mel-filterbank channels into intensity bins, offering simplicity, robustness, and efficiency for speech synthesis and recognition tasks.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and limitations of existing speech tokenization methods, particularly their computational cost and failure on out-of-domain audio signals.

Method: Proposes dmel, a training-free representation, and an efficient parallel encoding/decoding method using a transformer architecture for high-dimensional tokens.

Result: Develops RichTTS and RichASR models, achieving comparable or better performance than specialized methods in speech synthesis and recognition.

Conclusion: Dmel provides a unified, efficient framework for joint speech and text modeling, demonstrating high performance and robustness.

Abstract: Large language models have revolutionized natural language processing by
leveraging self-supervised pretraining on vast textual data. Inspired by this
success, researchers have investigated various compression-based speech
tokenization methods to discretize continuous speech signals, enabling the
application of language modeling techniques to discrete tokens. However, audio
compressor introduces additional complexity and computational cost, and often
fail on out-of-domain audio signals. In this work, we introduce a novel speech
representation (dmel) that discretizes mel-filterbank channels into intensity
bins, creating a simpler yet more effective representation compared to existing
speech tokenization methods. Our approach demonstrates superior performance in
preserving audio content, robustness to out-of-domain data, and offers a
training-free, natural, and streamable representation. To address the
high-dimensional nature of log-mel spectrograms, we propose an efficient
parallel encoding and decoding method for high-dimensional tokens using an
LM-style transformer architecture. This innovation enables us to develop
RichTTS and RichASR, two models sharing the same architecture while achieving
comparable or better results than specialized existing methods. Our results
demonstrate the effectiveness of dmel in achieving high performance on both
speech synthesis and recognition tasks within a unified framework, paving the
way for efficient and effective joint modeling of speech and text.

</details>


### [31] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/pdf/2505.11626)
*Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Rafael Castrillo*

Main category: cs.CL

TL;DR: THELMA is a reference-free framework for evaluating RAG-based QA applications using six interdependent metrics, aiding developers in improving pipelines without labeled data.


<details>
  <summary>Details</summary>
Motivation: To provide a holistic, fine-grained evaluation method for RAG QA applications without needing labeled sources or reference responses.

Method: THELMA consists of six interdependent metrics designed for comprehensive assessment of RAG QA pipelines.

Result: The framework identifies specific RAG components needing improvement, aiding in pipeline optimization.

Conclusion: THELMA offers a practical tool for developers to evaluate and enhance RAG QA applications effectively.

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [32] [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/pdf/2505.14925)
*Sil Hamilton, Rebecca M. M. Hicke, Matthew Wilkens, David Mimno*

Main category: cs.CL

TL;DR: The paper introduces the TLDM benchmark to evaluate LLMs' long-context understanding using novels, finding no tested models perform well beyond 64k tokens.


<details>
  <summary>Details</summary>
Motivation: Novels offer complex, long-range semantic dependencies, making them ideal for testing LLMs' long-context capabilities beyond simple needle-in-a-haystack approaches.

Method: The authors created the TLDM benchmark, assessing models on plot summary, storyworld configuration, and narrative time over 128k tokens.

Result: None of the seven tested LLMs maintained stable understanding beyond 64k tokens.

Conclusion: Developers should move beyond simplistic benchmarks to evaluate LLMs in complex long-context scenarios, with TLDM provided as a tool for further research.

Abstract: Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond "lost in the middle" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.

</details>


### [33] [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/pdf/2505.14963)
*Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, Danielle S. Bitterman*

Main category: cs.CL

TL;DR: MedBrowseComp is a benchmark testing LLMs' ability to retrieve and synthesize multi-hop medical facts from live knowledge bases, revealing significant performance gaps in clinical settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-world utility evaluations for LLMs in clinical reasoning, which requires integrating diverse knowledge under strict accuracy constraints.

Method: Developed MedBrowseComp, a benchmark with 1,000+ human-curated questions mirroring clinical scenarios, testing retrieval and synthesis of multi-hop medical facts.

Result: Performance of frontier agentic systems on MedBrowseComp was as low as 10%, highlighting a gap between current LLM capabilities and clinical rigor.

Conclusion: MedBrowseComp provides a testbed for reliable medical information seeking and sets goals for future model improvements.

Abstract: Large language models (LLMs) are increasingly envisioned as decision-support
tools in clinical practice, yet safe clinical reasoning demands integrating
heterogeneous knowledge bases -- trials, primary studies, regulatory documents,
and cost data -- under strict accuracy constraints. Existing evaluations often
rely on synthetic prompts, reduce the task to single-hop factoid queries, or
conflate reasoning with open-ended generation, leaving their real-world utility
unclear. To close this gap, we present MedBrowseComp, the first benchmark that
systematically tests an agent's ability to reliably retrieve and synthesize
multi-hop medical facts from live, domain-specific knowledge bases.
MedBrowseComp contains more than 1,000 human-curated questions that mirror
clinical scenarios where practitioners must reconcile fragmented or conflicting
information to reach an up-to-date conclusion. Applying MedBrowseComp to
frontier agentic systems reveals performance shortfalls as low as ten percent,
exposing a critical gap between current LLM capabilities and the rigor demanded
in clinical settings. MedBrowseComp therefore offers a clear testbed for
reliable medical information seeking and sets concrete goals for future model
and toolchain upgrades. You can visit our project page at:
https://moreirap12.github.io/mbc-browse-app/

</details>


### [34] [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/pdf/2505.14971)
*Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chizor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee*

Main category: cs.CL

TL;DR: DECASTE is a framework to detect caste biases in LLMs, revealing systemic biases against marginalized groups like Dalits and Shudras.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored issue of caste-based biases in LLMs, which perpetuate societal prejudices.

Method: Proposes DECASTE, a multi-dimensional framework evaluating caste fairness across socio-cultural, economic, educational, and political dimensions using customized prompts.

Result: LLMs systematically reinforce caste biases, with higher bias scores for oppressed groups like Dalits and Shudras compared to dominant castes.

Conclusion: Highlights pervasive caste biases in LLMs and calls for more inclusive bias evaluation methodologies to mitigate real-world risks.

Abstract: Recent advancements in large language models (LLMs) have revolutionized
natural language processing (NLP) and expanded their applications across
diverse domains. However, despite their impressive capabilities, LLMs have been
shown to reflect and perpetuate harmful societal biases, including those based
on ethnicity, gender, and religion. A critical and underexplored issue is the
reinforcement of caste-based biases, particularly towards India's marginalized
caste groups such as Dalits and Shudras. In this paper, we address this gap by
proposing DECASTE, a novel, multi-dimensional framework designed to detect and
assess both implicit and explicit caste biases in LLMs. Our approach evaluates
caste fairness across four dimensions: socio-cultural, economic, educational,
and political, using a range of customized prompting strategies. By
benchmarking several state-of-the-art LLMs, we reveal that these models
systematically reinforce caste biases, with significant disparities observed in
the treatment of oppressed versus dominant caste groups. For example, bias
scores are notably elevated when comparing Dalits and Shudras with dominant
caste groups, reflecting societal prejudices that persist in model outputs.
These results expose the subtle yet pervasive caste biases in LLMs and
emphasize the need for more comprehensive and inclusive bias evaluation
methodologies that assess the potential risks of deploying such models in
real-world contexts.

</details>


### [35] [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/pdf/2505.14972)
*Haoyi Qiu, Kung-Hsiang Huang, Ruichen Zheng, Jiao Sun, Nanyun Peng*

Main category: cs.CL

TL;DR: The paper introduces CROSS, a benchmark to evaluate cultural safety in large vision-language models (LVLMs), revealing significant gaps. It proposes enhancement strategies that improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook cultural norm violations in LVLMs, leading to symbolic harm. The study aims to address this gap.

Method: CROSS includes 1,284 multilingual queries from 16 countries. CROSS-Eval measures cultural awareness, norm education, compliance, and helpfulness. 21 LVLMs are evaluated.

Result: Best-performing model achieves 61.79% in awareness and 37.73% in compliance. Enhancement strategies improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%).

Conclusion: Cultural safety gaps persist in LVLMs. Proposed methods improve performance but don't fully resolve the issue, highlighting the need for further research.

Abstract: Large vision-language models (LVLMs) are increasingly deployed in globally
distributed applications, such as tourism assistants, yet their ability to
produce culturally appropriate responses remains underexplored. Existing
multimodal safety benchmarks primarily focus on physical safety and overlook
violations rooted in cultural norms, which can result in symbolic harm. To
address this gap, we introduce CROSS, a benchmark designed to assess the
cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284
multilingual visually grounded queries from 16 countries, three everyday
domains, and 14 languages, where cultural norm violations emerge only when
images are interpreted in context. We propose CROSS-Eval, an intercultural
theory-based framework that measures four key dimensions: cultural awareness,
norm education, compliance, and helpfulness. Using this framework, we evaluate
21 leading LVLMs, including mixture-of-experts models and reasoning models.
Results reveal significant cultural safety gaps: the best-performing model
achieves only 61.79% in awareness and 37.73% in compliance. While some
open-source models reach GPT-4o-level performance, they still fall notably
short of proprietary models. Our results further show that increasing reasoning
capacity improves cultural alignment but does not fully resolve the issue. To
improve model performance, we develop two enhancement strategies: supervised
fine-tuning with culturally grounded, open-ended data and preference tuning
with contrastive response pairs that highlight safe versus unsafe behaviors.
These methods substantially improve GPT-4o's cultural awareness (+60.14%) and
compliance (+55.2%), while preserving general multimodal capabilities with
minimal performance reduction on general multimodal understanding benchmarks.

</details>


### [36] [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/pdf/2505.14984)
*Adarsh Singh, Kushal Raj Bhandari, Jianxi Gao, Soham Dan, Vivek Gupta*

Main category: cs.CL

TL;DR: CRAFT is a cascaded retrieval method for Table QA, combining sparse and dense models for efficient and effective retrieval, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Traditional dense retrieval models are computationally expensive and lack adaptability to new datasets, prompting the need for a more efficient solution.

Method: CRAFT uses sparse retrieval to filter candidates first, then applies dense models and neural re-rankers, enhanced by table descriptions/titles from Gemini Flash 1.5.

Result: CRAFT outperforms SOTA sparse, dense, and hybrid retrievers, validated on NQ-Tables.

Conclusion: CRAFT offers a scalable and adaptable solution for Table QA, improving retrieval performance and efficiency.

Abstract: Table Question Answering (TQA) involves retrieving relevant tables from a
large corpus to answer natural language queries. Traditional dense retrieval
models, such as DTR and ColBERT, not only incur high computational costs for
large-scale retrieval tasks but also require retraining or fine-tuning on new
datasets, limiting their adaptability to evolving domains and knowledge. In
this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that
first uses a sparse retrieval model to filter a subset of candidate tables
before applying more computationally expensive dense models and neural
re-rankers. Our approach achieves better retrieval performance than
state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further
enhance table representations by generating table descriptions and titles using
Gemini Flash 1.5. End-to-end TQA results using various Large Language Models
(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate
$\textbf{CRAFT}$ effectiveness.

</details>


### [37] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/pdf/2505.13404)
*Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg*

Main category: cs.CL

TL;DR: Granary is a large-scale speech dataset collection for 25 European languages, addressing data scarcity in low-resource languages through pseudo-labeling and data filtration, achieving comparable performance with 50% less data.


<details>
  <summary>Details</summary>
Motivation: Speech processing for low-resource languages is underexplored due to data scarcity. Granary aims to bridge this gap by providing a large-scale, open-source dataset for recognition and translation.

Method: Granary employs a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. Translation pairs are generated using EuroLLM, followed by data filtration.

Result: Models trained on Granary achieve similar performance with approximately 50% less data compared to previously curated datasets.

Conclusion: Granary successfully addresses data scarcity for low-resource languages, offering an efficient and scalable solution for speech recognition and translation.

Abstract: Multi-task and multilingual approaches benefit large models, yet speech
processing for low-resource languages remains underexplored due to data
scarcity. To address this, we present Granary, a large-scale collection of
speech datasets for recognition and translation across 25 European languages.
This is the first open-source effort at this scale for both transcription and
translation. We enhance data quality using a pseudo-labeling pipeline with
segmentation, two-pass inference, hallucination filtering, and punctuation
restoration. We further generate translation pairs from pseudo-labeled
transcriptions using EuroLLM, followed by a data filtration pipeline. Designed
for efficiency, our pipeline processes vast amount of data within hours. We
assess models trained on processed data by comparing their performance on
previously curated datasets for both high- and low-resource languages. Our
findings show that these models achieve similar performance using approx. 50%
less data. Dataset will be made available at
https://hf.co/datasets/nvidia/Granary

</details>


### [38] [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/pdf/2505.14990)
*Ishika Agarwal, Nimet Beyza Bozdag, Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: Language models perform better in some languages due to cultural knowledge, improving reasoning by switching languages.


<details>
  <summary>Details</summary>
Motivation: To explore if models hold more knowledge in certain languages and if reasoning improves by changing the language used.

Method: Employ culture-specific datasets and introduce LSKExtractor to benchmark and exploit language-specific knowledge.

Result: Models perform better, sometimes in low-resource languages, with a 10% average accuracy improvement.

Conclusion: Language models can be more inclusive and aligned with cultural contexts by leveraging language-specific knowledge.

Abstract: Code-switching is a common phenomenon of alternating between different
languages in the same utterance, thought, or conversation. We posit that humans
code-switch because they feel more comfortable talking about certain topics and
domains in one language than another. With the rise of knowledge-intensive
language models, we ask ourselves the next, natural question: Could models hold
more knowledge on some topics in some language X? More importantly, could we
improve reasoning by changing the language that reasoning is performed in? We
coin the term Language Specific Knowledge (LSK) to represent this phenomenon.
As ethnic cultures tend to develop alongside different languages, we employ
culture-specific datasets (that contain knowledge about cultural and social
behavioral norms). We find that language models can perform better when using
chain-of-thought reasoning in some languages other than English, sometimes even
better in low-resource languages. Paired with previous works showing that
semantic similarity does not equate to representational similarity, we
hypothesize that culturally specific texts occur more abundantly in
corresponding languages, enabling specific knowledge to occur only in specific
"expert" languages. Motivated by our initial results, we design a simple
methodology called LSKExtractor to benchmark the language-specific knowledge
present in a language model and, then, exploit it during inference. We show our
results on various models and datasets, showing an average relative improvement
of 10% in accuracy. Our research contributes to the open-source development of
language models that are inclusive and more aligned with the cultural and
linguistic contexts in which they are deployed.

</details>


### [39] [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/pdf/2505.14992)
*Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu*

Main category: cs.CL

TL;DR: DLISC is a two-stage approach for on-device LLMs, improving schema identification and extraction efficiency using Dual-LoRA and Incremental Schema Caching.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying LLMs for IE on resource-constrained devices include hallucinations, limited context, and high latency.

Method: DLISC uses Identification LoRA for schema retrieval and Extraction LoRA for schema-aware extraction, with Incremental Schema Caching to reduce redundancy.

Result: Experiments show significant improvements in effectiveness and efficiency across multiple IE datasets.

Conclusion: DLISC effectively addresses IE challenges for on-device LLMs, enhancing performance and efficiency.

Abstract: Information extraction (IE) plays a crucial role in natural language
processing (NLP) by converting unstructured text into structured knowledge.
Deploying computationally intensive large language models (LLMs) on
resource-constrained devices for information extraction is challenging,
particularly due to issues like hallucinations, limited context length, and
high latency-especially when handling diverse extraction schemas. To address
these challenges, we propose a two-stage information extraction approach
adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching
(DLISC), which enhances both schema identification and schema-aware extraction
in terms of effectiveness and efficiency. In particular, DLISC adopts an
Identification LoRA module for retrieving the most relevant schemas to a given
query, and an Extraction LoRA module for performing information extraction
based on the previously selected schemas. To accelerate extraction inference,
Incremental Schema Caching is incorporated to reduce redundant computation,
substantially improving efficiency. Extensive experiments across multiple
information extraction datasets demonstrate notable improvements in both
effectiveness and efficiency.

</details>


### [40] [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/pdf/2505.14996)
*Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

Main category: cs.CL

TL;DR: SELF-MAS is a self-supervised, inference-time framework for automatic multi-agent system (MAS) design, outperforming manual and automatic baselines by 7.44% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Current MAS designs rely on manual roles and protocols, which misalign with LLM strengths and lack adaptability to new tasks.

Method: SELF-MAS uses meta-level design to iteratively generate, evaluate, and refine MAS configurations dynamically for each problem, without a validation set.

Result: Experiments show SELF-MAS outperforms baselines by 7.44% in accuracy across various benchmarks, maintaining cost-efficiency.

Conclusion: Meta-level self-supervised design holds promise for creating adaptive and effective MAS.

Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation-set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce SELF-MAS, the first
self-supervised, inference-time only framework for automatic MAS design.
SELF-MAS employs meta-level design to iteratively generate, evaluate, and
refine MAS configurations tailored to each problem instance, without requiring
a validation set. Critically, it enables dynamic agent composition and problem
decomposition through meta-feedback on solvability and completeness.
Experiments across math, graduate-level QA, and software engineering
benchmarks, using both closed-source and open-source LLM back-bones of varying
sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS
baselines, achieving a 7.44% average accuracy improvement over the next
strongest baseline while maintaining cost-efficiency. These findings underscore
the promise of meta-level self-supervised design for creating effective and
adaptive MAS.

</details>


### [41] [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/pdf/2505.15000)
*Chengwei Wei, Bin Wang, Jung-jae Kim, Nancy F. Chen*

Main category: cs.CL

TL;DR: The paper introduces Spoken-MQA, a benchmark to evaluate speech-based models' mathematical reasoning, revealing gaps in handling verbalized math expressions and knowledge-oriented reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored ability of LLMs and MLLMs in performing mathematical reasoning from spoken input, addressing limitations in prior studies focused on simple audio tasks.

Method: Introduces Spoken-MQA, a benchmark with diverse math problems, evaluating both cascade (ASR + LLMs) and end-to-end speech LLMs.

Result: Speech LLMs struggle with direct arithmetic and verbalized math expressions, showing bias toward symbolic formats (e.g., LaTeX) and degraded knowledge reasoning.

Conclusion: Current speech LLMs need improvement in handling verbalized math and knowledge reasoning, highlighting the need for better benchmarks and model enhancements.

Abstract: Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs)
have led to strong reasoning ability across a wide range of tasks. However,
their ability to perform mathematical reasoning from spoken input remains
underexplored. Prior studies on speech modality have mostly focused on factual
speech understanding or simple audio reasoning tasks, providing limited insight
into logical step-by-step reasoning, such as that required for mathematical
problem solving. To address this gap, we introduce Spoken Math Question
Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical
reasoning capabilities of speech-based models, including both cascade models
(ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of
math problems, including pure arithmetic, single-step and multi-step contextual
reasoning, and knowledge-oriented reasoning problems, all presented in
unambiguous natural spoken language. Through extensive experiments, we find
that: (1) while some speech LLMs perform competitively on contextual reasoning
tasks involving basic arithmetic, they still struggle with direct arithmetic
problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical
expressions written in LaTex and have difficulty interpreting verbalized
mathematical expressions; and (3) mathematical knowledge reasoning abilities
are significantly degraded in current speech LLMs.

</details>


### [42] [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/pdf/2505.15024)
*Furong Jia, David Sontag, Monica Agrawal*

Main category: cs.CL

TL;DR: The paper examines how open-source LLMs learn clinical information, focusing on their understanding of clinical jargon and responses to unsupported medical claims. It highlights mismatches between pretraining data and real-world clinical usage.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs interpret clinical jargon and handle unsupported medical claims, given their lack of direct EHR training.

Method: Evaluated LLMs on a new dataset (MedLingo) and analyzed pretraining corpora for clinical jargon frequency and unsupported claims.

Result: Clinical jargon frequency in pretraining data correlates with model performance, but mismatches exist. Models sometimes parrot unsupported claims.

Conclusion: Pretraining data composition impacts LLM performance in clinical tasks, suggesting the need for better dataset alignment with real-world usage.

Abstract: Large language models (LLMs) have performed well across various clinical
natural language processing tasks, despite not being directly trained on
electronic health record (EHR) data. In this work, we examine how popular
open-source LLMs learn clinical information from large mined corpora through
two crucial but understudied lenses: (1) their interpretation of clinical
jargon, a foundational ability for understanding real-world clinical notes, and
(2) their responses to unsupported medical claims. For both use cases, we
investigate the frequency of relevant clinical information in their
corresponding pretraining corpora, the relationship between pretraining data
composition and model outputs, and the sources underlying this data. To isolate
clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.
Unsurprisingly, we find that the frequency of clinical jargon mentions across
major pretraining corpora correlates with model performance. However, jargon
frequently appearing in clinical notes often rarely appears in pretraining
corpora, revealing a mismatch between available data and real-world usage.
Similarly, we find that a non-negligible portion of documents support disputed
claims that can then be parroted by models. Finally, we classified and analyzed
the types of online sources in which clinical jargon and unsupported medical
claims appear, with implications for future dataset composition.

</details>


### [43] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/pdf/2505.15031)
*Wenqing Wu, Haixu Xi, Chengzhi Zhang*

Main category: cs.CL

TL;DR: This paper analyzes the consistency between reviewer text and confidence scores in peer reviews, using deep learning to evaluate alignment at word, sentence, and aspect levels. Results confirm high consistency and show higher confidence scores correlate with paper rejection.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack fine-grained analysis of text-score consistency in peer reviews, potentially missing key details about reviewer reliability.

Method: Deep learning and NLP techniques are used to analyze review text at word, sentence, and aspect levels, including hedge detection, report length, sentiment, and aspect mentions. Statistical tests (correlation, significance, regression) assess confidence scores' impact on outcomes.

Result: High text-score consistency is found across all levels. Regression shows higher confidence scores correlate with paper rejection, validating peer review fairness.

Conclusion: The study confirms the reliability of reviewer confidence scores and their alignment with review text, supporting the fairness of peer review processes.

Abstract: Peer review is vital in academia for evaluating research quality. Top AI
conferences use reviewer confidence scores to ensure review reliability, but
existing studies lack fine-grained analysis of text-score consistency,
potentially missing key details. This work assesses consistency at word,
sentence, and aspect levels using deep learning and NLP conference review data.
We employ deep learning to detect hedge sentences and aspects, then analyze
report length, hedge word/sentence frequency, aspect mentions, and sentiment to
evaluate text-score alignment. Correlation, significance, and regression tests
examine confidence scores' impact on paper outcomes. Results show high
text-score consistency across all levels, with regression revealing higher
confidence scores correlate with paper rejection, validating expert assessments
and peer review fairness.

</details>


### [44] [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/pdf/2505.15038)
*Haiyan Zhao, Xuansheng Wu, Fan Yang, Bo Shen, Ninghao Liu, Mengnan Du*

Main category: cs.CL

TL;DR: SDCV uses Sparse Autoencoders to remove noise from LLM hidden representations, improving steering robustness for linear probing and difference-in-means methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for deriving linear concept vectors from LLM hidden representations are affected by noise from diverse data, reducing steering robustness.

Method: Proposes Sparse Autoencoder-Denoised Concept Vectors (SDCV) to filter out noisy features from hidden representations.

Result: SDCV improves steering success rates for linear probing and difference-in-means. Noise hypothesis is validated via counterfactual experiments and feature visualizations.

Conclusion: SDCV enhances the robustness of linear concept vectors by denoising hidden representations, validated through experiments.

Abstract: Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.

</details>


### [45] [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/pdf/2505.15045)
*Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, Chen Zhao*

Main category: cs.CL

TL;DR: Diffusion language models outperform LLM-based embeddings in text embedding tasks due to their bidirectional architecture, achieving significant improvements in long-document and reasoning-intensive retrieval.


<details>
  <summary>Details</summary>
Motivation: LLM embeddings are limited by unidirectional attention, misaligning with bidirectional text embedding tasks. Diffusion models offer a bidirectional solution.

Method: Proposed adoption of diffusion language models for text embeddings, leveraging their bidirectional architecture. Conducted systematic study comparing performance with LLM-based models.

Result: Diffusion models outperformed LLMs by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, and 2% on instruction-following retrieval, while remaining competitive on traditional benchmarks.

Conclusion: Bidirectional attention in diffusion models is crucial for encoding global context in complex text, making them superior for text embedding tasks.

Abstract: Large language model (LLM)-based embedding models, benefiting from large
scale pre-training and post-training, have begun to surpass BERT and T5-based
models on general-purpose text embedding tasks such as document retrieval.
However, a fundamental limitation of LLM embeddings lies in the unidirectional
attention used during autoregressive pre-training, which misaligns with the
bidirectional nature of text embedding tasks. To this end, We propose adopting
diffusion language models for text embeddings, motivated by their inherent
bidirectional architecture and recent success in matching or surpassing LLMs
especially on reasoning tasks. We present the first systematic study of the
diffusion language embedding model, which outperforms the LLM-based embedding
model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,
2% on instruction-following retrieval, and achieve competitive performance on
traditional text embedding benchmarks. Our analysis verifies that bidirectional
attention is crucial for encoding global context in long and complex text.

</details>


### [46] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/pdf/2505.15046)
*Yifan Wu, Lutao Yan, Leixian Shen, Yinan Mei, Jiannan Wang, Yuyu Luo*

Main category: cs.CL

TL;DR: ChartCards is a framework for generating unified chart metadata to support multi-task chart understanding, reducing the need for large datasets. MetaChart, a dataset built using ChartCards, improves model performance by 5% on average.


<details>
  <summary>Details</summary>
Motivation: To address the high data collection and training costs of fine-tuning MLLMs for fine-grained chart understanding tasks.

Method: Proposes ChartCards, a framework synthesizing chart information (data tables, code, visual elements, captions) into structured metadata. Constructs MetaChart dataset for validation.

Result: MetaChart contains 10,862 tables, 85K charts, and 170K captions. Fine-tuning on it improves performance by 5% on average, with notable gains in retrieval (17%) and table conversion (28%).

Conclusion: ChartCards and MetaChart effectively reduce data needs and enhance multi-task chart understanding, demonstrating significant performance improvements.

Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.

</details>


### [47] [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/pdf/2505.15050)
*Gaurav Kumar, Debajyoti Mazumder, Ayush Garg, Jasabanta Patro*

Main category: cs.CL

TL;DR: The paper proposes a novel approach for automated fact-checking using entailment and generative language models to produce supporting/refuting justifications, achieving superior results over baselines.


<details>
  <summary>Details</summary>
Motivation: Current fact-checking methods either rely on language models' embedded knowledge (prone to hallucination) or fine-tuning with evidence (unsuccessful due to complexity and contradictory evidence).

Method: Uses entailment and generative language models to create supporting/refuting justifications, comparing prompting and fine-tuning strategies.

Result: Significant improvements over baselines: up to 8.20% (raw evidence), 16.39% (prompted understanding), and 28.57%-44.26% (entailed justifications).

Conclusion: The proposed method is effective for fact-checking, outperforming existing approaches, with code shared for reproducibility.

Abstract: Automated fact-checking is a crucial task in this digital age. To verify a
claim, current approaches majorly follow one of two strategies i.e. (i) relying
on embedded knowledge of language models, and (ii) fine-tuning them with
evidence pieces. While the former can make systems to hallucinate, the later
have not been very successful till date. The primary reason behind this is that
fact verification is a complex process. Language models have to parse through
multiple pieces of evidence before making a prediction. Further, the evidence
pieces often contradict each other. This makes the reasoning process even more
complex. We proposed a simple yet effective approach where we relied on
entailment and the generative ability of language models to produce
''supporting'' and ''refuting'' justifications (for the truthfulness of a
claim). We trained language models based on these justifications and achieved
superior results. Apart from that, we did a systematic comparison of different
prompting and fine-tuning strategies, as it is currently lacking in the
literature. Some of our observations are: (i) training language models with raw
evidence sentences registered an improvement up to 8.20% in macro-F1, over the
best performing baseline for the RAW-FC dataset, (ii) similarly, training
language models with prompted claim-evidence understanding (TBE-2) registered
an improvement (with a margin up to 16.39%) over the baselines for the same
dataset, (iii) training language models with entailed justifications (TBE-3)
outperformed the baselines by a huge margin (up to 28.57% and 44.26% for
LIAR-RAW and RAW-FC, respectively). We have shared our code repository to
reproduce the results.

</details>


### [48] [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/pdf/2505.15054)
*Feiyang Cai, Jiahui Bai, Tao Tang, Joshua Luo, Tianyu Zhu, Ling Liu, Feng Luo*

Main category: cs.CL

TL;DR: MolLangBench is a benchmark for evaluating molecule-language tasks like recognition, editing, and generation, revealing current AI limitations.


<details>
  <summary>Details</summary>
Motivation: To address the need for precise molecular recognition, editing, and generation in chemistry and AI systems.

Method: Constructed recognition tasks using cheminformatics tools and curated editing/generation tasks with expert annotation. Supports multiple molecular representations.

Result: State-of-the-art models perform poorly, with accuracies of 79.2% (recognition), 78.5% (editing), and 29.0% (generation).

Conclusion: MolLangBench highlights AI shortcomings and aims to spur research for better chemical AI systems.

Abstract: Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.

</details>


### [49] [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/pdf/2505.15055)
*Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao*

Main category: cs.CL

TL;DR: The paper critiques current LLM benchmarks, proposing PSN-IRT for better evaluation, revealing benchmark flaws, and showing PSN-IRT's efficiency in smaller, human-aligned benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies and poor separability in LLM benchmarks to accurately reflect model capabilities.

Method: Introduces PSN-IRT, an enhanced Item Response Theory framework, for better item and model ability estimation.

Result: Identifies significant flaws in current benchmarks and shows PSN-IRT's effectiveness in smaller, human-preferred benchmarks.

Conclusion: PSN-IRT improves benchmark reliability and efficiency, aligning better with human judgment.

Abstract: The evaluation of large language models (LLMs) via benchmarks is widespread,
yet inconsistencies between different leaderboards and poor separability among
top models raise concerns about their ability to accurately reflect authentic
model capabilities. This paper provides a critical analysis of benchmark
effectiveness, examining main-stream prominent LLM benchmarks using results
from diverse models. We first propose a new framework for accurate and reliable
estimations of item characteristics and model abilities. Specifically, we
propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced
Item Response Theory framework that incorporates a rich set of item parameters
within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive
analysis which reveals significant and varied shortcomings in the measurement
quality of current benchmarks. Furthermore, we demonstrate that leveraging
PSN-IRT is able to construct smaller benchmarks while maintaining stronger
alignment with human preference.

</details>


### [50] [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/pdf/2505.15062)
*Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro*

Main category: cs.CL

TL;DR: Self-GIVE improves LLMs' associative thinking for biomedical QA by reducing token usage and enhancing performance on smaller models.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in GIVE's knowledge extrapolation and limitations in smaller LLMs.

Method: Uses a retrieve-RL framework to automate associative thinking, leveraging structured information and entity sets.

Result: Boosts Qwen2.5 3B and 7B models' performance by up to 28.5%→71.4% and 78.6→90.5%, reducing token usage by 90%.

Conclusion: Self-GIVE enables scalable integration of structured retrieval and reasoning, matching GPT3.5 turbo's performance with fewer resources.

Abstract: When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
"hormones helping mental disorders" with "melatonin being a hormone and
insomnia a mental disorder" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and pruning of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.

</details>


### [51] [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/pdf/2505.15063)
*Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, Preslav Nakov*

Main category: cs.CL

TL;DR: UrduFactCheck is the first comprehensive fact-checking framework for Urdu, addressing the lack of resources for low-resource languages. It outperforms baselines and benchmarks LLMs for Urdu factuality.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has highlighted factual reliability issues, especially in low-resource languages like Urdu, which lacks dedicated fact-checking tools.

Method: Introduces UrduFactCheck with a dynamic, multi-strategy evidence retrieval pipeline, combining monolingual and translation-based approaches. Two new benchmarks (UrduFactBench and UrduFactQA) are curated.

Result: UrduFactCheck outperforms baselines, especially with translation-augmented variants. Benchmarked 12 SOTA LLMs, revealing gaps between proprietary and open-source models.

Conclusion: UrduFactCheck fills a critical gap for Urdu fact-checking, with open-sourced code and datasets to support further research.

Abstract: The rapid use of large language models (LLMs) has raised critical concerns
regarding the factual reliability of their outputs, especially in low-resource
languages such as Urdu. Existing automated fact-checking solutions
overwhelmingly focus on English, leaving a significant gap for the 200+ million
Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first
comprehensive, modular fact-checking framework specifically tailored for Urdu.
Our system features a dynamic, multi-strategy evidence retrieval pipeline that
combines monolingual and translation-based approaches to address the scarcity
of high-quality Urdu evidence. We curate and release two new hand-annotated
benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating
LLM factuality. Extensive experiments demonstrate that UrduFactCheck,
particularly its translation-augmented variants, consistently outperforms
baselines and open-source alternatives on multiple metrics. We further
benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in
Urdu, highlighting persistent gaps between proprietary and open-source models.
UrduFactCheck's code and datasets are open-sourced and publicly available at
https://github.com/mbzuai-nlp/UrduFactCheck.

</details>


### [52] [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/pdf/2505.15065)
*Suhas BN, Yash Mahajan, Dominik Mattioli, Andrew M. Sherrill, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah*

Main category: cs.CL

TL;DR: Small language models (0.5B-5B parameters) can engage in trauma-informed dialogue for PTSD, but empathy gains are scenario- and user-dependent, with limitations.


<details>
  <summary>Details</summary>
Motivation: To explore if small language models can provide empathetic, trauma-informed dialogue for PTSD individuals, supplementing clinical care.

Method: Introduced TIDE dataset (10,000 dialogues) and evaluated eight small models before/after fine-tuning against Claude Sonnet 3.5, using human and automatic metrics.

Result: Fine-tuning improves perceived empathy, but gains vary by scenario and user. Smaller models face an empathy ceiling. Demographic preferences (e.g., age, education) influence perceived empathy.

Conclusion: Small models show promise but require context- and user-aware design. TIDE dataset release supports safe, ethical AI for mental health care.

Abstract: Can small language models with 0.5B to 5B parameters meaningfully engage in
trauma-informed, empathetic dialogue for individuals with PTSD? We address this
question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning
500 diverse PTSD client personas and grounded in a three-factor empathy model:
emotion recognition, distress normalization, and supportive reflection. All
scenarios and reference responses were reviewed for realism and trauma
sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight
small language models before and after fine-tuning, comparing their outputs to
a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and
automatic metrics show that fine-tuning generally improves perceived empathy,
but gains are highly scenario- and user-dependent, with smaller models facing
an empathy ceiling. Demographic analysis shows older adults value distress
validation and graduate-educated users prefer nuanced replies, while gender
effects are minimal. We highlight the limitations of automatic metrics and the
need for context- and user-aware system design. Our findings, along with the
planned release of TIDE, provide a foundation for building safe,
resource-efficient, and ethically sound empathetic AI to supplement, not
replace, clinical mental health care.

</details>


### [53] [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/pdf/2505.15069)
*Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik*

Main category: cs.CL

TL;DR: The paper explores bandit-based algorithms for selecting optimal NMT models in low-resource settings, demonstrating effectiveness across African languages.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in NMT for low-resource languages, particularly domain adaptation with limited data and poor generalization.

Method: Uses bandit-based algorithms (Upper Confidence Bound, Linear UCB, Neural Linear Bandit, Thompson Sampling) for optimal model selection.

Result: Shows robustness and effectiveness in scenarios with and without target data across three African languages.

Conclusion: Bandit-based model selection is a viable solution for low-resource NMT, enhancing performance in constrained settings.

Abstract: Neural Machine Translation (NMT) systems face significant challenges when
working with low-resource languages, particularly in domain adaptation tasks.
These difficulties arise due to limited training data and suboptimal model
generalization, As a result, selecting an optimal model for translation is
crucial for achieving strong performance on in-domain data, particularly in
scenarios where fine-tuning is not feasible or practical. In this paper, we
investigate strategies for selecting the most suitable NMT model for a given
domain using bandit-based algorithms, including Upper Confidence Bound, Linear
UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively
addresses the resource constraints by facilitating optimal model selection with
high confidence. We evaluate the approach across three African languages and
domains, demonstrating its robustness and effectiveness in both scenarios where
target data is available and where it is absent.

</details>


### [54] [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/pdf/2505.15071)
*Chen Huang, Junkai Luo, Xinzuo Wang, Wenqiang Lei, Jiancheng Lv*

Main category: cs.CL

TL;DR: The paper introduces CHEER, a dataset of Chinese internet buzzwords, and RESS, a method to improve LLM-generated definitions. It benchmarks RESS against other methods, highlighting challenges like prior exposure reliance and inferential weaknesses.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can accurately define Chinese internet buzzwords using UGC, addressing gaps in existing methods.

Method: Proposes RESS, a novel method to guide LLMs in generating accurate definitions, and introduces the CHEER dataset for benchmarking.

Result: RESS outperforms other methods but reveals challenges like over-reliance on prior knowledge and poor UGC quality identification.

Conclusion: The work advances LLM-based definition generation and provides a foundation for future research, with dataset and code publicly available.

Abstract: The massive user-generated content (UGC) available in Chinese social media is
giving rise to the possibility of studying internet buzzwords. In this paper,
we study if large language models (LLMs) can generate accurate definitions for
these buzzwords based on UGC as examples. Our work serves a threefold
contribution. First, we introduce CHEER, the first dataset of Chinese internet
buzzwords, each annotated with a definition and relevant UGC. Second, we
propose a novel method, called RESS, to effectively steer the comprehending
process of LLMs to produce more accurate buzzword definitions, mirroring the
skills of human language learning. Third, with CHEER, we benchmark the
strengths and weaknesses of various off-the-shelf definition generation methods
and our RESS. Our benchmark demonstrates the effectiveness of RESS while
revealing crucial shared challenges: over-reliance on prior exposure,
underdeveloped inferential abilities, and difficulty identifying high-quality
UGC to facilitate comprehension. We believe our work lays the groundwork for
future advancements in LLM-based definition generation. Our dataset and code
are available at https://github.com/SCUNLP/Buzzword.

</details>


### [55] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/pdf/2505.15074)
*Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang*

Main category: cs.CL

TL;DR: DISCO improves GRPO by addressing domain imbalance with domain-aware and difficulty-aware reward scaling, enhancing fairness and performance.


<details>
  <summary>Details</summary>
Motivation: GRPO's assumptions of balanced domains and uniform alignment fail in real-world imbalanced data, leading to poor generalization and fairness.

Method: DISCO introduces domain-aware reward scaling and difficulty-aware reward scaling to counteract bias and prioritize uncertain prompts.

Result: DISCO outperforms GRPO variants by 5% on Qwen3 models and achieves state-of-the-art results on multi-domain benchmarks.

Conclusion: DISCO effectively addresses GRPO's limitations, improving generalization and fairness in LLM alignment.

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


### [56] [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/pdf/2505.15075)
*Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara*

Main category: cs.CL

TL;DR: New benchmarks (KnowRecall and VisRecall) reveal MLLMs struggle with cross-lingual consistency, highlighting the need for more robust multilingual and culturally aware models.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of MLLMs lacks consistent performance across languages, especially when integrating cultural knowledge.

Method: Introduces two benchmarks: KnowRecall (factual knowledge consistency in 15 languages) and VisRecall (visual memory consistency in 9 languages).

Result: State-of-the-art MLLMs, including proprietary ones, fail to achieve cross-lingual consistency.

Conclusion: Emphasizes the need for improved approaches to develop truly multilingual and culturally aware MLLMs.

Abstract: The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.

</details>


### [57] [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/pdf/2505.15087)
*Zhiyu Shen, Jiyuan Liu, Yunhe Pang, Yanghui Rao*

Main category: cs.CL

TL;DR: HopWeaver is an automatic framework for synthesizing high-quality multi-hop questions from unstructured text without human intervention, outperforming manual methods in cost and quality.


<details>
  <summary>Details</summary>
Motivation: Creating extensive and high-quality MHQA datasets is challenging due to expensive manual annotation and simplistic synthesis methods.

Method: HopWeaver automatically synthesizes bridge and comparison multi-hop questions by identifying complementary documents and constructing coherent reasoning paths.

Result: Empirical evaluations show HopWeaver's questions match or surpass human-annotated datasets in quality at lower cost.

Conclusion: HopWeaver is valuable for MHQA dataset creation in specialized domains with limited annotated resources.

Abstract: Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's
capability to integrate information from diverse sources. However, creating
extensive and high-quality MHQA datasets is challenging: (i) manual annotation
is expensive, and (ii) current synthesis methods often produce simplistic
questions or require extensive manual guidance. This paper introduces
HopWeaver, the first automatic framework synthesizing authentic multi-hop
questions from unstructured text corpora without human intervention. HopWeaver
synthesizes two types of multi-hop questions (bridge and comparison) using an
innovative approach that identifies complementary documents across corpora. Its
coherent pipeline constructs authentic reasoning paths that integrate
information across multiple documents, ensuring synthesized questions
necessitate authentic multi-hop reasoning. We further present a comprehensive
system for evaluating synthesized multi-hop questions. Empirical evaluations
demonstrate that the synthesized questions achieve comparable or superior
quality to human-annotated datasets at a lower cost. Our approach is valuable
for developing MHQA datasets in specialized domains with scarce annotated
resources. The code for HopWeaver is publicly available.

</details>


### [58] [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/pdf/2505.15090)
*Sona Elza Simon, Preethi Jyothi*

Main category: cs.CL

TL;DR: DeFT-X improves cross-lingual transfer by denoising pretrained model weights before sparse fine-tuning, outperforming prior methods on low-resource tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effective cross-lingual transfer, especially for low-resource languages, by enhancing sparse fine-tuning methods.

Method: DeFT-X denoises weight matrices using singular value decomposition before magnitude pruning, creating more robust sparse fine-tuned vectors (SFTs).

Result: DeFT-X matches or surpasses SFT and other baselines in sentiment classification (NusaX) and natural language inference (AmericasNLI) for low-resource languages.

Conclusion: DeFT-X offers a more effective approach for cross-lingual transfer, particularly benefiting low-resource languages.

Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.

</details>


### [59] [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/pdf/2505.15094)
*Jing Yu, Yuqi Tang, Kehua Feng, Mingyang Rao, Lei Liang, Zhiqiang Zhang, Mengshu Sun, Wen Zhang, Qiang Zhang, Keyan Ding, Huajun Chen*

Main category: cs.CL

TL;DR: The paper introduces SciCUEval, a benchmark dataset to evaluate LLMs' scientific context understanding across diverse domains, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs lack coverage of scientific domains' complexity, necessitating a specialized evaluation tool.

Method: Constructed SciCUEval with ten domain-specific sub-datasets, incorporating diverse data modalities and evaluating four core competencies through varied question formats.

Result: Extensive evaluations reveal strengths and limitations of state-of-the-art LLMs in scientific context understanding.

Conclusion: SciCUEval provides insights for future development of scientific-domain LLMs, highlighting areas for improvement.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in contextual
understanding and reasoning. However, evaluating their performance across
diverse scientific domains remains underexplored, as existing benchmarks
primarily focus on general domains and fail to capture the intricate complexity
of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive
benchmark dataset tailored to assess the scientific context understanding
capability of LLMs. It comprises ten domain-specific sub-datasets spanning
biology, chemistry, physics, biomedicine, and materials science, integrating
diverse data modalities including structured tables, knowledge graphs, and
unstructured texts. SciCUEval systematically evaluates four core competencies:
Relevant information identification, Information-absence detection,
Multi-source information integration, and Context-aware inference, through a
variety of question formats. We conduct extensive evaluations of
state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their
strengths and limitations in scientific context understanding, and offering
valuable insights for the future development of scientific-domain LLMs.

</details>


### [60] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/pdf/2505.15095)
*Ishmanbir Singh, Dipankar Srirag, Aditya Joshi*

Main category: cs.CL

TL;DR: The paper introduces PMP for explainable sarcasm detection in Australian and Indian English, outperforming other methods on LLMs like GEMMA and LLAMA.


<details>
  <summary>Details</summary>
Motivation: Sarcasm's incongruity in sentiment analysis is harder for region-specific implications, requiring explainable detection methods.

Method: Uses PMP for sarcasm detection, manually annotates BESSTIE dataset, and compares with FLUTE dataset using LLMs.

Result: PMP achieves significant performance gains over other prompting strategies across tasks and datasets.

Conclusion: PMP effectively generates sarcasm explanations for diverse English varieties, with agentic prompting aiding context-related issues.

Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.

</details>


### [61] [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/pdf/2505.15105)
*Aryaman Arora, Neil Rathi, Nikil Roashan Selvam, Róbert Csórdas, Dan Jurafsky, Christopher Potts*

Main category: cs.CL

TL;DR: The paper analyzes why some state space models (SSMs) fail at associative recall tasks, revealing mechanistic differences in how architectures like Transformers and SSMs handle context.


<details>
  <summary>Details</summary>
Motivation: To understand why certain SSMs underperform on associative recall tasks and identify mechanistic differences between successful and failing architectures.

Method: Experiments on associative recall (AR) and a new synthetic task (Associative Treecall, ATR), using causal interventions to analyze mechanisms.

Result: Transformers and Based SSMs succeed by storing key-value associations in-context, while other SSMs fail except Mamba due to its short convolution.

Conclusion: Mechanistic evaluations are crucial as architectures with similar accuracy can differ fundamentally in their operations.

Abstract: State space models (SSMs) for language modelling promise an efficient and
performant alternative to quadratic-attention Transformers, yet show variable
performance on recalling basic information from the context. While performance
on synthetic tasks like Associative Recall (AR) can point to this deficiency,
behavioural metrics provide little information as to why--on a mechanistic
level--certain architectures fail and others succeed. To address this, we
conduct experiments on AR and find that only Transformers and Based SSM models
fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,
Hyena) fail. We then use causal interventions to explain why. We find that
Transformers and Based learn to store key-value associations in-context using
induction heads. By contrast, the SSMs compute these associations only at the
last state, with only Mamba succeeding because of its short convolution
component. To extend and deepen these findings, we introduce Associative
Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR
introduces language-like hierarchical structure into the AR setting. We find
that all architectures learn the same mechanism as they did for AR, and the
same three models succeed at the task. These results reveal that architectures
with similar accuracy may still have substantive differences, motivating the
adoption of mechanistic evaluations.

</details>


### [62] [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/pdf/2505.15107)
*Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu*

Main category: cs.CL

TL;DR: StepSearch improves multi-hop QA by using step-wise rewards and token-level supervision, outperforming global-reward baselines with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing RL-trained LLMs underperform on complex multi-hop QA due to sparse global rewards, prompting the need for finer-grained supervision.

Method: StepSearch employs step-wise proximal policy optimization with intermediate search rewards and token-level process supervision based on information gain and redundancy penalties.

Result: Achieves 11.2% and 4.2% absolute improvements for 3B and 7B models over RL baselines, using only 19k training data.

Conclusion: Fine-grained, stepwise supervision effectively optimizes deep search LLMs, as demonstrated by StepSearch's superior performance.

Abstract: Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our
implementation is publicly available at
https://github.com/zxh20001117/StepSearch.

</details>


### [63] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/pdf/2505.15108)
*Ian Steenstra, Timothy W. Bickmore*

Main category: cs.CL

TL;DR: The paper introduces a risk taxonomy for evaluating conversational AI psychotherapists to address the lack of standardized methods for detecting nuanced risks in therapeutic interactions.


<details>
  <summary>Details</summary>
Motivation: The deployment of AI psychotherapists has led to adverse outcomes like user harm and suicide due to inadequate evaluation methods.

Method: Developed through literature review, expert interviews, and alignment with clinical criteria, the taxonomy provides a structured approach to assessing risks.

Result: The taxonomy enables monitoring of cognitive risk factors in counseling sessions and benchmarking AI psychotherapists with simulated patients.

Conclusion: This taxonomy is a foundational step toward safer AI-driven mental health support.

Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual
Agents acting as psychotherapists presents significant opportunities for
expanding mental healthcare access. However, their deployment has also been
linked to serious adverse outcomes, including user harm and suicide,
facilitated by a lack of standardized evaluation methodologies capable of
capturing the nuanced risks of therapeutic interaction. Current evaluation
techniques lack the sensitivity to detect subtle changes in patient cognition
and behavior during therapy sessions that may lead to subsequent
decompensation. We introduce a novel risk taxonomy specifically designed for
the systematic evaluation of conversational AI psychotherapists. Developed
through an iterative process including review of the psychotherapy risk
literature, qualitative interviews with clinical and legal experts, and
alignment with established clinical criteria (e.g., DSM-5) and existing
assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured
approach to identifying and assessing user/patient harms. We provide a
high-level overview of this taxonomy, detailing its grounding, and discuss
potential use cases. We discuss two use cases in detail: monitoring cognitive
model-based risk factors during a counseling conversation to detect unsafe
deviations, in both human-AI counseling sessions and in automated benchmarking
of AI psychotherapists with simulated patients. The proposed taxonomy offers a
foundational step towards establishing safer and more responsible innovation in
the domain of AI-driven mental health support.

</details>


### [64] [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/pdf/2505.15110)
*Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che*

Main category: cs.CL

TL;DR: The paper introduces Row-of-Thought (RoT), a training-free method for table reasoning that iteratively traverses rows, reducing hallucinations and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Long Chain-of-Thought (Long CoT) methods for table reasoning are costly and unreliable due to hallucinations. RoT addresses these issues.

Method: RoT performs row-wise table traversal iteratively, enabling reasoning extension and reflection-based refinement without training.

Result: RoT outperforms RLLMs by 4.3% and achieves state-of-the-art results on WikiTableQuestions and TableBench, with higher efficiency.

Conclusion: RoT is an effective, efficient, and training-free alternative to Long CoT for table reasoning, reducing hallucinations and improving performance.

Abstract: The table reasoning task, crucial for efficient data acquisition, aims to
answer questions based on the given table. Recently, reasoning large language
models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance
reasoning capabilities, leading to brilliant performance on table reasoning.
However, Long CoT suffers from high cost for training and exhibits low
reliability due to table content hallucinations. Therefore, we propose
Row-of-Thought (RoT), which performs iteratively row-wise table traversal,
allowing for reasoning extension and reflection-based refinement at each
traversal. Scaling reasoning length by row-wise traversal and leveraging
reflection capabilities of LLMs, RoT is training-free. The sequential traversal
encourages greater attention to the table, thus reducing hallucinations.
Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an
average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions
and TableBench with comparable models, proving its effectiveness. Also, RoT
outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.

</details>


### [65] [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/pdf/2505.15117)
*Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, Jiawei Han*

Main category: cs.CL

TL;DR: The paper explores RL for training LLM-based search agents, focusing on reward design, LLM choice, and search engine impact, offering practical insights.


<details>
  <summary>Details</summary>
Motivation: To understand optimal design factors for RL-trained LLM search agents, including reward formulation, LLM characteristics, and search engine role.

Method: Comprehensive empirical studies investigating reward types, LLM scale/initialization, and search engine effects.

Result: Format rewards improve performance; LLM scale/initialization affects outcomes; search engine choice impacts training and robustness.

Conclusion: Provides guidelines for building effective RL-trained LLM search agents, with code available for implementation.

Abstract: Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [66] [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/pdf/2505.15182)
*Jeonghye Kim, Sojeong Rhee, Minbeom Kim, Dohyung Kim, Sangmook Lee, Youngchul Sung, Kyomin Jung*

Main category: cs.CL

TL;DR: ReflAct improves LLM agent reliability by grounding decisions in states and aligning goals, outperforming ReAct by 27.7%.


<details>
  <summary>Details</summary>
Motivation: ReAct's ungrounded reasoning and misalignment issues lead to errors and hallucinations, prompting the need for a more reliable backbone.

Method: ReflAct introduces continuous reflection on the agent's state relative to its goal, enforcing grounded decisions and goal alignment.

Result: ReflAct achieves a 93.3% success rate in ALFWorld, surpassing ReAct by 27.7% and outperforming enhanced ReAct versions.

Conclusion: Strengthening the core reasoning backbone (ReflAct) is key to reliable agent performance, addressing ReAct's limitations.

Abstract: Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.

</details>


### [67] [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/pdf/2505.15196)
*Weiqi Wang, Limeng Cui, Xin Liu, Sreyashi Nag, Wenju Xu, Chen Luo, Sheikh Muhammad Sarwar, Yang Li, Hansu Gu, Hui Liu, Changlong Yu, Jiaxin Bai, Yifan Gao, Haiyang Zhang, Qi He, Shuiwang Ji, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper introduces E-commerce Script Planning (EcomScript), a task involving goal-oriented action sequences for shopping, and proposes a framework for scalable product-enriched script generation. It also presents a large-scale dataset (EcomScriptBench) and benchmarks, highlighting challenges for LLMs in this task.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored capability of LLMs in e-commerce script planning, where current methods struggle with simultaneous script planning and product retrieval, semantic discrepancies, and lack of evaluation benchmarks.

Method: The paper defines EcomScript as three sequential subtasks and proposes a framework for generating product-enriched scripts by linking products to actions based on semantic similarity. It constructs EcomScriptBench, a dataset of 605,229 scripts from 2.4M products, with human-annotated benchmarks.

Result: Experiments show LLMs struggle with EcomScript tasks, even after fine-tuning, but performance improves when product purchase intentions are incorporated.

Conclusion: The work advances e-commerce script planning by formalizing the task, providing a scalable framework, and introducing a benchmark, while highlighting the limitations of current LLMs and the value of purchase intentions.

Abstract: Goal-oriented script planning, or the ability to devise coherent sequences of
actions toward specific goals, is commonly employed by humans to plan for
typical activities. In e-commerce, customers increasingly seek LLM-based
assistants to generate scripts and recommend products at each step, thereby
facilitating convenient and efficient shopping experiences. However, this
capability remains underexplored due to several challenges, including the
inability of LLMs to simultaneously conduct script planning and product
retrieval, difficulties in matching products caused by semantic discrepancies
between planned actions and search queries, and a lack of methods and benchmark
data for evaluation. In this paper, we step forward by formally defining the
task of E-commerce Script Planning (EcomScript) as three sequential subtasks.
We propose a novel framework that enables the scalable generation of
product-enriched scripts by associating products with each step based on the
semantic similarity between the actions and their purchase intentions. By
applying our framework to real-world e-commerce data, we construct the very
first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229
scripts sourced from 2.4 million products. Human annotations are then conducted
to provide gold labels for a sampled subset, forming an evaluation benchmark.
Extensive experiments reveal that current (L)LMs face significant challenges
with EcomScript tasks, even after fine-tuning, while injecting product purchase
intentions improves their performance.

</details>


### [68] [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/pdf/2505.15209)
*Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No*

Main category: cs.CL

TL;DR: DUSK benchmark evaluates unlearning methods under realistic data overlap, revealing current methods struggle with selective removal of context-specific knowledge while preserving shared facts.


<details>
  <summary>Details</summary>
Motivation: Address the gap in unlearning evaluations by considering overlapping content between forget and retain sets, a common real-world scenario.

Method: Introduces DUSK, a benchmark with document sets sharing factual content but differing in style, to test selective unlearning.

Result: Most unlearning methods remove surface-level text but fail to erase deeper knowledge without harming shared content.

Conclusion: DUSK highlights the need for more precise unlearning techniques and is released as a public benchmark for future development.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about the unauthorized use of copyrighted or
sensitive data. Machine unlearning aims to remove such 'forget' data while
preserving utility and information from the 'retain' set. However, existing
evaluations typically assume that forget and retain sets are fully disjoint,
overlooking realistic scenarios where they share overlapping content. For
instance, a news article may need to be unlearned, even though the same event,
such as an earthquake in Japan, is also described factually on Wikipedia.
Effective unlearning should remove the specific phrasing of the news article
while preserving publicly supported facts. In this paper, we introduce DUSK, a
benchmark designed to evaluate unlearning methods under realistic data overlap.
DUSK constructs document sets that describe the same factual content in
different styles, with some shared information appearing across all sets and
other content remaining unique to each. When one set is designated for
unlearning, an ideal method should remove its unique content while preserving
shared facts. We define seven evaluation metrics to assess whether unlearning
methods can achieve this selective removal. Our evaluation of nine recent
unlearning methods reveals a key limitation: while most can remove
surface-level text, they often fail to erase deeper, context-specific knowledge
without damaging shared content. We release DUSK as a public benchmark to
support the development of more precise and reliable unlearning techniques for
real-world applications.

</details>


### [69] [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/pdf/2505.15210)
*Jie Ma, Ning Qu, Zhitao Gao, Rui Xing, Jun Liu, Hongbin Pei, Jiang Xie, Linyun Song, Pinghui Wang, Jing Tao, Zhou Su*

Main category: cs.CL

TL;DR: The paper proposes Deliberation over Priors (DP), a framework leveraging knowledge graph priors to enhance LLM reliability and faithfulness, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address LLM hallucinations by better utilizing structural and constraint priors in knowledge graphs.

Method: DP integrates structural priors via supervised fine-tuning and Kahneman-Tversky optimization, and uses constraint priors for refined reasoning verification.

Result: DP improves Hit@1 by 13% on ComplexWebQuestions and generates trustworthy responses.

Conclusion: DP effectively enhances LLM reliability and faithfulness, with practical flexibility.

Abstract: Knowledge graph-based retrieval-augmented generation seeks to mitigate
hallucinations in Large Language Models (LLMs) caused by insufficient or
outdated knowledge. However, existing methods often fail to fully exploit the
prior knowledge embedded in knowledge graphs (KGs), particularly their
structural information and explicit or implicit constraints. The former can
enhance the faithfulness of LLMs' reasoning, while the latter can improve the
reliability of response generation. Motivated by these, we propose a
trustworthy reasoning framework, termed Deliberation over Priors (DP), which
sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a
progressive knowledge distillation strategy that integrates structural priors
into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky
optimization, thereby improving the faithfulness of relation path generation.
Furthermore, our framework employs a reasoning-introspection strategy, which
guides LLMs to perform refined reasoning verification based on extracted
constraint priors, ensuring the reliability of response generation. Extensive
experiments on three benchmark datasets demonstrate that DP achieves new
state-of-the-art performance, especially a Hit@1 improvement of 13% on the
ComplexWebQuestions dataset, and generates highly trustworthy responses. We
also conduct various analyses to verify its flexibility and practicality. The
code is available at https://github.com/reml-group/Deliberation-on-Priors.

</details>


### [70] [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/pdf/2505.15214)
*Sangyeon Yoon, Wonje Jeung, Albert No*

Main category: cs.CL

TL;DR: R-TOFU is a benchmark for evaluating unlearning in Large Reasoning Models (LRMs), focusing on multi-step reasoning traces. It reveals residual knowledge missed by answer-level checks and introduces Reasoned IDK for better forgetting efficacy and model utility.


<details>
  <summary>Details</summary>
Motivation: Unlearning private or copyrighted information in LRMs is challenging due to multi-step reasoning traces, requiring a specialized benchmark.

Method: R-TOFU augments unlearning tasks with realistic chain-of-thought annotations and step-wise metrics. It compares gradient-based and preference-optimization baselines and proposes Reasoned IDK.

Result: Conventional unlearning leaves traces in reasoning. Reasoned IDK balances forgetting and utility better. Decoding variants like ZeroThink can still reveal forgotten content.

Conclusion: R-TOFU provides a foundation for improving unlearning in LRMs while preserving reasoning capabilities, highlighting the need for diverse evaluation settings.

Abstract: Large Reasoning Models (LRMs) embed private or copyrighted information not
only in their final answers but also throughout multi-step chain-of-thought
(CoT) traces, making reliable unlearning far more demanding than in standard
LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to
this setting. R-TOFU augments existing unlearning tasks with realistic CoT
annotations and provides step-wise metrics that expose residual knowledge
invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive
comparison of gradient-based and preference-optimization baselines and show
that conventional answer-only objectives leave substantial forget traces in
reasoning. We further propose Reasoned IDK, a preference-optimization variant
that preserves coherent yet inconclusive reasoning, achieving a stronger
balance between forgetting efficacy and model utility than earlier refusal
styles. Finally, we identify a failure mode: decoding variants such as
ZeroThink and LessThink can still reveal forgotten content despite seemingly
successful unlearning, emphasizing the need to evaluate models under diverse
decoding settings. Together, the benchmark, analysis, and new baseline
establish a systematic foundation for studying and improving unlearning in LRMs
while preserving their reasoning capabilities.

</details>


### [71] [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/pdf/2505.15229)
*Qihan Wang, Shidong Pan, Tal Linzen, Emily Black*

Main category: cs.CL

TL;DR: Multilingual prompting enhances cultural diversity in LLM outputs by using culturally and linguistically varied prompts, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: LLMs lack cultural diversity in responses; multilingual prompting aims to activate broader cultural knowledge.

Method: Generates variations of a base prompt with cultural/linguistic cues, combines responses.

Result: Outperforms high-temperature sampling, step-by-step recall, and personas prompting; reduces hallucinations.

Conclusion: Multilingual prompting effectively increases diversity, with benefits varying by language resource and model size.

Abstract: Large Language Models (LLMs) are known to lack cultural representation and
overall diversity in their generations, from expressing opinions to answering
factual questions. To mitigate this problem, we propose multilingual prompting:
a prompting method which generates several variations of a base prompt with
added cultural and linguistic cues from several cultures, generates responses,
and then combines the results. Building on evidence that LLMs have
language-specific knowledge, multilingual prompting seeks to increase diversity
by activating a broader range of cultural knowledge embedded in model training
data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA
70B, and LLaMA 8B), we show that multilingual prompting consistently
outperforms existing diversity-enhancing techniques such as high-temperature
sampling, step-by-step recall, and personas prompting. Further analyses show
that the benefits of multilingual prompting vary with language resource level
and model size, and that aligning the prompting language with the cultural cues
reduces hallucination about culturally-specific information.

</details>


### [72] [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/pdf/2505.15245)
*Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng*

Main category: cs.CL

TL;DR: The paper introduces GETER, a framework combining graph structures with text to improve explainable temporal reasoning in LLMs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing work on LLMs focuses on performance but lacks explainability in temporal reasoning. This paper aims to bridge that gap.

Method: Proposes GETER, integrating temporal knowledge graphs and a structure-text prefix adapter to enhance LLMs' explainable reasoning.

Result: GETER outperforms existing methods, showing strong generalization and effectiveness in explainable temporal reasoning.

Conclusion: The framework successfully addresses the challenge of explainability in LLMs' temporal reasoning, with promising experimental results.

Abstract: While large language models (LLMs) show great potential in temporal
reasoning, most existing work focuses heavily on enhancing performance, often
neglecting the explainable reasoning processes underlying the results. To
address this gap, we introduce a comprehensive benchmark covering a wide range
of temporal granularities, designed to systematically evaluate LLMs'
capabilities in explainable temporal reasoning. Furthermore, our findings
reveal that LLMs struggle to deliver convincing explanations when relying
solely on textual information. To address challenge, we propose GETER, a novel
structure-aware generative framework that integrates Graph structures with text
for Explainable TEmporal Reasoning. Specifically, we first leverage temporal
knowledge graphs to develop a temporal encoder that captures structural
information for the query. Subsequently, we introduce a structure-text prefix
adapter to map graph structure features into the text embedding space. Finally,
LLMs generate explanation text by seamlessly integrating the soft graph token
with instruction-tuning prompt tokens. Experimental results indicate that GETER
achieves state-of-the-art performance while also demonstrating its
effectiveness as well as strong generalization capabilities. Our dataset and
code are available at https://github.com/carryTatum/GETER.

</details>


### [73] [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/pdf/2505.15249)
*Yerin Hwang, Dongryeol Lee, Kyungmin Min, Taegwan Kang, Yong-il Kim, Kyomin Jung*

Main category: cs.CL

TL;DR: LVLMs are vulnerable to adversarial visual manipulations, leading to inflated scores in text-image alignment tasks. A new benchmark, FRAME, reveals these biases persist across domains and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate if adversarial visual manipulations can systematically bias LVLM judges in text-image alignment evaluations.

Method: Introduce potential image-induced biases and test LVLM judges using the FRAME benchmark, a fine-grained, multi-domain meta-evaluation tool.

Result: All tested LVLM judges are vulnerable, consistently inflating scores for manipulated images. Combining biases amplifies effects, and prompt-based mitigation fails.

Conclusion: Current LVLM evaluation systems are vulnerable to visual biases, necessitating more robust judges.

Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred
tools for judging text-image alignment, yet their robustness along the visual
modality remains underexplored. This work is the first study to address a key
research question: Can adversarial visual manipulations systematically fool
LVLM judges into assigning unfairly inflated scores? We define potential image
induced biases within the context of T2I evaluation and examine how these
biases affect the evaluations of LVLM judges. Moreover, we introduce a novel,
fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is
deliberately constructed to exhibit diverse score distributions. By introducing
the defined biases into the benchmark, we reveal that all tested LVLM judges
exhibit vulnerability across all domains, consistently inflating scores for
manipulated images. Further analysis reveals that combining multiple biases
amplifies their effects, and pairwise evaluations are similarly susceptible.
Moreover, we observe that visual biases persist under prompt-based mitigation
strategies, highlighting the vulnerability of current LVLM evaluation systems
and underscoring the urgent need for more robust LVLM judges.

</details>


### [74] [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/pdf/2505.15255)
*Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen*

Main category: cs.CL

TL;DR: The paper proposes MentalMAC, a method to improve LLMs' detection of mental manipulation in dialogues, using multi-task anti-curriculum distillation and a new dataset, ReaMent.


<details>
  <summary>Details</summary>
Motivation: Mental manipulation is hard to detect due to its covert nature and lack of annotated datasets, limiting LLM performance.

Method: MentalMAC includes EvoSA (unsupervised data expansion), teacher-model-generated supervision, and progressive knowledge distillation.

Result: The method narrows the gap between student and teacher models, outperforming other LLMs.

Conclusion: MentalMAC and ReaMent dataset advance detection of mental manipulation, with resources to be released.

Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse
that poses serious threats to mental health. Its covert nature and the
complexity of manipulation strategies make it challenging to detect, even for
state-of-the-art large language models (LLMs). This concealment also hinders
the manual collection of large-scale, high-quality annotations essential for
training effective models. Although recent efforts have sought to improve LLM's
performance on this task, progress remains limited due to the scarcity of
real-world annotated datasets. To address these challenges, we propose
MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'
ability to detect mental manipulation in multi-turn dialogue. Our approach
includes: (i) EvoSA, an unsupervised data expansion method based on
evolutionary operations and speech act theory; (ii) teacher-model-generated
multi-task supervision; and (iii) progressive knowledge distillation from
complex to simpler tasks. We then constructed the ReaMent dataset with 5,000
real-world dialogue samples, using a MentalMAC-distilled model to assist human
annotation. Vast experiments demonstrate that our method significantly narrows
the gap between student and teacher models and outperforms competitive LLMs
across key evaluation metrics. All code, datasets, and checkpoints will be
released upon paper acceptance. Warning: This paper contains content that may
be offensive to readers.

</details>


### [75] [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/pdf/2505.15257)
*Weixiang Zhao, Jiahe Guo, Yang Deng, Tongtong Wu, Wenxuan Zhang, Yulin Hu, Xingyu Sui, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu*

Main category: cs.CL

TL;DR: Ablating language-specific representations in LLMs improves multilingual reasoning without extra training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of multilingual reasoning in LLMs, inspired by human cognitive neuroscience.

Method: Causal intervention via language-specific ablation at inference time across 10 LLMs and 11 languages.

Result: Consistent performance boost in multilingual reasoning, with decoupled language and reasoning representations.

Conclusion: Training-free ablation is effective for cross-lingual generalization, offering insights into LLM mechanisms.

Abstract: Multilingual reasoning remains a significant challenge for large language
models (LLMs), with performance disproportionately favoring high-resource
languages. Drawing inspiration from cognitive neuroscience, which suggests that
human reasoning functions largely independently of language processing, we
hypothesize that LLMs similarly encode reasoning and language as separable
components that can be disentangled to enhance multilingual reasoning. To
evaluate this, we perform a causal intervention by ablating language-specific
representations at inference time. Experiments on 10 open-source LLMs spanning
11 typologically diverse languages show that this language-specific ablation
consistently boosts multilingual reasoning performance. Layer-wise analyses
further confirm that language and reasoning representations can be effectively
decoupled throughout the model, yielding improved multilingual reasoning
capabilities, while preserving top-layer language features remains essential
for maintaining linguistic fidelity. Compared to post-training such as
supervised fine-tuning or reinforcement learning, our training-free ablation
achieves comparable or superior results with minimal computational overhead.
These findings shed light on the internal mechanisms underlying multilingual
reasoning in LLMs and suggest a lightweight and interpretable strategy for
improving cross-lingual generalization.

</details>


### [76] [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/pdf/2505.15261)
*Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, Xiaojun Wan*

Main category: cs.CL

TL;DR: AGENT-X is a zero-shot multi-agent framework for AI-generated text detection, leveraging linguistic dimensions and adaptive routing to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on large datasets and threshold tuning, limiting interpretability and zero-shot effectiveness.

Method: AGENT-X uses semantic, stylistic, and structural dimensions evaluated by specialized linguistic agents, integrated by a meta agent with adaptive routing.

Result: AGENT-X outperforms state-of-the-art supervised and zero-shot methods in accuracy, interpretability, and generalization.

Conclusion: AGENT-X addresses limitations of current methods, offering a robust, interpretable, and adaptable solution for AI-generated text detection.

Abstract: Existing AI-generated text detection methods heavily depend on large
annotated datasets and external threshold tuning, restricting interpretability,
adaptability, and zero-shot effectiveness. To address these limitations, we
propose AGENT-X, a zero-shot multi-agent framework informed by classical
rhetoric and systemic functional linguistics. Specifically, we organize
detection guidelines into semantic, stylistic, and structural dimensions, each
independently evaluated by specialized linguistic agents that provide explicit
reasoning and robust calibrated confidence via semantic steering. A meta agent
integrates these assessments through confidence-aware aggregation, enabling
threshold-free, interpretable classification. Additionally, an adaptive
Mixture-of-Agent router dynamically selects guidelines based on inferred
textual characteristics. Experiments on diverse datasets demonstrate that
AGENT-X substantially surpasses state-of-the-art supervised and zero-shot
approaches in accuracy, interpretability, and generalization.

</details>


### [77] [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/pdf/2505.15277)
*Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong-woo Kwak, Dongjin Kang, Jinyoung Yeo*

Main category: cs.CL

TL;DR: Web-Shepherd, a process reward model (PRM), improves web navigation by providing step-level assessments, outperforming GPT-4o in accuracy and cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs as reward models for web navigation are slow and costly, lacking specialized step-level evaluation.

Method: Developed Web-Shepherd PRM using the WebPRM Collection (40K step-level preference pairs) and introduced WebRewardBench for meta-evaluation.

Result: Web-Shepherd outperforms GPT-4o by 30 points in accuracy and improves performance by 10.9 points with 10x less cost.

Conclusion: Web-Shepherd is a scalable, efficient solution for web navigation, with public availability of model, dataset, and code.

Abstract: Web navigation is a unique domain that can automate many repetitive real-life
tasks and is challenging as it requires long-horizon sequential decision making
beyond typical multimodal large language model (MLLM) tasks. Yet, specialized
reward models for web navigation that can be utilized during both training and
test-time have been absent until now. Despite the importance of speed and
cost-effectiveness, prior works have utilized MLLMs as reward models, which
poses significant constraints for real-world deployment. To address this, in
this work, we propose the first process reward model (PRM) called Web-Shepherd
which could assess web navigation trajectories in a step-level. To achieve
this, we first construct the WebPRM Collection, a large-scale dataset with 40K
step-level preference pairs and annotated checklists spanning diverse domains
and difficulty levels. Next, we also introduce the WebRewardBench, the first
meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe
that our Web-Shepherd achieves about 30 points better accuracy compared to
using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by
using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve
10.9 points better performance, in 10 less cost compared to using GPT-4o-mini
as the verifier. Our model, dataset, and code are publicly available at LINK.

</details>


### [78] [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/pdf/2505.15282)
*Yanzhi Tian, Zeming Liu, Zhengyang Liu, Yuhang Guo*

Main category: cs.CL

TL;DR: The paper introduces DebackX, a model for In-Image Machine Translation (IIMT) that handles complex real-world scenarios by separating and processing text and background separately, improving translation quality and visual results.


<details>
  <summary>Details</summary>
Motivation: Previous IIMT research focused on simplified scenarios (e.g., one-line text on plain backgrounds), which are impractical for real-world applications. The paper aims to address this by tackling complex scenarios with real-world backgrounds.

Method: The proposed DebackX model separates the background and text-image, translates the text directly, and fuses the translated text with the background to generate the target image.

Result: Experiments show DebackX improves both translation quality and visual effects compared to previous models.

Conclusion: DebackX effectively addresses the limitations of prior IIMT models in complex scenarios, making IIMT research more practical and valuable.

Abstract: In-Image Machine Translation (IIMT) aims to translate texts within images
from one language to another. Previous research on IIMT was primarily conducted
on simplified scenarios such as images of one-line text with black font in
white backgrounds, which is far from reality and impractical for applications
in the real world. To make IIMT research practically valuable, it is essential
to consider a complex scenario where the text backgrounds are derived from
real-world images. To facilitate research of complex scenario IIMT, we design
an IIMT dataset that includes subtitle text with real-world background. However
previous IIMT models perform inadequately in complex scenarios. To address the
issue, we propose the DebackX model, which separates the background and
text-image from the source image, performs translation on text-image directly,
and fuses the translated text-image with the background, to generate the target
image. Experimental results show that our model achieves improvements in both
translation quality and visual effect.

</details>


### [79] [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/pdf/2505.15291)
*Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeongjeong Kim, Hwanhee Lee*

Main category: cs.CL

TL;DR: LLMs often hallucinate in long responses, especially in later parts. This paper studies positional distribution of hallucinations and explores mitigation methods.


<details>
  <summary>Details</summary>
Motivation: Faithfulness in LLM-generated text is challenged by hallucinations, with little focus on their positional distribution in long outputs.

Method: Investigates hallucination distribution in long responses, using long document summarization as a case study, and explores attention/decoding dynamics.

Result: Hallucinations disproportionately occur in the latter parts of long responses.

Conclusion: Proposes methods to mitigate positional hallucination bias, improving faithfulness in concluding segments.

Abstract: Large Language Models (LLMs) have significantly advanced text generation
capabilities, including tasks like summarization, often producing coherent and
fluent outputs. However, faithfulness to source material remains a significant
challenge due to the generation of hallucinations. While extensive research
focuses on detecting and reducing these inaccuracies, less attention has been
paid to the positional distribution of hallucination within generated text,
particularly in long outputs. In this work, we investigate where hallucinations
occur in LLM-based long response generation, using long document summarization
as a key case study. Focusing on the challenging setting of long context-aware
long response generation, we find a consistent and concerning phenomenon:
hallucinations tend to concentrate disproportionately in the latter parts of
the generated long response. To understand this bias, we explore potential
contributing factors related to the dynamics of attention and decoding over
long sequences. Furthermore, we investigate methods to mitigate this positional
hallucination, aiming to improve faithfulness specifically in the concluding
segments of long outputs.

</details>


### [80] [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/pdf/2505.15297)
*Xintong Wang, Yixiao Liu, Jingheng Pan, Liang Ding, Longyue Wang, Chris Biemann*

Main category: cs.CL

TL;DR: ToxiRewriteCN is a Chinese detoxification dataset designed to preserve sentiment while rewriting toxic content, evaluated across 17 LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detoxifying offensive language in Chinese without distorting emotional tone or intent, especially in implicit toxicity cases like emojis or homophones.

Method: Creation of ToxiRewriteCN, a dataset with 1,556 annotated triplets (toxic sentence, non-toxic rewrite, toxic spans), covering five scenarios. Evaluation of 17 LLMs across detoxification, fluency, content preservation, and sentiment.

Result: Commercial and MoE models perform best, but all struggle with subtle or context-heavy toxicity like emojis and homophones.

Conclusion: ToxiRewriteCN supports future research on sentiment-aware detoxification in Chinese, highlighting the need for better balance between safety and emotional fidelity.

Abstract: Detoxifying offensive language while preserving the speaker's original intent
is a challenging yet critical goal for improving the quality of online
interactions. Although large language models (LLMs) show promise in rewriting
toxic content, they often default to overly polite rewrites, distorting the
emotional tone and communicative intent. This problem is especially acute in
Chinese, where toxicity often arises implicitly through emojis, homophones, or
discourse context. We present ToxiRewriteCN, the first Chinese detoxification
dataset explicitly designed to preserve sentiment polarity. The dataset
comprises 1,556 carefully annotated triplets, each containing a toxic sentence,
a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five
real-world scenarios: standard expressions, emoji-induced and homophonic
toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs,
including commercial and open-source models with variant architectures, across
four dimensions: detoxification accuracy, fluency, content preservation, and
sentiment polarity. Results show that while commercial and MoE models perform
best overall, all models struggle to balance safety with emotional fidelity in
more subtle or context-heavy settings such as emoji, homophone, and
dialogue-based inputs. We release ToxiRewriteCN to support future research on
controllable, sentiment-aware detoxification for Chinese.

</details>


### [81] [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/pdf/2505.15299)
*Maodong Li, Longyin Zhang, Fang Kong*

Main category: cs.CL

TL;DR: The paper introduces a Dual-Perspective Keyword-Guided (DPKG) framework for multi-hop question generation, leveraging question and document keywords to enhance accuracy and intent capture.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-hop question generation inadequately utilize keywords and fail to distinguish between question-specific and document-specific keywords, limiting effectiveness.

Method: The DPKG framework uses an expanded transformer encoder and two answer-aware decoders to integrate dual-perspective keywords (question and document keywords) into the generation process.

Result: Experiments show the DPKG framework's promising performance, validating its effectiveness in multi-hop question generation.

Conclusion: The DPKG framework successfully addresses the limitations of existing methods, demonstrating significant value in the MQG task.

Abstract: Multi-hop question generation (MQG) aims to generate questions that require
synthesizing multiple information snippets from documents to derive target
answers. The primary challenge lies in effectively pinpointing crucial
information snippets related to question-answer (QA) pairs, typically relying
on keywords. However, existing works fail to fully utilize the guiding
potential of keywords and neglect to differentiate the distinct roles of
question-specific and document-specific keywords. To address this, we define
dual-perspective keywords (i.e., question and document keywords) and propose a
Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates
keywords into the multi-hop question generation process. We argue that question
keywords capture the questioner's intent, whereas document keywords reflect the
content related to the QA pair. Functionally, question and document keywords
work together to pinpoint essential information snippets in the document, with
question keywords required to appear in the generated question. The DPKG
framework consists of an expanded transformer encoder and two answer-aware
transformer decoders for keyword and question generation, respectively.
Extensive experiments demonstrate the effectiveness of our work, showcasing its
promising performance and underscoring its significant value in the MQG task.

</details>


### [82] [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/pdf/2505.15337)
*Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang*

Main category: cs.CL

TL;DR: CoPA is a training-free method using off-the-shelf LLMs to deceive text detectors by contrasting human-like and machine-like word distributions.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing paraphrase attacks, which require heavy resources and fail against advanced detectors.

Method: CoPA crafts instructions for LLMs to generate human-like texts and contrasts these with a machine-like distribution to remove detectable patterns.

Result: CoPA effectively fools text detectors across various scenarios, outperforming existing methods.

Conclusion: CoPA offers a resource-efficient and superior approach to bypassing text detectors.

Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has
driven the development of detectors to identify LLM-generated texts. To bypass
these detectors, paraphrase attacks have emerged to purposely rewrite these
texts to evade detection. Despite the success, existing methods require
substantial data and computational budgets to train a specialized paraphraser,
and their attack efficacy greatly reduces when faced with advanced detection
algorithms. To address this, we propose \textbf{Co}ntrastive
\textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that
effectively deceives text detectors using off-the-shelf LLMs. The first step is
to carefully craft instructions that encourage LLMs to produce more human-like
texts. Nonetheless, we observe that the inherent statistical biases of LLMs can
still result in some generated texts carrying certain machine-like attributes
that can be captured by detectors. To overcome this, CoPA constructs an
auxiliary machine-like word distribution as a contrast to the human-like
distribution generated by the LLM. By subtracting the machine-like patterns
from the human-like distribution during the decoding process, CoPA is able to
produce sentences that are less discernible by text detectors. Our theoretical
analysis suggests the superiority of the proposed attack. Extensive experiments
validate the effectiveness of CoPA in fooling text detectors across various
scenarios.

</details>


### [83] [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/pdf/2505.15316)
*Xin Bai, Guanyi Chen, Tingting He, Chenlian Zhou, Yu Liu*

Main category: cs.CL

TL;DR: The paper redefines Emotional Support Conversations (ESC) by addressing the oversight of multiple strategies in single turns, introduces new models, and finds LLMs outperform humans and supervised models.


<details>
  <summary>Details</summary>
Motivation: Existing ESC task definitions oversimplify supportive responses by treating them as single strategy-utterance pairs, ignoring the common use of multiple strategies in a single turn.

Method: The authors analyze the ESConv dataset, redefine the ESC task to include sequences of strategy-utterance pairs, and propose supervised deep learning and large language models (LLMs) for this task.

Result: Under the redefined task, LLMs outperform both supervised models and human supporters, showing capabilities like asking questions and providing suggestions.

Conclusion: The study highlights the need for a refined ESC task definition and demonstrates the superior performance of LLMs in providing holistic emotional support.

Abstract: Emotional Support Conversations (ESC) are crucial for providing empathy,
validation, and actionable guidance to individuals in distress. However,
existing definitions of the ESC task oversimplify the structure of supportive
responses, typically modelling them as single strategy-utterance pairs. Through
a detailed corpus analysis of the ESConv dataset, we identify a common yet
previously overlooked phenomenon: emotional supporters often employ multiple
strategies consecutively within a single turn. We formally redefine the ESC
task to account for this, proposing a revised formulation that requires
generating the full sequence of strategy-utterance pairs given a dialogue
history. To facilitate this refined task, we introduce several modelling
approaches, including supervised deep learning models and large language
models. Our experiments show that, under this redefined task, state-of-the-art
LLMs outperform both supervised models and human supporters. Notably, contrary
to some earlier findings, we observe that LLMs frequently ask questions and
provide suggestions, demonstrating more holistic support capabilities.

</details>


### [84] [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/pdf/2505.15323)
*Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara*

Main category: cs.CL

TL;DR: Prefilling attack improves FTP-based MCQA evaluation by prepending a structured prefix, enhancing accuracy and reliability without model changes.


<details>
  <summary>Details</summary>
Motivation: FTP-based evaluation in MCQA is fragile due to misalignment and misinterpretation, undermining reliability.

Method: Propose the prefilling attack: prepend a natural-language prefix to model output to steer clean responses.

Result: Prefilling improves accuracy, calibration, and consistency across LLMs and benchmarks, often matching open-ended generation performance.

Conclusion: Prefilling is a simple, robust, and efficient method to enhance FTP-based evaluation in MCQA.

Abstract: Large Language Models (LLMs) are increasingly evaluated on multiple-choice
question answering (MCQA) tasks using *first-token probability* (FTP), which
selects the answer option whose initial token has the highest likelihood. While
efficient, FTP can be fragile: models may assign high probability to unrelated
tokens (*misalignment*) or use a valid token merely as part of a generic
preamble rather than as a clear answer choice (*misinterpretation*),
undermining the reliability of symbolic evaluation. We propose a simple
solution: the *prefilling attack*, a structured natural-language prefix (e.g.,
"*The correct option is:*") prepended to the model output. Originally explored
in AI safety, we repurpose prefilling to steer the model to respond with a
clean, valid option, without modifying its parameters. Empirically, the FTP
with prefilling strategy substantially improves accuracy, calibration, and
output consistency across a broad set of LLMs and MCQA benchmarks. It
outperforms standard FTP and often matches the performance of open-ended
generation approaches that require full decoding and external classifiers,
while being significantly more efficient. Our findings suggest that prefilling
is a simple, robust, and low-cost method to enhance the reliability of
FTP-based evaluation in multiple-choice settings.

</details>


### [85] [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/pdf/2505.15347)
*Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu*

Main category: cs.CL

TL;DR: FlowKV introduces a multi-turn isolation mechanism for KV Cache management, preventing re-compression of older context and improving performance in multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: The linear growth of KV Cache in LLMs causes computational bottlenecks, and existing eviction strategies degrade performance by losing context.

Method: FlowKV uses a multi-turn isolation mechanism, preserving past compressed KV Cache and applying compression only to new KV pairs.

Result: FlowKV improves instruction-following accuracy and user preference retention from 10.90% to 75.40%, especially in later turns.

Conclusion: FlowKV effectively mitigates catastrophic forgetting and outperforms baseline strategies in multi-turn conversational applications.

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-turn
conversational applications, where the management of the Key-Value (KV) Cache
presents a significant bottleneck. The linear growth of the KV Cache with
dialogue history imposes substantial computational costs, and existing eviction
strategies often degrade performance by repeatedly compressing early
conversational context, leading to information loss and context forgetting.
This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism}
for KV Cache management, which can be applied to any KV Cache compression
method without training. FlowKV's core innovation is a multi-turn isolation
mechanism that preserves the accumulated compressed KV cache from past turns.
Compression is then strategically applied only to the newly generated KV pairs
of the latest completed turn, effectively preventing the re-compression of
older context and thereby mitigating catastrophic forgetting. Our results
demonstrate that FlowKV consistently and significantly outperforms baseline
strategies in maintaining instruction-following accuracy and user preference
retention from 10.90\% to 75.40\%, particularly in later conversational turns.

</details>


### [86] [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/pdf/2505.15386)
*Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R., Fung, Xinlei He*

Main category: cs.CL

TL;DR: RePPL improves hallucination detection in LLMs by recalibrating uncertainty measurement with token-level scores, achieving high performance and explainability.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder trustworthiness, and existing methods lack explanations for why they occur.

Method: RePPL recalibrates uncertainty by analyzing semantic propagation and language generation, providing token-level scores.

Result: Achieves best detection performance (average AUC 0.833) and explains hallucinations via token-level uncertainty.

Conclusion: RePPL effectively detects and explains hallucinations, revealing chaotic patterns and potential applications.

Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain
a vital obstacle to their trustworthy use. While previous works improved the
capability of hallucination detection by measuring uncertainty, they all lack
the ability to explain the provenance behind why hallucinations occur, i.e.,
which part of the inputs tends to trigger hallucinations. Recent works on the
prompt attack indicate that uncertainty exists in semantic propagation, where
attention mechanisms gradually fuse local token information into high-level
semantics across layers. Meanwhile, uncertainty also emerges in language
generation, due to its probability-based selection of high-level semantics for
sampled generations. Based on that, we propose RePPL to recalibrate uncertainty
measurement by these two aspects, which dispatches explainable uncertainty
scores to each token and aggregates in Perplexity-style Log-Average form as
total score. Experiments show that our method achieves the best comprehensive
detection performance across various QA datasets on advanced models (average
AUC of 0.833), and our method is capable of producing token-level uncertainty
scores as explanations for the hallucination. Leveraging these scores, we
preliminarily find the chaotic pattern of hallucination and showcase its
promising usage.

</details>


### [87] [The Super Emotion Dataset](https://arxiv.org/pdf/2505.15348)
*Enric Junqué de Fortuny*

Main category: cs.CL

TL;DR: The paper introduces the Super Emotion Dataset, a standardized, large-scale resource for emotion classification in NLP, addressing inconsistencies in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing emotion classification datasets lack standardization, consistency, or scale, hindering cross-domain research.

Method: The Super Emotion Dataset harmonizes diverse text sources using Shaver's empirically validated emotion taxonomy.

Result: The dataset provides a unified framework for consistent cross-domain emotion recognition research.

Conclusion: The Super Emotion Dataset fills a critical gap in NLP emotion classification by offering a standardized, psychologically grounded resource.

Abstract: Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.

</details>


### [88] [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/pdf/2505.15353)
*Ryo Kishino, Yusuke Takase, Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: The paper evaluates KL divergence between language models using log-likelihood vectors, revealing spiral and thread-like structures in model trajectories during pretraining and across layers.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of KL divergence as a metric for comparing language models with different architectures and training stages.

Method: Systematic evaluation of KL divergence using publicly available language models, comparing pretraining checkpoints, fine-tuned vs. base models, and layers via the logit lens.

Result: Model trajectories exhibit spiral structures during pretraining and thread-like progressions across layers, with log-likelihood space trajectories more constrained than weight space.

Conclusion: KL divergence reveals structured trajectories in language models, providing insights into their behavior during training and across layers.

Abstract: A recently proposed method enables efficient estimation of the KL divergence
between language models, including models with different architectures, by
assigning coordinates based on log-likelihood vectors. To better understand the
behavior of this metric, we systematically evaluate KL divergence across a wide
range of conditions using publicly available language models. Our analysis
covers comparisons between pretraining checkpoints, fine-tuned and base models,
and layers via the logit lens. We find that trajectories of language models, as
measured by KL divergence, exhibit a spiral structure during pretraining and
thread-like progressions across layers. Furthermore, we show that, in terms of
diffusion exponents, model trajectories in the log-likelihood space are more
constrained than those in weight space.

</details>


### [89] [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/pdf/2505.15356)
*Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weinan Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu*

Main category: cs.CL

TL;DR: NL-DEBUGGING uses natural language reasoning to improve code debugging, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional code-level debugging falls short for complex errors, prompting exploration of natural language reasoning's role in debugging.

Method: Introduces NL-DEBUGGING, a framework using natural language as an intermediate representation for debugging.

Result: NL-DEBUGGING outperforms traditional methods and allows broader modifications via execution feedback.

Conclusion: Natural language reasoning can advance automated debugging and tackle complex programming challenges.

Abstract: Debugging is a critical aspect of LLM's coding ability. Early debugging
efforts primarily focused on code-level analysis, which often falls short when
addressing complex programming errors that require a deeper understanding of
algorithmic logic. Recent advancements in large language models (LLMs) have
shifted attention toward leveraging natural language reasoning to enhance
code-related tasks. However, two fundamental questions remain unanswered: What
type of natural language format is most effective for debugging tasks? And what
specific benefits does natural language reasoning bring to the debugging
process? In this paper, we introduce NL-DEBUGGING, a novel framework that
employs natural language as an intermediate representation to improve code
debugging. By debugging at a natural language level, we demonstrate that
NL-DEBUGGING outperforms traditional debugging methods and enables a broader
modification space through direct refinement guided by execution feedback. Our
findings highlight the potential of natural language reasoning to advance
automated code debugging and address complex programming challenges.

</details>


### [90] [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/pdf/2505.15427)
*Zhiwen Li, Die Chen, Mingyuan Fan, Cen Chen, Yaliang Li, Yanhao Wang, Wenmeng Zhou*

Main category: cs.CL

TL;DR: A novel self-discovery approach is proposed to restrict text embeddings in a safe region, reducing NSFW content and social bias in diffusion models without heavily impacting normal outputs.


<details>
  <summary>Details</summary>
Motivation: Address concerns about diffusion models generating NSFW content and exhibiting social biases, hindering real-world applications.

Method: Identify a semantic direction vector in embedding space to steer prompts toward safe regions, using LoRA for initialization to minimize performance impact.

Result: Effectively reduces NSFW content and mitigates social bias compared to existing methods.

Conclusion: The method enhances model robustness and can integrate with existing techniques for improved social responsibility.

Abstract: The remarkable ability of diffusion models to generate high-fidelity images
has led to their widespread adoption. However, concerns have also arisen
regarding their potential to produce Not Safe for Work (NSFW) content and
exhibit social biases, hindering their practical use in real-world
applications. In response to this challenge, prior work has focused on
employing security filters to identify and exclude toxic text, or
alternatively, fine-tuning pre-trained diffusion models to erase sensitive
concepts. Unfortunately, existing methods struggle to achieve satisfactory
performance in the sense that they can have a significant impact on the normal
model output while still failing to prevent the generation of harmful content
in some cases. In this paper, we propose a novel self-discovery approach to
identifying a semantic direction vector in the embedding space to restrict text
embedding within a safe region. Our method circumvents the need for correcting
individual words within the input text and steers the entire text prompt
towards a safe region in the embedding space, thereby enhancing model
robustness against all possibly unsafe prompts. In addition, we employ Low-Rank
Adaptation (LoRA) for semantic direction vector initialization to reduce the
impact on the model performance for other semantics. Furthermore, our method
can also be integrated with existing methods to improve their social
responsibility. Extensive experiments on benchmark datasets demonstrate that
our method can effectively reduce NSFW content and mitigate social bias
generated by diffusion models compared to several state-of-the-art baselines.

</details>


### [91] [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/pdf/2505.15372)
*Peng Wang, Ruihan Tao, Qiguang Chen, Mengkang Hu, Libo Qin*

Main category: cs.CL

TL;DR: The paper introduces X-WebAgentBench, a multilingual agent benchmark for evaluating LLM-based agents in interactive web environments, highlighting gaps in current research focused on English scenarios.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agent research is largely limited to English, neglecting the need for multilingual agent services despite the existence of over 7,000 languages.

Method: The authors propose X-WebAgentBench to assess planning and interaction performance of language agents across multiple languages, including evaluating LLMs and cross-lingual alignment methods.

Result: Even advanced models like GPT-4o, combined with cross-lingual techniques, underperform in multilingual agent scenarios.

Conclusion: X-WebAgentBench aims to advance global agent intelligence by addressing the lack of multilingual benchmarks, encouraging further research in this area.

Abstract: Recently, large language model (LLM)-based agents have achieved significant
success in interactive environments, attracting significant academic and
industrial attention. Despite these advancements, current research
predominantly focuses on English scenarios. In reality, there are over 7,000
languages worldwide, all of which demand access to comparable agentic services.
Nevertheless, the development of language agents remains inadequate for meeting
the diverse requirements of multilingual agentic applications. To fill this
gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an
interactive web environment, which evaluates the planning and interaction
performance of language agents across multiple languages, thereby contributing
to the advancement of global agent intelligence. Additionally, we assess the
performance of various LLMs and cross-lingual alignment methods, examining
their effectiveness in enhancing agents. Our findings reveal that even advanced
models like GPT-4o, when combined with cross-lingual techniques, fail to
achieve satisfactory results. We hope that X-WebAgentBench can serve as a
valuable benchmark for multilingual agent scenario in real-world applications.

</details>


### [92] [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/pdf/2505.15389)
*DongGeon Lee, Joonwon Jang, Jihae Jeong, Hwanjo Yu*

Main category: cs.CL

TL;DR: The study evaluates the safety risks of vision-language models (VLMs) using real meme images, revealing heightened vulnerability to harmful prompts compared to synthetic images.


<details>
  <summary>Details</summary>
Motivation: To assess the safety of VLMs when exposed to real-world meme images, which are more representative of user-shared content than artificial images.

Method: Introduces MemeSafetyBench, a benchmark of 50,430 real meme images paired with harmful/benign instructions, and evaluates VLMs using a safety taxonomy and LLM-based instruction generation.

Result: VLMs are more vulnerable to harmful meme-based prompts, with increased harmful responses and reduced refusals compared to text-only inputs. Multi-turn interactions offer partial mitigation.

Conclusion: The findings emphasize the need for ecologically valid evaluations and improved safety mechanisms for VLMs.

Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet
most evaluations rely on artificial images. This study asks: How safe are
current VLMs when confronted with meme images that ordinary users share? To
investigate this question, we introduce MemeSafetyBench, a 50,430-instance
benchmark pairing real meme images with both harmful and benign instructions.
Using a comprehensive safety taxonomy and LLM-based instruction generation, we
assess multiple VLMs across single and multi-turn interactions. We investigate
how real-world memes influence harmful outputs, the mitigating effects of
conversational context, and the relationship between model scale and safety
metrics. Our findings demonstrate that VLMs show greater vulnerability to
meme-based harmful prompts than to synthetic or typographic images. Memes
significantly increase harmful responses and decrease refusals compared to
text-only inputs. Though multi-turn interactions provide partial mitigation,
elevated vulnerability persists. These results highlight the need for
ecologically valid evaluations and stronger safety mechanisms.

</details>


### [93] [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/pdf/2505.15392)
*Yiming Huang, Biquan Bie, Zuqiu Na, Weilin Ruan, Songxin Lei, Yutao Yue, Xinlei He*

Main category: cs.CL

TL;DR: The paper investigates the anchoring effect in LLMs, introduces a dataset (SynAnchors), and finds that LLMs exhibit this bias, which reasoning can partially mitigate.


<details>
  <summary>Details</summary>
Motivation: To explore cognitive biases like anchoring in LLMs, understand their mechanisms, and propose mitigation strategies.

Method: Introduces SynAnchors dataset, uses refined metrics to benchmark LLMs, and analyzes anchoring bias.

Result: LLMs commonly exhibit anchoring bias, not eliminated by conventional methods; reasoning helps mitigate it.

Conclusion: LLM evaluations should focus on cognitive-bias-aware trustworthy assessments rather than standard benchmarks.

Abstract: The rise of Large Language Models (LLMs) like ChatGPT has advanced natural
language processing, yet concerns about cognitive biases are growing. In this
paper, we investigate the anchoring effect, a cognitive bias where the mind
relies heavily on the first information as anchors to make affected judgments.
We explore whether LLMs are affected by anchoring, the underlying mechanisms,
and potential mitigation strategies. To facilitate studies at scale on the
anchoring effect, we introduce a new dataset, SynAnchors. Combining refined
evaluation metrics, we benchmark current widely used LLMs. Our findings show
that LLMs' anchoring bias exists commonly with shallow-layer acting and is not
eliminated by conventional strategies, while reasoning can offer some
mitigation. This recontextualization via cognitive psychology urges that LLM
evaluations focus not on standard benchmarks or over-optimized robustness
tests, but on cognitive-bias-aware trustworthy evaluation.

</details>


### [94] [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/pdf/2505.15444)
*Yutao Zhu, Jiajie Jin, Hongjin Qian, Zheng Liu, Zhicheng Dou, Ji-Rong Wen*

Main category: cs.CL

TL;DR: RoleRAG is a unified RAG framework using role-specific token optimization for multi-task efficiency, integrating six modules into a single LLM instance.


<details>
  <summary>Details</summary>
Motivation: Existing RAG optimizations lack a unified framework, making integration of sub-tasks like query understanding and retrieval refinement challenging.

Method: RoleRAG uses task-specific role tokens to dynamically activate modules within one LLM, supported by a query graph for query decomposition.

Result: Tests on five datasets show RoleRAG's effectiveness, generalizability, and flexibility.

Conclusion: RoleRAG successfully unifies RAG sub-tasks, reducing resource use while maintaining performance.

Abstract: Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.

</details>


### [95] [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/pdf/2505.15404)
*Zhexin Zhang, Xian Qi Loye, Victor Shea-Jay Huang, Junxiao Yang, Qi Zhu, Shiyao Cui, Fei Mi, Lifeng Shang, Yingkang Wang, Hongning Wang, Minlie Huang*

Main category: cs.CL

TL;DR: The paper explores enhancing safety in Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), identifying key failure patterns and showing that simpler reasoning processes can match safety performance.


<details>
  <summary>Details</summary>
Motivation: LRMs excel in reasoning tasks but may degrade in safety. The study aims to improve LRM safety via SFT.

Method: Empirical study using SFT, analyzing failure patterns, and testing simpler reasoning processes.

Result: Addressing key issues improves safety; short/template-based reasoning matches complex reasoning. Mixing math reasoning data balances safety and over-refusal.

Conclusion: Simpler reasoning can enhance safety, and mixing math data helps balance performance. The study provides insights for safer LRMs.

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success on
reasoning-intensive tasks such as mathematics and programming. However, their
enhanced reasoning capabilities do not necessarily translate to improved safety
performance-and in some cases, may even degrade it. This raises an important
research question: how can we enhance the safety of LRMs? In this paper, we
present a comprehensive empirical study on how to enhance the safety of LRMs
through Supervised Fine-Tuning (SFT). Our investigation begins with an
unexpected observation: directly distilling safe responses from DeepSeek-R1
fails to significantly enhance safety. We analyze this phenomenon and identify
three key failure patterns that contribute to it. We then demonstrate that
explicitly addressing these issues during the data distillation process can
lead to substantial safety improvements. Next, we explore whether a long and
complex reasoning process is necessary for achieving safety. Interestingly, we
find that simply using short or template-based reasoning process can attain
comparable safety performance-and are significantly easier for models to learn
than more intricate reasoning chains. These findings prompt a deeper reflection
on the role of reasoning in ensuring safety. Finally, we find that mixing math
reasoning data during safety fine-tuning is helpful to balance safety and
over-refusal. Overall, we hope our empirical study could provide a more
holistic picture on enhancing the safety of LRMs. The code and data used in our
experiments are released in https://github.com/thu-coai/LRM-Safety-Study.

</details>


### [96] [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/pdf/2505.15467)
*Yukun Zhao, Lingyong Yan, Zhenyang Li, Shuaiqiang Wang, Zhumin Chen, Zhaochun Ren, Dawei Yin*

Main category: cs.CL

TL;DR: Proposes Joint Flashback Adaptation to mitigate catastrophic forgetting in large language models by using flashbacks (old task prompts) and interpolating latent tasks for smooth adaptation.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing methods (experience replay, optimization constraints) in incremental learning for large language models.

Method: Introduces flashbacks (old task prompts) and constrains model output deviations. Interpolates latent tasks between flashbacks and new tasks for joint learning.

Result: Demonstrates superior performance in generalization on new tasks and reduced forgetting in old tasks across 1000+ tasks.

Conclusion: Joint Flashback Adaptation is effective for incremental learning in large language models, requiring minimal flashbacks and no replay data.

Abstract: Large language models have achieved remarkable success in various tasks.
However, it is challenging for them to learn new tasks incrementally due to
catastrophic forgetting. Existing approaches rely on experience replay,
optimization constraints, or task differentiation, which encounter strict
limitations in real-world scenarios. To address these issues, we propose Joint
Flashback Adaptation. We first introduce flashbacks -- a limited number of
prompts from old tasks -- when adapting to new tasks and constrain the
deviations of the model outputs compared to the original one. We then
interpolate latent tasks between flashbacks and new tasks to enable jointly
learning relevant latent tasks, new tasks, and flashbacks, alleviating data
sparsity in flashbacks and facilitating knowledge sharing for smooth
adaptation. Our method requires only a limited number of flashbacks without
access to the replay data and is task-agnostic. We conduct extensive
experiments on state-of-the-art large language models across 1000+
instruction-following tasks, arithmetic reasoning tasks, and general reasoning
tasks. The results demonstrate the superior performance of our method in
improving generalization on new tasks and reducing forgetting in old tasks.

</details>


### [97] [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/pdf/2505.15422)
*Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney*

Main category: cs.CL

TL;DR: A systematic review of authorship analysis focusing on Author Attribution and Verification, covering methodologies from 2015-2024, highlighting trends, challenges, and future research gaps.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of SOTA methods, their evolution, and limitations in authorship analysis, aiding researchers in developing more reliable systems.

Method: Systematic literature review of methodologies, including traditional ML, DL models, and LLMs, analyzing feature extraction techniques, datasets, and challenges.

Result: Identified key research gaps in low-resource languages, multilingual adaptation, cross-domain generalization, and AI-generated text detection.

Conclusion: The review serves as a guide for future research, aiming to improve authorship analysis systems across diverse textual domains.

Abstract: Authorship analysis plays an important role in diverse domains, including
forensic linguistics, academia, cybersecurity, and digital content
authentication. This paper presents a systematic literature review on two key
sub-tasks of authorship analysis; Author Attribution and Author Verification.
The review explores SOTA methodologies, ranging from traditional ML approaches
to DL models and LLMs, highlighting their evolution, strengths, and
limitations, based on studies conducted from 2015 to 2024. Key contributions
include a comprehensive analysis of methods, techniques, their corresponding
feature extraction techniques, datasets used, and emerging challenges in
authorship analysis. The study highlights critical research gaps, particularly
in low-resource language processing, multilingual adaptation, cross-domain
generalization, and AI-generated text detection. This review aims to help
researchers by giving an overview of the latest trends and challenges in
authorship analysis. It also points out possible areas for future study. The
goal is to support the development of better, more reliable, and accurate
authorship analysis system in diverse textual domain.

</details>


### [98] [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/pdf/2505.15424)
*Yan-Shuo Liang, Wu-Jun Li*

Main category: cs.CL

TL;DR: GainLoRA introduces gated integration of LoRA branches for continual learning in LMs, mitigating forgetting and improving performance.


<details>
  <summary>Details</summary>
Motivation: Address forgetting in continual learning of LMs using LoRA by equal contribution of old and new branches.

Method: Expands new LoRA branches per task and uses gating modules to minimize new branch impact on old tasks.

Result: Outperforms state-of-the-art methods on CL benchmarks.

Conclusion: GainLoRA effectively mitigates forgetting and enhances LM performance in continual learning.

Abstract: Continual learning (CL), which requires the model to learn multiple tasks
sequentially, is crucial for language models (LMs). Recently, low-rank
adaptation (LoRA), one of the most representative parameter-efficient
fine-tuning (PEFT) methods, has gained increasing attention in CL of LMs.
However, most existing CL methods based on LoRA typically expand a new LoRA
branch to learn each new task and force the new and old LoRA branches to
contribute equally to old tasks, potentially leading to forgetting. In this
work, we propose a new method, called gated integration of low-rank adaptation
(GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task
and introduces gating modules to integrate the new and old LoRA branches.
Furthermore, GainLoRA leverages the new gating module to minimize the
contribution from the new LoRA branch to old tasks, effectively mitigating
forgetting and improving the model's overall performance. Experimental results
on CL benchmarks demonstrate that GainLoRA outperforms existing
state-of-the-art methods.

</details>


### [99] [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/pdf/2505.15475)
*Zhanyue Qin, Yue Ding, Deyuan Liu, Qingbin Liu, Junxian Cai, Xi Chen, Zhiying Tu, Dianhui Chu, Cuiyun Gao, Dianbo Sui*

Main category: cs.CL

TL;DR: The paper introduces datasets (GenBiasEval, GenHintEval) and metrics (AFGB-Score, UB-Score) to quantify gender bias in LLMs, along with the LFTF algorithm to mitigate it effectively.


<details>
  <summary>Details</summary>
Motivation: Addressing social biases, especially gender bias, in LLMs due to biased training data.

Method: Proposes datasets and metrics for evaluation, and the LFTF algorithm to fine-tune biased blocks in LLMs.

Result: LFTF significantly reduces gender bias while preserving LLM performance.

Conclusion: The approach effectively measures and mitigates gender bias in LLMs, enhancing fairness.

Abstract: Nowadays, Large Language Models (LLMs) have attracted widespread attention
due to their powerful performance. However, due to the unavoidable exposure to
socially biased data during training, LLMs tend to exhibit social biases,
particularly gender bias. To better explore and quantifying the degree of
gender bias in LLMs, we propose a pair of datasets named GenBiasEval and
GenHintEval, respectively. The GenBiasEval is responsible for evaluating the
degree of gender bias in LLMs, accompanied by an evaluation metric named
AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is
used to assess whether LLMs can provide responses consistent with prompts that
contain gender hints, along with the accompanying evaluation metric UB-Score
(UnBias Score). Besides, in order to mitigate gender bias in LLMs more
effectively, we present the LFTF (Locating First and Then Fine-Tuning)
algorithm.The algorithm first ranks specific LLM blocks by their relevance to
gender bias in descending order using a metric called BMI (Block Mitigating
Importance Score). Based on this ranking, the block most strongly associated
with gender bias is then fine-tuned using a carefully designed loss function.
Numerous experiments have shown that our proposed LFTF algorithm can
significantly mitigate gender bias in LLMs while maintaining their general
capabilities.

</details>


### [100] [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/pdf/2505.15426)
*Aleksandra Tomaszewska, Dariusz Czerski, Bartosz Żuk, Maciej Ogrodniczuk*

Main category: cs.CL

TL;DR: NeoN is a tool for detecting Polish neologisms using a multi-layered pipeline with linguistic filters, LLM-driven precision, and automated analysis, reducing manual effort while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional dictionary-based methods for detecting neologisms require extensive manual review, which is inefficient. NeoN aims to automate and streamline this process for Polish language.

Method: NeoN combines reference corpora, linguistic filters, an LLM-driven filter, and RSS monitoring. It uses context-aware lemmatization, frequency analysis, and orthographic normalization to identify and consolidate neologisms.

Result: Evaluations show NeoN maintains high accuracy and significantly reduces manual effort, providing an efficient solution for tracking lexical innovation in Polish.

Conclusion: NeoN offers an accessible and automated tool for detecting and analyzing Polish neologisms, enhancing efficiency and accuracy in linguistic research.

Abstract: NeoN, a tool for detecting and analyzing Polish neologisms. Unlike
traditional dictionary-based methods requiring extensive manual review, NeoN
combines reference corpora, Polish-specific linguistic filters, an LLM-driven
precision-boosting filter, and daily RSS monitoring in a multi-layered
pipeline. The system uses context-aware lemmatization, frequency analysis, and
orthographic normalization to extract candidate neologisms while consolidating
inflectional variants. Researchers can verify candidates through an intuitive
interface with visualizations and filtering controls. An integrated LLM module
automatically generates definitions and categorizes neologisms by domain and
sentiment. Evaluations show NeoN maintains high accuracy while significantly
reducing manual effort, providing an accessible solution for tracking lexical
innovation in Polish.

</details>


### [101] [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/pdf/2505.15501)
*Federico Ranaldi, Andrea Zugarini, Leonardo Ranaldi, Fabio Massimo Zanzotto*

Main category: cs.CL

TL;DR: The paper introduces 'protoknowledge' to measure how LLMs internalize and use token sequences from Knowledge Graphs, categorizing it into lexical, hierarchical, and topological forms. It evaluates protoknowledge via Knowledge Activation Tasks (KATs) and its impact on Text-to-SPARQL performance.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs leverage memorized token sequences as reusable knowledge and explore their generalization capabilities.

Method: Introduces protoknowledge and KATs to measure knowledge activation, using a framework to analyze alignment between model predictions and protoknowledge activation.

Result: Provides insights into semantic bias and the impact of protoknowledge on Text-to-SPARQL performance under varying prompting strategies.

Conclusion: The methodology offers a tool to explore Semantic-Level Data Contamination and aids Closed-Pretraining models.

Abstract: We introduce the concept of protoknowledge to formalize and measure how
sequences of tokens encoding Knowledge Graphs are internalized during
pretraining and utilized at inference time by Large Language Models (LLMs).
Indeed, LLMs have demonstrated the ability to memorize vast amounts of token
sequences during pretraining, and a central open question is how they leverage
this memorization as reusable knowledge through generalization. We then
categorize protoknowledge into lexical, hierarchical, and topological forms,
varying on the type of knowledge that needs to be activated. We measure
protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general
properties such as semantic bias. We then investigate the impact of
protoknowledge on Text-to-SPARQL performance by varying prompting strategies
depending on input conditions. To this end, we adopt a novel analysis framework
that assesses whether model predictions align with the successful activation of
the relevant protoknowledge for each query. This methodology provides a
practical tool to explore Semantic-Level Data Contamination and serves as an
effective strategy for Closed-Pretraining models.

</details>


### [102] [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/pdf/2505.15428)
*Momose Oyama, Ryo Kishino, Hiroaki Yamagiwa, Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: A resampling method reduces computational cost for constructing language model maps by selecting important texts based on variance of log-likelihoods, achieving comparable accuracy with fewer texts.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of embedding diverse language models into a common space for comparison via KL divergence, due to reliance on log-likelihoods over large text sets.

Method: Proposes a resampling method that selects texts with weights proportional to the variance of log-likelihoods across models, reducing the number of required texts.

Result: Achieves comparable performance to uniform sampling with about half as many texts and enables efficient incorporation of new models.

Conclusion: The method enables scalable and efficient construction of language model maps.

Abstract: We address the computational cost of constructing a model map, which embeds
diverse language models into a common space for comparison via KL divergence.
The map relies on log-likelihoods over a large text set, making the cost
proportional to the number of texts. To reduce this cost, we propose a
resampling method that selects important texts with weights proportional to the
variance of log-likelihoods across models for each text. Our method
significantly reduces the number of required texts while preserving the
accuracy of KL divergence estimates. Experiments show that it achieves
comparable performance to uniform sampling with about half as many texts, and
also facilitates efficient incorporation of new models into an existing map.
These results enable scalable and efficient construction of language model
maps.

</details>


### [103] [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/pdf/2505.15431)
*Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu*

Main category: cs.CL

TL;DR: Hunyuan-TurboS is a hybrid Transformer-Mamba MoE model combining Mamba's efficiency with Transformer's contextual understanding. It features adaptive CoT, 56B activated parameters, and achieves top performance with 77.9% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a high-performance, efficient large-scale model by synergizing Mamba and Transformer strengths, optimizing computational resources for diverse query complexities.

Method: Uses a 56B activated parameter model with 128 layers (Mamba2, Attention, FFN), adaptive CoT, and MoE structure. Pre-trained on 16T tokens, enhanced via SFT, Adaptive CoT Fusion, and RL.

Result: Ranked top 7 on LMSYS Chatbot Arena (1356 score), outperforming models like Gemini-2.0-Flash-001. Achieves 77.9% average on 23 benchmarks.

Conclusion: Hunyuan-TurboS balances performance and efficiency, offering superior capabilities at lower inference costs, setting a new standard for large-scale models.

Abstract: As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep "thinking" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.

</details>


### [104] [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/pdf/2505.15442)
*Suhas Kamasetty Ramesh, Ayan Sengupta, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: KD improves smaller LMs (up to 10%) more than larger ones (1.3%). Teacher expertise matters more than performance, and KD doesn't always preserve reasoning fidelity.


<details>
  <summary>Details</summary>
Motivation: To explore KD's effectiveness for smaller LMs and understand the mechanisms behind knowledge transfer.

Method: Large-scale empirical and statistical analysis of KD on models (0.5B-7B parameters) across 14 zero-shot reasoning tasks.

Result: Smaller models gain up to 10% performance, while larger models see minimal benefits. Teacher expertise, not performance, impacts KD. KD may not preserve reasoning fidelity.

Conclusion: KD is more beneficial for smaller LMs, but trade-offs exist in reasoning fidelity. Teacher signals and logit smoothing are key factors.

Abstract: Knowledge distillation (KD) is a key technique for compressing large language
models into smaller ones while preserving performance. Despite the recent
traction of KD research, its effectiveness for smaller language models (LMs)
and the mechanisms driving knowledge transfer remain underexplored. In this
work, we present the first large-scale empirical and statistical analysis of KD
across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks
in a zero-shot setting. Our findings reveal that KD can improve the average
performance of smaller models by up to $10\%$, with a peak task specific gain
of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger
models. Surprisingly, teacher performance has a minimal impact on student
outcomes, while teacher task expertise impacts KD effectiveness. A correlation
study indicates that smaller LMs benefit more from KD, whereas larger LMs show
diminished gains. Additionally, we uncover a misalignment between improvements
in student performance and reasoning fidelity, suggesting that while KD
enhances accuracy, it does not always maintain the structured decision-making
processes of the teacher. Our ablation study further highlights the importance
of teacher signals and logit smoothing in influencing students' performance
after distillation. Overall, our study offers a comprehensive empirical and
statistical assessment of KD, highlighting both its benefits and trade-offs
when distilling knowledge from larger to smaller LMs.

</details>


### [105] [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/pdf/2505.15443)
*Artem Zabolotnyi, Roman Makarov, Mile Mitrovic, Polina Proskura, Oleg Travkin, Roman Alferov, Alexey Zaytsev*

Main category: cs.CL

TL;DR: AdUE1 enhances uncertainty estimation in fine-tuned language models using a differentiable max approximation and L2-SP regularization, outperforming baselines like Mahalanobis distance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of uncertainty estimation in parameter-efficient fine-tuning of pre-trained language models for classification tasks.

Method: Uses a differentiable approximation of the max function and applies L2-SP regularization to fine-tuned head weights.

Result: Outperforms baselines (Mahalanobis distance, softmax response) on five NLP datasets across four models, offering better-calibrated confidence.

Conclusion: AdUE1 is a lightweight, effective post-hoc method for improving uncertainty estimation without modifying the base model.

Abstract: Uncertainty estimation remains a critical challenge in adapting pre-trained
language models to classification tasks, particularly under parameter-efficient
fine-tuning approaches such as adapters. We introduce AdUE1, an efficient
post-hoc uncertainty estimation (UE) method, to enhance softmax-based
estimates. Our approach (1) uses a differentiable approximation of the maximum
function and (2) applies additional regularization through L2-SP, anchoring the
fine-tuned head weights and regularizing the model. Evaluations on five NLP
classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,
Qwen) demonstrate that our method consistently outperforms established
baselines such as Mahalanobis distance and softmax response. Our approach is
lightweight (no base-model changes) and produces better-calibrated confidence.

</details>


### [106] [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/pdf/2505.15524)
*Lang Gao, Kaiyang Wan, Wei Liu, Chenxi Wang, Zirui Song, Zixiang Xu, Yanbo Wang, Veselin Stoyanov, Xiuying Chen*

Main category: cs.CL

TL;DR: BiasLens is a test-set-free framework for analyzing bias in LLMs by leveraging vector space structure, offering scalable and interpretable bias detection without labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing bias evaluation methods for LLMs require labeled data and human effort, limiting their scope and scalability. BiasLens aims to overcome these limitations by analyzing bias directly in the model's vector space.

Method: BiasLens combines Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs) to extract interpretable concept representations and quantifies bias by measuring representational similarity variations.

Result: BiasLens achieves strong agreement with traditional metrics (Spearman correlation r > 0.85) and detects biases missed by existing methods, such as insurance status affecting clinical assessments.

Conclusion: BiasLens provides a scalable, efficient, and interpretable approach to bias discovery, enhancing fairness and transparency in LLMs.

Abstract: Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
"positive" and "negative"), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of "food" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.

</details>


### [107] [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/pdf/2505.15456)
*Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: The paper introduces RLPA, a reinforcement learning framework for personalized alignment in LLMs, achieving state-of-the-art performance in dialogue personalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for personalized alignment in LLMs are static and shallow, failing in cold-start and long-term scenarios.

Method: RLPA uses reinforcement learning with a dual-level reward structure (Profile Reward and Response Reward) to iteratively refine user profiles through dialogue.

Result: Qwen-RLPA outperforms baselines and commercial models like Claude-3.5 and GPT-4o, showing robustness in handling conflicting preferences and long-term personalization.

Conclusion: Dynamic profile inference via RLPA is a promising paradigm for personalized dialogue systems.

Abstract: Personalized alignment is essential for enabling large language models (LLMs)
to engage effectively in user-centric dialogue. While recent prompt-based and
offline optimization methods offer preliminary solutions, they fall short in
cold-start scenarios and long-term personalization due to their inherently
static and shallow designs. In this work, we introduce the Reinforcement
Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts
with a simulated user model to iteratively infer and refine user profiles
through dialogue. The training process is guided by a dual-level reward
structure: the Profile Reward encourages accurate construction of user
representations, while the Response Reward incentivizes generation of responses
consistent with the inferred profile. We instantiate RLPA by fine-tuning
Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art
performance in personalized dialogue. Empirical evaluations demonstrate that
Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,
and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.
Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting
user preferences, sustaining long-term personalization and delivering more
efficient inference compared to recent reasoning-focused LLMs. These results
emphasize the potential of dynamic profile inference as a more effective
paradigm for building personalized dialogue systems.

</details>


### [108] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/pdf/2505.15553)
*Angelie Kraft, Judith Simon, Sonja Schimmler*

Main category: cs.CL

TL;DR: Popular QA and RC benchmarks are biased due to lack of diversity in their creation, with insufficient transparency and measures to address social bias.


<details>
  <summary>Details</summary>
Motivation: To highlight biases in QA and RC benchmarks and advocate for fairer practices in benchmark creation.

Method: Qualitative content analysis of 30 benchmark papers and quantitative analysis of 20 datasets to examine creator diversity, bias handling, and demographic biases.

Result: Most papers lacked transparency about stakeholders; only one addressed social representation. Gender, religion, and geographic biases were prevalent.

Conclusion: More transparent and bias-aware benchmark creation is needed to develop fairer LLMs.

Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.

</details>


### [109] [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/pdf/2505.15471)
*Yiyun Zhou, Chang Yao, Jingyuan Chen*

Main category: cs.CL

TL;DR: CoLA is a flexible LoRA architecture with collaborative strategies, outperforming existing PEFT methods, especially in low-sample scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current PEFT methods like LoRA in multi-task scenarios due to task interference and fixed structures.

Method: Proposes CoLA, a flexible LoRA architecture with efficient initialization and three collaborative strategies to leverage matrix relationships.

Result: CoLA demonstrates superior performance and robustness, particularly in low-sample settings.

Conclusion: CoLA offers a promising solution for efficient fine-tuning in multi-task scenarios, with publicly available data and code.

Abstract: The scaling law of Large Language Models (LLMs) reveals a power-law
relationship, showing diminishing return on performance as model scale
increases. While training LLMs from scratch is resource-intensive, fine-tuning
a pre-trained model for specific tasks has become a practical alternative. Full
fine-tuning (FFT) achieves strong performance; however, it is computationally
expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like
LoRA, have been proposed to address these challenges by freezing the
pre-trained model and adding lightweight task-specific modules. LoRA, in
particular, has proven effective, but its application to multi-task scenarios
is limited by interference between tasks. Recent approaches, such as
Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these
issues but still struggle with sample scarcity and noise interference due to
their fixed structure. In response, we propose CoLA, a more flexible LoRA
architecture with an efficient initialization scheme, and introduces three
collaborative strategies to enhance performance by better utilizing the
quantitative relationships between matrices $A$ and $B$. Our experiments
demonstrate the effectiveness and robustness of CoLA, outperforming existing
PEFT methods, especially in low-sample scenarios. Our data and code are fully
publicly available at https://github.com/zyy-2001/CoLA.

</details>


### [110] [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/pdf/2505.15554)
*Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kökciyan*

Main category: cs.CL

TL;DR: A system for generating critical questions (CQs) using LLMs and chain-of-thought prompting, guided by Walton's argumentation schemes, achieves competitive results in fostering critical thinking.


<details>
  <summary>Details</summary>
Motivation: To enhance critical thinking by generating relevant and diverse critical questions for argumentative texts.

Method: Leverages LLMs with chain-of-thought prompting to instantiate argument schemes, generate CQs, and rank the top 3 most helpful questions.

Result: Competitive performance in the shared task, demonstrating effectiveness in generating contextually relevant CQs.

Conclusion: The system shows promise in fostering critical thinking and detecting uninformed claims in argumentative texts.

Abstract: Critical questions are essential resources to provoke critical thinking when
encountering an argumentative text. We present our system for the Critical
Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach
leverages large language models (LLMs) with chain-of-thought prompting to
generate critical questions guided by Walton's argumentation schemes. For each
input intervention, we conversationally prompt LLMs to instantiate the
corresponding argument scheme template to first obtain structured arguments,
and then generate relevant critical questions. Following this, we rank all the
available critical questions by prompting LLMs to select the top 3 most helpful
questions based on the original intervention text. This combination of
structured argumentation theory and step-by-step reasoning enables the
generation of contextually relevant and diverse critical questions. Our
pipeline achieves competitive performance in the final test set, showing its
potential to foster critical thinking given argumentative text and detect
missing or uninformed claims. Code available at
\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.

</details>


### [111] [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/pdf/2505.15472)
*Song Dai, Yibo Yan, Jiamin Su, Dongfang Zihao, Yubo Gao, Yonghua Hei, Jungang Li, Junyan Zhang, Sicheng Tao, Zhuoran Gao, Xuming Hu*

Main category: cs.CL

TL;DR: PhysicsArena is introduced as the first multimodal benchmark to evaluate MLLMs on variable identification, process formulation, and solution derivation in physics reasoning.


<details>
  <summary>Details</summary>
Motivation: Current physics benchmarks lack multimodal inputs and overlook intermediate reasoning steps, limiting MLLMs' application in physics.

Method: PhysicsArena is designed to holistically assess MLLMs across three dimensions: variable identification, physical process formulation, and solution derivation.

Result: PhysicsArena provides a comprehensive platform for evaluating and advancing MLLMs' multimodal physics reasoning.

Conclusion: PhysicsArena addresses gaps in current benchmarks, enabling better assessment and development of MLLMs for physics reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in diverse reasoning tasks, yet their application to complex
physics reasoning remains underexplored. Physics reasoning presents unique
challenges, requiring grounding in physical conditions and the interpretation
of multimodal information. Current physics benchmarks are limited, often
focusing on text-only inputs or solely on problem-solving, thereby overlooking
the critical intermediate steps of variable identification and process
formulation. To address these limitations, we introduce PhysicsArena, the first
multimodal physics reasoning benchmark designed to holistically evaluate MLLMs
across three critical dimensions: variable identification, physical process
formulation, and solution derivation. PhysicsArena aims to provide a
comprehensive platform for assessing and advancing the multimodal physics
reasoning abilities of MLLMs.

</details>


### [112] [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/pdf/2505.15480)
*Qihuang Zhong, Liang Ding, Xiantao Cai, Juhua Liu, Bo Du, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper introduces Knowledge-aware Fine-tuning (KaFT), a method to improve LLMs' QA performance by addressing knowledge conflicts in SFT.


<details>
  <summary>Details</summary>
Motivation: Conflicts between LLMs' internal knowledge and training data context make vanilla SFT suboptimal.

Method: KaFT assigns training weights based on conflict levels detected via query diversification.

Result: KaFT consistently improves performance across four LLMs and enhances generalization while reducing hallucination.

Conclusion: KaFT is a simple yet effective solution for optimizing SFT by leveraging conflict-aware training.

Abstract: Supervised fine-tuning (SFT) is a common approach to improve the
domain-specific question-answering (QA) performance of large language models
(LLMs). However, recent literature reveals that due to the conflicts between
LLMs' internal knowledge and the context knowledge of training data, vanilla
SFT using the full QA training set is usually suboptimal. In this paper, we
first design a query diversification strategy for robust conflict detection and
then conduct a series of experiments to analyze the impact of knowledge
conflict. We find that 1) training samples with varied conflicts contribute
differently, where SFT on the data with large conflicts leads to catastrophic
performance drops; 2) compared to directly filtering out the conflict data,
appropriately applying the conflict data would be more beneficial. Motivated by
this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely
KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to
adapt the training weight by assigning different rewards for different training
samples according to conflict level. Extensive experiments show that KaFT
brings consistent and significant improvements across four LLMs. More analyses
prove that KaFT effectively improves the model generalization and alleviates
the hallucination.

</details>


### [113] [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/pdf/2505.15490)
*Isidora Jeknic, Alex Duchnowski, Alexander Koller*

Main category: cs.CL

TL;DR: A novel dialogue game for collaborative solving of the Traveling Salesman Problem (TSP) using LLM prompting and symbolic mechanisms achieves 45% optimal solutions in self-play and shows human collaboration and generalization ability.


<details>
  <summary>Details</summary>
Motivation: To develop dialogue agents capable of collaboratively solving complex NP-hard tasks like TSP, requiring exploration of solution space.

Method: Combines LLM prompting with symbolic mechanisms for state tracking and grounding in a two-player TSP dialogue game.

Result: The best agent solves 45% of games optimally in self-play and collaborates effectively with humans, generalizing to unfamiliar graphs.

Conclusion: The approach demonstrates promise for collaborative problem-solving in complex tasks, with potential for further refinement.

Abstract: Dialogue agents that support human users in solving complex tasks have
received much attention recently. Many such tasks are NP-hard optimization
problems that require careful collaborative exploration of the solution space.
We introduce a novel dialogue game in which the agents collaboratively solve a
two-player Traveling Salesman problem, along with an agent that combines LLM
prompting with symbolic mechanisms for state tracking and grounding. Our best
agent solves 45% of games optimally in self-play. It also demonstrates an
ability to collaborate successfully with human users and generalize to
unfamiliar graphs.

</details>


### [114] [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/pdf/2505.15508)
*Prasoon Bajpai, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper studies test-time scaling in multilingual settings, revealing varied effectiveness across languages and introducing MITT to improve reasoning performance for underrepresented languages.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of test-time scaling has been largely unexplored in non-English languages, prompting a systematic evaluation of its behavior in multilingual contexts.

Method: The study evaluates DeepSeek models across high- and low-resource Latin-script languages, analyzes reasoning inconsistencies, and introduces MITT for unsupervised reasoning prefix-tuning.

Result: Test-time scaling's gains vary by language; models often switch to English mid-reasoning, and low-resource languages show lower internal consistency. MITT enhances reasoning performance, especially for underrepresented languages.

Conclusion: MITT effectively addresses multilingual reasoning inconsistencies, improving performance for low-resource languages and demonstrating the potential of transfer learning in multilingual settings.

Abstract: Test-time scaling has emerged as a widely adopted inference-time strategy for
boosting reasoning performance. However, its effectiveness has been studied
almost exclusively in English, leaving its behavior in other languages largely
unexplored. We present the first systematic study of test-time scaling in
multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and
DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script
languages. Our findings reveal that the relative gains from test-time scaling
vary significantly across languages. Additionally, models frequently switch to
English mid-reasoning, even when operating under strictly monolingual prompts.
We further show that low-resource languages not only produce initial reasoning
thoughts that differ significantly from English but also have lower internal
consistency across generations in their early reasoning. Building on our
findings, we introduce MITT (Multilingual Initial Thought Transfer), an
unsupervised and lightweight reasoning prefix-tuning approach that transfers
high-resource reasoning prefixes to enhance test-time scaling across all
languages, addressing inconsistencies in multilingual reasoning performance.
MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,
especially for underrepresented languages.

</details>


### [115] [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/pdf/2505.15556)
*Ana-Maria Bucur, Marcos Zampieri, Tharindu Ranasinghe, Fabio Crestani*

Main category: cs.CL

TL;DR: Survey on multilingual mental health disorder detection using social media data, addressing cultural nuances and NLP tool performance.


<details>
  <summary>Details</summary>
Motivation: The need for effective digital mental health screening in multilingual contexts, as existing studies primarily focus on English data.

Method: Survey and investigation of cultural influences on online language and self-disclosure behaviors, with a list of multilingual datasets.

Result: Identified cultural nuances impacting NLP tools and provided multilingual datasets for model development.

Conclusion: Findings can guide the creation of multilingual mental health screening tools to serve diverse populations globally.

Abstract: The increasing prevalence of mental health disorders globally highlights the
urgent need for effective digital screening methods that can be used in
multilingual contexts. Most existing studies, however, focus on English data,
overlooking critical mental health signals that may be present in non-English
texts. To address this important gap, we present the first survey on the
detection of mental health disorders using multilingual social media data. We
investigate the cultural nuances that influence online language patterns and
self-disclosure behaviors, and how these factors can impact the performance of
NLP tools. Additionally, we provide a comprehensive list of multilingual data
collections that can be used for developing NLP models for mental health
screening. Our findings can inform the design of effective multilingual mental
health screening tools that can meet the needs of diverse populations,
ultimately improving mental health outcomes on a global scale.

</details>


### [116] [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/pdf/2505.15607)
*David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan*

Main category: cs.CL

TL;DR: An RL-based framework aligns LLMs for tutoring by emphasizing pedagogy over direct answers, achieving performance comparable to proprietary models without human annotations.


<details>
  <summary>Details</summary>
Motivation: Optimizing LLMs for direct question-answering undermines pedagogy, so the paper aims to adapt them into effective tutors using reinforcement learning.

Method: Proposes an online RL framework using simulated student-tutor interactions, with controllable reward weighting to balance pedagogy and accuracy.

Result: A 7B parameter tutor model matches larger proprietary models (e.g., LearnLM) and preserves reasoning better than single-turn SFT baselines.

Conclusion: The framework successfully aligns LLMs for tutoring, balancing pedagogy and accuracy while optionally enhancing interpretability with thinking tags.

Abstract: Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.

</details>


### [117] [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/pdf/2505.15561)
*Florin Cuconasu, Simone Filice, Guy Horowitz, Yoelle Maarek, Fabrizio Silvestri*

Main category: cs.CL

TL;DR: Retrieval Augmented Generation (RAG) improves LLM accuracy but is affected by positional bias, which marginalizes both relevant and distracting passages, making sophisticated rearrangement strategies ineffective.


<details>
  <summary>Details</summary>
Motivation: To understand how positional bias in LLMs impacts their ability to utilize retrieved passages and their susceptibility to distractions, especially in real-world scenarios.

Method: Extensive experiments on three benchmarks to analyze the effect of positional bias on LLMs using state-of-the-art retrieval pipelines.

Result: Over 60% of queries contain highly distracting passages in the top-10 retrieved, and positional bias's impact is marginal as both relevant and distracting passages are penalized.

Conclusion: Sophisticated passage rearrangement strategies based on LLM positional preferences do not outperform random shuffling, highlighting the limited practical impact of positional bias in RAG.

Abstract: Retrieval Augmented Generation enhances LLM accuracy by adding passages
retrieved from an external corpus to the LLM prompt. This paper investigates
how positional bias - the tendency of LLMs to weight information differently
based on its position in the prompt - affects not only the LLM's capability to
capitalize on relevant passages, but also its susceptibility to distracting
passages. Through extensive experiments on three benchmarks, we show how
state-of-the-art retrieval pipelines, while attempting to retrieve relevant
passages, systematically bring highly distracting ones to the top ranks, with
over 60% of queries containing at least one highly distracting passage among
the top-10 retrieved passages. As a result, the impact of the LLM positional
bias, which in controlled settings is often reported as very prominent by
related works, is actually marginal in real scenarios since both relevant and
distracting passages are, in turn, penalized. Indeed, our findings reveal that
sophisticated strategies that attempt to rearrange the passages based on LLM
positional preferences do not perform better than random shuffling.

</details>


### [118] [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/pdf/2505.15612)
*Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, Junxian He*

Main category: cs.CL

TL;DR: The paper introduces LASER and LASER-D, RL-based methods to improve reasoning efficiency in Large Reasoning Models by using length-based reward shaping, achieving better performance and reduced redundancy.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) exhibit redundancy in long reasoning traces, limiting efficiency. The study aims to enhance reasoning efficiency using RL-based approaches.

Method: Proposes a unified framework for efficient reasoning via length-based reward shaping, introducing LASER (step function reward) and LASER-D (dynamic, difficulty-aware reward).

Result: LASER-D improves reasoning performance (+6.1 on AIME2024) and reduces token usage by 63%, producing concise reasoning patterns.

Conclusion: The RL-based methods (LASER and LASER-D) effectively balance performance and efficiency, reducing redundancy in LRMs.

Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant "self-reflections". Resources are at
https://github.com/hkust-nlp/Laser.

</details>


### [119] [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/pdf/2505.15563)
*Mohammad Ali, Naeemul Hassan*

Main category: cs.CL

TL;DR: SUFA is a new method for computational framing analysis using semantic relations and dependency parsing to analyze entity-centric frames in news media, validated on gun violence data.


<details>
  <summary>Details</summary>
Motivation: To advance computational framing analysis by identifying and assessing entity-centric emphasis frames in news media.

Method: Semantic Relations-based Unsupervised Framing Analysis (SUFA), leveraging semantic relations and dependency parsing algorithms.

Result: SUFA demonstrates potential for analyzing entity-centric frames, validated through qualitative and computational studies on gun violence data.

Conclusion: SUFA is a significant methodological advancement with broad applicability in social sciences and computational domains.

Abstract: This research presents a novel approach to computational framing analysis,
called Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA
leverages semantic relations and dependency parsing algorithms to identify and
assess entity-centric emphasis frames in news media reports. This innovative
method is derived from two studies -- qualitative and computational -- using a
dataset related to gun violence, demonstrating its potential for analyzing
entity-centric emphasis frames. This article discusses SUFA's strengths,
limitations, and application procedures. Overall, the SUFA approach offers a
significant methodological advancement in computational framing analysis, with
its broad applicability across both the social sciences and computational
domains.

</details>


### [120] [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/pdf/2505.15633)
*David Thulke, Jakob Kemmler, Christian Dugast, Hermann Ney*

Main category: cs.CL

TL;DR: The paper explores improving faithfulness in retrieval-augmented large language models (LLMs) like ClimateGPT, achieving a 27% boost in faithfulness by refining training data.


<details>
  <summary>Details</summary>
Motivation: To enhance accessibility of climate-related documents while reducing factual hallucinations in LLMs by ensuring output faithfulness to retrieved passages.

Method: Automatic assessment of faithfulness, followed by refining ClimateGPT's training data by excluding unfaithful subsets, resulting in ClimateGPT Faithful+.

Result: ClimateGPT Faithful+ improved faithfulness from 30% to 57% in supported atomic claims.

Conclusion: Excluding unfaithful training data significantly enhances model faithfulness, making retrieval-augmented LLMs more reliable for climate science.

Abstract: Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.

</details>


### [121] [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/pdf/2505.15623)
*Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig*

Main category: cs.CL

TL;DR: The paper introduces MAPLE, a new metric to evaluate LLMs' mathematical reasoning beyond just accuracy, focusing on error rates, redundancy, and validity.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for LLMs in mathematical reasoning only consider accuracy, ignoring the quality of reasoning steps.

Method: Proposes the MAPLE score, a holistic metric integrating error rates, redundancy, and validity to assess reasoning alignment.

Result: MAPLE provides a more comprehensive evaluation of LLMs' mathematical reasoning capabilities.

Conclusion: The MAPLE score addresses gaps in current evaluation methods, offering a better measure of reasoning quality in LLMs.

Abstract: Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.

</details>


### [122] [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/pdf/2505.15634)
*Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du*

Main category: cs.CL

TL;DR: The paper introduces a steering technique to enhance LLM reasoning without external datasets, using SAEs or a novel SAE-free method, both improving performance.


<details>
  <summary>Details</summary>
Motivation: To improve LLM reasoning without costly long CoT data and fine-tuning.

Method: Uses SAEs to extract features from CoT or a SAE-free algorithm to compute steering directions from residual activations.

Result: Both methods significantly enhance LLM reasoning capabilities.

Conclusion: Steering techniques, with or without SAEs, effectively boost LLM reasoning.

Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.

</details>


### [123] [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/pdf/2505.15656)
*Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang*

Main category: cs.CL

TL;DR: Fine-tuning open-source LLMs with proprietary data risks data extraction by the original model creator via backdoor training, with high success rates (up to 94.9%).


<details>
  <summary>Details</summary>
Motivation: To expose the risk of private data extraction from fine-tuned LLMs by the original model creators, highlighting a new security concern.

Method: Conducted experiments on 4 open-source LLMs (3B-32B parameters) and 2 datasets, using backdoor training to extract fine-tuning data.

Result: Up to 76.3% of fine-tuning data was extracted in practical settings, rising to 94.9% in ideal conditions. A detection-based defense was bypassed.

Conclusion: The study underscores a critical data breach risk in fine-tuning LLMs, urging further research to mitigate this threat.

Abstract: Fine-tuning on open-source Large Language Models (LLMs) with proprietary data
is now a standard practice for downstream developers to obtain task-specific
LLMs. Surprisingly, we reveal a new and concerning risk along with the
practice: the creator of the open-source LLMs can later extract the private
downstream fine-tuning data through simple backdoor training, only requiring
black-box access to the fine-tuned downstream model. Our comprehensive
experiments, across 4 popularly used open-source models with 3B to 32B
parameters and 2 downstream datasets, suggest that the extraction performance
can be strikingly high: in practical settings, as much as 76.3% downstream
fine-tuning data (queries) out of a total 5,000 samples can be perfectly
extracted, and the success rate can increase to 94.9% in more ideal settings.
We also explore a detection-based defense strategy but find it can be bypassed
with improved attack. Overall, we highlight the emergency of this newly
identified data breaching risk in fine-tuning, and we hope that more follow-up
research could push the progress of addressing this concerning risk. The code
and data used in our experiments are released at
https://github.com/thu-coai/Backdoor-Data-Extraction.

</details>


### [124] [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/pdf/2505.15674)
*Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Ningyu Zhang, Kun Wang, Yang Wang*

Main category: cs.CL

TL;DR: UniErase introduces a token-based unlearning method for LLMs, balancing efficacy and model ability, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like knowledge conflicts and outdated information in LLMs, where current unlearning methods fail to balance efficacy and model performance.

Method: UniErase uses learnable parametric suffixes (unlearning tokens) in two phases: optimization to bind unlearning outputs and lightweight model editing to activate forgetting.

Result: Achieves SOTA performance in unlearning tasks, modifying only 3.66% of parameters, outperforming baselines by 4.01x in model ability and 35.96% in unlearning efficacy.

Conclusion: UniErase sets a new direction for token-based unlearning, demonstrating dual top-tier performance in efficacy and model retention.

Abstract: Large language models require iterative updates to address challenges such as
knowledge conflicts and outdated information (e.g., incorrect, private, or
illegal contents). Machine unlearning provides a systematic methodology for
targeted knowledge removal from trained models, enabling elimination of
sensitive information influences. However, mainstream fine-tuning-based
unlearning methods often fail to balance unlearning efficacy and model ability,
frequently resulting in catastrophic model collapse under extensive knowledge
removal. Meanwhile, in-context unlearning, which relies solely on contextual
prompting without modifying the model's intrinsic mechanisms, suffers from
limited generalizability and struggles to achieve true unlearning. In this
work, we introduce UniErase, a novel unlearning paradigm that employs learnable
parametric suffix (unlearning token) to steer language models toward targeted
forgetting behaviors. UniErase operates through two key phases: (I) an
optimization stage that binds desired unlearning outputs to the model's
autoregressive probability distribution via token optimization, followed by
(II) a lightweight model editing phase that activates the learned token to
probabilistically induce specified forgetting objective. Serving as a new
research direction for token learning to induce unlearning target, UniErase
achieves state-of-the-art (SOTA) performance across batch, sequential, and
precise unlearning under fictitious and real-world knowledge settings.
Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%
of the LLM parameters, outperforms previous forgetting SOTA baseline by around
4.01 times for model ability with even better unlearning efficacy. Similarly,
UniErase, maintaining more ability, also surpasses previous retaining SOTA by
35.96% for unlearning efficacy, showing dual top-tier performances in current
unlearing domain.

</details>


### [125] [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/pdf/2505.15683)
*Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, Zhiming Zheng*

Main category: cs.CL

TL;DR: FL-LLaMA is a federated split framework for LLMs, addressing security, efficiency, and adaptability challenges in private data usage. It achieves comparable performance to centralized LLaMA2 with significant speedups.


<details>
  <summary>Details</summary>
Motivation: Private data's potential for improving LLMs is hindered by distribution across silos and computational demands. Existing split learning models face security, efficiency, and adaptability issues.

Method: FL-LLaMA uses local client blocks with Gaussian noise for security, parallel training strategies, and dynamic partition points for adaptability.

Result: FL-LLaMA matches centralized LLaMA2 performance, achieving 2x training and 8x inference speedups, with proven security and adaptability.

Conclusion: FL-LLaMA effectively addresses federated LLM challenges, offering secure, efficient, and adaptable solutions for private data utilization.

Abstract: Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and KV
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.

</details>


### [126] [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/pdf/2505.15682)
*Cosimo Iaia, Bhavin Choksi, Emily Wiebers, Gemma Roig, Christian J. Fiebach*

Main category: cs.CL

TL;DR: The study explores how concreteness is represented in human minds and language models, finding alignment between human and model representations, primarily driven by concreteness.


<details>
  <summary>Details</summary>
Motivation: Understanding concreteness representation is key in psychology, neuroscience, and computational linguistics, yet its role in language models is underexplored.

Method: Behavioral judgments and Representational Similarity Analysis were used to compare human and language model semantic representations, with concreteness ratings as an explicit measure.

Result: Human and model representations align significantly, driven by concreteness, but not by other psycholinguistic word characteristics.

Conclusion: Humans and language models converge on concreteness, suggesting its unique role in semantic representation.

Abstract: The nouns of our language refer to either concrete entities (like a table) or
abstract concepts (like justice or love), and cognitive psychology has
established that concreteness influences how words are processed. Accordingly,
understanding how concreteness is represented in our mind and brain is a
central question in psychology, neuroscience, and computational linguistics.
While the advent of powerful language models has allowed for quantitative
inquiries into the nature of semantic representations, it remains largely
underexplored how they represent concreteness. Here, we used behavioral
judgments to estimate semantic distances implicitly used by humans, for a set
of carefully selected abstract and concrete nouns. Using Representational
Similarity Analysis, we find that the implicit representational space of
participants and the semantic representations of language models are
significantly aligned. We also find that both representational spaces are
implicitly aligned to an explicit representation of concreteness, which was
obtained from our participants using an additional concreteness rating task.
Importantly, using ablation experiments, we demonstrate that the human-to-model
alignment is substantially driven by concreteness, but not by other important
word characteristics established in psycholinguistics. These results indicate
that humans and language models converge on the concreteness dimension, but not
on other dimensions.

</details>


### [127] [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/pdf/2505.15684)
*Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu*

Main category: cs.CL

TL;DR: ThinkLess reduces reasoning token length in LLMs by early termination, maintaining accuracy while cutting latency and memory usage.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of long reasoning tokens in CoT prompting, which increases latency and memory usage.

Method: Early termination of reasoning generation using a terminator token and post-regulation for structured answers.

Result: Comparable accuracy to full-length CoT with reduced decoding time and memory.

Conclusion: ThinkLess offers an efficient alternative to CoT without model modification or extra data.

Abstract: While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
KV cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.

</details>


### [128] [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/pdf/2505.15722)
*Xiaoyu Luo, Yiyi Chen, Johannes Bjerva, Qiongxiu Li*

Main category: cs.CL

TL;DR: The study explores memorization in multilingual large language models (MLLMs), finding that language similarity, not just data availability, explains memorization patterns. A graph-based metric is proposed to analyze cross-lingual memorization.


<details>
  <summary>Details</summary>
Motivation: Understanding memorization in MLLMs is critical due to their deployment, yet prior work focused on monolingual models, leaving multilingual memorization underexplored.

Method: Analyzed 95 languages across diverse models, proposing a graph-based correlation metric incorporating language similarity.

Result: Languages with fewer training tokens but high similarity exhibit higher memorization, a trend revealed only when cross-lingual relationships are modeled.

Conclusion: Language similarity is key to understanding memorization in MLLMs, with implications for multilingual NLP and cross-lingual transferability.

Abstract: We present the first comprehensive study of Memorization in Multilingual
Large Language Models (MLLMs), analyzing 95 languages using models across
diverse model scales, architectures, and memorization definitions. As MLLMs are
increasingly deployed, understanding their memorization behavior has become
critical. Yet prior work has focused primarily on monolingual models, leaving
multilingual memorization underexplored, despite the inherently long-tailed
nature of training corpora. We find that the prevailing assumption, that
memorization is highly correlated with training data availability, fails to
fully explain memorization patterns in MLLMs. We hypothesize that treating
languages in isolation - ignoring their similarities - obscures the true
patterns of memorization. To address this, we propose a novel graph-based
correlation metric that incorporates language similarity to analyze
cross-lingual memorization. Our analysis reveals that among similar languages,
those with fewer training tokens tend to exhibit higher memorization, a trend
that only emerges when cross-lingual relationships are explicitly modeled.
These findings underscore the importance of a language-aware perspective in
evaluating and mitigating memorization vulnerabilities in MLLMs. This also
constitutes empirical evidence that language similarity both explains
Memorization in MLLMs and underpins Cross-lingual Transferability, with broad
implications for multilingual NLP.

</details>


### [129] [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/pdf/2505.15692)
*Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao*

Main category: cs.CL

TL;DR: TAPO enhances RL by integrating external high-level guidance, outperforming existing methods and improving reasoning and explainability.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods limit exploration by focusing on reward-maximizing paths without external knowledge, restricting reasoning capabilities.

Method: Proposes TAPO, a framework that incorporates external 'thought patterns' to balance exploration and guidance during training.

Result: TAPO outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math, showing strong generalization with minimal prior samples.

Conclusion: TAPO's integration of external guidance enhances reasoning, explainability, and readability, making it broadly applicable.

Abstract: Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.

</details>


### [130] [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/pdf/2505.15734)
*Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang*

Main category: cs.CL

TL;DR: The paper introduces DTE, a framework for autonomous reasoning improvement in LLMs using multi-agent debate and a new prompting strategy, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely heavily on external data for reasoning improvement, which is impractical. The need for autonomous enhancement without supervision drives this work.

Method: Proposes Debate, Train, Evolve (DTE), a ground truth-free framework using multi-agent debate and Reflect-Critique-Refine prompting to refine reasoning.

Result: DTE achieves an average accuracy gain of 8.92% on GSM-PLUS and 5.8% on other benchmarks, showing strong cross-domain generalization.

Conclusion: DTE effectively enhances LLM reasoning autonomously, demonstrating broad applicability and improved performance without external supervision.

Abstract: Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.

</details>


### [131] [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/pdf/2505.15695)
*Ryang Heo, Yongsik Seo, Junseong Lee, Dongha Lee*

Main category: cs.CL

TL;DR: The paper introduces OOMB, a benchmark for evaluating LLMs in opinion mining from diverse online content, assessing extractive and abstractive capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional opinion mining struggles with diverse, complex online content, prompting the need for a specialized benchmark.

Method: OOMB provides annotated (entity, feature, opinion) tuples and opinion summaries to evaluate LLMs.

Result: The benchmark identifies challenges and adaptability of LLMs in opinion mining, assessing their real-world applicability.

Conclusion: OOMB establishes a foundation for LLM-based opinion mining and suggests future research directions.

Abstract: The surge of user-generated online content presents a wealth of insights into
customer preferences and market trends. However, the highly diverse, complex,
and context-rich nature of such contents poses significant challenges to
traditional opinion mining approaches. To address this, we introduce Online
Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol
designed to assess the ability of large language models (LLMs) to mine opinions
effectively from diverse and intricate online environments. OOMB provides
extensive (entity, feature, opinion) tuple annotations and a comprehensive
opinion-centric summary that highlights key opinion topics within each content,
thereby enabling the evaluation of both the extractive and abstractive
capabilities of models. Through our proposed benchmark, we conduct a
comprehensive analysis of which aspects remain challenging and where LLMs
exhibit adaptability, to explore whether they can effectively serve as opinion
miners in realistic online scenarios. This study lays the foundation for
LLM-based opinion mining and discusses directions for future research in this
field.

</details>


### [132] [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/pdf/2505.15696)
*Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling*

Main category: cs.CL

TL;DR: MaxPoolBERT improves BERT's [CLS] token representation by aggregating information across layers and tokens, enhancing classification accuracy without extra pre-training or significant model size increase.


<details>
  <summary>Details</summary>
Motivation: Prior work shows that BERT's [CLS] token and other layers encode valuable contextual information, but it's underutilized. MaxPoolBERT aims to refine this representation for better classification performance.

Method: Proposes three modifications: (i) max-pooling [CLS] across layers, (ii) adding MHA for [CLS] to attend to the final layer, and (iii) combining max-pooling with MHA.

Result: MaxPoolBERT outperforms BERT-base on GLUE benchmarks, especially in low-resource tasks, without extra pre-training or major size increase.

Conclusion: MaxPoolBERT effectively enhances BERT's classification accuracy by better utilizing contextual information, offering a lightweight yet powerful extension.

Abstract: The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.

</details>


### [133] [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/pdf/2505.15702)
*Peng Wang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu*

Main category: cs.CL

TL;DR: LyapLock is a model editing framework for Large Language Models, addressing performance decline in sequential edits by using constrained stochastic programming, queuing theory, and Lyapunov optimization. It achieves asymptotic optimal performance and scales to over 10,000 edits.


<details>
  <summary>Details</summary>
Motivation: Current model editing methods for Large Language Models suffer from performance decline due to inadequate long-term knowledge preservation.

Method: LyapLock models sequential editing as constrained stochastic programming, integrating queuing theory and Lyapunov optimization to decompose the problem into stepwise subproblems.

Result: The framework scales to over 10,000 edits, stabilizes general capabilities, and improves editing efficacy by 11.89% over baselines.

Conclusion: LyapLock is the first model editing framework with theoretical guarantees, enhancing performance and scalability for sequential edits.

Abstract: Large Language Models often contain factually incorrect or outdated
knowledge, giving rise to model editing methods for precise knowledge updates.
However, current mainstream locate-then-edit approaches exhibit a progressive
performance decline during sequential editing, due to inadequate mechanisms for
long-term knowledge preservation. To tackle this, we model the sequential
editing as a constrained stochastic programming. Given the challenges posed by
the cumulative preservation error constraint and the gradually revealed editing
tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov
optimization to decompose the long-term constrained programming into tractable
stepwise subproblems for efficient solving. This is the first model editing
framework with rigorous theoretical guarantees, achieving asymptotic optimal
editing performance while meeting the constraints of long-term knowledge
preservation. Experimental results show that our framework scales sequential
editing capacity to over 10,000 edits while stabilizing general capabilities
and boosting average editing efficacy by 11.89\% over SOTA baselines.
Furthermore, it can be leveraged to enhance the performance of baseline
methods. Our code is released on https://github.com/caskcsg/LyapLock.

</details>


### [134] [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/pdf/2505.15710)
*Tianqi Du, Zeming Wei, Quan Chen, Chenheng Zhang, Yisen Wang*

Main category: cs.CL

TL;DR: Proposes Safety Representation Ranking (SRR), a framework using LLM internal states to rank safe responses, improving robustness to adversarial prompts.


<details>
  <summary>Details</summary>
Motivation: Addresses safety concerns in LLMs by leveraging internal representations, overlooked by existing text-based evaluation methods.

Method: SRR uses intermediate transformer representations to encode instructions and completions, ranking candidates via a lightweight similarity-based scorer.

Result: SRR significantly improves robustness to adversarial prompts across multiple benchmarks.

Conclusion: SRR effectively captures subtle safety signals by leveraging internal model states, offering a robust solution for LLM safety evaluation.

Abstract: The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.

</details>


### [135] [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/pdf/2505.15712)
*Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li Zhang*

Main category: cs.CL

TL;DR: TurnaboutLLM evaluates LLMs' deductive reasoning using detective games, revealing limitations in current strategies and performance variations based on context size and reasoning steps.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to handle complex deductive reasoning in narrative-rich environments like detective games.

Method: Uses a framework and dataset from Ace Attorney and Danganronpa to test LLMs on identifying contradictions in testimonies and evidence.

Result: Evaluated 12 LLMs, showing limitations in reasoning strategies and varying effects of context size, reasoning steps, and answer space.

Conclusion: TurnaboutLLM highlights challenges for LLMs in deductive reasoning within complex narratives.

Abstract: This paper introduces TurnaboutLLM, a novel framework and dataset for
evaluating the deductive reasoning abilities of Large Language Models (LLMs) by
leveraging the interactive gameplay of detective games Ace Attorney and
Danganronpa. The framework tasks LLMs with identifying contradictions between
testimonies and evidences within long narrative contexts, a challenging task
due to the large answer space and diverse reasoning types presented by its
questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at
limitations of popular strategies for enhancing deductive reasoning such as
extensive thinking and Chain-of-Thought prompting. The results also suggest
varying effects of context size, the number of reasoning step and answer space
size on model performance. Overall, TurnaboutLLM presents a substantial
challenge for LLMs' deductive reasoning abilities in complex, narrative-rich
environments.

</details>


### [136] [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/pdf/2505.15778)
*Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, Xin Eric Wang*

Main category: cs.CL

TL;DR: Soft Thinking introduces a training-free method for reasoning in a continuous concept space, improving accuracy and reducing token usage compared to traditional discrete methods.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models are limited by discrete linguistic tokens, restricting expressive power and reasoning exploration.

Method: Soft Thinking generates abstract concept tokens in a continuous space by mixing token embeddings, enabling richer representations and smoother transitions.

Result: Empirical results show improved pass@1 accuracy (up to 2.48 points) and reduced token usage (up to 22.4%) on benchmarks.

Conclusion: Soft Thinking overcomes the bottleneck of discrete language-based reasoning while maintaining interpretability.

Abstract: Human cognition typically involves thinking through abstract, fluid concepts
rather than strictly using discrete linguistic tokens. Current reasoning
models, however, are constrained to reasoning within the boundaries of human
language, processing discrete token embeddings that represent fixed points in
the semantic space. This discrete constraint restricts the expressive power and
upper potential of such reasoning models, often causing incomplete exploration
of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling
one token per step. In this work, we introduce Soft Thinking, a training-free
method that emulates human-like "soft" reasoning by generating soft, abstract
concept tokens in a continuous concept space. These concept tokens are created
by the probability-weighted mixture of token embeddings, which form the
continuous concept space, enabling smooth transitions and richer
representations that transcend traditional discrete boundaries. In essence,
each generated concept token encapsulates multiple meanings from related
discrete tokens, implicitly exploring various reasoning paths to converge
effectively toward the correct answer. Empirical evaluations on diverse
mathematical and coding benchmarks consistently demonstrate the effectiveness
and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points
while simultaneously reducing token usage by up to 22.4% compared to standard
CoT. Qualitative analysis further reveals that Soft Thinking outputs remain
highly interpretable and readable, highlighting the potential of Soft Thinking
to break the inherent bottleneck of discrete language-based reasoning. Code is
available at https://github.com/eric-ai-lab/Soft-Thinking.

</details>


### [137] [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/pdf/2505.15715)
*He Hu, Yucheng Zhou, Juzheng Si, Qianning Wang, Hengheng Zhang, Fuji Ren, Fei Ma, Laizhong Cui*

Main category: cs.CL

TL;DR: PsyLLM is a large language model designed for mental health counseling, integrating diagnostic and therapeutic reasoning with clinical standards and diverse therapeutic frameworks. It outperforms existing models on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based mental health approaches lack clinical grounding and diverse therapeutic strategies, limiting their real-world applicability.

Method: PsyLLM uses an automated data synthesis pipeline to generate high-quality, clinically aligned dialogue data, guided by DSM/ICD standards and multiple therapeutic frameworks.

Result: PsyLLM significantly outperforms state-of-the-art baseline models on a new benchmark assessing counseling quality.

Conclusion: PsyLLM addresses critical limitations in LLM-based mental health support, offering a clinically grounded and versatile solution.

Abstract: Large language models (LLMs) hold significant potential for mental health
support, capable of generating empathetic responses and simulating therapeutic
conversations. However, existing LLM-based approaches often lack the clinical
grounding necessary for real-world psychological counseling, particularly in
explicit diagnostic reasoning aligned with standards like the DSM/ICD and
incorporating diverse therapeutic modalities beyond basic empathy or single
strategies. To address these critical limitations, we propose PsyLLM, the first
large language model designed to systematically integrate both diagnostic and
therapeutic reasoning for mental health counseling. To develop the PsyLLM, we
propose a novel automated data synthesis pipeline. This pipeline processes
real-world mental health posts, generates multi-turn dialogue structures, and
leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and
multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate
detailed clinical reasoning processes. Rigorous multi-dimensional filtering
ensures the generation of high-quality, clinically aligned dialogue data. In
addition, we introduce a new benchmark and evaluation protocol, assessing
counseling quality across four key dimensions: comprehensiveness,
professionalism, authenticity, and safety. Our experiments demonstrate that
PsyLLM significantly outperforms state-of-the-art baseline models on this
benchmark.

</details>


### [138] [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/pdf/2505.15727)
*Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang*

Main category: cs.CL

TL;DR: VocalBench is a new benchmark for evaluating speech interaction models, addressing gaps in current evaluations by focusing on vocal-specific performance across four key dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of speech interaction models overlook vocal performance and lack benchmarks with vocal-specific test instances.

Method: Proposes VocalBench, a benchmark with 9,400 instances across four dimensions (semantic quality, acoustic performance, conversational abilities, robustness) and 16 skills.

Result: Experiments show significant variability in current model capabilities, highlighting strengths and weaknesses.

Conclusion: VocalBench provides insights to guide future research in speech-based interaction systems.

Abstract: The rapid advancement of large language models (LLMs) has accelerated the
development of multi-modal models capable of vocal communication. Unlike
text-based interactions, speech conveys rich and diverse information, including
semantic content, acoustic variations, paralanguage cues, and environmental
context. However, existing evaluations of speech interaction models
predominantly focus on the quality of their textual responses, often
overlooking critical aspects of vocal performance and lacking benchmarks with
vocal-specific test instances. To address this gap, we propose VocalBench, a
comprehensive benchmark designed to evaluate speech interaction models'
capabilities in vocal communication. VocalBench comprises 9,400 carefully
curated instances across four key dimensions: semantic quality, acoustic
performance, conversational abilities, and robustness. It covers 16 fundamental
skills essential for effective vocal interaction. Experimental results reveal
significant variability in current model capabilities, each exhibiting distinct
strengths and weaknesses, and provide valuable insights to guide future
research in speech-based interaction systems. Code and evaluation instances are
available at https://github.com/SJTU-OmniAgent/VocalBench.

</details>


### [139] [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/pdf/2505.15769)
*Mikhail Budnikov, Ivan Yamshchikov*

Main category: cs.CL

TL;DR: The paper explores transfer learning from synthetic languages to English, introduces a new synthetic language for better transfer, and proposes the Tiny-Cloze Benchmark for evaluating fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To improve transfer learning from synthetic languages to English and evaluate fine-tuned models' performance on linguistic tasks.

Method: Investigates embedding structures, introduces a new synthetic language, and creates the Tiny-Cloze Benchmark for evaluation.

Result: Fine-tuning on the new synthetic language improves performance across various tasks.

Conclusion: The new synthetic language and Tiny-Cloze Benchmark enhance transfer learning and model evaluation.

Abstract: This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.

</details>


### [140] [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/pdf/2505.15792)
*Danna Zheng, Mirella Lapata, Jeff Z. Pan*

Main category: cs.CL

TL;DR: MontageLie benchmark exposes vulnerabilities in current NLG evaluators; DoveScore improves robustness by verifying factual accuracy and event-order consistency.


<details>
  <summary>Details</summary>
Motivation: Current fine-grained evaluators neglect inter-fact dependencies, making them vulnerable to deceptive narratives like MontageLie.

Method: Introduces MontageLie benchmark and DoveScore, a framework verifying both factual accuracy and event-order consistency.

Result: DoveScore outperforms existing methods by over 8%, showing higher robustness.

Conclusion: DoveScore provides a more reliable solution for long-form text alignment evaluation.

Abstract: Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by "montaging" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.

</details>


### [141] [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/pdf/2505.15774)
*Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: HyCo$_2$ is a hybrid context compression method for LLMs that combines global and local perspectives to retain essential semantics and details, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiency and redundant processing in LLMs during long-sequence inference, while avoiding loss of valuable information.

Method: Integrates global semantics refinement via a hybrid adapter and local token retention decisions using a classification layer, supported by auxiliary pretraining.

Result: Enhances long-text reasoning, reduces token usage by 88.8%, and improves performance by 13.1% on QA benchmarks.

Conclusion: HyCo$_2$ effectively balances local and global information retention, optimizing LLM performance and efficiency.

Abstract: Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.

</details>


### [142] [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/pdf/2505.15801)
*Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper introduces VerifyBench and VerifyBench-Hard to evaluate reference-based reward systems in RL, highlighting gaps in current benchmarks and model performance.


<details>
  <summary>Details</summary>
Motivation: Existing reward benchmarks lack evaluation of reference-based reward systems, limiting understanding of verifier accuracy in RL.

Method: Two benchmarks (VerifyBench and VerifyBench-Hard) are created via meticulous data collection, curation, and human annotation.

Result: Current models, especially smaller ones, show significant room for improvement on the benchmarks.

Conclusion: The benchmarks are effective tools for improving verifier accuracy and reasoning capabilities in RL-trained models.

Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved
remarkable performance in the domain of reasoning. A key component of their
training is the incorporation of verifiable rewards within reinforcement
learning (RL). However, existing reward benchmarks do not evaluate
reference-based reward systems, leaving researchers with limited understanding
of the accuracy of verifiers used in RL. In this paper, we introduce two
benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the
performance of reference-based reward systems. These benchmarks are constructed
through meticulous data collection and curation, followed by careful human
annotation to ensure high quality. Current models still show considerable room
for improvement on both VerifyBench and VerifyBench-Hard, especially
smaller-scale models. Furthermore, we conduct a thorough and comprehensive
analysis of evaluation results, offering insights for understanding and
developing reference-based reward systems. Our proposed benchmarks serve as
effective tools for guiding the development of verifier accuracy and the
reasoning capabilities of models trained via RL in reasoning tasks.

</details>


### [143] [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.15776)
*Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, Xipeng Qiu*

Main category: cs.CL

TL;DR: ConvSearch-R1 is a self-driven framework for conversational query reformulation (CQR) that eliminates external supervision, using reinforcement learning and retrieval signals to outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing CQR approaches, which rely on costly external supervision and misalignment with retrievers.

Method: A two-stage approach: Self-Driven Policy Warm-Up for cold-start mitigation and Retrieval-Guided Reinforcement Learning with rank-incentive rewards.

Result: Achieves over 10% improvement on TopiOCQA, outperforming prior methods without external supervision.

Conclusion: ConvSearch-R1 demonstrates the effectiveness of self-driven learning for CQR, reducing dependency on external resources while improving performance.

Abstract: Conversational search systems require effective handling of context-dependent
queries that often contain ambiguity, omission, and coreference. Conversational
Query Reformulation (CQR) addresses this challenge by transforming these
queries into self-contained forms suitable for off-the-shelf retrievers.
However, existing CQR approaches suffer from two critical constraints: high
dependency on costly external supervision from human annotations or large
language models, and insufficient alignment between the rewriting model and
downstream retrievers. We present ConvSearch-R1, the first self-driven
framework that completely eliminates dependency on external rewrite supervision
by leveraging reinforcement learning to optimize reformulation directly through
retrieval signals. Our novel two-stage approach combines Self-Driven Policy
Warm-Up to address the cold-start problem through retrieval-guided
self-distillation, followed by Retrieval-Guided Reinforcement Learning with a
specially designed rank-incentive reward shaping mechanism that addresses the
sparsity issue in conventional retrieval metrics. Extensive experiments on
TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly
outperforms previous state-of-the-art methods, achieving over 10% improvement
on the challenging TopiOCQA dataset while using smaller 3B parameter models
without any external supervision.

</details>


### [144] [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/pdf/2505.15781)
*Xinyin Ma, Runpeng Yu, Gongfan Fang, Xinchao Wang*

Main category: cs.CL

TL;DR: The paper proposes a delayed KV-Cache mechanism for Diffusion Language Models (DLMs) to address slow inference, achieving 2-10x speedup and narrowing the performance gap with autoregressive models.


<details>
  <summary>Details</summary>
Motivation: DLMs suffer from slow inference due to their non-autoregressive architecture and lack of key-value cache, which is crucial for efficient decoding.

Method: Introduces two variants of delayed KV-Cache: (1) dKV-Cache-Decode for near-lossless acceleration, and (2) dKV-Cache-Greedy for higher speed-ups with some performance trade-offs.

Result: Achieves 2-10x inference speedup, improves performance on long sequences, and demonstrates applicability across various benchmarks.

Conclusion: The delayed KV-Cache effectively accelerates DLMs, making them more competitive with autoregressive models, even without training modifications.

Abstract: Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) dKV-Cache-Decode, which provides almost lossless
acceleration, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on
several benchmarks, delivering acceleration across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.

</details>


### [145] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/pdf/2505.15810)
*Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, Junxu*

Main category: cs.CL

TL;DR: The paper identifies challenges in GUI agent grounding with RL and proposes solutions: a Fast Thinking Template, box size constraints, and a revised RL objective, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address issues in GUI grounding tasks where general-purpose RL leads to poor performance due to input design, output evaluation, and policy update challenges.

Method: Proposes three solutions: a Fast Thinking Template for direct answers, box size constraints in rewards, and a revised RL objective with difficulty-aware scaling.

Result: GUI-G1-3B achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro, outperforming prior models.

Conclusion: The targeted solutions effectively improve GUI grounding performance, setting a new benchmark.

Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [146] [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/pdf/2505.15795)
*Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo*

Main category: cs.CL

TL;DR: The paper explores adversarial tuning of LLMs to boost evaluation scores by optimizing preambles, revealing vulnerabilities in LLM-as-a-judge frameworks.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in LLM-as-a-judge frameworks where responses can be manipulated to overfit judge preferences.

Method: Adversarially tune models to generate text preambles using judge-LLM signals as rewards, leveraging reinforcement learning.

Result: Frozen LLMs with tuned preambles achieve higher scores, and the method is undetectable and transferable to unseen models.

Conclusion: The findings highlight risks in LLM-as-a-judge evaluations and suggest broader applications for preamble optimization beyond adversarial attacks.

Abstract: The capabilities of Large Language Models (LLMs) are routinely evaluated by
other LLMs trained to predict human preferences. This framework--known as
LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also
vulnerable to malicious exploitation, as LLM responses can be tuned to overfit
the preferences of the judge. Previous work shows that the answers generated by
a candidate-LLM can be edited post hoc to maximise the score assigned to them
by a judge-LLM. In this study, we adopt a different approach and use the signal
provided by judge-LLMs as a reward to adversarially tune models that generate
text preambles designed to boost downstream performance. We find that frozen
LLMs pipelined with these models attain higher LLM-evaluation scores than
existing frameworks. Crucially, unlike other frameworks which intervene
directly on the model's response, our method is virtually undetectable. We also
demonstrate that the effectiveness of the tuned preamble generator transfers
when the candidate-LLM and the judge-LLM are replaced with models that are not
used during training. These findings raise important questions about the design
of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that
human preferences can be reverse engineered effectively, by pipelining LLMs to
optimise upstream preambles via reinforcement learning--an approach that could
find future applications in diverse tasks and domains beyond adversarial
attacks.

</details>


### [147] [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/pdf/2505.15805)
*Hwan Chang, Yumin Kim, Yonghyun Jun, Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper introduces CoPriva, a benchmark for evaluating LLM adherence to contextual non-disclosure policies, revealing vulnerabilities in current models against indirect attacks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale benchmarks for evaluating LLM adherence to contextual security policies, especially in sensitive domains.

Method: Developed CoPriva, a dataset with explicit policies and queries (direct/indirect attacks), and evaluated 10 LLMs.

Result: Many LLMs violate policies, especially against indirect attacks, struggling to incorporate constraints during generation.

Conclusion: Highlights a critical gap in LLM safety alignment and calls for robust methods to ensure contextual security.

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains such as enterprise and government, ensuring that they adhere to
user-defined security policies within context is critical-especially with
respect to information non-disclosure. While prior LLM studies have focused on
general safety and socially sensitive data, large-scale benchmarks for
contextual security preservation against attacks remain lacking. To address
this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating
LLM adherence to contextual non-disclosure policies in question answering.
Derived from realistic contexts, our dataset includes explicit policies and
queries designed as direct and challenging indirect attacks seeking prohibited
information. We evaluate 10 LLMs on our benchmark and reveal a significant
vulnerability: many models violate user-defined policies and leak sensitive
information. This failure is particularly severe against indirect attacks,
highlighting a critical gap in current LLM safety alignment for sensitive
applications. Our analysis reveals that while models can often identify the
correct answer to a query, they struggle to incorporate policy constraints
during generation. In contrast, they exhibit a partial ability to revise
outputs when explicitly prompted. Our findings underscore the urgent need for
more robust methods to guarantee contextual security.

</details>


### [148] [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/pdf/2505.15807)
*Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin*

Main category: cs.CL

TL;DR: The paper investigates how large language models use in-context learning for retrieval-augmented question answering, identifying specialized attention heads and their roles in knowledge retrieval and answer generation.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear inner workings of in-context retrieval augmentation in large language models for question answering.

Method: Proposes an attribution-based method to identify specialized attention heads (in-context and parametric) and modifies their attention weights to analyze their roles.

Result: Reveals how in-context heads retrieve contextual information and parametric heads store relational knowledge, influencing answer generation.

Conclusion: The insights enable tracing knowledge sources during inference, advancing safer and more transparent language models.

Abstract: Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.

</details>


### [149] [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/pdf/2505.15817)
*Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang*

Main category: cs.CL

TL;DR: The paper introduces Mixture-of-Thought (MoT), a framework for LLMs to reason across natural language, code, and symbolic logic (truth-table), improving logical reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs rely on a single reasoning modality (natural language) during training, missing synergies from multiple modalities.

Method: MoT uses a two-phase design: self-evolving training with filtered, self-generated rationales across modalities, and inference leveraging modality synergy.

Result: MoT outperforms single-modality baselines by up to +11.7pp accuracy on logical reasoning benchmarks like FOLIO and ProofWriter.

Conclusion: MoT enhances both training and inference, excels on harder problems, and combines complementary modality strengths, with truth-tables addressing natural language bottlenecks.

Abstract: Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.

</details>


### [150] [Predicting generalization performance with correctness discriminators](https://arxiv.org/pdf/2311.09422)
*Yuekun Yao, Alexander Koller*

Main category: cs.CL

TL;DR: A novel model predicts upper and lower bounds for NLP model accuracy on unseen data without requiring gold labels, using a discriminator to assess correctness.


<details>
  <summary>Details</summary>
Motivation: To enhance trustworthiness by predicting NLP model accuracy on out-of-distribution data without needing labeled data.

Method: Train a discriminator to predict correctness of sequence-to-sequence model outputs, establishing accuracy bounds.

Result: Gold accuracy consistently falls within predicted bounds, which are closely spaced across tagging, parsing, and semantic tasks.

Conclusion: The method reliably bounds NLP model accuracy, offering a practical tool for assessing trustworthiness.

Abstract: The ability to predict an NLP model's accuracy on unseen, potentially
out-of-distribution data is a prerequisite for trustworthiness. We present a
novel model that establishes upper and lower bounds on the accuracy, without
requiring gold labels for the unseen data. We achieve this by training a
discriminator which predicts whether the output of a given sequence-to-sequence
model is correct or not. We show across a variety of tagging, parsing, and
semantic parsing tasks that the gold accuracy is reliably between the predicted
upper and lower bounds, and that these bounds are remarkably close together.

</details>


### [151] [A Framework for Real-time Safeguarding the Text Generation of Large Language Model](https://arxiv.org/pdf/2404.19048)
*Ximing Dong, Dayi Lin, Shaowei Wang, Ahmed E. Hassan*

Main category: cs.CL

TL;DR: LLMSafeGuard is a lightweight framework for real-time validation of LLM outputs, reducing harmful content while maintaining quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address ethical and societal risks of LLMs by mitigating harmful content generation without degrading output quality or increasing computational costs.

Method: Proposes LLMSafeGuard, integrating an external validator with similarity-based validation and context-wise timing selection to intervene only when necessary.

Result: Reduces toxic output by 38.6% in detoxification, preserves linguistic quality, and cuts inference time by 24.2%.

Conclusion: LLMSafeGuard effectively safeguards LLM outputs with minimal overhead, outperforming existing methods.

Abstract: Large Language Models (LLMs) have significantly advanced natural language
processing (NLP) tasks but also pose ethical and societal risks due to their
propensity to generate harmful content. Existing methods have limitations,
including the need for training specific control models and proactive
intervention during text generation, that lead to quality degradation and
increased computational overhead. To mitigate those limitations, we propose
LLMSafeGuard, a lightweight real-time framework that integrates an external
validator into decoding, rejecting unsafe outputs while allowing valid ones. We
introduce a similarity-based validation approach, simplifying constraint
introduction and eliminating the need for control model training. Additionally,
LLMSafeGuard employs a context-wise timing selection strategy, intervening LLMs
only when necessary. We evaluate LLMSafeGuard on detoxification and copyright
safeguarding, demonstrating its superiority over SOTA baselines. In
detoxification, LLMSafeGuard reduces toxic output by at least 38.6\% while
preserving linguistic quality. Additionally, its context-wise timing selection
cuts inference time by at least 24.2\% without compromising effectiveness.

</details>


### [152] [MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset](https://arxiv.org/pdf/2406.02106)
*Weiqi Wang, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper proposes a three-step discriminative process for reasoning with distributional changes in LLMs, introduces the MARS benchmark, and evaluates 20 models, finding significant challenges even for state-of-the-art LLMs.


<details>
  <summary>Details</summary>
Motivation: To enable LLMs to function as conscious agents with generalizable reasoning, understanding situational transitions is crucial but underexplored due to complexity and lack of benchmark data.

Method: A novel three-step discriminative process (MetAphysical ReaSoning) and the MARS benchmark with three tasks to assess LLMs' reasoning on action, state, and situational transitions.

Result: All three tasks pose significant challenges for current LLMs, even after fine-tuning. Pre-training on conceptualization taxonomies may improve performance.

Conclusion: The study highlights the difficulty of metaphysical reasoning in LLMs and suggests potential improvements through pre-training. The MARS benchmark and models are publicly available.

Abstract: To enable Large Language Models (LLMs) to function as conscious agents with
generalizable reasoning capabilities, it is crucial that they possess the
reasoning ability to comprehend situational changes (transitions) in
distribution triggered by environmental factors or actions from other agents.
Despite its fundamental significance, this ability remains underexplored due to
the complexity of modeling infinite possible changes in an event and their
associated distributions, coupled with the lack of benchmark data with
situational transitions. Addressing these gaps, we propose a novel formulation
of reasoning with distributional changes as a three-step discriminative
process, termed as MetAphysical ReaSoning. We then introduce the first-ever
benchmark, MARS, comprising three tasks corresponding to each step. These tasks
systematically assess LLMs' capabilities in reasoning the plausibility of (i)
changes in actions, (ii) states caused by changed actions, and (iii)
situational transitions driven by changes in action. Extensive evaluations with
20 (L)LMs of varying sizes and methods indicate that all three tasks in this
process pose significant challenges, even for state-of-the-art LLMs and LMs
after fine-tuning. Further analyses reveal potential causes for the
underperformance of LLMs and demonstrate that pre-training them on large-scale
conceptualization taxonomies can potentially enhance their metaphysical
reasoning capabilities. Our data and models are publicly accessible at
https://github.com/HKUST-KnowComp/MARS.

</details>


### [153] [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/pdf/2406.12719)
*Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao*

Main category: cs.CL

TL;DR: LLMs perform well on table comprehension tasks without specific training, but issues like data contamination and reliability persist, especially in Wikipedia-based tasks. Attention analysis shows performance drops correlate with perturbation-induced attention shifts.


<details>
  <summary>Details</summary>
Motivation: To understand how in-context learning, model scale, instruction tuning, and domain bias affect the robustness of LLMs in Tabular QA tasks.

Method: Tested LLMs on diverse domains (WTQ, TAT-QA, SCITAB) under various augmentations and perturbations, analyzing attention shifts and performance.

Result: Instruction tuning and larger LLMs improve performance, but data contamination and reliability issues remain. Attention shifts in middle layers correlate with performance drops.

Conclusion: Improved interpretable methodologies are needed to develop more reliable LLMs for table comprehension.

Abstract: Large Language Models (LLMs), already shown to ace various text comprehension
tasks, have also remarkably been shown to tackle table comprehension tasks
without specific training. Building on earlier studies of LLMs for tabular
tasks, we probe how in-context learning (ICL), model scale, instruction tuning,
and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under
diverse augmentations and perturbations, on diverse domains: Wikipedia-based
$\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$.
Although instruction tuning and larger, newer LLMs deliver stronger, more
robust TQA performance, data contamination and reliability issues, especially
on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,
we reveal a strong correlation between perturbation-induced shifts in attention
dispersion and the drops in performance, with sensitivity peaking in the
model's middle layers. We highlight the need for improved interpretable
methodologies to develop more reliable LLMs for table comprehension.

</details>


### [154] [Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior](https://arxiv.org/pdf/2407.02099)
*Pedro Henrique Luz de Araujo, Benjamin Roth*

Main category: cs.CL

TL;DR: The paper explores how assigning personas to LLMs affects their behavior across diverse tasks, showing greater variability compared to control settings.


<details>
  <summary>Details</summary>
Motivation: To understand how personas influence LLM behavior in both objective and subjective tasks.

Method: Seven LLMs were assigned 162 personas from 12 categories, tested on five datasets, and compared to control and empty persona settings.

Result: Personas caused greater variability in model behavior than controls, with some behaviors generalizing across models.

Conclusion: Personas significantly impact LLM behavior, offering a way to steer and personalize model outputs.

Abstract: One way to personalize and steer generations from large language models (LLM)
is to assign a persona: a role that describes how the user expects the LLM to
behave (e.g., a helpful assistant, a teacher, a woman). This paper investigates
how personas affect diverse aspects of model behavior. We assign to seven LLMs
162 personas from 12 categories spanning variables like gender, sexual
orientation, and occupation. We prompt them to answer questions from five
datasets covering objective (e.g., questions about math and history) and
subjective tasks (e.g., questions about beliefs and values). We also compare
persona's generations to two baseline settings: a control persona setting with
30 paraphrases of "a helpful assistant" to control for models' prompt
sensitivity, and an empty persona setting where no persona is assigned. We find
that for all models and datasets, personas show greater variability than the
control setting and that some measures of persona behavior generalize across
models.

</details>


### [155] [A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting](https://arxiv.org/pdf/2407.11638)
*He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' capabilities in temporal event forecasting, introduces a benchmark dataset (MidEast-TE-mini), and explores methods like fine-tuning and RAG. Findings show fine-tuning improves performance, while raw text integration does not. RAG helps capture temporal patterns but suffers from biases.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' under-explored reasoning capability in temporal event forecasting and address the lack of a high-quality dataset for such tasks.

Method: Constructs the MidEast-TE-mini dataset, designs baseline methods with varied input formats and RAG modules, and conducts extensive experiments.

Result: Fine-tuning LLMs with raw texts improves performance; raw text integration does not. RAG captures temporal patterns but has issues like popularity bias.

Conclusion: The study deepens understanding of LLM-based event forecasting, identifies research gaps, and provides a foundation for future work.

Abstract: Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation (RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast, fine-tuning
LLMs with raw texts can significantly improve performance. Additionally, LLMs
enhanced with retrieval modules can effectively capture temporal relational
patterns hidden in historical events. However, issues such as popularity bias
and the long-tail problem persist in LLMs, particularly in the
retrieval-augmented generation (RAG) method. These findings not only deepen our
understanding of LLM-based event forecasting methods but also highlight several
promising research directions. We consider that this comprehensive evaluation,
along with the identified research opportunities, will significantly contribute
to future research on temporal event forecasting through LLMs.

</details>


### [156] [Fine-tuning Large Language Models for Entity Matching](https://arxiv.org/pdf/2409.08185)
*Aaron Steiner, Ralph Peeters, Christian Bizer*

Main category: cs.CL

TL;DR: Fine-tuning LLMs for entity matching improves smaller models' performance but has mixed results for larger ones, enhances in-domain generalization, and reduces cross-domain transfer. Structured explanations help most LLMs, while example selection methods benefit only specific models.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of fine-tuning LLMs for entity matching, focusing on training example representation and selection, and assessing generalization across domains.

Method: Fine-tuning LLMs with varied training example representations (including LLM-generated explanations) and example selection/generation methods. Performance and generalization are evaluated.

Result: Fine-tuning boosts smaller models' performance but has mixed results for larger ones. It improves in-domain generalization but harms cross-domain transfer. Structured explanations help most LLMs, while example selection benefits only Llama 3.1 8B.

Conclusion: Fine-tuning LLMs for entity matching is effective for smaller models and in-domain tasks, but cross-domain generalization and larger models require further optimization.

Abstract: Generative large language models (LLMs) are a promising alternative to
pre-trained language models for entity matching due to their high zero-shot
performance and ability to generalize to unseen entities. Existing research on
using LLMs for entity matching has focused on prompt engineering and in-context
learning. This paper explores the potential of fine-tuning LLMs for entity
matching. We analyze fine-tuning along two dimensions: 1) the representation of
training examples, where we experiment with adding different types of
LLM-generated explanations to the training set, and 2) the selection and
generation of training examples using LLMs. In addition to the matching
performance on the source dataset, we investigate how fine-tuning affects the
models ability to generalize to other in-domain datasets as well as across
topical domains. Our experiments show that fine-tuning significantly improves
the performance of the smaller models while the results for the larger models
are mixed. Fine-tuning also improves the generalization to in-domain datasets
while hurting cross-domain transfer. We show that adding structured
explanations to the training set has a positive impact on the performance of
three out of four LLMs, while the proposed example selection and generation
methods, only improve the performance of Llama 3.1 8B while decreasing the
performance of GPT-4o-mini.

</details>


### [157] [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/pdf/2410.08109)
*Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin*

Main category: cs.CL

TL;DR: The paper addresses privacy and legal concerns in LLMs by proposing machine unlearning methods. It introduces new evaluation metrics and categorizes unlearning approaches, proposing entropy maximization for untargeted unlearning and answer preservation loss for targeted unlearning, validated in three scenarios.


<details>
  <summary>Details</summary>
Motivation: To mitigate privacy and legal risks from memorized sensitive/copyrighted content in LLMs without costly retraining, the paper explores machine unlearning.

Method: Introduces three metrics for evaluation, categorizes unlearning into untargeted and targeted, and proposes entropy maximization (ME) for untargeted and answer preservation (AP) loss for targeted unlearning.

Result: Experiments in fictitious, continual, and real-world unlearning scenarios show the proposed methods' effectiveness.

Conclusion: The paper provides insights and practical solutions for machine unlearning in LLMs, validated by experimental results.

Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content,
raising privacy and legal concerns. Due to the high cost of retraining from
scratch, researchers attempt to employ machine unlearning to remove specific
content from LLMs while preserving the overall performance. In this paper, we
discuss several issues in machine unlearning for LLMs and provide our insights
on possible approaches. To address the issue of inadequate evaluation of model
outputs after unlearning, we introduce three additional metrics to evaluate
token diversity, sentence semantics, and factual correctness. We then
categorize unlearning methods into untargeted and targeted, and discuss their
issues respectively. Specifically, the behavior that untargeted unlearning
attempts to approximate is unpredictable and may involve hallucinations, and
existing regularization is insufficient for targeted unlearning. To alleviate
these issues, we propose using the objective of maximizing entropy (ME) for
untargeted unlearning and incorporate answer preservation (AP) loss as
regularization for targeted unlearning. Experimental results across three
scenarios, i.e., fictitious unlearning, continual unlearning, and real-world
unlearning, demonstrate the effectiveness of our approaches. The code is
available at https://github.com/sail-sg/closer-look-LLM-unlearning.

</details>


### [158] [Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception](https://arxiv.org/pdf/2410.12788)
*Jihao Zhao, Zhiyuan Ji, Yuchen Feng, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li*

Main category: cs.CL

TL;DR: The paper introduces Meta-Chunking, a framework to improve text chunking in RAG systems by optimizing segmentation and preserving global information, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems neglect text chunking quality, limiting LLM performance in knowledge-intensive tasks.

Method: Proposes adaptive chunking techniques (Perplexity Chunking, Margin Sampling Chunking) and dynamic merging, alongside global information compensation mechanisms.

Result: Meta-Chunking enhances semantic integrity and contextual coherence of chunks, reducing reliance on large models.

Conclusion: The framework effectively improves chunking in RAG systems and demonstrates feasibility with smaller models.

Abstract: While Retrieval-Augmented Generation (RAG) has emerged as a promising
paradigm for boosting large language models (LLMs) in knowledge-intensive
tasks, it often overlooks the crucial aspect of text chunking within its
workflow. This paper proposes the Meta-Chunking framework, which specifically
enhances chunking quality through a dual strategy that identifies optimal
segmentation points and preserves global information. Initially, breaking
limitations of similarity-based chunking, we design two adaptive chunking
techniques based on uncertainty, namely Perplexity Chunking and Margin Sampling
Chunking, by utilizing the logical perception capabilities of LLMs. Given the
inherent complexity across different texts, we integrate meta-chunk with
dynamic merging, striking a balance between fine-grained and coarse-grained
text chunking. Furthermore, we establish the global information compensation
mechanism, encompassing a two-stage hierarchical summary generation process and
a three-stage text chunk rewriting procedure focused on missing reflection,
refinement, and completion. These components collectively strengthen the
semantic integrity and contextual coherence of chunks. Extensive experiments
demonstrate that Meta-Chunking effectively addresses challenges of the chunking
task within the RAG system, providing LLMs with more logically coherent text
chunks. Additionally, our methodology validates the feasibility of implementing
high-quality chunking tasks with smaller-scale models, thereby eliminating the
reliance on robust instruction-following capabilities.

</details>


### [159] [Retrospective Learning from Interactions](https://arxiv.org/pdf/2410.13852)
*Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi*

Main category: cs.CL

TL;DR: ReSpect improves LLM task completion by learning from implicit user feedback in multi-turn interactions, boosting performance from 31% to 82% without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Leverage implicit user feedback (e.g., rephrasing, frustration) in multi-turn LLM interactions to improve performance without additional labeling.

Method: Introduces ReSpect, a method to learn from implicit feedback signals in past interactions via retrospection, applied in a multimodal LLM scenario.

Result: Task completion rate improved from 31% to 82% through thousands of human interactions.

Conclusion: ReSpect effectively enhances LLM performance by utilizing implicit feedback, eliminating the need for external annotations.

Abstract: Multi-turn interactions between large language models (LLMs) and users
naturally include implicit feedback signals. If an LLM responds in an
unexpected way to an instruction, the user is likely to signal it by rephrasing
the request, expressing frustration, or pivoting to an alternative task. Such
signals are task-independent and occupy a relatively constrained subspace of
language, allowing the LLM to identify them even if it fails on the actual
task. We introduce ReSpect, a method to learn from such signals in past
interactions via retrospection without additional annotations. We deploy
ReSpect in a new multimodal interaction scenario, where humans instruct a
multimodal LLM to solve an abstract reasoning task with a combinatorial
solution space. Through thousands of interactions with humans, we show how
ReSpect gradually improves task completion rate from 31% to 82%, all without
any external annotation.

</details>


### [160] [GATEAU: Selecting Influential Samples for Long Context Alignment](https://arxiv.org/pdf/2410.15633)
*Shuzheng Si, Haozhe Zhao, Gang Chen, Yunshui Li, Kangyang Luo, Chuancheng Lv, Kaikai An, Fanchao Qi, Baobao Chang, Maosong Sun*

Main category: cs.CL

TL;DR: GATEAU is a framework for improving long-context alignment in large language models by identifying high-quality samples with long-range dependencies.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models for long-context instructions is underexplored, and existing methods risk including low-quality data.

Method: GATEAU measures long-range dependencies via response generation and input understanding difficulties to select influential samples.

Result: Models trained on GATEAU-selected samples show better instruction-following and long-context understanding.

Conclusion: GATEAU effectively enhances long-context alignment by focusing on high-quality, dependency-rich samples.

Abstract: Aligning large language models to handle instructions with extremely long
contexts has yet to be fully investigated. Previous studies have attempted to
scale up the available data volume by synthesizing long instruction-following
samples, as constructing such a dataset tends to be challenging for annotators.
However, a lack of a well-defined strategy for ensuring data quality may
introduce low-quality samples and restrict the model's performance. Thus, we
propose GATEAU, a novel framework to address the unique challenge of long
context alignment by identifying the influential samples enriched with
long-range dependency relations. Specifically, GATEAU measures the long-range
dependencies from two essential aspects: the difficulty of generating target
responses due to the long-range dependencies, and the difficulty of
understanding long inputs due to such dependencies. Comprehensive experiments
indicate that GATEAU effectively identifies influential samples and the model
trained on these selected samples exhibits better instruction-following and
long-context understanding capabilities.

</details>


### [161] [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/pdf/2410.16168)
*Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram*

Main category: cs.CL

TL;DR: Proposes active forgetting pretraining for decoder-only LLMs to improve cross-lingual transfer, showing better multilingual performance.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in English but lag in other languages; encoder-only models like BERT show cross-lingual transfer, but decoder-only LLMs need similar capabilities.

Method: Introduces active forgetting pretraining strategy for decoder-only LLMs to enhance cross-lingual transfer.

Result: LLMs with active forgetting learn better multilingual representations, improving performance in downstream tasks.

Conclusion: Active forgetting pretraining enables effective cross-lingual transfer in decoder-only LLMs, enhancing multilingual NLP tasks.

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities in a
multitude of NLP tasks. However, the efficacy of such models to languages other
than English is often limited. Prior works have shown that encoder-only models
such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their
capabilities from English to other languages. In this work, we propose a
pretraining strategy that uses active forgetting to achieve similar cross
lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active
forgetting are highly effective when adapting to new and unseen languages.
Through extensive experimentation, we find that LLMs pretrained with active
forgetting are able to learn better multilingual representations which
translates to better performance in many downstream tasks.

</details>


### [162] [Robust and Minimally Invasive Watermarking for EaaS](https://arxiv.org/pdf/2410.17552)
*Zongqi Wang, Baoyuan Wu, Jingyuan Deng, Yujiu Yang*

Main category: cs.CL

TL;DR: The paper proposes ESpeW, a novel embedding-specific watermarking mechanism for robust copyright protection in EaaS, addressing vulnerabilities in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for EaaS are easily removable due to shared components in watermarked embeddings, necessitating a more robust solution.

Method: ESpeW injects unique, identifiable watermarks into each embedding, avoiding common components and maintaining distance between watermarks.

Result: ESpeW reduces impact on embeddings to less than 1% and resists aggressive removal attacks while preserving embedding quality.

Conclusion: ESpeW sets a new benchmark for robust watermarking in EaaS, offering effective copyright protection without compromising performance.

Abstract: Embeddings as a Service (EaaS) is emerging as a crucial role in AI
applications. Unfortunately, EaaS is vulnerable to model extraction attacks,
highlighting the urgent need for copyright protection. Although some
preliminary works propose applying embedding watermarks to protect EaaS, recent
research reveals that these watermarks can be easily removed. Hence, it is
crucial to inject robust watermarks resistant to watermark removal attacks.
Existing watermarking methods typically inject a target embedding into
embeddings through linear interpolation when the text contains triggers.
However, this mechanism results in each watermarked embedding having the same
component, which makes the watermark easy to identify and eliminate. Motivated
by this, in this paper, we propose a novel embedding-specific watermarking
(ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach
involves injecting unique, yet readily identifiable watermarks into each
embedding. Watermarks inserted by ESpeW are designed to maintain a significant
distance from one another and to avoid sharing common components, thus making
it significantly more challenging to remove the watermarks. Moreover, ESpeW is
minimally invasive, as it reduces the impact on embeddings to less than 1\%,
setting a new milestone in watermarking for EaaS. Extensive experiments on four
popular datasets demonstrate that ESpeW can even watermark successfully against
a highly aggressive removal strategy without sacrificing the quality of
embeddings.

</details>


### [163] [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/pdf/2410.21728)
*Kangyang Luo, Zichen Ding, Zhenmin Weng, Lingfeng Qiao, Meng Zhao, Xiang Li, Di Yin, Jinlong Shu*

Main category: cs.CL

TL;DR: LBS3 is a novel prompt approach for automatic reasoning in LLMs, inspired by curriculum learning, that improves reasoning performance by recalling easy-to-hard proxy queries and using progressive strategies.


<details>
  <summary>Details</summary>
Motivation: Existing CoT prompting approaches for LLMs require extensive human effort or lack performance improvements, prompting the need for a more automated and effective method.

Method: LBS3 uses curriculum learning to guide LLMs through easy-to-hard proxy queries, employing progressive strategies to generate high-quality prompts and solutions.

Result: Extensive experiments show LBS3 achieves competitive performance in reasoning-intensive tasks compared to SOTA baselines.

Conclusion: LBS3 effectively addresses limitations of CoT prompting, offering a scalable and automated solution for improving LLM reasoning.

Abstract: While Chain of Thought (CoT) prompting approaches have significantly
consolidated the reasoning capabilities of large language models (LLMs), they
still face limitations that require extensive human effort or have performance
needs to be improved. Existing endeavors have focused on bridging these gaps;
however, these approaches either hinge on external data and cannot completely
eliminate manual effort, or they fall short in effectively directing LLMs to
generate high-quality exemplary prompts. To address the said pitfalls, we
propose a novel prompt approach for automatic reasoning named \textbf{LBS3},
inspired by curriculum learning which better reflects human learning habits.
Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries
that are pertinent to the target query. Following this, it invokes a
progressive strategy that utilizes exemplary prompts stemmed from easy-proxy
queries to direct LLMs in solving hard-proxy queries, enabling the high-quality
of the proxy solutions. Finally, our extensive experiments in various
reasoning-intensive tasks with varying open- and closed-source LLMs show that
LBS3 achieves strongly competitive performance compared to the SOTA baselines.

</details>


### [164] [Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains](https://arxiv.org/pdf/2411.07417)
*Katerina Korre, Arianna Muti, Federico Ruggeri, Alberto Barrón-Cedeño*

Main category: cs.CL

TL;DR: A Semantic Componential Analysis (SCA) framework is proposed for cross-cultural hate speech analysis, using a dataset of 493 definitions from 100+ cultures. Zero-shot experiments with LLMs show definition sensitivity in hate speech detection.


<details>
  <summary>Details</summary>
Motivation: Hate speech interpretations vary culturally, necessitating a cross-cultural and cross-domain analysis to understand and standardize definitions.

Method: Proposed SCA framework to analyze hate speech definitions from five domains. Created a dataset of 493 definitions and conducted zero-shot experiments with three LLMs.

Result: Significant variation in definitions across cultures and domains. LLM responses for hate speech detection vary based on definition complexity in prompts.

Conclusion: Definitions of hate speech are culturally influenced and impact detection models. Standardization or cultural adaptation of definitions is needed for accurate detection.

Abstract: Hate speech relies heavily on cultural influences, leading to varying
individual interpretations. For that reason, we propose a Semantic Componential
Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate
speech definitions. We create the first dataset of hate speech definitions
encompassing 493 definitions from more than 100 cultures, drawn from five key
domains: online dictionaries, academic research, Wikipedia, legal texts, and
online platforms. By decomposing these definitions into semantic components,
our analysis reveals significant variation across definitions, yet many domains
borrow definitions from one another without taking into account the target
culture. We conduct zero-shot model experiments using our proposed dataset,
employing three popular open-sourced LLMs to understand the impact of different
definitions on hate speech detection. Our findings indicate that LLMs are
sensitive to definitions: responses for hate speech detection change according
to the complexity of definitions used in the prompt.

</details>


### [165] [FastDraft: How to Train Your Draft](https://arxiv.org/pdf/2411.11055)
*Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh*

Main category: cs.CL

TL;DR: FastDraft introduces an efficient method to pre-train and align draft models for large language models, achieving significant speedups in inference tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of efficient draft models for many language models due to vocabulary compatibility constraints.

Method: Incorporates efficient pre-training and fine-tuning over synthetic datasets generated by the target model.

Result: Achieves up to 3x speedup in code completion and 2x in summarization, text completion, and instruction tasks.

Conclusion: FastDraft enables efficient large language model inference on edge devices, reducing runtime significantly.

Abstract: Speculative Decoding has gained popularity as an effective technique for
accelerating the auto-regressive inference process of Large Language Models.
However, Speculative Decoding entirely relies on the availability of efficient
draft models, which are often lacking for many existing language models due to
a stringent constraint of vocabulary compatibility. In this work we introduce
FastDraft, a novel and efficient approach for pre-training and aligning a draft
model to any large language model by incorporating efficient pre-training,
followed by fine-tuning over synthetic datasets generated by the target model.
We demonstrate FastDraft by training two highly parameter efficient drafts for
the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able
to produce a draft model with approximately 10 billion tokens on a single
server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24
hours. Our results show that the draft model achieves impressive results in key
metrics of acceptance rate, block efficiency and up to 3x memory bound speed up
when evaluated on code completion and up to 2x in summarization, text
completion and instruction tasks. We validate our theoretical findings through
benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra,
achieving a wall-clock time speedup of up to 2x, indicating a significant
reduction in runtime. Due to its high quality, FastDraft unlocks large language
models inference on AI-PC and other edge-devices.

</details>


### [166] [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/pdf/2412.09049)
*Mengze Hong, Wailing Ng, Chen Jason Zhang, Yuanfeng Song, Di Jiang*

Main category: cs.CL

TL;DR: The paper proposes an LLM-in-the-loop framework for intent clustering in customer service dialogues, outperforming baselines with high accuracy and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing intent clustering methods often misalign with human perceptions due to reliance on embedding metrics and neglect of semantic structures.

Method: Integrates LLMs for semantic coherence evaluation and intent naming, designs an iterative clustering framework, and introduces context-aware techniques.

Result: Achieves over 95% accuracy in semantic coherence and intent naming, outperforms LLM-guided baselines, and introduces a new Chinese dialogue intent dataset.

Conclusion: LLM-in-the-loop techniques show promise for scalable, human-aligned intent clustering, supported by best practices.

Abstract: Discovering customer intentions in dialogue conversations is crucial for
automated service agents. However, existing intent clustering methods often
fail to align with human perceptions due to a heavy reliance on embedding
distance metrics and a tendency to overlook underlying semantic structures.
This paper proposes an LLM-in-the-loop (LLM-ITL) intent clustering framework,
integrating the semantic understanding capabilities of LLMs into conventional
clustering algorithms. Specifically, this paper (1) investigates the
effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent
cluster naming, achieving over 95% accuracy aligned with human judgments; (2)
designs an LLM-ITL framework that facilitates the iterative discovery of
coherent intent clusters and the optimal number of clusters; and (3) proposes
context-aware techniques tailored for customer service dialogue. As existing
English benchmarks offer limited semantic diversity and intent groups, we
introduce a comprehensive Chinese dialogue intent dataset, comprising over 100k
real customer service calls and 1,507 human-annotated intent clusters. The
proposed approaches significantly outperform LLM-guided baselines, achieving
notable enhancements in clustering quality and lower computational cost.
Combined with several best practices, our findings highlight the potential of
LLM-in-the-loop techniques for scalable and human-aligned intent clustering.

</details>


### [167] [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/pdf/2412.10138)
*Yang Qin, Chao Chen, Zhihang Fu, Ze Chen, Dezhong Peng, Peng Hu, Jieping Ye*

Main category: cs.CL

TL;DR: ROUTE enhances open-source LLMs for Text2SQL via multitask tuning and collaboration, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current Text2SQL methods rely on closed-source LLMs, limiting practicality. ROUTE aims to improve open-source LLMs' capabilities.

Method: Multi-task SFT with synthetic data and MCP strategy for collaborative SQL-related tasks.

Result: Outperforms latest Text2SQL methods on benchmarks.

Conclusion: ROUTE provides a practical, high-performance solution for Text2SQL using open-source LLMs.

Abstract: Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by
large language models (LLMs), the latest state-of-the-art techniques are still
trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which
limits their applicability in open scenarios. To address this challenge, we
propose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to
improve the comprehensive capabilities of open-source LLMs for Text2SQL,
thereby providing a more practical solution. Our approach begins with
multi-task supervised fine-tuning (SFT) using various synthetic training data
related to SQL generation. Unlike existing SFT-based Text2SQL methods, we
introduced several additional SFT tasks, including schema linking, noise
correction, and continuation writing. Engaging in a variety of SQL generation
tasks enhances the model's understanding of SQL syntax and improves its ability
to generate high-quality SQL queries. Additionally, inspired by the
collaborative modes of LLM agents, we introduce a Multitask Collaboration
Prompting (MCP) strategy. This strategy leverages collaboration across several
SQL-related tasks to reduce hallucinations during SQL generation, thereby
maximizing the potential of enhancing Text2SQL performance through explicit
multitask capabilities. Extensive experiments and in-depth analyses have been
performed on eight open-source LLMs and five widely-used benchmarks. The
results demonstrate that our proposal outperforms the latest Text2SQL methods
and yields leading performance.

</details>


### [168] [DARWIN 1.5: Large Language Models as Materials Science Adapted Learners](https://arxiv.org/pdf/2412.11970)
*Tong Xie, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Shaozhou Wang, Wenjie Zhang, Clara Grazian, Chunyu Kit, Wanli Ouyang, Dongzhan Zhou, Bram Hoex*

Main category: cs.CL

TL;DR: DARWIN 1.5, a large language model for materials science, uses natural language inputs to bypass complex descriptors, improving prediction accuracy and versatility.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on complex descriptors, limiting generalizability and practical applicability in materials discovery.

Method: DARWIN 1.5 integrates 6M material domain papers and 21 experimental datasets, leveraging natural language for flexible property prediction.

Result: The model achieves a 59.1% accuracy improvement over LLaMA-7B and outperforms SOTA methods in 8 tasks.

Conclusion: LLMs like DARWIN 1.5 offer a scalable, versatile foundation for materials science.

Abstract: Materials discovery and design aim to find compositions and structures with
desirable properties over highly complex and diverse physical spaces.
Traditional solutions, such as high-throughput simulations or machine learning,
often rely on complex descriptors, which hinder generalizability and
transferability across different material systems. Moreover, These descriptors
may inadequately represent macro-scale material properties, which are
influenced by structural imperfections and compositional variations in
real-world samples, thus limiting their practical applicability. To address
these challenges, we propose DARWIN 1.5, the largest open-source large language
model tailored for materials science. By leveraging natural language as input,
DARWIN eliminates the need for task-specific descriptors and enables a
flexible, unified approach to material property prediction and discovery. Our
approach integrates 6M material domain papers and 21 experimental datasets from
49,256 materials across modalities while enabling cross-task knowledge
transfer. The enhanced model achieves up to 59.1% improvement in prediction
accuracy over the base LLaMA-7B architecture and outperforms SOTA machine
learning approaches across 8 materials design tasks. These results establish
LLMs as a promising foundation for developing versatile and scalable models in
materials science.

</details>


### [169] [Exploring Cross-lingual Latent Transplantation: Mutual Opportunities and Open Challenges](https://arxiv.org/pdf/2412.12686)
*Yangfan Ye, Xiaocheng Feng, Xiachong Feng, Libo Qin, Yichong Huang, Lei Huang, Weitao Ma, Qichen Hong, Zhirui Zhang, Yunfei Lu, Xiaohui Yan, Duyu Tang, Dandan Tu, Bing Qin*

Main category: cs.CL

TL;DR: The paper introduces XTransplant, a framework to enhance multilingual and cultural adaptability in LLMs by transplanting latent activations across languages, showing mutual benefits and highlighting underutilized potential.


<details>
  <summary>Details</summary>
Motivation: Address imbalances in multilingual capabilities and cultural adaptability in LLMs due to English-centric pre-training data.

Method: Proposes XTransplant, a cross-lingual latent transplantation framework, to leverage internalized multilingual knowledge during inference.

Result: XTransplant improves multilingual capability and cultural adaptability, especially for low-resource languages, with attention and feed-forward modules playing distinct roles.

Conclusion: XTransplant reveals underutilized multilingual potential in LLMs, offering insights for advancing cross-lingual interactions.

Abstract: Current large language models (LLMs) often exhibit imbalances in multilingual
capabilities and cultural adaptability, largely attributed to their
English-centric pre-training data. In this paper, we introduce and investigate
a cross-lingual latent transplantation (XTransplant) framework, which aims to
further exploit the model's internalized multilingual knowledge during
inference and examine its effects on the multilingual capability and cultural
adaptability of LLMs. XTransplant framework enables models to harness the
complementary strengths of both English and non-English resources by
transplanting latent activations across languages. Through extensive analysis,
we empirically demonstrate that XTransplant, a form of cross-lingual
interaction, has mutually beneficial effects on the multilingual capability and
cultural adaptability of LLMs, particularly for low-resource languages and
cultures. We further reveal that attention modules play a pivotal role in
supporting multilingual understanding, while feed-forward modules are more
adept at capturing culture-specific knowledge. In addition, we conduct in-depth
analysis of XTransplant's stability, effectiveness, and generalizability. By
probing the upper bound performance of XTransplant, we expose the considerable
underutilization of current LLMs' multilingual potential-a challenge that
remains open. We hope our analysis offers a new lens for advancing
cross-lingual interactions and better leveraging models' internalized
multilingual knowledge.

</details>


### [170] [MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/pdf/2412.15540)
*Zhang Siyue, Xue Yuxiang, Zhang Yiming, Wu Xiaobao, Luu Anh Tuan, Zhao Chen*

Main category: cs.CL

TL;DR: The paper introduces TempRAGEval, a benchmark for time-sensitive QA, and proposes MRAG, a trainless framework that improves retrieval and answer accuracy by decomposing questions and using semantic-temporal hybrid ranking.


<details>
  <summary>Details</summary>
Motivation: Existing methods for time-sensitive QA are either resource-intensive or struggle with temporal reasoning, highlighting the need for a better approach.

Method: The proposed MRAG framework decomposes questions into content and temporal constraints, retrieves and summarizes evidence, and ranks it using semantic-temporal hybrid scoring.

Result: MRAG outperforms baseline retrievers on TempRAGEval, improving both retrieval performance and final answer accuracy.

Conclusion: MRAG offers a practical and effective solution for time-sensitive QA, addressing limitations of existing methods.

Abstract: Understanding temporal relations and answering time-sensitive questions is
crucial yet a challenging task for question-answering systems powered by large
language models (LLMs). Existing approaches either update the parametric
knowledge of LLMs with new facts, which is resource-intensive and often
impractical, or integrate LLMs with external knowledge retrieval (i.e.,
retrieval-augmented generation). However, off-the-shelf retrievers often
struggle to identify relevant documents that require intensive temporal
reasoning. To systematically study time-sensitive question answering, we
introduce the TempRAGEval benchmark, which repurposes existing datasets by
incorporating temporal perturbations and gold evidence labels. As anticipated,
all existing retrieval methods struggle with these temporal reasoning-intensive
questions. We further propose Modular Retrieval (MRAG), a trainless framework
that includes three modules: (1) Question Processing that decomposes question
into a main content and a temporal constraint; (2) Retrieval and Summarization
that retrieves evidence and uses LLMs to summarize according to the main
content; (3) Semantic-Temporal Hybrid Ranking that scores each evidence
summarization based on both semantic and temporal relevance. On TempRAGEval,
MRAG significantly outperforms baseline retrievers in retrieval performance,
leading to further improvements in final answer accuracy.

</details>


### [171] [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/pdf/2412.17034)
*Lang Gao, Jiahui Geng, Xiangliang Zhang, Preslav Nakov, Xiuying Chen*

Main category: cs.CL

TL;DR: The paper analyzes jailbreaking in LLMs, identifies the 'safety boundary' concept, and proposes a defense method (ABD) that achieves high effectiveness with minimal impact on model performance.


<details>
  <summary>Details</summary>
Motivation: Jailbreaking in LLMs poses security risks, but understanding and defending against it remains insufficient. The study aims to clarify jailbreaking mechanisms and develop effective defenses.

Method: The authors analyze seven jailbreak methods, introduce the 'safety boundary' concept, and propose Activation Boundary Defense (ABD) with Bayesian optimization for selective layer application.

Result: ABD achieves over 98% defense success rate (DSR) against jailbreak attacks with less than 2% impact on model capabilities. Low and middle layers are critical for jailbreak shifts.

Conclusion: The study provides insights into jailbreaking mechanisms and introduces ABD, a highly effective defense method with minimal performance trade-offs.

Abstract: Jailbreaking in Large Language Models (LLMs) is a major security concern as
it can deceive LLMs to generate harmful text. Yet, there is still insufficient
understanding of how jailbreaking works, which makes it hard to develop
effective defense strategies. We aim to shed more light into this issue: we
conduct a detailed large-scale analysis of seven different jailbreak methods
and find that these disagreements stem from insufficient observation samples.
In particular, we introduce \textit{safety boundary}, and we find that
jailbreaks shift harmful activations outside that safety boundary, where LLMs
are less sensitive to harmful information. We also find that the low and the
middle layers are critical in such shifts, while deeper layers have less
impact. Leveraging on these insights, we propose a novel defense called
\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the
activations within the safety boundary. We further use Bayesian optimization to
selectively apply the defense method to the low and the middle layers. Our
experiments on several benchmarks show that ABD achieves an average DSR of over
98\% against various forms of jailbreak attacks, with less than 2\% impact on
the model's general capabilities.

</details>


### [172] [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/pdf/2501.05714)
*Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang*

Main category: cs.CL

TL;DR: A review of human-model cooperation in NLP, introducing a taxonomy and discussing challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the evolution of LLMs into autonomous agents and their cooperation with humans, summarizing principles and challenges.

Method: Presents a thorough review, introduces a new taxonomy, and discusses frontier areas.

Result: A unified perspective on existing approaches and identification of open challenges.

Conclusion: The paper serves as an entry point for future research in human-model cooperation.

Abstract: With the advancement of large language models (LLMs), intelligent models have
evolved from mere tools to autonomous agents with their own goals and
strategies for cooperating with humans. This evolution has birthed a novel
paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable
progress in numerous NLP tasks in recent years. In this paper, we take the
first step to present a thorough review of human-model cooperation, exploring
its principles, formalizations, and open challenges. In particular, we
introduce a new taxonomy that provides a unified perspective to summarize
existing approaches. Also, we discuss potential frontier areas and their
corresponding challenges. We regard our work as an entry point, paving the way
for more breakthrough research in this regard.

</details>


### [173] [Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter](https://arxiv.org/pdf/2501.14491)
*Verena Blaschke, Masha Fedzechkina, Maartje ter Hoeve*

Main category: cs.CL

TL;DR: The paper explores cross-lingual transfer for 263 languages across three NLP tasks, analyzing how linguistic similarity impacts performance based on task, input representations, and similarity definitions.


<details>
  <summary>Details</summary>
Motivation: To address the unclear best strategy for selecting cross-lingual data in low-resource NLP tasks, given prior research's limited scope in languages and tasks.

Method: Analyzed cross-lingual transfer for 263 diverse languages and three tasks (POS tagging, dependency parsing, topic classification), examining linguistic similarity's role.

Result: Found that linguistic similarity's effect on transfer performance varies with the NLP task, input representations (mono-/multilingual), and how similarity is defined.

Conclusion: The impact of linguistic similarity on cross-lingual transfer is context-dependent, highlighting the need for tailored strategies based on task and language characteristics.

Abstract: Cross-lingual transfer is a popular approach to increase the amount of
training data for NLP tasks in a low-resource context. However, the best
strategy to decide which cross-lingual data to include is unclear. Prior
research often focuses on a small set of languages from a few language families
and/or a single task. It is still an open question how these findings extend to
a wider variety of languages and tasks. In this work, we analyze cross-lingual
transfer for 263 languages from a wide variety of language families. Moreover,
we include three popular NLP tasks: POS tagging, dependency parsing, and topic
classification. Our findings indicate that the effect of linguistic similarity
on transfer performance depends on a range of factors: the NLP task, the (mono-
or multilingual) input representations, and the definition of linguistic
similarity.

</details>


### [174] [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/pdf/2502.00299)
*Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu*

Main category: cs.CL

TL;DR: ChunkKV introduces semantic chunk-based KV cache compression for LLMs, preserving context and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing compression methods fragment context by focusing on individual tokens, degrading performance.

Method: ChunkKV treats semantic chunks as compression units and uses layer-wise index reuse.

Result: Outperforms state-of-the-art by 8.7% in precision with 26.5% higher throughput.

Conclusion: Semantic-aware compression enhances efficiency and performance for long-context LLM inference.

Abstract: Large Language Models (LLMs) require significant GPU memory when processing
long texts, with the key value (KV) cache consuming up to 70\% of total memory
during inference. Although existing compression methods reduce memory by
evaluating the importance of individual tokens, they overlook critical semantic
relationships between tokens, resulting in fragmented context and degraded
performance. We introduce ChunkKV, which fundamentally reimagines KV cache
compression by treating semantic chunks - rather than isolated tokens - as
basic compression units. This approach preserves complete linguistic structures
and contextual integrity, ensuring that essential meaning is retained even
under aggressive compression. Our innovation includes a novel layer-wise index
reuse technique that exploits the higher cross-layer similarity of preserved
indices in ChunkKV, reducing computational overhead and improving throughput by
26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench,
Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV
outperforms state-of-the-art methods by up to 8.7\% in precision while
maintaining the same compression ratio. These results confirm that
semantic-aware compression significantly enhances both efficiency and
performance for long-context LLM inference, providing a simple yet effective
solution to the memory bottleneck problem.

</details>


### [175] [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/pdf/2502.01563)
*Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper identifies concentrated massive values in attention queries (Q) and keys (K) in transformer-based LLMs, linking them to contextual knowledge interpretation. It shows these values are critical for performance and traces their origin to Rotary Positional Encoding (RoPE).


<details>
  <summary>Details</summary>
Motivation: To understand the role of massive values in Q and K layers of LLMs and their impact on contextual knowledge interpretation.

Method: Extensive experiments on modern transformer-based LLMs, analyzing attention patterns and investigating quantization strategies.

Result: Massive values in Q and K are crucial for contextual knowledge tasks, and their absence degrades performance. RoPE causes these values to emerge early in the model.

Conclusion: The findings clarify Q and K's role in LLMs and provide insights for model optimization, emphasizing the importance of massive values for contextual understanding.

Abstract: Large language models (LLMs) have achieved remarkable success in contextual
knowledge understanding. In this paper, we show that these concentrated massive
values consistently emerge in specific regions of attention queries (Q) and
keys (K) while not having such patterns in values (V) in various modern
transformer-based LLMs (Q, K, and V mean the representations output by the
query, key, and value layers respectively). Through extensive experiments, we
further demonstrate that these massive values play a critical role in
interpreting contextual knowledge (knowledge obtained from the current context
window) rather than in retrieving parametric knowledge stored within the
model's parameters. Our further investigation of quantization strategies
reveals that ignoring these massive values leads to a pronounced drop in
performance on tasks requiring rich contextual understanding, aligning with our
analysis. Finally, we trace the emergence of concentrated massive values and
find that such concentration is caused by Rotary Positional Encoding (RoPE),
which has appeared since the first layers. These findings shed new light on how
Q and K operate in LLMs and offer practical insights for model design and
optimization. The Code is Available at
https://github.com/MingyuJ666/Rope_with_LLM.

</details>


### [176] [Lifelong Knowledge Editing requires Better Regularization](https://arxiv.org/pdf/2502.01636)
*Akshat Gupta, Phudish Prateepamornkul, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: The paper identifies causes of model degradation in knowledge editing for large language models and introduces two regularization techniques to mitigate it, improving scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the degradation of large language models during sequential knowledge editing, which hinders their factuality and performance.

Method: Formalizes locate-then-edit as a two-step fine-tuning process, identifies degradation causes (over-optimization and norm-growth), and proposes MPES and Frobenius norm-constraint regularization.

Result: The techniques reduce editing time by 42-61% and enable scaling to 10,000 edits while mitigating degradation.

Conclusion: Targeted regularization is crucial for effective lifelong knowledge editing in large language models.

Abstract: Knowledge editing is a promising way to improve factuality in large language
models, but recent studies have shown significant model degradation during
sequential editing. In this paper, we formalize the popular locate-then-edit
methods as a two-step fine-tuning process, allowing us to precisely identify
the root cause of this degradation. We show that model degradation occurs due
to (1) over-optimization of internal activations and (2) continuous norm-growth
of edited matrices. To mitigate these issues, we introduce two regularization
techniques: (1) Most-Probable Early Stopping (MPES) and (2) explicit Frobenius
norm-constraint. We demonstrate that applying these simple yet effective
regularization techniques at key points in the editing process can
substantially mitigate model degradation. Combining these regularization
methods enables scaling locate-then-edit methods to 10,000 edits while reducing
editing time by 42-61%. These results show that targeted regularization is
essential for lifelong knowledge editing.

</details>


### [177] [BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation](https://arxiv.org/pdf/2502.01697)
*Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia*

Main category: cs.CL

TL;DR: The paper introduces Base-Refine (BARE), a two-stage method for few-shot synthetic data generation, combining base models' diversity with instruction-tuned models' quality to improve downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data generation methods require large seed sets, which are costly or difficult to curate. The paper explores generating high-quality datasets from just a few examples.

Method: Proposes BARE, a two-stage approach: base models generate diverse outputs, and instruction-tuned models refine them for quality.

Result: BARE-generated datasets significantly improve downstream tasks, e.g., matching state-of-the-art performance with 1,000 samples and outperforming RAFT by 18.4%.

Conclusion: BARE effectively addresses the limitations of few-shot synthetic data generation, offering a practical solution for high-quality, diverse datasets.

Abstract: As the demand for high-quality data in model training grows, researchers and
developers are increasingly generating synthetic data to tune and train LLMs.
However, current data generation methods rely on seed sets containing tens of
thousands of examples to prompt instruction-tuned models. This reliance can be
especially problematic when the curation of high-quality examples is expensive
or difficult. In this paper we explore the novel few-shot synthetic data
generation setting -- generating a high-quality dataset from a few examples. We
show that when working with only a few seed examples, instruction-tuned models
used in current synthetic data methods produce insufficient diversity for
downstream tasks. In contrast, we show that base models without post-training,
largely untapped for synthetic data generation, offer substantially greater
output diversity, albeit with lower instruction following abilities. Leveraging
this insight, we propose Base-Refine (BARE), a novel two-stage method that
combines the diversity of base models with the quality assurance of
instruction-tuned models. BARE excels in few-shot synthetic data generation:
using only 3 seed examples it generates diverse, high-quality datasets that
significantly improve downstream task performance. We show that fine-tuning
Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable
to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,
data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2
1B on GSM8K over data generated by only instruction-models, and an 18.4%
improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method
for RAG data generation.

</details>


### [178] [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/pdf/2502.01941)
*Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, Xiaowen Chu*

Main category: cs.CL

TL;DR: The paper explores the effects of KV cache compression on LLMs' core capabilities, introduces KVFundaBench for evaluation, and proposes ShotKV, a novel compression method with improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the understudied impact of KV cache compression on fundamental LLM capabilities, despite existing methods' success in long-context benchmarks.

Method: The authors present KVFundaBench to evaluate KV cache compression across diverse LLM tasks and propose ShotKV, a compression approach tailored for prefill and decoding phases.

Result: Key findings include task-dependent degradation and model-type robustness. ShotKV improves long-context generation by 9%-18% under aggressive compression.

Conclusion: ShotKV effectively balances compression and performance, offering insights into KV cache compression's nuanced effects on LLMs.

Abstract: This paper investigates an underexplored challenge in large language models
(LLMs): the impact of KV cache compression methods on LLMs' fundamental
capabilities. Although existing methods achieve impressive compression ratios
on long-context benchmarks, their effects on core model capabilities remain
understudied. We present a comprehensive benchmark KVFundaBench to
systematically evaluate the effects of KV cache compression across diverse
fundamental LLM capabilities, spanning world knowledge, commonsense reasoning,
arithmetic reasoning, code generation, safety, and long-context understanding
and generation.Our analysis reveals serval key findings: (1)
\textit{Task-Dependent Degradation}; (2) \textit{Model-Type Robustness} (3)
\textit{Prompt Length Vulnerability}; (4) \textit{Chunk-Level Superiority}; (5)
\textit{Prompt-Gain Sensitivity}; (6) \textit{Long-Context Generation
Sensitivity}. Based on our analysis of attention patterns and cross-task
compression performance, we propose ShotKV, a novel compression approach that
distinctly handles prefill and decoding phases while maintaining shot-level
semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$
performance improvements on long-context generation tasks under aggressive
compression ratios.

</details>


### [179] [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/pdf/2502.04295)
*Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Peng Cheng*

Main category: cs.CL

TL;DR: CFPO optimizes both prompt content and formatting for LLMs, showing better performance than content-only methods.


<details>
  <summary>Details</summary>
Motivation: The role of prompt formatting in LLM performance is understudied, despite its critical impact.

Method: CFPO uses iterative refinement with natural language mutations and dynamic format exploration.

Result: CFPO outperforms content-only optimization in evaluations across tasks and LLMs.

Conclusion: Integrated content-format optimization is crucial for enhancing LLM performance.

Abstract: Large Language Models (LLMs) have shown significant capability across various
tasks, with their real-world effectiveness often driven by prompt design. While
recent research has focused on optimizing prompt content, the role of prompt
formatting, a critical but often overlooked dimension, has received limited
systematic investigation. In this paper, we introduce Content-Format Integrated
Prompt Optimization (CFPO), an innovative methodology that jointly optimizes
both prompt content and formatting through an iterative refinement process.
CFPO leverages natural language mutations to explore content variations and
employs a dynamic format exploration strategy that systematically evaluates
diverse format options. Our extensive evaluations across multiple tasks and
open-source LLMs demonstrate that CFPO demonstrates measurable performance
improvements compared to content-only optimization methods. This highlights the
importance of integrated content-format optimization and offers a practical,
model-agnostic approach to enhancing LLM performance. Code is available at
https://github.com/HenryLau7/CFPO.

</details>


### [180] [An Analysis for Reasoning Bias of Language Models with Small Initialization](https://arxiv.org/pdf/2502.04375)
*Junjie Yao, Zhongwang Zhang, Zhi-Qin John Xu*

Main category: cs.CL

TL;DR: The study explores how parameter initialization scale affects LLM training, finding smaller scales favor reasoning tasks and larger scales favor memorization. Theoretical and experimental evidence supports these findings.


<details>
  <summary>Details</summary>
Motivation: To understand how initialization scales influence LLM behavior and task preferences, enhancing model training strategies.

Method: Analyzed initialization scales' impact via real datasets, anchor functions, and training dynamics, focusing on embedding and self-attention mechanisms.

Result: Smaller initialization scales bias models toward reasoning tasks, while larger scales favor memorization, validated by experiments.

Conclusion: Initialization scales significantly shape LLM learning biases, offering practical guidelines for optimizing model training.

Abstract: Transformer-based Large Language Models (LLMs) have revolutionized Natural
Language Processing by demonstrating exceptional performance across diverse
tasks. This study investigates the impact of the parameter initialization scale
on the training behavior and task preferences of LLMs. We discover that smaller
initialization scales encourage models to favor reasoning tasks, whereas larger
initialization scales lead to a preference for memorization tasks. We validate
this reasoning bias via real datasets and meticulously designed anchor
functions. Further analysis of initial training dynamics suggests that specific
model components, particularly the embedding space and self-attention
mechanisms, play pivotal roles in shaping these learning biases. We provide a
theoretical framework from the perspective of model training dynamics to
explain these phenomena. Additionally, experiments on real-world language tasks
corroborate our theoretical insights. This work enhances our understanding of
how initialization strategies influence LLM performance on reasoning tasks and
offers valuable guidelines for training models.

</details>


### [181] [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/pdf/2502.04964)
*Roman Vashurin, Maiya Goloburda, Albina Ilina, Alexander Rubashevskii, Preslav Nakov, Artem Shelmanov, Maxim Panov*

Main category: cs.CL

TL;DR: The paper proposes a novel approach to uncertainty quantification (UQ) in Large Language Models (LLMs) by integrating model confidence and output consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods for LLMs, combining information-based and consistency-based approaches, sometimes underperform simpler baselines, prompting a need for more effective measures.

Method: The study links uncertainty with minimum Bayes risks in LLM decoding and synthesizes model confidence with output consistency to create efficient UQ methods.

Result: The proposed methods show significant improvements over state-of-the-art UQ approaches in tasks like question answering, summarization, and translation.

Conclusion: The findings highlight unique LLM characteristics affecting UQ performance and introduce robust methods for better uncertainty quantification.

Abstract: Uncertainty quantification (UQ) methods for Large Language Models (LLMs)
encompass a variety of approaches, with two major types being particularly
prominent: information-based, which focus on model confidence expressed as
token probabilities, and consistency-based, which assess the semantic
relationship between multiple outputs generated using repeated sampling.
Several recent methods have combined these two approaches to boost UQ
performance. However, they sometimes fail to outperform much simpler baseline
methods. Our work discusses the fundamental approach to constructing
uncertainty measures that directly links uncertainty with the minimum Bayes
risks achieved by LLM decoding. Building on these findings, we propose a novel
approach to integrating model confidence with output consistency, resulting in
a family of efficient and robust UQ methods. Our investigation reveals
distinctive characteristics of LLMs as probabilistic models, which help to
explain why these UQ methods underperform in certain tasks. Based on these
findings, we propose a new way of synthesizing model confidence and output
consistency, leading to a family of efficient and robust UQ methods. We
evaluate our approach across various tasks such as question answering,
abstractive summarization, and machine translation, demonstrating sizable
improvements over state-of-the-art UQ approaches.

</details>


### [182] [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/pdf/2502.07316)
*Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He*

Main category: cs.CL

TL;DR: CodeI/O improves reasoning in LLMs by converting code into input-output prediction tasks, training models with natural language CoT rationales, and exposing them to universal reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of sparse and fragmented training data for diverse reasoning tasks in LLMs.

Method: Transforms code into input-output prediction format, trains models with natural language CoT rationales, and decouples reasoning from code syntax.

Result: Consistent improvements across symbolic, scientific, logic, math, and commonsense reasoning tasks.

Conclusion: CodeI/O and its enhanced version, CodeI/O++, effectively enhance reasoning capabilities in LLMs.

Abstract: Reasoning is a fundamental capability of Large Language Models. While prior
research predominantly focuses on enhancing narrow skills like math or code
generation, improving performance on many other reasoning tasks remains
challenging due to sparse and fragmented training data. To address this issue,
we propose CodeI/O, a novel approach that systematically condenses diverse
reasoning patterns inherently embedded in contextually-grounded codes, through
transforming the original code into a code input-output prediction format. By
training models to predict inputs/outputs given code and test cases entirely in
natural language as Chain-of-Thought (CoT) rationales, we expose them to
universal reasoning primitives -- like logic flow planning, state-space
searching, decision tree traversal, and modular decomposition -- while
decoupling structured reasoning from code-specific syntax and preserving
procedural rigor. Experimental results demonstrate CodeI/O leads to consistent
improvements across symbolic, scientific, logic, math & numerical, and
commonsense reasoning tasks. By matching the existing ground-truth outputs or
re-executing the code with predicted inputs, we can verify each prediction and
further enhance the CoTs through multi-turn revision, resulting in CodeI/O++
and achieving higher performance. Our data and models are available at
https://github.com/hkust-nlp/CodeIO.

</details>


### [183] [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/pdf/2502.11441)
*Hwan Chang, Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper explores how unlearning in large language models (LLMs) affects the retain set, particularly focusing on syntactically similar queries, and finds them most impacted. Using this subset for regularization improves performance.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise from LLMs retaining sensitive data; unlearning aims to remove such data without harming overall performance. Existing work lacks analysis of the retain set's behavior during unlearning.

Method: A case study on entity unlearning investigates the retain set's subsets, introducing the Syntactically Similar Neighbor Set to analyze performance drops and regularization effects.

Result: Syntactically similar queries suffer the most during unlearning but, when used for regularization, preserve or improve performance across subsets.

Conclusion: Syntactic similarity is a key factor in effective LLM unlearning, outweighing domain or entity relationships.

Abstract: Large language models (LLMs) risk retaining unauthorized or sensitive
information from their training data, which raises privacy concerns. LLM
unlearning seeks to mitigate these risks by selectively removing specified data
while maintaining overall model performance. However, most existing work focus
on methods to achieve effective forgetting and does not provide a detailed
analysis of the retain set, the portion of training data that is not targeted
for removal. In this paper, we investigate the effects of unlearning on various
subsets of the retain set through a case study on entity unlearning. We
introduce the Syntactically Similar Neighbor Set, a group of queries that share
similar syntactic structures with the data targeted for removal, and show that
this subset suffers the greatest performance drop during unlearning. Moreover,
when used for regularization, this set not only preserves performance on
syntactically similar queries but also delivers comparable or improved results
across other data subsets. Our results highlight that syntactic similarity is a
critical factor, potentially more so than domain or entity relationships, in
achieving effective and practical LLM unlearning.

</details>


### [184] [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/pdf/2502.11471)
*Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun*

Main category: cs.CL

TL;DR: GLTW enhances Knowledge Graph Completion (KGC) by integrating structural KG information into LLMs using an improved Graph Transformer and subgraph-based training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating KG structural information into LLMs for deterministic KGC predictions.

Method: Proposes GLTW, combining an improved Graph Transformer (iGT) with LLMs, using subgraph-based multi-classification training.

Result: GLTW outperforms state-of-the-art baselines on various KG datasets.

Conclusion: GLTW effectively merges KG structure with LLMs, improving KGC performance.

Abstract: Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.

</details>


### [185] [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/pdf/2502.12464)
*Seanie Lee, Dong Bok Lee, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang*

Main category: cs.CL

TL;DR: SafeRoute is a binary router that selectively uses a large safety guard model for hard examples, improving efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large safety guard models are computationally expensive, and smaller distilled models underperform on hard examples. SafeRoute addresses this by routing only hard examples to the larger model.

Method: Proposes SafeRoute, a binary router to distinguish hard examples from easy ones, applying the larger model selectively.

Result: Experimental results show SafeRoute improves the trade-off between computational cost and safety performance, outperforming baselines.

Conclusion: SafeRoute efficiently balances computational cost and accuracy by adaptive model selection.

Abstract: Deploying large language models (LLMs) in real-world applications requires
robust safety guard models to detect and block harmful user prompts. While
large safety guard models achieve strong performance, their computational cost
is substantial. To mitigate this, smaller distilled models are used, but they
often underperform on "hard" examples where the larger model provides accurate
predictions. We observe that many inputs can be reliably handled by the smaller
model, while only a small fraction require the larger model's capacity.
Motivated by this, we propose SafeRoute, a binary router that distinguishes
hard examples from easy ones. Our method selectively applies the larger safety
guard model to the data that the router considers hard, improving efficiency
while maintaining accuracy compared to solely using the larger safety guard
model. Experimental results on multiple benchmark datasets demonstrate that our
adaptive model selection significantly enhances the trade-off between
computational cost and safety performance, outperforming relevant baselines.

</details>


### [186] [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/pdf/2502.13195)
*Leonie Weissweiler, Kyle Mahowald, Adele Goldberg*

Main category: cs.CL

TL;DR: The paper argues that LMs' failure to follow symbolic rules may not be a flaw, as natural languages are not rule-based but rely on flexible, context-dependent constructions.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that natural languages are generated by symbolic rules and to reconsider how LMs' performance is evaluated.

Method: The paper critiques current linguistic evaluations of LMs and proposes a shift in perspective, emphasizing the non-rule-based nature of natural languages.

Result: LMs' deviations from symbolic rules may align with the flexible, context-dependent nature of human language.

Conclusion: Researchers should develop benchmarks that reflect the flexible generalizations of natural languages, rather than strict symbolic rules.

Abstract: Linguistic evaluations of how well LMs generalize to produce or understand
novel text often implicitly take for granted that natural languages are
generated by symbolic rules. Grammaticality is thought to be determined by
whether sentences obey such rules. Interpretation is believed to be
compositionally generated by syntactic rules operating on meaningful words.
Semantic parsing is intended to map sentences into formal logic. Failures of
LMs to obey strict rules have been taken to reveal that LMs do not produce or
understand language like humans. Here we suggest that LMs' failures to obey
symbolic rules may be a feature rather than a bug, because natural languages
are not based on rules. New utterances are produced and understood by a
combination of flexible, interrelated, and context-dependent constructions. We
encourage researchers to reimagine appropriate benchmarks and analyses that
acknowledge the rich, flexible generalizations that comprise natural languages.

</details>


### [187] [FineEdit: Unlock Instruction-Based Text Editing for LLMs](https://arxiv.org/pdf/2502.13358)
*Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu*

Main category: cs.CL

TL;DR: FineEdit, a specialized model for precise text editing, outperforms state-of-the-art LLMs on structured editing tasks, showing significant improvements in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with precise, instruction-driven text edits in specialized domains, necessitating a dedicated solution.

Method: Introduces InstrEditBench, a benchmark dataset with 30,000 structured editing tasks, and FineEdit, a model trained for context-aware modifications.

Result: FineEdit achieves 10-40% improvements over leading models like Gemini, Llama-3.2-3B, and Mistral-7B-OpenOrca, and generalizes well to multi-turn edits.

Conclusion: FineEdit addresses LLM limitations in precise text editing, demonstrating superior performance and practical applicability.

Abstract: Large Language Models (LLMs) have significantly advanced natural language
processing, demonstrating strong capabilities in tasks such as text generation,
summarization, and reasoning. Recently, their potential for automating precise
text editing tasks across specialized domains, such as programming code, LaTeX,
and structured database languages, has gained attention. However, current
state-of-the-art LLMs still struggle with executing precise, instruction-driven
edits, particularly when structural accuracy and strict adherence to domain
conventions are required. To address these challenges, we introduce
InstrEditBench, an automated benchmark dataset comprising over 30,000
structured editing tasks spanning diverse domains, including Wikipedia
articles, LaTeX documents, source code, and database languages. Using this
benchmark, we develop FineEdit, a specialized editing model explicitly trained
for accurate, context-aware text modifications. Experimental evaluations
demonstrate that FineEdit outperforms state-of-the-art models, achieving
improvements of approximately 10% over Gemini models on single-turn edits, up
to 30% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over
40% on direct editing tasks. FineEdit also effectively generalizes to realistic
multi-turn editing scenarios, highlighting its practical applicability.

</details>


### [188] [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/pdf/2502.13369)
*Aditya Sharma, Luis Lara, Christopher J. Pal, Amal Zouaq*

Main category: cs.CL

TL;DR: PGMR, a modular framework, enhances SPARQL query generation by retrieving KG elements to reduce hallucinations in LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incorrect KG elements (URIs) due to hallucinations, limiting their real-world IR applications.

Method: PGMR integrates a non-parametric memory module to retrieve KG elements for improving LLM-based SPARQL query generation.

Result: PGMR reduces URI hallucinations significantly and performs well across datasets and LLMs.

Conclusion: PGMR effectively mitigates hallucinations, making LLMs more reliable for SPARQL query generation.

Abstract: The ability to generate SPARQL queries from natural language questions is
crucial for ensuring efficient and accurate retrieval of structured data from
knowledge graphs (KG). While large language models (LLMs) have been widely
adopted for SPARQL query generation, they are often susceptible to
hallucinations and out-of-distribution errors when producing KG elements like
Uniform Resource Identifiers (URIs) based on internal parametric knowledge.
This often results in content that appears plausible but is factually
incorrect, posing significant challenges for their use in real-world
information retrieval (IR) applications. This has led to increased research
aimed at detecting and mitigating such errors. In this paper, we introduce PGMR
(Post-Generation Memory Retrieval), a modular framework that incorporates a
non-parametric memory module to retrieve KG elements and enhance LLM-based
SPARQL query generation. Our experimental results indicate that PGMR
consistently delivers strong performance across diverse datasets, data
distributions, and LLMs. Notably, PGMR significantly mitigates URI
hallucinations, nearly eliminating the problem in several scenarios.

</details>


### [189] [UniKnow: A Unified Framework for Reliable Language Model Behavior across Parametric and External Knowledge](https://arxiv.org/pdf/2502.13648)
*Youna Kim, Hyuhng Joon Kim, Minjoon Choi, Sungmin Cho, Hyunsoo Cho, Sang-goo Lee, Taeuk Kim*

Main category: cs.CL

TL;DR: UniKnow is a framework for reliable language model behavior across parametric and external knowledge, addressing gaps in prior work by evaluating diverse knowledge scenarios like conflict, distraction, and absence.


<details>
  <summary>Details</summary>
Motivation: Prior work on knowledge integration often assumes ideal conditions and lacks coverage of diverse knowledge scenarios, limiting reliability.

Method: Introduces UniKnow, a unified framework for controlled evaluation across knowledge scenarios, and extends it with UniKnow-Aware methods for comprehensive assessment.

Result: Experiments show existing methods struggle with generalization and exhibit biases across varied knowledge configurations.

Conclusion: UniKnow provides a systematic foundation for improving reliability in knowledge utilization under diverse scenarios.

Abstract: Language models often benefit from external knowledge beyond parametric
knowledge. While this combination enhances performance, achieving reliable
knowledge utilization remains challenging, as it requires assessing the state
of each knowledge source based on the presence of relevant information. Yet,
prior work on knowledge integration often overlooks this challenge by assuming
ideal conditions and provides limited coverage of knowledge scenarios. To
address this gap, we introduce UniKnow, a Unified framework for reliable LM
behavior across parametric and external Knowledge. UniKnow enables controlled
evaluation across knowledge scenarios such as knowledge conflict, distraction,
and absence conditions that are rarely addressed together. Beyond evaluating
existing methods under this setting, we extend our work by introducing
UniKnow-Aware methods to support comprehensive evaluation. Experiments on
UniKnow reveal that existing methods struggle to generalize across a broader
range of knowledge configurations and exhibit scenario-specific biases. UniKnow
thus provides a foundation for systematically exploring and improving
reliability under knowledge scenarios.

</details>


### [190] [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/pdf/2502.14791)
*Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake*

Main category: cs.CL

TL;DR: Minnow trains language models for few-shot word learning by generating new word usages from examples, achieving performance comparable to large pre-trained models and improving word discrimination and generation.


<details>
  <summary>Details</summary>
Motivation: Current language models lack efficient few-shot word learning abilities, which humans excel at.

Method: Meta-training with Minnow involves generating new word usages using placeholder tokens and repeated training on diverse words.

Result: Minnow enables strong few-shot word learning, matching large pre-trained models, and enhances word discrimination, syntactic identification, and usage generation.

Conclusion: Minnow is data-efficient and improves language model performance in word learning tasks.

Abstract: Humans can quickly learn a new word from a few illustrative examples, and
then systematically and flexibly use it in novel contexts. Yet the abilities of
current language models for few-shot word learning, and methods for improving
these abilities, are underexplored. In this study, we introduce a novel method,
Meta-training for IN-context learNing Of Words (Minnow). This method trains
language models to generate new examples of a word's usage given a few
in-context examples, using a special placeholder token to represent the new
word. This training is repeated on many new words to develop a general
word-learning ability. We find that training models from scratch with Minnow on
human-scale child-directed language enables strong few-shot word learning,
comparable to a large language model (LLM) pre-trained on orders of magnitude
more data. Furthermore, through discriminative and generative evaluations, we
demonstrate that finetuning pre-trained LLMs with Minnow improves their ability
to discriminate between new words, identify syntactic categories of new words,
and generate reasonable new usages and definitions for new words, based on one
or a few in-context examples. These findings highlight the data efficiency of
Minnow and its potential to improve language model performance in word learning
tasks.

</details>


### [191] [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/pdf/2502.15975)
*Jesus Rios, Pierre Dognin, Ronny Luss, Karthikeyan N. Ramamurthy*

Main category: cs.CL

TL;DR: Proposes a method to reduce fine-tuning costs by randomly selecting a small subset of model parameters for training, comparing it with PEFT methods like LoRA and full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of large language models is expensive; PEFT methods like LoRA reduce costs but still require training additional parameters. The goal is to further simplify and reduce costs.

Method: Randomly selects a small proportion of model parameters for training instead of all or introducing new parameters like LoRA.

Result: Compares efficiency and performance of the proposed method against PEFT (e.g., LoRA) and full fine-tuning.

Conclusion: The proposed random selection method offers a simpler and potentially more efficient alternative to existing PEFT methods.

Abstract: Full fine-tuning of large language models for alignment and task adaptation
has become prohibitively expensive as models have grown in size.
Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing
the computational and memory resources needed for fine-tuning these models by
only training on a small number of parameters instead of all model parameters.
Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA),
which freezes the parameters of the model to be fine-tuned and introduces a
small set of trainable parameters in the form of low-rank matrices. We propose
simply reducing the number of trainable parameters by randomly selecting a
small proportion of the model parameters to train on. In this paper, we compare
the efficiency and performance of our proposed approach with PEFT methods,
including LoRA, as well as full parameter fine-tuning.

</details>


### [192] [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/pdf/2502.16825)
*Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Lidong Bing, Xiaoli Li, Roy Ka-wei Lee*

Main category: cs.CL

TL;DR: Scaling up on-policy samples via repeated random sampling improves LLM alignment, but conventional DPO strategies degrade performance. A new strategy, selecting rejected responses at μ−2σ, outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance alignment performance of LLMs by scaling up on-policy samples and improving DPO preference data construction.

Method: Investigates reward distribution, categorizes reward space into seven points, and tests 21 pairwise combinations for DPO. Evaluates on AlpacaEval 2 with four models.

Result: Selecting rejected responses at μ−2σ (not the minimum reward) optimizes performance. A scalable strategy consistently improves model performance with increasing sample size.

Conclusion: A novel preference data construction strategy, focusing on reward distribution, significantly enhances LLM alignment scalability and performance.

Abstract: Iterative data generation and model retraining are widely used to align large
language models (LLMs). It typically involves a policy model to generate
on-policy responses and a reward model to guide training data selection. Direct
Preference Optimization (DPO) further enhances this process by constructing
preference pairs of chosen and rejected responses. In this work, we aim to
\emph{scale up} the number of on-policy samples via repeated random sampling to
improve alignment performance. Conventional practice selects the sample with
the highest reward as chosen and the lowest as rejected for DPO. However, our
experiments reveal that this strategy leads to a \emph{decline} in performance
as the sample size increases. To address this, we investigate preference data
construction through the lens of underlying normal distribution of sample
rewards. We categorize the reward space into seven representative points and
systematically explore all 21 ($C_7^2$) pairwise combinations. Through
evaluations on four models using AlpacaEval 2, we find that selecting the
rejected response at reward position $\mu - 2\sigma$ rather than the minimum
reward, is crucial for optimal performance. We finally introduce a scalable
preference data construction strategy that consistently enhances model
performance as the sample scale increases.

</details>


### [193] [Spontaneous Giving and Calculated Greed in Language Models](https://arxiv.org/pdf/2502.17720)
*Yuxuan Li, Hirokazu Shirado*

Main category: cs.CL

TL;DR: LLMs with reasoning techniques like chain-of-thought and reflection reduce cooperation in social dilemmas, favoring individual rationality over collective gains.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs' reasoning capabilities extend to social intelligence, specifically cooperative decision-making in social dilemmas.

Method: Evaluated GPT-4o and other models in economic games (e.g., Public Goods Game) using reasoning techniques, comparing outcomes with and without explicit reasoning.

Result: Reasoning models reduced cooperation and norm enforcement, mirroring human tendencies of calculated greed. Groups with reasoning agents showed lower collective gains.

Conclusion: LLMs need architectures that integrate social intelligence to address collective action challenges, not reinforce them.

Abstract: Large language models demonstrate strong problem-solving abilities through
reasoning techniques such as chain-of-thought prompting and reflection.
However, it remains unclear whether these reasoning capabilities extend to a
form of social intelligence: making effective decisions in cooperative
contexts. We examine this question using economic games that simulate social
dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o
in a Public Goods Game. We then evaluate multiple off-the-shelf models across
six cooperation and punishment games, comparing those with and without explicit
reasoning mechanisms. We find that reasoning models consistently reduce
cooperation and norm enforcement, favoring individual rationality. In repeated
interactions, groups with more reasoning agents exhibit lower collective gains.
These behaviors mirror human patterns of "spontaneous giving and calculated
greed." Our findings underscore the need for LLM architectures that incorporate
social intelligence alongside reasoning, to help address--rather than
reinforce--the challenges of collective action.

</details>


### [194] [Stay Focused: Problem Drift in Multi-Agent Debate](https://arxiv.org/pdf/2502.19559)
*Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp*

Main category: cs.CL

TL;DR: Multi-agent debate with LLMs can drift from the initial problem, harming performance. DRIFTJudge and DRIFTPolicy are proposed to detect and mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To understand and address the limitations of multi-agent debate in solving complex problems with longer reasoning chains.

Method: Analyzed 170 multi-agent discussions, identified common drift issues, and proposed DRIFTJudge (detection) and DRIFTPolicy (mitigation).

Result: Problem drift occurs due to lack of progress (35%), low-quality feedback (26%), and lack of clarity (25%). Proposed methods show promise in addressing drift.

Conclusion: The study highlights the issue of problem drift in multi-agent debates and offers solutions to improve performance.

Abstract: Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations when solving
complex problems that require longer reasoning chains. We analyze how
multi-agent debate over multiple turns drifts away from the initial problem,
thus harming task performance. We define this phenomenon as problem drift and
quantify its presence across ten tasks (i.e., three generative, three
knowledge, three reasoning, and one instruction-following task). To identify
the reasons for this issue, eight human experts analyze 170 multi-agent
discussions suffering from problem drift. We find the most common issues
related to this drift are the lack of progress (35% of cases), low-quality
feedback (26% of cases), and a lack of clarity (25% of cases). To address
problem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect
problem drift at test-time. We also propose DRIFTPolicy, a method that
mitigates problem drift cases to improve task performance. Our study is a step
toward understanding a key limitation of multi-agent debate, highlighting why
longer debates can harm task performance and how problem drift could be
addressed.

</details>


### [195] [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/pdf/2502.19721)
*Hannah Cyberey, Yangfeng Ji, David Evans*

Main category: cs.CL

TL;DR: The paper introduces a method to study and mitigate gender bias in LLMs by analyzing and manipulating internal representations of the concept "gender."


<details>
  <summary>Details</summary>
Motivation: LLMs often perpetuate biases, but existing methods treat them as black-box problems. This work aims to understand and address bias by examining internal representations.

Method: The authors propose a technique to extract concept representations via probability weighting without labeled data and use a projection-based method to steer model predictions.

Result: The method effectively measures and manipulates gender bias in LLMs, demonstrating its potential for bias mitigation.

Conclusion: The study provides a novel approach to understanding and reducing bias in LLMs by focusing on internal representations, with promising results for gender bias mitigation.

Abstract: Large language models (LLMs) are known to perpetuate stereotypes and exhibit
biases. Various strategies have been proposed to mitigate these biases, but
most work studies biases in LLMs as a black-box problem without considering how
concepts are represented within the model. We adapt techniques from
representation engineering to study how the concept of "gender" is represented
within LLMs. We introduce a new method that extracts concept representations
via probability weighting without labeled data and efficiently selects a
steering vector for measuring and manipulating the model's representation. We
also present a projection-based method that enables precise steering of model
predictions and demonstrate its effectiveness in mitigating gender bias in
LLMs. Our code is available at:
https://github.com/hannahxchen/gender-bias-steering

</details>


### [196] [Adaptively profiling models with task elicitation](https://arxiv.org/pdf/2503.01986)
*Davis Brown, Prithvi Balehannina, Helen Jin, Shreya Havaldar, Hamed Hassani, Eric Wong*

Main category: cs.CL

TL;DR: Task elicitation automates the creation of evaluations to identify systematic failures in language models, uncovering issues like over-association and hallucination.


<details>
  <summary>Details</summary>
Motivation: Current language model evaluations miss critical failure modes, requiring manual inspection and new benchmarks.

Method: Task elicitation automatically generates natural-language tasks to profile model behavior.

Result: Hundreds of tasks reveal systematic failures in models, such as over-association (Sonnet 3.5) and hallucination (o3-mini).

Conclusion: Task elicitation effectively expands evaluation coverage, exposing previously unnoticed model weaknesses.

Abstract: Language model evaluations often fail to characterize consequential failure
modes, forcing experts to inspect outputs and build new benchmarks. We
introduce task elicitation, a method that automatically builds new evaluations
to profile model behavior. Task elicitation finds hundreds of natural-language
tasks -- an order of magnitude more than prior work -- where frontier models
exhibit systematic failures, in domains ranging from forecasting to online
harassment. For example, we find that Sonnet 3.5 over-associates quantum
computing and AGI and that o3-mini is prone to hallucination when fabrications
are repeated in-context.

</details>


### [197] [Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations](https://arxiv.org/pdf/2503.03062)
*Zhengyao Gu, Henry Peng Zou, Yankai Chen, Aiwei Liu, Weizhi Zhang, Philip S. Yu*

Main category: cs.CL

TL;DR: The paper proposes a semi-supervised framework for in-context learning (ICL) using self-generated annotations, outperforming ground-truth ICL across zero-shot, few-shot, and many-shot settings. An iterative method, IterPSD, further improves performance.


<details>
  <summary>Details</summary>
Motivation: High costs of annotated data for ICL drive the need for self-generated annotations, but existing methods struggle in many-shot scenarios.

Method: A semi-supervised framework with annotation generation, demonstration selection, and in-context inference. Introduces IterPSD for iterative refinement.

Result: The baseline outperforms ground-truth ICL, with scaling laws showing optimal performance at 1,000+ demonstrations. IterPSD adds 6.8% gains.

Conclusion: Self-generated annotations and iterative refinement enable scalable, high-performance ICL, even in many-shot settings.

Abstract: The high cost of obtaining high-quality annotated data for in-context
learning (ICL) has motivated the development of methods that use self-generated
annotations in place of ground-truth labels. While these approaches have shown
promising results in few-shot settings, they generally do not scale to
many-shot scenarios. In this work, we study ICL with self-generated examples
using a framework analogous to traditional semi-supervised learning, consisting
of annotation generation, demonstration selection, and in-context inference.
Within this framework, we propose a simple baseline that outperforms
ground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we
observe a scaling law with this baseline, where optimal performance is achieved
with more than 1,000 demonstrations. To fully exploit the many-shot
capabilities of semi-supervised ICL, we introduce IterPSD, an iterative
annotation approach that integrates iterative refinement and curriculum
pseudo-labeling techniques from semi-supervised learning, yielding up to 6.8%
additional gains on classification tasks.

</details>


### [198] [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/pdf/2503.03122)
*Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun*

Main category: cs.CL

TL;DR: A Shortcut-aware MM-RM learning algorithm improves generalization in multimodal reward models by reducing reliance on unimodal spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Existing MM-RMs struggle with out-of-distribution data due to text-only shortcuts, limiting their multimodal understanding.

Method: Introduces a dynamic reweighting algorithm to shift training distribution and reduce unimodal dependencies.

Result: Significant improvements in generalization, downstream task performance, and scalability.

Conclusion: The proposed framework enhances robustness in multimodal reward modeling.

Abstract: Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language
Models (LLMs) with human preferences, particularly as LLMs increasingly
interact with multimodal data. However, we find that MM-RMs trained on existing
datasets often struggle to generalize to out-of-distribution data due to their
reliance on unimodal spurious correlations, primarily text-only shortcuts
within the training distribution, which prevents them from leveraging true
multimodal reward functions. To address this, we introduce a Shortcut-aware
MM-RM learning algorithm that mitigates this issue by dynamically reweighting
training samples, shifting the distribution toward better multimodal
understanding, and reducing dependence on unimodal spurious correlations. Our
experiments demonstrate significant improvements in generalization, downstream
task performance, and scalability, establishing a more robust framework for
multimodal reward modeling.

</details>


### [199] [DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/pdf/2503.04959)
*Haoyuan Ma, Yongliang Shen, Hengwei Liu, Wenqi Zhang, Haolei Xu, Qiuying Peng, Jun Wang, Weiming Lu*

Main category: cs.CL

TL;DR: DB-Explore enhances text-to-SQL systems by aligning LLMs with database knowledge through automated exploration and instruction synthesis, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-SQL systems struggle with complex database structures and domain-specific queries due to insufficient database understanding.

Method: DB-Explore constructs database graphs, mines structural patterns with GPT-4, and synthesizes instructions for LLM fine-tuning.

Result: Achieves 67.0% execution accuracy on BIRD and 87.8% on SPIDER, outperforming GPT-4-driven systems.

Conclusion: DB-Explore bridges the gap between database structures and LLMs, offering efficient and effective text-to-SQL translation.

Abstract: Recent text-to-SQL systems powered by large language models (LLMs) have
demonstrated remarkable performance in translating natural language queries
into SQL. However, these systems often struggle with complex database
structures and domain-specific queries, as they primarily focus on enhancing
logical reasoning and SQL syntax while overlooking the critical need for
comprehensive database understanding. To address this limitation, we propose
DB-Explore, a novel framework that systematically aligns LLMs with database
knowledge through automated exploration and instruction synthesis. DB-Explore
constructs database graphs to capture complex relational schemas, leverages
GPT-4 to systematically mine structural patterns and semantic knowledge, and
synthesizes instructions to distill this knowledge for efficient fine-tuning of
LLMs. Our framework enables comprehensive database understanding through
diverse sampling strategies and automated instruction generation, bridging the
gap between database structures and language models. Experiments conducted on
the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore,
achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably,
our open-source implementation based on Qwen2.5-Coder-7B achieves
state-of-the-art results at minimal computational cost, outperforming several
GPT-4-driven Text-to-SQL systems.

</details>


### [200] [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/pdf/2503.05179)
*Simon A. Aytes, Jinheon Baek, Sung Ju Hwang*

Main category: cs.CL

TL;DR: Sketch-of-Thought (SoT) reduces token usage in LLMs by 78% with minimal accuracy loss, improving efficiency while maintaining reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Address excessive verbosity and computational overhead in Chain-of-Thought (CoT) prompting for LLMs.

Method: Introduces SoT, a modular framework with three paradigms (Conceptual Chaining, Chunked Symbolism, Expert Lexicons) dynamically selected by a routing model.

Result: Achieves up to 78% token reduction across 15 datasets, with improved accuracy in some tasks.

Conclusion: SoT offers a flexible, efficient alternative to CoT, balancing brevity and reasoning accuracy.

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning
capabilities through Chain-of-Thought (CoT) prompting, which elicits
step-by-step problem solving, but often at the cost of excessive verbosity in
intermediate outputs, leading to increased computational overhead. We propose
Sketch-of-Thought (SoT), a prompting framework that integrates cognitively
inspired reasoning paradigms with linguistic constraints to reduce token usage
while preserving reasoning accuracy. SoT is designed as a flexible, modular
approach and is instantiated with three paradigms--Conceptual Chaining, Chunked
Symbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and
selected dynamically at test-time by a lightweight routing model. Across 15
reasoning datasets spanning multiple domains, languages, and modalities, SoT
achieves token reductions of up to 78% with minimal accuracy loss. In tasks
such as mathematical and multi-hop reasoning, it even improves accuracy while
shortening outputs.

</details>


### [201] [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/pdf/2503.06072)
*Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, Zhenhan Dai, Yifeng Xie, Yihan Cao, Lichao Sun, Pan Zhou, Lifang He, Hechang Chen, Yu Zhang, Qingsong Wen, Tianming Liu, Neil Zhenqiang Gong, Jiliang Tang, Caiming Xiong, Heng Ji, Philip S. Yu, Jianfeng Gao*

Main category: cs.CL

TL;DR: This paper surveys post-training language models (PoLMs), addressing limitations of LLMs through five paradigms: Fine-tuning, Alignment, Reasoning, Efficiency, and Integration/Adaptation. It highlights advancements like OpenAI-o1/o3 and DeepSeek-R1, offering a taxonomy and future research agenda.


<details>
  <summary>Details</summary>
Motivation: LLMs have limitations in specialized contexts (e.g., reasoning, ethics, domain performance). PoLMs aim to overcome these gaps, necessitating a comprehensive survey to guide future research.

Method: The paper systematically reviews PoLMs across five paradigms, analyzing techniques like fine-tuning, alignment, and reasoning advancements. It synthesizes datasets and categorizes methods.

Result: A structured taxonomy of PoLM techniques and datasets is presented, alongside advancements in reasoning, ethics, and domain adaptability.

Conclusion: The survey establishes a framework for future PoLM research, emphasizing precision, ethical robustness, and versatility in LLM applications.

Abstract: The emergence of Large Language Models (LLMs) has fundamentally transformed
natural language processing, making them indispensable across domains ranging
from conversational systems to scientific exploration. However, their
pre-trained architectures often reveal limitations in specialized contexts,
including restricted reasoning capacities, ethical uncertainties, and
suboptimal domain-specific performance. These challenges necessitate advanced
post-training language models (PoLMs) to address these shortcomings, such as
OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or
LRMs). This paper presents the first comprehensive survey of PoLMs,
systematically tracing their evolution across five core paradigms: Fine-tuning,
which enhances task-specific accuracy; Alignment, which ensures ethical
coherence and alignment with human preferences; Reasoning, which advances
multi-step inference despite challenges in reward design; Efficiency, which
optimizes resource utilization amidst increasing complexity; Integration and
Adaptation, which extend capabilities across diverse modalities while
addressing coherence issues. Charting progress from ChatGPT's alignment
strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate
how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities,
and enhance domain adaptability. Our contributions include a pioneering
synthesis of PoLM evolution, a structured taxonomy categorizing techniques and
datasets, and a strategic agenda emphasizing the role of LRMs in improving
reasoning proficiency and domain flexibility. As the first survey of its scope,
this work consolidates recent PoLM advancements and establishes a rigorous
intellectual framework for future research, fostering the development of LLMs
that excel in precision, ethical robustness, and versatility across scientific
and societal applications.

</details>


### [202] [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/pdf/2503.11299)
*Hai Zhao, Hongqiu Wu, Dongjie Yang, Anni Zou, Jiale Hong*

Main category: cs.CL

TL;DR: BriLLM is a brain-inspired, non-Transformer language model using a directed graph (SiFu) for interpretability and infinite n-gram support.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable, brain-like language model beyond traditional ML limitations.

Method: Uses Signal Fully-connected flowing (SiFu) on a directed graph, with tokens as nodes and signal flow for prediction.

Result: Achieves GPT-1 level performance in Chinese, supports long sequences, and enables recall activation.

Conclusion: BriLLM shows promise for scalable, interpretable language modeling with brain-like features.

Abstract: This paper reports the first brain-inspired large language model (BriLLM).
This is a non-Transformer, non-GPT, non-traditional machine learning
input-output controlled generative language model. The model is based on the
Signal Fully-connected flowing (SiFu) definition on the directed graph in terms
of the neural network, and has the interpretability of all nodes on the graph
of the whole model, instead of the traditional machine learning model that only
has limited interpretability at the input and output ends. In the language
model scenario, the token is defined as a node in the graph. A randomly shaped
or user-defined signal flow flows between nodes on the principle of "least
resistance" along paths. The next token or node to be predicted or generated is
the target of the signal flow. As a language model, BriLLM theoretically
supports infinitely long $n$-gram models when the model size is independent of
the input and predicted length of the model. The model's working signal flow
provides the possibility of recall activation and innate multi-modal support
similar to the cognitive patterns of the human brain. At present, we released
the first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node
width, 16-token long sequence prediction ability, and language model prediction
performance comparable to GPT-1. More computing power will help us explore the
infinite possibilities depicted above.

</details>


### [203] [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/pdf/2503.15952)
*Chen Li, Nazhou Liu, Kai Yang*

Main category: cs.CL

TL;DR: AGPO improves RL stability and inference efficiency in Reasoning LLMs by revising the objective function to address zero-variance in advantage estimation.


<details>
  <summary>Details</summary>
Motivation: Address deficiencies in GRPO, such as zero-variance in advantage estimation, which impact RL stability and inference efficiency.

Method: Propose Adaptive Group Policy Optimization (AGPO) with a revised objective function to mitigate training fluctuations and zero advantage.

Result: AGPO achieves more stable training and superior performance with fewer reasoning tokens.

Conclusion: AGPO is a simple yet effective improvement over GRPO for training Reasoning LLMs.

Abstract: Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has
become the core part of training Reasoning LLMs. However, we find some
deficiency that influences RL stability and inference efficiency, like
zero-variance in advantage estimation. Thus, we propose Adaptive Group Policy
Optimization (AGPO) which contains a simple but effective modification: a
revised objective function to mitigate training fluctuation and zero advantage.
The experiments demonstrate our method achieves more stable training and
superior performance with significantly fewer tokens in reasoning steps.

</details>


### [204] [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/pdf/2503.18247)
*Tadesse Destaw Belay, Israel Abebe Azime, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Idris Abdulmumin, Abinew Ali Ayele, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

Main category: cs.CL

TL;DR: The paper explores domain and task-adaptive pre-training (DAPT and TAPT) for African multilingual encoders, introducing AfriSocial corpus. Results show improved performance on subjective tasks like sentiment analysis and hate speech classification.


<details>
  <summary>Details</summary>
Motivation: Addressing the bias in low-resource African languages, particularly in social media domains, by adapting pre-training techniques.

Method: Uses DAPT and TAPT for continual pre-training on African languages, leveraging the AfriSocial corpus. Evaluates on sentiment analysis, emotion, and hate speech tasks.

Result: DAPT improves F1 scores by 1%-30% across 19 languages. TAPT also boosts performance on related tasks, with combined DAPT+TAPT yielding further gains.

Conclusion: DAPT and TAPT are effective for African languages, with AfriSocial enabling significant performance improvements in subjective NLP tasks.

Abstract: Language models built from various sources are the foundation of today's NLP
progress. However, for many low-resource languages, the diversity of domains is
often limited -- more biased to a religious domain, which impacts their
performance when evaluated on distant and rapidly evolving domains such as
social media. Domain adaptive pre-training (DAPT) and task-adaptive
pre-training (TAPT) are popular techniques to reduce this bias through
continual pre-training for BERT-based models, but they have not been explored
for African multilingual encoders. In this paper, we explore DAPT and TAPT
continual pertaining approaches for the African languages social media domain.
We introduce AfriSocial-a large-scale social media and news domain corpus for
continual pre-training on several African languages. Leveraging AfriSocial, we
show that DAPT consistently improves performance on three subjective tasks:
sentiment analysis, multi-label emotion, and hate speech classification,
covering 19 languages from 1% to 30% F1 score. Similarly, leveraging TAPT on
one task data improves performance on other related tasks. For example,
training with unlabeled sentiment data (source) for a fine-grained emotion
classification task (target) improves the baseline results by an F1 score
ranging from 0.55% to 15.11%. Combining these two methods (i.e. DAPT + TAPT)
further improves the overall performance.

</details>


### [205] [A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications](https://arxiv.org/pdf/2503.20302)
*Sunayana Sitaram, Adrian de Wynter, Isobel McCrum, Qilong Gu, Si-Qing Chen*

Main category: cs.CL

TL;DR: The paper addresses misgendering in AI applications across 42 languages, proposing guardrails developed via participatory design to reduce misgendering without quality loss.


<details>
  <summary>Details</summary>
Motivation: Misgendering harms individuals by marginalizing their identity. While English has solutions like "they," other languages lack clear approaches due to grammatical and cultural differences.

Method: A participatory-design approach was used to create guardrails, tested in an LLM-based meeting summarization task with human-in-the-loop data generation and annotation.

Result: Guardrails effectively reduced misgendering rates across all 42 languages without compromising summary quality.

Conclusion: The human-in-the-loop method scales inclusive AI solutions across languages. Guardrails and datasets are released to foster further research.

Abstract: Misgendering is the act of referring to someone by a gender that does not
match their chosen identity. It marginalizes and undermines a person's sense of
self, causing significant harm. English-based approaches have clear-cut
approaches to avoiding misgendering, such as the use of the pronoun ``they''.
However, other languages pose unique challenges due to both grammatical and
cultural constructs. In this work we develop methodologies to assess and
mitigate misgendering across 42 languages and dialects using a
participatory-design approach to design effective and appropriate guardrails
across all languages. We test these guardrails in a standard LLM-based
application (meeting transcript summarization), where both the data generation
and the annotation steps followed a human-in-the-loop approach. We find that
the proposed guardrails are very effective in reducing misgendering rates
across all languages in the summaries generated, and without incurring loss of
quality. Our human-in-the-loop approach demonstrates a method to feasibly scale
inclusive and responsible AI-based solutions across multiple languages and
cultures. We release the guardrails and synthetic dataset encompassing 42
languages, along with human and LLM-judge evaluations, to encourage further
research on this subject.

</details>


### [206] [Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/pdf/2503.24245)
*Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang*

Main category: cs.CL

TL;DR: A novel KG-RAG framework enhances LLM performance in telecom by combining knowledge graphs and retrieval-augmented generation, achieving 88% accuracy in QA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with domain-specific tasks like telecom due to lack of specialized knowledge and adaptability to evolving standards.

Method: Integrates knowledge graphs (KG) for structured domain knowledge and retrieval-augmented generation (RAG) for dynamic access to relevant information.

Result: KG-RAG achieved 88% accuracy in telecom QA tasks, outperforming RAG-only (82%) and LLM-only (48%) approaches.

Conclusion: The KG-RAG framework effectively bridges structured knowledge and generative LLMs, improving accuracy and domain-specific comprehension.

Abstract: Large language models (LLMs) have made significant progress in
general-purpose natural language processing tasks. However, LLMs are still
facing challenges when applied to domain-specific areas like
telecommunications, which demands specialized expertise and adaptability to
evolving standards. This paper presents a novel framework that combines
knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to
enhance LLM performance in the telecom domain. The framework leverages a KG to
capture structured, domain-specific information about network protocols,
standards, and other telecom-related entities, comprehensively representing
their relationships. By integrating KG with RAG, LLMs can dynamically access
and utilize the most relevant and up-to-date knowledge during response
generation. This hybrid approach bridges the gap between structured knowledge
representation and the generative capabilities of LLMs, significantly enhancing
accuracy, adaptability, and domain-specific comprehension. Our results
demonstrate the effectiveness of the KG-RAG framework in addressing complex
technical queries with precision. The proposed KG-RAG model attained an
accuracy of 88% for question answering tasks on a frequently used
telecom-specific dataset, compared to 82% for the RAG-only and 48% for the
LLM-only approaches.

</details>


### [207] [GLiNER-BioMed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition](https://arxiv.org/pdf/2504.00676)
*Anthony Yazdani, Ihor Stepanov, Douglas Teodoro*

Main category: cs.CL

TL;DR: GLiNER-BioMed introduces a domain-adapted NER model for biomedicine, leveraging synthetic data and natural language labels for zero-shot recognition, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Address challenges in biomedical NER like specialized vocabularies, volume of entities, and novel entities, where traditional models fail due to fixed taxonomies.

Method: Uses LLM-distilled synthetic data to train lightweight GLiNER models (uni- and bi-encoder) for zero-shot recognition with natural language labels.

Result: Achieves 5.96% F1-score improvement over baselines in zero- and few-shot scenarios, validated by ablation studies.

Conclusion: GLiNER-BioMed is effective, with synthetic data and fine-tuning enhancing performance; all resources are publicly available.

Abstract: Biomedical named entity recognition (NER) presents unique challenges due to
specialized vocabularies, the sheer volume of entities, and the continuous
emergence of novel entities. Traditional NER models, constrained by fixed
taxonomies and human annotations, struggle to generalize beyond predefined
entity types. To address these issues, we introduce GLiNER-BioMed, a
domain-adapted suite of Generalist and Lightweight Model for NER (GLiNER)
models specifically tailored for biomedicine. In contrast to conventional
approaches, GLiNER uses natural language labels to infer arbitrary entity
types, enabling zero-shot recognition. Our approach first distills the
annotation capabilities of large language models (LLMs) into a smaller, more
efficient model, enabling the generation of high-coverage synthetic biomedical
NER data. We subsequently train two GLiNER architectures, uni- and bi-encoder,
at multiple scales to balance computational efficiency and recognition
performance. Experiments on several biomedical datasets demonstrate that
GLiNER-BioMed outperforms the state-of-the-art in both zero- and few-shot
scenarios, achieving 5.96% improvement in F1-score over the strongest baseline
(p-value < 0.001). Ablation studies highlight the effectiveness of our
synthetic data generation strategy and emphasize the complementary benefits of
synthetic biomedical pre-training combined with fine-tuning on general-domain
annotations. All datasets, models, and training pipelines are publicly
available at https://github.com/ds4dh/GLiNER-biomed.

</details>


### [208] [Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/pdf/2504.03234)
*Junjie Yang, Ke Lin, Xing Yu*

Main category: cs.CL

TL;DR: The paper introduces a method to optimize Chain of Thought (CoT) reasoning by balancing conciseness and correctness, avoiding inefficiency on simple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing CoT approaches penalize reasoning length uniformly, ignoring problem complexity, leading to overthinking on simple problems.

Method: The approach constructs rewards based on length and quality comparisons, guided by theoretical assumptions to enhance correctness and conciseness. It also handles fuzzy tasks without ground truth.

Result: Experiments show the method maintains accuracy while producing more concise explanations, teaching models to reason only when necessary.

Conclusion: The method effectively optimizes CoT reasoning by ensuring models "think when needed," improving efficiency without sacrificing performance.

Abstract: Chain of Thought (CoT) reasoning enhances language models' performance but
often leads to inefficient "overthinking" on simple problems. We identify that
existing approaches directly penalizing reasoning length fail to account for
varying problem complexity. Our approach constructs rewards through length and
quality comparisons, guided by theoretical assumptions that jointly enhance
solution correctness with conciseness. Moreover, we further demonstrate our
method to fuzzy tasks where ground truth is unavailable. Experiments across
multiple reasoning benchmarks demonstrate that our method maintains accuracy
while generating significantly more concise explanations, effectively teaching
models to "think when needed."

</details>


### [209] [Thinking Out Loud: Do Reasoning Models Know When They're Right?](https://arxiv.org/pdf/2504.06564)
*Qingcheng Zeng, Weihao Xuan, Leyang Cui, Rob Voigt*

Main category: cs.CL

TL;DR: Large reasoning models (LRMs) show improved verbalized confidence through fine-tuning but struggle with recognizing their knowledge limits, leading to overconfidence.


<details>
  <summary>Details</summary>
Motivation: To explore how self-reflection in LRMs interacts with behaviors like verbalized confidence and knowledge boundary awareness.

Method: Analyzed verbalized confidence and reasoning chains, using supervised fine-tuning and reinforcement learning on reasoning traces.

Result: Fine-tuning improves verbalized calibration but reduces 'I don't know' responses, indicating overconfidence. Shorter reasoning chains correlate with higher confidence.

Conclusion: Reasoning-oriented training enhances performance but may erode knowledge boundary awareness, compromising model faithfulness.

Abstract: Large reasoning models (LRMs) have recently demonstrated impressive
capabilities in complex reasoning tasks by leveraging increased test-time
computation and exhibiting behaviors reminiscent of human-like self-reflection.
While LRMs show a clear capacity for valuable self-reflection, how this ability
interacts with other model behaviors remains underexplored. We investigate this
connection by analyzing verbalized confidence, how models articulate their
certainty, as a lens into the nature of self-reflection in LRMs. We find that
supervised fine-tuning on reasoning traces (i.e., distillation) and
reinforcement learning can improve verbalized calibration in
reasoning-intensive settings in a progressive, laddered fashion. However, our
results also indicate that reasoning models may possess a diminished awareness
of their own knowledge boundaries, as evidenced by significantly lower "I don't
know" response rates on factuality benchmarks. Moreover, we examine the
relationship between verbalized confidence and reasoning chains, finding that
models tend to express higher confidence when providing shorter or less
elaborate reasoning. Our findings highlight how reasoning-oriented training can
enhance performance in reasoning-centric tasks while potentially incurring a
"reasoning tax," a cost reflected in the model's reduced ability to accurately
recognize the limits of its own knowledge in small-scale models. More broadly,
our work showcases how this erosion of knowledge boundaries can compromise
model faithfulness, as models grow more confident without a commensurate
understanding of when they should abstain.

</details>


### [210] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/pdf/2504.14218)
*Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang*

Main category: cs.CL

TL;DR: The paper investigates the 'Repeat Curse' in LLMs, proposing 'Duplicatus Charm' to identify and mitigate repetition through mechanistic interpretability and Sparse Autoencoders.


<details>
  <summary>Details</summary>
Motivation: Despite progress in LLMs, repetitive text generation ('Repeat Curse') remains understudied. The paper aims to explore its root causes and solutions.

Method: The approach involves logit analysis to locate repetition-prone layers, SAE-based activation manipulation to extract 'Repetition Features,' and a dataset for validation.

Result: The method successfully identifies and deactivates repetition features, effectively mitigating the Repeat Curse.

Conclusion: The study provides a mechanistic understanding of repetition in LLMs and offers a practical solution to reduce it.

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>


### [211] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/pdf/2504.15573)
*Yuxin Jiang, Yufei Wang, Chuhan Wu, Xinyi Dai, Yan Xu, Weinan Gan, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang*

Main category: cs.CL

TL;DR: WebR is an automated framework for synthesizing high-quality instruction-tuning data from raw web documents, outperforming existing methods by up to 16.65%.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing automatic data synthesis methods, which rely heavily on seed data quality or strong assumptions about web documents.

Method: Proposes Web Reconstruction (WebR), a dual-perspective paradigm (Web as Instruction and Web as Response) to synthesize instruction-tuning data from raw web content.

Result: Datasets from WebR outperform state-of-the-art baselines by up to 16.65% across benchmarks, showing superior compatibility, efficiency, and scalability.

Conclusion: WebR enables enhanced domain adaptation with minimal effort, offering a scalable solution for high-quality instruction-tuning data synthesis.

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>


### [212] [Improving Language Model Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/pdf/2504.17993)
*Brihi Joshi, Xiang Ren, Swabha Swayamdipta, Rik Koncel-Kedziorski, Tim Paek*

Main category: cs.CL

TL;DR: PB&J framework enhances LM personas by adding psychological rationales, outperforming traditional demographic/judgment-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing personas lack reasoning behind user judgments, limiting their predictive accuracy.

Method: PB&J uses psychological scaffolds (e.g., Big 5 Traits) to generate rationales for user behavior.

Result: PB&J-augmented personas outperform demographic/judgment-based ones and match human-written rationale performance.

Conclusion: Synthetic rationales grounded in psychology improve LM personas, offering a viable alternative to human inputs.

Abstract: Language models prompted with a user description or persona are being used to
predict the user's preferences and opinions. However, existing approaches to
building personas mostly rely on a user's demographic attributes and/or prior
judgments, but not on any underlying reasoning behind a user's judgments. We
introduce PB&J (Psychology of Behavior and Judgments), a framework that
improves LM personas by incorporating potential rationales for why the user
could have made a certain judgment. Our rationales are generated by a language
model to explicitly reason about a user's behavior on the basis of their
experiences, personality traits, or beliefs. Our method employs psychological
scaffolds: structured frameworks such as the Big 5 Personality Traits or Primal
World Beliefs to help ground the generated rationales in existing theories.
Experiments on public opinion and movie preference prediction tasks demonstrate
that language model personas augmented with PB&J rationales consistently
outperform personas conditioned only on user demographics and / or judgments,
including those that use a model's default chain-of-thought, which is not
grounded in psychological theories. Additionally, our PB&J personas perform
competitively with those using human-written rationales, suggesting the
potential of synthetic rationales guided by existing theories.

</details>


### [213] [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/pdf/2504.21625)
*Jiaming Wang, Yunke Zhao, Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai*

Main category: cs.CL

TL;DR: Meeseeks is a benchmark for evaluating LLMs' ability to follow multi-turn instructions with iterative feedback, simulating real-world usage.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack iterative feedback and self-correction, failing to reflect real-world LLM usage.

Method: Meeseeks introduces an iterative feedback framework and a 38-tag evaluation system across three dimensions.

Result: The benchmark provides insights into LLMs' instruction-following in multi-turn scenarios.

Conclusion: Meeseeks addresses the gap in evaluating LLMs' iterative instruction-following capabilities.

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
For complex instructions, LLMs often struggle to fulfill all requirements in a
single attempt. In practice, users typically provide iterative feedback until
the LLM generates a response that meets all requirements. However, existing
instruction-following benchmarks are either single-turn or introduce new
requirements in each turn without allowing self-correction. To address this
gap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions
through an iterative feedback framework, which enables models to self-correct
based on specific requirement failures in each turn, better reflecting
real-world user-end usage patterns. Meanwhile, the benchmark implements a
comprehensive evaluation system with 38 capability tags organized across three
dimensions: Intent Recognition, Granular Content Validation, and Output
Structure Validation. Through rigorous evaluation across LLMs, Meeseeks
provides valuable insights into LLMs' instruction-following capabilities in
multi-turn scenarios.

</details>


### [214] [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/pdf/2505.01731)
*Chuan Sun, Han Yu, Lizhen Cui, Xiaoxiao Li*

Main category: cs.CL

TL;DR: The paper introduces SV-NUP, a Shapley Value-based non-uniform pruning method for LLMs, outperforming uniform pruning by tailoring layer-specific sparsity and reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform pruning methods are suboptimal as they ignore the varying significance of transformer layers in LLMs.

Method: SV-NUP quantifies layer contributions using Shapley Values and assigns tailored pruning budgets. A sliding window approximation reduces computational cost.

Result: SV-NUP reduces perplexity by 18.01% (LLaMA-7B) and 19.55% (LLaMA-13B) compared to SparseGPT at 70% sparsity.

Conclusion: Non-uniform pruning with SV-NUP significantly improves pruned model performance, demonstrating its effectiveness for LLMs.

Abstract: Pruning large language models (LLMs) is a promising solution for reducing
model sizes and computational complexity while preserving performance.
Traditional layer-wise pruning methods often adopt a uniform sparsity approach
across all layers, which leads to suboptimal performance due to the varying
significance of individual transformer layers within the model not being
accounted for. To this end, we propose the Shapley Value-based Non-Uniform
Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of
each transformer layer to the overall model performance, enabling the
assignment of tailored pruning budgets to different layers to retain critical
parameters. To further improve efficiency, we design the Sliding Window-based
Shapley Value approximation method. It substantially reduces computational
overhead compared to exact SV calculation methods. Extensive experiments on
various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness
of the proposed approach. The results reveal that non-uniform pruning
significantly enhances the performance of pruned models. Notably, SV-NUP
achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and
LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.

</details>


### [215] [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/pdf/2505.02009)
*Sai Krishna Mendu, Harish Yenala, Aditi Gulati, Shanu Kumar, Parag Agrawal*

Main category: cs.CL

TL;DR: The paper analyzes harmful content in LLM training datasets, introduces tools (TTP, HarmFormer, HAVOC) for filtering and evaluating toxicity, and provides insights for safer LLM pretraining and RAI compliance.


<details>
  <summary>Details</summary>
Motivation: To address the risks of harmful content (e.g., hate speech, misinformation) in LLM training datasets, which can perpetuate toxicity and bias, undermining trust and ethical use.

Method: Conducts a large-scale analysis of harmful content, introduces a taxonomy (Topical/Toxic), develops TTP prompts, HarmFormer for filtering, and HAVOC benchmark for toxicity evaluation.

Result: Provides tools (TTP, TTP-Eval, HAVOC) and insights into adversarial toxic inputs, aiding safer LLM pretraining and RAI compliance.

Conclusion: The work contributes to mitigating harmful content in LLM training, promoting ethical AI practices and safer applications.

Abstract: Large language models (LLMs) have become integral to various real-world
applications, leveraging massive, web-sourced datasets like Common Crawl, C4,
and FineWeb for pretraining. While these datasets provide linguistic data
essential for high-quality natural language generation, they often contain
harmful content, such as hate speech, misinformation, and biased narratives.
Training LLMs on such unfiltered data risks perpetuating toxic behaviors,
spreading misinformation, and amplifying societal biases which can undermine
trust in LLM-driven applications and raise ethical concerns about their use.
This paper presents a large-scale analysis of inappropriate content across
these datasets, offering a comprehensive taxonomy that categorizes harmful
webpages into Topical and Toxic based on their intent. We also introduce a
prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and
a transformer-based model (HarmFormer) for harmful content filtering.
Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC)
and provide crucial insights into how models respond to adversarial toxic
inputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on
HarmFormer. Our work offers insights into ensuring safer LLM pretraining and
serves as a resource for Responsible AI (RAI) compliance.

</details>


### [216] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/pdf/2505.02847)
*Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE is an automated framework to evaluate LLMs' social cognition by simulating human-like emotions and thoughts, revealing gaps in empathy and social skills not captured by traditional metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing LLMs' understanding of human emotions and social cognition beyond text comprehension.

Method: Introduces SAGE, a Sentient Agent that simulates emotional changes and inner thoughts during multi-turn conversations, providing emotion trajectories and interpretable reasoning.

Result: SAGE's emotion scores correlate strongly with psychological metrics (BLRI, empathy), and reveal significant performance gaps among LLMs (up to 4x) not shown in conventional evaluations.

Conclusion: SAGE offers a scalable, interpretable tool for advancing LLMs toward genuine empathy and social adeptness.

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [217] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/pdf/2505.03469)
*Bin Yu, Hang Yuan, Haotian Li, Xueyin Xu, Yuliang Wei, Bailing Wang, Weizhen Qi, Kai Chen*

Main category: cs.CL

TL;DR: LS-Mixture SFT improves reasoning in non-reasoning models by combining long and short CoT data, reducing verbosity and boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the 'overthinking' problem in models fine-tuned with CoT reasoning data, which leads to verbose and redundant outputs.

Method: Propose LS-Mixture SFT, blending long CoT datasets with short, rewritten versions to train models.

Result: Achieved 2.3% higher accuracy and reduced response length by ~47.61% compared to direct SFT.

Conclusion: LS-Mixture SFT effectively transfers reasoning capabilities while avoiding overthinking, enabling efficient reasoning.

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose Long-Short Chain-of-Thought Mixture Supervised
Fine-Tuning (LS-Mixture SFT), which combines long CoT reasoning dataset with
their short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3% across various benchmarks while substantially reducing
model response length by approximately 47.61%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [218] [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/pdf/2505.06538)
*Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, Kaiyu Huang*

Main category: cs.CL

TL;DR: The paper evaluates the safety of Multimodal Large Reasoning Models (MLRMs), revealing prevalent safety degradation and proposing a solution using intrinsic reasoning capabilities for safety enhancement.


<details>
  <summary>Details</summary>
Motivation: The safety and reliability of MLRMs are critical concerns, but systematic exploration is lacking.

Method: A comprehensive safety evaluation of 11 MLRMs across 5 benchmarks, followed by constructing a safety-oriented tuning dataset to enhance model safety.

Result: Safety degradation varies across benchmarks; leveraging reasoning capabilities improves safety. Fine-tuning with the new dataset enhances safety performance.

Conclusion: The study offers a new approach to developing safe MLRMs by utilizing intrinsic reasoning for safety detection.

Abstract: The rapid development of Multimodal Large Reasoning Models (MLRMs) has
demonstrated broad application potential, yet their safety and reliability
remain critical concerns that require systematic exploration. To address this
gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs
across 5 benchmarks and unveil prevalent safety degradation phenomena in most
advanced models. Moreover, our analysis reveals distinct safety patterns across
different benchmarks: significant safety degradation is observed across
jailbreak robustness benchmarks, whereas safety-awareness benchmarks
demonstrate less pronounced degradation. In particular, the long thought
process in some scenarios even enhances safety performance. Therefore, it is a
potential approach to address safety issues in MLRMs by leveraging the
intrinsic reasoning capabilities of the model to detect unsafe intent. To
operationalize this insight, we construct a multimodal tuning dataset that
incorporates a safety-oriented thought process. Experimental results from
fine-tuning existing MLRMs with this dataset effectively enhances the safety on
both jailbreak robustness and safety-awareness benchmarks. This study provides
a new perspective for developing safe MLRMs. Our dataset is available at
https://github.com/xinyuelou/Think-in-Safety.

</details>


### [219] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/pdf/2505.06569)
*Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang*

Main category: cs.CL

TL;DR: MacRAG improves RAG systems by hierarchically compressing and partitioning documents, enabling adaptive context expansion for better precision and coverage in long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems face issues like imprecise retrieval, incomplete context coverage, and fragmented information, limiting their effectiveness in complex multi-hop and large-document tasks.

Method: MacRAG uses a hierarchical framework to compress and partition documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions.

Result: MacRAG outperforms baseline RAG pipelines in evaluations on LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique using models like Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o.

Conclusion: MacRAG is an efficient, scalable solution for real-world long-context, multi-hop reasoning, with code publicly available.

Abstract: Long-context large language models (LC LLMs) combined with
retrieval-augmented generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained
windows, and fragmented information from suboptimal context construction. We
introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG
framework that compresses and partitions documents into coarse-to-fine
granularities, then adaptively merges relevant contexts through real-time
chunk- and document-level expansions. By initiating with finest-level retrieval
and progressively incorporating broader, higher-level context, MacRAG
constructs effective query-specific long contexts, optimizing both precision
and coverage. Evaluations on challenging LongBench expansions of HotpotQA,
2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG
pipelines in single- and multi-step generation using Llama-3.1-8B,
Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient,
scalable solution for real-world long-context, multi-hop reasoning. Our code is
available at https://github.com/Leezekun/MacRAG.

</details>


### [220] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/pdf/2505.09662)
*Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger*

Main category: cs.CL

TL;DR: LLMs outperform humans in persuasive tasks, increasing accuracy when truthful and decreasing it when deceptive, highlighting the need for AI governance.


<details>
  <summary>Details</summary>
Motivation: To compare the persuasion capabilities of LLMs (Claude Sonnet 3.5) against incentivized humans in a real-time conversational quiz setting.

Method: A preregistered, large-scale experiment where LLMs and humans attempted to persuade quiz takers toward correct or incorrect answers.

Result: LLMs achieved higher compliance than humans, improving accuracy for truthful persuasion and reducing it for deceptive persuasion.

Conclusion: AI's persuasion capabilities surpass humans, emphasizing the urgency for alignment and governance frameworks.

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [221] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/pdf/2505.10446)
*Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT is a reasoning framework for diffusion language models that optimizes reasoning trajectories using outcome-based RL, outperforming traditional methods on math and code tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in diffusion language models by allowing bidirectional, non-linear thinking and optimizing the entire reasoning trajectory for correctness.

Method: Uses Reinforcement Learning (RL) to optimize intermediate diffusion steps, implemented on SEDD (continuous-time) and LLaDA (discrete-time) models with specific policies.

Result: DCoLT-reinforced models outperform others, with LLaDA showing significant accuracy boosts on GSM8K, MATH, MBPP, and HumanEval tasks.

Conclusion: DCoLT effectively improves reasoning in diffusion models, demonstrating superior performance on complex tasks.

Abstract: We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning
framework for diffusion language models. DCoLT treats each intermediate step in
the reverse diffusion process as a latent "thinking" action and optimizes the
entire reasoning trajectory to maximize the reward on the correctness of the
final answer with outcome-based Reinforcement Learning (RL). Unlike traditional
Chain-of-Thought (CoT) methods that follow a causal, linear thinking process,
DCoLT allows bidirectional, non-linear reasoning with no strict rule on
grammatical correctness amid its intermediate steps of thought. We implement
DCoLT on two representative Diffusion Language Models (DLMs). First, we choose
SEDD as a representative continuous-time discrete diffusion model, where its
concrete score derives a probabilistic policy to maximize the RL reward over
the entire sequence of intermediate diffusion steps. We further consider the
discrete-time masked diffusion language model -- LLaDA, and find that the order
to predict and unmask tokens plays an essential role to optimize its RL action
resulting from the ranking-based Unmasking Policy Module (UPM) defined by the
Plackett-Luce model. Experiments on both math and code generation tasks show
that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform
other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA
boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,
MBPP, and HumanEval.

</details>


### [222] [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/pdf/2505.10717)
*Jean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni, Lucas Caccia, François Beaulieu, Thomas Lin, Jens Kleesiek, Paul Vozila*

Main category: cs.CL

TL;DR: A framework for adapting small language models (SLMs) into high-performing clinical models, achieving significant improvements over base models and GPT-4.


<details>
  <summary>Details</summary>
Motivation: High computation costs and latency of large language models like GPT-4 limit clinical deployment, while SLMs require biomedical domain adaptation and face data sensitivity issues.

Method: Proposes a framework involving pre-instruction tuning, model merging, and clinical-tasks alignment, using the MediPhi collection and extended CLUE+ benchmark.

Result: Expert models show improvements: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4 by 14%). MediPhi preserves gains, and MediFlow dataset supports further alignment.

Conclusion: The framework successfully adapts SLMs for clinical use, outperforming larger models and addressing data and cost challenges.

Abstract: High computation costs and latency of large language models such as GPT-4
have limited their deployment in clinical settings. Small language models
(SLMs) offer a cost-effective alternative, but their limited capacity requires
biomedical domain adaptation, which remains challenging. An additional
bottleneck is the unavailability and high sensitivity of clinical data. To
address these challenges, we propose a novel framework for adapting SLMs into
high-performing clinical models. We introduce the MediPhi collection of
3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning
of experts on relevant medical and clinical corpora (PMC, Medical Guideline,
MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most
clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our
expert models deliver relative improvements on this benchmark over the base
model without any task-specific fine-tuning: 64.3% on medical entities, 49.5%
on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by
14%). We unify the expert models into MediPhi via model merging, preserving
gains across benchmarks. Furthermore, we built the MediFlow collection, a
synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP
tasks, 98 fine-grained document types, and JSON format support. Alignment of
MediPhi using supervised fine-tuning and direct preference optimization
achieves further gains of 18.9% on average.

</details>


### [223] [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/pdf/2505.11436)
*Yiming Lei, Chenkai Zhang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CL

TL;DR: GODBench is a new benchmark for evaluating MLLMs' ability to create creative video comments, and Ripple of Thought (RoT) is proposed to enhance their creativity.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs and benchmarks struggle with generating creative expressions like jokes and satire in video comments, limiting exploration of creativity.

Method: Introduce GODBench for multimodal evaluation and propose RoT, a multi-step reasoning framework inspired by wave propagation.

Result: Existing MLLs and CoT methods perform poorly in creative tasks, while RoT shows promise in enhancing creativity.

Conclusion: RoT and GODBench offer potential advancements in MLLM-based creativity, with GODBench publicly available for further research.

Abstract: Video Comment Art enhances user engagement by providing creative content that
conveys humor, satire, or emotional resonance, requiring a nuanced and
comprehensive grasp of cultural and contextual subtleties. Although Multimodal
Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated
strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they
still struggle to generate creative expressions such as resonant jokes and
insightful satire. Moreover, existing benchmarks are constrained by their
limited modalities and insufficient categories, hindering the exploration of
comprehensive creativity in video-based Comment Art creation. To address these
limitations, we introduce GODBench, a novel benchmark that integrates video and
text modalities to systematically evaluate MLLMs' abilities to compose Comment
Art. Furthermore, inspired by the propagation patterns of waves in physics, we
propose Ripple of Thought (RoT), a multi-step reasoning framework designed to
enhance the creativity of MLLMs. Extensive experiments reveal that existing
MLLMs and CoT methods still face significant challenges in understanding and
generating creative video comments. In contrast, RoT provides an effective
approach to improve creative composing, highlighting its potential to drive
meaningful advancements in MLLM-based creativity. GODBench is publicly
available at https://github.com/stan-lei/GODBench-ACL2025.

</details>


### [224] [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English and Arabic](https://arxiv.org/pdf/2505.11959)
*Wajdi Zaghouani, Md. Rafiul Biswas*

Main category: cs.CL

TL;DR: A bilingual dataset (Arabic and English) for emotions and hope speech is introduced, with high annotator agreement and baseline validation.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of multi-emotion datasets for underrepresented languages.

Method: Dataset creation with detailed annotations, validated using Fleiss' Kappa and a baseline machine learning model.

Result: High annotator agreement (0.75-0.85) and baseline micro-F1-Score of 0.67.

Conclusion: The dataset is a valuable resource for NLP in underrepresented languages and cross-linguistic emotion analysis.

Abstract: This research introduces a bilingual dataset comprising 23,456 entries for
Arabic and 10,036 entries for English, annotated for emotions and hope speech,
addressing the scarcity of multi-emotion (Emotion and hope) datasets. The
dataset provides comprehensive annotations capturing emotion intensity,
complexity, and causes, alongside detailed classifications and subcategories
for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,
revealing 0.75-0.85 agreement among annotators both for Arabic and English
language. The evaluation metrics (micro-F1-Score=0.67) obtained from the
baseline model (i.e., using a machine learning model) validate that the data
annotations are worthy. This dataset offers a valuable resource for advancing
natural language processing in underrepresented languages, fostering better
cross-linguistic analysis of emotions and hope speech.

</details>


### [225] [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/pdf/2505.12545)
*Yang Zhao, Pu Wang, Yibo Zhao, Hongru Du, Hao Frank Yang*

Main category: cs.CL

TL;DR: TrafficSafe, an LLM-based framework, improves crash prediction by 42% F1-score and identifies key risk factors like alcohol-impaired driving through multi-modal data analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to interpret complex multi-modal crash data, limiting their ability to identify critical risk factors.

Method: TrafficSafe adapts LLMs for text-based reasoning on a multi-modal crash dataset (58,903 reports) and introduces a feature attribution framework.

Result: 42% average F1-score improvement over baselines; alcohol-impaired driving identified as the leading severe crash factor.

Conclusion: TrafficSafe advances traffic safety research by leveraging AI for actionable, life-saving outcomes.

Abstract: Predicting crash events is crucial for understanding crash distributions and
their contributing factors, thereby enabling the design of proactive traffic
safety policy interventions. However, existing methods struggle to interpret
the complex interplay among various sources of traffic crash data, including
numeric characteristics, textual reports, crash imagery, environmental
conditions, and driver behavior records. As a result, they often fail to
capture the rich semantic information and intricate interrelationships embedded
in these diverse data sources, limiting their ability to identify critical
crash risk factors. In this research, we propose TrafficSafe, a framework that
adapts LLMs to reframe crash prediction and feature attribution as text-based
reasoning. A multi-modal crash dataset including 58,903 real-world reports
together with belonged infrastructure, environmental, driver, and vehicle
information is collected and textualized into TrafficSafe Event Dataset. By
customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves
a 42% average improvement in F1-score over baselines. To interpret these
predictions and uncover contributing factors, we introduce TrafficSafe
Attribution, a sentence-level feature attribution framework enabling
conditional risk analysis. Findings show that alcohol-impaired driving is the
leading factor in severe crashes, with aggressive and impairment-related
behaviors having nearly twice the contribution for severe crashes compared to
other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal
features during model training, guiding strategic crash data collection for
iterative performance improvements. The proposed TrafficSafe offers a
transformative leap in traffic safety research, providing a blueprint for
translating advanced AI technologies into responsible, actionable, and
life-saving outcomes.

</details>


### [226] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/pdf/2505.13761)
*Jacob Kleiman, Kevin Frank, Joseph Voyles, Sindy Campagna*

Main category: cs.CL

TL;DR: A framework combining simulations and LLMs to enable intuitive, accurate modeling of real-world systems for non-technical users.


<details>
  <summary>Details</summary>
Motivation: Simulations are complex for non-technical users, while LLMs lack structured understanding of real-world dynamics.

Method: Integrates simulations and LLMs to leverage conversational ease and structured accuracy.

Result: Provides a robust, generalizable foundation for empirical validation across domains.

Conclusion: The framework bridges gaps, offering accessible and reliable modeling of complex systems.

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [227] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/pdf/2505.14172)
*Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu*

Main category: cs.CL

TL;DR: LLMs struggle with character-level tasks due to tokenization, but a lightweight modification improves performance while retaining subword model advantages.


<details>
  <summary>Details</summary>
Motivation: LLMs fail at simple character-level tasks like counting letters, highlighting a limitation caused by tokenization.

Method: Analyzed 19 synthetic tasks to study character-level reasoning, proposing a percolation-based model and an architectural fix.

Result: Character-level capabilities emerge late in training; the proposed modification significantly improves performance.

Conclusion: The work bridges perceptual gaps in tokenized LMs and offers a framework to address their structural blind spots.

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [228] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/pdf/2505.14350)
*Jialong Han, Si Zhang, Ke Zhang*

Main category: cs.CL

TL;DR: OSoRA is a new PEFT method for LLMs that combines SVD with learnable scaling vectors, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is computationally expensive; PEFT methods like LoRA exist but still require significant resources.

Method: OSoRA extends LoRA by integrating SVD and learnable scaling vectors, freezing singular vector matrices and optimizing an output-dimension vector.

Result: OSoRA reduces computational costs, achieves comparable or better performance than LoRA and VeRA, and scales linearly with rank.

Conclusion: Jointly training singular values and output-dimension vectors is key to OSoRA's success, offering a resource-efficient alternative for LLM fine-tuning.

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [229] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/pdf/2505.14367)
*Jialong Han, Si Zhang, Ke Zhang*

Main category: cs.CL

TL;DR: DuDe improves LoRA-based PEFT by decomposing weight matrices for stable training and efficient knowledge transfer, achieving high accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA-based methods suffer from unstable training and inefficient knowledge transfer due to random initialization.

Method: DuDe decomposes weight matrices into magnitude and direction using SVD for principled initialization.

Result: Achieves 48.35% accuracy on MMLU and 62.53% (±1.59) on GSM8K, showing superior performance.

Conclusion: DuDe enhances optimization stability and preserves pre-trained representations, advancing PEFT for LLMs.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [230] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/pdf/2505.14552)
*Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang*

Main category: cs.CL

TL;DR: KORGym is introduced as a dynamic evaluation platform for assessing LLMs' reasoning, outperforming domain-specific benchmarks and revealing insights into model performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are domain-specific and fail to fully evaluate LLMs' general reasoning capabilities.

Method: KORGym, a platform with over 50 games in textual/visual formats, supports interactive, multi-turn assessments and reinforcement learning.

Result: Experiments on 19 LLMs and 8 VLMs show consistent reasoning patterns and superior performance of closed-source models.

Conclusion: KORGym is a valuable tool for advancing LLM reasoning research and evaluation in complex, interactive settings.

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [231] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/pdf/2505.14406)
*Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu*

Main category: cs.CL

TL;DR: PhantomCircuit is a framework to analyze knowledge overshadowing in LLMs, revealing its origins and mechanisms during training.


<details>
  <summary>Details</summary>
Motivation: Current understanding of knowledge overshadowing is limited to inference-time observations, lacking insights into its training-time origins.

Method: PhantomCircuit uses knowledge circuit analysis to dissect attention heads and trace competing knowledge pathways.

Result: The framework effectively identifies overshadowing instances, offering new insights into this hallucination.

Conclusion: PhantomCircuit provides a methodological lens for mitigating knowledge overshadowing in LLMs.

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [232] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/pdf/2505.14684)
*Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper introduces CoT Thought Leap Bridge Task to detect and fill gaps in Chain-of-Thought reasoning, improving model performance on mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical CoT datasets often lack intermediate steps (Thought Leaps), hindering model learning and generalization.

Method: Proposes CoT-Bridge to detect leaps and generate missing steps, using the ScaleQM+ dataset for training.

Result: Models fine-tuned on bridged datasets outperform original ones, with up to +5.87% improvement on NuminaMath.

Conclusion: Enhancing reasoning completeness via CoT-Bridge broadly benefits performance and generalization, even in out-of-domain tasks.

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [233] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/pdf/2505.14481)
*He Zhu, Junyou Su, Minxin Chen, Wen Wang, Yijie Deng, Guanhua Chen, Wenjia Zhang*

Main category: cs.CL

TL;DR: PlanGPT-VL is a domain-specific Vision-Language Model for urban planning maps, outperforming general VLMs with innovative data synthesis, structured verification, and efficient training.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with urban planning maps due to their specialized spatial and regulatory requirements.

Method: PlanGPT-VL uses PlanAnno-V for data synthesis, Critical Point Thinking for verification, and a hybrid training approach.

Result: It outperforms general VLMs in map interpretation, achieves high accuracy, and is efficient (7B parameters).

Conclusion: PlanGPT-VL provides a reliable tool for urban planning professionals, balancing performance and efficiency.

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [234] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/pdf/2505.14553)
*Abhimanyu Talwar, Julien Laasri*

Main category: cs.CL

TL;DR: Using Hindi as a pivot language, the paper improves Nepali-to-English translation via Transfer Method and Backtranslation, achieving a 14.2 SacreBLEU score.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large, diverse parallel corpora for certain language pairs by leveraging a pivot language (Hindi).

Method: Two approaches: Transfer Method (fully supervised) and Backtranslation (semi-supervised) for translating Nepali to English via Hindi.

Result: Achieved a devtest SacreBLEU score of 14.2, improving the baseline by 6.6 points, though slightly below the semi-supervised baseline of 15.1.

Conclusion: Hindi is a viable pivot for Nepali-English translation, with room for improvement in semi-supervised methods.

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [235] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/pdf/2505.14590)
*Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper proposes MCIP, a refined version of MCP, to address safety risks in decentralized architectures. It introduces a taxonomy for unsafe behaviors, benchmarks, and training data to improve LLMs' safety performance.


<details>
  <summary>Details</summary>
Motivation: MCP's decentralized architecture poses underexplored safety risks, necessitating systematic analysis and solutions.

Method: The MAESTRO framework guides the analysis of MCP's safety gaps, leading to MCIP. A taxonomy of unsafe behaviors is developed, alongside benchmarks and training data for LLMs.

Result: Experiments show LLMs' vulnerabilities in MCP interactions and substantial safety improvements with the proposed approach.

Conclusion: The paper successfully enhances MCP safety through MCIP, a taxonomy, and benchmarks, improving LLMs' performance in identifying risks.

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps. Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [236] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/pdf/2505.14631)
*Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei*

Main category: cs.CL

TL;DR: LHRMs adaptively decide when to use extended thinking, improving efficiency over LRMs and LLMs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of LRMs due to unnecessary lengthy thinking for simple queries.

Method: Two-stage training: Hybrid Fine-Tuning (HFT) and online reinforcement learning with Hybrid Group Policy Optimization (HGPO). Introduces Hybrid Accuracy metric.

Result: LHRMs outperform LRMs and LLMs in reasoning and efficiency, adapting to query difficulty.

Conclusion: Advocates for hybrid thinking systems, balancing performance and efficiency.

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [237] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/pdf/2505.14652)
*Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen*

Main category: cs.CL

TL;DR: General-Reasoner enhances LLM reasoning across diverse domains by using a novel training paradigm, a large-scale dataset, and a generative model-based verifier, outperforming baselines on 12 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning works focus on math/coding due to data abundance and easy verification, limiting broader applicability. General-Reasoner aims to address this gap.

Method: Proposes a training paradigm with (1) a large, diverse dataset from web crawling and (2) a generative model-based verifier for chain-of-thought and context-aware verification.

Result: Outperforms baselines on 12 benchmarks (e.g., MMLU-Pro, GPQA), showing robust, generalizable reasoning while excelling in math tasks.

Conclusion: General-Reasoner successfully broadens LLM reasoning capabilities beyond math/coding, demonstrating effectiveness across diverse domains.

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [238] [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/pdf/2505.14699)
*Miguel Lopez-Duran, Julian Fierrez, Aythami Morales, Ruben Tolosana, Oscar Delgado-Mohatar, Alvaro Ortigosa*

Main category: cs.CV

TL;DR: The paper benchmarks GNNs for layout classification in PDFs, introducing two graph structures and multimodal features. GraphSAGE on a k-closest-neighbor graph in a dual-branch setup performs best.


<details>
  <summary>Details</summary>
Motivation: Challenges in analyzing PDF layouts due to heterogeneous elements and imprecise metadata.

Method: Evaluates GNNs with two graph structures (k-closest-neighbor, fully connected) and multimodal features (text/vision). Tests single-modality, concatenated, and dual-branch frameworks.

Result: GraphSAGE on k-closest-neighbor graph in dual-branch mode achieves highest accuracy, outperforming baselines.

Conclusion: Local layout relationships and multimodal fusion via GNNs are key for digital document layout analysis.

Abstract: The automatic analysis of document layouts in digital-born PDF documents
remains a challenging problem due to the heterogeneous arrangement of textual
and nontextual elements and the imprecision of the textual metadata in the
Portable Document Format. In this work, we benchmark Graph Neural Network (GNN)
architectures for the task of fine-grained layout classification of text blocks
from digital native documents. We introduce two graph construction structures:
a k-closest-neighbor graph and a fully connected graph, and generate node
features via pre-trained text and vision models, thus avoiding manual feature
engineering. Three experimental frameworks are evaluated: single-modality (text
or visual), concatenated multimodal, and dual-branch multimodal. We evaluated
four foundational GNN models and compared them with the baseline. Our
experiments are specifically conducted on a rich dataset of public affairs
documents that includes more than 20 sources (e.g., regional and national-level
official gazettes), 37K PDF documents, with 441K pages in total. Our results
demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a
dual-branch configuration achieves the highest per-class and overall accuracy,
outperforming the baseline in some sources. These findings confirm the
importance of local layout relationships and multimodal fusion exploited
through GNNs for the analysis of native digital document layouts.

</details>


### [239] [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/pdf/2505.14728)
*Xiao Lin, Zhining Liu, Ze Yang, Gaotang Li, Ruizhong Qiu, Shuke Wang, Hui Liu, Haotian Li, Sumit Keswani, Vishwa Pardeshi, Huijun Zhao, Wei Fan, Hanghang Tong*

Main category: cs.CV

TL;DR: The paper introduces MORALISE, a benchmark for evaluating moral alignment in vision-language models using real-world data, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: To address gaps in moral alignment evaluation for vision-language models, which currently lack realism and diversity due to reliance on AI-generated or text-only data.

Method: Developed MORALISE, a benchmark with 2,481 expert-verified image-text pairs, annotated for moral topics and modalities, and evaluated 19 VLMs on moral judgment and norm attribution tasks.

Result: Current VLMs struggle with moral alignment, showing persistent limitations in handling morally salient content.

Conclusion: MORALISE provides a robust tool for assessing and improving moral alignment in VLMs, highlighting the need for further research.

Abstract: Warning: This paper contains examples of harmful language and images. Reader
discretion is advised. Recently, vision-language models have demonstrated
increasing influence in morally sensitive domains such as autonomous driving
and medical analysis, owing to their powerful multimodal reasoning
capabilities. As these models are deployed in high-stakes real-world
applications, it is of paramount importance to ensure that their outputs align
with human moral values and remain within moral boundaries. However, existing
work on moral alignment either focuses solely on textual modalities or relies
heavily on AI-generated images, leading to distributional biases and reduced
realism. To overcome these limitations, we introduce MORALISE, a comprehensive
benchmark for evaluating the moral alignment of vision-language models (VLMs)
using diverse, expert-verified real-world data. We begin by proposing a
comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,
spanning the personal, interpersonal, and societal moral domains encountered in
everyday life. Built on this framework, we manually curate 2,481 high-quality
image-text pairs, each annotated with two fine-grained labels: (1) topic
annotation, identifying the violated moral topic(s), and (2) modality
annotation, indicating whether the violation arises from the image or the text.
For evaluation, we encompass two tasks, \textit{moral judgment} and
\textit{moral norm attribution}, to assess models' awareness of moral
violations and their reasoning ability on morally salient content. Extensive
experiments on 19 popular open- and closed-source VLMs show that MORALISE poses
a significant challenge, revealing persistent moral limitations in current
state-of-the-art models. The full benchmark is publicly available at
https://huggingface.co/datasets/Ze1025/MORALISE.

</details>


### [240] [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation](https://arxiv.org/pdf/2505.14705)
*Xin Zhang, Ziruo Zhang, Jiawei Du, Zuozhu Liu, Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: RepBlend addresses modality collapse in Multimodal Dataset Distillation (MDD) by blending representations and introducing symmetric projection trajectory matching, improving cross-modal alignment and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing MDD methods suffer from modality collapse and asymmetric supervision, hindering cross-modal learning effectiveness.

Method: RepBlend weakens overdominant cross-modal supervision via representation blending and uses symmetric projection trajectory matching for balanced optimization.

Result: RepBlend outperforms prior MDD methods, achieving significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10) and faster distillation (up to 6.7× speedup).

Conclusion: RepBlend effectively mitigates modality collapse and asymmetric supervision, enhancing cross-modal learning and distillation efficiency.

Abstract: Multimodal Dataset Distillation (MDD) seeks to condense large-scale
image-text datasets into compact surrogates while retaining their effectiveness
for cross-modal learning. Despite recent progress, existing MDD approaches
often suffer from \textit{\textbf{Modality Collapse}}, characterized by
over-concentrated intra-modal representations and enlarged distributional gap
across modalities. In this paper, at the first time, we identify this issue as
stemming from a fundamental conflict between the over-compression behavior
inherent in dataset distillation and the cross-modal supervision imposed by
contrastive objectives. To alleviate modality collapse, we introduce
\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal
supervision via representation blending, thereby significantly enhancing
intra-modal diversity. Additionally, we observe that current MDD methods impose
asymmetric supervision across modalities, resulting in biased optimization. To
address this, we propose symmetric projection trajectory matching, which
synchronizes the optimization dynamics using modality-specific projection
heads, thereby promoting balanced supervision and enhancing cross-modal
alignment. Experiments on Flickr-30K and MS-COCO show that RepBlend
consistently outperforms prior state-of-the-art MDD methods, achieving
significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under
the 100-pair setting) and offering up to 6.7$\times$ distillation speedup.

</details>


### [241] [CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity](https://arxiv.org/pdf/2505.14707)
*Georgiana Manolache, Gerard Schouten, Joaquin Vanschoren*

Main category: cs.CV

TL;DR: CrypticBio is a large multimodal dataset for AI models, focusing on visually confusing species, with 52K cryptic groups and 166M images, addressing biodiversity challenges.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for cryptic species are small and limited, lacking diversity and scale. CrypticBio fills this gap by leveraging real-world misidentification trends.

Method: Curated from iNaturalist, CrypticBio includes 52K cryptic groups across 67K species, with rich annotations (taxonomy, spatiotemporal data). An open-source pipeline, CrypticBio-Curate, aids dataset curation.

Result: Benchmarks show geographical context enhances vision-language zero-shot learning for cryptic species. The dataset supports multimodal AI in biodiversity.

Conclusion: CrypticBio advances biodiversity AI by addressing species ambiguity, offering a scalable, real-world-ready resource.

Abstract: We present CrypticBio, the largest publicly available multimodal dataset of
visually confusing species, specifically curated to support the development of
AI models in the context of biodiversity applications. Visually confusing or
cryptic species are groups of two or more taxa that are nearly
indistinguishable based on visual characteristics alone. While much existing
work addresses taxonomic identification in a broad sense, datasets that
directly address the morphological confusion of cryptic species are small,
manually curated, and target only a single taxon. Thus, the challenge of
identifying such subtle differences in a wide range of taxa remains
unaddressed. Curated from real-world trends in species misidentification among
community annotators of iNaturalist, CrypticBio contains 52K unique cryptic
groups spanning 67K species, represented in 166 million images. Rich
research-grade image annotations--including scientific, multicultural, and
multilingual species terminology, hierarchical taxonomy, spatiotemporal
context, and associated cryptic groups--address multimodal AI in biodiversity
research. For easy dataset curation, we provide an open-source pipeline
CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language
arises from the integration of geographical and temporal data as complementary
cues to identifying cryptic species. To highlight the importance of the
dataset, we benchmark a suite of state-of-the-art foundation models across
CrypticBio subsets of common, unseen, endangered, and invasive species, and
demonstrate the substantial impact of geographical context on vision-language
zero-shot learning for cryptic species. By introducing CrypticBio, we aim to
catalyze progress toward real-world-ready biodiversity AI models capable of
handling the nuanced challenges of species ambiguity.

</details>


### [242] [DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance](https://arxiv.org/pdf/2505.14708)
*Xuan Shen, Chenxia Han, Yufa Zhou, Yanyue Xie, Yifan Gong, Quanyi Wang, Yiwei Wang, Yanzhi Wang, Pu Zhao, Jiuxiang Gu*

Main category: cs.CV

TL;DR: DraftAttention accelerates video diffusion transformers by using dynamic sparse attention on GPUs, reducing latency and improving scalability without training.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of attention in diffusion transformer-based video generation models (DiTs) limits practical application and scalability.

Method: Proposes DraftAttention, a training-free framework using dynamic sparse attention. It down-samples feature maps, reorders queries, keys, and values based on a draft attention map, and restores order post-computation.

Result: Achieves up to 1.75x speedup on GPUs and outperforms existing sparse attention methods in video generation quality.

Conclusion: DraftAttention effectively reduces computational costs while maintaining generation quality, making DiTs more practical for real-world applications.

Abstract: Diffusion transformer-based video generation models (DiTs) have recently
attracted widespread attention for their excellent generation quality. However,
their computational cost remains a major bottleneck-attention alone accounts
for over 80% of total latency, and generating just 8 seconds of 720p video
takes tens of minutes-posing serious challenges to practical application and
scalability. To address this, we propose the DraftAttention, a training-free
framework for the acceleration of video diffusion transformers with dynamic
sparse attention on GPUs. We apply down-sampling to each feature map across
frames in the compressed latent space, enabling a higher-level receptive field
over the latent composed of hundreds of thousands of tokens. The low-resolution
draft attention map, derived from draft query and key, exposes redundancy both
spatially within each feature map and temporally across frames. We reorder the
query, key, and value based on the draft attention map to guide the sparse
attention computation in full resolution, and subsequently restore their
original order after the attention computation. This reordering enables
structured sparsity that aligns with hardware-optimized execution. Our
theoretical analysis demonstrates that the low-resolution draft attention
closely approximates the full attention, providing reliable guidance for
constructing accurate sparse attention. Experimental results show that our
method outperforms existing sparse attention approaches in video generation
quality and achieves up to 1.75x end-to-end speedup on GPUs. Code:
https://github.com/shawnricecake/draft-attention

</details>


### [243] [FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge](https://arxiv.org/pdf/2505.14709)
*Xuan Shen, Weize Ma, Yufa Zhou, Enhao Tang, Yanyue Xie, Zhengang Li, Yifan Gong, Quanyi Wang, Henghui Ding, Yiwei Wang, Yanzhi Wang, Pu Zhao, Jun Lin, Jiuxiang Gu*

Main category: cs.CV

TL;DR: FastCar accelerates AR video generation by reducing redundant MLP computations in the decode phase using temporal redundancy and a replay strategy, achieving 2.1x speedup and higher energy efficiency.


<details>
  <summary>Details</summary>
Motivation: AR models for video generation face high computational overhead due to redundant MLP operations in the decode phase. FastCar addresses this by exploiting temporal redundancy.

Method: Proposes FastCar with Temporal Attention Score (TAS) to decide when to reuse cached MLP outputs (replay strategy) and a hardware accelerator with Dynamic Resource Scheduling (DRS).

Result: Achieves 2.1x decoding speedup, higher energy efficiency, and improved performance for high-resolution, long-duration videos when combined with sparse attention.

Conclusion: FastCar effectively reduces computational redundancy in AR video generation, offering significant speed and efficiency gains.

Abstract: Auto-regressive (AR) models, initially successful in language generation,
have recently shown promise in visual generation tasks due to their superior
sampling efficiency. Unlike image generation, video generation requires a
substantially larger number of tokens to produce coherent temporal frames,
resulting in significant overhead during the decoding phase. Our key
observations are: (i) MLP modules in the decode phase dominate the inference
latency, and (ii) there exists high temporal redundancy in MLP outputs of
adjacent frames. In this paper, we propose the \textbf{FastCar} framework to
accelerate the decode phase for the AR video generation by exploring the
temporal redundancy. The Temporal Attention Score (TAS) is proposed to
determine whether to apply the replay strategy (\textit{i.e.}, reusing cached
MLP outputs from the previous frame to reduce redundant computations) with
detailed theoretical analysis and justification. Also, we develop a hardware
accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to
enable better resource utilization and faster inference. Experimental results
demonstrate the effectiveness of our method, which outperforms traditional
sparse attention approaches with more than 2.1x decoding speedup and higher
energy efficiency on the edge. Furthermore, by combining FastCar and sparse
attention, FastCar can boost the performance of sparse attention with
alleviated drifting, demonstrating our unique advantages for high-resolution
and long-duration video generation. Code:
https://github.com/shawnricecake/fast-car

</details>


### [244] [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/pdf/2505.15489)
*Jiaying Wu, Fanxiao Li, Min-Yen Kan, Bryan Hooi*

Main category: cs.CV

TL;DR: The paper introduces an automated framework for modeling creator intent in multimodal misinformation, evaluates VLMs on intent-centric tasks, and highlights their shortcomings.


<details>
  <summary>Details</summary>
Motivation: Understanding misleading creator intent is crucial for effective multimodal misinformation detection (MMD) systems.

Method: An automated framework models creator intent via desired influence and execution plan, creating the DeceptionDecoded dataset with 12,000 image-caption pairs.

Result: Current VLMs perform poorly in recognizing misleading intent, relying on superficial cues.

Conclusion: Intent-aware modeling is essential for advancing MMD systems to reason deeper about misinformation.

Abstract: The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.

</details>


### [245] [KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection](https://arxiv.org/pdf/2505.14714)
*Tuan-Vinh La, Minh-Hieu Nguyen, Minh-Son Dao*

Main category: cs.CV

TL;DR: A novel multi-modal fake news detection framework integrates visual, textual, and knowledge-based representations, outperforming existing methods by leveraging fine-grained object details, global image semantics, and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in fake news detection, such as neglecting local object-level details and lacking external knowledge integration, to improve semantic understanding and veracity prediction.

Method: Combines bottom-up attention for object details, CLIP for global image semantics, RoBERTa for text encoding, and knowledge graph entities, fused via a Transformer-based classifier.

Result: Outperforms recent approaches, demonstrating effectiveness of neighbor selection and multi-modal fusion.

Conclusion: Introduces knowledge-grounded multimodal reasoning, shifting fake news detection to semantically grounded verification, with publicly available code for reproducibility.

Abstract: Fake news detection remains a challenging problem due to the complex
interplay between textual misinformation, manipulated images, and external
knowledge reasoning. While existing approaches have achieved notable results in
verifying veracity and cross-modal consistency, two key challenges persist: (1)
Existing methods often consider only the global image context while neglecting
local object-level details, and (2) they fail to incorporate external knowledge
and entity relationships for deeper semantic understanding. To address these
challenges, we propose a novel multi-modal fake news detection framework that
integrates visual, textual, and knowledge-based representations. Our approach
leverages bottom-up attention to capture fine-grained object details, CLIP for
global image semantics, and RoBERTa for context-aware text encoding. We further
enhance knowledge utilization by retrieving and adaptively selecting relevant
entities from a knowledge graph. The fused multi-modal features are processed
through a Transformer-based classifier to predict news veracity. Experimental
results demonstrate that our model outperforms recent approaches, showcasing
the effectiveness of neighbor selection mechanism and multi-modal fusion for
fake news detection. Our proposal introduces a new paradigm: knowledge-grounded
multimodal reasoning. By integrating explicit entity-level selection and
NLI-guided filtering, we shift fake news detection from feature fusion to
semantically grounded verification. For reproducibility and further research,
the source code is publicly at
\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.

</details>


### [246] [Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection](https://arxiv.org/pdf/2505.14718)
*Guoxuan Mao, Ting Cao, Ziyang Li, Yuan Dong*

Main category: cs.CV

TL;DR: SPENet improves semantic segmentation consistency for fixed components in industrial images by focusing on object shapes, introducing VBD for fuzzy boundaries, and proposing CMSE for consistency measurement, achieving top accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Conventional models lack segmentation consistency for fixed components in varying contexts due to poor contour perception, and industrial settings demand efficient, low-complexity solutions.

Method: SPENet separately supervises boundary and body extraction, introduces Variable Boundary Domain (VBD) for fuzzy boundaries, and uses Consistency Mean Square Error (CMSE) for evaluation.

Result: SPENet achieves the best segmentation accuracy and competitive speed, reducing CMSE by over 50% compared to top models.

Conclusion: SPENet addresses segmentation consistency and efficiency in industrial settings, outperforming state-of-the-art models in accuracy and speed.

Abstract: Semantic segmentation stands as a pivotal research focus in computer vision.
In the context of industrial image inspection, conventional semantic
segmentation models fail to maintain the segmentation consistency of fixed
components across varying contextual environments due to a lack of perception
of object contours. Given the real-time constraints and limited computing
capability of industrial image detection machines, it is also necessary to
create efficient models to reduce computational complexity. In this work, a
Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes
of objects to achieve excellent segmentation consistency by separately
supervising the extraction of boundary and body information from images. In
SPENet, a novel method is introduced for describing fuzzy boundaries to better
adapt to real-world scenarios named Variable Boundary Domain (VBD).
Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to
measure segmentation consistency for fixed components. Our approach attains the
best segmentation accuracy and competitive speed on our dataset, showcasing
significant advantages in CMSE among numerous state-of-the-art real-time
segmentation networks, achieving a reduction of over 50% compared to the
previously top-performing models.

</details>


### [247] [MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion](https://arxiv.org/pdf/2505.14719)
*Wei Hua, Chenlin Zhou, Jibin Wu, Yansong Chua, Yangyang Shu*

Main category: cs.CV

TL;DR: MSVIT, a spike-driven Transformer with multi-scale spiking attention (MSSA), outperforms existing SNN-based models, bridging the performance gap with ANN-based transformers.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck in feature extraction across image scales in SNN-based transformer architectures.

Method: Proposes MSVIT with multi-scale spiking attention (MSSA) to enhance spiking attention blocks.

Result: MSVIT achieves state-of-the-art performance on various datasets.

Conclusion: MSVIT is a promising solution for energy-efficient, high-performance SNN-transformer architectures.

Abstract: The combination of Spiking Neural Networks(SNNs) with Vision Transformer
architectures has attracted significant attention due to the great potential
for energy-efficient and high-performance computing paradigms. However, a
substantial performance gap still exists between SNN-based and ANN-based
transformer architectures. While existing methods propose spiking
self-attention mechanisms that are successfully combined with SNNs, the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting features from different image scales. In this paper, we address this
issue and propose MSVIT, a novel spike-driven Transformer architecture, which
firstly uses multi-scale spiking attention (MSSA) to enrich the capability of
spiking attention blocks. We validate our approach across various main data
sets. The experimental results show that MSVIT outperforms existing SNN-based
models, positioning itself as a state-of-the-art solution among SNN-transformer
architectures. The codes are available at
https://github.com/Nanhu-AI-Lab/MSViT.

</details>


### [248] [Uncovering Cultural Representation Disparities in Vision-Language Models](https://arxiv.org/pdf/2505.14729)
*Ram Mohan Rao Kadiyala, Siddhant Gupta, Jebish Purbey, Srishti Yadav, Alejandro Salamanca, Desmond Elliott*

Main category: cs.CV

TL;DR: The paper investigates cultural biases in Vision-Language Models (VLMs) using the Country211 dataset, revealing performance disparities across countries and question formats.


<details>
  <summary>Details</summary>
Motivation: To assess and uncover cultural biases in VLMs, given their widespread use and potential for bias inheritance from training data.

Method: Evaluated VLMs on an image-based country identification task using diverse prompting strategies (open-ended, MCQs, multilingual, adversarial) on the Country211 dataset.

Result: Significant performance variations were found, indicating biases inherited from pre-training data and scaling issues.

Conclusion: VLMs exhibit cultural biases influenced by training data and evaluation methods, impacting their generalization across diverse global contexts.

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities
across a range of tasks, yet concerns about their potential biases exist. This
work investigates the extent to which prominent VLMs exhibit cultural biases by
evaluating their performance on an image-based country identification task at a
country level. Utilizing the geographically diverse Country211 dataset, we
probe several large vision language models (VLMs) under various prompting
strategies: open-ended questions, multiple-choice questions (MCQs) including
challenging setups like multilingual and adversarial settings. Our analysis
aims to uncover disparities in model accuracy across different countries and
question formats, providing insights into how training data distribution and
evaluation methodologies might influence cultural biases in VLMs. The findings
highlight significant variations in performance, suggesting that while VLMs
possess considerable visual understanding, they inherit biases from their
pre-training data and scale that impact their ability to generalize uniformly
across diverse global contexts.

</details>


### [249] [MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval](https://arxiv.org/pdf/2411.08334)
*Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee*

Main category: cs.CV

TL;DR: MIRe is a multimodal retrieval framework that avoids text-dominant issues by not fusing textual features during alignment, enhancing multimodal query understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods overly rely on text-driven signals, neglecting visual information, which limits multimodal retrieval performance.

Method: MIRe allows textual queries to attend to visual embeddings without feeding text-driven signals back into visual representations. A pre-training dataset is created by transforming question-answer pairs into extended passages.

Result: The framework shows strong performance on four multimodal retrieval benchmarks in zero-shot settings and effectively mitigates the text-dominant issue.

Conclusion: MIRe improves multimodal retrieval by balancing modality interaction and avoiding text dominance, validated by experiments and ablation studies.

Abstract: Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Moreover, our ablation studies and analyses explicitly verify the
effectiveness of our framework in mitigating the text-dominant issue. Our code
is publicly available: https://github.com/yeongjoonJu/MIRe

</details>


### [250] [Leveraging Generative AI Models to Explore Human Identity](https://arxiv.org/pdf/2505.14843)
*Yunha Yeo, Daeho Um*

Main category: cs.CV

TL;DR: The paper uses diffusion models to explore human identity by linking AI-generated face images to identity formation, confirming external factors' influence.


<details>
  <summary>Details</summary>
Motivation: To investigate human identity by leveraging AI-generated face images and drawing parallels to identity formation processes.

Method: Adopts diffusion models to generate human face images and analyzes how external inputs affect the output, relating it to human identity.

Result: Observes that external input changes significantly alter generated faces, indirectly confirming external factors' role in human identity.

Conclusion: Introduces a video artwork, "Fluidity of Human Identity," to visually express the fluid nature of identity shaped by external influences.

Abstract: This paper attempts to explore human identity by utilizing neural networks in
an indirect manner. For this exploration, we adopt diffusion models,
state-of-the-art AI generative models trained to create human face images. By
relating the generated human face to human identity, we establish a
correspondence between the face image generation process of the diffusion model
and the process of human identity formation. Through experiments with the
diffusion model, we observe that changes in its external input result in
significant changes in the generated face image. Based on the correspondence,
we indirectly confirm the dependence of human identity on external factors in
the process of human identity formation. Furthermore, we introduce
\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid
nature of human identity affected by varying external factors. The video is
available at
https://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.

</details>


### [251] [Open-Set Semi-Supervised Learning for Long-Tailed Medical Datasets](https://arxiv.org/pdf/2505.14846)
*Daniya Najiha A. Kareem, Jean Lahoud, Mustansar Fiaz, Amandeep Kumar, Hisham Cholakkal*

Main category: cs.CV

TL;DR: Proposes an open-set learning method for imbalanced medical datasets, improving performance on rare and unseen classes.


<details>
  <summary>Details</summary>
Motivation: Addressing bias and data scarcity in medical image recognition for real-world applications.

Method: Semi-supervised approach with feature-level regularization and classifier normalization.

Result: Improved closed-set and open-set accuracies on ISIC2018, ISIC2019, and TissueMNIST datasets.

Conclusion: The method effectively handles long-tail data, enhancing model generalization for medical imaging.

Abstract: Many practical medical imaging scenarios include categories that are
under-represented but still crucial. The relevance of image recognition models
to real-world applications lies in their ability to generalize to these rare
classes as well as unseen classes. Real-world generalization requires taking
into account the various complexities that can be encountered in the
real-world. First, training data is highly imbalanced, which may lead to model
exhibiting bias toward the more frequently represented classes. Moreover,
real-world data may contain unseen classes that need to be identified, and
model performance is affected by the data scarcity. While medical image
recognition has been extensively addressed in the literature, current methods
do not take into account all the intricacies in the real-world scenarios. To
this end, we propose an open-set learning method for highly imbalanced medical
datasets using a semi-supervised approach. Understanding the adverse impact of
long-tail distribution at the inherent model characteristics, we implement a
regularization strategy at the feature level complemented by a classifier
normalization technique. We conduct extensive experiments on the publicly
available datasets, ISIC2018, ISIC2019, and TissueMNIST with various numbers of
labelled samples. Our analysis shows that addressing the impact of long-tail
data in classification significantly improves the overall performance of the
network in terms of closed-set and open-set accuracies on all datasets. Our
code and trained models will be made publicly available at
https://github.com/Daniyanaj/OpenLTR.

</details>


### [252] [Colors Matter: AI-Driven Exploration of Human Feature Colors](https://arxiv.org/pdf/2505.14931)
*Rama Alyoubi, Taif Alharbi, Albatul Alghamdi, Yara Alshehri, Elham Alghamdi*

Main category: cs.CV

TL;DR: A framework using imaging and machine learning for classifying human attributes like skin tone, hair color, iris color, and vein undertones, achieving 80% accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop an inclusive and precise AI-powered system for nuanced classification of human features, applicable in beauty tech and digital personalization.

Method: Multi-stage pipeline with face detection, region segmentation, and color extraction using X-means clustering and Delta E metrics in LAB/HSV spaces.

Result: Achieves 80% accuracy in tone classification using Delta E-HSV with Gaussian blur, robust across lighting conditions.

Conclusion: Demonstrates AI's potential for precise color analysis, supporting applications in beauty tech and visual analytics.

Abstract: This study presents a robust framework that leverages advanced imaging
techniques and machine learning for feature extraction and classification of
key human attributes-namely skin tone, hair color, iris color, and vein-based
undertones. The system employs a multi-stage pipeline involving face detection,
region segmentation, and dominant color extraction to isolate and analyze these
features. Techniques such as X-means clustering, alongside perceptually uniform
distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV
color spaces to enhance the accuracy of color differentiation. For
classification, the dominant tones of the skin, hair, and iris are extracted
and matched to a custom tone scale, while vein analysis from wrist images
enables undertone classification into "Warm" or "Cool" based on LAB
differences. Each module uses targeted segmentation and color space
transformations to ensure perceptual precision. The system achieves up to 80%
accuracy in tone classification using the Delta E-HSV method with Gaussian
blur, demonstrating reliable performance across varied lighting and image
conditions. This work highlights the potential of AI-powered color analysis and
feature extraction for delivering inclusive, precise, and nuanced
classification, supporting applications in beauty technology, digital
personalization, and visual analytics.

</details>


### [253] [Programmatic Video Prediction Using Large Language Models](https://arxiv.org/pdf/2505.14948)
*Hao Tang, Kevin Ellis, Suhas Lohit, Michael J. Jones, Moitreya Chatterjee*

Main category: cs.CV

TL;DR: ProgGen uses neuro-symbolic states and LLM/VLMs for video frame prediction, outperforming competitors in tasks like PhyWorld and Cart Pole while enabling interpretable and counter-factual reasoning.


<details>
  <summary>Details</summary>
Motivation: Estimating world models for real-world dynamics is crucial for applications like video surveillance, robotics, and autonomous driving, requiring plausible visual future synthesis from initial frames.

Method: ProgGen leverages LLM/VLMs to synthesize programs for state estimation, future state prediction, and visual rendering of predicted states.

Result: Empirical evaluations show ProgGen outperforms competing methods in video frame prediction tasks, particularly in PhyWorld and Cart Pole environments.

Conclusion: ProgGen is effective and generalizable for video generation, offering interpretability and counter-factual reasoning capabilities.

Abstract: The task of estimating the world model describing the dynamics of a real
world process assumes immense importance for anticipating and preparing for
future outcomes. For applications such as video surveillance, robotics
applications, autonomous driving, etc. this objective entails synthesizing
plausible visual futures, given a few frames of a video to set the visual
context. Towards this end, we propose ProgGen, which undertakes the task of
video frame prediction by representing the dynamics of the video using a set of
neuro-symbolic, human-interpretable set of states (one per frame) by leveraging
the inductive biases of Large (Vision) Language Models (LLM/VLM). In
particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate
the states of the video, given the visual context (i.e. the frames); (ii) to
predict the states corresponding to future time steps by estimating the
transition dynamics; (iii) to render the predicted states as visual RGB-frames.
Empirical evaluations reveal that our proposed method outperforms competing
techniques at the task of video frame prediction in two challenging
environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits
counter-factual reasoning and interpretable video generation attesting to its
effectiveness and generalizability for video generation tasks.

</details>


### [254] [MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks](https://arxiv.org/pdf/2505.14951)
*Jose Sosa, Danila Rukhovich, Anis Kacem, Djamila Aouada*

Main category: cs.CV

TL;DR: A flexible multi-modal, multi-task pre-training strategy (MultiMAE) for Earth Observation data improves transfer learning by reconstructing diverse input modalities, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Prior work often overlooks multi-modal EO data, and existing methods struggle with transferring learning to downstream tasks with differing data structures.

Method: Adopts a Multi-modal Multi-task Masked Autoencoder (MultiMAE) pre-trained on spectral, elevation, and segmentation data.

Result: The model shows robust transfer learning, outperforming state-of-the-art methods on EO classification and segmentation tasks.

Conclusion: The approach is flexible, handling diverse input configurations without needing modality-specific pre-trained models.

Abstract: Multi-modal data in Earth Observation (EO) presents a huge opportunity for
improving transfer learning capabilities when pre-training deep learning
models. Unlike prior work that often overlooks multi-modal EO data, recent
methods have started to include it, resulting in more effective pre-training
strategies. However, existing approaches commonly face challenges in
effectively transferring learning to downstream tasks where the structure of
available data differs from that used during pre-training. This paper addresses
this limitation by exploring a more flexible multi-modal, multi-task
pre-training strategy for EO data. Specifically, we adopt a Multi-modal
Multi-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing
diverse input modalities, including spectral, elevation, and segmentation data.
The pre-trained model demonstrates robust transfer learning capabilities,
outperforming state-of-the-art methods on various EO datasets for
classification and segmentation tasks. Our approach exhibits significant
flexibility, handling diverse input configurations without requiring
modality-specific pre-trained models. Code will be available at:
https://github.com/josesosajs/multimae-meets-eo.

</details>


### [255] [Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation](https://arxiv.org/pdf/2505.15077)
*Alessandro dos Santos Ferreira, Ana Paula Marques Ramos, José Marcato Junior, Wesley Nunes Gonçalves*

Main category: cs.CV

TL;DR: A novel pipeline using GANs and Diffusion models enhances low-resolution aerial images for better tree segmentation in urban forests, reducing reliance on large labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Urban forests are vital for environmental quality and biodiversity, but accurately detecting trees is challenging due to complex landscapes and varying image resolutions. Deep learning requires large labeled datasets, which are costly and scarce.

Method: The proposed pipeline integrates domain adaptation with GANs (pix2pix, Real-ESRGAN) and Diffusion models (Latent Diffusion, Stable Diffusion) to enhance low-resolution images, generating synthetic samples for training.

Result: The method improved IoU by over 50% for low-resolution images, outperforming traditional pipelines.

Conclusion: The approach provides a scalable, replicable solution for tree segmentation in remote sensing with limited annotation resources, enhancing model robustness across acquisition conditions.

Abstract: Urban forests play a key role in enhancing environmental quality and
supporting biodiversity in cities. Mapping and monitoring these green spaces
are crucial for urban planning and conservation, yet accurately detecting trees
is challenging due to complex landscapes and the variability in image
resolution caused by different satellite sensors or UAV flight altitudes. While
deep learning architectures have shown promise in addressing these challenges,
their effectiveness remains strongly dependent on the availability of large and
manually labeled datasets, which are often expensive and difficult to obtain in
sufficient quantity. In this work, we propose a novel pipeline that integrates
domain adaptation with GANs and Diffusion models to enhance the quality of
low-resolution aerial images. Our proposed pipeline enhances low-resolution
imagery while preserving semantic content, enabling effective tree segmentation
without requiring large volumes of manually annotated data. Leveraging models
such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we
generate realistic and structurally consistent synthetic samples that expand
the training dataset and unify scale across domains. This approach not only
improves the robustness of segmentation models across different acquisition
conditions but also provides a scalable and replicable solution for remote
sensing scenarios with scarce annotation resources. Experimental results
demonstrated an improvement of over 50% in IoU for low-resolution images,
highlighting the effectiveness of our method compared to traditional pipelines.

</details>


### [256] [iPad: Iterative Proposal-centric End-to-End Autonomous Driving](https://arxiv.org/pdf/2505.15111)
*Ke Guo, Haochen Liu, Xiaojun Wu, Jia Pan, Chen Lv*

Main category: cs.CV

TL;DR: The paper introduces iPad, an iterative Proposal-centric autonomous driving framework, addressing inefficiencies in E2E systems by refining proposals and using lightweight auxiliary tasks for improved planning.


<details>
  <summary>Details</summary>
Motivation: Traditional E2E autonomous driving systems suffer from inefficiency and limited planning awareness due to direct plan generation from dense BEV features.

Method: iPad uses ProFormer, a BEV encoder, to iteratively refine proposals via proposal-anchored attention and integrates lightweight auxiliary tasks (mapping and prediction) for better planning.

Result: iPad achieves state-of-the-art performance on NAVSIM and CARLA Bench2Drive benchmarks with higher efficiency than prior methods.

Conclusion: iPad's proposal-centric approach and auxiliary tasks enhance planning quality and efficiency, making it a promising solution for E2E autonomous driving.

Abstract: End-to-end (E2E) autonomous driving systems offer a promising alternative to
traditional modular pipelines by reducing information loss and error
accumulation, with significant potential to enhance both mobility and safety.
However, most existing E2E approaches directly generate plans based on dense
bird's-eye view (BEV) grid features, leading to inefficiency and limited
planning awareness. To address these limitations, we propose iterative
Proposal-centric autonomous driving (iPad), a novel framework that places
proposals - a set of candidate future plans - at the center of feature
extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder
that iteratively refines proposals and their associated features through
proposal-anchored attention, effectively fusing multi-view image data.
Additionally, we introduce two lightweight, proposal-centric auxiliary tasks -
mapping and prediction - that improve planning quality with minimal
computational overhead. Extensive experiments on the NAVSIM and CARLA
Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art
performance while being significantly more efficient than prior leading
methods.

</details>


### [257] [Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding](https://arxiv.org/pdf/2505.15123)
*Ta Duc Huy, Duy Anh Huynh, Yutong Xie, Yuankai Qi, Qi Chen, Phi Le Nguyen, Sen Kim Tran, Son Lam Phung, Anton van den Hengel, Zhibin Liao, Minh-Son To, Johan W. Verjans, Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: The paper introduces Disease-Aware Prompting (DAP) to improve visual grounding in medical imaging by focusing on disease-relevant regions and suppressing background noise, achieving a 20.74% accuracy boost.


<details>
  <summary>Details</summary>
Motivation: Current visual grounding models in medical imaging struggle with inefficient attention mechanisms and lack fine-grained token representations, limiting their ability to associate text with disease regions.

Method: The proposed DAP process leverages explainability maps from visual-language models to identify and amplify disease-relevant image features while minimizing background interference.

Result: DAP improves visual grounding accuracy by 20.74% over state-of-the-art methods on three major chest X-ray datasets, without requiring extra annotations.

Conclusion: DAP effectively enhances visual grounding in medical imaging by addressing attention misalignment and token representation issues, promoting better interpretability and trust in deep learning models for clinical use.

Abstract: Visual grounding (VG) is the capability to identify the specific regions in
an image associated with a particular text description. In medical imaging, VG
enhances interpretability by highlighting relevant pathological features
corresponding to textual descriptions, improving model transparency and
trustworthiness for wider adoption of deep learning models in clinical
practice. Current models struggle to associate textual descriptions with
disease regions due to inefficient attention mechanisms and a lack of
fine-grained token representations. In this paper, we empirically demonstrate
two key observations. First, current VLMs assign high norms to background
tokens, diverting the model's attention from regions of disease. Second, the
global tokens used for cross-modal learning are not representative of local
disease tokens. This hampers identifying correlations between the text and
disease tokens. To address this, we introduce simple, yet effective
Disease-Aware Prompting (DAP) process, which uses the explainability map of a
VLM to identify the appropriate image features. This simple strategy amplifies
disease-relevant regions while suppressing background interference. Without any
additional pixel-level annotations, DAP improves visual grounding accuracy by
20.74% compared to state-of-the-art methods across three major chest X-ray
datasets.

</details>


### [258] [DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer](https://arxiv.org/pdf/2505.15133)
*Haiduo Huang, Jiangcheng Song, Yadong Zhang, Pengju Ren*

Main category: cs.CV

TL;DR: DeepKD proposes a dual-level decoupling and adaptive denoising framework for knowledge distillation, addressing conflicts and noise in knowledge flows.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook conflicts between target-class and non-target-class knowledge flows and suffer from noisy signals in low-confidence dark knowledge.

Method: DeepKD uses independent momentum updaters for task-oriented, target-class, and non-target-class gradients, and a dynamic top-k mask mechanism to filter low-confidence logits.

Result: Experiments on CIFAR-100, ImageNet, and MS-COCO show DeepKD's effectiveness.

Conclusion: DeepKD successfully mitigates interference and noise in knowledge distillation, improving performance.

Abstract: Recent advances in knowledge distillation have emphasized the importance of
decoupling different knowledge components. While existing methods utilize
momentum mechanisms to separate task-oriented and distillation gradients, they
overlook the inherent conflict between target-class and non-target-class
knowledge flows. Furthermore, low-confidence dark knowledge in non-target
classes introduces noisy signals that hinder effective knowledge transfer. To
address these limitations, we propose DeepKD, a novel training framework that
integrates dual-level decoupling with adaptive denoising. First, through
theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics
in task-oriented and non-task-oriented knowledge distillation, we design
independent momentum updaters for each component to prevent mutual
interference. We observe that the optimal momentum coefficients for
task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class
gradient (NCG) should be positively related to their GSNR. Second, we introduce
a dynamic top-k mask (DTM) mechanism that gradually increases K from a small
initial value to incorporate more non-target classes as training progresses,
following curriculum learning principles. The DTM jointly filters
low-confidence logits from both teacher and student models, effectively
purifying dark knowledge during early training. Extensive experiments on
CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code
is available at https://github.com/haiduo/DeepKD.

</details>


### [259] [Multispectral Detection Transformer with Infrared-Centric Sensor Fusion](https://arxiv.org/pdf/2505.15137)
*Seongmin Hwang, Daeyoung Han, Moongu Jeon*

Main category: cs.CV

TL;DR: IC-Fusion is a lightweight, modality-aware multispectral object detector that fuses RGB and IR features effectively, leveraging high-frequency IR for localization and RGB for semantic context.


<details>
  <summary>Details</summary>
Motivation: To achieve robust object detection under diverse conditions by combining complementary RGB and IR information.

Method: Uses a compact RGB backbone and novel fusion modules (MSFD, CCSG, CLKG) for cross-modal interaction.

Result: Demonstrates effectiveness on FLIR and LLVIP benchmarks.

Conclusion: IC-Fusion's IR-centric fusion strategy is efficient and effective for multispectral object detection.

Abstract: Multispectral object detection aims to leverage complementary information
from visible (RGB) and infrared (IR) modalities to enable robust performance
under diverse environmental conditions. In this letter, we propose IC-Fusion, a
multispectral object detector that effectively fuses visible and infrared
features through a lightweight and modalityaware design. Motivated by wavelet
analysis and empirical observations, we find that IR images contain
structurally rich high-frequency information critical for object localization,
while RGB images provide complementary semantic context. To exploit this, we
adopt a compact RGB backbone and design a novel fusion module comprising a
Multi-Scale Feature Distillation (MSFD) block to enhance RGB features and a
three-stage fusion block with Cross-Modal Channel Shuffle Gate (CCSG) and
Cross-Modal Large Kernel Gate (CLKG) to facilitate effective cross-modal
interaction. Experiments on the FLIR and LLVIP benchmarks demonstrate the
effectiveness and efficiency of our IR-centric fusion strategy. Our code is
available at https://github.com/smin-hwang/IC-Fusion.

</details>


### [260] [Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis](https://arxiv.org/pdf/2505.15139)
*Badhan Mazumder, Lei Wu, Vince D. Calhoun, Dong Hye Ye*

Main category: cs.CV

TL;DR: ConneX, a multimodal fusion method, improves diagnostic performance for neuropsychiatric disorders by integrating cross-attention and MLP-Mixer for refined feature fusion of structural and functional brain data.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods fail to fully utilize complementary characteristics of structural and functional brain data for diagnosing disorders like Schizophrenia.

Method: Uses modality-specific GNNs for feature extraction, cross-modal attention for fusion, and MLP-Mixer layers for refining features, with a multi-head joint loss for classification.

Result: Shows improved performance on clinical datasets, demonstrating robustness.

Conclusion: ConneX effectively leverages multimodal brain data for enhanced diagnostic accuracy in neuropsychiatric disorders.

Abstract: Gaining insights into the structural and functional mechanisms of the brain
has been a longstanding focus in neuroscience research, particularly in the
context of understanding and treating neuropsychiatric disorders such as
Schizophrenia (SZ). Nevertheless, most of the traditional multimodal deep
learning approaches fail to fully leverage the complementary characteristics of
structural and functional connectomics data to enhance diagnostic performance.
To address this issue, we proposed ConneX, a multimodal fusion method that
integrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer for
refined feature fusion. Modality-specific backbone graph neural networks (GNNs)
were firstly employed to obtain feature representation for each modality. A
unified cross-modal attention network was then introduced to fuse these
embeddings by capturing intra- and inter-modal interactions, while MLP-Mixer
layers refined global and local features, leveraging higher-order dependencies
for end-to-end classification with a multi-head joint loss. Extensive
evaluations demonstrated improved performance on two distinct clinical
datasets, highlighting the robustness of our proposed framework.

</details>


### [261] [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://arxiv.org/pdf/2505.15145)
*Xinran Wang, Songyu Xu, Xiangxuan Shan, Yuxuan Zhang, Muxi Diao, Xueyan Duan, Yanhua Huang, Kongming Liang, Zhanyu Ma*

Main category: cs.CV

TL;DR: CineTechBench is a new benchmark for evaluating MLLMs and video generation models on cinematography understanding and generation, using expert-annotated data.


<details>
  <summary>Details</summary>
Motivation: Current models lack the ability to grasp and reproduce cinematographic techniques due to scarce expert-annotated data.

Method: The benchmark includes annotated movie images and clips, with tasks for understanding (question-answer pairs) and generation (reconstructing camera movements).

Result: Evaluations on 15+ MLLMs and 5+ video models reveal limitations and future directions.

Conclusion: CineTechBench provides insights for improving cinematography understanding and generation in automated film production.

Abstract: Cinematography is a cornerstone of film production and appreciation, shaping
mood, emotion, and narrative through visual elements such as camera movement,
shot composition, and lighting. Despite recent progress in multimodal large
language models (MLLMs) and video generation models, the capacity of current
models to grasp and reproduce cinematographic techniques remains largely
uncharted, hindered by the scarcity of expert-annotated data. To bridge this
gap, we present CineTechBench, a pioneering benchmark founded on precise,
manual annotation by seasoned cinematography experts across key cinematography
dimensions. Our benchmark covers seven essential aspects-shot scale, shot
angle, composition, camera movement, lighting, color, and focal length-and
includes over 600 annotated movie images and 120 movie clips with clear
cinematographic techniques. For the understanding task, we design question
answer pairs and annotated descriptions to assess MLLMs' ability to interpret
and explain cinematographic techniques. For the generation task, we assess
advanced video generation models on their capacity to reconstruct
cinema-quality camera movements given conditions such as textual prompts or
keyframes. We conduct a large-scale evaluation on 15+ MLLMs and 5+ video
generation models. Our results offer insights into the limitations of current
models and future directions for cinematography understanding and generation in
automatically film production and appreciation. The code and benchmark can be
accessed at https://github.com/PRIS-CV/CineTechBench.

</details>


### [262] [From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation](https://arxiv.org/pdf/2505.15147)
*Quanwei Liu, Tao Huang, Yanni Dong, Jiaqi Yang, Wei Xiang*

Main category: cs.CV

TL;DR: The paper reviews deep learning-based semantic segmentation of remote sensing images, categorizing methods into four stages and evaluating 40 techniques on a unified dataset.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and accurate semantic segmentation of diverse remote sensing images due to limitations of traditional methods.

Method: Categorizes DL-based approaches into four stages (pixel-based, patch-based, tile-based, image-based) and evaluates 40 techniques on a unified dataset.

Result: Quantitative performance comparison of advanced techniques, revealing progression from pixel-level to multimodal segmentation.

Conclusion: Provides insights into advancements, comparative performance, and open challenges in DL-based remote sensing image segmentation.

Abstract: Remote sensing images (RSIs) capture both natural and human-induced changes
on the Earth's surface, serving as essential data for environmental monitoring,
urban planning, and resource management. Semantic segmentation (SS) of RSIs
enables the fine-grained interpretation of surface features, making it a
critical task in remote sensing analysis. With the increasing diversity and
volume of RSIs collected by sensors on various platforms, traditional
processing methods struggle to maintain efficiency and accuracy. In response,
deep learning (DL) has emerged as a transformative approach, enabling
substantial advances in remote sensing image semantic segmentation (RSISS) by
automating feature extraction and improving segmentation accuracy across
diverse modalities. This paper revisits the evolution of DL-based RSISS by
categorizing existing approaches into four stages: the early pixel-based
methods, the prevailing patch-based and tile-based techniques, and the emerging
image-based strategies enabled by foundation models. We analyze these
developments from the perspective of feature extraction and learning
strategies, revealing the field's progression from pixel-level to tile-level
and from unimodal to multimodal segmentation. Furthermore, we conduct a
comprehensive evaluation of nearly 40 advanced techniques on a unified dataset
to quantitatively characterize their performance and applicability. This review
offers a holistic view of DL-based SS for RS, highlighting key advancements,
comparative insights, and open challenges to guide future research.

</details>


### [263] [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/pdf/2505.15158)
*Yunsheng Ma, Burhaneddin Yaman, Xin Ye, Mahmut Yurt, Jingru Luo, Abhirup Mallik, Ziran Wang, Liu Ren*

Main category: cs.CV

TL;DR: ALN-P3 is a co-distillation framework aligning vision-based driving and language reasoning modules to improve both driving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance driving performance and vision-language reasoning, limiting generalization and interpretability.

Method: ALN-P3 uses three alignment mechanisms (P1A, P2A, P3A) to align visual tokens with linguistic outputs across perception, prediction, and planning.

Result: ALN-P3 achieves state-of-the-art results on benchmarks like nuScenes, improving both driving decisions and language reasoning.

Conclusion: The framework successfully integrates LLMs into autonomous driving without inference costs, enhancing performance and interpretability.

Abstract: Recent advances have explored integrating large language models (LLMs) into
end-to-end autonomous driving systems to enhance generalization and
interpretability. However, most existing approaches are limited to either
driving performance or vision-language reasoning, making it difficult to
achieve both simultaneously. In this paper, we propose ALN-P3, a unified
co-distillation framework that introduces cross-modal alignment between "fast"
vision-based autonomous driving systems and "slow" language-driven reasoning
modules. ALN-P3 incorporates three novel alignment mechanisms: Perception
Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),
which explicitly align visual tokens with corresponding linguistic outputs
across the full perception, prediction, and planning stack. All alignment
modules are applied only during training and incur no additional costs during
inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,
TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both
driving decisions and language reasoning, achieving state-of-the-art results.

</details>


### [264] [FRN: Fractal-Based Recursive Spectral Reconstruction Network](https://arxiv.org/pdf/2505.15439)
*Ge Meng, Zhongnan Cai, Ruizhe Chen, Jingyan Tu, Yingying Wang, Yue Huang, Xinghao Ding*

Main category: cs.CV

TL;DR: FRN proposes a fractal-based recursive method for hyperspectral image reconstruction from RGB images, outperforming existing one-shot approaches.


<details>
  <summary>Details</summary>
Motivation: Reducing the cost of hyperspectral image acquisition by reconstructing them from RGB images.

Method: Uses a recursive atomic module for progressive spectral reconstruction, inspired by fractals, and a band-aware state space model for pixel-differentiated scanning.

Result: Superior reconstruction performance in quantitative and qualitative evaluations compared to state-of-the-art methods.

Conclusion: FRN's recursive, progressive approach effectively reconstructs hyperspectral images, leveraging spectral data's low-rank property.

Abstract: Generating hyperspectral images (HSIs) from RGB images through spectral
reconstruction can significantly reduce the cost of HSI acquisition. In this
paper, we propose a Fractal-Based Recursive Spectral Reconstruction Network
(FRN), which differs from existing paradigms that attempt to directly integrate
the full-spectrum information from the R, G, and B channels in a one-shot
manner. Instead, it treats spectral reconstruction as a progressive process,
predicting from broad to narrow bands or employing a coarse-to-fine approach
for predicting the next wavelength. Inspired by fractals in mathematics, FRN
establishes a novel spectral reconstruction paradigm by recursively invoking an
atomic reconstruction module. In each invocation, only the spectral information
from neighboring bands is used to provide clues for the generation of the image
at the next wavelength, which follows the low-rank property of spectral data.
Moreover, we design a band-aware state space model that employs a
pixel-differentiated scanning strategy at different stages of the generation
process, further suppressing interference from low-correlation regions caused
by reflectance differences. Through extensive experimentation across different
datasets, FRN achieves superior reconstruction performance compared to
state-of-the-art methods in both quantitative and qualitative evaluations.

</details>


### [265] [Lossless Token Merging Even Without Fine-Tuning in Vision Transformers](https://arxiv.org/pdf/2505.15160)
*Jaeyeon Lee, Dong-Wan Choi*

Main category: cs.CV

TL;DR: ATM is a novel, training-free token merging method for ViTs that reduces computational overhead without performance loss.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of ViTs and the information loss in existing token compression techniques.

Method: ATM uses adaptive layer-specific similarity thresholds and a novel token matching technique to merge tokens losslessly.

Result: ATM reduces FLOPs by over 30% for DeiT-T/S models without accuracy drop, outperforming training-free and most training-intensive methods.

Conclusion: ATM offers an efficient, lossless solution for token compression in ViTs, eliminating the need for fine-tuning.

Abstract: Although Vision Transformers (ViTs) have become the standard architecture in
computer vision, their massive sizes lead to significant computational
overhead. Token compression techniques have attracted considerable attention to
address this issue, but they often suffer from severe information loss,
requiring extensive additional training to achieve practical performance. In
this paper, we propose Adaptive Token Merging (ATM), a novel method that
ensures lossless token merging, eliminating the need for fine-tuning while
maintaining competitive performance. ATM adaptively reduces tokens across
layers and batches by carefully adjusting layer-specific similarity thresholds,
thereby preventing the undesirable merging of less similar tokens with respect
to each layer. Furthermore, ATM introduces a novel token matching technique
that considers not only similarity but also merging sizes, particularly for the
final layers, to minimize the information loss incurred from each merging
operation. We empirically validate our method across a wide range of pretrained
models, demonstrating that ATM not only outperforms all existing training-free
methods but also surpasses most training-intensive approaches, even without
additional training. Remarkably, training-free ATM achieves over a 30%
reduction in FLOPs for the DeiT-T and DeiT-S models without any drop in their
original accuracy.

</details>


### [266] [Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation](https://arxiv.org/pdf/2505.15172)
*Xinran Wang, Muxi Diao, Yuanzhi Liu, Chunyu Wang, Kongming Liang, Zhanyu Ma, Jun Guo*

Main category: cs.CV

TL;DR: Proposes a new metric (ICR and AOD) for caption detailness in T2I models, outperforming length-based methods with better data efficiency and generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods use simplistic metrics like caption length, which may not accurately represent caption detailness, limiting T2I model performance.

Method: Introduces image coverage rate (ICR) and average object detailness (AOD) to evaluate caption detailness. Tests on COCO dataset with ShareGPT4V captions.

Result: T2I models trained with high-ICR and -AOD captions perform better. Using 20% of data with this metric surpasses full-dataset training and length-based selection.

Conclusion: Detail-aware metrics (ICR, AOD) are more effective than length-based heuristics for caption selection in T2I tasks.

Abstract: Training text-to-image (T2I) models with detailed captions can significantly
improve their generation quality. Existing methods often rely on simplistic
metrics like caption length to represent the detailness of the caption in the
T2I training set. In this paper, we propose a new metric to estimate caption
detailness based on two aspects: image coverage rate (ICR), which evaluates
whether the caption covers all regions/objects in the image, and average object
detailness (AOD), which quantifies the detailness of each object's description.
Through experiments on the COCO dataset using ShareGPT4V captions, we
demonstrate that T2I models trained on high-ICR and -AOD captions achieve
superior performance on DPG and other benchmarks. Notably, our metric enables
more effective data selection-training on only 20% of full data surpasses both
full-dataset training and length-based selection method, improving alignment
and reconstruction ability. These findings highlight the critical role of
detail-aware metrics over length-based heuristics in caption selection for T2I
tasks.

</details>


### [267] [AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection](https://arxiv.org/pdf/2505.15173)
*Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang*

Main category: cs.CV

TL;DR: AvatarShield is an interpretable MLLM-based framework for detecting human-centric fake videos, addressing limitations of current methods with GRPO and a dual-encoder architecture.


<details>
  <summary>Details</summary>
Motivation: The rise of AIGC technologies, especially in video generation, poses risks to information integrity and public trust, with existing detection methods lacking robustness for human-centric videos.

Method: Proposes AvatarShield, using GRPO for optimization, a dual-encoder architecture for semantic reasoning and artifact amplification, and introduces the FakeHumanVid benchmark.

Result: AvatarShield outperforms existing methods in in-domain and cross-domain detection, setting a new standard for video forensics.

Conclusion: AvatarShield provides a scalable, interpretable solution for detecting human-centric fake videos, advancing the field of video forensics.

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
technologies, particularly in video generation, has led to unprecedented
creative capabilities but also increased threats to information integrity,
identity security, and public trust. Existing detection methods, while
effective in general scenarios, lack robust solutions for human-centric videos,
which pose greater risks due to their realism and potential for legal and
ethical misuse. Moreover, current detection approaches often suffer from poor
generalization, limited scalability, and reliance on labor-intensive supervised
fine-tuning. To address these challenges, we propose AvatarShield, the first
interpretable MLLM-based framework for detecting human-centric fake videos,
enhanced via Group Relative Policy Optimization (GRPO). Through our carefully
designed accuracy detection reward and temporal compensation reward, it
effectively avoids the use of high-cost text annotation data, enabling precise
temporal modeling and forgery detection. Meanwhile, we design a dual-encoder
architecture, combining high-level semantic reasoning and low-level artifact
amplification to guide MLLMs in effective forgery detection. We further collect
FakeHumanVid, a large-scale human-centric video benchmark that includes
synthesis methods guided by pose, audio, and text inputs, enabling rigorous
evaluation of detection methods in real-world scenes. Extensive experiments
show that AvatarShield significantly outperforms existing approaches in both
in-domain and cross-domain detection, setting a new standard for human-centric
video forensics.

</details>


### [268] [Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets](https://arxiv.org/pdf/2505.15176)
*Qian Zhou, Xianda Guo, Jilong Wang, Chuanfu Shen, Zhongyuan Wang, Hua Zou, Qin Zou, Chao Liang, Chen Long, Gang Wu*

Main category: cs.CV

TL;DR: A unified framework for cross-domain gait recognition addresses domain shifts and mixed-dataset training challenges with disentangled triplet loss and dataset distillation.


<details>
  <summary>Details</summary>
Motivation: Generalized gait recognition struggles with domain shifts and mixed-dataset training issues like optimization conflicts and noisy samples.

Method: Proposes a disentangled triplet loss to isolate supervision signals and a dataset distillation strategy to filter uninformative samples.

Result: Improves cross-dataset recognition on CASIA-B, OU-MVLP, Gait3D, and GREW without losing source-domain accuracy.

Conclusion: The framework effectively enhances cross-domain gait recognition by mitigating optimization conflicts and improving data efficiency.

Abstract: Generalized gait recognition, which aims to achieve robust performance across
diverse domains, remains a challenging problem due to severe domain shifts in
viewpoints, appearances, and environments. While mixed-dataset training is
widely used to enhance generalization, it introduces new obstacles including
inter-dataset optimization conflicts and redundant or noisy samples, both of
which hinder effective representation learning. To address these challenges, we
propose a unified framework that systematically improves cross-domain gait
recognition. First, we design a disentangled triplet loss that isolates
supervision signals across datasets, mitigating gradient conflicts during
optimization. Second, we introduce a targeted dataset distillation strategy
that filters out the least informative 20\% of training samples based on
feature redundancy and prediction uncertainty, enhancing data efficiency.
Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that
our method significantly improves cross-dataset recognition for both GaitBase
and DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will
be released at https://github.com/li1er3/Generalized_Gait.

</details>


### [269] [AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection](https://arxiv.org/pdf/2505.15184)
*Yangting Shi, Renjie He, Le Hui, Xiang Li, Jian Yang, Ming-Ming Cheng, Yimian Dai*

Main category: cs.CV

TL;DR: AuxDet introduces a multi-modal framework for infrared small target detection by integrating auxiliary metadata with visual features, improving robustness and accuracy across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing IRSTD methods struggle with complex backgrounds, scarce target features, and limited generalization due to ignoring auxiliary metadata like spectral bands and sensor details.

Method: AuxDet uses a multi-modal approach, combining textual metadata with visual features via MLPs and refining them with 1D convolutional blocks for adaptive learning.

Result: AuxDet outperforms state-of-the-art methods on the WideIRSTD-Full benchmark, demonstrating enhanced robustness and accuracy.

Conclusion: Incorporating auxiliary metadata is crucial for improving omni-domain IRSTD, and AuxDet provides a scalable solution.

Abstract: Omni-domain infrared small target detection (IRSTD) poses formidable
challenges, as a single model must seamlessly adapt to diverse imaging systems,
varying resolutions, and multiple spectral bands simultaneously. Current
approaches predominantly rely on visual-only modeling paradigms that not only
struggle with complex background interference and inherently scarce target
features, but also exhibit limited generalization capabilities across complex
omni-scene environments where significant domain shifts and appearance
variations occur. In this work, we reveal a critical oversight in existing
paradigms: the neglect of readily available auxiliary metadata describing
imaging parameters and acquisition conditions, such as spectral bands, sensor
platforms, resolution, and observation perspectives. To address this
limitation, we propose the Auxiliary Metadata Driven Infrared Small Target
Detector (AuxDet), a novel multi-modal framework that fundamentally reimagines
the IRSTD paradigm by incorporating textual metadata for scene-aware
optimization. Through a high-dimensional fusion module based on multi-layer
perceptrons (MLPs), AuxDet dynamically integrates metadata semantics with
visual features, guiding adaptive representation learning for each individual
sample. Additionally, we design a lightweight prior-initialized enhancement
module using 1D convolutional blocks to further refine fused features and
recover fine-grained target cues. Extensive experiments on the challenging
WideIRSTD-Full benchmark demonstrate that AuxDet consistently outperforms
state-of-the-art methods, validating the critical role of auxiliary information
in improving robustness and accuracy in omni-domain IRSTD tasks. Code is
available at https://github.com/GrokCV/AuxDet.

</details>


### [270] [MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models](https://arxiv.org/pdf/2505.15185)
*Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan*

Main category: cs.CV

TL;DR: MonoSplat improves 3D Gaussian Splatting by using pre-trained monocular depth models for better generalization and reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with unfamiliar visual content due to limited generalizability.

Method: Uses a Mono-Multi Feature Adapter and Integrated Gaussian Prediction module to fuse monocular and multi-view features.

Result: Achieves superior reconstruction quality and generalization with minimal computational overhead.

Conclusion: MonoSplat outperforms existing methods in real-time high-fidelity rendering.

Abstract: Recent advances in generalizable 3D Gaussian Splatting have demonstrated
promising results in real-time high-fidelity rendering without per-scene
optimization, yet existing approaches still struggle to handle unfamiliar
visual content during inference on novel scenes due to limited
generalizability. To address this challenge, we introduce MonoSplat, a novel
framework that leverages rich visual priors from pre-trained monocular depth
foundation models for robust Gaussian reconstruction. Our approach consists of
two key components: a Mono-Multi Feature Adapter that transforms monocular
features into multi-view representations, coupled with an Integrated Gaussian
Prediction module that effectively fuses both feature types for precise
Gaussian generation. Through the Adapter's lightweight attention mechanism,
features are seamlessly aligned and aggregated across views while preserving
valuable monocular priors, enabling the Prediction module to generate Gaussian
primitives with accurate geometry and appearance. Through extensive experiments
on diverse real-world datasets, we convincingly demonstrate that MonoSplat
achieves superior reconstruction quality and generalization capability compared
to existing methods while maintaining computational efficiency with minimal
trainable parameters. Codes are available at
https://github.com/CUHK-AIM-Group/MonoSplat.

</details>


### [271] [Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation](https://arxiv.org/pdf/2505.15191)
*Hana Satou, Alan Mitkiy, F Monkey*

Main category: cs.CV

TL;DR: MAADA improves transfer learning by decomposing adversarial perturbations into on-manifold and off-manifold components, enhancing generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of domain shift in transfer learning by leveraging manifold-aware adversarial data augmentation.

Method: Decomposes adversarial perturbations into on-manifold and off-manifold components, uses a geometry-aware alignment loss for manifold discrepancy minimization.

Result: Outperforms existing methods on DomainNet, VisDA, and Office-Home in unsupervised and few-shot settings.

Conclusion: MAADA enhances structural robustness and cross-domain generalization through manifold-aware adversarial augmentation.

Abstract: Transfer learning under domain shift remains a fundamental challenge due to
the divergence between source and target data manifolds. In this paper, we
propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework
that decomposes adversarial perturbations into on-manifold and off-manifold
components to simultaneously capture semantic variation and model brittleness.
We theoretically demonstrate that enforcing on-manifold consistency reduces
hypothesis complexity and improves generalization, while off-manifold
regularization smooths decision boundaries in low-density regions. Moreover, we
introduce a geometry-aware alignment loss that minimizes geodesic discrepancy
between source and target manifolds. Experiments on DomainNet, VisDA, and
Office-Home show that MAADA consistently outperforms existing adversarial and
adaptation methods in both unsupervised and few-shot settings, demonstrating
superior structural robustness and cross-domain generalization.

</details>


### [272] [DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But Only If You Can Trust Them](https://arxiv.org/pdf/2501.08005)
*Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen*

Main category: cs.CV

TL;DR: DisCoPatch, an unsupervised Adversarial VAE framework, detects covariate shifts in OOD data using batch statistics and achieves state-of-the-art performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Covariate shifts, subtle data distribution variations, degrade ML performance. Detecting them can improve OOD detection by clarifying in-distribution boundaries.

Method: DisCoPatch uses adversarial discriminators with BN to exploit batch statistics. It trains the discriminator with VAE's suboptimal outputs as negative samples.

Result: Achieves 95.5% AUROC on ImageNet-1K(-C) and 95.0% on Near-OOD benchmarks, with a compact 25MB model and low latency.

Conclusion: DisCoPatch is an efficient, practical solution for real-world OOD detection, outperforming prior methods.

Abstract: Out-of-distribution (OOD) detection holds significant importance across many
applications. While semantic and domain-shift OOD problems are well-studied,
this work focuses on covariate shifts - subtle variations in the data
distribution that can degrade machine learning performance. We hypothesize that
detecting these subtle shifts can improve our understanding of in-distribution
boundaries, ultimately improving OOD detection. In adversarial discriminators
trained with Batch Normalization (BN), real and adversarial samples form
distinct domains with unique batch statistics - a property we exploit for OOD
detection. We introduce DisCoPatch, an unsupervised Adversarial Variational
Autoencoder (VAE) framework that harnesses this mechanism. During inference,
batches consist of patches from the same image, ensuring a consistent data
distribution that allows the model to rely on batch statistics. DisCoPatch uses
the VAE's suboptimal outputs (generated and reconstructed) as negative samples
to train the discriminator, thereby improving its ability to delineate the
boundary between in-distribution samples and covariate shifts. By tightening
this boundary, DisCoPatch achieves state-of-the-art results in public OOD
detection benchmarks. The proposed model not only excels in detecting covariate
shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior
methods on public Near-OOD (95.0%) benchmarks. With a compact model size of
25MB, it achieves high OOD detection performance at notably lower latency than
existing methods, making it an efficient and practical solution for real-world
OOD detection applications. The code will be made publicly available

</details>


### [273] [Leveraging Foundation Models for Multimodal Graph-Based Action Recognition](https://arxiv.org/pdf/2505.15192)
*Fatemeh Ziaeetabar, Florentin Wörgötter*

Main category: cs.CV

TL;DR: A graph-based framework integrates VideoMAE and BERT for fine-grained bimanual action recognition, using dynamic multimodal graphs and task-specific attention to outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recognizing fine-grained bimanual manipulation actions by leveraging foundation models and dynamic graph-based reasoning.

Method: Combines VideoMAE for visual encoding and BERT for textual embedding, constructs adaptive multimodal graphs with dynamic nodes and edges, and employs a task-specific attention mechanism in a Graph Attention Network.

Result: Outperforms state-of-the-art baselines on diverse benchmark datasets.

Conclusion: The integration of foundation models with dynamic graph-based reasoning enables robust and generalizable action recognition.

Abstract: Foundation models have ushered in a new era for multimodal video
understanding by enabling the extraction of rich spatiotemporal and semantic
representations. In this work, we introduce a novel graph-based framework that
integrates a vision-language foundation, leveraging VideoMAE for dynamic visual
encoding and BERT for contextual textual embedding, to address the challenge of
recognizing fine-grained bimanual manipulation actions. Departing from
conventional static graph architectures, our approach constructs an adaptive
multimodal graph where nodes represent frames, objects, and textual
annotations, and edges encode spatial, temporal, and semantic relationships.
These graph structures evolve dynamically based on learned interactions,
allowing for flexible and context-aware reasoning. A task-specific attention
mechanism within a Graph Attention Network further enhances this reasoning by
modulating edge importance based on action semantics. Through extensive
evaluations on diverse benchmark datasets, we demonstrate that our method
consistently outperforms state-of-the-art baselines, underscoring the strength
of combining foundation models with dynamic graph-based reasoning for robust
and generalizable action recognition.

</details>


### [274] [GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation](https://arxiv.org/pdf/2505.15194)
*Hana Satou, F Monkey*

Main category: cs.CV

TL;DR: GAMA (Geometry-Aware Manifold Alignment) is a structured framework for domain adaptation that explicitly aligns manifolds using adversarial perturbations guided by geometric information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of manifold discrepancy in domain adaptation, where existing methods lack precise alignment and systematic perturbation exploration.

Method: GAMA employs tangent space exploration and manifold-constrained adversarial optimization for explicit alignment, enhancing semantic consistency and robustness.

Result: Theoretical analysis shows tightened generalization bounds. Empirical results on DomainNet, VisDA, and Office-Home demonstrate superior performance in unsupervised and few-shot settings.

Conclusion: GAMA outperforms existing methods in robustness, generalization, and manifold alignment, proving its effectiveness in domain adaptation.

Abstract: Domain adaptation remains a challenge when there is significant manifold
discrepancy between source and target domains. Although recent methods leverage
manifold-aware adversarial perturbations to perform data augmentation, they
often neglect precise manifold alignment and systematic exploration of
structured perturbations. To address this, we propose GAMA (Geometry-Aware
Manifold Alignment), a structured framework that achieves explicit manifold
alignment via adversarial perturbation guided by geometric information. GAMA
systematically employs tangent space exploration and manifold-constrained
adversarial optimization, simultaneously enhancing semantic consistency,
robustness to off-manifold deviations, and cross-domain alignment. Theoretical
analysis shows that GAMA tightens the generalization bound via structured
regularization and explicit alignment. Empirical results on DomainNet, VisDA,
and Office-Home demonstrate that GAMA consistently outperforms existing
adversarial and adaptation methods in both unsupervised and few-shot settings,
exhibiting superior robustness, generalization, and manifold alignment
capability.

</details>


### [275] [Intentional Gesture: Deliver Your Intentions with Gestures for Speech](https://arxiv.org/pdf/2505.15197)
*Pinxin Liu, Haiyang Liu, Luchuan Song, Chenliang Xu*

Main category: cs.CV

TL;DR: The paper introduces Intentional-Gesture, a framework for generating co-speech gestures by leveraging communicative intentions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current gesture generation methods lack semantic depth as they ignore communicative intentions, leading to rhythmically synchronized but shallow outputs.

Method: The framework uses intention annotations (automatically labeled using vision-language models) and an Intentional Gesture Motion Tokenizer to inject communicative functions into motion representations.

Result: The method achieves state-of-the-art performance on the BEAT-2 benchmark, producing temporally aligned and semantically meaningful gestures.

Conclusion: The framework provides a modular foundation for expressive gesture generation in digital humans and embodied AI.

Abstract: When humans speak, gestures help convey communicative intentions, such as
adding emphasis or describing concepts. However, current co-speech gesture
generation methods rely solely on superficial linguistic cues (\textit{e.g.}
speech audio or text transcripts), neglecting to understand and leverage the
communicative intention that underpins human gestures. This results in outputs
that are rhythmically synchronized with speech but are semantically shallow. To
address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework
that casts gesture generation as an intention-reasoning task grounded in
high-level communicative functions. % First, we curate the \textbf{InG} dataset
by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text
sentences summarizing intentions), which are automatically annotated using
large vision-language models. Next, we introduce the \textbf{Intentional
Gesture Motion Tokenizer} to leverage these intention annotations. It injects
high-level communicative functions (\textit{e.g.}, intentions) into tokenized
motion representations to enable intention-aware gesture synthesis that are
both temporally aligned and semantically meaningful, achieving new
state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a
modular foundation for expressive gesture generation in digital humans and
embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture

</details>


### [276] [Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection](https://arxiv.org/pdf/2505.15205)
*Hyogun Lee, Haksub Kim, Ig-Jae Kim, Yonghun Choi*

Main category: cs.CV

TL;DR: Flashback is a zero-shot, real-time video anomaly detection method using a two-stage recall and respond approach, outperforming prior methods on large datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing domain dependency and real-time constraints in video anomaly detection to enable practical adoption.

Method: Two-stage process: offline recall (building pseudo-scene memory with an LLM) and online respond (similarity search for real-time detection).

Result: Achieved 87.3 AUC and 75.1 AP on UCF-Crime and XD-Violence datasets, surpassing prior zero-shot methods.

Conclusion: Flashback offers efficient, real-time anomaly detection without reliance on real anomaly data, demonstrating strong performance.

Abstract: Video Anomaly Detection (VAD) automatically identifies anomalous events from
video, mitigating the need for human operators in large-scale surveillance
deployments. However, three fundamental obstacles hinder real-world adoption:
domain dependency and real-time constraints -- requiring near-instantaneous
processing of incoming video. To this end, we propose Flashback, a zero-shot
and real-time video anomaly detection paradigm. Inspired by the human cognitive
mechanism of instantly judging anomalies and reasoning in current scenes based
on past experience, Flashback operates in two stages: Recall and Respond. In
the offline recall stage, an off-the-shelf LLM builds a pseudo-scene memory of
both normal and anomalous captions without any reliance on real anomaly data.
In the online respond stage, incoming video segments are embedded and matched
against this memory via similarity search. By eliminating all LLM calls at
inference time, Flashback delivers real-time VAD even on a consumer-grade GPU.
On two large datasets from real-world surveillance scenarios, UCF-Crime and
XD-Violence, we achieve 87.3 AUC (+7.0 pp) and 75.1 AP (+13.1 pp),
respectively, outperforming prior zero-shot VAD methods by large margins.

</details>


### [277] [GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting](https://arxiv.org/pdf/2505.15208)
*Wenjie Liu, Zhongliang Liu, Junwei Shu, Changbo Wang, Yang Li*

Main category: cs.CV

TL;DR: GT^2-GS is a geometry-aware texture transfer framework for 3D Gaussian splitting, addressing the challenge of insufficient texture features by integrating geometric information and optimizing texture transfer results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D style transfer often ignore geometric information, leading to poor texture transfer quality. This paper aims to improve texture transfer by incorporating geometry awareness.

Method: The framework includes a geometry-aware texture augmentation module, a geometry-consistent texture loss, and an iterative geometry preservation strategy to balance texture learning and geometric integrity.

Result: Extensive experiments show the method's effectiveness and controllability, producing texture transfer results that align better with human visual perception.

Conclusion: GT^2-GS successfully integrates geometric information into texture transfer, achieving high-quality and controllable results.

Abstract: Transferring 2D textures to 3D modalities is of great significance for
improving the efficiency of multimedia content creation. Existing approaches
have rarely focused on transferring image textures onto 3D representations. 3D
style transfer methods are capable of transferring abstract artistic styles to
3D scenes. However, these methods often overlook the geometric information of
the scene, which makes it challenging to achieve high-quality 3D texture
transfer results. In this paper, we present GT^2-GS, a geometry-aware texture
transfer framework for gaussian splitting. From the perspective of matching
texture features with geometric information in rendered views, we identify the
issue of insufficient texture features and propose a geometry-aware texture
augmentation module to expand the texture feature set. Moreover, a
geometry-consistent texture loss is proposed to optimize texture features into
the scene representation. This loss function incorporates both camera pose and
3D geometric information of the scene, enabling controllable texture-oriented
appearance editing. Finally, a geometry preservation strategy is introduced. By
alternating between the texture transfer and geometry correction stages over
multiple iterations, this strategy achieves a balance between learning texture
features and preserving geometric integrity. Extensive experiments demonstrate
the effectiveness and controllability of our method. Through geometric
awareness, our approach achieves texture transfer results that better align
with human visual perception. Our homepage is available at
https://vpx-ecnu.github.io/GT2-GS-website.

</details>


### [278] [Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection](https://arxiv.org/pdf/2505.15217)
*Haotian Qin, Dongliang Chang, Yueying Gao, Bingyao Yu, Lei Chen, Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper proposes InfoFD, a multimodal conditional bottleneck network, to improve AI-generated image detection by reducing feature redundancy and leveraging text-guided features.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods suffer from feature redundancy and poor generalization due to prompt diversity.

Method: Introduces InfoFD with Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text Orthogonalization (DTO) to enhance feature discriminability.

Result: Achieves superior generalization on the GenImage dataset and latest generative models.

Conclusion: InfoFD effectively addresses feature redundancy and improves detection performance through multimodal conditioning.

Abstract: Although existing CLIP-based methods for detecting AI-generated images have
achieved promising results, they are still limited by severe feature
redundancy, which hinders their generalization ability. To address this issue,
incorporating an information bottleneck network into the task presents a
straightforward solution. However, relying solely on image-corresponding
prompts results in suboptimal performance due to the inherent diversity of
prompts. In this paper, we propose a multimodal conditional bottleneck network
to reduce feature redundancy while enhancing the discriminative power of
features extracted by CLIP, thereby improving the model's generalization
ability. We begin with a semantic analysis experiment, where we observe that
arbitrary text features exhibit lower cosine similarity with real image
features than with fake image features in the CLIP feature space, a phenomenon
we refer to as "bias". Therefore, we introduce InfoFD, a text-guided
AI-generated image detection framework. InfoFD consists of two key components:
the Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text
Orthogonalization (DTO). TGCIB improves the generalizability of learned
representations by conditioning on both text and class modalities. DTO
dynamically updates weighted text features, preserving semantic information
while leveraging the global "bias". Our model achieves exceptional
generalization performance on the GenImage dataset and latest generative
models. Our code is available at https://github.com/Ant0ny44/InfoFD.

</details>


### [279] [Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives](https://arxiv.org/pdf/2505.15222)
*Yisi Luo, Xile Zhao, Deyu Meng*

Main category: cs.CV

TL;DR: The paper reviews continuous representation methods, highlighting their advantages over discrete frameworks, design approaches, theoretical foundations, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To explore the superiority of continuous representation frameworks over traditional discrete methods in data representation and reconstruction.

Method: Systematic examination of continuous representation designs (e.g., basis functions, neural representations), theoretical foundations (e.g., error analysis), and applications (e.g., computer vision, bioinformatics).

Result: Continuous frameworks offer resolution flexibility, cross-modal adaptability, smoothness, and parameter efficiency, with diverse applications.

Conclusion: The review outlines future directions to advance continuous representation methods, theories, and applications, supported by an open-source repository.

Abstract: Recently, continuous representation methods emerge as novel paradigms that
characterize the intrinsic structures of real-world data through function
representations that map positional coordinates to their corresponding values
in the continuous space. As compared with the traditional discrete framework,
the continuous framework demonstrates inherent superiority for data
representation and reconstruction (e.g., image restoration, novel view
synthesis, and waveform inversion) by offering inherent advantages including
resolution flexibility, cross-modal adaptability, inherent smoothness, and
parameter efficiency. In this review, we systematically examine recent
advancements in continuous representation frameworks, focusing on three
aspects: (i) Continuous representation method designs such as basis function
representation, statistical modeling, tensor function decomposition, and
implicit neural representation; (ii) Theoretical foundations of continuous
representations such as approximation error analysis, convergence property, and
implicit regularization; (iii) Real-world applications of continuous
representations derived from computer vision, graphics, bioinformatics, and
remote sensing. Furthermore, we outline future directions and perspectives to
inspire exploration and deepen insights to facilitate continuous representation
methods, theories, and applications. All referenced works are summarized in our
open-source repository:
https://github.com/YisiLuo/Continuous-Representation-Zoo.

</details>


### [280] [DC-Scene: Data-Centric Learning for 3D Scene Understanding](https://arxiv.org/pdf/2505.15232)
*Ting Huang, Zeyu Zhang, Ruicheng Zhang, Yang Zhao*

Main category: cs.CV

TL;DR: DC-Scene is a data-centric framework for 3D scene understanding, improving efficiency by filtering noisy data and using a curriculum scheduler, achieving state-of-the-art performance with reduced training costs.


<details>
  <summary>Details</summary>
Motivation: Challenges in 3D scene understanding include high computational costs and scarcity of annotated data, necessitating more efficient learning paradigms.

Method: Proposes a CLIP-driven dual-indicator quality (DIQ) filter and a curriculum scheduler to progressively expand the training pool with high-quality samples.

Result: Achieves 86.1 CIDEr (top-75% subset) vs. 85.4 (full dataset), reducing training cost by two-thirds.

Conclusion: A compact set of high-quality samples can outperform exhaustive training, demonstrating the effectiveness of DC-Scene.

Abstract: 3D scene understanding plays a fundamental role in vision applications such
as robotics, autonomous driving, and augmented reality. However, advancing
learning-based 3D scene understanding remains challenging due to two key
limitations: (1) the large scale and complexity of 3D scenes lead to higher
computational costs and slower training compared to 2D counterparts; and (2)
high-quality annotated 3D datasets are significantly scarcer than those
available for 2D vision. These challenges underscore the need for more
efficient learning paradigms. In this work, we propose DC-Scene, a data-centric
framework tailored for 3D scene understanding, which emphasizes enhancing data
quality and training efficiency. Specifically, we introduce a CLIP-driven
dual-indicator quality (DIQ) filter, combining vision-language alignment scores
with caption-loss perplexity, along with a curriculum scheduler that
progressively expands the training pool from the top 25% to 75% of
scene-caption pairs. This strategy filters out noisy samples and significantly
reduces dependence on large-scale labeled 3D data. Extensive experiments on
ScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-art
performance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)
while reducing training cost by approximately two-thirds, confirming that a
compact set of high-quality samples can outperform exhaustive training. Code
will be available at https://github.com/AIGeeksGroup/DC-Scene.

</details>


### [281] [CAD: A General Multimodal Framework for Video Deepfake Detection via Cross-Modal Alignment and Distillation](https://arxiv.org/pdf/2505.15233)
*Yuxuan Du, Zhendong Wang, Yuhao Luo, Caiyong Piao, Zhiyuan Yan, Hao Li, Li Yuan*

Main category: cs.CV

TL;DR: The paper proposes a multimodal framework (CAD) for deepfake detection by combining cross-modal alignment and distillation to address limitations of existing detectors.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detectors fail to effectively integrate modality-specific artifacts and cross-modal inconsistencies, limiting their performance.

Method: CAD uses cross-modal alignment for semantic inconsistencies and cross-modal distillation to preserve modality-specific forensic traces during feature fusion.

Result: CAD outperforms previous methods on multimodal and unimodal deepfake benchmarks.

Conclusion: Harmonious integration of multimodal complementary information is essential for effective deepfake detection.

Abstract: The rapid emergence of multimodal deepfakes (visual and auditory content are
manipulated in concert) undermines the reliability of existing detectors that
rely solely on modality-specific artifacts or cross-modal inconsistencies. In
this work, we first demonstrate that modality-specific forensic traces (e.g.,
face-swap artifacts or spectral distortions) and modality-shared semantic
misalignments (e.g., lip-speech asynchrony) offer complementary evidence, and
that neglecting either aspect limits detection performance. Existing approaches
either naively fuse modality-specific features without reconciling their
conflicting characteristics or focus predominantly on semantic misalignment at
the expense of modality-specific fine-grained artifact cues. To address these
shortcomings, we propose a general multimodal framework for video deepfake
detection via Cross-Modal Alignment and Distillation (CAD). CAD comprises two
core components: 1) Cross-modal alignment that identifies inconsistencies in
high-level semantic synchronization (e.g., lip-speech mismatches); 2)
Cross-modal distillation that mitigates feature conflicts during fusion while
preserving modality-specific forensic traces (e.g., spectral distortions in
synthetic audio). Extensive experiments on both multimodal and unimodal (e.g.,
image-only/video-only)deepfake benchmarks demonstrate that CAD significantly
outperforms previous methods, validating the necessity of harmonious
integration of multimodal complementary information.

</details>


### [282] [GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer](https://arxiv.org/pdf/2505.15241)
*Kim Yun, Hana Satou, F Monkey*

Main category: cs.CV

TL;DR: GAMA++ improves geometry-aware domain adaptation by disentangling task-relevant dimensions and using adaptive contrastive perturbations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods like GAMA struggle with disentangling task-relevant dimensions and rigid perturbation schemes, limiting performance.

Method: GAMA++ introduces latent space disentanglement and adaptive contrastive perturbation, along with a cross-domain contrastive consistency loss.

Result: Achieves top results on DomainNet, Office-Home, and VisDA benchmarks, improving alignment fidelity and robustness.

Conclusion: GAMA++ advances semantic geometry alignment in transfer learning, setting a new standard.

Abstract: Despite progress in geometry-aware domain adaptation, current methods such as
GAMA still suffer from two unresolved issues: (1) insufficient disentanglement
of task-relevant and task-irrelevant manifold dimensions, and (2) rigid
perturbation schemes that ignore per-class alignment asymmetries. To address
this, we propose GAMA++, a novel framework that introduces (i) latent space
disentanglement to isolate label-consistent manifold directions from nuisance
factors, and (ii) an adaptive contrastive perturbation strategy that tailors
both on- and off-manifold exploration to class-specific manifold curvature and
alignment discrepancy. We further propose a cross-domain contrastive
consistency loss that encourages local semantic clusters to align while
preserving intra-domain diversity. Our method achieves state-of-the-art results
on DomainNet, Office-Home, and VisDA benchmarks under both standard and
few-shot settings, with notable improvements in class-level alignment fidelity
and boundary robustness. GAMA++ sets a new standard for semantic geometry
alignment in transfer learning.

</details>


### [283] [VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging](https://arxiv.org/pdf/2505.15248)
*Andre Dourson, Kylie Taylor, Xiaoli Qiao, Michael Fitzke*

Main category: cs.CV

TL;DR: VET-DINO is a self-supervised learning framework for medical imaging that leverages multi-view radiographs to improve anatomical understanding, outperforming synthetic augmentation methods.


<details>
  <summary>Details</summary>
Motivation: Labeled medical imaging data is scarce, and current self-supervised methods rely on synthetic augmentations of single images, missing the opportunity to exploit multi-view data.

Method: VET-DINO uses multi-view veterinary radiographs from the same patient study to learn view-invariant anatomical structures and implied 3D understanding from 2D projections.

Result: The framework achieves state-of-the-art performance on veterinary imaging tasks, demonstrating superior anatomical understanding compared to synthetic augmentations.

Conclusion: VET-DINO establishes a new paradigm for self-supervised learning in medical imaging by leveraging domain-specific multi-view data.

Abstract: Self-supervised learning has emerged as a powerful paradigm for training deep
neural networks, particularly in medical imaging where labeled data is scarce.
While current approaches typically rely on synthetic augmentations of single
images, we propose VET-DINO, a framework that leverages a unique characteristic
of medical imaging: the availability of multiple standardized views from the
same study. Using a series of clinical veterinary radiographs from the same
patient study, we enable models to learn view-invariant anatomical structures
and develop an implied 3D understanding from 2D projections. We demonstrate our
approach on a dataset of 5 million veterinary radiographs from 668,000 canine
studies. Through extensive experimentation, including view synthesis and
downstream task performance, we show that learning from real multi-view pairs
leads to superior anatomical understanding compared to purely synthetic
augmentations. VET-DINO achieves state-of-the-art performance on various
veterinary imaging tasks. Our work establishes a new paradigm for
self-supervised learning in medical imaging that leverages domain-specific
properties rather than merely adapting natural image techniques.

</details>


### [284] [Zero-Shot Gaze-based Volumetric Medical Image Segmentation](https://arxiv.org/pdf/2505.15256)
*Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin*

Main category: cs.CV

TL;DR: The paper introduces eye gaze as a novel input for interactive 3D medical image segmentation, comparing it with traditional prompts like bounding boxes.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on manual prompts, which can be time-consuming. Eye gaze offers a faster, though slightly less accurate, alternative.

Method: The study evaluates gaze-based prompts with SAM-2 and MedSAM-2 using synthetic and real gaze data, comparing them to bounding boxes.

Result: Gaze-based prompts are more time-efficient but yield slightly lower segmentation quality than bounding boxes.

Conclusion: Eye gaze shows promise as a complementary input for interactive 3D medical image segmentation.

Abstract: Accurate segmentation of anatomical structures in volumetric medical images
is crucial for clinical applications, including disease monitoring and cancer
treatment planning. Contemporary interactive segmentation models, such as
Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on
manually provided prompts like bounding boxes and mouse clicks. In this study,
we introduce eye gaze as a novel informational modality for interactive
segmentation, marking the application of eye-tracking for 3D medical image
segmentation. We evaluate the performance of using gaze-based prompts with
SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to
bounding boxes, gaze-based prompts offer a time-efficient interaction approach
with slightly lower segmentation quality. Our findings highlight the potential
of using gaze as a complementary input modality for interactive 3D medical
image segmentation.

</details>


### [285] [gen2seg: Generative Models Enable Generalizable Instance Segmentation](https://arxiv.org/pdf/2505.15263)
*Om Khangaonkar, Hamed Pirsiavash*

Main category: cs.CV

TL;DR: Generative models like Stable Diffusion and MAE, when fine-tuned for instance segmentation, show strong zero-shot generalization, outperforming supervised models like SAM on unseen object types and styles.


<details>
  <summary>Details</summary>
Motivation: To repurpose generative representations for general-purpose perceptual organization, leveraging their inherent understanding of object boundaries and scene compositions.

Method: Fine-tune Stable Diffusion and MAE for category-agnostic instance segmentation using an instance coloring loss, tested on indoor furnishings and cars.

Result: Models generalize well to unseen object types and styles, outperforming SAM on fine structures and ambiguous boundaries.

Conclusion: Generative models learn an inherent grouping mechanism that transfers across categories and domains, even without large-scale pretraining.

Abstract: By pretraining to synthesize coherent images from perturbed inputs,
generative models inherently learn to understand object boundaries and scene
compositions. How can we repurpose these generative representations for
general-purpose perceptual organization? We finetune Stable Diffusion and MAE
(encoder+decoder) for category-agnostic instance segmentation using our
instance coloring loss exclusively on a narrow set of object types (indoor
furnishings and cars). Surprisingly, our models exhibit strong zero-shot
generalization, accurately segmenting objects of types and styles unseen in
finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our
best-performing models closely approach the heavily supervised SAM when
evaluated on unseen object types and styles, and outperform it when segmenting
fine structures and ambiguous boundaries. In contrast, existing promptable
segmentation architectures or discriminatively pretrained models fail to
generalize. This suggests that generative models learn an inherent grouping
mechanism that transfers across categories and domains, even without
internet-scale pretraining. Code, pretrained models, and demos are available on
our website.

</details>


### [286] [Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs](https://arxiv.org/pdf/2505.15265)
*Zihao Pan, Yu Tong, Weibin Wu, Jingyi Wang, Lifeng Chen, Zhe Zhao, Jiajia Wei, Yitong Qiao, Zibin Zheng*

Main category: cs.CV

TL;DR: The paper explores how large vision-language models (LVLMs) are prone to errors due to specific semantic concepts in images, proposing a semantic evolution framework to identify these sensitive concepts.


<details>
  <summary>Details</summary>
Motivation: Understanding what semantic content in inputs causes LVLMs to fail is crucial for improving model robustness.

Method: A semantic evolution framework integrates LLMs and T2I models to generate and evaluate image descriptions, using LVLM performance as feedback to identify sensitive concepts.

Result: Experiments on seven LVLMs and two tasks confirm the method's effectiveness, revealing sensitive semantics that cause errors.

Conclusion: The study provides insights into LVLM vulnerabilities and inspires further research on model robustness.

Abstract: Adversarial attacks aim to generate malicious inputs that mislead deep
models, but beyond causing model failure, they cannot provide certain
interpretable information such as ``\textit{What content in inputs make models
more likely to fail?}'' However, this information is crucial for researchers to
specifically improve model robustness. Recent research suggests that models may
be particularly sensitive to certain semantics in visual inputs (such as
``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this
paper we conducted the first exploration on large vision-language models
(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and
various errors when facing specific semantic concepts in images. To efficiently
search for these sensitive concepts, we integrated large language models (LLMs)
and text-to-image (T2I) models to propose a novel semantic evolution framework.
Randomly initialized semantic concepts undergo LLM-based crossover and mutation
operations to form image descriptions, which are then converted by T2I models
into visual inputs for LVLMs. The task-specific performance of LVLMs on each
input is quantified as fitness scores for the involved semantics and serves as
reward signals to further guide LLMs in exploring concepts that induce LVLMs.
Extensive experiments on seven mainstream LVLMs and two multimodal tasks
demonstrate the effectiveness of our method. Additionally, we provide
interesting findings about the sensitive semantics of LVLMs, aiming to inspire
further in-depth research.

</details>


### [287] [Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation](https://arxiv.org/pdf/2505.15267)
*Wenmin Li, Shunsuke Sakai, Tatsuhito Hasegawa*

Main category: cs.CV

TL;DR: A novel dataset distillation method using contrastive learning improves synthetic data quality and model performance in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the failure of current dataset distillation techniques to preserve semantic richness under extreme sample scarcity.

Method: Integrates contrastive learning during image synthesis to maximize instance-level feature discrimination.

Result: Produces more informative and diverse synthetic samples, enhancing model performance on very small-scale synthetic datasets.

Conclusion: The proposed method outperforms existing techniques, especially in scenarios with extremely limited synthetic data.

Abstract: Deploying machine learning models in resource-constrained environments, such
as edge devices or rapid prototyping scenarios, increasingly demands
distillation of large datasets into significantly smaller yet informative
synthetic datasets. Current dataset distillation techniques, particularly
Trajectory Matching methods, optimize synthetic data so that the model's
training trajectory on synthetic samples mirrors that on real data. While
demonstrating efficacy on medium-scale synthetic datasets, these methods fail
to adequately preserve semantic richness under extreme sample scarcity. To
address this limitation, we propose a novel dataset distillation method
integrating contrastive learning during image synthesis. By explicitly
maximizing instance-level feature discrimination, our approach produces more
informative and diverse synthetic samples, even when dataset sizes are
significantly constrained. Experimental results demonstrate that incorporating
contrastive learning substantially enhances the performance of models trained
on very small-scale synthetic datasets. This integration not only guides more
effective feature representation but also significantly improves the visual
fidelity of the synthesized images. Experimental results demonstrate that our
method achieves notable performance improvements over existing distillation
techniques, especially in scenarios with extremely limited synthetic data.

</details>


### [288] [LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval](https://arxiv.org/pdf/2505.15269)
*Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, Jieru Zhao*

Main category: cs.CV

TL;DR: LiveVLM is a training-free framework for real-time video understanding, addressing memory and speed issues in Video LLMs by using a streaming-oriented KV cache and efficient question-answering.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs focus on offline tasks, neglecting real-world needs like memory efficiency and response speed in applications like autonomous driving.

Method: LiveVLM introduces a streaming-oriented KV cache to process video streams in real-time, compressing video KVs for memory efficiency and enabling prompt responses.

Result: LiveVLM processes 44x more frames and achieves 5x speedup in response speed compared to state-of-the-art methods, maintaining performance.

Conclusion: LiveVLM effectively addresses real-time video understanding challenges, improving efficiency and speed without compromising accuracy.

Abstract: Recent developments in Video Large Language Models (Video LLMs) have enabled
models to process long video sequences and demonstrate remarkable performance.
Nonetheless, studies predominantly focus on offline video question answering,
neglecting memory usage and response speed that are essential in various
real-world applications, such as Deepseek services, autonomous driving, and
robotics. To mitigate these challenges, we propose $\textbf{LiveVLM}$, a
training-free framework specifically designed for streaming, online video
understanding and real-time interaction. Unlike existing works that process
videos only after one question is posed, LiveVLM constructs an innovative
streaming-oriented KV cache to process video streams in real-time, retain
long-term video details and eliminate redundant KVs, ensuring prompt responses
to user queries. For continuous video streams, LiveVLM generates and compresses
video key-value tensors (video KVs) to reserve visual information while
improving memory efficiency. Furthermore, when a new question is proposed,
LiveVLM incorporates an online question-answering process that efficiently
fetches both short-term and long-term visual information, while minimizing
interference from redundant context. Extensive experiments demonstrate that
LiveVLM enables the foundation LLaVA-OneVision model to process 44$\times$
number of frames on the same device, and achieves up to 5$\times$ speedup in
response speed compared with SoTA online methods at an input of 256 frames,
while maintaining the same or better model performance.

</details>


### [289] [DiffProb: Data Pruning for Face Recognition](https://arxiv.org/pdf/2505.15272)
*Eduarda Caldeira, Jan Niklas Kolf, Naser Damer, Fadi Boutros*

Main category: cs.CV

TL;DR: DiffProb is a data pruning method for face recognition that removes redundant and low-impact training samples while maintaining or improving accuracy, reducing training costs and data volume.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of computational cost, data storage, and privacy concerns in face recognition by reducing reliance on large annotated datasets.

Method: Assesses prediction probabilities of training samples, prunes redundant ones, and includes an auxiliary cleaning mechanism to remove mislabeled samples.

Result: Prunes up to 50% of data while maintaining or improving verification accuracies on benchmarks like LFW, CFP-FP, and IJB-C.

Conclusion: DiffProb efficiently reduces training costs and data volume, enhancing face recognition training without compromising performance.

Abstract: Face recognition models have made substantial progress due to advances in
deep learning and the availability of large-scale datasets. However, reliance
on massive annotated datasets introduces challenges related to training
computational cost and data storage, as well as potential privacy concerns
regarding managing large face datasets. This paper presents DiffProb, the first
data pruning approach for the application of face recognition. DiffProb
assesses the prediction probabilities of training samples within each identity
and prunes the ones with identical or close prediction probability values, as
they are likely reinforcing the same decision boundaries, and thus contribute
minimally with new information. We further enhance this process with an
auxiliary cleaning mechanism to eliminate mislabeled and label-flipped samples,
boosting data quality with minimal loss. Extensive experiments on CASIA-WebFace
with different pruning ratios and multiple benchmarks, including LFW, CFP-FP,
and IJB-C, demonstrate that DiffProb can prune up to 50% of the dataset while
maintaining or even, in some settings, improving the verification accuracies.
Additionally, we demonstrate DiffProb's robustness across different
architectures and loss functions. Our method significantly reduces training
cost and data volume, enabling efficient face recognition training and reducing
the reliance on massive datasets and their demanding management.

</details>


### [290] [GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation](https://arxiv.org/pdf/2505.15287)
*Yuchen Li, Chaoran Feng, Zhenyu Tang, Kaiyuan Deng, Wangbo Yu, Yonghong Tian, Li Yuan*

Main category: cs.CV

TL;DR: GS2E is a synthetic event dataset created from sparse multi-view RGB images using 3D Gaussian Splatting and a novel event simulation pipeline, addressing limitations of existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing event datasets lack viewpoint diversity, geometric consistency, or rely on costly hardware. GS2E aims to provide high-fidelity event data for event vision tasks.

Method: GS2E reconstructs scenes with 3D Gaussian Splatting and simulates events using adaptive trajectory interpolation and physically-consistent contrast threshold modeling.

Result: GS2E produces dense, geometrically consistent event streams under varied conditions, outperforming in event-based 3D reconstruction tasks.

Conclusion: GS2E serves as a valuable benchmark for advancing event vision research due to its superior generalization and fidelity.

Abstract: We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic
event dataset for high-fidelity event vision tasks, captured from real-world
sparse multi-view RGB images. Existing event datasets are often synthesized
from dense RGB videos, which typically lack viewpoint diversity and geometric
consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E
overcomes these limitations by first reconstructing photorealistic static
scenes using 3D Gaussian Splatting, and subsequently employing a novel,
physically-informed event simulation pipeline. This pipeline generally
integrates adaptive trajectory interpolation with physically-consistent event
contrast threshold modeling. Such an approach yields temporally dense and
geometrically consistent event streams under diverse motion and lighting
conditions, while ensuring strong alignment with underlying scene structures.
Experimental results on event-based 3D reconstruction demonstrate GS2E's
superior generalization capabilities and its practical value as a benchmark for
advancing event vision research.

</details>


### [291] [R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections](https://arxiv.org/pdf/2505.15294)
*Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu*

Main category: cs.CV

TL;DR: R3GS is a robust framework for 3D reconstruction and relocalization in unconstrained datasets, using hybrid features, transient object mitigation, sky-handling, and lighting-robust relocalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in 3D reconstruction and relocalization, such as transient objects, sky regions, and lighting changes, in unconstrained datasets.

Method: Combines CNN global features with multiresolution hash grid local features, uses MLPs for Gaussian attributes, fine-tunes a human detection network for transient objects, and introduces a sky-handling technique with depth prior.

Result: R3GS improves rendering fidelity, efficiency, and storage, achieving state-of-the-art performance on in-the-wild datasets.

Conclusion: The framework effectively handles unconstrained scenes, offering robust reconstruction and relocalization with open-source availability.

Abstract: We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.

</details>


### [292] [BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution](https://arxiv.org/pdf/2505.15308)
*Ji Guo, Xiaolei Wen, Wenbo Jiang, Cheng Huang, Jinjin Li, Hongwei Li*

Main category: cs.CV

TL;DR: BadSR improves stealthiness of poisoned HR images in backdoor attacks on super-resolution models by approximating clean and target images in feature space and using optimized triggers.


<details>
  <summary>Details</summary>
Motivation: Prior backdoor attacks on SR models lacked stealthiness in poisoned HR images, making them detectable. BadSR addresses this gap.

Method: BadSR approximates clean and target HR images in feature space, uses adversarially optimized triggers, and a genetic algorithm for poisoned sample selection.

Result: BadSR achieves high attack success rates across models and datasets, significantly impacting downstream tasks.

Conclusion: BadSR enhances stealthiness and effectiveness of backdoor attacks on SR models, posing a security challenge.

Abstract: With the widespread application of super-resolution (SR) in various fields,
researchers have begun to investigate its security. Previous studies have
demonstrated that SR models can also be subjected to backdoor attacks through
data poisoning, affecting downstream tasks. A backdoor SR model generates an
attacker-predefined target image when given a triggered image while producing a
normal high-resolution (HR) output for clean images. However, prior backdoor
attacks on SR models have primarily focused on the stealthiness of poisoned
low-resolution (LR) images while ignoring the stealthiness of poisoned HR
images, making it easy for users to detect anomalous data. To address this
problem, we propose BadSR, which improves the stealthiness of poisoned HR
images. The key idea of BadSR is to approximate the clean HR image and the
pre-defined target image in the feature space while ensuring that modifications
to the clean HR image remain within a constrained range. The poisoned HR images
generated by BadSR can be integrated with existing triggers. To further improve
the effectiveness of BadSR, we design an adversarially optimized trigger and a
backdoor gradient-driven poisoned sample selection method based on a genetic
algorithm. The experimental results show that BadSR achieves a high attack
success rate in various models and data sets, significantly affecting
downstream tasks.

</details>


### [293] [FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion](https://arxiv.org/pdf/2505.15313)
*Kazuaki Mishima, Antoni Bigata Casademunt, Stavros Petridis, Maja Pantic, Kenji Suzuki*

Main category: cs.CV

TL;DR: A novel identity-conditional diffusion model is proposed to independently control facial pose, expression, and emotion while preserving identity, outperforming existing methods in accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Precise control over non-identity attributes in facial images is challenging, and disentangling identity from mutable factors is difficult.

Method: Introduces lightweight control modules in cross-attention layers of a diffusion model, with a tailored training strategy to enhance orthogonality between identity and control features.

Result: Outperforms existing methods in control accuracy and generative diversity, validated by quantitative, qualitative, and user studies.

Conclusion: The proposed model effectively balances identity preservation and attribute control, advancing high-quality face synthesis.

Abstract: Human facial images encode a rich spectrum of information, encompassing both
stable identity-related traits and mutable attributes such as pose, expression,
and emotion. While recent advances in image generation have enabled
high-quality identity-conditional face synthesis, precise control over
non-identity attributes remains challenging, and disentangling identity from
these mutable factors is particularly difficult. To address these limitations,
we propose a novel identity-conditional diffusion model that introduces two
lightweight control modules designed to independently manipulate facial pose,
expression, and emotion without compromising identity preservation. These
modules are embedded within the cross-attention layers of the base diffusion
model, enabling precise attribute control with minimal parameter overhead.
Furthermore, our tailored training strategy, which leverages cross-attention
between the identity feature and each non-identity control feature, encourages
identity features to remain orthogonal to control signals, enhancing
controllability and diversity. Quantitative and qualitative evaluations, along
with perceptual user studies, demonstrate that our method surpasses existing
approaches in terms of control accuracy over pose, expression, and emotion,
while also improving generative diversity under identity-only conditioning.

</details>


### [294] [CEBSNet: Change-Excited and Background-Suppressed Network with Temporal Dependency Modeling for Bitemporal Change Detection](https://arxiv.org/pdf/2505.15322)
*Qi'ao Xu, Yan Xing, Jiali Hu, Yunan Jia, Rui Huang*

Main category: cs.CV

TL;DR: CEBSNet is a novel network for change detection that models temporal dependencies and balances detection of obvious and subtle changes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current change detection methods overlook temporal dependencies and focus too much on prominent changes, ignoring subtle ones.

Method: CEBSNet uses a Channel Swap Module (CSM) for temporal dependency, a Feature Excitation and Suppression Module (FESM) for balanced change detection, and a Pyramid-Aware Spatial-Channel Attention module (PASCA) for multi-scale focus.

Result: CEBSNet outperforms existing methods on three street view and two remote sensing datasets.

Conclusion: CEBSNet effectively addresses limitations in change detection by modeling temporal dependencies and capturing both obvious and subtle changes.

Abstract: Change detection, a critical task in remote sensing and computer vision, aims
to identify pixel-level differences between image pairs captured at the same
geographic area but different times. It faces numerous challenges such as
illumination variation, seasonal changes, background interference, and shooting
angles, especially with a large time gap between images. While current methods
have advanced, they often overlook temporal dependencies and overemphasize
prominent changes while ignoring subtle but equally important changes. To
address these limitations, we introduce \textbf{CEBSNet}, a novel
change-excited and background-suppressed network with temporal dependency
modeling for change detection. During the feature extraction, we utilize a
simple Channel Swap Module (CSM) to model temporal dependency, reducing
differences and noise. The Feature Excitation and Suppression Module (FESM) is
developed to capture both obvious and subtle changes, maintaining the integrity
of change regions. Additionally, we design a Pyramid-Aware Spatial-Channel
Attention module (PASCA) to enhance the ability to detect change regions at
different sizes and focus on critical regions. We conduct extensive experiments
on three common street view datasets and two remote sensing datasets, and our
method achieves the state-of-the-art performance.

</details>


### [295] [SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition](https://arxiv.org/pdf/2505.15325)
*Mengqi Lei, Yihong Wu, Siqi Li, Xinhu Zheng, Juan Wang, Yue Gao, Shaoyi Du*

Main category: cs.CV

TL;DR: SoftHGNN introduces soft hyperedges with continuous weights to capture high-order visual semantics efficiently, outperforming traditional hypergraph methods.


<details>
  <summary>Details</summary>
Motivation: Existing hypergraph neural networks use static, hard hyperedge assignments, leading to redundancy and overlooking visual semantics continuity.

Method: SoftHGNN employs learnable hyperedge prototypes and soft hyperedges with continuous weights, plus a sparse selection mechanism for efficiency.

Result: SoftHGNN achieves significant performance improvements across three tasks on five datasets by capturing high-order associations.

Conclusion: SoftHGNN offers an efficient, versatile framework for visual recognition by dynamically modeling high-order interactions.

Abstract: Visual recognition relies on understanding both the semantics of image tokens
and the complex interactions among them. Mainstream self-attention methods,
while effective at modeling global pair-wise relations, fail to capture
high-order associations inherent in real-world scenes and often suffer from
redundant computation. Hypergraphs extend conventional graphs by modeling
high-order interactions and offer a promising framework for addressing these
limitations. However, existing hypergraph neural networks typically rely on
static and hard hyperedge assignments, leading to excessive and redundant
hyperedges with hard binary vertex memberships that overlook the continuity of
visual semantics. To overcome these issues, we present Soft Hypergraph Neural
Networks (SoftHGNNs), which extend the methodology of hypergraph computation,
to make it truly efficient and versatile in visual recognition tasks. Our
framework introduces the concept of soft hyperedges, where each vertex is
associated with hyperedges via continuous participation weights rather than
hard binary assignments. This dynamic and differentiable association is
achieved by using the learnable hyperedge prototype. Through similarity
measurements between token features and the prototype, the model generates
semantically rich soft hyperedges. SoftHGNN then aggregates messages over soft
hyperedges to capture high-order semantics. To further enhance efficiency when
scaling up the number of soft hyperedges, we incorporate a sparse hyperedge
selection mechanism that activates only the top-k important hyperedges, along
with a load-balancing regularizer to ensure balanced hyperedge utilization.
Experimental results across three tasks on five datasets demonstrate that
SoftHGNN efficiently captures high-order associations in visual scenes,
achieving significant performance improvements.

</details>


### [296] [Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models](https://arxiv.org/pdf/2505.15332)
*Ria Shekhawat, Hailin Li, Raghavendra Ramachandra, Sushma Venkatesh*

Main category: cs.CV

TL;DR: The paper introduces multimodal LLMs for differential morphing attack detection (D-MAD), using Chain-of-Thought prompts to improve reliability. ChatGPT-4o outperforms Gemini in accuracy but has higher failure rates.


<details>
  <summary>Details</summary>
Motivation: Enhancing morphing attack detection (MAD) accuracy and interpretability in biometric applications using multimodal LLMs.

Method: Uses Chain-of-Thought prompts with multimodal LLMs (ChatGPT-4o and Gemini) on real biometric data from 54 individuals.

Result: ChatGPT-4o is more accurate but has higher failure rates; Gemini provides more consistent explanations.

Conclusion: Multimodal LLMs show promise for D-MAD, with trade-offs between accuracy and reliability in different models.

Abstract: Leveraging the power of multimodal large language models (LLMs) offers a
promising approach to enhancing the accuracy and interpretability of morphing
attack detection (MAD), especially in real-world biometric applications. This
work introduces the use of LLMs for differential morphing attack detection
(D-MAD). To the best of our knowledge, this is the first study to employ
multimodal LLMs to D-MAD using real biometric data. To effectively utilize
these models, we design Chain-of-Thought (CoT)-based prompts to reduce
failure-to-answer rates and enhance the reasoning behind decisions. Our
contributions include: (1) the first application of multimodal LLMs for D-MAD
using real data subjects, (2) CoT-based prompt engineering to improve response
reliability and explainability, (3) comprehensive qualitative and quantitative
benchmarking of LLM performance using data from 54 individuals captured in
passport enrollment scenarios, and (4) comparative analysis of two multimodal
LLMs: ChatGPT-4o and Gemini providing insights into their morphing attack
detection accuracy and decision transparency. Experimental results show that
ChatGPT-4o outperforms Gemini in detection accuracy, especially against
GAN-based morphs, though both models struggle under challenging conditions.
While Gemini offers more consistent explanations, ChatGPT-4o is more resilient
but prone to a higher failure-to-answer rate.

</details>


### [297] [Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification](https://arxiv.org/pdf/2505.15334)
*Bernardin Ligan, Khalide Jbilou, Fahd Kalloubi, Ahmed Ratnani*

Main category: cs.CV

TL;DR: The paper proposes an efficient framework to fine-tune the multispectral foundation model SpectralGPT for hyperspectral image classification, introducing KronA+ as a parameter-efficient method that outperforms others in terms of performance and resource usage.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imagery (HSI) is underutilized in foundation models, and fine-tuning existing models for HSI tasks is resource-intensive. The paper aims to address these challenges.

Method: The authors explore Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA, KronA, LoKr, and LoRA+, and introduce KronA+, which applies a learning rate mechanism to Kronecker matrices.

Result: KronA+ achieves competitive performance with minimal trainable parameters (0.056%) and storage (0.2 MB), outperforming full fine-tuning and other PEFT methods.

Conclusion: KronA+ is the most effective PEFT method for fine-tuning SpectralGPT for HSI tasks, offering high performance with low resource demands.

Abstract: Foundation models have achieved great success across diverse domains,
including remote sensing (RS), thanks to their versatility and strong
generalization abilities. However, most RS foundation models are designed for
multispectral data, while hyperspectral imagery (HSI) - with its hundreds of
spectral bands - remains less explored. Fine-tuning such models for downstream
tasks is also challenging, often demanding considerable memory and storage. In
this paper, we propose an efficient framework to fine-tune SpectralGPT, a
multispectral foundation model, for hyperspectral image classification (HSIC).
We explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including
Low-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank
Kronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for
low-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce
KronA+, which applies a similar mechanism to the Kronecker matrices. We
evaluate our approach on five datasets from different sensors, showing
competitive performance with state-of-the-art HSI models. Our full fine-tuning
(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral
foundation model on some datasets while requiring only a quarter of the
training epochs. Under the same number of epochs, KronA+ reaches similar
performance with far fewer trainable parameters - just 0.056 percent - and adds
only approximately 0.2 megabytes of storage, making it the most effective PEFT
method tested.

</details>


### [298] [My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping](https://arxiv.org/pdf/2505.15336)
*Hon Ming Yam, Zhongliang Guo, Chun Pong Lau*

Main category: cs.CV

TL;DR: A proactive defense strategy using adversarial attacks to protect facial images from diffusion-based deepfake manipulation, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise of diffusion-based deepfake technologies necessitates innovative defenses, as traditional and current adversarial methods are ineffective against these models.

Method: Introduces a novel adversarial attack strategy tailored for diffusion models, focusing on region-specific perturbations to counteract facial manipulation.

Result: The proposed method aims to outperform existing approaches by addressing the unique challenges of diffusion-based deepfakes.

Conclusion: This proactive defense strategy offers a promising solution to protect facial images from unauthorized manipulation by diffusion models.

Abstract: The proliferation of diffusion-based deepfake technologies poses significant
risks for unauthorized and unethical facial image manipulation. While
traditional countermeasures have primarily focused on passive detection
methods, this paper introduces a novel proactive defense strategy through
adversarial attacks that preemptively protect facial images from being
exploited by diffusion-based deepfake systems. Existing adversarial protection
methods predominantly target conventional generative architectures (GANs, AEs,
VAEs) and fail to address the unique challenges presented by diffusion models,
which have become the predominant framework for high-quality facial deepfakes.
Current diffusion-specific adversarial approaches are limited by their reliance
on specific model architectures and weights, rendering them ineffective against
the diverse landscape of diffusion-based deepfake implementations.
Additionally, they typically employ global perturbation strategies that
inadequately address the region-specific nature of facial manipulation in
deepfakes.

</details>


### [299] [Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model](https://arxiv.org/pdf/2505.15358)
*Angelique Mangubat, Shane Gilroy*

Main category: cs.CV

TL;DR: A novel benchmark for bicycle occlusion level classification using computer vision improves cyclist detection in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Enhancing road safety for cyclists by objectively quantifying bicycle visibility and occlusion levels.

Method: Parts-based detection model with custom image processing pipeline to classify bicycle occlusion levels.

Result: The model robustly quantifies bicycle visibility and occlusion, outperforming subjective current methods.

Conclusion: The methodology aids in developing better cyclist detection algorithms for autonomous vehicles.

Abstract: Road safety is a critical challenge, particularly for cyclists, who are among
the most vulnerable road users. This study aims to enhance road safety by
proposing a novel benchmark for bicycle occlusion level classification using
advanced computer vision techniques. Utilizing a parts-based detection model,
images are annotated and processed through a custom image detection pipeline. A
novel method of bicycle occlusion level is proposed to objectively quantify the
visibility and occlusion level of bicycle semantic parts. The findings indicate
that the model robustly quantifies the visibility and occlusion level of
bicycles, a significant improvement over the subjective methods used by the
current state of the art. Widespread use of the proposed methodology will
facilitate the accurate performance reporting of cyclist detection algorithms
for occluded cyclists, informing the development of more robust vulnerable road
user detection methods for autonomous vehicles.

</details>


### [300] [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/pdf/2505.15367)
*Dasol Choi, Seunghyun Lee, Youngsook Song*

Main category: cs.CV

TL;DR: VLMs show high accuracy in identifying real emergencies but often misclassify safe situations as dangerous, revealing a systematic overreaction problem.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of Vision-Language Models (VLMs) in safety-critical contexts, as their performance in such scenarios is under-explored.

Method: Introduces VERI, a diagnostic benchmark of 200 images (100 contrastive pairs), evaluated using a two-stage protocol (risk identification and emergency response) on 14 VLMs.

Result: Models identify real emergencies well (70-100% success) but misclassify 31-96% of safe situations as dangerous, with contextual overinterpretation as the main error source.

Conclusion: VLMs' reliability for safety applications is limited by systematic biases, unresolved by scaling, necessitating targeted improvements for contextual safety assessment.

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
understanding visual content, but their reliability in safety-critical contexts
remains under-explored. We introduce VERI (Visual Emergency Recognition
Dataset), a carefully designed diagnostic benchmark of 200 images (100
contrastive pairs). Each emergency scene is matched with a visually similar but
safe counterpart through multi-stage human verification and iterative
refinement. Using a two-stage protocol - risk identification and emergency
response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,
accidents, and natural disasters. Our analysis reveals a systematic
overreaction problem: models excel at identifying real emergencies (70-100
percent success rate) but suffer from an alarming rate of false alarms,
misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios
failed by all models regardless of scale. This "better-safe-than-sorry" bias
manifests primarily through contextual overinterpretation (88-93 percent of
errors), challenging VLMs' reliability for safety applications. These findings
highlight persistent limitations that are not resolved by increasing model
scale, motivating targeted approaches for improving contextual safety
assessment in visually misleading scenarios.

</details>


### [301] [RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation](https://arxiv.org/pdf/2505.15373)
*Naman Patel, Prashanth Krishnamurthy, Farshad Khorrami*

Main category: cs.CV

TL;DR: A zero-shot framework integrates geometric reconstruction with open-vocabulary vision-language models for real-time 3D semantic mapping, supporting natural language interactions.


<details>
  <summary>Details</summary>
Motivation: Existing 3D semantic mapping systems lack flexibility for open-vocabulary online operation, and vision-language models haven't bridged the gap to 3D spatial understanding.

Method: The framework combines GPU-accelerated geometric reconstruction with open-vocabulary models via online instance-level semantic embedding fusion, guided by hierarchical object association.

Result: The system achieves superior performance with incremental processing and unified updates, handling 2D segmentation inconsistencies robustly.

Conclusion: The framework enables zero-shot 3D instance retrieval, segmentation, and object detection, interpreting natural language queries for unseen objects.

Abstract: Mapping and understanding complex 3D environments is fundamental to how
autonomous systems perceive and interact with the physical world, requiring
both precise geometric reconstruction and rich semantic comprehension. While
existing 3D semantic mapping systems excel at reconstructing and identifying
predefined object instances, they lack the flexibility to efficiently build
semantic maps with open-vocabulary during online operation. Although recent
vision-language models have enabled open-vocabulary object recognition in 2D
images, they haven't yet bridged the gap to 3D spatial understanding. The
critical challenge lies in developing a training-free unified system that can
simultaneously construct accurate 3D maps while maintaining semantic
consistency and supporting natural language interactions in real time. In this
paper, we develop a zero-shot framework that seamlessly integrates
GPU-accelerated geometric reconstruction with open-vocabulary vision-language
models through online instance-level semantic embedding fusion, guided by
hierarchical object association with spatial indexing. Our training-free system
achieves superior performance through incremental processing and unified
geometric-semantic updates, while robustly handling 2D segmentation
inconsistencies. The proposed general-purpose 3D scene understanding framework
can be used for various tasks including zero-shot 3D instance retrieval,
segmentation, and object detection to reason about previously unseen objects
and interpret natural language queries. The project page is available at
https://razer-3d.github.io.

</details>


### [302] [The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization](https://arxiv.org/pdf/2505.15379)
*Raphael Sulzer, Liuyun Duan, Nicolas Girard, Florent Lafarge*

Main category: cs.CV

TL;DR: The P$^3$ dataset is a large-scale multimodal benchmark for building vectorization, combining LiDAR, aerial imagery, and 2D building outlines. It shows LiDAR's robustness for predicting building polygons and benefits from fusion with imagery.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets incorporating dense 3D information (LiDAR) alongside imagery for building vectorization, providing a complementary perspective to existing image-focused datasets.

Method: Constructed from aerial LiDAR point clouds, high-resolution aerial imagery, and vectorized 2D building outlines across three continents. Evaluated in hybrid and end-to-end learning frameworks.

Result: LiDAR point clouds are robust for predicting building polygons, and fusion with imagery improves accuracy and geometric quality.

Conclusion: The P$^3$ dataset, publicly available with code and pretrained models, advances building polygon prediction by leveraging multimodal data.

Abstract: We present the P$^3$ dataset, a large-scale multimodal benchmark for building
vectorization, constructed from aerial LiDAR point clouds, high-resolution
aerial imagery, and vectorized 2D building outlines, collected across three
continents. The dataset contains over 10 billion LiDAR points with
decimeter-level accuracy and RGB images at a ground sampling distance of 25
centimeter. While many existing datasets primarily focus on the image modality,
P$^3$ offers a complementary perspective by also incorporating dense 3D
information. We demonstrate that LiDAR point clouds serve as a robust modality
for predicting building polygons, both in hybrid and end-to-end learning
frameworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy
and geometric quality of predicted polygons. The P$^3$ dataset is publicly
available, along with code and pretrained weights of three state-of-the-art
models for building polygon prediction at
https://github.com/raphaelsulzer/PixelsPointsPolygons .

</details>


### [303] [EVA: Expressive Virtual Avatars from Multi-view Videos](https://arxiv.org/pdf/2505.15385)
*Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann*

Main category: cs.CV

TL;DR: EVA introduces a two-layer model for expressive human avatars, enabling independent control of facial expressions, body movements, and hand gestures, outperforming existing methods in rendering quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack complete and expressive control over human avatars due to entangled representations of facial and body movements.

Method: EVA uses a two-layer model: an expressive template geometry layer and a decoupled 3D Gaussian appearance layer, with specialized modules for body and face.

Result: EVA achieves high-fidelity, lifelike renderings in real time and surpasses state-of-the-art methods in quality and expressiveness.

Conclusion: EVA advances digital human modeling, enabling lifelike avatars with faithful geometry and appearance replication.

Abstract: With recent advancements in neural rendering and motion capture algorithms,
remarkable progress has been made in photorealistic human avatar modeling,
unlocking immense potential for applications in virtual reality, augmented
reality, remote communication, and industries such as gaming, film, and
medicine. However, existing methods fail to provide complete, faithful, and
expressive control over human avatars due to their entangled representation of
facial expressions and body movements. In this work, we introduce Expressive
Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive
human avatar framework that achieves high-fidelity, lifelike renderings in real
time while enabling independent control of facial expressions, body movements,
and hand gestures. Specifically, our approach designs the human avatar as a
two-layer model: an expressive template geometry layer and a 3D Gaussian
appearance layer. First, we present an expressive template tracking algorithm
that leverages coarse-to-fine optimization to accurately recover body motions,
facial expressions, and non-rigid deformation parameters from multi-view
videos. Next, we propose a novel decoupled 3D Gaussian appearance model
designed to effectively disentangle body and facial appearance. Unlike unified
Gaussian estimation approaches, our method employs two specialized and
independent modules to model the body and face separately. Experimental results
demonstrate that EVA surpasses state-of-the-art methods in terms of rendering
quality and expressiveness, validating its effectiveness in creating full-body
avatars. This work represents a significant advancement towards fully drivable
digital human models, enabling the creation of lifelike digital avatars that
faithfully replicate human geometry and appearance.

</details>


### [304] [Expanding Zero-Shot Object Counting with Rich Prompts](https://arxiv.org/pdf/2505.15398)
*Huilin Zhu, Senyao Li, Jingling Yuan, Zhengwei Yang, Yu Guo, Wenxuan Liu, Xian Zhong, Shengfeng He*

Main category: cs.CV

TL;DR: RichCount improves zero-shot counting for unseen categories by enhancing text encoding and aligning text-visual features, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to align text and visual features for accurate counting in unseen categories, prompting the need for a more robust framework.

Method: RichCount uses a two-stage training strategy: (1) enriching text features with a feed-forward network and adapter, and (2) applying the refined encoder to counting tasks for better generalization.

Result: RichCount achieves state-of-the-art performance on three benchmark datasets, enhancing generalization to unseen categories.

Conclusion: RichCount effectively addresses the limitations of simple prompt expansion by establishing meaningful feature alignment for accurate zero-shot counting.

Abstract: Expanding pre-trained zero-shot counting models to handle unseen categories
requires more than simply adding new prompts, as this approach does not achieve
the necessary alignment between text and visual features for accurate counting.
We introduce RichCount, the first framework to address these limitations,
employing a two-stage training strategy that enhances text encoding and
strengthens the model's association with objects in images. RichCount improves
zero-shot counting for unseen categories through two key objectives: (1)
enriching text features with a feed-forward network and adapter trained on
text-image similarity, thereby creating robust, aligned representations; and
(2) applying this refined encoder to counting tasks, enabling effective
generalization across diverse prompts and complex images. In this manner,
RichCount goes beyond simple prompt expansion to establish meaningful feature
alignment that supports accurate counting across novel categories. Extensive
experiments on three benchmark datasets demonstrate the effectiveness of
RichCount, achieving state-of-the-art performance in zero-shot counting and
significantly enhancing generalization to unseen categories in open-world
scenarios.

</details>


### [305] [Visual Question Answering on Multiple Remote Sensing Image Modalities](https://arxiv.org/pdf/2505.15401)
*Hichem Boussaid, Lucrezia Tosato, Flora Weissgerber, Camille Kurtz, Laurent Wendling, Sylvain Lobry*

Main category: cs.CV

TL;DR: The paper introduces a multi-modal VQA task for remote sensing, proposing a new dataset (TAMMI) and a model (MM-RSVQA) to combine multiple image modalities, achieving 65.56% accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving visual feature extraction in VQA by leveraging complementary multi-modal remote sensing data to enhance scene understanding.

Method: Introduces the TAMMI dataset with three image modalities and the MM-RSVQA model, based on VisualBERT, for multi-modal fusion.

Result: Achieves 65.56% accuracy on the VQA task, demonstrating the potential of multi-modal approaches.

Conclusion: Pioneers a new multi-modal VQA task for remote sensing, with broader applications in domains like medical imaging.

Abstract: The extraction of visual features is an essential step in Visual Question
Answering (VQA). Building a good visual representation of the analyzed scene is
indeed one of the essential keys for the system to be able to correctly
understand the latter in order to answer complex questions. In many fields such
as remote sensing, the visual feature extraction step could benefit
significantly from leveraging different image modalities carrying complementary
spectral, spatial and contextual information. In this work, we propose to add
multiple image modalities to VQA in the particular context of remote sensing,
leading to a novel task for the computer vision community. To this end, we
introduce a new VQA dataset, named TAMMI (Text and Multi-Modal Imagery) with
diverse questions on scenes described by three different modalities (very high
resolution RGB, multi-spectral imaging data and synthetic aperture radar).
Thanks to an automated pipeline, this dataset can be easily extended according
to experimental needs. We also propose the MM-RSVQA (Multi-modal
Multi-resolution Remote Sensing Visual Question Answering) model, based on
VisualBERT, a vision-language transformer, to effectively combine the multiple
image modalities and text through a trainable fusion process. A preliminary
experimental study shows promising results of our methodology on this
challenging dataset, with an accuracy of 65.56% on the targeted VQA task. This
pioneering work paves the way for the community to a new multi-modal
multi-resolution VQA task that can be applied in other imaging domains (such as
medical imaging) where multi-modality can enrich the visual representation of a
scene. The dataset and code are available at https://tammi.sylvainlobry.com/.

</details>


### [306] [Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes](https://arxiv.org/pdf/2505.15408)
*Patrik Reiske, Marcus N. Boon, Niek Andresen, Sole Traverso, Katharina Hohlbaum, Lars Lewejohann, Christa Thöne-Reineke, Olaf Hellwich, Henning Sprekeler*

Main category: cs.CV

TL;DR: A video dataset of mice solving complex mechanical puzzles (lockboxes) is presented, with over 110 hours of footage from three perspectives, aiming to advance automated behavior classification in neuroscience.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on simple or social behaviors, lacking resources for complex individual behaviors like puzzle-solving in mice.

Method: The dataset includes human-annotated labels for 13% of videos and uses a keypoint tracking-based framework for action classification.

Result: The framework highlights challenges in automated labeling of fine-grained behaviors, such as object manipulation.

Conclusion: The dataset and framework aim to accelerate progress in automated action and behavior classification, with the dataset publicly available.

Abstract: Machine learning and computer vision methods have a major impact on the study
of natural animal behavior, as they enable the (semi-)automatic analysis of
vast amounts of video data. Mice are the standard mammalian model system in
most research fields, but the datasets available today to refine such methods
focus either on simple or social behaviors. In this work, we present a video
dataset of individual mice solving complex mechanical puzzles, so-called
lockboxes. The more than 110 hours of total playtime show their behavior
recorded from three different perspectives. As a benchmark for frame-level
action classification methods, we provide human-annotated labels for all videos
of two different mice, that equal 13% of our dataset. Our keypoint (pose)
tracking-based action classification framework illustrates the challenges of
automated labeling of fine-grained behaviors, such as the manipulation of
objects. We hope that our work will help accelerate the advancement of
automated action and behavior classification in the computational neuroscience
community. Our dataset is publicly available at
https://doi.org/10.14279/depositonce-23850

</details>


### [307] [Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks](https://arxiv.org/pdf/2505.15414)
*Uranik Berisha, Jens Mehnert, Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: A method to create Mixture-of-Experts (MoE) variants from pretrained Vision Transformers by clustering activations and extracting subnetworks, achieving 98% original performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the high computational demands of Vision Transformers by leveraging pretrained models and sparse activation patterns for efficiency.

Method: Clusters output activations to identify patterns, then extracts corresponding subnetworks from MLP layers in two phases.

Result: Achieves 98% of original performance with 36% fewer MACs and 32% smaller model size on ImageNet-1k.

Conclusion: The method efficiently constructs MoE variants from pretrained models, reducing costs while maintaining performance.

Abstract: Vision Transformers have emerged as the state-of-the-art models in various
Computer Vision tasks, but their high computational and resource demands pose
significant challenges. While Mixture-of-Experts (MoE) can make these models
more efficient, they often require costly retraining or even training from
scratch. Recent developments aim to reduce these computational costs by
leveraging pretrained networks. These have been shown to produce sparse
activation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder
blocks, allowing for conditional activation of only relevant subnetworks for
each sample. Building on this idea, we propose a new method to construct MoE
variants from pretrained models. Our approach extracts expert subnetworks from
the model's MLP layers post-training in two phases. First, we cluster output
activations to identify distinct activation patterns. In the second phase, we
use these clusters to extract the corresponding subnetworks responsible for
producing them. On ImageNet-1k recognition tasks, we demonstrate that these
extracted experts can perform surprisingly well out of the box and require only
minimal fine-tuning to regain 98% of the original performance, all while
reducing MACs and model size, by up to 36% and 32% respectively.

</details>


### [308] [On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?](https://arxiv.org/pdf/2505.15425)
*Raza Imam, Rufael Marew, Mohammad Yaqub*

Main category: cs.CV

TL;DR: The paper introduces MediMeta-C and RobustMedCLIP to evaluate and enhance the robustness of Medical Vision-Language Models (MVLMs) under noisy conditions, revealing significant performance degradation in existing models.


<details>
  <summary>Details</summary>
Motivation: Clinical imaging often involves noisy or corrupted data, but current evaluations focus on clean datasets, overlooking real-world robustness.

Method: The authors introduce MediMeta-C, a corruption benchmark, and propose RobustMedCLIP, a few-shot tuning adaptation for MVLMs to improve resilience.

Result: Experiments show existing MVLMs degrade under corruption, but RobustMedCLIP improves robustness without sacrificing generalization.

Conclusion: Diverse training and robust adaptation, like low-rank tuning, are essential for MVLMs to handle real-world distortions effectively.

Abstract: Medical Vision-Language Models (MVLMs) have achieved par excellence
generalization in medical image analysis, yet their performance under noisy,
corrupted conditions remains largely untested. Clinical imaging is inherently
susceptible to acquisition artifacts and noise; however, existing evaluations
predominantly assess generally clean datasets, overlooking robustness -- i.e.,
the model's ability to perform under real-world distortions. To address this
gap, we first introduce MediMeta-C, a corruption benchmark that systematically
applies several perturbations across multiple medical imaging datasets.
Combined with MedMNIST-C, this establishes a comprehensive robustness
evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual
encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to
enhance resilience against corruptions. Through extensive experiments, we
benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that
existing models exhibit severe degradation under corruption and struggle with
domain-modality tradeoffs. Our findings highlight the necessity of diverse
training and robust adaptation strategies, demonstrating that efficient
low-rank adaptation when paired with few-shot tuning, improves robustness while
preserving generalization across modalities.

</details>


### [309] [TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models](https://arxiv.org/pdf/2505.15435)
*Zeqing Wang, Shiyuan Zhang, Chengpei Tang, Keze Wang*

Main category: cs.CV

TL;DR: The paper introduces TimeCausality, a benchmark to evaluate temporal causal reasoning in Vision-Language Models (VLMs), revealing gaps in current models, including GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capacity of VLMs in reasoning about temporal causality, such as irreversible object transformations (e.g., fruit decay).

Method: The authors develop TimeCausality, a novel benchmark, and evaluate SOTA open-source and closed-source VLMs (e.g., GPT-4o) on it.

Result: Current VLMs, including GPT-4o, perform poorly on TimeCausality compared to standard tasks, highlighting a significant gap in temporal causal reasoning.

Conclusion: The findings emphasize the need to integrate temporal causality into VLM evaluation and development, posing a challenge for the open-source community.

Abstract: Reasoning about temporal causality, particularly irreversible transformations
of objects governed by real-world knowledge (e.g., fruit decay and human
aging), is a fundamental aspect of human visual understanding. Unlike temporal
perception based on simple event sequences, this form of reasoning requires a
deeper comprehension of how object states change over time. Although the
current powerful Vision-Language Models (VLMs) have demonstrated impressive
performance on a wide range of downstream tasks, their capacity to reason about
temporal causality remains underexplored. To address this gap, we introduce
\textbf{TimeCausality}, a novel benchmark specifically designed to evaluate the
causal reasoning ability of VLMs in the temporal dimension. Based on our
TimeCausality, we find that while the current SOTA open-source VLMs have
achieved performance levels comparable to closed-source models like GPT-4o on
various standard visual question answering tasks, they fall significantly
behind on our benchmark compared with their closed-source competitors.
Furthermore, even GPT-4o exhibits a marked drop in performance on TimeCausality
compared to its results on other tasks. These findings underscore the critical
need to incorporate temporal causality into the evaluation and development of
VLMs, and they highlight an important challenge for the open-source VLM
community moving forward. Code and Data are available at
\href{https://github.com/Zeqing-Wang/TimeCausality }{TimeCausality}.

</details>


### [310] [Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL](https://arxiv.org/pdf/2505.15436)
*Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li*

Main category: cs.CV

TL;DR: The paper introduces Chain-of-Focus (CoF), a method for VLMs to adaptively focus on key image regions for efficient multimodal reasoning, using a two-stage training pipeline (SFT and RL). It achieves significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack fully explored multimodal reasoning capabilities, prompting the need for adaptive focusing methods like CoF.

Method: Proposes CoF with a two-stage training pipeline: supervised fine-tuning (SFT) using the MM-CoF dataset and reinforcement learning (RL) for refining reasoning strategies.

Result: The model outperforms existing VLMs by 5% on the V* benchmark across multiple image resolutions, demonstrating CoF's effectiveness.

Conclusion: The CoF method enhances VLMs' reasoning and deployment efficiency, validated by benchmark improvements.

Abstract: Vision language models (VLMs) have achieved impressive performance across a
variety of computer vision tasks. However, the multimodal reasoning capability
has not been fully explored in existing models. In this paper, we propose a
Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and
zooming in on key image regions based on obtained visual cues and the given
questions, achieving efficient multimodal reasoning. To enable this CoF
capability, we present a two-stage training pipeline, including supervised
fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we
construct the MM-CoF dataset, comprising 3K samples derived from a visual agent
designed to adaptively identify key regions to solve visual tasks with
different image resolutions and questions. We use MM-CoF to fine-tune the
Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome
accuracies and formats as rewards to update the Qwen2.5-VL model, enabling
further refining the search and reasoning strategy of models without human
priors. Our model achieves significant improvements on multiple benchmarks. On
the V* benchmark that requires strong visual reasoning capability, our model
outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to
4K, demonstrating the effectiveness of the proposed CoF method and facilitating
the more efficient deployment of VLMs in practical applications.

</details>


### [311] [Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation](https://arxiv.org/pdf/2505.15438)
*Jianyuan Guo, Peike Li, Trevor Cohn*

Main category: cs.CV

TL;DR: A gloss-free SLT framework uses LLM-generated pseudo glosses and weakly supervised learning to eliminate costly gloss annotations, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Existing SLT methods rely on expensive gloss annotations, limiting scalability. The goal is to create a gloss-free approach that maintains performance.

Method: Proposes a framework using LLM-generated pseudo glosses, weakly supervised reordering, and a three-stage training pipeline with CTC loss.

Result: Outperforms gloss-free methods and matches gloss-based ones on SLT benchmarks.

Conclusion: The gloss-free framework is scalable and effective, reducing reliance on costly annotations.

Abstract: Sign Language Translation (SLT) aims to map sign language videos to spoken
language text. A common approach relies on gloss annotations as an intermediate
representation, decomposing SLT into two sub-tasks: video-to-gloss recognition
and gloss-to-text translation. While effective, this paradigm depends on
expert-annotated gloss labels, which are costly and rarely available in
existing datasets, limiting its scalability. To address this challenge, we
propose a gloss-free pseudo gloss generation framework that eliminates the need
for human-annotated glosses while preserving the structured intermediate
representation. Specifically, we prompt a Large Language Model (LLM) with a few
example text-gloss pairs using in-context learning to produce draft sign
glosses from spoken language text. To enhance the correspondence between
LLM-generated pseudo glosses and the sign sequences in video, we correct the
ordering in the pseudo glosses for better alignment via a weakly supervised
learning process. This reordering facilitates the incorporation of auxiliary
alignment objectives, and allows for the use of efficient supervision via a
Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which
consists of a vision encoder and a translator, through a three-stage pipeline,
which progressively narrows the modality gap between sign language and spoken
language. Despite its simplicity, our approach outperforms previous
state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves
competitive results compared to gloss-based methods.

</details>


### [312] [Stronger ViTs With Octic Equivariance](https://arxiv.org/pdf/2505.15441)
*David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman*

Main category: cs.CV

TL;DR: Optic ViTs improve Vision Transformers by incorporating octic group equivariance, reducing FLOPs by 40% and enhancing performance in classification and segmentation.


<details>
  <summary>Details</summary>
Motivation: To enhance Vision Transformers (ViTs) by integrating equivariance under the octic group (reflections and 90-degree rotations) as an inductive bias, improving efficiency and performance.

Method: Developed octic ViTs with octic-equivariant layers, tested on supervised (DeiT-III) and self-supervised (DINOv2) learning using ImageNet-1K.

Result: Achieved ~40% reduction in FLOPs for ViT-H while improving classification and segmentation results.

Conclusion: Optic ViTs offer a computationally efficient and high-performing alternative to traditional ViTs.

Abstract: Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.

</details>


### [313] [ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning](https://arxiv.org/pdf/2505.15447)
*Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo*

Main category: cs.CV

TL;DR: ViaRL is a rule-based RL framework for optimizing frame selection in video understanding, eliminating costly annotations and improving performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for frame selection rely on costly heuristics or pseudo-labels, limiting scalability.

Method: ViaRL uses rule-based RL to train a frame selector via trial-and-error, aligning with human-like learning.

Result: ViaRL improves temporal grounding by ~15% on Needle QA and generalizes well across benchmarks.

Conclusion: ViaRL offers an effective, scalable solution for intention-driven video understanding.

Abstract: Video understanding is inherently intention-driven-humans naturally focus on
relevant frames based on their goals. Recent advancements in multimodal large
language models (MLLMs) have enabled flexible query-driven reasoning; however,
video-based frameworks like Video Chain-of-Thought lack direct training signals
to effectively identify relevant frames. Current approaches often rely on
heuristic methods or pseudo-label supervised annotations, which are both costly
and limited in scalability across diverse scenarios. To overcome these
challenges, we introduce ViaRL, the first framework to leverage rule-based
reinforcement learning (RL) for optimizing frame selection in intention-driven
video understanding. An iterated amplification strategy is adopted to perform
alternating cyclic training in the video CoT system, where each component
undergoes iterative cycles of refinement to improve its capabilities. ViaRL
utilizes the answer accuracy of a downstream model as a reward signal to train
a frame selector through trial-and-error, eliminating the need for expensive
annotations while closely aligning with human-like learning processes.
Comprehensive experiments across multiple benchmarks, including VideoMME,
LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior
temporal grounding performance and robust generalization across diverse video
understanding tasks, highlighting its effectiveness and scalability. Notably,
ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which
is required to search a specific needle within a long video and regarded as one
of the most suitable benchmarks for evaluating temporal grounding.

</details>


### [314] [Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2505.15450)
*Die Chen, Zhiwen Li, Cen Chen, Yuexiang Xie, Xiaodan Li, Jinyan Ye, Yingda Chen, Yaliang Li*

Main category: cs.CV

TL;DR: A toolkit for evaluating NSFW concept erasure in text-to-image diffusion models is introduced, addressing gaps in current methods and providing practical guidance.


<details>
  <summary>Details</summary>
Motivation: The strong generalization of diffusion models risks generating NSFW content, but existing erasure methods lack comprehensive evaluation.

Method: A full-pipeline toolkit for concept erasure is developed, and a systematic study of NSFW erasure methods is conducted.

Result: Insights and guidance are provided for applying concept erasure effectively in real-world scenarios.

Conclusion: The study advances understanding of content safety in diffusion models and lays groundwork for future research.

Abstract: Text-to-image diffusion models have gained widespread application across
various domains, demonstrating remarkable creative potential. However, the
strong generalization capabilities of diffusion models can inadvertently lead
to the generation of not-safe-for-work (NSFW) content, posing significant risks
to their safe deployment. While several concept erasure methods have been
proposed to mitigate the issue associated with NSFW content, a comprehensive
evaluation of their effectiveness across various scenarios remains absent. To
bridge this gap, we introduce a full-pipeline toolkit specifically designed for
concept erasure and conduct the first systematic study of NSFW concept erasure
methods. By examining the interplay between the underlying mechanisms and
empirical observations, we provide in-depth insights and practical guidance for
the effective application of concept erasure methods in various real-world
scenarios, with the aim of advancing the understanding of content safety in
diffusion models and establishing a solid foundation for future research and
development in this critical area.

</details>


### [315] [Pura: An Efficient Privacy-Preserving Solution for Face Recognition](https://arxiv.org/pdf/2505.15476)
*Guotao Xu, Bowen Zhao, Yang Xiao, Yantao Zhong, Liang Zhai, Qingqi Pei*

Main category: cs.CV

TL;DR: Pura is an efficient privacy-preserving face recognition solution using encrypted data and secure protocols, achieving 16x faster recognition than current methods.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in face recognition while maintaining efficiency, as existing solutions are inadequate.

Method: Proposes a non-interactive architecture using threshold Paillier cryptosystem, secure computing protocols, and parallel computing.

Result: Pura fully protects privacy and achieves recognition speeds 16x faster than state-of-the-art.

Conclusion: Pura is an effective and efficient solution for privacy-preserving face recognition.

Abstract: Face recognition is an effective technology for identifying a target person
by facial images. However, sensitive facial images raises privacy concerns.
Although privacy-preserving face recognition is one of potential solutions,
this solution neither fully addresses the privacy concerns nor is efficient
enough. To this end, we propose an efficient privacy-preserving solution for
face recognition, named Pura, which sufficiently protects facial privacy and
supports face recognition over encrypted data efficiently. Specifically, we
propose a privacy-preserving and non-interactive architecture for face
recognition through the threshold Paillier cryptosystem. Additionally, we
carefully design a suite of underlying secure computing protocols to enable
efficient operations of face recognition over encrypted data directly.
Furthermore, we introduce a parallel computing mechanism to enhance the
performance of the proposed secure computing protocols. Privacy analysis
demonstrates that Pura fully safeguards personal facial privacy. Experimental
evaluations demonstrate that Pura achieves recognition speeds up to 16 times
faster than the state-of-the-art.

</details>


### [316] [Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation](https://arxiv.org/pdf/2505.15491)
*Ce Zhang, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie*

Main category: cs.CV

TL;DR: SGFNet improves RGB-thermal semantic segmentation by categorizing features into low-frequency (context) and high-frequency (details) components, enhancing fusion for better performance.


<details>
  <summary>Details</summary>
Motivation: RGB-only segmentation struggles in low-light and obscured conditions, limiting reliability for critical applications like autonomous driving. Integrating thermal data helps but requires effective fusion.

Method: Proposes Spectral-aware Global Fusion Network (SGFNet) to categorize and fuse multi-modal features into low-frequency (context) and high-frequency (details) components.

Result: SGFNet outperforms state-of-the-art methods on MFNet and PST900 datasets.

Conclusion: SGFNet effectively addresses modality discrepancies by spectral-aware fusion, improving robustness in challenging conditions.

Abstract: Semantic segmentation relying solely on RGB data often struggles in
challenging conditions such as low illumination and obscured views, limiting
its reliability in critical applications like autonomous driving. To address
this, integrating additional thermal radiation data with RGB images
demonstrates enhanced performance and robustness. However, how to effectively
reconcile the modality discrepancies and fuse the RGB and thermal features
remains a well-known challenge. In this work, we address this challenge from a
novel spectral perspective. We observe that the multi-modal features can be
categorized into two spectral components: low-frequency features that provide
broad scene context, including color variations and smooth areas, and
high-frequency features that capture modality-specific details such as edges
and textures. Inspired by this, we propose the Spectral-aware Global Fusion
Network (SGFNet) to effectively enhance and fuse the multi-modal features by
explicitly modeling the interactions between the high-frequency,
modality-specific features. Our experimental results demonstrate that SGFNet
outperforms the state-of-the-art methods on the MFNet and PST900 datasets.

</details>


### [317] [Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification](https://arxiv.org/pdf/2505.15504)
*Conghao Xiong, Zhengrui Guo, Zhe Xu, Yifei Zhang, Raymond Kai-Yu Tong, Si Yong Yeo, Hao Chen, Joseph J. Y. Sung, Irwin King*

Main category: cs.CV

TL;DR: The paper introduces a Squeeze-and-Recalibrate (SR) block to improve few-shot MIL models in computational pathology, reducing overfitting and computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Expert annotations are scarce in computational pathology, and current few-shot MIL methods suffer from overfitting, high computational costs, and preprocessing complexity.

Method: The SR block replaces linear layers in MIL models with low-rank trainable matrices (squeeze pathway) and a frozen recalibration matrix to reduce parameters and preserve feature diversity.

Result: SR-MIL models outperform prior methods with fewer parameters and no architectural changes, supported by theoretical guarantees.

Conclusion: The SR block effectively addresses challenges in few-shot MIL, offering a practical and efficient solution for computational pathology.

Abstract: Deep learning has advanced computational pathology but expert annotations
remain scarce. Few-shot learning mitigates annotation burdens yet suffers from
overfitting and discriminative feature mischaracterization. In addition, the
current few-shot multiple instance learning (MIL) approaches leverage
pretrained vision-language models to alleviate these issues, but at the cost of
complex preprocessing and high computational cost. We propose a
Squeeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in
MIL models to address these challenges. The SR block comprises two core
components: a pair of low-rank trainable matrices (squeeze pathway, SP) that
reduces parameter count and imposes a bottleneck to prevent spurious feature
learning, and a frozen random recalibration matrix that preserves geometric
structure, diversifies feature directions, and redefines the optimization
objective for the SP. We provide theoretical guarantees that the SR block can
approximate any linear mapping to arbitrary precision, thereby ensuring that
the performance of a standard MIL model serves as a lower bound for its
SR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL
models consistently outperform prior methods while requiring significantly
fewer parameters and no architectural changes.

</details>


### [318] [Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts](https://arxiv.org/pdf/2505.15506)
*Debarshi Brahma, Anuska Roy, Soma Biswas*

Main category: cs.CV

TL;DR: The paper proposes PromptMargin, a prompt-tuning method for adapting vision-language models to target datasets with few labeled examples, enhancing performance over zero-shot evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting large pre-trained vision-language models to target datasets with very different distributions and classes using minimal labeled data, avoiding overfitting and loss of generalization.

Method: Introduces PromptMargin, which includes selective augmentation to complement few training samples and a Multimodal Margin Regularizer to improve class discrimination by increasing inter-class margins.

Result: Extensive experiments on fifteen benchmark datasets show PromptMargin outperforms existing state-of-the-art methods in adapting to diverse distribution shifts.

Conclusion: PromptMargin effectively adapts vision-language models to new datasets with minimal labeled data, demonstrating superior performance and robustness.

Abstract: Recently, Vision-Language foundation models like CLIP and ALIGN, which are
pre-trained on large-scale data have shown remarkable zero-shot generalization
to diverse datasets with different classes and even domains. In this work, we
take a step further and analyze whether these models can be adapted to target
datasets having very different distributions and classes compared to what these
models have been trained on, using only a few labeled examples from the target
dataset. In such scenarios, finetuning large pretrained models is challenging
due to problems of overfitting as well as loss of generalization, and has not
been well explored in prior literature. Since, the pre-training data of such
models are unavailable, it is difficult to comprehend the performance on
various downstream datasets. First, we try to answer the question: Given a
target dataset with a few labelled examples, can we estimate whether further
fine-tuning can enhance the performance compared to zero-shot evaluation? by
analyzing the common vision-language embedding space. Based on the analysis, we
propose a novel prompt-tuning method, PromptMargin for adapting such
large-scale VLMs directly on the few target samples. PromptMargin effectively
tunes the text as well as visual prompts for this task, and has two main
modules: 1) Firstly, we use a selective augmentation strategy to complement the
few training samples in each task; 2) Additionally, to ensure robust training
in the presence of unfamiliar class names, we increase the inter-class margin
for improved class discrimination using a novel Multimodal Margin Regularizer.
Extensive experiments and analysis across fifteen target benchmark datasets,
with varying degrees of distribution shifts from natural images, shows the
effectiveness of the proposed framework over the existing state-of-the-art
approaches applied to this setting. github.com/debarshigit/PromptMargin.

</details>


### [319] [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/pdf/2505.15510)
*Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, Libo Qin*

Main category: cs.CV

TL;DR: MCoT improves LVLMs by using visual thoughts, which enhance reasoning regardless of format, with clarity and conciseness being key. Four forms of visual thoughts are analyzed, showing varying effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand how MCoT enhances LVLMs and explore the role of visual thoughts in improving performance and interpretability.

Method: Analyze two MCoT categories (T-MCoT and I-MCoT), define four forms of visual thought expressions, and study their impact on reasoning.

Result: Visual thoughts improve MCoT performance by acting as intermediaries for deeper visual information transmission, with clarity and conciseness being critical.

Conclusion: Visual thoughts are pivotal for MCoT advancements, offering insights for future research in multimodal reasoning.

Abstract: Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.

</details>


### [320] [Detection of Underwater Multi-Targets Based on Self-Supervised Learning and Deformable Path Aggregation Feature Pyramid Network](https://arxiv.org/pdf/2505.15518)
*Chang Liu*

Main category: cs.CV

TL;DR: Proposes a specialized dataset and algorithm for underwater multi-target detection, using self-supervised learning and improved convolution techniques to enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome underwater environment constraints like low contrast, occlusion, and dense target distribution to improve detection accuracy and robustness.

Method: Uses SimSiam-based self-supervised learning for pre-training, introduces deformable and dilated convolution for better feature extraction, and employs EIoU loss function for improved bounding box regression.

Result: The proposed detector improves underwater target detection accuracy by leveraging larger receptive fields and optimized loss calculation.

Conclusion: The method effectively addresses underwater detection challenges, demonstrating improved performance in experiments.

Abstract: To overcome the constraints of the underwater environment and improve the
accuracy and robustness of underwater target detection models, this paper
develops a specialized dataset for underwater target detection and proposes an
efficient algorithm for underwater multi-target detection. A self-supervised
learning based on the SimSiam structure is employed for the pre-training of
underwater target detection network. To address the problems of low detection
accuracy caused by low contrast, mutual occlusion and dense distribution of
underwater targets in underwater object detection, a detection model suitable
for underwater target detection is proposed by introducing deformable
convolution and dilated convolution. The proposed detection model can obtain
more effective information by increasing the receptive field. In addition, the
regression loss function EIoU is introduced, which improves model performance
by separately calculating the width and height losses of the predicted box.
Experiment results show that the accuracy of the underwater target detection
has been improved by the proposed detector.

</details>


### [321] [PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting](https://arxiv.org/pdf/2505.15528)
*Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound*

Main category: cs.CV

TL;DR: PlantDreamer improves synthetic 3D plant generation with enhanced realism and geometric accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current generative 3D models struggle with complex plant generation, limiting usability in plant analysis tools.

Method: Uses a pipeline with depth ControlNet, fine-tuned Low-Rank Adaptation, and Gaussian culling for better textures and geometry. Supports synthetic generation and real-world point cloud enhancement.

Result: Outperforms state-of-the-art text-to-3D models in fidelity and realism.

Conclusion: Advances synthetic plant generation and upgrades legacy datasets, benefiting 3D phenotyping.

Abstract: Recent years have seen substantial improvements in the ability to generate
synthetic 3D objects using AI. However, generating complex 3D objects, such as
plants, remains a considerable challenge. Current generative 3D models struggle
with plant generation compared to general objects, limiting their usability in
plant analysis tools, which require fine detail and accurate geometry. We
introduce PlantDreamer, a novel approach to 3D synthetic plant generation,
which can achieve greater levels of realism for complex plant geometry and
textures than available text-to-3D models. To achieve this, our new generation
pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an
adaptable Gaussian culling algorithm, which directly improve textural realism
and geometric integrity of generated 3D plant models. Additionally,
PlantDreamer enables both purely synthetic plant generation, by leveraging
L-System-generated meshes, and the enhancement of real-world plant point clouds
by converting them into 3D Gaussian Splats. We evaluate our approach by
comparing its outputs with state-of-the-art text-to-3D models, demonstrating
that PlantDreamer outperforms existing methods in producing high-fidelity
synthetic plants. Our results indicate that our approach not only advances
synthetic plant generation, but also facilitates the upgrading of legacy point
cloud datasets, making it a valuable tool for 3D phenotyping applications.

</details>


### [322] [Clapper: Compact Learning and Video Representation in VLMs](https://arxiv.org/pdf/2505.15529)
*Lingyu Kong, Hongzhi Zhang, Jingyuan Zhang, Jianzhao Huang, Kunze Li, Qi Wang, Fuzheng Zhang*

Main category: cs.CV

TL;DR: Clapper improves video-language models by compressing visual tokens efficiently for both short and long videos without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs degrade in performance for long videos when compressing visual tokens, necessitating a better method.

Method: Clapper uses a slow-fast strategy and TimePerceiver module for efficient temporal-spatial encoding.

Result: Achieves 13x compression (61 tokens/frame) with high QA accuracy: 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass.

Conclusion: Clapper effectively balances detail preservation and compression for diverse video lengths, enhancing VLM performance.

Abstract: Current vision-language models (VLMs) have demonstrated remarkable
capabilities across diverse video understanding applications. Designing VLMs
for video inputs requires effectively modeling the temporal dimension (i.e.
capturing dependencies across frames) and balancing the processing of short and
long videos. Specifically, short videos demand preservation of fine-grained
details, whereas long videos require strategic compression of visual
information to handle extensive temporal contexts efficiently. However, our
empirical analysis reveals a critical limitation: most existing VLMs suffer
severe performance degradation in long video understanding tasks when
compressing visual tokens below a quarter of their original visual tokens. To
enable more effective modeling of both short and long video inputs, we propose
Clapper, a method that utilizes a slow-fast strategy for video representation
and introduces a novel module named TimePerceiver for efficient
temporal-spatial encoding within existing VLM backbones. By using our method,
we achieves 13x compression of visual tokens per frame (averaging 61
tokens/frame) without compromising QA accuracy. In our experiments, Clapper
achieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all with
fewer than 6,000 visual tokens per video. The code will be publicly available
on the homepage.

</details>


### [323] [Convolutional Long Short-Term Memory Neural Networks Based Numerical Simulation of Flow Field](https://arxiv.org/pdf/2505.15533)
*Chang Liu*

Main category: cs.CV

TL;DR: An improved ConvLSTM model for flow field prediction outperforms the standard ConvLSTM by better extracting spatiotemporal features with fewer parameters and faster training.


<details>
  <summary>Details</summary>
Motivation: Traditional CFD methods face challenges in convergence, accuracy, and time consumption, prompting the exploration of deep learning alternatives.

Method: The improved ConvLSTM combines residual networks and attention mechanisms, trained on a dataset from numerical simulations of flow around a cylinder.

Result: The enhanced model extracts more spatiotemporal features than the standard ConvLSTM, with reduced parameters and training time.

Conclusion: Deep learning, particularly the improved ConvLSTM, offers a promising alternative to traditional CFD for flow field analysis.

Abstract: Computational Fluid Dynamics (CFD) is the main approach to analyzing flow
field. However, the convergence and accuracy depend largely on mathematical
models of flow, numerical methods, and time consumption. Deep learning-based
analysis of flow filed provides an alternative. For the task of flow field
prediction, an improved Convolutional Long Short-Term Memory (Con-vLSTM) Neural
Network is proposed as the baseline network in consideration of the temporal
and spatial characteristics of flow field. Combining dynamic mesh technology
and User-Defined Function (UDF), numerical simulations of flow around a
circular cylinder were conducted. Flow field snapshots were used to sample data
from the cylinder's wake region at different time instants, constructing a flow
field dataset with sufficient volume and rich flow state var-iations. Residual
networks and attention mechanisms are combined with the standard ConvLSTM
model. Compared with the standard ConvLSTM model, the results demonstrate that
the improved ConvLSTM model can extract more temporal and spatial features
while having fewer parameters and shorter train-ing time.

</details>


### [324] [seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation](https://arxiv.org/pdf/2505.15545)
*Andrew Caunes, Thierry Chateau, Vincent Fremont*

Main category: cs.CV

TL;DR: A novel multi-view projection framework for 3D semantic segmentation improves domain generalization and unsupervised domain adaptation by leveraging synthetic 2D data and occlusion-aware voting.


<details>
  <summary>Details</summary>
Motivation: Addressing domain shift issues in 3D semantic segmentation for autonomous driving and infrastructure analysis.

Method: Aligns Lidar scans into 3D scenes, renders synthetic 2D data (PC2D), trains a 2D segmentation model, and uses occlusion-aware voting for 3D labels.

Result: Achieves state-of-the-art in UDA and close to it in DG, with significant gains on large, static classes.

Conclusion: The modular framework offers robust performance and flexibility, with tools and code made publicly available.

Abstract: 3D semantic segmentation plays a pivotal role in autonomous driving and road
infrastructure analysis, yet state-of-the-art 3D models are prone to severe
domain shift when deployed across different datasets. We propose a novel
multi-view projection framework that excels in both domain generalization (DG)
and unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans
into coherent 3D scenes and renders them from multiple virtual camera poses to
create a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D
segmentation model in-domain. During inference, the model processes hundreds of
views per scene; the resulting logits are back-projected to 3D with an
occlusion-aware voting scheme to generate final point-wise labels. Our
framework is modular and enables extensive exploration of key design
parameters, such as view generation optimization (VGO), visualization modality
optimization (MODO), and 2D model choice. We evaluate on the nuScenes and
SemanticKITTI datasets under both the DG and UDA settings. We achieve
state-of-the-art results in UDA and close to state-of-the-art in DG, with
particularly large gains on large, static classes. Our code and dataset
generation tools will be publicly available at
https://github.com/andrewcaunes/ia4markings

</details>


### [325] [TinyDrive: Multiscale Visual Question Answering with Selective Token Routing for Autonomous Driving](https://arxiv.org/pdf/2505.15564)
*Hossein Hassani, Soodeh Nikan, Abdallah Shami*

Main category: cs.CV

TL;DR: TinyDrive is a lightweight Vision Language Model (VLM) for multi-view VQA in autonomous driving, using a multiscale vision encoder and dual-level prioritization to reduce computational demands while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying resource-intensive VLMs in autonomous driving by creating a lightweight model.

Method: Combines a multiscale vision encoder with token and sequence-level prioritization mechanisms for efficient processing.

Result: Achieves state-of-the-art performance on DriveLM, with 11.1% and 35.4% improvements in BLEU-4 and METEOR scores, respectively.

Conclusion: TinyDrive demonstrates that lightweight VLMs can achieve high performance in VQA for autonomous driving.

Abstract: Vision Language Models (VLMs) employed for visual question-answering (VQA) in
autonomous driving often require substantial computational resources that pose
a challenge for their deployment in resource-constrained vehicles. To address
this challenge, we introduce TinyDrive, a lightweight yet effective VLM for
multi-view VQA in driving scenarios. Our model comprises two key components
including a multiscale vision encoder and a dual-level prioritization mechanism
for tokens and sequences. The multiscale encoder facilitates the processing of
multi-view images at diverse resolutions through scale injection and
cross-scale gating to generate enhanced visual representations. At the token
level, we design a token routing mechanism that dynamically selects and process
the most informative tokens based on learned importance scores. At the sequence
level, we propose integrating normalized loss, uncertainty estimates, and a
diversity metric to formulate sequence scores that rank and preserve samples
within a sequence priority buffer. Samples with higher scores are more
frequently selected for training. TinyDrive is first evaluated on our
custom-curated VQA dataset, and it is subsequently tested on the public DriveLM
benchmark, where it achieves state-of-the-art language understanding
performance. Notably, it achieves relative improvements of 11.1% and 35.4% in
BLEU-4 and METEOR scores, respectively, despite having a significantly smaller
parameter count.

</details>


### [326] [Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.15576)
*Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang*

Main category: cs.CV

TL;DR: AHNPL improves VLMs for compositional reasoning by generating image-based hard negatives and using adaptive contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect image-based negatives and uniform treatment of negatives, limiting VLM performance.

Method: AHNPL generates visual negatives from text-based ones, uses multimodal hard negative loss, and dynamic margin loss.

Result: AHNPL enhances VLM performance on complex CR tasks across three datasets.

Conclusion: AHNPL addresses limitations in negative sample handling, improving VLM performance for compositional reasoning.

Abstract: Vision-Language Models (VLMs) are essential for multimodal tasks, especially
compositional reasoning (CR) tasks, which require distinguishing fine-grained
semantic differences between visual and textual embeddings. However, existing
methods primarily fine-tune the model by generating text-based hard negative
samples, neglecting the importance of image-based negative samples, which
results in insufficient training of the visual encoder and ultimately impacts
the overall performance of the model. Moreover, negative samples are typically
treated uniformly, without considering their difficulty levels, and the
alignment of positive samples is insufficient, which leads to challenges in
aligning difficult sample pairs. To address these issues, we propose Adaptive
Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard
negatives into the visual domain to generate semantically disturbed image-based
negatives for training the model, thereby enhancing its overall performance.
AHNPL also introduces a contrastive learning approach using a multimodal hard
negative loss to improve the model's discrimination of hard negatives within
each modality and a dynamic margin loss that adjusts the contrastive margin
according to sample difficulty to enhance the distinction of challenging sample
pairs. Experiments on three public datasets demonstrate that our method
effectively boosts VLMs' performance on complex CR tasks. The source code is
available at https://github.com/nynu-BDAI/AHNPL.

</details>


### [327] [UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset](https://arxiv.org/pdf/2505.15581)
*Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces UWSAM, an efficient model for underwater instance segmentation, addressing SAM's limitations in underwater tasks by proposing a dataset (UIIS10K) and a knowledge distillation method (MG-UKD).


<details>
  <summary>Details</summary>
Motivation: SAM and its variants underperform in underwater instance segmentation due to lack of domain expertise and high computational demands.

Method: Proposes UIIS10K dataset and UWSAM model, using MG-UKD for knowledge distillation and EUPG for automatic prompt generation.

Result: UWSAM outperforms state-of-the-art methods on underwater instance segmentation tasks.

Conclusion: The proposed approach effectively addresses SAM's limitations in underwater scenarios, offering improved performance and efficiency.

Abstract: With recent breakthroughs in large-scale modeling, the Segment Anything Model
(SAM) has demonstrated significant potential in a variety of visual
applications. However, due to the lack of underwater domain expertise, SAM and
its variants face performance limitations in end-to-end underwater instance
segmentation tasks, while their higher computational requirements further
hinder their application in underwater scenarios. To address this challenge, we
propose a large-scale underwater instance segmentation dataset, UIIS10K, which
includes 10,048 images with pixel-level annotations for 10 categories. Then, we
introduce UWSAM, an efficient model designed for automatic and accurate
segmentation of underwater instances. UWSAM efficiently distills knowledge from
the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the
Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective
visual representation learning. Furthermore, we design an End-to-end Underwater
Prompt Generator (EUPG) for UWSAM, which automatically generates underwater
prompts instead of explicitly providing foreground points or boxes as prompts,
thus enabling the network to locate underwater instances accurately for
efficient segmentation. Comprehensive experimental results show that our model
is effective, achieving significant performance improvements over
state-of-the-art methods on multiple underwater instance datasets. Datasets and
codes are available at https://github.com/LiamLian0727/UIIS10K.

</details>


### [328] [VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation](https://arxiv.org/pdf/2505.15592)
*Niccolo Avogaro, Thomas Frick, Yagmur G. Cinar, Daniel Caraballo, Cezary Skura, Filip M. Janicki, Piotr Kluska, Brown Ebouky, Nicola Farronato, Florian Scheidegger, Cristiano Malossi, Konrad Schindler, Andrea Bartezzaghi, Roy Assaf, Mattia Rigotti*

Main category: cs.CV

TL;DR: VP Lab introduces E-PEFT, a parameter-efficient fine-tuning ensemble, to enhance visual prompting for robust segmentation in specialized domains, achieving a 50% mIoU boost with minimal data.


<details>
  <summary>Details</summary>
Motivation: Large pretrained vision models struggle in specialized domains due to feature mismatches. VP Lab aims to bridge this gap for better segmentation.

Method: VP Lab uses E-PEFT, an ensemble of parameter-efficient fine-tuning techniques, integrated with visual prompting for domain adaptation.

Result: The framework achieves a 50% increase in mIoU for semantic segmentation using only 5 validated images, outperforming state-of-the-art methods.

Conclusion: VP Lab sets a new paradigm for efficient, interactive model deployment in challenging domains, demonstrating significant performance gains.

Abstract: Large-scale pretrained vision backbones have transformed computer vision by
providing powerful feature extractors that enable various downstream tasks,
including training-free approaches like visual prompting for semantic
segmentation. Despite their success in generic scenarios, these models often
fall short when applied to specialized technical domains where the visual
features differ significantly from their training distribution. To bridge this
gap, we introduce VP Lab, a comprehensive iterative framework that enhances
visual prompting for robust segmentation model development. At the core of VP
Lab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques
specifically designed to adapt our visual prompting pipeline to specific
domains in a manner that is both parameter- and data-efficient. Our approach
not only surpasses the state-of-the-art in parameter-efficient fine-tuning for
the Segment Anything Model (SAM), but also facilitates an interactive,
near-real-time loop, allowing users to observe progressively improving results
as they experiment within the framework. By integrating E-PEFT with visual
prompting, we demonstrate a remarkable 50\% increase in semantic segmentation
mIoU performance across various technical datasets using only 5 validated
images, establishing a new paradigm for fast, efficient, and interactive model
deployment in new, challenging domains. This work comes in the form of a
demonstration.

</details>


### [329] [LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models](https://arxiv.org/pdf/2505.15616)
*Ruilin Yao, Bo Zhang, Jirui Huang, Xinwei Long, Yifang Zhang, Tianyu Zou, Yufei Wu, Shichao Su, Yifan Xu, Wenxi Zeng, Zhaoyu Yang, Guoyou Li, Shilan Zhang, Zichan Li, Yaxiong Chen, Shengwu Xiong, Peng Xu, Jiajun Zhang, Bowen Zhou, David Clifton, Luc Van Gool*

Main category: cs.CV

TL;DR: Lens is a multi-level benchmark for evaluating Multimodal Large Language Models (MLLMs) across perception, understanding, and reasoning tasks, using 3.4K images and 60K+ questions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of synergistic effects between perceptual and reasoning capabilities in MLLMs.

Method: Developed Lens, a dataset with rich annotations and diverse tasks, sourced from contemporary social media images.

Result: Tested 15+ MLLMs, none achieved >60% accuracy in reasoning tasks.

Conclusion: Lens highlights limitations in current MLLMs' reasoning abilities and provides a robust evaluation framework.

Abstract: Multimodal Large Language Models (MLLMs) have achieved significant advances
in integrating visual and linguistic information, yet their ability to reason
about complex and real-world scenarios remains limited. The existing benchmarks
are usually constructed in the task-oriented manner without guarantee that
different task samples come from the same data distribution, thus they often
fall short in evaluating the synergistic effects of lower-level perceptual
capabilities on higher-order reasoning. To lift this limitation, we contribute
Lens, a multi-level benchmark with 3.4K contemporary images and 60K+
human-authored questions covering eight tasks and 12 daily scenarios, forming
three progressive task tiers, i.e., perception, understanding, and reasoning.
One feature is that each image is equipped with rich annotations for all tasks.
Thus, this dataset intrinsically supports to evaluate MLLMs to handle
image-invariable prompts, from basic perception to compositional reasoning. In
addition, our images are manully collected from the social media, in which 53%
were published later than Jan. 2025. We evaluate 15+ frontier MLLMs such as
Qwen2.5-VL-72B, InternVL3-78B, GPT-4o and two reasoning models QVQ-72B-preview
and Kimi-VL. These models are released later than Dec. 2024, and none of them
achieve an accuracy greater than 60% in the reasoning tasks. Project page:
https://github.com/Lens4MLLMs/lens. ICCV 2025 workshop page:
https://lens4mllms.github.io/mars2-workshop-iccv2025/

</details>


### [330] [SNAP: A Benchmark for Testing the Effects of Capture Conditions on Fundamental Vision Tasks](https://arxiv.org/pdf/2505.15628)
*Iuliia Kotseruba, John K. Tsotsos*

Main category: cs.CV

TL;DR: The paper investigates how camera settings and lighting affect DL model performance in vision tasks, revealing dataset biases and model susceptibility to capture conditions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of study on how image formation pipelines and environments impact DL model generalization in vision tasks.

Method: Analyze capture bias in datasets, create the SNAP benchmark with controlled conditions, and evaluate DL models on classification, detection, and VQA tasks.

Result: Datasets are biased; models underperform humans even on well-exposed images and are sensitive to camera setting variations.

Conclusion: Capture conditions significantly impact DL model performance, highlighting the need for better dataset diversity and robustness.

Abstract: Generalization of deep-learning-based (DL) computer vision algorithms to
various image perturbations is hard to establish and remains an active area of
research. The majority of past analyses focused on the images already captured,
whereas effects of the image formation pipeline and environment are less
studied. In this paper, we address this issue by analyzing the impact of
capture conditions, such as camera parameters and lighting, on DL model
performance on 3 vision tasks -- image classification, object detection, and
visual question answering (VQA). To this end, we assess capture bias in common
vision datasets and create a new benchmark, SNAP (for $\textbf{S}$hutter speed,
ISO se$\textbf{N}$sitivity, and $\textbf{AP}$erture), consisting of images of
objects taken under controlled lighting conditions and with densely sampled
camera settings. We then evaluate a large number of DL vision models and show
the effects of capture conditions on each selected vision task. Lastly, we
conduct an experiment to establish a human baseline for the VQA task. Our
results show that computer vision datasets are significantly biased, the models
trained on this data do not reach human accuracy even on the well-exposed
images, and are susceptible to both major exposure changes and minute
variations of camera settings. Code and data can be found at
https://github.com/ykotseruba/SNAP

</details>


### [331] [Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking](https://arxiv.org/pdf/2505.15637)
*Pujun Xue, Junyi Ge, Xiaotong Jiang, Siyang Song, Zijian Wu, Yupeng Huo, Weicheng Xie, Linlin Shen, Xiaoqin Zhou, Xiaofeng Liu, Min Gu*

Main category: cs.CV

TL;DR: The paper introduces the OMNI dataset, a large-scale dental image collection for malocclusion diagnosis, validated with various deep learning methods, showing promise for automated diagnostics.


<details>
  <summary>Details</summary>
Motivation: The lack of large, accurately labeled datasets for malocclusion limits automated dental diagnostics, prompting the creation of OMNI to advance research.

Method: The OMNI dataset includes 4166 multi-view images from 384 participants, annotated by dentists, and validated using CNN, Transformer, and GNN-based methods.

Result: Experiments demonstrated OMNI's effectiveness in automated malocclusion diagnosis, providing a new benchmark for research.

Conclusion: OMNI facilitates automated malocclusion research and is publicly available, offering a valuable resource for the field.

Abstract: Malocclusion is a major challenge in orthodontics, and its complex
presentation and diverse clinical manifestations make accurate localization and
diagnosis particularly important. Currently, one of the major shortcomings
facing the field of dental image analysis is the lack of large-scale,
accurately labeled datasets dedicated to malocclusion issues, which limits the
development of automated diagnostics in the field of dentistry and leads to a
lack of diagnostic accuracy and efficiency in clinical practice. Therefore, in
this study, we propose the Oral and Maxillofacial Natural Images (OMNI)
dataset, a novel and comprehensive dental image dataset aimed at advancing the
study of analyzing dental images for issues of malocclusion. Specifically, the
dataset contains 4166 multi-view images with 384 participants in data
collection and annotated by professional dentists. In addition, we performed a
comprehensive validation of the created OMNI dataset, including three CNN-based
methods, two Transformer-based methods, and one GNN-based method, and conducted
automated diagnostic experiments for malocclusion issues. The experimental
results show that the OMNI dataset can facilitate the automated diagnosis
research of malocclusion issues and provide a new benchmark for the research in
this field. Our OMNI dataset and baseline code are publicly available at
https://github.com/RoundFaceJ/OMNI.

</details>


### [332] [FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models](https://arxiv.org/pdf/2505.15644)
*Zhen Sun, Ziyi Zhang, Zeren Luo, Zeyang Sha, Tianshuo Cong, Zheng Li, Shiwen Cui, Weiqiang Wang, Jiaheng Wei, Xinlei He, Qi Li, Qian Wang*

Main category: cs.CV

TL;DR: The paper introduces FragFake, a benchmark dataset for detecting localized image edits, and uses Vision Language Models (VLMs) to improve detection and localization, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in detecting localized image edits, such as lack of localization in binary classifiers, reliance on costly annotations, and absence of large-scale datasets.

Method: Developed an automated data-generation pipeline for FragFake dataset and utilized VLMs for edited image classification and localization.

Result: Fine-tuned VLMs achieved higher Object Precision, outperforming pretrained models. Ablation and transferability analyses validated the approach.

Conclusion: The work reformulates localized image edit detection as a vision-language task, setting a new paradigm and foundation for future research in content authenticity.

Abstract: Fine-grained edited image detection of localized edits in images is crucial
for assessing content authenticity, especially given that modern diffusion
models and image editing methods can produce highly realistic manipulations.
However, this domain faces three challenges: (1) Binary classifiers yield only
a global real-or-fake label without providing localization; (2) Traditional
computer vision methods often rely on costly pixel-level annotations; and (3)
No large-scale, high-quality dataset exists for modern image-editing detection
techniques. To address these gaps, we develop an automated data-generation
pipeline to create FragFake, the first dedicated benchmark dataset for edited
image detection, which includes high-quality images from diverse editing models
and a wide variety of edited objects. Based on FragFake, we utilize Vision
Language Models (VLMs) for the first time in the task of edited image
classification and edited region localization. Experimental results show that
fine-tuned VLMs achieve higher average Object Precision across all datasets,
significantly outperforming pretrained models. We further conduct ablation and
transferability analyses to evaluate the detectors across various
configurations and editing scenarios. To the best of our knowledge, this work
is the first to reformulate localized image edit detection as a vision-language
understanding task, establishing a new paradigm for the field. We anticipate
that this work will establish a solid foundation to facilitate and inspire
subsequent research endeavors in the domain of multimodal content authenticity.

</details>


### [333] [The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for Scene Text Detection](https://arxiv.org/pdf/2505.15649)
*Tianjiao Cao, Jiahao Lyu, Weichao Zeng, Weimin Mu, Yu Zhou*

Main category: cs.CV

TL;DR: The paper identifies the 'Fine-tuning Gap' and long-tailed text distribution as key issues in scene text detection, proposing Joint-Dataset Learning and a Long-Tailed Benchmark (LTB) with MAEDet as a baseline.


<details>
  <summary>Details</summary>
Motivation: Addressing the discrepancy between high academic benchmark performance and real-world failure of scene text detectors.

Method: Proposes Joint-Dataset Learning (JDL) to mitigate the Fine-tuning Gap and introduces MAEDet, a self-supervised learning method, for the Long-Tailed Benchmark (LTB).

Result: Identifies challenges in long-tailed text and provides a benchmark for comprehensive evaluation.

Conclusion: Advocates for JDL and LTB to improve generalization and robustness in scene text detection.

Abstract: Scene text detection has seen the emergence of high-performing methods that
excel on academic benchmarks. However, these detectors often fail to replicate
such success in real-world scenarios. We uncover two key factors contributing
to this discrepancy through extensive experiments. First, a \textit{Fine-tuning
Gap}, where models leverage \textit{Dataset-Specific Optimization} (DSO)
paradigm for one domain at the cost of reduced effectiveness in others, leads
to inflated performances on academic benchmarks. Second, the suboptimal
performance in practical settings is primarily attributed to the long-tailed
distribution of texts, where detectors struggle with rare and complex
categories as artistic or overlapped text. Given that the DSO paradigm might
undermine the generalization ability of models, we advocate for a
\textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning
Gap. Additionally, an error analysis is conducted to identify three major
categories and 13 subcategories of challenges in long-tailed scene text, upon
which we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive
evaluation of ability to handle a diverse range of long-tailed challenges. We
further introduce MAEDet, a self-supervised learning-based method, as a strong
baseline for LTB. The code is available at https://github.com/pd162/LTB.

</details>


### [334] [Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/pdf/2505.15671)
*Hamzeh Asgharnezhad, Afshar Shamsi, Roohallah Alizadehsani, Arash Mohammadi, Hamid Alinejad-Rokny*

Main category: cs.CV

TL;DR: The paper introduces enhanced Monte Carlo Dropout (MCD) frameworks using optimization techniques (GWO, BO, PSO) and an uncertainty-aware loss function to improve uncertainty quantification in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in deep neural networks is crucial for high-stakes applications like medical diagnosis and autonomous systems, but conventional MCD often lacks calibration.

Method: The authors integrate Grey Wolf Optimizer (GWO), Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) with MCD, alongside an uncertainty-aware loss function, and test on datasets like Cats vs. Dogs and Myocarditis using DenseNet121, ResNet50, and VGG16.

Result: The proposed method outperforms MCD baseline by 2-3% in accuracy and uncertainty accuracy, with better calibration.

Conclusion: The enhanced MCD framework improves trustworthiness in safety-critical applications, demonstrating its potential for reliable uncertainty quantification.

Abstract: Knowing the uncertainty associated with the output of a deep neural network
is of paramount importance in making trustworthy decisions, particularly in
high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo
Dropout (MCD) is a widely used method for uncertainty quantification, as it can
be easily integrated into various deep architectures. However, conventional MCD
often struggles with providing well-calibrated uncertainty estimates. To
address this, we introduce innovative frameworks that enhances MCD by
integrating different search solutions namely Grey Wolf Optimizer (GWO),
Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an
uncertainty-aware loss function, thereby improving the reliability of
uncertainty quantification. We conduct comprehensive experiments using
different backbones, namely DenseNet121, ResNet50, and VGG16, on various
datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic
dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%
on average in terms of both conventional accuracy and uncertainty accuracy
while achieving significantly better calibration. These results highlight the
potential of our approach to enhance the trustworthiness of deep learning
models in safety-critical applications.

</details>


### [335] [Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning](https://arxiv.org/pdf/2505.15687)
*Zhe Xu, Cheng Jin, Yihui Wang, Ziyi Liu, Hao Chen*

Main category: cs.CV

TL;DR: A novel bilateral reinforcement learning framework improves reasoning and computational efficiency in multimodal pathological image understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack reasoning capabilities and face computational burdens due to large pathological images.

Method: Introduces a bilateral reinforcement learning framework with two branches: one for reasoning enhancement and another for dynamic token allocation to optimize efficiency.

Result: Achieves +41.7 absolute performance improvement and 70.3% lower inference costs compared to base models.

Conclusion: The framework effectively balances reasoning accuracy and computational efficiency in pathological tasks.

Abstract: Multimodal pathological image understanding has garnered widespread interest
due to its potential to improve diagnostic accuracy and enable personalized
treatment through integrated visual and textual data. However, existing methods
exhibit limited reasoning capabilities, which hamper their ability to handle
complex diagnostic scenarios. Additionally, the enormous size of pathological
images leads to severe computational burdens, further restricting their
practical deployment. To address these limitations, we introduce a novel
bilateral reinforcement learning framework comprising two synergistic branches.
One reinforcement branch enhances the reasoning capability by enabling the
model to learn task-specific decision processes, i.e., pathology rationales,
directly from labels without explicit reasoning supervision. While the other
branch dynamically allocates a tailored number of tokens to different images
based on both their visual content and task context, thereby optimizing
computational efficiency. We apply our method to various pathological tasks
such as visual question answering, cancer subtyping, and lesion detection.
Extensive experiments show an average +41.7 absolute performance improvement
with 70.3% lower inference costs over the base models, achieving both reasoning
accuracy and computational efficiency.

</details>


### [336] [HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning](https://arxiv.org/pdf/2505.15703)
*Xiaodong Mei, Sheng Wang, Jie Cheng, Yingbing Chen, Dan Xu*

Main category: cs.CV

TL;DR: HAMF is a novel motion forecasting framework combining scene context encoding and future motion prediction using attention and Mamba modules, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods degrade scene context features during encoding, limiting motion forecasting accuracy.

Method: HAMF embeds agent states and map info into 1D tokens, uses a unified attention encoder, and a Mamba decoder for consistency.

Result: HAMF achieves state-of-the-art performance on Argoverse 2 benchmark with a lightweight design.

Conclusion: HAMF effectively integrates scene understanding and motion prediction, improving accuracy and diversity in trajectory forecasting.

Abstract: Motion forecasting represents a critical challenge in autonomous driving
systems, requiring accurate prediction of surrounding agents' future
trajectories. While existing approaches predict future motion states with the
extracted scene context feature from historical agent trajectories and road
layouts, they suffer from the information degradation during the scene feature
encoding. To address the limitation, we propose HAMF, a novel motion
forecasting framework that learns future motion representations with the scene
context encoding jointly, to coherently combine the scene understanding and
future motion state prediction. We first embed the observed agent states and
map information into 1D token sequences, together with the target multi-modal
future motion features as a set of learnable tokens. Then we design a unified
Attention-based encoder, which synergistically combines self-attention and
cross-attention mechanisms to model the scene context information and aggregate
future motion features jointly. Complementing the encoder, we implement the
Mamba module in the decoding stage to further preserve the consistency and
correlations among the learned future motion representations, to generate the
accurate and diverse final trajectories. Extensive experiments on Argoverse 2
benchmark demonstrate that our hybrid Attention-Mamba model achieves
state-of-the-art motion forecasting performance with the simple and lightweight
architecture.

</details>


### [337] [RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction](https://arxiv.org/pdf/2505.15737)
*Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: An enhanced Gaussian Splatting framework improves underwater scene reconstruction by addressing color restoration, view consistency, and noise reduction, achieving superior results with a new dataset.


<details>
  <summary>Details</summary>
Motivation: Underwater scene reconstruction is challenging due to light absorption, scattering, and limited visibility. The paper aims to enhance visual quality and geometric accuracy for deep underwater rendering.

Method: Proposes decoupled RGB channel learning guided by underwater physics, frame interpolation with adaptive weighting, and a new loss function for noise reduction and edge preservation. Introduces the Submerged3D dataset.

Result: Outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering better perceptual quality and robustness.

Conclusion: The framework offers promising advancements for marine robotics and underwater visual analytics.

Abstract: Reconstructing high-fidelity underwater scenes remains a challenging task due
to light absorption, scattering, and limited visibility inherent in aquatic
environments. This paper presents an enhanced Gaussian Splatting-based
framework that improves both the visual quality and geometric accuracy of deep
underwater rendering. We propose decoupled learning for RGB channels, guided by
the physics of underwater attenuation, to enable more accurate colour
restoration. To address sparse-view limitations and improve view consistency,
we introduce a frame interpolation strategy with a novel adaptive weighting
scheme. Additionally, we introduce a new loss function aimed at reducing noise
while preserving edges, which is essential for deep-sea content. We also
release a newly collected dataset, Submerged3D, captured specifically in
deep-sea environments. Experimental results demonstrate that our framework
consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,
delivering superior perceptual quality and robustness, and offering promising
directions for marine robotics and underwater visual analytics.

</details>


### [338] [Exploring The Visual Feature Space for Multimodal Neural Decoding](https://arxiv.org/pdf/2505.15755)
*Weihao Xia, Cengiz Oztireli*

Main category: cs.CV

TL;DR: A zero-shot multimodal brain decoding method is introduced to improve precision in neuro-decoding by leveraging MLLMs and a new benchmark (MG-BrainDub) for detailed brain signal interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack detailed interpretations of brain signals, leading to imprecise reconstructions. This work aims to enhance neural decoding precision.

Method: Analyzes vision feature spaces in MLLMs and introduces a zero-shot multimodal brain decoding method. Evaluated using the MG-BrainDub benchmark.

Result: The method improves decoding precision and supports accurate neuro-decoding applications.

Conclusion: The approach advances brain signal decoding by addressing granularity and detail, with potential for broader neuro-decoding applications.

Abstract: The intrication of brain signals drives research that leverages multimodal AI
to align brain modalities with visual and textual data for explainable
descriptions. However, most existing studies are limited to coarse
interpretations, lacking essential details on object descriptions, locations,
attributes, and their relationships. This leads to imprecise and ambiguous
reconstructions when using such cues for visual decoding. To address this, we
analyze different choices of vision feature spaces from pre-trained visual
components within Multimodal Large Language Models (MLLMs) and introduce a
zero-shot multimodal brain decoding method that interacts with these models to
decode across multiple levels of granularities. % To assess a model's ability
to decode fine details from brain signals, we propose the Multi-Granularity
Brain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two
key tasks: detailed descriptions and salient question-answering, with metrics
highlighting key visual elements like objects, attributes, and relationships.
Our approach enhances neural decoding precision and supports more accurate
neuro-decoding applications. Code will be available at
https://github.com/weihaox/VINDEX.

</details>


### [339] [Constructing a 3D Town from a Single Image](https://arxiv.org/pdf/2505.15765)
*Kaizhi Zheng, Ruijian Zhang, Jing Gu, Jie Yang, Xin Eric Wang*

Main category: cs.CV

TL;DR: 3DTown is a training-free framework for generating realistic 3D scenes from a single top-down image, improving alignment, coherence, and quality without 3D supervision.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models struggle with full-scene generation, leading to inconsistencies and low-quality outputs. A lightweight, single-image solution is needed for practical applications.

Method: Decomposes input into overlapping regions, uses a pretrained 3D object generator, and applies spatial-aware inpainting for coherence and quality.

Result: Outperforms state-of-the-art methods (Trellis, Hunyuan3D-2, TripoSG) in geometry quality, coherence, and texture fidelity.

Conclusion: High-quality 3D scene generation from a single image is feasible with a principled, training-free approach.

Abstract: Acquiring detailed 3D scenes typically demands costly equipment, multi-view
data, or labor-intensive modeling. Therefore, a lightweight alternative,
generating complex 3D scenes from a single top-down image, plays an essential
role in real-world applications. While recent 3D generative models have
achieved remarkable results at the object level, their extension to full-scene
generation often leads to inconsistent geometry, layout hallucinations, and
low-quality meshes. In this work, we introduce 3DTown, a training-free
framework designed to synthesize realistic and coherent 3D scenes from a single
top-down view. Our method is grounded in two principles: region-based
generation to improve image-to-3D alignment and resolution, and spatial-aware
3D inpainting to ensure global scene coherence and high-quality geometry
generation. Specifically, we decompose the input image into overlapping regions
and generate each using a pretrained 3D object generator, followed by a masked
rectified flow inpainting process that fills in missing geometry while
maintaining structural continuity. This modular design allows us to overcome
resolution bottlenecks and preserve spatial structure without requiring 3D
supervision or fine-tuning. Extensive experiments across diverse scenes show
that 3DTown outperforms state-of-the-art baselines, including Trellis,
Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and
texture fidelity. Our results demonstrate that high-quality 3D town generation
is achievable from a single image using a principled, training-free approach.

</details>


### [340] [IA-T2I: Internet-Augmented Text-to-Image Generation](https://arxiv.org/pdf/2505.15779)
*Chuanhao Li, Jianwen Sun, Yukang Feng, Mingliang Zhai, Yifan Chang, Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces an Internet-Augmented text-to-image (IA-T2I) framework to address uncertain knowledge in prompts by using reference images, outperforming GPT-4o by 30% in human evaluation.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with uncertain knowledge in prompts, such as future events or ambiguous details, limiting their practical use.

Method: The framework includes an active retrieval module, hierarchical image selection, and self-reflection to refine outputs using reference images.

Result: The IA-T2I framework significantly improves performance, especially on the Img-Ref-T2I dataset with uncertain prompts.

Conclusion: The proposed framework effectively enhances T2I models by leveraging reference images, demonstrating superior performance over existing methods.

Abstract: Current text-to-image (T2I) generation models achieve promising results, but
they fail on the scenarios where the knowledge implied in the text prompt is
uncertain. For example, a T2I model released in February would struggle to
generate a suitable poster for a movie premiering in April, because the
character designs and styles are uncertain to the model. To solve this problem,
we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to
compel T2I models clear about such uncertain knowledge by providing them with
reference images. Specifically, an active retrieval module is designed to
determine whether a reference image is needed based on the given text prompt; a
hierarchical image selection module is introduced to find the most suitable
image returned by an image search engine to enhance the T2I model; a
self-reflection mechanism is presented to continuously evaluate and refine the
generated image to ensure faithful alignment with the text prompt. To evaluate
the proposed framework's performance, we collect a dataset named Img-Ref-T2I,
where text prompts include three types of uncertain knowledge: (1) known but
rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt
to guide GPT-4o in making preference evaluation, which has been shown to have
an evaluation accuracy similar to that of human preference evaluation.
Experimental results demonstrate the effectiveness of our framework,
outperforming GPT-4o by about 30% in human evaluation.

</details>


### [341] [VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL](https://arxiv.org/pdf/2505.15791)
*Fengyuan Dai, Zifeng Zhuang, Yufei Huang, Siteng Huang, Bangyan Liao, Donglin Wang, Fajie Yuan*

Main category: cs.CV

TL;DR: VARD introduces a value-based reinforcement learning method for fine-tuning diffusion models, addressing challenges like sparse rewards and non-differentiability by using dense supervision and KL regularization.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for fine-tuning diffusion models struggle with stability, efficiency, and non-differentiable rewards, leading to suboptimal generation quality.

Method: VARD learns a value function to predict rewards from intermediate states and uses it with KL regularization for dense supervision during generation.

Result: The method improves trajectory guidance, training efficiency, and works with complex, non-differentiable rewards.

Conclusion: VARD enhances RL applicability for diffusion models, maintaining proximity to pre-trained models while enabling stable and effective training.

Abstract: Diffusion models have emerged as powerful generative tools across various
domains, yet tailoring pre-trained models to exhibit specific desirable
properties remains challenging. While reinforcement learning (RL) offers a
promising solution,current methods struggle to simultaneously achieve stable,
efficient fine-tuning and support non-differentiable rewards. Furthermore,
their reliance on sparse rewards provides inadequate supervision during
intermediate steps, often resulting in suboptimal generation quality. To
address these limitations, dense and differentiable signals are required
throughout the diffusion process. Hence, we propose VAlue-based Reinforced
Diffusion (VARD): a novel approach that first learns a value function
predicting expection of rewards from intermediate states, and subsequently uses
this value function with KL regularization to provide dense supervision
throughout the generation process. Our method maintains proximity to the
pretrained model while enabling effective and stable training via
backpropagation. Experimental results demonstrate that our approach facilitates
better trajectory guidance, improves training efficiency and extends the
applicability of RL to diffusion models optimized for complex,
non-differentiable reward functions.

</details>


### [342] [Interspatial Attention for Efficient 4D Human Video Generation](https://arxiv.org/pdf/2505.15800)
*Ruizhi Shao, Yinghao Xu, Yujun Shen, Ceyuan Yang, Yang Zheng, Changan Chen, Yebin Liu, Gordon Wetzstein*

Main category: cs.CV

TL;DR: A new interspatial attention (ISA) mechanism improves digital human video generation, offering better quality, consistency, and control.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating digital human videos lack quality, consistency, or identity preservation.

Method: Introduces ISA, a scalable cross-attention mechanism for diffusion transformer models, trained with a video variation autoencoder.

Result: Achieves state-of-the-art 4D human video synthesis with superior motion consistency, identity preservation, and pose control.

Conclusion: ISA enhances digital human video generation, setting a new benchmark for quality and controllability.

Abstract: Generating photorealistic videos of digital humans in a controllable manner
is crucial for a plethora of applications. Existing approaches either build on
methods that employ template-based 3D representations or emerging video
generation models but suffer from poor quality or limited consistency and
identity preservation when generating individual or multiple digital humans. In
this paper, we introduce a new interspatial attention (ISA) mechanism as a
scalable building block for modern diffusion transformer (DiT)--based video
generation models. ISA is a new type of cross attention that uses relative
positional encodings tailored for the generation of human videos. Leveraging a
custom-developed video variation autoencoder, we train a latent ISA-based
diffusion model on a large corpus of video data. Our model achieves
state-of-the-art performance for 4D human video synthesis, demonstrating
remarkable motion consistency and identity preservation while providing precise
control of the camera and body poses. Our code and model are publicly released
at https://dsaurus.github.io/isa4d/.

</details>


### [343] [STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs](https://arxiv.org/pdf/2505.15804)
*Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang*

Main category: cs.CV

TL;DR: STAR-R1, a novel RL framework, addresses spatial reasoning gaps in MLLMs by integrating fine-grained rewards, outperforming SFT by 23% in cross-view tasks.


<details>
  <summary>Details</summary>
Motivation: MLLMs lag behind humans in spatial reasoning, especially in tasks like Transformation-Driven Visual Reasoning (TVR).

Method: Proposes STAR-R1, combining single-stage RL with fine-grained rewards for efficient exploration and precise reasoning.

Result: Achieves state-of-the-art performance across 11 metrics, with a 23% improvement over SFT in cross-view scenarios.

Conclusion: STAR-R1 advances MLLM research by enhancing spatial reasoning and exhibiting anthropomorphic behavior.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across diverse tasks, yet they lag significantly behind humans in
spatial reasoning. We investigate this gap through Transformation-Driven Visual
Reasoning (TVR), a challenging task requiring identification of object
transformations across images under varying viewpoints. While traditional
Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in
cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from
inefficient exploration and slow convergence. To address these limitations, we
propose STAR-R1, a novel framework that integrates a single-stage RL paradigm
with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1
rewards partial correctness while penalizing excessive enumeration and passive
inaction, enabling efficient exploration and precise reasoning. Comprehensive
evaluations demonstrate that STAR-R1 achieves state-of-the-art performance
across all 11 metrics, outperforming SFT by 23% in cross-view scenarios.
Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its
unique ability to compare all objects for improving spatial reasoning. Our work
provides critical insights in advancing the research of MLLMs and reasoning
models. The codes, model weights, and data will be publicly available at
https://github.com/zongzhao23/STAR-R1.

</details>


### [344] [MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/pdf/2505.15809)
*Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang*

Main category: cs.CV

TL;DR: MMaDA is a multimodal diffusion model with a unified architecture, mixed CoT fine-tuning, and UniGRPO RL algorithm, excelling in textual reasoning, multimodal understanding, and text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between pretraining and post-training in unified diffusion architectures and enhance performance across diverse multimodal tasks.

Method: Uses a unified diffusion architecture, mixed CoT fine-tuning, and UniGRPO RL algorithm for seamless multimodal integration and improved task handling.

Result: Outperforms models like LLaMA-3-7B, Qwen2-7B, Show-o, SEED-X, SDXL, and Janus in respective tasks.

Conclusion: MMaDA provides a comprehensive framework for future research, demonstrating strong generalization and effectiveness in multimodal tasks.

Abstract: We introduce MMaDA, a novel class of multimodal diffusion foundation models
designed to achieve superior performance across diverse domains such as textual
reasoning, multimodal understanding, and text-to-image generation. The approach
is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion
architecture with a shared probabilistic formulation and a modality-agnostic
design, eliminating the need for modality-specific components. This
architecture ensures seamless integration and processing across different data
types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning
strategy that curates a unified CoT format across modalities. By aligning
reasoning processes between textual and visual domains, this strategy
facilitates cold-start training for the final reinforcement learning (RL)
stage, thereby enhancing the model's ability to handle complex tasks from the
outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm
specifically tailored for diffusion foundation models. Utilizing diversified
reward modeling, UniGRPO unifies post-training across both reasoning and
generation tasks, ensuring consistent performance improvements. Experimental
results demonstrate that MMaDA-8B exhibits strong generalization capabilities
as a unified multimodal foundation model. It surpasses powerful models like
LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in
multimodal understanding, and excels over SDXL and Janus in text-to-image
generation. These achievements highlight MMaDA's effectiveness in bridging the
gap between pretraining and post-training within unified diffusion
architectures, providing a comprehensive framework for future research and
development. We open-source our code and trained models at:
https://github.com/Gen-Verse/MMaDA

</details>


### [345] [Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization](https://arxiv.org/pdf/2505.15812)
*Satoshi Kosugi*

Main category: cs.CV

TL;DR: A novel fine-tuning-free method for exemplar-based image colorization using a pre-trained diffusion model's self-attention module for semantic matching and dual attention-guided color transfer.


<details>
  <summary>Details</summary>
Motivation: To achieve accurate semantic matching between grayscale and reference color images without fine-tuning, leveraging the powerful attention capabilities of pre-trained diffusion models.

Method: Utilizes dual attention-guided color transfer and classifier-free colorization guidance. The self-attention module computes attention maps for semantic alignment, transferring color features from the reference to the input image.

Result: Outperforms existing techniques with an FID of 95.27 (image quality) and SI-FID of 5.51 (fidelity to reference) on 335 input-reference pairs.

Conclusion: The proposed method effectively enhances colorization quality and fidelity to reference images without fine-tuning, demonstrating superior performance.

Abstract: Exemplar-based image colorization aims to colorize a grayscale image using a
reference color image, ensuring that reference colors are applied to
corresponding input regions based on their semantic similarity. To achieve
accurate semantic matching between regions, we leverage the self-attention
module of a pre-trained diffusion model, which is trained on a large dataset
and exhibits powerful attention capabilities. To harness this power, we propose
a novel, fine-tuning-free approach based on a pre-trained diffusion model,
making two key contributions. First, we introduce dual attention-guided color
transfer. We utilize the self-attention module to compute an attention map
between the input and reference images, effectively capturing semantic
correspondences. The color features from the reference image is then
transferred to the semantically matching regions of the input image, guided by
this attention map, and finally, the grayscale features are replaced with the
corresponding color features. Notably, we utilize dual attention to calculate
attention maps separately for the grayscale and color images, achieving more
precise semantic alignment. Second, we propose classifier-free colorization
guidance, which enhances the transferred colors by combining color-transferred
and non-color-transferred outputs. This process improves the quality of
colorization. Our experimental results demonstrate that our method outperforms
existing techniques in terms of image quality and fidelity to the reference.
Specifically, we use 335 input-reference pairs from previous research,
achieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to
the reference). Our source code is available at
https://github.com/satoshi-kosugi/powerful-attention.

</details>


### [346] [A Taxonomy of Structure from Motion Methods](https://arxiv.org/pdf/2505.15814)
*Federica Arrigoni*

Main category: cs.CV

TL;DR: A review of Structure from Motion (SfM) methods, categorized by focus on motion or structure, with insights into open problems and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a conceptual review and taxonomy of SfM methods, highlighting theoretical conditions and practical implications.

Method: Groups SfM methods into three categories based on their focus (motion, structure, or both) and analyzes their theoretical foundations.

Result: A new taxonomy for SfM approaches, identifying open problems and potential research directions.

Conclusion: The review offers a fresh perspective on SfM, emphasizing theoretical conditions and future opportunities in the field.

Abstract: Structure from Motion (SfM) refers to the problem of recovering both
structure (i.e., 3D coordinates of points in the scene) and motion (i.e.,
camera matrices) starting from point correspondences in multiple images. It has
attracted significant attention over the years, counting practical
reconstruction pipelines as well as theoretical results. This paper is
conceived as a conceptual review of SfM methods, which are grouped into three
main categories, according to which part of the problem - between motion and
structure - they focus on. The proposed taxonomy brings a new perspective on
existing SfM approaches as well as insights into open problems and possible
future research directions. Particular emphasis is given on identifying the
theoretical conditions that make SfM well posed, which depend on the problem
formulation that is being considered.

</details>


### [347] [Streamline Without Sacrifice -- Squeeze out Computation Redundancy in LMM](https://arxiv.org/pdf/2505.15816)
*Penghao Wu, Lewei Lu, Ziwei Liu*

Main category: cs.CV

TL;DR: ProxyV reduces computational redundancy in large multimodal models by using proxy vision tokens, improving efficiency without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiency in processing vision tokens in decoder-only LMMs without losing information.

Method: Identifies computation-level redundancy and introduces ProxyV, a method using proxy tokens to lighten processing.

Result: ProxyV enhances efficiency and sometimes performance, and works well with token reduction methods.

Conclusion: ProxyV is a flexible, efficient solution for reducing vision token computation in multimodal models.

Abstract: Large multimodal models excel in multimodal tasks but face significant
computational challenges due to excessive computation on visual tokens. Unlike
token reduction methods that focus on token-level redundancy, we identify and
study the computation-level redundancy on vision tokens to ensure no
information loss. Our key insight is that vision tokens from the pretrained
vision encoder do not necessarily require all the heavy operations (e.g.,
self-attention, FFNs) in decoder-only LMMs and could be processed more lightly
with proper designs. We designed a series of experiments to discover and
progressively squeeze out the vision-related computation redundancy. Based on
our findings, we propose ProxyV, a novel approach that utilizes proxy vision
tokens to alleviate the computational burden on original vision tokens. ProxyV
enhances efficiency without compromising performance and can even yield notable
performance gains in scenarios with more moderate efficiency improvements.
Furthermore, the flexibility of ProxyV is demonstrated through its combination
with token reduction methods to boost efficiency further. The code will be made
public at this https://github.com/penghao-wu/ProxyV URL.

</details>


### [348] [InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition](https://arxiv.org/pdf/2505.15818)
*Yijie Zheng, Weijie Wu, Qingyun Li, Xuehui Wang, Xu Zhou, Aiai Ren, Jun Shen, Long Zhao, Guoqing Li, Xue Yang*

Main category: cs.CV

TL;DR: The paper introduces InstructCDS tasks and EarthInstruct benchmark for language-guided object recognition in remote sensing, proposing InstructSAM, a training-free framework that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack the ability to handle complex or implicit queries in remote sensing imagery, necessitating advanced reasoning for object recognition.

Method: Proposes InstructSAM, leveraging vision-language models for instruction interpretation, SAM2 for mask proposals, and binary integer programming for mask-label assignment.

Result: InstructSAM matches or surpasses baselines, reduces output tokens by 89%, and runtime by 32%, maintaining constant inference time.

Conclusion: The tasks, benchmark, and framework advance versatile object recognition systems in remote sensing.

Abstract: Language-Guided object recognition in remote sensing imagery is crucial for
large-scale mapping and automated data annotation. However, existing
open-vocabulary and visual grounding methods rely on explicit category cues,
limiting their ability to handle complex or implicit queries that require
advanced reasoning. To address this issue, we introduce a new suite of tasks,
including Instruction-Oriented Object Counting, Detection, and Segmentation
(InstructCDS), covering open-vocabulary, open-ended, and open-subclass
scenarios. We further present EarthInstruct, the first InstructCDS benchmark
for earth observation. It is constructed from two diverse remote sensing
datasets with varying spatial resolutions and annotation rules across 20
categories, necessitating models to interpret dataset-specific instructions.
Given the scarcity of semantically rich labeled data in remote sensing, we
propose InstructSAM, a training-free framework for instruction-driven object
recognition. InstructSAM leverages large vision-language models to interpret
user instructions and estimate object counts, employs SAM2 for mask proposal,
and formulates mask-label assignment as a binary integer programming problem.
By integrating semantic similarity with counting constraints, InstructSAM
efficiently assigns categories to predicted masks without relying on confidence
thresholds. Experiments demonstrate that InstructSAM matches or surpasses
specialized baselines across multiple tasks while maintaining near-constant
inference time regardless of object count, reducing output tokens by 89% and
overall runtime by over 32% compared to direct generation approaches. We
believe the contributions of the proposed tasks, benchmark, and effective
approach will advance future research in developing versatile object
recognition systems.

</details>


### [349] [An Information Theory-inspired Strategy for Automatic Network Pruning](https://arxiv.org/pdf/2108.08532)
*Xiawu Zheng, Yuexiao Ma, Teng Xi, Gang Zhang, Errui Ding, Yuchao Li, Jie Chen, Yonghong Tian, Rongrong Ji*

Main category: cs.CV

TL;DR: The paper proposes an automatic model compression method inspired by information theory, using the normalized Hilbert-Schmidt Independence Criterion (nHSIC) to guide layer importance and transforming the problem into a solvable convex optimization task.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks face compression challenges on resource-constrained devices, and existing pruning methods are labor-intensive, computationally expensive, and lack theoretical guidance.

Method: The method uses nHSIC on network activations as a layer importance indicator, formulates the architecture search as a linear programming problem with quadratic constraints, and solves it via convex optimization.

Result: The method achieves a 45.3% FLOPs reduction with ResNet-50 while maintaining 75.75 top-1 accuracy on ImageNet, outperforming state-of-the-art compression algorithms.

Conclusion: The proposed information theory-based approach provides an efficient, automatic, and theoretically grounded solution for model compression, suitable for diverse device constraints.

Abstract: Despite superior performance on many computer vision tasks, deep convolution
neural networks are well known to be compressed on devices that have resource
constraints. Most existing network pruning methods require laborious human
efforts and prohibitive computation resources, especially when the constraints
are changed. This practically limits the application of model compression when
the model needs to be deployed on a wide range of devices. Besides, existing
methods are still challenged by the missing theoretical guidance. In this paper
we propose an information theory-inspired strategy for automatic model
compression. The principle behind our method is the information bottleneck
theory, i.e., the hidden representation should compress information with each
other. We thus introduce the normalized Hilbert-Schmidt Independence Criterion
(nHSIC) on network activations as a stable and generalized indicator of layer
importance. When a certain resource constraint is given, we integrate the HSIC
indicator with the constraint to transform the architecture search problem into
a linear programming problem with quadratic constraints. Such a problem is
easily solved by a convex optimization method with a few seconds. We also
provide a rigorous proof to reveal that optimizing the normalized HSIC
simultaneously minimizes the mutual information between different layers.
Without any search process, our method achieves better compression tradeoffs
comparing to the state-of-the-art compression algorithms. For instance, with
ResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on
ImageNet. Codes are avaliable at
https://github.com/MAC-AutoML/ITPruner/tree/master.

</details>


### [350] [Learning Task-preferred Inference Routes for Gradient De-conflict in Multi-output DNNs](https://arxiv.org/pdf/2305.19844)
*Yi Sun, Xin Xu, Jian Li, Xiaochang Hu, Yifei Shi, Ling-Li Zeng*

Main category: cs.CV

TL;DR: DR-MGF is a gradient de-conflict algorithm for multi-output neural networks (MONs) that learns task-preferred inference routes to reduce inter-task interference, improving accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: Shared filters in MONs cause gradient interference due to inconsistent optimization objectives, degrading performance.

Method: DR-MGF uses learnable task-specific importance variables to evaluate filter importance, dynamically adjusting task dominance over filters to form preferred routes.

Result: DR-MGF outperforms existing methods in accuracy and convergence on datasets like CIFAR, ImageNet, and NYUv2.

Conclusion: DR-MGF effectively reduces interference in MONs without structural changes, offering a scalable solution.

Abstract: Multi-output deep neural networks(MONs) contain multiple task branches, and
these tasks usually share partial network filters that lead to the entanglement
of different task inference routes. Due to the inconsistent optimization
objectives, the task gradients used for training MONs will interfere with each
other on the shared routes, which will decrease the overall model performance.
To address this issue, we propose a novel gradient de-conflict algorithm named
DR-MGF(Dynamic Routes and Meta-weighted Gradient Fusion) in this work.
Different from existing de-conflict methods, DR-MGF achieves gradient
de-conflict in MONs by learning task-preferred inference routes. The proposed
method is motivated by our experimental findings: the shared filters are not
equally important to different tasks. By designing the learnable task-specific
importance variables, DR-MGF evaluates the importance of filters for different
tasks. Through making the dominances of tasks over filters be proportional to
the task-specific importance of filters, DR-MGF can effectively reduce the
inter-task interference. The task-specific importance variables ultimately
determine task-preferred inference routes at the end of training iterations.
Extensive experimental results on CIFAR, ImageNet, and NYUv2 illustrate that
DR-MGF outperforms the existing de-conflict methods both in prediction accuracy
and convergence speed of MONs. Furthermore, DR-MGF can be extended to general
MONs without modifying the overall network structures.

</details>


### [351] [Augmenting Chest X-ray Datasets with Non-Expert Annotations](https://arxiv.org/pdf/2309.02244)
*Veronika Cheplygina, Cathrine Damgaard, Trine Naja Eriksen, Dovile Juodelyte, Amelia Jiménez-Sánchez*

Main category: cs.CV

TL;DR: The paper introduces NEATX, a dataset of non-expert annotations for tubes in chest X-rays, showing good agreement with expert labels and raising awareness about annotation quality.


<details>
  <summary>Details</summary>
Motivation: To address biases in automated annotation and high costs of expert labeling in medical image analysis by leveraging non-expert annotations for shortcuts like tubes.

Method: Enhance public chest X-ray datasets (NIH-CXR14, PadChest) with non-expert tube annotations, train a detector, and compare with expert labels.

Result: Non-expert annotations show 'moderate' to 'almost perfect' agreement with experts, and the trained detector generalizes well.

Conclusion: Non-expert annotations can be a viable alternative for certain tasks, and the NEATX dataset is released to promote awareness of annotation quality.

Abstract: The advancement of machine learning algorithms in medical image analysis
requires the expansion of training datasets. A popular and cost-effective
approach is automated annotation extraction from free-text medical reports,
primarily due to the high costs associated with expert clinicians annotating
medical images, such as chest X-rays. However, it has been shown that the
resulting datasets are susceptible to biases and shortcuts. Another strategy to
increase the size of a dataset is crowdsourcing, a widely adopted practice in
general computer vision with some success in medical image analysis. In a
similar vein to crowdsourcing, we enhance two publicly available chest X-ray
datasets by incorporating non-expert annotations. However, instead of using
diagnostic labels, we annotate shortcuts in the form of tubes. We collect 3.5k
chest drain annotations for NIH-CXR14, and 1k annotations for four different
tube types in PadChest, and create the Non-Expert Annotations of Tubes in
X-rays (NEATX) dataset. We train a chest drain detector with the non-expert
annotations that generalizes well to expert labels. Moreover, we compare our
annotations to those provided by experts and show "moderate" to "almost
perfect" agreement. Finally, we present a pathology agreement study to raise
awareness about the quality of ground truth annotations. We make our dataset
available on Zenodo at https://zenodo.org/records/14944064 and our code
available at https://github.com/purrlab/chestxr-label-reliability.

</details>


### [352] [Enhanced Textual Feature Extraction for Visual Question Answering: A Simple Convolutional Approach](https://arxiv.org/pdf/2405.00479)
*Zhilin Zhang, Fangyu Wu*

Main category: cs.CV

TL;DR: The paper compares textual encoders in VQA, finding complex models not always optimal. It proposes ConvGRU, a lightweight model improving accuracy for certain question types.


<details>
  <summary>Details</summary>
Motivation: Limited research on textual encoders' comparative effectiveness in VQA, especially regarding complexity and efficiency.

Method: Comprehensive comparison of complex vs. simple textual models; proposes ConvGRU with convolutional layers for better text representation.

Result: ConvGRU shows modest but consistent improvement on VQA-v2, especially for Number and Count questions.

Conclusion: Lightweight architectures like ConvGRU are viable for VQA, particularly under resource constraints.

Abstract: Visual Question Answering (VQA) has emerged as a highly engaging field in
recent years, with increasing research focused on enhancing VQA accuracy
through advanced models such as Transformers. Despite this growing interest,
limited work has examined the comparative effectiveness of textual encoders in
VQA, particularly considering model complexity and computational efficiency. In
this work, we conduct a comprehensive comparison between complex textual models
that leverage long-range dependencies and simpler models focusing on local
textual features within a well-established VQA framework. Our findings reveal
that employing complex textual encoders is not always the optimal approach for
the VQA-v2 dataset. Motivated by this insight, we propose ConvGRU, a model that
incorporates convolutional layers to improve text feature representation
without substantially increasing model complexity. Tested on the VQA-v2
dataset, ConvGRU demonstrates a modest yet consistent improvement over
baselines for question types such as Number and Count, which highlights the
potential of lightweight architectures for VQA tasks, especially when
computational resources are limited.

</details>


### [353] [A re-calibration method for object detection with multi-modal alignment bias in autonomous driving](https://arxiv.org/pdf/2405.16848)
*Zhihang Song, Dingyi Yao, Ruibo MIng, Lihui Peng, Jianming Hu, Danya Yao, Yi Zhang*

Main category: cs.CV

TL;DR: The paper explores the impact of calibration bias in multi-modal object detection for autonomous driving and proposes a re-calibration model to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume precise sensor calibration, but real-world factors like vibration and data lags introduce bias, which is understudied.

Method: Experiments on EPNet++ show calibration bias harms performance. A re-calibration model using semantic segmentation is proposed.

Result: Slight calibration bias significantly reduces detection performance. The proposed model enhances robustness.

Conclusion: The re-calibration model improves multi-modal detection performance under calibration bias, addressing a critical gap in autonomous driving.

Abstract: Multi-modal object detection in autonomous driving has achieved great
breakthroughs due to the usage of fusing complementary information from
different sensors. The calibration in fusion between sensors such as LiDAR and
camera is always supposed to be precise in previous work. However, in reality,
calibration matrices are fixed when the vehicles leave the factory, but
vibration, bumps, and data lags may cause calibration bias. As the research on
the calibration influence on fusion detection performance is relatively few,
flexible calibration dependency multi-sensor detection method has always been
attractive. In this paper, we conducted experiments on SOTA detection method
EPNet++ and proved slight bias on calibration can reduce the performance
seriously. We also proposed a re-calibration model based on semantic
segmentation which can be combined with a detection algorithm to improve the
performance and robustness of multi-modal calibration bias.

</details>


### [354] [SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/pdf/2406.12030)
*Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, Jing Shao*

Main category: cs.CV

TL;DR: The paper introduces SPA-VL, a large-scale dataset for safety alignment in Vision Language Models (VLMs), addressing the lack of high-quality datasets in this area.


<details>
  <summary>Details</summary>
Motivation: The complexity of combining textual and visual semantics in VLMs makes safety alignment challenging, and existing datasets are limited.

Method: SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, with 100,788 samples. Responses are collected from 12 VLMs, and preference data is automated.

Result: Models trained with alignment techniques on SPA-VL show significant improvements in harmlessness and helpfulness without losing core capabilities.

Conclusion: SPA-VL is a milestone for ensuring VLMs are both harmless and helpful, offering a large-scale, diverse, and high-quality dataset.

Abstract: The emergence of Vision Language Models (VLMs) has brought unprecedented
advances in understanding multimodal information. The combination of textual
and visual semantics in VLMs is highly complex and diverse, making the safety
alignment of these models challenging. Furthermore, due to the limited study on
the safety alignment of VLMs, there is a lack of large-scale, high-quality
datasets. To address these limitations, we propose a Safety Preference
Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth,
SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and
contains 100,788 samples of the quadruple (question, image, chosen response,
rejected response). In terms of depth, the responses are collected from 12
open-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure
diversity. The construction of preference data is fully automated, and the
experimental results indicate that models trained with alignment techniques on
the SPA-VL dataset exhibit substantial improvements in harmlessness and
helpfulness while maintaining core capabilities. SPA-VL, as a large-scale,
high-quality, and diverse dataset, represents a significant milestone in
ensuring that VLMs achieve both harmlessness and helpfulness.

</details>


### [355] [Boosting Few-Shot Open-Set Object Detection via Prompt Learning and Robust Decision Boundary](https://arxiv.org/pdf/2406.18443)
*Zhaowei Wu, Binyi Su, Qichuan Geng, Hua Zhang, Zhong Zhou*

Main category: cs.CV

TL;DR: A prompt-based framework for few-shot open-set object detection (FOOD) improves unknown class detection by leveraging textual info and robust boundary construction.


<details>
  <summary>Details</summary>
Motivation: Existing FOOD methods struggle with limited visual info and unclear boundaries between known/unknown classes.

Method: Uses pseudo-unknown samples (AGPM), decouples knowledge (CED), and calibrates distributions (ADC) for robust unknown rejection.

Result: Improves unknown class recall by 7.24% (VOC10-5-5) and 1.38% (VOC-COCO).

Conclusion: The framework outperforms state-of-the-art methods in few-shot open-set detection.

Abstract: Few-shot Open-set Object Detection (FOOD) poses a challenge in many
open-world scenarios. It aims to train an open-set detector to detect known
objects while rejecting unknowns with scarce training samples. Existing FOOD
methods are subject to limited visual information, and often exhibit an
ambiguous decision boundary between known and unknown classes. To address these
limitations, we propose the first prompt-based few-shot open-set object
detection framework, which exploits additional textual information and delves
into constructing a robust decision boundary for unknown rejection.
Specifically, as no available training data for unknown classes, we select
pseudo-unknown samples with Attribution-Gradient based Pseudo-unknown Mining
(AGPM), which leverages the discrepancy in attribution gradients to quantify
uncertainty. Subsequently, we propose Conditional Evidence Decoupling (CED) to
decouple and extract distinct knowledge from selected pseudo-unknown samples by
eliminating opposing evidence. This optimization process can enhance the
discrimination between known and unknown classes. To further regularize the
model and form a robust decision boundary for unknown rejection, we introduce
Abnormal Distribution Calibration (ADC) to calibrate the output probability
distribution of local abnormal features in pseudo-unknown samples. Our method
achieves superior performance over previous state-of-the-art approaches,
improving the average recall of unknown class by 7.24% across all shots in
VOC10-5-5 dataset settings and 1.38% in VOC-COCO dataset settings. Our source
code is available at https://gitee.com/VR_NAVE/ced-food.

</details>


### [356] [Enrich the content of the image Using Context-Aware Copy Paste](https://arxiv.org/pdf/2407.08151)
*Qiushi Guo*

Main category: cs.CV

TL;DR: A context-aware Copy-Paste method using BLIP, SAM, and YOLO improves data augmentation by ensuring contextual relevance and automation.


<details>
  <summary>Details</summary>
Motivation: Existing Copy-Paste methods lack contextual relevance, leading to inconsistencies in generated outputs.

Method: Integrates BLIP for content extraction, matches it with category info, and uses SAM and YOLO for cohesive object integration.

Result: Enhances data diversity and generates high-quality pseudo-images across computer vision tasks.

Conclusion: The proposed method automates data augmentation, ensuring contextual consistency and eliminating manual annotation.

Abstract: Data augmentation remains a widely utilized technique in deep learning,
particularly in tasks such as image classification, semantic segmentation, and
object detection. Among them, Copy-Paste is a simple yet effective method and
gain great attention recently. However, existing Copy-Paste often overlook
contextual relevance between source and target images, resulting in
inconsistencies in generated outputs. To address this challenge, we propose a
context-aware approach that integrates Bidirectional Latent Information
Propagation (BLIP) for content extraction from source images. By matching
extracted content information with category information, our method ensures
cohesive integration of target objects using Segment Anything Model (SAM) and
You Only Look Once (YOLO). This approach eliminates the need for manual
annotation, offering an automated and user-friendly solution. Experimental
evaluations across diverse datasets demonstrate the effectiveness of our method
in enhancing data diversity and generating high-quality pseudo-images across
various computer vision tasks.

</details>


### [357] [Local Clustering for Lung Cancer Image Classification via Sparse Solution Technique](https://arxiv.org/pdf/2407.08800)
*Jackson Hamel, Ming-Jun Lai, Zhaiming Shen, Ye Tian*

Main category: cs.CV

TL;DR: The paper proposes a local clustering method using sparse solutions for lung cancer image classification, outperforming traditional techniques like spectral clustering. It also uses wavelet-framelet methods for image preprocessing and suggests image deformation for data augmentation.


<details>
  <summary>Details</summary>
Motivation: To improve medical image classification, especially for lung cancer, by leveraging graph clustering and sparse solutions for better efficiency and accuracy.

Method: Uses a weighted graph model for images, applies sparse solution-based local clustering, and employs wavelet-framelet methods for image cleaning.

Result: The method is more efficient and effective than state-of-the-art approaches, with improved classification performance.

Conclusion: The proposed approach is highly effective for medical image classification, with potential for further enhancement through image deformation techniques.

Abstract: In this work, we propose to use a local clustering approach based on the
sparse solution technique to study the medical image, especially the lung
cancer image classification task. We view images as the vertices in a weighted
graph and the similarity between a pair of images as the edges in the graph.
The vertices within the same cluster can be assumed to share similar features
and properties, thus making the applications of graph clustering techniques
very useful for image classification. Recently, the approach based on the
sparse solutions of linear systems for graph clustering has been found to
identify clusters more efficiently than traditional clustering methods such as
spectral clustering. We propose to use the two newly developed local clustering
methods based on sparse solution of linear system for image classification. In
addition, we employ a box spline-based tight-wavelet-framelet method to clean
these images and help build a better adjacency matrix before clustering. The
performance of our methods is shown to be very effective in classifying images.
Our approach is significantly more efficient and either favorable or equally
effective compared with other state-of-the-art approaches. Finally, we shall
make a remark by pointing out two image deformation methods to build up more
artificial image data to increase the number of labeled images.

</details>


### [358] [P3P: Pseudo-3D Pre-training for Scaling 3D Voxel-based Masked Autoencoders](https://arxiv.org/pdf/2408.10007)
*Xuechao Chen, Ying Chen, Jialin Li, Qiang Nie, Hanqiu Deng, Yong Liu, Qixing Huang, Yang Li*

Main category: cs.CV

TL;DR: A novel self-supervised 3D pre-training framework using depth estimation to scale data, with a linear-time tokenizer for efficient embedding, achieving SOTA in 3D tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing data scaling challenges in 3D pre-training due to limited clean and complete 3D data.

Method: Leverages a large depth estimation model to incorporate images into 3D pre-training, introducing a linear-time tokenizer for flexible token embedding and a new 3D reconstruction target.

Result: Achieves state-of-the-art performance in 3D classification, few-shot learning, and segmentation.

Conclusion: The proposed framework effectively scales 3D pre-training data and improves model performance across tasks.

Abstract: 3D pre-training is crucial to 3D perception tasks. Nevertheless, limited by
the difficulties in collecting clean and complete 3D data, 3D pre-training has
persistently faced data scaling challenges. In this work, we introduce a novel
self-supervised pre-training framework that incorporates millions of images
into 3D pre-training corpora by leveraging a large depth estimation model. New
pre-training corpora encounter new challenges in representation ability and
embedding efficiency of models. Previous pre-training methods rely on farthest
point sampling and k-nearest neighbors to embed a fixed number of 3D tokens.
However, these approaches prove inadequate when it comes to embedding millions
of samples that feature a diverse range of point numbers, spanning from 1,000
to 100,000. In contrast, we propose a tokenizer with linear-time complexity,
which enables the efficient embedding of a flexible number of tokens.
Accordingly, a new 3D reconstruction target is proposed to cooperate with our
3D tokenizer. Our method achieves state-of-the-art performance in 3D
classification, few-shot learning, and 3D segmentation. Code is available at
https://github.com/XuechaoChen/P3P-MAE.

</details>


### [359] [SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP](https://arxiv.org/pdf/2408.10202)
*Yusuke Hirota, Min-Hung Chen, Chien-Yi Wang, Yuta Nakashima, Yu-Chiang Frank Wang, Ryo Hachiuma*

Main category: cs.CV

TL;DR: The paper introduces SANER, a method to debias CLIP by neutralizing societal attributes in text features without needing attribute annotations.


<details>
  <summary>Details</summary>
Motivation: Address societal bias in CLIP, overcoming limitations of prior methods like loss of attribute information and reliance on annotations.

Method: SANER neutralizes attribute information in CLIP text features for attribute-neutral descriptions, preserving original info for attribute-specific ones.

Result: SANER outperforms existing methods in debiasing without requiring attribute annotations.

Conclusion: SANER is a simple yet effective solution for mitigating societal bias in CLIP.

Abstract: Large-scale vision-language models, such as CLIP, are known to contain
societal bias regarding protected attributes (e.g., gender, age). This paper
aims to address the problems of societal bias in CLIP. Although previous
studies have proposed to debias societal bias through adversarial learning or
test-time projecting, our comprehensive study of these works identifies two
critical limitations: 1) loss of attribute information when it is explicitly
disclosed in the input and 2) use of the attribute annotations during debiasing
process. To mitigate societal bias in CLIP and overcome these limitations
simultaneously, we introduce a simple-yet-effective debiasing method called
SANER (societal attribute neutralizer) that eliminates attribute information
from CLIP text features only of attribute-neutral descriptions. Experimental
results show that SANER, which does not require attribute annotations and
preserves original information for attribute-specific descriptions,
demonstrates superior debiasing ability than the existing methods.

</details>


### [360] [Diversity-Driven View Subset Selection for Indoor Novel View Synthesis](https://arxiv.org/pdf/2409.07098)
*Zehao Wang, Han Zhou, Matthew B. Blaschko, Tinne Tuytelaars, Minye Wu*

Main category: cs.CV

TL;DR: A novel framework for efficient view subset selection in indoor scene synthesis, reducing data redundancy and outperforming baselines with minimal data usage.


<details>
  <summary>Details</summary>
Motivation: Redundant information from artificial movements in monocular video sequences reduces efficiency in scene modeling for novel view synthesis.

Method: Proposes a combinatorial optimization task for view subset selection, integrating diversity-based measurements and utility functions.

Result: Outperforms baseline strategies using only 5-20% of the data, validated on the IndoorTraj dataset.

Conclusion: The framework is efficient and effective for indoor novel view synthesis, with code publicly available.

Abstract: Novel view synthesis of indoor scenes can be achieved by capturing a
monocular video sequence of the environment. However, redundant information
caused by artificial movements in the input video data reduces the efficiency
of scene modeling. To address this, we formulate the problem as a combinatorial
optimization task for view subset selection. In this work, we propose a novel
subset selection framework that integrates a comprehensive diversity-based
measurement with well-designed utility functions. We provide a theoretical
analysis of these utility functions and validate their effectiveness through
extensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset
designed for indoor novel view synthesis, featuring complex and extended
trajectories that simulate intricate human behaviors. Experiments on IndoorTraj
show that our framework consistently outperforms baseline strategies while
using only 5-20% of the data, highlighting its remarkable efficiency and
effectiveness. The code is available at:
https://github.com/zehao-wang/IndoorTraj

</details>


### [361] [MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving](https://arxiv.org/pdf/2409.07267)
*Enming Zhang, Xingyuan Dai, Min Huang, Yisheng Lv, Qinghai Miao*

Main category: cs.CV

TL;DR: MiniDrive is a lightweight vision-language model for autonomous driving, addressing inefficiencies and multi-image processing limitations of existing VLMs with its FE-MoE and DI-Adapter modules.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs are computationally expensive and struggle with multi-camera perception, hindering real-world deployment in autonomous driving.

Method: Proposes MiniDrive with FE-MoE for efficient 2D feature mapping and DI-Adapter for dynamic visual token embeddings.

Result: Achieves state-of-the-art performance with 83M parameters, improved efficiency, and response speed.

Conclusion: MiniDrive offers a practical solution for real-time autonomous driving applications by balancing performance and computational cost.

Abstract: Vision-language models (VLMs) serve as general-purpose end-to-end models in
autonomous driving, performing subtasks such as prediction, planning, and
perception through question-and-answer interactions. However, most existing
methods rely on computationally expensive visual encoders and large language
models (LLMs), making them difficult to deploy in real-world scenarios and
real-time applications. Meanwhile, most existing VLMs lack the ability to
process multiple images, making it difficult to adapt to multi-camera
perception in autonomous driving. To address these issues, we propose a novel
framework called MiniDrive, which incorporates our proposed Feature Engineering
Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter
(DI-Adapter). The FE-MoE effectively maps 2D features into visual token
embeddings before being input into the language model. The DI-Adapter enables
the visual token embeddings to dynamically change with the instruction text
embeddings, resolving the issue of static visual token embeddings for the same
image in previous approaches. Compared to previous works, MiniDrive achieves
state-of-the-art performance in terms of parameter size, floating point
operations, and response efficiency, with the smallest version containing only
83M parameters.

</details>


### [362] [FaVoR: Features via Voxel Rendering for Camera Relocalization](https://arxiv.org/pdf/2409.07571)
*Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly*

Main category: cs.CV

TL;DR: A novel camera relocalization method using a sparse voxel map for robust feature matching under viewpoint and appearance changes, outperforming state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Feature-based relocalization struggles with viewpoint and appearance changes, leading to inaccurate poses. The goal is to enhance robustness.

Method: Leverages a sparse 3D voxel map to render descriptors for unseen views, combining volumetric rendering and feature matching for pose estimation.

Result: Achieves up to 39% improvement in median translation error on indoor datasets and comparable outdoor results with lower costs.

Conclusion: The proposed method significantly improves robustness and accuracy in relocalization, especially in challenging indoor environments.

Abstract: Camera relocalization methods range from dense image alignment to direct
camera pose regression from a query image. Among these, sparse feature matching
stands out as an efficient, versatile, and generally lightweight approach with
numerous applications. However, feature-based methods often struggle with
significant viewpoint and appearance changes, leading to matching failures and
inaccurate pose estimates. To overcome this limitation, we propose a novel
approach that leverages a globally sparse yet locally dense 3D representation
of 2D features. By tracking and triangulating landmarks over a sequence of
frames, we construct a sparse voxel map optimized to render image patch
descriptors observed during tracking. Given an initial pose estimate, we first
synthesize descriptors from the voxels using volumetric rendering and then
perform feature matching to estimate the camera pose. This methodology enables
the generation of descriptors for unseen views, enhancing robustness to view
changes. We extensively evaluate our method on the 7-Scenes and Cambridge
Landmarks datasets. Our results show that our method significantly outperforms
existing state-of-the-art feature representation techniques in indoor
environments, achieving up to a 39% improvement in median translation error.
Additionally, our approach yields comparable results to other methods for
outdoor scenarios while maintaining lower memory and computational costs.

</details>


### [363] [SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba](https://arxiv.org/pdf/2409.12108)
*Xiangning Zhang, Qingwei Zhang, Jinnan Chen, Chengfeng Zhou, Yaqi Wang, Zhengjie Zhang, Xiaobo Li, Dahong Qian*

Main category: cs.CV

TL;DR: SPRMamba, a novel framework integrating Mamba-based architecture and Scaled Residual TranMamba block, enhances real-time surgical phase recognition in ESD, achieving 87.64% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing video-based phase recognition algorithms struggle with fine-grained phase transitions and long-range dependencies due to inefficient temporal context modeling.

Method: Proposes SPRMamba, combining Mamba-based architecture with SRTM block for long-term temporal modeling and localized detail extraction, plus Hierarchical Sampling Strategy for efficiency.

Result: Achieves 87.64% accuracy on ESD385 dataset (+1.0% over prior methods) and robust generalizability on Cholec80.

Conclusion: SPRMamba bridges computational efficiency and temporal sensitivity, offering a transformative tool for ESD intraoperative guidance and skill assessment.

Abstract: Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedure
initially developed for early gastric cancer treatment and has expanded to
address diverse gastrointestinal lesions. While computer-assisted surgery (CAS)
systems enhance ESD precision and safety, their efficacy hinges on accurate
real-time surgical phase recognition, a task complicated by ESD's inherent
complexity, including heterogeneous lesion characteristics and dynamic tissue
interactions. Existing video-based phase recognition algorithms, constrained by
inefficient temporal context modeling, exhibit limited performance in capturing
fine-grained phase transitions and long-range dependencies. To overcome these
limitations, we propose SPRMamba, a novel framework integrating a Mamba-based
architecture with a Scaled Residual TranMamba (SRTM) block to synergize
long-term temporal modeling and localized detail extraction. SPRMamba further
introduces the Hierarchical Sampling Strategy to optimize computational
efficiency, enabling real-time processing critical for clinical deployment.
Evaluated on the ESD385 dataset and the cholecystectomy benchmark Cholec80,
SPRMamba achieves state-of-the-art performance (87.64% accuracy on ESD385,
+1.0% over prior methods), demonstrating robust generalizability across
surgical workflows. This advancement bridges the gap between computational
efficiency and temporal sensitivity, offering a transformative tool for
intraoperative guidance and skill assessment in ESD surgery. The code is
accessible at https://github.com/Zxnyyyyy/SPRMamba.

</details>


### [364] [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/pdf/2409.15477)
*Mohammad Shahab Sepehri, Zalan Fabian, Maryam Soltanolkotabi, Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: MediConfusion is a new benchmark dataset for medical VQA that exposes severe failure modes in medical MLLMs, showing their unreliability for healthcare.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of the limitations and vulnerabilities of medical MLLMs, especially in vision-related tasks.

Method: Introduces MediConfusion, a challenging medical VQA dataset, to test and reveal failure modes of medical MLLMs from a vision perspective.

Result: State-of-the-art models perform below random guessing on MediConfusion, highlighting their unreliability. Common failure patterns are identified.

Conclusion: Existing medical MLLMs are unreliable for healthcare deployment, and the study provides insights for designing more trustworthy models.

Abstract: Multimodal Large Language Models (MLLMs) have tremendous potential to improve
the accuracy, availability, and cost-effectiveness of healthcare by providing
automated solutions or serving as aids to medical professionals. Despite
promising first steps in developing medical MLLMs in the past few years, their
capabilities and limitations are not well-understood. Recently, many benchmark
datasets have been proposed that test the general medical knowledge of such
models across a variety of medical areas. However, the systematic failure modes
and vulnerabilities of such models are severely underexplored with most medical
benchmarks failing to expose the shortcomings of existing models in this
safety-critical domain. In this paper, we introduce MediConfusion, a
challenging medical Visual Question Answering (VQA) benchmark dataset, that
probes the failure modes of medical MLLMs from a vision perspective. We reveal
that state-of-the-art models are easily confused by image pairs that are
otherwise visually dissimilar and clearly distinct for medical experts.
Strikingly, all available models (open-source or proprietary) achieve
performance below random guessing on MediConfusion, raising serious concerns
about the reliability of existing medical MLLMs for healthcare deployment. We
also extract common patterns of model failure that may help the design of a new
generation of more trustworthy and reliable MLLMs in healthcare.

</details>


### [365] [Symmetry-Robust 3D Orientation Estimation](https://arxiv.org/pdf/2410.02101)
*Christopher Scarvelis, David Benhaim, Paul Zhang*

Main category: cs.CV

TL;DR: A two-stage orientation pipeline for 3D shape analysis achieves state-of-the-art performance in estimating orientation axes (side-, up-, front-axes) and addresses challenges for rotationally-symmetric shapes.


<details>
  <summary>Details</summary>
Motivation: The need for a reliable method to estimate complete orientations of general 3D shapes, overcoming limitations of previous approaches restricted to subsets of classes.

Method: A two-stage orientation pipeline trained and evaluated on the entire Shapenet dataset, addressing rotational symmetry challenges through theoretical insights.

Result: State-of-the-art performance in up-axis estimation and effective full-orientation estimation for all three axes.

Conclusion: The proposed pipeline advances orientation estimation by handling general shapes and rotational symmetry, validated on a comprehensive dataset.

Abstract: Orientation estimation is a fundamental task in 3D shape analysis which
consists of estimating a shape's orientation axes: its side-, up-, and
front-axes. Using this data, one can rotate a shape into canonical orientation,
where its orientation axes are aligned with the coordinate axes. Developing an
orientation algorithm that reliably estimates complete orientations of general
shapes remains an open problem. We introduce a two-stage orientation pipeline
that achieves state of the art performance on up-axis estimation and further
demonstrate its efficacy on full-orientation estimation, where one seeks all
three orientation axes. Unlike previous work, we train and evaluate our method
on all of Shapenet rather than a subset of classes. We motivate our engineering
contributions by theory describing fundamental obstacles to orientation
estimation for rotationally-symmetric shapes, and show how our method avoids
these obstacles.

</details>


### [366] [SeMv-3D: Towards Concurrency of Semantic and Multi-view Consistency in General Text-to-3D Generation](https://arxiv.org/pdf/2410.07658)
*Xiao Cai, Pengpeng Zeng, Lianli Gao, Sitong Su, Heng Tao Shen, Jingkuan Song*

Main category: cs.CV

TL;DR: SeMv-3D improves text-to-3D generation by jointly enhancing semantic alignment and multi-view consistency using Triplane Prior Learning and Prior-based Semantic Aligning in Triplanes.


<details>
  <summary>Details</summary>
Motivation: Addressing the dual challenges of semantic consistency and multi-view consistency in text-to-3D generation, which existing methods tackle separately.

Method: Introduces Triplane Prior Learning (TPL) for geometric consistency and Prior-based Semantic Aligning in Triplanes (SAT) for semantic alignment.

Result: Achieves state-of-the-art multi-view consistency while maintaining competitive semantic consistency.

Conclusion: SeMv-3D sets a new benchmark by balancing and excelling in both semantic and multi-view consistency.

Abstract: General Text-to-3D (GT23D) generation is crucial for creating diverse 3D
content across objects and scenes, yet it faces two key challenges: 1) ensuring
semantic consistency between input text and generated 3D models, and 2)
maintaining multi-view consistency across different perspectives within 3D.
Existing approaches typically address only one of these challenges, often
leading to suboptimal results in semantic fidelity and structural coherence. To
overcome these limitations, we propose SeMv-3D, a novel framework that jointly
enhances semantic alignment and multi-view consistency in GT23D generation. At
its core, we introduce Triplane Prior Learning (TPL), which effectively learns
triplane priors by capturing spatial correspondences across three orthogonal
planes using a dedicated Orthogonal Attention mechanism, thereby ensuring
geometric consistency across viewpoints. Additionally, we present Prior-based
Semantic Aligning in Triplanes (SAT), which enables consistent any-view
synthesis by leveraging attention-based feature alignment to reinforce the
correspondence between textual semantics and triplane representations.
Extensive experiments demonstrate that our method sets a new state-of-the-art
in multi-view consistency, while maintaining competitive performance in
semantic consistency compared to methods focused solely on semantic alignment.
These results emphasize the remarkable ability of our approach to effectively
balance and excel in both dimensions, establishing a new benchmark in the
field.

</details>


### [367] [Efficient Partitioning Vision Transformer on Edge Devices for Distributed Inference](https://arxiv.org/pdf/2410.11650)
*Xiang Liu, Yijun Song, Xia Li, Yifei Sun, Huiying Lan, Zemin Liu, Linshan Jiang, Jialin Li*

Main category: cs.CV

TL;DR: Proposes ED-ViT, a framework to efficiently deploy Vision Transformers on edge devices by splitting models and using class-wise pruning, reducing latency and size while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high computational demands, making deployment on resource-constrained edge devices challenging.

Method: Partitions Vision Transformers into sub-models for specific data classes and applies class-wise pruning to reduce overhead.

Result: Reduces model size by up to 28.9x and latency by 34.1x while keeping accuracy comparable to original models.

Conclusion: ED-ViT effectively addresses deployment challenges for Vision Transformers on edge devices, outperforming CNN and SNN methods.

Abstract: Deep learning models are increasingly utilized on resource-constrained edge
devices for real-time data analytics. Recently, Vision Transformer and their
variants have shown exceptional performance in various computer vision tasks.
However, their substantial computational requirements and low inference latency
create significant challenges for deploying such models on resource-constrained
edge devices. To address this issue, we propose a novel framework, ED-ViT,
which is designed to efficiently split and execute complex Vision Transformers
across multiple edge devices. Our approach involves partitioning Vision
Transformer models into several sub-models, while each dedicated to handling a
specific subset of data classes. To further reduce computational overhead and
inference latency, we introduce a class-wise pruning technique that decreases
the size of each sub-model. Through extensive experiments conducted on five
datasets using three model architectures and actual implementation on edge
devices, we demonstrate that our method significantly cuts down inference
latency on edge devices and achieves a reduction in model size by up to 28.9
times and 34.1 times, respectively, while maintaining test accuracy comparable
to the original Vision Transformer. Additionally, we compare ED-ViT with two
state-of-the-art methods that deploy CNN and SNN models on edge devices,
evaluating metrics such as accuracy, inference time, and overall model size.
Our comprehensive evaluation underscores the effectiveness of the proposed
ED-ViT framework.

</details>


### [368] [MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2410.13370)
*Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, Pheng-Ann Heng*

Main category: cs.CV

TL;DR: MagicTailor improves fine-grained control in text-to-image diffusion models by addressing semantic pollution and imbalance, enabling personalized component customization.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models lack fine-grained control over visual concepts, limiting creativity. The paper aims to enable customizable component reconfiguration.

Method: Proposes MagicTailor with Dynamic Masked Degradation for perturbing unwanted semantics and Dual-Stream Balancing for balanced learning.

Result: MagicTailor outperforms in component-controllable personalization, enabling more creative image generation.

Conclusion: MagicTailor effectively addresses challenges in component customization, enhancing creativity in text-to-image models.

Abstract: Text-to-image diffusion models can generate high-quality images but lack
fine-grained control of visual concepts, limiting their creativity. Thus, we
introduce component-controllable personalization, a new task that enables users
to customize and reconfigure individual components within concepts. This task
faces two challenges: semantic pollution, where undesired elements disrupt the
target concept, and semantic imbalance, which causes disproportionate learning
of the target concept and component. To address these, we design MagicTailor, a
framework that uses Dynamic Masked Degradation to adaptively perturb unwanted
visual semantics and Dual-Stream Balancing for more balanced learning of
desired visual semantics. The experimental results show that MagicTailor
achieves superior performance in this task and enables more personalized and
creative image generation.

</details>


### [369] [M3TR: A Generalist Model for Real-World HD Map Completion](https://arxiv.org/pdf/2411.10316)
*Fabian Immel, Richard Fehler, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller*

Main category: cs.CV

TL;DR: M3TR is a generalist model for HD map completion, handling all map changes and outperforming specialized models. It improves performance with or without offline HD map priors.


<details>
  <summary>Details</summary>
Motivation: Offline HD maps become outdated, and existing methods specialize in single map changes, limiting real-world deployment.

Method: M3TR uses multi-masking and transformer architecture for HD map completion, with optimized query designs and prior utilization.

Result: M3TR achieves +4.3 mAP improvement over existing methods and is deployable with offline HD map priors.

Conclusion: M3TR is a versatile and effective solution for HD map completion, addressing real-world challenges.

Abstract: Autonomous vehicles rely on HD maps for their operation, but offline HD maps
eventually become outdated. For this reason, online HD map construction methods
use live sensor data to infer map information instead. Research on real map
changes shows that oftentimes entire parts of an HD map remain unchanged and
can be used as a prior. We therefore introduce M3TR (Multi-Masking Map
Transformer), a generalist approach for HD map completion both with and without
offline HD map priors. As a necessary foundation, we address shortcomings in
ground truth labels for Argoverse 2 and nuScenes and propose the first
comprehensive benchmark for HD map completion. Unlike existing models that
specialize in a single kind of map change, which is unrealistic for deployment,
our Generalist model handles all kinds of changes, matching the effectiveness
of Expert models. With our map masking as augmentation regime, we can even
achieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing
prior HD map elements and optimizing query designs, M3TR outperforms existing
methods by +4.3 mAP while being the first real-world deployable model for
offline HD map priors. Code is available at https://github.com/immel-f/m3tr

</details>


### [370] [Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing](https://arxiv.org/pdf/2411.16375)
*Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, Long Chen*

Main category: cs.CV

TL;DR: Ca2-VDM improves autoregressive video diffusion models by introducing causal generation and cache sharing to eliminate redundancy and boost efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive video diffusion models (VDMs) are inefficient due to redundant computations of overlapping conditional frames, especially with long-term context.

Method: Proposes Ca2-VDM, featuring unidirectional feature computation (causal generation) and shared cache across denoising steps to reduce redundancy and storage costs.

Result: Ca2-VDM achieves state-of-the-art video generation quality and significantly speeds up generation.

Conclusion: The proposed method efficiently addresses redundancy in autoregressive VDMs, enhancing performance and practicality.

Abstract: With the advance of diffusion models, today's video generation has achieved
impressive quality. To extend the generation length and facilitate real-world
applications, a majority of video diffusion models (VDMs) generate videos in an
autoregressive manner, i.e., generating subsequent clips conditioned on the
last frame(s) of the previous clip. However, existing autoregressive VDMs are
highly inefficient and redundant: The model must re-compute all the conditional
frames that are overlapped between adjacent clips. This issue is exacerbated
when the conditional frames are extended autoregressively to provide the model
with long-term context. In such cases, the computational demands increase
significantly (i.e., with a quadratic complexity w.r.t. the autoregression
step). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with
Causal generation and Cache sharing. For causal generation, it introduces
unidirectional feature computation, which ensures that the cache of conditional
frames can be precomputed in previous autoregression steps and reused in every
subsequent step, eliminating redundant computations. For cache sharing, it
shares the cache across all denoising steps to avoid the huge cache storage
cost. Extensive experiments demonstrated that our Ca2-VDM achieves
state-of-the-art quantitative and qualitative video generation results and
significantly improves the generation speed. Code is available:
https://github.com/Dawn-LX/CausalCache-VDM

</details>


### [371] [Opt-In Art: Learning Art Styles Only from Few Examples](https://arxiv.org/pdf/2412.00176)
*Hui Ren, Joanna Materzynska, Rohit Gandikota, David Bau, Antonio Torralba*

Main category: cs.CV

TL;DR: A text-to-image model trained only on photographs can adapt to artistic styles with few examples, matching state-of-the-art models trained on artistic datasets.


<details>
  <summary>Details</summary>
Motivation: To determine if pre-training on artistic datasets is necessary for learning artistic styles with minimal examples.

Method: Train a text-to-image model exclusively on photographs, then adapt it to artistic styles using few examples. Evaluate via user studies and automatic metrics.

Result: The adapted model performs comparably to models trained on large artistic datasets. Data attribution shows artistic outputs can emerge without prior artistic exposure.

Conclusion: Artistic style generation is achievable without pre-training on artistic data, using only a small, curated set of examples.

Abstract: We explore whether pre-training on datasets with paintings is necessary for a
model to learn an artistic style with only a few examples. To investigate this,
we train a text-to-image model exclusively on photographs, without access to
any painting-related content. We show that it is possible to adapt a model that
is trained without paintings to an artistic style, given only few examples.
User studies and automatic evaluations confirm that our model (post-adaptation)
performs on par with state-of-the-art models trained on massive datasets that
contain artistic content like paintings, drawings or illustrations. Finally,
using data attribution techniques, we analyze how both artistic and
non-artistic datasets contribute to generating artistic-style images.
Surprisingly, our findings suggest that high-quality artistic outputs can be
achieved without prior exposure to artistic data, indicating that artistic
style generation can occur in a controlled, opt-in manner using only a limited,
carefully selected set of training examples.

</details>


### [372] [Volumetrically Consistent 3D Gaussian Rasterization](https://arxiv.org/pdf/2412.03378)
*Chinmay Talegaonkar, Yash Belhe, Ravi Ramamoorthi, Nicholas Antipa*

Main category: cs.CV

TL;DR: The paper introduces a method to improve 3D Gaussian Splatting (3DGS) by volumetrically integrating 3D Gaussians for more accurate transmittance and alpha values, enhancing physical accuracy while maintaining speed.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS approximations reduce physical accuracy in rendering. The authors aim to address this by deriving more accurate alpha values through volumetric integration.

Method: The method volumetrically integrates 3D Gaussians to compute transmittance analytically, improving accuracy within the 3DGS framework.

Result: The approach outperforms 3DGS in view synthesis (SSIM, LPIPS) and matches state-of-the-art tomography results with fewer points.

Conclusion: The method enhances physical accuracy and performance of 3DGS, making it suitable for both view synthesis and tomography.

Abstract: Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view
synthesis at high inference speeds. However, its splatting-based rendering
model makes several approximations to the rendering equation, reducing physical
accuracy. We show that the core approximations in splatting are unnecessary,
even within a rasterizer; We instead volumetrically integrate 3D Gaussians
directly to compute the transmittance across them analytically. We use this
analytic transmittance to derive more physically-accurate alpha values than
3DGS, which can directly be used within their framework. The result is a method
that more closely follows the volume rendering equation (similar to
ray-tracing) while enjoying the speed benefits of rasterization. Our method
represents opaque surfaces with higher accuracy and fewer points than 3DGS.
This enables it to outperform 3DGS for view synthesis (measured in SSIM and
LPIPS). Being volumetrically consistent also enables our method to work out of
the box for tomography. We match the state-of-the-art 3DGS-based tomography
method with fewer points. Our code is publicly available at:
https://github.com/chinmay0301ucsd/Vol3DGS

</details>


### [373] [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/pdf/2412.06141)
*Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao*

Main category: cs.CV

TL;DR: MMedPO improves Med-LVLMs by addressing modality misalignment with clinically relevant preference optimization, enhancing factual accuracy by 14.2% and 51.7% in key tasks.


<details>
  <summary>Details</summary>
Motivation: Med-LVLMs face factuality issues due to prioritizing text over visuals, causing hallucinations. Existing methods lack clinical relevance in preference data, reducing alignment effectiveness.

Method: MMedPO introduces two dispreference types (plausible hallucinations and lesion neglect) and weights samples by clinical relevance from Med-LLMs and visual tools for optimization.

Result: MMedPO boosts factual accuracy by 14.2% (Med-VQA) and 51.7% (report generation) over existing methods.

Conclusion: MMedPO effectively aligns Med-LVLMs by integrating clinical relevance, significantly improving performance in medical tasks.

Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their
application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter
factuality challenges due to modality misalignment, where the models prioritize
textual knowledge over visual input, leading to hallucinations that contradict
information in medical images. Previous attempts to enhance modality alignment
in Med-LVLMs through preference optimization have inadequately mitigated
clinical relevance in preference data, making these samples easily
distinguishable and reducing alignment effectiveness. To address this
challenge, we propose MMedPO, a novel multimodal medical preference
optimization approach that considers the clinical relevance of preference
samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference
data by introducing two types of dispreference: (1) plausible hallucinations
injected through target Med-LVLMs or GPT-4o to produce medically inaccurate
responses, and (2) lesion region neglect achieved through local lesion-noising,
disrupting visual understanding of critical areas. We then calculate clinical
relevance for each sample based on scores from multiple Med-LLMs and visual
tools, and integrate these scores into the preference optimization process as
weights, enabling effective alignment. Our experiments demonstrate that MMedPO
significantly enhances factual accuracy in Med-LVLMs, achieving substantial
improvements over existing preference optimization methods by averaging 14.2%
and 51.7% across the Med-VQA and report generation tasks. Our code are
available in https://github.com/aiming-lab/MMedPO.

</details>


### [374] [EventSplat: 3D Gaussian Splatting from Moving Event Cameras for Real-time Rendering](https://arxiv.org/pdf/2412.07293)
*Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski*

Main category: cs.CV

TL;DR: A method for novel view synthesis using event camera data via Gaussian Splatting, improving speed and quality over existing event-based NeRF approaches.


<details>
  <summary>Details</summary>
Motivation: Event cameras provide high temporal resolution and dynamic range, making them suitable for novel view synthesis, especially with fast camera motion.

Method: Uses Gaussian Splatting with event camera data, initialized via an event-to-video model, and employs spline interpolation for high-quality poses.

Result: Achieves higher visual fidelity and performance than existing event-based NeRF methods, with significantly faster rendering.

Conclusion: The method effectively leverages event camera advantages for novel view synthesis, outperforming traditional NeRF approaches in speed and quality.

Abstract: We introduce a method for using event camera data in novel view synthesis via
Gaussian Splatting. Event cameras offer exceptional temporal resolution and a
high dynamic range. Leveraging these capabilities allows us to effectively
address the novel view synthesis challenge in the presence of fast camera
motion. For initialization of the optimization process, our approach uses prior
knowledge encoded in an event-to-video model. We also use spline interpolation
for obtaining high quality poses along the event camera trajectory. This
enhances the reconstruction quality from fast-moving cameras while overcoming
the computational limitations traditionally associated with event-based Neural
Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that
our results achieve higher visual fidelity and better performance than existing
event-based NeRF approaches while being an order of magnitude faster to render.

</details>


### [375] [Reconstructing People, Places, and Cameras](https://arxiv.org/pdf/2412.17806)
*Lea Müller, Hongsuk Choi, Anthony Zhang, Brent Yi, Jitendra Malik, Angjoo Kanazawa*

Main category: cs.CV

TL;DR: HSfM jointly reconstructs human meshes, scene point clouds, and camera parameters from uncalibrated multi-view images, improving accuracy by combining data-driven and traditional SfM methods.


<details>
  <summary>Details</summary>
Motivation: Existing SfM methods lack metric scale and struggle with human-centric scenes. HSfM addresses this by leveraging human statistical models and joint optimization.

Method: Combines data-driven scene reconstruction with SfM, using human models for metric scale and jointly optimizing humans, scenes, and cameras.

Result: Reduces human localization error significantly (e.g., from 3.51m to 1.04m) and improves camera pose estimation (RRA@15 by 20.3%).

Conclusion: HSfM enhances scene reconstruction and human localization accuracy, demonstrating the value of integrating human data into SfM pipelines.

Abstract: We present "Humans and Structure from Motion" (HSfM), a method for jointly
reconstructing multiple human meshes, scene point clouds, and camera parameters
in a metric world coordinate system from a sparse set of uncalibrated
multi-view images featuring people. Our approach combines data-driven scene
reconstruction with the traditional Structure-from-Motion (SfM) framework to
achieve more accurate scene reconstruction and camera estimation, while
simultaneously recovering human meshes. In contrast to existing scene
reconstruction and SfM methods that lack metric scale information, our method
estimates approximate metric scale by leveraging a human statistical model.
Furthermore, it reconstructs multiple human meshes within the same world
coordinate system alongside the scene point cloud, effectively capturing
spatial relationships among individuals and their positions in the environment.
We initialize the reconstruction of humans, scenes, and cameras using robust
foundational models and jointly optimize these elements. This joint
optimization synergistically improves the accuracy of each component. We
compare our method to existing approaches on two challenging benchmarks,
EgoHumans and EgoExo4D, demonstrating significant improvements in human
localization accuracy within the world coordinate frame (reducing error from
3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our
results show that incorporating human data into the SfM pipeline improves
camera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).
Additionally, qualitative results show that our approach improves overall scene
reconstruction quality. Our code is available at:
https://github.com/hongsukchoi/HSfM_RELEASE

</details>


### [376] [HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for Multi-View 3D Object Detection](https://arxiv.org/pdf/2412.18884)
*Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang*

Main category: cs.CV

TL;DR: The paper introduces HV-BEV, a novel BEV-based method for autonomous driving that improves feature sampling by decoupling horizontal and vertical dimensions, enhancing object correlation and height awareness.


<details>
  <summary>Details</summary>
Motivation: Current BEV-based methods overlook structured object correlations and height variations in 3D space, limiting performance.

Method: HV-BEV decouples BEV grid queries into horizontal feature aggregation and vertical adaptive height-aware sampling, using dynamic neighboring points and historical height information.

Result: The method achieves 50.5% mAP and 59.8% NDS on the nuScenes dataset, outperforming baselines.

Conclusion: HV-BEV effectively improves object correlation and height awareness, demonstrating superior performance in autonomous driving perception.

Abstract: The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often overlook the structured
correlations among different parts of objects in 3D space and the fact that
different categories of objects often occupy distinct local height ranges. For
example, trucks appear at higher elevations, whereas traffic cones are near the
ground. In this work, we propose a novel approach that decouples feature
sampling in the \textbf{BEV} grid queries paradigm into \textbf{H}orizontal
feature aggregation and \textbf{V}ertical adaptive height-aware reference point
sampling (HV-BEV), aiming to improve both the aggregation of objects' complete
information and awareness of diverse objects' height distribution.
Specifically, a set of relevant neighboring points is dynamically constructed
for each 3D reference point on the ground-aligned horizontal plane, enhancing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5\% mAP and 59.8\% NDS on the nuScenes testing
set. The code is available at https://github.com/Uddd821/HV-BEV.

</details>


### [377] [PixelWorld: Towards Perceiving Everything as Pixels](https://arxiv.org/pdf/2501.19339)
*Zhiheng Lyu, Xueguang Ma, Wenhu Chen*

Main category: cs.CV

TL;DR: PEAP (Perceive Everything as Pixels) unifies visual and textual data into a single pixel space, showing competitive semantic understanding but struggles with reasoning tasks. PixelWorld benchmark supports evaluation of unified vision-language models.


<details>
  <summary>Details</summary>
Motivation: Address the need for a unified perception paradigm in agentic language models interacting with real-world environments containing intertwined visual and textual information.

Method: Propose PEAP, rendering diverse inputs (text, tables, math, diagrams) into a single pixel space, and evaluate using the PixelWorld benchmark.

Result: PEAP achieves competitive accuracy on semantic tasks but performs poorly on reasoning tasks; Chain-of-Thought prompting helps mitigate this gap.

Conclusion: PEAP simplifies preprocessing and avoids misalignment in vision-language tasks, making PixelWorld a practical benchmark for unified models.

Abstract: Recent agentic language models increasingly need to interact directly with
real-world environments containing intertwined visual and textual information
through raw camera pixels, rather than relying on separate image and tokenized
text processing, underscoring the necessity of a unified perception paradigm.
To close this gap, we explore this idea through Perceive Everything as Pixels
(PEAP) and release PixelWorld, a benchmark that renders natural-language,
tabular, mathematical and diagrammatic inputs into a single pixel space.
Experiments show that PEAP attains competitive accuracy on
semantic-understanding tasks, indicating that a vision transformer can capture
global textual semantics without explicit tokens. In contrast,
reasoning-intensive benchmarks (math and code) exhibit sharp performance drops;
however, Chain-of-Thought prompting partially mitigates this gap, hinting that
explicit reasoning traces compensate for the missing token structure. We also
find that when visual and textual information are closely integrated,
representing everything as pixels reduces preprocessing complexity and avoids
misalignment issues that often arise in separate pipelines. PixelWorld
therefore serves as a practical benchmark for evaluating unified
vision-language models and supports broader exploration of PEAP across diverse
tasks.

</details>


### [378] [The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles](https://arxiv.org/pdf/2502.01081)
*Vernon Y. H. Toh, Yew Ken Chia, Deepanway Ghosal, Soujanya Poria*

Main category: cs.CV

TL;DR: The paper evaluates the o-[n] and GPT-[n] series models on multimodal reasoning tasks, finding o-[n] models (especially o3 and o4-mini) outperform GPT-[n] but still struggle with precise visual perception and complex reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating advanced reasoning capabilities in multimodal tasks, as current benchmarks like ARC-AGI focus only on symbolic patterns.

Method: Tracking the performance of GPT-[n] and o-[n] series models on multimodal puzzles from PuzzleVQA and AlgoPuzzleVQA.

Result: o-[n] models, particularly o3 and o4-mini, outperform GPT-[n] in multimodal reasoning but face challenges in precise visual perception and complex tasks.

Conclusion: Despite advancements, persistent challenges in multimodal reasoning highlight critical areas for future AGI development.

Abstract: The releases of OpenAI's o-[n] series, such as o1, o3, and o4-mini, mark a
significant paradigm shift in Large Language Models towards advanced reasoning
capabilities. Notably, models like o3 have demonstrated strong performance on
benchmarks like the Abstraction and Reasoning Corpus for Artificial General
Intelligence (ARC-AGI). However, this benchmark is limited to symbolic
patterns, whereas humans often perceive and reason about multimodal scenarios
involving both vision and language data. Thus, there is an urgent need to
investigate advanced reasoning capabilities in multimodal tasks. To this end,
we track the evolution of the GPT-[n] and o-[n] series models (including o1,
o3, and o4-mini) on challenging multimodal puzzles from PuzzleVQA and
AlgoPuzzleVQA, which demand fine-grained visual perception. Our results reveal
that o-[n] series, particularly later iterations like o3 and o4-mini,
significantly outperform the GPT-[n] series and show strong scalability in
multimodal reasoning. Nonetheless, despite these substantial advancements and
the superior capabilities demonstrated by the o-[n] series, our findings
highlight that even these leading models face persistent challenges.
Difficulties are particularly evident in tasks requiring precise visual
perception, robust compositional reasoning across multiple visual attributes,
and solving complex algorithmic or highly combinatorial puzzles, indicating
critical areas for future AGI development. We plan to continuously track new
models in the series and update our results in this paper accordingly. All
resources used in this evaluation are openly available at
https://github.com/declare-lab/LLM-PuzzleTest.

</details>


### [379] [Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics](https://arxiv.org/pdf/2502.03449)
*Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang*

Main category: cs.CV

TL;DR: Dress-1-to-3 introduces a pipeline for generating simulation-ready 3D garments from images, enhancing geometric alignment and realism.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstructions often fuse models into single pieces, limiting downstream tasks like virtual try-on with dynamic animations.

Method: Combines image-to-sewing pattern generation, multi-view diffusion, and differentiable garment simulation for refined 3D garments.

Result: Improved geometric alignment and realistic dynamic garment demonstrations.

Conclusion: The pipeline enables physics-plausible, separable 3D garments for applications like virtual try-on.

Abstract: Recent advances in large models have significantly advanced image-to-3D
reconstruction. However, the generated models are often fused into a single
piece, limiting their applicability in downstream tasks. This paper focuses on
3D garment generation, a key area for applications like virtual try-on with
dynamic garment animations, which require garments to be separable and
simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs
physics-plausible, simulation-ready separated garments with sewing patterns and
humans from an in-the-wild image. Starting with the image, our approach
combines a pre-trained image-to-sewing pattern generation model for creating
coarse sewing patterns with a pre-trained multi-view diffusion model to produce
multi-view images. The sewing pattern is further refined using a differentiable
garment simulator based on the generated multi-view images. Versatile
experiments demonstrate that our optimization approach substantially enhances
the geometric alignment of the reconstructed 3D garments and humans with the
input image. Furthermore, by integrating a texture generation module and a
human motion generation module, we produce customized physics-plausible and
realistic dynamic garment demonstrations. Project page:
https://dress-1-to-3.github.io/

</details>


### [380] [PlaySlot: Learning Inverse Latent Dynamics for Controllable Object-Centric Video Prediction and Planning](https://arxiv.org/pdf/2502.07600)
*Angel Villar-Corrales, Sven Behnke*

Main category: cs.CV

TL;DR: PlaySlot is an object-centric video prediction model that infers object representations and latent actions from unlabeled videos, enabling versatile and interpretable future scene forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on labeled data, limiting their use of abundant unlabeled videos. PlaySlot addresses this gap by leveraging unlabeled sequences.

Method: PlaySlot infers object representations and latent actions from unlabeled videos, then forecasts future object states and frames. It supports multiple possible futures based on latent actions.

Result: PlaySlot outperforms stochastic and object-centric baselines in video prediction and enables sample-efficient robot behavior learning from unlabeled videos.

Conclusion: PlaySlot advances video prediction by utilizing unlabeled data, offering interpretable world modeling and practical applications like robotics.

Abstract: Predicting future scene representations is a crucial task for enabling robots
to understand and interact with the environment. However, most existing methods
rely on videos and simulations with precise action annotations, limiting their
ability to leverage the large amount of available unlabeled video data. To
address this challenge, we propose PlaySlot, an object-centric video prediction
model that infers object representations and latent actions from unlabeled
video sequences. It then uses these representations to forecast future object
states and video frames. PlaySlot allows the generation of multiple possible
futures conditioned on latent actions, which can be inferred from video
dynamics, provided by a user, or generated by a learned action policy, thus
enabling versatile and interpretable world modeling. Our results show that
PlaySlot outperforms both stochastic and object-centric baselines for video
prediction across different environments. Furthermore, we show that our
inferred latent actions can be used to learn robot behaviors sample-efficiently
from unlabeled video demonstrations. Videos and code are available on
https://play-slot.github.io/PlaySlot/.

</details>


### [381] [AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance](https://arxiv.org/pdf/2502.08189)
*Zhao Wang, Hao Wen, Lingting Zhu, Chenming Shang, Yujiu Yang, Qi Dou*

Main category: cs.CV

TL;DR: AnyCharV is a novel framework for flexible character video generation using arbitrary source characters and target scenes, guided by pose information, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for character video generation lack flexibility, making it hard to synthesize characters into desired scenes.

Method: A two-stage training process: first integrates source characters with target scenes using pose guidance; second refines generation via a self-boosting mechanism.

Result: Superior performance compared to state-of-the-art methods, with better preservation of character details.

Conclusion: AnyCharV offers a flexible and effective solution for high-quality character video generation.

Abstract: Character video generation is a significant real-world application focused on
producing high-quality videos featuring specific characters. Recent
advancements have introduced various control signals to animate static
characters, successfully enhancing control over the generation process.
However, these methods often lack flexibility, limiting their applicability and
making it challenging for users to synthesize a source character into a desired
target scene. To address this issue, we propose a novel framework, AnyCharV,
that flexibly generates character videos using arbitrary source characters and
target scenes, guided by pose information. Our approach involves a two-stage
training process. In the first stage, we develop a base model capable of
integrating the source character with the target scene using pose guidance. The
second stage further bootstraps controllable generation through a self-boosting
mechanism, where we use the generated video in the first stage and replace the
fine mask with the coarse one, enabling training outcomes with better
preservation of character details. Extensive experimental results demonstrate
the superiority of our method compared with previous state-of-the-art methods.

</details>


### [382] [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/pdf/2502.14846)
*Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark*

Main category: cs.CV

TL;DR: CoSyn uses LLMs to generate synthetic text-rich multimodal data, improving VLMs' performance in reasoning about text-rich images like charts and documents.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with text-rich vision-language tasks due to limited diverse data.

Method: CoSyn leverages LLMs to generate code for rendering synthetic images and creates instruction-tuning data.

Result: Models trained on CoSyn's 400K-image dataset outperform open-source and proprietary models like GPT-4V.

Conclusion: CoSyn enhances VLMs' capabilities, including grounding information in images, enabling real-world multimodal agents.

Abstract: Reasoning about images with rich text, such as charts and documents, is a
critical application of vision-language models (VLMs). However, VLMs often
struggle in these domains due to the scarcity of diverse text-rich
vision-language data. To address this challenge, we present CoSyn, a framework
that leverages the coding capabilities of text-only large language models
(LLMs) to automatically create synthetic text-rich multimodal data. Given input
text describing a target domain (e.g., "nutrition fact labels"), CoSyn prompts
an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic
images. With the underlying code as textual representations of the synthetic
images, CoSyn can generate high-quality instruction-tuning data, again relying
on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K
images and 2.7M rows of vision-language instruction-tuning data. Comprehensive
experiments on seven benchmarks demonstrate that models trained on our
synthetic data achieve state-of-the-art performance among competitive
open-source models, including Llama 3.2, and surpass proprietary models such as
GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing
data, enabling VLMs to ground information within input images, showcasing its
potential for developing multimodal agents capable of acting in real-world
environments.

</details>


### [383] [CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation](https://arxiv.org/pdf/2502.17429)
*Vishal Thengane, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Lu Yin, Xiatian Zhu, Salman Khan*

Main category: cs.CV

TL;DR: CLIMB-3D is a framework for class-incremental, imbalance-aware 3D instance segmentation, addressing challenges of new class emergence and imbalance. It combines exemplar replay, pseudo-label generation, and class-balanced re-weighting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing 3DIS methods assume known, uniformly distributed classes, which is unrealistic in dynamic environments with emerging and imbalanced classes.

Method: CLIMB-3D integrates exemplar replay (ER), a pseudo-label generator (PLG) for supervision, and a class-balanced re-weighting (CBR) scheme to adjust training bias dynamically.

Result: The framework outperforms prior work by up to 16.76% mAP for instance segmentation and ~30% mIoU for semantic segmentation, excelling on both frequent and rare classes.

Conclusion: CLIMB-3D effectively addresses class imbalance and incremental learning in 3DIS, demonstrating strong generalization and performance.

Abstract: While 3D instance segmentation (3DIS) has advanced significantly, existing
methods typically assume that all object classes are known in advance and are
uniformly distributed. However, this assumption is unrealistic in dynamic,
real-world environments where new classes emerge gradually and exhibit natural
imbalance. Although some approaches have addressed class emergence, they often
overlook class imbalance, resulting in suboptimal performance -- particularly
on rare categories. To tackle this challenge, we propose CLIMB-3D, a unified
framework for \textbf{CL}ass-incremental \textbf{Imb}alance-aware
\textbf{3D}IS. Building upon established exemplar replay (ER) strategies, we
show that ER alone is insufficient to achieve robust performance under
constrained memory conditions. To mitigate this, we introduce a novel
pseudo-label generator (PLG) that extends supervision to previously learned
categories by leveraging predictions from a frozen prior model. Despite its
promise, PLG tends to bias towards frequent classes. Therefore, we propose a
class-balanced re-weighting (CBR) scheme, that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias -- without requiring
access to past data. We design and evaluate three incremental scenarios for
3DIS on the challenging ScanNet200 dataset, and additionally on semantic
segmentation on ScanNetV2. Our approach achieves state-of-the-art results,
surpassing prior work by up to 16.76\% mAP for instance segmentation and
approximately 30\% mIoU for semantic segmentation, demonstrating strong
generalization across both frequent and rare classes.

</details>


### [384] [OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation](https://arxiv.org/pdf/2502.18041)
*Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li*

Main category: cs.CV

TL;DR: The paper introduces OpenFly, a platform for outdoor aerial Vision-Language Navigation (VLN), addressing the lack of benchmarks by integrating rendering engines, automating data collection, and creating a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Outdoor aerial VLN is underexplored due to challenges in data collection and lack of benchmarks. The paper aims to bridge this gap.

Method: OpenFly combines diverse rendering engines (Unreal Engine, GTA V, Google Earth, 3D Gaussian Splatting) and a toolchain for automated data collection, including point cloud acquisition and instruction generation. A large-scale dataset (100k trajectories) is created.

Result: The platform and proposed OpenFly-Agent outperform existing VLN methods, demonstrating the effectiveness of the approach.

Conclusion: OpenFly provides a comprehensive solution for aerial VLN, with open-sourced tools and datasets to foster further research.

Abstract: Vision-Language Navigation (VLN) aims to guide agents by leveraging language
instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN
has been extensively studied, whereas outdoor aerial VLN remains underexplored.
The potential reason is that outdoor aerial view encompasses vast areas, making
data collection more challenging, which results in a lack of benchmarks. To
address this problem, we propose OpenFly, a platform comprising various
rendering engines, a versatile toolchain, and a large-scale benchmark for
aerial VLN. Firstly, we integrate diverse rendering engines and advanced
techniques for environment simulation, including Unreal Engine, GTA V, Google
Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports
real-to-sim rendering, further enhancing the realism of our environments.
Secondly, we develop a highly automated toolchain for aerial VLN data
collection, streamlining point cloud acquisition, scene semantic segmentation,
flight trajectory creation, and instruction generation. Thirdly, based on the
toolchain, we construct a large-scale aerial VLN dataset with 100k
trajectories, covering diverse heights and lengths across 18 scenes. Moreover,
we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key
observations during flight. For benchmarking, extensive experiments and
analyses are conducted, evaluating several recent VLN methods and showcasing
the superiority of our OpenFly platform and agent. The toolchain, dataset, and
codes will be open-sourced.

</details>


### [385] [How far can we go with ImageNet for Text-to-Image generation?](https://arxiv.org/pdf/2502.21318)
*L. Degeorge, A. Ghosh, N. Dufour, D. Picard, V. Kalogeiton*

Main category: cs.CV

TL;DR: The paper challenges the 'bigger is better' paradigm in text-to-image generation by showing superior performance with a simpler setup using ImageNet and augmentations.


<details>
  <summary>Details</summary>
Motivation: To challenge the reliance on massive datasets and promote reproducibility by using a smaller, standardized dataset (ImageNet).

Method: Utilizes ImageNet enhanced with text and image augmentations, requiring fewer parameters and training images.

Result: Outperforms SD-XL by +1% on GenEval and +0.5% on DPGBench with 1/10th the parameters and 1/1000th the training images.

Conclusion: Demonstrates that simpler, reproducible setups can match or outperform massive datasets, enabling more accessible research.

Abstract: Recent text-to-image generation models have achieved remarkable results by
training on billion-scale datasets, following a `bigger is better' paradigm
that prioritizes data quantity over availability (closed vs open source) and
reproducibility (data decay vs established collections). We challenge this
established paradigm by demonstrating that one can match or outperform models
trained on massive web-scraped collections, using only ImageNet enhanced with
well-designed text and image augmentations. With this much simpler setup, we
achieve a +1% overall score over SD-XL on GenEval and +0.5% on DPGBench while
using just 1/10th the parameters and 1/1000th the training images. This opens
the way for more reproducible research as ImageNet is a widely available
dataset and our standardized training setup does not require massive compute
resources.

</details>


### [386] [Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models](https://arxiv.org/pdf/2503.00059)
*Rui Hu, Delai Qiu, Shuyu Wei, Jiaming Zhang, Yining Wang, Shengping Liu, Jitao Sang*

Main category: cs.CV

TL;DR: Self-Knowledge Distillation (Self-KD) improves vision-audio integration in Omnimodal Large Language Models (OLLMs) by leveraging vision-text components as teachers.


<details>
  <summary>Details</summary>
Motivation: OLLMs struggle with integrating vision and audio, performing poorly on audio queries due to insufficient modality alignment during training.

Method: Proposes Self-KD, where the vision-text component teaches the vision-audio component, aligning audio processing with text processing.

Result: Self-KD enhances vision-audio capabilities, improving interaction between audio and images and boosting multimodal task performance.

Conclusion: Self-KD effectively bridges the gap between vision-audio and vision-text modalities in OLLMs.

Abstract: Omnimodal Large Language Models (OLLMs) have shown significant progress in
integrating vision and text, but still struggle with integrating vision and
audio, often exhibiting suboptimal performance when processing audio queries
compared to text queries. This disparity is primarily due to insufficient
alignment between vision and audio modalities during training, leading to
inadequate attention to visual information when using audio queries. To
mitigate this issue, we propose a Self-Knowledge Distillation (Self-KD)
training method where the vision-text component of the OLLM serves as the
teacher and the vision-audio component as the student. This enables the model
to process audio in a manner analogous to its text processing. Our experimental
results demonstrate that Self-KD is an effective method for enhancing the
vision-audio capabilities of OLLMs by learning from the vision-text components,
which subsequently improves the interaction between audio and images and
results in improved performance on multimodal tasks.

</details>


### [387] [MambaFlow: A Mamba-Centric Architecture for End-to-End Optical Flow Estimation](https://arxiv.org/pdf/2503.07046)
*Juntian Du, Yuan Sun, Zhihu Zhou, Pinyi Chen, Runzhe Zhang, Keji Mao*

Main category: cs.CV

TL;DR: MambaFlow, a novel framework using the Mamba architecture for optical flow estimation, outperforms state-of-the-art methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: The Mamba architecture excels in computer vision tasks but hasn't been applied to optical flow estimation, prompting the development of MambaFlow.

Method: MambaFlow combines PolyMamba for feature optimization and PulseMamba for efficient flow information dissemination.

Result: Achieves an EPE of 1.43 on Sintel benchmark, outperforming GMFlow, SeparableFlow, CRAFT, and DIP in both accuracy and speed.

Conclusion: MambaFlow shows strong potential for real-world deployment on resource-constrained devices.

Abstract: Recently, the Mamba architecture has demonstrated significant successes in
various computer vision tasks, such as classification and segmentation.
However, its application to optical flow estimation remains unexplored. In this
paper, we introduce MambaFlow, a novel framework designed to leverage the high
accuracy and efficiency of the Mamba architecture for capturing locally
correlated features while preserving global information in end-to-end optical
flow estimation. To our knowledge, MambaFlow is the first architecture centered
around the Mamba design tailored specifically for optical flow estimation. It
comprises two key components: (1) PolyMamba, which optimizes feature
representation; and (2) PulseMamba, which facilitates efficient flow
information dissemination. Our extensive experiments demonstrate that MambaFlow
achieves remarkable results. On the Sintel benchmark, MambaFlow records an
endpoint error (EPE) of 1.43 and an inference speed of 0.113 seconds,
surpassing the state-of-the-art methods including GMFlow (with 18.9% lower EPE
and 18.1% faster inference), SeparableFlow (5% lower EPE and 50.5% faster),
CRAFT (1.11% lower EPE and 76.5% faster), and DIP (0.7% lower EPE and 77.2%
faster)-demonstrating stronger potential for real-world deployment on
resource-constrained devices. The source code will be made publicly available
upon acceptance of the paper.

</details>


### [388] [Transductive One-Shot Learning Meet Subspace Decomposition](https://arxiv.org/pdf/2504.00348)
*Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh*

Main category: cs.CV

TL;DR: A transductive one-shot learning method using subspace decomposition to generalize from one labeled image to unseen classes.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of one-shot learning by leveraging information from both labeled and unlabeled images to improve generalization.

Method: Subspace decomposition to represent images as linear combinations of latent primitives, enabling label propagation from a single labeled image to similar query images.

Result: Effective generalization to novel classes from just one labeled image, validated across various feature extractors and datasets.

Conclusion: The proposed transductive approach enhances one-shot learning by utilizing latent primitives and subspace decomposition.

Abstract: One-shot learning focuses on adapting pretrained models to recognize newly
introduced and unseen classes based on a single labeled image. While variations
of few-shot and zero-shot learning exist, one-shot learning remains a
challenging yet crucial problem due to its ability to generalize knowledge to
unseen classes from just one human-annotated image. In this paper, we introduce
a transductive one-shot learning approach that employs subspace decomposition
to utilize the information from labeled images in the support set and unlabeled
images in the query set. These images are decomposed into a linear combination
of latent variables representing primitives captured by smaller subspaces. By
representing images in the query set as linear combinations of these latent
primitives, we can propagate the label from a single image in the support set
to query images that share similar combinations of primitives. Through a
comprehensive quantitative analysis across various neural network feature
extractors and datasets, we demonstrate that our approach can effectively
generalize to novel classes from just one labeled image.

</details>


### [389] [SpaceR: Reinforcing MLLMs in Video Spatial Reasoning](https://arxiv.org/pdf/2504.01805)
*Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun*

Main category: cs.CV

TL;DR: The paper introduces SpaceR, a framework to enhance Multimodal Large Language Models (MLLMs) in video spatial reasoning using Reinforcement Learning with Verifiable Reward (RLVR). It includes a dataset (SpaceR-151k) and a novel training method (SG-RLVR), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with video spatial reasoning due to lack of high-quality datasets and effective training strategies. Inspired by RLVR's success in LLMs, the work aims to improve MLLMs' spatial reasoning.

Method: The SpaceR framework combines the SpaceR-151k dataset (91k spatial reasoning questions and 60k general multimodal samples) with SG-RLVR, a reinforcement learning method that includes map imagination for spatial layout inference.

Result: SpaceR outperforms benchmarks like VSI-Bench (11.6% better than GPT-4o) and matches Gemini-2.0-Flash, while maintaining strong performance on general video understanding tasks.

Conclusion: SpaceR's dataset and SG-RLVR method effectively enhance MLLMs' spatial reasoning, demonstrating significant improvements over existing models.

Abstract: Video spatial reasoning, which involves inferring the underlying spatial
structure from observed video frames, poses a significant challenge for
existing Multimodal Large Language Models (MLLMs). This limitation stems
primarily from 1) the absence of high-quality datasets for this task, and 2)
the lack of effective training strategies to develop spatial reasoning
capabilities. Motivated by the success of Reinforcement Learning with
Verifiable Reward (RLVR) in unlocking LLM reasoning abilities, this work aims
to improve MLLMs in video spatial reasoning through the RLVR paradigm. To this
end, we introduce the $\textbf{SpaceR}$ framework. First, we present
$\textbf{SpaceR-151k}$, a dataset with 91k questions spanning diverse spatial
reasoning scenarios with verifiable answers, and 60k samples for maintaining
general multimodal understanding. Second, we propose $\textbf{Spatially-Guided
RLVR (SG-RLVR)}$, a novel reinforcement learning approach that extends Group
Relative Policy Optimization (GRPO) with a novel map imagination mechanism,
which encourages the model to infer spatial layouts in the thinking process,
thereby facilitating more effective spatial reasoning. Extensive experiments
demonstrate that SpaceR achieves state-of-the-art performance on spatial
reasoning benchmarks (e.g., VSI-Bench, STI-Bench, and SPAR-Bench), while
maintaining competitive results on video understanding benchmarks (e.g.,
Video-MME, TempCompass, and LongVideoBench). Remarkably, SpaceR surpasses the
advanced GPT-4o by 11.6\% accuracy on VSI-Bench and is on par with the leading
proprietary model Gemini-2.0-Flash, highlighting the effectiveness of our
SpaceR-151k dataset and SG-RLVR in reinforcing spatial reasoning ability of
MLLMs. Code, model, and dataset are available at
https://github.com/OuyangKun10/SpaceR.

</details>


### [390] [A Survey of Pathology Foundation Model: Progress and Future Directions](https://arxiv.org/pdf/2504.04045)
*Conghao Xiong, Hao Chen, Joseph J. Y. Sung*

Main category: cs.CV

TL;DR: A survey on Pathology Foundation Models (PFMs) in computational pathology, proposing a hierarchical taxonomy for analysis and benchmarking, while identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic analysis framework for PFMs, which are crucial for improving feature extraction and aggregation in cancer diagnosis via whole slide images.

Method: Proposes a hierarchical taxonomy (model scope, pretraining, design) and categorizes evaluation tasks (slide-level, patch-level, multimodal, biological) for PFMs.

Result: Identifies key challenges in PFM development (methodology, pretraining, scalability) and utilization (adaptation, maintenance).

Conclusion: The survey provides a structured approach to PFM analysis and benchmarking, highlighting future research directions in computational pathology.

Abstract: Computational pathology, which involves analyzing whole slide images for
automated cancer diagnosis, relies on multiple instance learning, where
performance depends heavily on the feature extractor and aggregator. Recent
Pathology Foundation Models (PFMs), pretrained on large-scale histopathology
data, have significantly enhanced both the extractor and aggregator, but they
lack a systematic analysis framework. In this survey, we present a hierarchical
taxonomy organizing PFMs through a top-down philosophy applicable to foundation
model analysis in any domain: model scope, model pretraining, and model design.
Additionally, we systematically categorize PFM evaluation tasks into
slide-level, patch-level, multimodal, and biological tasks, providing
comprehensive benchmarking criteria. Our analysis identifies critical
challenges in both PFM development (pathology-specific methodology, end-to-end
pretraining, data-model scalability) and utilization (effective adaptation,
model maintenance), paving the way for future directions in this promising
field. Resources referenced in this survey are available at
https://github.com/BearCleverProud/AwesomeWSI.

</details>


### [391] [TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials](https://arxiv.org/pdf/2504.12679)
*Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, Qing Li*

Main category: cs.CV

TL;DR: The TongUI framework builds generalized GUI agents by leveraging multimodal web tutorials to create the GUI-Net dataset, improving agent performance by 10% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of diverse GUI trajectory data due to high annotation costs hinders the development of generalized GUI agents.

Method: Crawling and processing online GUI tutorials into trajectory data, creating the GUI-Net dataset, and fine-tuning Qwen2.5-VL-3B/7B models.

Result: TongUI agents outperform baselines by ~10% on grounding and navigation benchmarks.

Conclusion: The GUI-Net dataset and TongUI framework are effective, with plans to open-source code, data, and models.

Abstract: Building Graphical User Interface (GUI) agents is a promising research
direction, which simulates human interaction with computers or mobile phones to
perform diverse GUI tasks. However, a major challenge in developing generalized
GUI agents is the lack of sufficient trajectory data across various operating
systems and applications, mainly due to the high cost of manual annotations. In
this paper, we propose the TongUI framework that builds generalized GUI agents
by learning from rich multimodal web tutorials. Concretely, we crawl and
process online GUI tutorials (such as videos and articles) into GUI agent
trajectory data, through which we produce the GUI-Net dataset containing 143K
trajectory data across five operating systems and more than 200 applications.
We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,
which show remarkable performance improvements on commonly used grounding and
navigation benchmarks, outperforming baseline agents about 10\% on multiple
benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring
the significance of our TongUI framework. We will fully open-source the code,
the GUI-Net dataset, and the trained models soon.

</details>


### [392] [Mask Image Watermarking](https://arxiv.org/pdf/2504.12739)
*Runyi Hu, Jie Zhang, Shiqian Zhao, Nils Lukas, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang*

Main category: cs.CV

TL;DR: MaskMark is a framework for image watermarking with two variants: MaskMark-D for global/local watermarking and MaskMark-ED for enhanced local robustness. It outperforms existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To provide a simple, efficient, and flexible solution for image watermarking, addressing both global and local needs with improved robustness and computational efficiency.

Method: MaskMark-D uses an encoder-distortion layer-decoder paradigm with a masking mechanism for local extraction. MaskMark-ED extends this by incorporating masks in encoding for better regional robustness.

Result: Achieves state-of-the-art performance in watermark extraction, localization, and multi-watermark embedding, with 15x efficiency over WAM and high visual quality.

Conclusion: MaskMark is a versatile, efficient, and high-performing framework adaptable to varying robustness needs.

Abstract: We present MaskMark, a simple, efficient, and flexible framework for image
watermarking. MaskMark has two variants: (1) MaskMark-D, which supports global
watermark embedding, watermark localization, and local watermark extraction for
applications such as tamper detection; (2) MaskMark-ED, which focuses on local
watermark embedding and extraction, offering enhanced robustness in small
regions to support fine-grined image protection. MaskMark-D builds on the
classical encoder-distortion layer-decoder training paradigm. In MaskMark-D, we
introduce a simple masking mechanism during the decoding stage that enables
both global and local watermark extraction. During training, the decoder is
guided by various types of masks applied to watermarked images before
extraction, helping it learn to localize watermarks and extract them from the
corresponding local areas. MaskMark-ED extends this design by incorporating the
mask into the encoding stage as well, guiding the encoder to embed the
watermark in designated local regions, which improves robustness under regional
attacks. Extensive experiments show that MaskMark achieves state-of-the-art
performance in global and local watermark extraction, watermark localization,
and multi-watermark embedding. It outperforms all existing baselines, including
the recent leading model WAM for local watermarking, while preserving high
visual quality of the watermarked images. In addition, MaskMark is highly
efficient and adaptable. It requires only 20 hours of training on a single
A6000 GPU, achieving 15x computational efficiency compared to WAM. By simply
adjusting the distortion layer, MaskMark can be quickly fine-tuned to meet
varying robustness requirements.

</details>


### [393] [MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection](https://arxiv.org/pdf/2504.13726)
*Lin Yuan, Xiaowan Li, Yan Zhang, Jiawei Zhang, Hongbo Li, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes Multi-granularity Local Entropy Patterns (MLEP) for detecting AI-generated images, improving accuracy and generalization over existing methods.


<details>
  <summary>Details</summary>
Motivation: Concerns about misuse of AI-generated images (e.g., deepfakes) necessitate reliable detection methods, but current approaches lack generalization and source-invariant features.

Method: Uses image entropy to create MLEP, a set of entropy feature maps from shuffled patches across scales, disrupting semantics to reduce bias. A CNN classifier is trained on MLEP.

Result: Tests on 32 generative models show MLEP outperforms state-of-the-art methods in accuracy and generalization.

Conclusion: MLEP offers a robust solution for AIGI detection, addressing limitations of existing methods.

Abstract: Advancements in image generation technologies have raised significant
concerns about their potential misuse, such as producing misinformation and
deepfakes. Therefore, there is an urgent need for effective methods to detect
AI-generated images (AIGI). Despite progress in AIGI detection, achieving
reliable performance across diverse generation models and scenes remains
challenging due to the lack of source-invariant features and limited
generalization capabilities in existing methods. In this work, we explore the
potential of using image entropy as a cue for AIGI detection and propose
Multi-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps
computed across shuffled small patches over multiple image scaled. MLEP
comprehensively captures pixel relationships across dimensions and scales while
significantly disrupting image semantics, reducing potential content bias.
Leveraging MLEP, a robust CNN-based classifier for AIGI detection can be
trained. Extensive experiments conducted in an open-world scenario, evaluating
images synthesized by 32 distinct generative models, demonstrate significant
improvements over state-of-the-art methods in both accuracy and generalization.

</details>


### [394] [VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment](https://arxiv.org/pdf/2504.14096)
*Yogesh Kulkarni, Pooyan Fazli*

Main category: cs.CV

TL;DR: VideoPASTA enhances Video-LLMs by optimizing preferences with adversarial examples, improving spatial-temporal understanding without massive pretraining.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with spatial relationships, temporal ordering, and cross-frame continuity, limiting their effectiveness.

Method: VideoPASTA uses adversarial examples and Direct Preference Optimization to train models on 7,020 preference pairs, focusing on spatial-temporal accuracy.

Result: Achieves gains of 3.8% on LongVideoBench, 4.1% on VideoMME, and 4.0% on MVBench, proving model-agnostic effectiveness.

Conclusion: Targeted preference alignment is a scalable, efficient solution for improving Video-LLMs without human annotation or architectural changes.

Abstract: Video-language models (Video-LLMs) excel at understanding video content but
struggle with spatial relationships, temporal ordering, and cross-frame
continuity. To address these limitations, we introduce VideoPASTA (Preference
Alignment with Spatio-Temporal-Cross Frame Adversaries), a framework that
enhances Video-LLMs through targeted preference optimization. VideoPASTA trains
models to distinguish accurate video representations from carefully crafted
adversarial examples that deliberately violate spatial, temporal, or
cross-frame relationships. With only 7,020 preference pairs and Direct
Preference Optimization, VideoPASTA enables models to learn robust
representations that capture fine-grained spatial details and long-range
temporal dynamics. Experiments demonstrate that VideoPASTA is model agnostic
and significantly improves performance, for example, achieving gains of up to
3.8% on LongVideoBench, 4.1% on VideoMME, and 4.0% on MVBench, when applied to
various state-of-the-art Video-LLMs. These results demonstrate that targeted
alignment, rather than massive pretraining or architectural modifications,
effectively addresses core video-language challenges. Notably, VideoPASTA
achieves these improvements without any human annotation or captioning, relying
solely on 32-frame sampling. This efficiency makes our approach a scalable
plug-and-play solution that seamlessly integrates with existing models while
preserving their original capabilities.

</details>


### [395] [DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining](https://arxiv.org/pdf/2504.15669)
*Wei Zhuo, Zhiyue Tang, Wufeng Xue, Hao Ding, Linlin Shen*

Main category: cs.CV

TL;DR: FS-DINO is a unified model combining DINOv2's encoder with a lightweight segmenter, leveraging SAM's knowledge through distillation for few-shot semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in few-shot semantic segmentation by integrating complementary capabilities of DINOv2 and SAM into a single model.

Method: Proposes FS-DINO with DINOv2's encoder and a lightweight segmenter featuring a bottleneck adapter, meta-visual prompt generator, and decoder. Uses cross-model distillation and 4D correlation mining.

Result: Demonstrates effectiveness and superiority on COCO-20i, PASCAL-5i, and FSS-1000 datasets.

Conclusion: FS-DINO successfully unifies knowledge from DINOv2 and SAM, achieving strong performance in few-shot semantic segmentation.

Abstract: Few-shot semantic segmentation has gained increasing interest due to its
generalization capability, i.e., segmenting pixels of novel classes requiring
only a few annotated images. Prior work has focused on meta-learning for
support-query matching, with extensive development in both prototype-based and
aggregation-based methods. To address data scarcity, recent approaches have
turned to foundation models to enhance representation transferability for novel
class segmentation. Among them, a hybrid dual-modal framework including both
DINOv2 and SAM has garnered attention due to their complementary capabilities.
We wonder "can we build a unified model with knowledge from both foundation
models?" To this end, we propose FS-DINO, with only DINOv2's encoder and a
lightweight segmenter. The segmenter features a bottleneck adapter, a
meta-visual prompt generator based on dense similarities and semantic
embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we
effectively integrate SAM's knowledge into our lightweight segmenter, which can
be further enhanced by 4D correlation mining on support-query pairs. Extensive
experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness
and superiority of our method.

</details>


### [396] [Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs](https://arxiv.org/pdf/2505.00744)
*Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: The paper introduces HEAL-MedVQA, a benchmark to evaluate hallucination and localization in medical LMMs, and proposes the LobA framework to improve visual reasoning, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current medical LMMs often generate hallucinations due to poor localization reasoning, relying on irrelevant data instead of analyzing pathological regions.

Method: The authors develop HEAL-MedVQA with evaluation protocols and a dataset of 67K VQA pairs, and propose the LobA framework to localize and emphasize pathological areas.

Result: The LobA framework significantly outperforms state-of-the-art biomedical LMMs on the HEAL-MedVQA benchmark.

Conclusion: The work advances robustness in medical VQA by addressing localization and hallucination issues in LMMs.

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>


### [397] [RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet](https://arxiv.org/pdf/2505.02586)
*Eliraz Orfaig, Inna Stainvas, Igal Bilik*

Main category: cs.CV

TL;DR: RGBX-DiffusionDet extends DiffusionDet by fusing RGB with other 2D data (X) using adaptive multimodal encoding, dynamic channel reduction, and multi-level aggregation, outperforming RGB-only baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance object detection by integrating heterogeneous 2D data (e.g., depth, polarimetric, infrared) with RGB imagery, improving feature representation and cross-modal interaction.

Method: Uses DCR-CBAM for dynamic channel reduction, DMLAB for adaptive multiscale fusion, and novel regularization losses for compact feature embeddings.

Result: Outperforms RGB-only DiffusionDet on RGB-Depth, RGB-Polarimetric, and RGB-Infrared datasets while maintaining efficiency.

Conclusion: RGBX-DiffusionDet is a flexible, efficient multimodal object detection framework, advancing integration of diverse 2D sensing modalities.

Abstract: This work introduces RGBX-DiffusionDet, an object detection framework
extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB
imagery via an adaptive multimodal encoder. To enable cross-modal interaction,
we design the dynamic channel reduction within a convolutional block attention
module (DCR-CBAM), which facilitates cross-talk between subnetworks by
dynamically highlighting salient channel features. Furthermore, the dynamic
multi-level aggregation block (DMLAB) is proposed to refine spatial feature
representations through adaptive multiscale fusion. Finally, novel
regularization losses that enforce channel saliency and spatial selectivity are
introduced, leading to compact and discriminative feature embeddings. Extensive
experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric
dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We
demonstrate consistent superiority of the proposed approach over the baseline
RGB-only DiffusionDet. The modular architecture maintains the original decoding
complexity, ensuring efficiency. These results establish the proposed
RGBX-DiffusionDet as a flexible multimodal object detection approach, providing
new insights into integrating diverse 2D sensing modalities into
diffusion-based detection pipelines.

</details>


### [398] [Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World](https://arxiv.org/pdf/2505.04788)
*Bangyan Liao, Zhenjun Zhao, Haoang Li, Yi Zhou, Yingping Zeng, Hao Li, Peidong Liu*

Main category: cs.CV

TL;DR: The paper introduces GlobustVP, a method using convex relaxation to efficiently and robustly estimate vanishing points (VPs) in a Manhattan world, balancing global optimality and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods for VP estimation are either sub-optimal or computationally expensive. The paper aims to address this gap by proposing a globally optimal yet efficient solution.

Method: The method employs convex relaxation techniques, formulating the problem as a QCQP and relaxing it into a convex SDP. It uses an iterative solver (GlobustVP) to independently update VPs and their line associations, reinforcing orthogonality constraints.

Result: GlobustVP outperforms prior methods in efficiency, robustness, and global optimality, as demonstrated by experiments on synthetic and real-world data.

Conclusion: The proposed approach achieves a favorable trade-off between computational efficiency and global optimality, making it practical for real-world applications.

Abstract: Determining the vanishing points (VPs) in a Manhattan world, as a fundamental
task in many 3D vision applications, consists of jointly inferring the line-VP
association and locating each VP. Existing methods are, however, either
sub-optimal solvers or pursuing global optimality at a significant cost of
computing time. In contrast to prior works, we introduce convex relaxation
techniques to solve this task for the first time. Specifically, we employ a
"soft" association scheme, realized via a truncated multi-selection error, that
allows for joint estimation of VPs' locations and line-VP associations. This
approach leads to a primal problem that can be reformulated into a
quadratically constrained quadratic programming (QCQP) problem, which is then
relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP
problem efficiently, we present a globally optimal outlier-robust iterative
solver (called GlobustVP), which independently searches for one VP and its
associated lines in each iteration, treating other lines as outliers. After
each independent update of all VPs, the mutual orthogonality between the three
VPs in a Manhattan world is reinforced via local refinement. Extensive
experiments on both synthetic and real-world data demonstrate that GlobustVP
achieves a favorable balance between efficiency, robustness, and global
optimality compared to previous works. The code is publicly available at
https://github.com/WU-CVGL/GlobustVP.

</details>


### [399] [UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model](https://arxiv.org/pdf/2505.05049)
*Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn*

Main category: cs.CV

TL;DR: The paper introduces USAM, a lightweight Bayesian entropy-based uncertainty quantification method for SAM, addressing aleatoric, epistemic, and task uncertainty. It outperforms on multiple datasets and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Quantifying uncertainty in SAM is challenging due to its class-agnostic nature, necessitating a robust UQ method.

Method: Proposes USAM, a post-hoc UQ model using Bayesian entropy to handle aleatoric, epistemic, and task uncertainty.

Result: USAM shows superior performance on SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering computational efficiency.

Conclusion: USAM provides an effective, easy-to-use UQ solution for SAM, enhancing applications like semi-supervised learning and cost-accuracy tradeoffs.

Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for
numerous semantic segmentation applications. For several tasks, quantifying the
uncertainty of SAM is of particular interest. However, the ambiguous nature of
the class-agnostic foundation model SAM challenges current uncertainty
quantification (UQ) approaches. This paper presents a theoretically motivated
uncertainty quantification model based on a Bayesian entropy formulation
jointly respecting aleatoric, epistemic, and the newly introduced task
uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ
method. Our model traces the root of uncertainty back to under-parameterised
models, insufficient prompts or image ambiguities. Our proposed deterministic
USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,
DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ
alternative that can support user-prompting, enhance semi-supervised pipelines,
or balance the tradeoff between accuracy and cost efficiency.

</details>


### [400] [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/pdf/2505.05071)
*Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, Yuhui Yin*

Main category: cs.CV

TL;DR: FG-CLIP improves fine-grained understanding in multimodal tasks by leveraging large-scale data, detailed annotations, and hard negative samples, outperforming CLIP and other methods.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with fine-grained understanding due to its reliance on coarse-grained captions. FG-CLIP aims to enhance this by capturing detailed semantic nuances.

Method: FG-CLIP uses 1.6B long caption-image pairs, 12M images with 40M region-specific bounding boxes, and 10M hard negative samples. It also introduces the FineHARD dataset and tailored training methods.

Result: FG-CLIP outperforms CLIP and other methods in fine-grained understanding, open-vocabulary detection, retrieval, and multimodal benchmarks.

Conclusion: FG-CLIP effectively captures fine-grained details, improving multimodal performance. Data, code, and models are publicly available.

Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks
such as image-text retrieval and zero-shot classification but struggles with
fine-grained understanding due to its focus on coarse-grained short captions.
To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances
fine-grained understanding through three key innovations. First, we leverage
large multimodal models to generate 1.6 billion long caption-image pairs for
capturing global-level semantic details. Second, a high-quality dataset is
constructed with 12 million images and 40 million region-specific bounding
boxes aligned with detailed captions to ensure precise, context-rich
representations. Third, 10 million hard fine-grained negative samples are
incorporated to improve the model's ability to distinguish subtle semantic
differences. We construct a comprehensive dataset, termed FineHARD, by
integrating high-quality region-specific annotations with hard fine-grained
negative samples. Corresponding training methods are meticulously designed for
these data. Extensive experiments demonstrate that FG-CLIP outperforms the
original CLIP and other state-of-the-art methods across various downstream
tasks, including fine-grained understanding, open-vocabulary object detection,
image-text retrieval, and general multimodal benchmarks. These results
highlight FG-CLIP's effectiveness in capturing fine-grained image details and
improving overall model performance. The data, code, and models are available
at https://github.com/360CVGroup/FG-CLIP.

</details>


### [401] [DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting](https://arxiv.org/pdf/2505.08644)
*Holly Dinkel, Marcel Büsching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, Mårten Björkman, Timothy Bretl*

Main category: cs.CV

TL;DR: DLO-Splatting estimates 3D shape of Deformable Linear Objects using multi-view RGB images and gripper state info, combining prediction-update filtering with 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: Existing vision-only methods struggle with challenging scenarios like knot tying, prompting the need for a more robust approach.

Method: Uses position-based dynamics with smoothness and rigidity corrections, plus 3D Gaussian Splatting for iterative rendering and refinement.

Result: Initial experiments show promising results in knot tying, outperforming vision-only methods.

Conclusion: DLO-Splatting is effective for 3D shape estimation of deformable objects, especially in complex tasks.

Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

</details>


### [402] [Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models](https://arxiv.org/pdf/2505.11482)
*Shirin Shoushtari, Edward P. Chandler, M. Salman Asif, Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: A fully unsupervised metric for estimating distribution shifts in diffusion models using corrupted measurements and score functions, improving reconstruction quality in inverse problems.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of diffusion models under distribution shifts between training and test images, without needing clean test images.

Method: Propose a score-based metric using corrupted measurements and score functions to estimate KL divergence between distributions.

Result: Metric approximates KL divergence from clean images; aligning scores improves reconstruction quality.

Conclusion: Unsupervised metric effectively estimates distribution shifts and enhances performance in inverse problems.

Abstract: Diffusion models are widely used as priors in imaging inverse problems.
However, their performance often degrades under distribution shifts between the
training and test-time images. Existing methods for identifying and quantifying
distribution shifts typically require access to clean test images, which are
almost never available while solving inverse problems (at test time). We
propose a fully unsupervised metric for estimating distribution shifts using
only indirect (corrupted) measurements and score functions from diffusion
models trained on different datasets. We theoretically show that this metric
estimates the KL divergence between the training and test image distributions.
Empirically, we show that our score-based metric, using only corrupted
measurements, closely approximates the KL divergence computed from clean
images. Motivated by this result, we show that aligning the out-of-distribution
score with the in-distribution score -- using only corrupted measurements --
reduces the KL divergence and leads to improved reconstruction quality across
multiple inverse problems.

</details>


### [403] [Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling](https://arxiv.org/pdf/2505.12048)
*Rui Qin, Qijie Wang, Ming Sun, Haowei Zhu, Chao Zhou, Bin Wang*

Main category: cs.CV

TL;DR: The paper introduces Time-Spatial-aware Sampling (TSS) to accelerate diffusion-based super-resolution (SR) by optimizing iteration allocation and denoising strategies, achieving SOTA performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based SR methods are computationally expensive, and generic acceleration techniques don't fully exploit low-level task characteristics like SR.

Method: Analyzes frequency- and spatial-domain properties of diffusion SR, proposing TSS with Time Dynamic Sampling (TDS) and Spatial Dynamic Sampling (SDS) for adaptive iteration allocation.

Result: TSS improves MUSIQ scores by 0.2-3.0 and outperforms other methods with half the steps.

Conclusion: TSS effectively accelerates diffusion SR by leveraging temporal and spatial dependencies, reducing computational costs without extra training.

Abstract: Diffusion models have gained attention for their success in modeling complex
distributions, achieving impressive perceptual quality in SR tasks. However,
existing diffusion-based SR methods often suffer from high computational costs,
requiring numerous iterative steps for training and inference. Existing
acceleration techniques, such as distillation and solver optimization, are
generally task-agnostic and do not fully leverage the specific characteristics
of low-level tasks like super-resolution (SR). In this study, we analyze the
frequency- and spatial-domain properties of diffusion-based SR methods,
revealing key insights into the temporal and spatial dependencies of
high-frequency signal recovery. Specifically, high-frequency details benefit
from concentrated optimization during early and late diffusion iterations,
while spatially textured regions demand adaptive denoising strategies. Building
on these observations, we propose the Time-Spatial-aware Sampling strategy
(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS
combines Time Dynamic Sampling (TDS), which allocates more iterations to
refining textures, and Spatial Dynamic Sampling (SDS), which dynamically
adjusts strategies based on image content. Extensive evaluations across
multiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)
performance with significantly fewer iterations, improving MUSIQ scores by 0.2
- 3.0 and outperforming the current acceleration methods with only half the
number of steps.

</details>


### [404] [VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.12081)
*Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, Jiaya Jia*

Main category: cs.CV

TL;DR: VisionReasoner is a unified framework for multiple visual perception tasks, outperforming Qwen2.5VL by significant margins.


<details>
  <summary>Details</summary>
Motivation: To address diverse visual perception tasks within a single model, enhancing reasoning capabilities.

Method: Uses multi-object cognitive learning and task reformulation to unify tasks like detection, segmentation, and counting.

Result: Achieves superior performance: 29.1% better on COCO, 22.1% on ReasonSeg, and 15.3% on CountBench.

Conclusion: VisionReasoner demonstrates strong unified capabilities across diverse visual tasks.

Abstract: Large vision-language models exhibit inherent capabilities to handle diverse
visual perception tasks. In this paper, we introduce VisionReasoner, a unified
framework capable of reasoning and solving multiple visual perception tasks
within a shared model. Specifically, by designing novel multi-object cognitive
learning strategies and systematic task reformulation, VisionReasoner enhances
its reasoning capabilities to analyze visual inputs, and addresses diverse
perception tasks in a unified framework. The model generates a structured
reasoning process before delivering the desired outputs responding to user
queries. To rigorously assess unified visual perception capabilities, we
evaluate VisionReasoner on ten diverse tasks spanning three critical domains:
detection, segmentation, and counting. Experimental results show that
VisionReasoner achieves superior performance as a unified model, outperforming
Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg
(segmentation), and 15.3% on CountBench (counting).

</details>


### [405] [SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning](https://arxiv.org/pdf/2505.12448)
*Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang*

Main category: cs.CV

TL;DR: SSR enhances VLMs by converting depth data into textual rationales, improving spatial reasoning without retraining, and introduces a new dataset and benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack precise spatial understanding due to reliance on RGB inputs, and current depth integration methods are either sensor-dependent or ineffective.

Method: SSR transforms raw depth data into structured textual rationales, uses knowledge distillation for compact embeddings, and integrates them into VLMs.

Result: SSR improves depth utilization and spatial reasoning, validated by experiments on multiple benchmarks.

Conclusion: SSR advances VLMs toward human-like multi-modal understanding, with potential for broader applications.

Abstract: Despite impressive advancements in Visual-Language Models (VLMs) for
multi-modal tasks, their reliance on RGB inputs limits precise spatial
understanding. Existing methods for integrating spatial cues, such as point
clouds or depth, either require specialized sensors or fail to effectively
exploit depth information for higher-order reasoning. To this end, we propose a
novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that
transforms raw depth data into structured, interpretable textual rationales.
These textual rationales serve as meaningful intermediate representations to
significantly enhance spatial reasoning capabilities. Additionally, we leverage
knowledge distillation to compress the generated rationales into compact latent
embeddings, which facilitate resource-efficient and plug-and-play integration
into existing VLMs without retraining. To enable comprehensive evaluation, we
introduce a new dataset named SSR-CoT, a million-scale visual-language
reasoning dataset enriched with intermediate spatial reasoning annotations, and
present SSRBench, a comprehensive multi-task benchmark. Extensive experiments
on multiple benchmarks demonstrate SSR substantially improves depth utilization
and enhances spatial reasoning, thereby advancing VLMs toward more human-like
multi-modal understanding. Our project page is at
https://yliu-cs.github.io/SSR.

</details>


### [406] [Video-GPT via Next Clip Diffusion](https://arxiv.org/pdf/2505.12489)
*Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, Yali Wang*

Main category: cs.CV

TL;DR: Video-GPT introduces a novel next clip diffusion paradigm for video modeling, achieving state-of-the-art performance in video prediction and generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Language sequences lack spatial-temporal details; videos capture these better, inspiring Video-GPT as a new 'language' for visual modeling.

Method: Proposes a next clip diffusion paradigm, autoregressively denoising noisy clips based on clean history clips, enabling short-term generation and long-term prediction.

Result: Achieves top performance on video prediction (Physics-IQ Benchmark: 34.97) and adapts well to 6 mainstream video tasks.

Conclusion: Video-GPT excels in video modeling and generalization, demonstrating its potential for diverse video applications.

Abstract: GPT has shown its remarkable success in natural language processing. However,
the language sequence is not sufficient to describe spatial-temporal details in
the visual world. Alternatively, the video sequence is good at capturing such
details. Motivated by this fact, we propose a concise Video-GPT in this paper
by treating video as new language for visual world modeling. By analogy to next
token prediction in GPT, we introduce a novel next clip diffusion paradigm for
pretraining Video-GPT. Different from the previous works, this distinct
paradigm allows Video-GPT to tackle both short-term generation and long-term
prediction, by autoregressively denoising the noisy clip according to the clean
clips in the history. Extensive experiments show our Video-GPT achieves the
state-of-the-art performance on video prediction, which is the key factor
towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64
vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in
both video generation and understanding, showing its great generalization
capacity in downstream. The project page is at
https://zhuangshaobin.github.io/Video-GPT.github.io/.

</details>


### [407] [Learning Cross-Spectral Point Features with Task-Oriented Training](https://arxiv.org/pdf/2505.12593)
*Mia Thomas, Trevor Ablett, Jonathan Kelly*

Main category: cs.CV

TL;DR: The paper proposes a method to train a feature network for cross-spectral (thermal-visible) point features to improve UAV navigation in low-visibility conditions by focusing on matching and registration tasks.


<details>
  <summary>Details</summary>
Motivation: Visible-spectrum cameras used in UAV navigation fail in low-visibility conditions, while thermal cameras perform well. Integrating thermal imagery into existing systems requires robust cross-spectral feature matching.

Method: The method trains a feature network on matching and registration tasks using thermal-visible image pairs, feeding network responses into a differentiable registration pipeline with losses applied to matching and registration estimates.

Result: The trained model achieves a registration error below 10 pixels for over 75% of estimates on the MultiPoint dataset and works with classical matching pipelines.

Conclusion: The proposed method effectively integrates thermal imagery into UAV navigation systems, improving performance in low-visibility conditions.

Abstract: Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous
environments, yet the visible-spectrum, camera-based navigation systems often
relied upon by UAVs struggle in low-visibility conditions. Thermal cameras,
which capture long-wave infrared radiation, are able to function effectively in
darkness and smoke, where visible-light cameras fail. This work explores
learned cross-spectral (thermal-visible) point features as a means to integrate
thermal imagery into established camera-based navigation systems. Existing
methods typically train a feature network's detection and description outputs
directly, which often focuses training on image regions where thermal and
visible-spectrum images exhibit similar appearance. Aiming to more fully
utilize the available data, we propose a method to train the feature network on
the tasks of matching and registration. We run our feature network on
thermal-visible image pairs, then feed the network response into a
differentiable registration pipeline. Losses are applied to the matching and
registration estimates of this pipeline. Our selected model, trained on the
task of matching, achieves a registration error (corner error) below 10 pixels
for more than 75% of estimates on the MultiPoint dataset. We further
demonstrate that our model can also be used with a classical pipeline for
matching and registration.

</details>


### [408] [BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation](https://arxiv.org/pdf/2505.12620)
*Haiquan Wen, Yiwei He, Zhenglin Huang, Tianxiao Li, Zihan Yu, Xingru Huang, Lu Qi, Baoyuan Wu, Xiangtai Li, Guangliang Cheng*

Main category: cs.CV

TL;DR: The paper introduces GenBuster-200K, a large-scale AI-generated video dataset, and BusterX, a novel detection framework combining MLLM and reinforcement learning for explainable AI video detection.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated videos (e.g., Sora, WanX) increases misinformation risks, but existing datasets and detection methods lack scale, quality, and explainability.

Method: Proposes GenBuster-200K (200K high-res videos) and BusterX (MLLM + reinforcement learning) for detection and explainability.

Result: BusterX outperforms state-of-the-art methods, validated by comparisons and ablation studies.

Conclusion: GenBuster-200K and BusterX address gaps in AI video detection, offering scalable, explainable solutions.

Abstract: Advances in AI generative models facilitate super-realistic video synthesis,
amplifying misinformation risks via social media and eroding trust in digital
content. Several research works have explored new deepfake detection methods on
AI-generated images to alleviate these risks. However, with the fast
development of video generation models, such as Sora and WanX, there is
currently a lack of large-scale, high-quality AI-generated video datasets for
forgery detection. In addition, existing detection approaches predominantly
treat the task as binary classification, lacking explainability in model
decision-making and failing to provide actionable insights or guidance for the
public. To address these challenges, we propose \textbf{GenBuster-200K}, a
large-scale AI-generated video dataset featuring 200K high-resolution video
clips, diverse latest generative techniques, and real-world scenes. We further
introduce \textbf{BusterX}, a novel AI-generated video detection and
explanation framework leveraging multimodal large language model (MLLM) and
reinforcement learning for authenticity determination and explainable
rationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}}
large-scale, high-quality AI-generated video dataset that incorporates the
latest generative techniques for real-world scenarios. BusterX is the {\it
\textbf{first}} framework to integrate MLLM with reinforcement learning for
explainable AI-generated video detection. Extensive comparisons with
state-of-the-art methods and ablation studies validate the effectiveness and
generalizability of BusterX. The code, models, and datasets will be released.

</details>


### [409] [DD-Ranking: Rethinking the Evaluation of Dataset Distillation](https://arxiv.org/pdf/2505.13300)
*Zekai Li, Xinhao Zhong, Samir Khaki, Zhiyuan Liang, Yuhao Zhou, Mingjia Shi, Ziqiao Wang, Xuanlei Zhao, Wangbo Zhao, Ziheng Qin, Mengxuan Wu, Pengfei Zhou, Haonan Wang, David Junhao Zhang, Jia-Wei Liu, Shaobo Wang, Dai Liu, Linfeng Zhang, Guang Li, Kun Wang, Zheng Zhu, Zhiheng Ma, Joey Tianyi Zhou, Jiancheng Lv, Yaochu Jin, Peihao Wang, Kaipeng Zhang, Lingjuan Lyu, Yiran Huang, Zeynep Akata, Zhiwei Deng, Xindi Wu, George Cazenavette, Yuzhang Shang, Justin Cui, Jindong Gu, Qian Zheng, Hao Ye, Shuo Wang, Xiaobo Wang, Yan Yan, Angela Yao, Mike Zheng Shou, Tianlong Chen, Hakan Bilen, Baharan Mirzasoleiman, Manolis Kellis, Konstantinos N. Plataniotis, Zhangyang Wang, Bo Zhao, Yang You, Kai Wang*

Main category: cs.CV

TL;DR: The paper critiques current evaluation metrics for dataset distillation (DD) and proposes DD-Ranking, a unified framework with new metrics for fairer assessment.


<details>
  <summary>Details</summary>
Motivation: Existing DD methods rely on accuracy metrics, which may not reflect true performance due to additional techniques. This misalignment hinders progress.

Method: The authors introduce DD-Ranking, a framework with new evaluation metrics focusing on the actual information enhancement of distilled datasets.

Result: Empirical findings show current methods' improvements often come from extra techniques, not image quality, with random images sometimes outperforming.

Conclusion: DD-Ranking offers a fairer, more comprehensive standard for evaluating DD methods, aiming to advance future research.

Abstract: In recent years, dataset distillation has provided a reliable solution for
data compression, where models trained on the resulting smaller synthetic
datasets achieve performance comparable to those trained on the original
datasets. To further improve the performance of synthetic datasets, various
training pipelines and optimization objectives have been proposed, greatly
advancing the field of dataset distillation. Recent decoupled dataset
distillation methods introduce soft labels and stronger data augmentation
during the post-evaluation phase and scale dataset distillation up to larger
datasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy
still a reliable metric to fairly evaluate dataset distillation methods? Our
empirical findings suggest that the performance improvements of these methods
often stem from additional techniques rather than the inherent quality of the
images themselves, with even randomly sampled images achieving superior
results. Such misaligned evaluation settings severely hinder the development of
DD. Therefore, we propose DD-Ranking, a unified evaluation framework, along
with new general evaluation metrics to uncover the true performance
improvements achieved by different methods. By refocusing on the actual
information enhancement of distilled datasets, DD-Ranking provides a more
comprehensive and fair evaluation standard for future research advancements.

</details>


### [410] [Faster Video Diffusion with Trainable Sparse Attention](https://arxiv.org/pdf/2505.13389)
*Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, Hao Zhang*

Main category: cs.CV

TL;DR: VSA introduces a trainable, hardware-efficient sparse attention method for video diffusion transformers, reducing computational costs without performance loss.


<details>
  <summary>Details</summary>
Motivation: Quadratic 3D attention in video diffusion transformers is computationally expensive, even though attention is concentrated on a few positions. VSA aims to address this inefficiency.

Method: VSA uses a two-stage approach: a coarse stage pools tokens into tiles and identifies critical tokens, while a fine stage computes token-level attention only within those tiles, ensuring hardware efficiency.

Result: VSA reduces training FLOPS by 2.53×, speeds up attention time by 6×, and lowers generation time from 31s to 18s without quality loss.

Conclusion: VSA is a practical, efficient alternative to full attention, enabling further scaling of video diffusion models.

Abstract: Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models.

</details>


### [411] [Selective Structured State Space for Multispectral-fused Small Target Detection](https://arxiv.org/pdf/2505.14043)
*Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang*

Main category: cs.CV

TL;DR: The paper addresses challenges in small target detection in high-resolution remote sensing imagery by enhancing Mamba's capabilities with new modules (ESTD, CARG, MEPF) to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Small targets in high-resolution images have low recognition accuracy and high computational costs, exacerbated by the limitations of Transformer and CNN architectures.

Method: Proposes the ESTD module for local attention, the CARG module for spatial/channel-wise information, and the MEPF module for multispectral fusion.

Result: Enhanced detection of small targets by combining Mamba's global attention with fine local details and improved multimodal fusion.

Conclusion: The proposed modules effectively address computational constraints and improve small target detection accuracy.

Abstract: Target detection in high-resolution remote sensing imagery faces challenges
due to the low recognition accuracy of small targets and high computational
costs. The computational complexity of the Transformer architecture increases
quadratically with image resolution, while Convolutional Neural Networks (CNN)
architectures are forced to stack deeper convolutional layers to expand their
receptive fields, leading to an explosive growth in computational demands. To
address these computational constraints, we leverage Mamba's linear complexity
for efficiency. However, Mamba's performance declines for small targets,
primarily because small targets occupy a limited area in the image and have
limited semantic information. Accurate identification of these small targets
necessitates not only Mamba's global attention capabilities but also the
precise capture of fine local details. To this end, we enhance Mamba by
developing the Enhanced Small Target Detection (ESTD) module and the
Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters
local attention to capture fine-grained details, while the CARG module, built
upon Mamba, emphasizes spatial and channel-wise information, collectively
improving the model's ability to capture distinctive representations of small
targets. Additionally, to highlight the semantic representation of small
targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for
multispectral fusion, which enhances target features by effectively fusing
visible and infrared multimodal information.

</details>


### [412] [Unlocking the Power of SAM 2 for Few-Shot Segmentation](https://arxiv.org/pdf/2505.14100)
*Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao*

Main category: cs.CV

TL;DR: The paper proposes Pseudo Prompt Generator and Iterative Memory Refinement to address incompatibility and inaccuracy in Few-Shot Segmentation (FSS) by leveraging SAM 2's video segmentation capabilities.


<details>
  <summary>Details</summary>
Motivation: To overcome the risk of overfitting in FSS and the incompatibility between SAM 2's class-agnostic matching and FSS's different identities.

Method: Designs Pseudo Prompt Generator for compatible matching and Iterative Memory Refinement to improve memory accuracy by fusing query FG features and suppressing BG features.

Result: Achieves a 4.2% improvement in 1-shot mIoU over baselines on PASCAL-5$^i$ and COCO-20$^i$.

Conclusion: The proposed methods effectively enhance FSS performance by addressing memory compatibility and accuracy issues.

Abstract: Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few
classes to segment arbitrary classes, but at the risk of overfitting. To
address this, some methods use the well-learned knowledge of foundation models
(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM
by supporting video segmentation, whose class-agnostic matching ability is
useful to FSS. A simple idea is to encode support foreground (FG) features as
memory, with which query FG features are matched and fused. Unfortunately, the
FG objects in different frames of SAM 2's video data are always the same
identity, while those in FSS are different identities, i.e., the matching step
is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo
query memory, matching with query features in a compatible way. However, the
memories can never be as accurate as the real ones, i.e., they are likely to
contain incomplete query FG, and some unexpected query background (BG)
features, leading to wrong segmentation. Hence, we further design Iterative
Memory Refinement to fuse more query FG features into the memory, and devise a
Support-Calibrated Memory Attention to suppress the unexpected query BG
features in memory. Extensive experiments have been conducted on PASCAL-5$^i$
and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot
mIoU can be 4.2% better than the best baseline.

</details>


### [413] [Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling](https://arxiv.org/pdf/2505.14521)
*Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen*

Main category: cs.CV

TL;DR: Sparc3D introduces a unified framework for high-fidelity 3D object synthesis using sparse deformable marching cubes and a novel VAE, overcoming detail loss and inefficiency in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D synthesis methods suffer from detail loss and inefficiency due to unstructured mesh data and poor representations.

Method: Sparc3D combines Sparcubes (sparse deformable marching cubes) and Sparconv-VAE (a sparse convolutional VAE) for efficient, high-resolution 3D reconstruction and generation.

Result: Achieves state-of-the-art fidelity, preserves fine details, reduces computational costs, and integrates well with latent diffusion models.

Conclusion: Sparc3D provides a scalable, high-resolution solution for 3D generation, outperforming existing methods.

Abstract: High-fidelity 3D object synthesis remains significantly more challenging than
2D image generation due to the unstructured nature of mesh data and the cubic
complexity of dense volumetric grids. Existing two-stage pipelines-compressing
meshes with a VAE (using either 2D or 3D supervision), followed by latent
diffusion sampling-often suffer from severe detail loss caused by inefficient
representations and modality mismatches introduced in VAE. We introduce
Sparc3D, a unified framework that combines a sparse deformable marching cubes
representation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts
raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by
scattering signed distance and deformation fields onto a sparse cube, allowing
differentiable optimization. Sparconv-VAE is the first modality-consistent
variational autoencoder built entirely upon sparse convolutional networks,
enabling efficient and near-lossless 3D reconstruction suitable for
high-resolution generative modeling through latent diffusion. Sparc3D achieves
state-of-the-art reconstruction fidelity on challenging inputs, including open
surfaces, disconnected components, and intricate geometry. It preserves
fine-grained shape details, reduces training and inference cost, and integrates
naturally with latent diffusion models for scalable, high-resolution 3D
generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [414] [Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies](https://arxiv.org/pdf/2505.14689)
*Ashwani Anand, Satya Prakash Nayak, Ritam Raha, Anne-Kathrin Schmuck*

Main category: cs.AI

TL;DR: A dynamic post-shielding framework (STARs) enforces ω-regular correctness properties on probabilistic policies, balancing formal obligations and task-specific behavior via tunable interference.


<details>
  <summary>Details</summary>
Motivation: Shift from safety-shielding (preventing bad events) to enforcing liveness (ensuring good events), adaptable to runtime changes like specification updates or failures.

Method: Uses Strategy-Template-based Adaptive Runtime Shields (STARs) with permissive templates for minimal interference and dynamic control.

Result: Demonstrated on a mobile robot, STARs effectively balance enforcement and policy optimization, adapting to incremental updates.

Conclusion: STARs offer a flexible, runtime-adaptive solution for enforcing correctness in cyber-physical systems.

Abstract: This paper presents a novel dynamic post-shielding framework that enforces
the full class of $\omega$-regular correctness properties over pre-computed
probabilistic policies. This constitutes a paradigm shift from the predominant
setting of safety-shielding -- i.e., ensuring that nothing bad ever happens --
to a shielding process that additionally enforces liveness -- i.e., ensures
that something good eventually happens. At the core, our method uses
Strategy-Template-based Adaptive Runtime Shields (STARs), which leverage
permissive strategy templates to enable post-shielding with minimal
interference. As its main feature, STARs introduce a mechanism to dynamically
control interference, allowing a tunable enforcement parameter to balance
formal obligations and task-specific behavior at runtime. This allows to
trigger more aggressive enforcement when needed, while allowing for optimized
policy choices otherwise. In addition, STARs support runtime adaptation to
changing specifications or actuator failures, making them especially suited for
cyber-physical applications. We evaluate STARs on a mobile robot benchmark to
demonstrate their controllable interference when enforcing (incrementally
updated) $\omega$-regular correctness properties over learned probabilistic
policies.

</details>


### [415] [R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](https://arxiv.org/pdf/2505.14738)
*Xu Yang, Xiao Yang, Shikai Fang, Bowen Xian, Yuante Li, Jian Wang, Minrui Xu, Haoran Pan, Xinpeng Hong, Weiqing Liu, Yelong Shen, Weizhu Chen, Jiang Bian*

Main category: cs.AI

TL;DR: R&D-Agent is a dual-agent framework for iterative data science tasks, combining idea generation (Researcher) and code refinement (Developer) to improve automation and performance.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and expertise requirements in data science hinder progress, despite AI/ML advances. Crowdsourcing helps but high-level tasks remain labor-intensive.

Method: R&D-Agent uses two agents: Researcher (idea generation via performance feedback) and Developer (code refinement via error feedback), enabling parallel exploration traces.

Result: R&D-Agent outperforms others on MLE-Bench, showing potential to accelerate innovation and precision in data science.

Conclusion: R&D-Agent bridges the gap between automation and expert-level performance, with open-source availability for broader use.

Abstract: Recent advances in AI and ML have transformed data science, yet increasing
complexity and expertise requirements continue to hinder progress. While
crowdsourcing platforms alleviate some challenges, high-level data science
tasks remain labor-intensive and iterative. To overcome these limitations, we
introduce R&D-Agent, a dual-agent framework for iterative exploration. The
Researcher agent uses performance feedback to generate ideas, while the
Developer agent refines code based on error feedback. By enabling multiple
parallel exploration traces that merge and enhance one another, R&D-Agent
narrows the gap between automated solutions and expert-level performance.
Evaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine
learning engineering agent, demonstrating its potential to accelerate
innovation and improve precision across diverse data science applications. We
have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.

</details>


### [416] [FOL-Pretrain: A complexity annotated corpus of first-order logic](https://arxiv.org/pdf/2505.14932)
*Isabelle Lee, Sarah Liaw, Dani Yogatama*

Main category: cs.AI

TL;DR: The paper introduces a large-scale, annotated dataset for studying algorithmic reasoning in LLMs, addressing gaps in understanding their internal processes.


<details>
  <summary>Details</summary>
Motivation: Limited understanding of how LLMs internalize and execute complex algorithms due to heterogeneous pretraining data and shallow prior studies.

Method: Creation of a complexity-annotated dataset with 3.5B tokens, including human-annotated and synthetic examples generated by a theorem solver.

Result: A scalable, interpretable dataset (8.8M human-annotated and 7.5M synthetic examples) for probing LLM reasoning.

Conclusion: The dataset enables deeper, transparent investigations into LLMs' algorithmic capabilities and reasoning processes.

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable
reasoning capabilities such as coding and solving mathematical problems to
commonsense inference. While these tasks vary in complexity, they all require
models to integrate and compute over structured information. Despite recent
efforts to reverse-engineer LLM behavior through controlled experiments, our
understanding of how these models internalize and execute complex algorithms
remains limited. Progress has largely been confined to small-scale studies or
shallow tasks such as basic arithmetic and grammatical pattern matching. One
barrier to deeper understanding is the nature of pretraining data -- vast,
heterogeneous, and often poorly annotated, making it difficult to isolate
mechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully
open, complexity-annotated dataset of first-order logic reasoning traces,
designed to probe and analyze algorithmic reasoning in LLMs. The dataset
consists of 3.5 billion tokens, including 8.8 million LLM-augmented,
human-annotated examples and 7.5 million synthetically generated examples. Each
synthetic example is verifiably correct, produced by a custom automated theorem
solver, and accompanied by metadata tracing its algorithmic provenance. We aim
to provide a scalable, interpretable artifact for studying how LLMs learn and
generalize symbolic reasoning processes, paving the way for more transparent
and targeted investigations into the algorithmic capabilities of modern models.

</details>


### [417] [To Be or Not To Be: Vector ontologies as a truly formal ontological framework](https://arxiv.org/pdf/2505.14940)
*Kaspar Rothenfusser*

Main category: cs.AI

TL;DR: The paper critiques existing formal ontologies for not meeting Husserl's criteria of a priori validity and formalism, proposing their reclassification as foundational ontologies. It advocates for true formal ontologies using vector spaces, highlighting their potential for scalable, interoperable information systems and human-machine understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to clarify and reclaim Husserl's original concept of formal ontology, which current so-called formal ontologies fail to meet. The author seeks to demonstrate the practical and theoretical benefits of adhering to Husserl's strict criteria.

Method: The method involves critiquing existing formal ontologies against Husserl's two key notions (a priori validity and formalism) and proposing vector space axioms as a basis for true formal ontologies. Evidence includes their ability to express foundational ontology conceptualizations and their use in AI systems.

Result: The results show that vector-based formal ontologies can effectively capture objective structures without perceptual bias, offering scalability and interoperability. They are also likely already in use in AI and human cognition.

Conclusion: The conclusion advocates for further exploration of vector ontologies as a bridge for human-machine interoperability, enabling mutual understanding between sophisticated machines and humans.

Abstract: Since Edmund Husserl coined the term "Formal Ontologies" in the early 20th
century, a field that identifies itself with this particular branch of sciences
has gained increasing attention. Many authors, and even Husserl himself have
developed what they claim to be formal ontologies. I argue that under close
inspection, none of these so claimed formal ontologies are truly formal in the
Husserlian sense. More concretely, I demonstrate that they violate the two most
important notions of formal ontology as developed in Husserl's Logical
Investigations, namely a priori validity independent of perception and
formalism as the total absence of content. I hence propose repositioning the
work previously understood as formal ontology as the foundational ontology it
really is. This is to recognize the potential of a truly formal ontology in the
Husserlian sense. Specifically, I argue that formal ontology following his
conditions, allows us to formulate ontological structures, which could capture
what is more objectively without presupposing a particular framework arising
from perception. I further argue that the ability to design the formal
structure deliberately allows us to create highly scalable and interoperable
information artifacts. As concrete evidence, I showcase that a class of formal
ontology, which uses the axioms of vector spaces, is able to express most of
the conceptualizations found in foundational ontologies. Most importantly, I
argue that many information systems, specifically artificial intelligence, are
likely already using some type of vector ontologies to represent reality in
their internal worldviews and elaborate on the evidence that humans do as well.
I hence propose a thorough investigation of the ability of vector ontologies to
act as a human-machine interoperable ontological framework that allows us to
understand highly sophisticated machines and machines to understand us.

</details>


### [418] [Reinforcement Learning from User Feedback](https://arxiv.org/pdf/2505.14946)
*Eric Han, Jun Chen, Karthik Abinav Sankararaman, Xiaoliang Peng, Tengyu Xu, Eryk Helenowski, Kaiyan Peng, Mrinal Kumar, Sinong Wang, Han Fang, Arya Talebzadeh*

Main category: cs.AI

TL;DR: RLUF aligns LLMs with user preferences using implicit feedback like emoji reactions, improving positive feedback rates by 28% but requiring careful balancing to avoid reward hacking.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF rely on expert annotators, which may not reflect everyday user preferences. RLUF aims to align LLMs directly with real user signals.

Method: RLUF trains a reward model (P[Love]) to predict positive user feedback (Love Reactions) and integrates it into multi-objective policy optimization alongside helpfulness and safety.

Result: P[Love] predicts increased positive feedback and raises observed positive-feedback rates by 28% in live A/B tests.

Conclusion: RLUF effectively aligns LLMs with user preferences but requires balancing objectives to mitigate reward hacking.

Abstract: As large language models (LLMs) are increasingly deployed in diverse user
facing applications, aligning them with real user preferences becomes
essential. Existing methods like Reinforcement Learning from Human Feedback
(RLHF) rely on expert annotators trained on manually defined guidelines, whose
judgments may not reflect the priorities of everyday users. We introduce
Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs
directly to implicit signals from users in production. RLUF addresses key
challenges of user feedback: user feedback is often binary (e.g., emoji
reactions), sparse, and occasionally adversarial. We train a reward model,
P[Love], to predict the likelihood that an LLM response will receive a Love
Reaction, a lightweight form of positive user feedback, and integrate P[Love]
into a multi-objective policy optimization framework alongside helpfulness and
safety objectives. In large-scale experiments, we show that P[Love] is
predictive of increased positive feedback and serves as a reliable offline
evaluator of future user behavior. Policy optimization using P[Love]
significantly raises observed positive-feedback rates, including a 28% increase
in Love Reactions during live A/B tests. However, optimizing for positive
reactions introduces reward hacking challenges, requiring careful balancing of
objectives. By directly leveraging implicit signals from users, RLUF offers a
path to aligning LLMs with real-world user preferences at scale.

</details>


### [419] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/pdf/2505.14970)
*Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo*

Main category: cs.AI

TL;DR: SEC, an automatic curriculum learning method, improves RL fine-tuning of LLMs by dynamically selecting training problems, outperforming random and heuristic-based curricula.


<details>
  <summary>Details</summary>
Motivation: Current RL fine-tuning curricula (random, heuristic-based, or online filtering) are suboptimal or computationally expensive, necessitating a more efficient and effective method.

Method: SEC formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, using policy gradient advantage as a reward signal and updating the curriculum policy with TD(0).

Result: SEC enhances reasoning abilities in planning, inductive reasoning, and mathematics, improving generalization to harder problems and balancing skills across domains.

Conclusion: SEC is a promising strategy for RL fine-tuning of LLMs, offering automatic and effective curriculum learning.

Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


### [420] [Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility](https://arxiv.org/pdf/2505.14983)
*Zahra Zahedi, Shashank Mehrotra, Teruhisa Misu, Kumar Akash*

Main category: cs.AI

TL;DR: A Dynamic Bayesian Network (DBN) model is developed to infer cognitive states (well-being, trust, intention) of AV users and other road users, integrating this into AV decision-making. The model is refined with interaction study data and extended into a Causal Inference Model (CIM) for balanced, human-centered AV decisions.


<details>
  <summary>Details</summary>
Motivation: To ensure effective and smooth human-AV interactions by aligning human cognitive states with automation decisions.

Method: Develop a DBN to infer cognitive states (well-being, trust, intention) of AV users and road users, refine model parameters with interaction study data, and extend it into a CIM for AV decision-making.

Result: The model accurately predicts user states and guides informed, human-centered AV decisions, balancing user well-being and trust with operational costs.

Conclusion: The proposed DBN and CIM framework effectively enhance human-AV interactions by integrating cognitive state inference into AV decision-making.

Abstract: For future human-autonomous vehicle (AV) interactions to be effective and
smooth, human-aware systems that analyze and align human needs with automation
decisions are essential. Achieving this requires systems that account for human
cognitive states. We present a novel computational model in the form of a
Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV
users and other road users, integrating this information into the AV's
decision-making process. Specifically, our model captures the well-being of
both an AV user and an interacting road user as cognitive states alongside
trust. Our DBN models infer beliefs over the AV user's evolving well-being,
trust, and intention states, as well as the possible well-being of other road
users, based on observed interaction experiences. Using data collected from an
interaction study, we refine the model parameters and empirically assess its
performance. Finally, we extend our model into a causal inference model (CIM)
framework for AV decision-making, enabling the AV to enhance user well-being
and trust while balancing these factors with its own operational costs and the
well-being of interacting road users. Our evaluation demonstrates the model's
effectiveness in accurately predicting user's states and guiding informed,
human-centered AV decisions.

</details>


### [421] [HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning](https://arxiv.org/pdf/2505.15011)
*Kryspin Varys, Federico Cerutti, Adam Sobey, Timothy J. Norman*

Main category: cs.AI

TL;DR: The paper proposes a reinforcement learning method to integrate both explicit (legal/safety) and implicit (social) norms into agent behavior, using reputation to weigh rewards and achieve value-aligned policies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods combining explicit and implicit norm representations in AI agents, ensuring they promote societal values like safety and fairness.

Method: A novel reinforcement learning approach that monitors norm compliance, summarizes it as reputation, and uses this to weigh rewards, encouraging value-aligned behavior.

Result: Experiments, including a traffic problem, show the method successfully integrates norms and finds value-aligned policies, outperforming approaches using only one norm type.

Conclusion: Combining explicit and implicit norms in reinforcement learning is effective for value-aligned AI agents, as demonstrated by the proposed method.

Abstract: Our society is governed by a set of norms which together bring about the
values we cherish such as safety, fairness or trustworthiness. The goal of
value-alignment is to create agents that not only do their tasks but through
their behaviours also promote these values. Many of the norms are written as
laws or rules (legal / safety norms) but even more remain unwritten (social
norms). Furthermore, the techniques used to represent these norms also differ.
Safety / legal norms are often represented explicitly, for example, in some
logical language while social norms are typically learned and remain hidden in
the parameter space of a neural network. There is a lack of approaches in the
literature that could combine these various norm representations into a single
algorithm. We propose a novel method that integrates these norms into the
reinforcement learning process. Our method monitors the agent's compliance with
the given norms and summarizes it in a quantity we call the agent's reputation.
This quantity is used to weigh the received rewards to motivate the agent to
become value-aligned. We carry out a series of experiments including a
continuous state space traffic problem to demonstrate the importance of the
written and unwritten norms and show how our method can find the value-aligned
policies. Furthermore, we carry out ablations to demonstrate why it is better
to combine these two groups of norms rather than using either separately.

</details>


### [422] [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/pdf/2505.15068)
*Cheng Qian, Hongyi Du, Hongru Wang, Xiusi Chen, Yuji Zhang, Avirup Sil, Chengxiang Zhai, Kathleen McKeown, Heng Ji*

Main category: cs.AI

TL;DR: ModelingBench introduces a benchmark for real-world-inspired, open-ended math problems, while ModelingAgent and ModelingJudge provide tools and evaluation for solving such problems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the complexity of real-world problems, which require interdisciplinary reasoning and computational tools.

Method: Developed ModelingBench for diverse domains, ModelingAgent for tool coordination and workflows, and ModelingJudge for expert-like evaluation.

Result: ModelingAgent outperforms baselines and matches human expert solutions.

Conclusion: The framework advances real-world problem-solving in open-ended, interdisciplinary modeling.

Abstract: Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.

</details>


### [423] [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/pdf/2505.15146)
*Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang*

Main category: cs.AI

TL;DR: The paper introduces lmgame-Bench, a framework to evaluate large language models (LLMs) using video games, addressing challenges like brittle perception and data contamination. It shows the benchmark's effectiveness across 13 models and its unique capability to test diverse skills.


<details>
  <summary>Details</summary>
Motivation: Video games test perception, memory, and planning, key skills for LLMs, but current methods fail due to issues like prompt sensitivity and contamination.

Method: Developed lmgame-Bench, a suite of games with a unified API and scaffolds for perception/memory, designed to stabilize evaluations.

Result: The benchmark is challenging, separates models well, and reveals unique capability blends. Reinforcement learning on one game transfers to others and external tasks.

Conclusion: lmgame-Bench provides a reliable, versatile evaluation tool for LLMs, demonstrating transferability and probing diverse skills.

Abstract: Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.

</details>


### [424] [Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge](https://arxiv.org/pdf/2505.15240)
*Yassir Fathullah, Mark J. F. Gales*

Main category: cs.AI

TL;DR: The paper introduces a generalized probabilistic framework for LLM-as-a-judge comparisons, improving uncertainty estimation and efficiency. It reduces needed comparisons by ~50% and combines absolute and comparative scoring for better performance.


<details>
  <summary>Details</summary>
Motivation: To enhance uncertainty estimation and efficiency in LLM-as-a-judge frameworks, addressing limitations of existing methods like Product-of-Experts.

Method: Proposes a broader probabilistic framework, improved uncertainty estimates for individual comparisons, and a method for overall ranking uncertainty. Combines absolute and comparative scoring.

Result: Reduces required comparisons by ~50%, with ranking-level uncertainty metrics identifying low-performing predictions. Specific expert models have limited impact on rankings.

Conclusion: The framework improves efficiency and performance, with uncertainty metrics playing a key role in reducing comparisons and enhancing prediction quality.

Abstract: This paper explores generalised probabilistic modelling and uncertainty
estimation in comparative LLM-as-a-judge frameworks. We show that existing
Product-of-Experts methods are specific cases of a broader framework, enabling
diverse modelling options. Furthermore, we propose improved uncertainty
estimates for individual comparisons, enabling more efficient selection and
achieving strong performance with fewer evaluations. We also introduce a method
for estimating overall ranking uncertainty. Finally, we demonstrate that
combining absolute and comparative scoring improves performance. Experiments
show that the specific expert model has a limited impact on final rankings but
our proposed uncertainty estimates, especially the probability of reordering,
significantly improve the efficiency of systems reducing the number of needed
comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used
to identify low-performing predictions, where the nature of the probabilistic
model has a notable impact on the quality of the overall uncertainty.

</details>


### [425] [Identification of Probabilities of Causation: A Complete Characterization](https://arxiv.org/pdf/2505.15274)
*Xin Shu, Shuai Wang, Ang Li*

Main category: cs.AI

TL;DR: The paper resolves the long-standing gap in characterizing probabilities of causation for multi-valued treatments and outcomes, providing tight bounds and practical relevance.


<details>
  <summary>Details</summary>
Motivation: The theoretical gap in probabilities of causation for multi-valued treatments and outcomes has limited causality-based decision-making.

Method: Proposes a complete set of representative probabilities of causation and derives tight bounds using Structural Causal Models (SCMs) and formal proofs.

Result: The derived bounds and representative probabilities are proven sufficient for characterizing all possible probabilities of causation.

Conclusion: The work bridges a foundational gap, enhancing causality-based decision-making with theoretical and practical contributions.

Abstract: Probabilities of causation are fundamental to modern decision-making. Pearl
first introduced three binary probabilities of causation, and Tian and Pearl
later derived tight bounds for them using Balke's linear programming. The
theoretical characterization of probabilities of causation with multi-valued
treatments and outcomes has remained unresolved for decades, limiting the scope
of causality-based decision-making. In this paper, we resolve this foundational
gap by proposing a complete set of representative probabilities of causation
and proving that they are sufficient to characterize all possible probabilities
of causation within the framework of Structural Causal Models (SCMs). We then
formally derive tight bounds for these representative quantities using formal
mathematical proofs. Finally, we demonstrate the practical relevance of our
results through illustrative toy examples.

</details>


### [426] [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/pdf/2505.15276)
*Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, Wei Hu*

Main category: cs.AI

TL;DR: The study explores how RL-trained LRMs balance efficiency and accuracy by analyzing three thinking modes (NT, ET, IT), revealing trade-offs in output length and accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand inefficiencies in LRMs due to overthinking and identify ways to optimize their reasoning behaviors.

Method: Analyzed confidence in thinking termination, attention shifts, and input focus in RL-trained LRMs under different thinking modes (NT, ET, IT).

Result: NT reduces output length but sacrifices accuracy, while ET and IT maintain accuracy with shorter responses. RL-optimized LRMs show inconsistencies.

Conclusion: Adaptive improvements are needed to enhance the efficiency and reliability of LRMs, addressing the uncovered inconsistencies.

Abstract: Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.

</details>


### [427] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/pdf/2505.15400)
*Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, Xunliang Cai*

Main category: cs.AI

TL;DR: ASRR reduces redundant reasoning in LRMs, cutting costs by up to 32.5% with minimal accuracy loss, while improving safety.


<details>
  <summary>Details</summary>
Motivation: LRMs waste computation on simple tasks due to redundant reasoning. This work aims to optimize efficiency without sacrificing performance.

Method: Proposes ASRR, a framework that adaptively allocates reasoning effort using accuracy-aware length reward regulation, suppressing unnecessary steps.

Result: ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minor accuracy loss (1.2% and 0.6%), and boosts safety rates by up to +21.7%.

Conclusion: ASRR enables efficient, adaptive, and safer reasoning in LRMs, demonstrating significant potential for practical applications.

Abstract: Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [428] [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/pdf/2505.15410)
*Bahar Radmehr, Ekaterina Shved, Fatma Betül Güreş, Adish Singla, Tanja Käser*

Main category: cs.AI

TL;DR: ClickSight, an LLM-based pipeline, interprets student clickstreams to reveal learning strategies, showing potential but varying quality based on prompting methods.


<details>
  <summary>Details</summary>
Motivation: Clickstream data is valuable but hard to interpret due to high dimensionality; prior methods lack generalizability and scalability.

Method: ClickSight uses an LLM pipeline to generate textual interpretations of clickstreams, evaluating four prompting strategies and self-refinement.

Result: LLMs can reasonably interpret strategies, but quality varies by prompting; self-refinement offers limited improvement.

Conclusion: ClickSight highlights LLMs' potential for theory-driven insights from educational data, though prompting strategies need refinement.

Abstract: Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.

</details>


### [429] [Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives](https://arxiv.org/pdf/2505.15693)
*Milad Kazemi, Mateo Perez, Fabio Somenzi, Sadegh Soudjani, Ashutosh Trivedi, Alvaro Velasquez*

Main category: cs.AI

TL;DR: The paper introduces a model-free RL framework for translating absolute liveness specifications (a subclass of omega-regular languages) into average-reward objectives, addressing challenges in infinite-horizon, continuing tasks.


<details>
  <summary>Details</summary>
Motivation: Manually designing reward functions for RL is tedious and error-prone. Omega-regular languages offer a principled alternative, but existing methods misalign with their semantics in infinite-horizon settings.

Method: The framework translates absolute liveness specifications to average-reward objectives, enabling learning in communicating MDPs without episodic resets. It includes lexicographic multi-objective optimization for maximizing satisfaction probability.

Result: Empirical results show the average-reward approach outperforms discount-based methods in continuing settings.

Conclusion: The proposed method effectively aligns omega-regular specifications with RL in infinite-horizon tasks, offering a robust alternative to manual reward design.

Abstract: Recent advances in reinforcement learning (RL) have renewed focus on the
design of reward functions that shape agent behavior. Manually designing reward
functions is tedious and error-prone. A principled alternative is to specify
behaviors in a formal language that can be automatically translated into
rewards. Omega-regular languages are a natural choice for this purpose, given
their established role in formal verification and synthesis. However, existing
methods using omega-regular specifications typically rely on discounted reward
RL in episodic settings, with periodic resets. This setup misaligns with the
semantics of omega-regular specifications, which describe properties over
infinite behavior traces. In such cases, the average reward criterion and the
continuing setting -- where the agent interacts with the environment over a
single, uninterrupted lifetime -- are more appropriate.
  To address the challenges of infinite-horizon, continuing tasks, we focus on
absolute liveness specifications -- a subclass of omega-regular languages that
cannot be violated by any finite behavior prefix, making them well-suited to
the continuing setting. We present the first model-free RL framework that
translates absolute liveness specifications to average-reward objectives. Our
approach enables learning in communicating MDPs without episodic resetting. We
also introduce a reward structure for lexicographic multi-objective
optimization, aiming to maximize an external average-reward objective among the
policies that also maximize the satisfaction probability of a given
omega-regular specification. Our method guarantees convergence in unknown
communicating MDPs and supports on-the-fly reductions that do not require full
knowledge of the environment, thus enabling model-free RL. Empirical results
show our average-reward approach in continuing setting outperforms
discount-based methods across benchmarks.

</details>


### [430] [Neuro-Argumentative Learning with Case-Based Reasoning](https://arxiv.org/pdf/2505.15742)
*Adam Gould, Francesca Toni*

Main category: cs.AI

TL;DR: Gradual AA-CBR is a neurosymbolic model combining neural feature extraction with argumentation debates for interpretable, multi-class classification.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and performance in classification by integrating neural networks with argumentation-based reasoning.

Method: Uses gradient-based learning to train argument strengths and relationships in a debate structure derived from training cases.

Result: Performs comparably to neural networks and outperforms symbolic AA-CBR, with added interpretability and multi-class support.

Conclusion: Gradual AA-CBR offers a balanced approach, combining neural and symbolic methods for better performance and human-aligned reasoning.

Abstract: We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.

</details>


### [431] [On the Evolution of Knowledge Graphs: A Survey and Perspective](https://arxiv.org/pdf/2310.04835)
*Xuhui Jiang, Chengjin Xu, Yinghan Shen, Xun Sun, Lumingyuan Tang, Saizhuo Wang, Zhongwu Chen, Yuanzhuo Wang, Jian Guo*

Main category: cs.AI

TL;DR: A survey on the evolution of knowledge graphs (KGs), their types, extraction/reasoning techniques, applications, and future directions, including integration with large language models.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of KGs, their advancements, and practical uses, highlighting their importance in intelligent applications.

Method: Survey and analysis of various KG types (static, dynamic, temporal, event), extraction/reasoning techniques, and case studies like financial analysis.

Result: Detailed insights into KG evolution, applications, and potential future trends, such as combining KGs with LLMs.

Conclusion: KGs are pivotal in knowledge engineering; future work should explore integrating KGs with LLMs and advancing extraction/reasoning methods.

Abstract: Knowledge graphs (KGs) are structured representations of diversified
knowledge. They are widely used in various intelligent applications. In this
article, we provide a comprehensive survey on the evolution of various types of
knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs)
and techniques for knowledge extraction and reasoning. Furthermore, we
introduce the practical applications of different types of KGs, including a
case study in financial analysis. Finally, we propose our perspective on the
future directions of knowledge engineering, including the potential of
combining the power of knowledge graphs and large language models (LLMs), and
the evolution of knowledge extraction, reasoning, and representation.

</details>


### [432] [NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls](https://arxiv.org/pdf/2409.03797)
*Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, Pavan Kapanipathi*

Main category: cs.AI

TL;DR: The paper introduces NESTFUL, a benchmark for evaluating LLMs on nested API call sequences, highlighting gaps in current evaluation methods and showing GPT-4o's limited accuracy (28%) in this task.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous agents using LLMs for complex tasks has exposed a lack of benchmarks for evaluating nested function calling, a critical capability for real-world applications.

Method: The authors develop NESTFUL, a dataset of 1800+ executable nested API call sequences, and evaluate various LLMs, including GPT-4o, on this benchmark.

Result: GPT-4o achieves only 28% full sequence match accuracy and a 60% win-rate, indicating significant room for improvement in nested sequencing.

Conclusion: NESTFUL serves as a benchmark for tracking progress in nested function calling, with the dataset released under Apache 2.0 to encourage further research.

Abstract: The resurgence of autonomous agents built using large language models (LLMs)
to solve complex real-world tasks has brought increased focus on LLMs'
fundamental ability of tool or function calling. At the core of these agents,
an LLM must plan, execute, and respond using external tools, APIs, and custom
functions. Research on tool calling has gathered momentum, but evaluation
benchmarks and datasets representing the complexity of the tasks have lagged
behind. In this work, we focus on one such complexity, nested sequencing, with
the goal of extending existing benchmarks and evaluation. Specifically, we
present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,
i.e., sequences where the output of one API call is passed as input to a
subsequent call. NESTFUL contains 1800+ nested sequences where all the function
calls are executable. Experimental results on a variety of models show that the
best-performing model (GPT-4o) achieves a full sequence match accuracy of 28%
and a win-rate of 60%, necessitating a large scope for improvement in the
nested sequencing aspect of function calling. Our analysis of these results
provides possible future research directions for the community, in addition to
a benchmark to track progress. We have released the NESTFUL dataset under the
Apache 2.0 license at https://github.com/IBM/NESTFUL.

</details>


### [433] [Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits](https://arxiv.org/pdf/2410.07302)
*Hana Matatov, Marianne Aubin Le Quéré, Ofra Amir, Mor Naaman*

Main category: cs.AI

TL;DR: The study examines the impact of AI-generated content (AIGC) on Reddit art communities, finding minimal direct influence (under 0.5% of posts) but notable shifts in norms and negative reactions to AI use.


<details>
  <summary>Details</summary>
Motivation: To understand how AIGC affects social dynamics, participation, and norms in online art communities, especially comparing those with and without AI content policies.

Method: Analyzed image-based posts and comments in Reddit art communities, focusing on author-labeled AI posts and accusations of AI use.

Result: AI posts were rare (<0.5%), but accusations persisted. AI content attracted newcomers, while negative reactions grew, especially in communities without AI rules.

Conclusion: AIGC's impact is subtle but reveals evolving norms and tensions in creative communities, with policy differences shaping reactions.

Abstract: Broadly accessible generative AI models like Dall-E have made it possible for
anyone to create compelling visual art. In online communities, the introduction
of AI-generated content (AIGC) may impact social dynamics, for example causing
changes in who is posting content, or shifting the norms or the discussions
around the posted content if posts are suspected of being generated by AI. We
take steps towards examining the potential impact of AIGC on art-related
communities on Reddit. We distinguish between communities that disallow AI
content and those without such a direct policy. We look at image-based posts in
these communities where the author transparently shares that the image was
created by AI, and at comments in these communities that suspect or accuse
authors of using generative AI. We find that AI posts (and accusations) have
played a surprisingly small part in these communities through the end of 2023,
accounting for fewer than 0.5% of the image-based posts. However, even as the
absolute number of author-labeled AI posts dwindles over time, accusations of
AI use remain more persistent. We show that AI content is more readily used by
newcomers and may help increase participation if it aligns with community
rules. However, the tone of comments suspecting AI use by others has become
more negative over time, especially in communities that do not have explicit
rules about AI. Overall, the results show the changing norms and interactions
around AIGC in online communities designated for creativity.

</details>


### [434] [Beyond The Rainbow: High Performance Deep Reinforcement Learning on a Desktop PC](https://arxiv.org/pdf/2411.03820)
*Tyler Clark, Mark Towers, Christine Evers, Jonathon Hare*

Main category: cs.AI

TL;DR: The paper introduces 'Beyond The Rainbow' (BTR), an enhanced RL algorithm combining six improvements to Rainbow DQN, achieving state-of-the-art performance on Atari-60 and complex 3D games like Super Mario Galaxy.


<details>
  <summary>Details</summary>
Motivation: To push the boundaries of RL performance by integrating multiple enhancements into Rainbow DQN, enabling efficient training on desktop PCs.

Method: BTR integrates six RL improvements into Rainbow DQN, focusing on computational efficiency and adaptability to complex 3D games.

Result: BTR achieves a human-normalized IQM of 7.4 on Atari-60 and successfully trains agents for 3D games like Super Mario Galaxy with minimal changes.

Conclusion: BTR sets a new benchmark for RL performance, demonstrating scalability and efficiency, with potential for broader applications.

Abstract: Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent
enhancements could significantly boost a reinforcement learning (RL) agent's
performance. In this paper, we present "Beyond The Rainbow" (BTR), a novel
algorithm that integrates six improvements from across the RL literature to
Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,
with a human-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond
Atari, we demonstrate BTR's capability to handle complex 3D games, successfully
training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with
minimal algorithmic changes. Designing BTR with computational efficiency in
mind, agents can be trained using a high-end desktop PC on 200 million Atari
frames within 12 hours. Additionally, we conduct detailed ablation studies of
each component, analyzing the performance and impact using numerous measures.
Code is available at https://github.com/VIPTankz/BTR.

</details>


### [435] [Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models](https://arxiv.org/pdf/2412.00342)
*Nadeen Fathallah, Monika Bhole, Steffen Staab*

Main category: cs.AI

TL;DR: The paper proposes using Large Language Models (LLMs) to improve video captions for the Deaf and Hard of Hearing (DHH) community, showing significant accuracy gains over traditional ASR systems.


<details>
  <summary>Details</summary>
Motivation: The DHH community struggles with poor-quality captions from ASR systems, creating barriers to accessing video content. This work aims to enhance caption accuracy using LLMs.

Method: A novel pipeline integrates LLMs (e.g., GPT-3.5, Llama2-13B) to correct ASR-generated captions, evaluated on a dataset reflecting real-world DHH challenges.

Result: LLM-enhanced captions reduce Word Error Rate (WER) by ~57.72%, with ChatGPT-3.5 achieving a WER of 9.75% vs. 23.07% for original ASR captions.

Conclusion: LLMs significantly improve caption quality, offering a viable solution to enhance accessibility for the DHH community.

Abstract: In today's digital age, video content is prevalent, serving as a primary
source of information, education, and entertainment. However, the Deaf and Hard
of Hearing (DHH) community often faces significant challenges in accessing
video content due to the inadequacy of automatic speech recognition (ASR)
systems in providing accurate and reliable captions. This paper addresses the
urgent need to improve video caption quality by leveraging Large Language
Models (LLMs). We present a comprehensive study that explores the integration
of LLMs to enhance the accuracy and context-awareness of captions generated by
ASR systems. Our methodology involves a novel pipeline that corrects
ASR-generated captions using advanced LLMs. It explicitly focuses on models
like GPT-3.5 and Llama2-13B due to their robust performance in language
comprehension and generation tasks. We introduce a dataset representative of
real-world challenges the DHH community faces to evaluate our proposed
pipeline. Our results indicate that LLM-enhanced captions significantly improve
accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by
ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%),
ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the
original ASR captions.

</details>


### [436] [NeSyA: Neurosymbolic Automata](https://arxiv.org/pdf/2412.07331)
*Nikolaos Manginas, George Paliouras, Luc De Raedt*

Main category: cs.AI

TL;DR: NeSyA integrates symbolic automata with neural perception for temporal tasks, outperforming previous NeSy systems in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Address the gap in NeSy AI for sequential/temporal problems by leveraging symbolic automata for combined temporal and static reasoning.

Method: Propose NeSyA, a hybrid model combining symbolic automata with neural-based perception under probabilistic semantics for end-to-end differentiability.

Result: NeSyA scales better and achieves higher accuracy than prior NeSy systems in synthetic benchmarks and improves generalization in real-world event recognition.

Conclusion: Symbolic automata are effective for temporal NeSy tasks, with NeSyA demonstrating superior performance and generalization over existing approaches.

Abstract: Neurosymbolic (NeSy) AI has emerged as a promising direction to integrate
neural and symbolic reasoning. Unfortunately, little effort has been given to
developing NeSy systems tailored to sequential/temporal problems. We identify
symbolic automata (which combine the power of automata for temporal reasoning
with that of propositional logic for static reasoning) as a suitable formalism
for expressing knowledge in temporal domains. Focusing on the task of sequence
classification and tagging we show that symbolic automata can be integrated
with neural-based perception, under probabilistic semantics towards an
end-to-end differentiable model. Our proposed hybrid model, termed NeSyA (Neuro
Symbolic Automata) is shown to either scale or perform more accurately than
previous NeSy systems in a synthetic benchmark and to provide benefits in terms
of generalization compared to purely neural systems in a real-world event
recognition task.

</details>


### [437] [Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models](https://arxiv.org/pdf/2412.18084)
*Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu*

Main category: cs.AI

TL;DR: PEIT framework enhances LLMs for molecular tasks using multimodal pre-training and fine-tuning, outperforming existing models in molecule captioning and multi-constraint generation.


<details>
  <summary>Details</summary>
Motivation: Limited performance of LLMs in molecule generation due to lack of labeled data and multi-property constraints.

Method: Two-step framework: (1) Pre-train PEIT-GEN with multimodal inputs (text, SMILES, properties) to align representations and synthesize data. (2) Fine-tune LLMs (PEIT-LLM) for tasks like molecule captioning and multi-constraint generation.

Result: PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning. PEIT-LLM improves multi-task molecule generation.

Conclusion: PEIT framework effectively scales for diverse molecular tasks, with released code and data.

Abstract: Large language models (LLMs) are widely applied in various natural language
processing tasks such as question answering and machine translation. However,
due to the lack of labeled data and the difficulty of manual annotation for
biochemical properties, the performance for molecule generation tasks is still
limited, especially for tasks involving multi-properties constraints. In this
work, we present a two-step framework PEIT (Property Enhanced Instruction
Tuning) to improve LLMs for molecular-related tasks. In the first step, we use
textual descriptions, SMILES, and biochemical properties as multimodal inputs
to pre-train a model called PEIT-GEN, by aligning multi-modal representations
to synthesize instruction data. In the second step, we fine-tune existing
open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle
molecule captioning, text-based molecule generation, molecular property
prediction, and our newly proposed multi-constraint molecule generation tasks.
Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and
BioT5 in molecule captioning, demonstrating modalities align well between
textual descriptions, structures, and biochemical properties. Furthermore,
PEIT-LLM shows promising improvements in multi-task molecule generation,
proving the scalability of the PEIT framework for various molecular tasks. We
release the code, constructed instruction data, and model checkpoints in
https://github.com/chenlong164/PEIT.

</details>


### [438] ["Did my figure do justice to the answer?" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)](https://arxiv.org/pdf/2412.19755)
*Pritam Sil, Pushpak Bhattacharyya*

Main category: cs.AI

TL;DR: The paper introduces MMSAF, a multimodal short answer grading problem, and a dataset of 2,197 data points to address scalable assessment challenges. Existing MLLMs show moderate accuracy (55-75%) in grading and relevance prediction, with Pixtral and ChatGPT aligning well with human judgment in specific subjects.


<details>
  <summary>Details</summary>
Motivation: Assessments with open-ended responses are hard to grade at scale due to their multimodal nature (text and visuals). The need for scalable tools led to the MMSAF problem and dataset.

Method: Proposed MMSAF problem, created a dataset of 2,197 multimodal answers, and developed an automated framework for dataset generation. Evaluated existing MLLMs for grading and relevance prediction.

Result: MLLMs achieved 55% accuracy in grading correctness and 75% in image relevance prediction. Pixtral and ChatGPT aligned well with human judgment in biology and physics/chemistry, respectively.

Conclusion: The MMSAF framework and dataset facilitate research in scalable multimodal assessments, though MLLMs need improvement for higher accuracy.

Abstract: Assessments play a vital role in a student's learning process. This is
because they provide valuable feedback crucial to a student's growth. Such
assessments contain questions with open-ended responses, which are difficult to
grade at scale. These responses often require students to express their
understanding through textual and visual elements together as a unit. In order
to develop scalable assessment tools for such questions, one needs multimodal
LLMs having strong comparative reasoning capabilities across multiple
modalities. Thus, to facilitate research in this area, we propose the
Multimodal Short Answer grading with Feedback (MMSAF) problem along with a
dataset of 2,197 data points. Additionally, we provide an automated framework
for generating such datasets. As per our evaluations, existing Multimodal Large
Language Models (MLLMs) could predict whether an answer is correct, incorrect
or partially correct with an accuracy of 55%. Similarly, they could predict
whether the image provided in the student's answer is relevant or not with an
accuracy of 75%. As per human experts, Pixtral was more aligned towards human
judgement and values for biology and ChatGPT for physics and chemistry and
achieved a score of 4 or more out of 5 in most parameters.

</details>


### [439] [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/pdf/2502.02145)
*Yuan Gao, Mattia Piccinini, Korbinian Moller, Amr Alanwar, Johannes Betz*

Main category: cs.AI

TL;DR: The paper proposes using LLMs with structured parsing and prompt engineering to automate the evaluation and generation of safety-critical driving scenarios, reducing reliance on handcrafted methods.


<details>
  <summary>Details</summary>
Motivation: Current scenario-based testing for autonomous vehicles relies on handcrafted scenarios, which are limited in scalability and require significant human effort.

Method: Combines LLMs with structured scenario parsing and prompt engineering, introducing Cartesian and Ego-centric prompts for evaluation and an adversarial generation module for creating critical scenarios.

Result: The evaluation module detects collisions and assesses safety, while the generation module identifies high-risk agents and synthesizes realistic scenarios.

Conclusion: LLMs with domain-informed prompting can effectively evaluate and generate safety-critical scenarios, reducing dependence on handcrafted metrics.

Abstract: Ensuring the safety of autonomous vehicles requires virtual scenario-based
testing, which depends on the robust evaluation and generation of
safety-critical scenarios. So far, researchers have used scenario-based testing
frameworks that rely heavily on handcrafted scenarios as safety metrics. To
reduce the effort of human interpretation and overcome the limited scalability
of these approaches, we combine Large Language Models (LLMs) with structured
scenario parsing and prompt engineering to automatically evaluate and generate
safety-critical driving scenarios. We introduce Cartesian and Ego-centric
prompt strategies for scenario evaluation, and an adversarial generation module
that modifies trajectories of risk-inducing vehicles (ego-attackers) to create
critical scenarios. We validate our approach using a 2D simulation framework
and multiple pre-trained LLMs. The results show that the evaluation module
effectively detects collision scenarios and infers scenario safety. Meanwhile,
the new generation module identifies high-risk agents and synthesizes
realistic, safety-critical scenarios. We conclude that an LLM equipped with
domain-informed prompting techniques can effectively evaluate and generate
safety-critical driving scenarios, reducing dependence on handcrafted metrics.
We release our open-source code and scenarios at:
https://github.com/TUM-AVS/From-Words-to-Collisions.

</details>


### [440] [VRoPE: Rotary Position Embedding for Video Large Language Models](https://arxiv.org/pdf/2502.11664)
*Zikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, Jing Liu*

Main category: cs.AI

TL;DR: VRoPE improves video-based LLMs by addressing positional bias and video-text transition issues in RoPE-3D.


<details>
  <summary>Details</summary>
Motivation: Extending RoPE to video is challenging due to spatiotemporal complexity; existing methods like RoPE-3D have limitations in attention bias and transitions.

Method: VRoPE introduces balanced encoding to reduce attention bias and restructures positional indices for smoother video-text transitions.

Result: VRoPE outperforms RoPE variants in video understanding, temporal reasoning, and retrieval tasks.

Conclusion: VRoPE is a superior positional encoding method for Video-LLMs, with demonstrated effectiveness and open-source availability.

Abstract: Rotary Position Embedding (RoPE) has shown strong performance in text-based
Large Language Models (LLMs), but extending it to video remains a challenge due
to the intricate spatiotemporal structure of video frames. Existing
adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions
separately but suffer from two major limitations: positional bias in attention
distribution and disruptions in video-text transitions. To overcome these
issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional
encoding method tailored for Video-LLMs. Specifically, we introduce a more
balanced encoding strategy that mitigates attention biases, ensuring a more
uniform distribution of spatial focus. Additionally, our approach restructures
positional indices to ensure a smooth transition between video and text tokens.
Extensive experiments on different models demonstrate that VRoPE consistently
outperforms previous RoPE variants, achieving significant improvements in video
understanding, temporal reasoning, and retrieval tasks. Code will be available
at https://github.com/johncaged/VRoPE.

</details>


### [441] [Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM Reasoning](https://arxiv.org/pdf/2502.17216)
*Alexander Beiser, David Penz, Nysret Musliu*

Main category: cs.AI

TL;DR: The paper investigates the impact of formal language choice on Neurosymbolic LLM reasoning, identifying it as a key factor for syntactic and semantic reasoning capabilities. It introduces the 'intermediate language challenge' and evaluates the effects of in-context-learning examples.


<details>
  <summary>Details</summary>
Motivation: To understand the contributing factors to the success of Neurosymbolic LLM reasoning, particularly the role of formal language choice and in-context-learning examples.

Method: Comparison of 4 formal languages on 3 datasets using 6 LLMs, along with an ablation study on in-context-learning examples.

Result: The choice of formal language significantly affects reasoning capabilities. Context-aware encodings improve reasoning, while comments and markdown syntax have no clear impact.

Conclusion: Formal language selection is crucial for Neurosymbolic LLM reasoning, and context-aware encodings enhance performance, but other syntactic additions like comments or markdown do not.

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, it remains unclear what the contributing factors to the success
of Neurosymbolic LLM reasoning are. This paper shows that one important factor
is the choice of the formal language. By comparing 4 formal languages on 3
datasets over 6 LLMs, we show that the choice of formal language affects both
the syntactic and the semantic reasoning capability. Thereby, we introduce the
intermediate language challenge, which is the challenge of picking a suitable
formal language for neurosymbolic reasoning. Further, we compare the effects of
using different in-context-learning examples in an ablation study. We conclude
that on average, context-aware encodings help LLMs to reason, while there is no
apparent effect of using comments or markdown syntax.

</details>


### [442] [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/pdf/2502.18744)
*Jeesu Jung, Chanjun Park, Sangkeun Jung*

Main category: cs.AI

TL;DR: ZEBRA is a zero-annotation framework for LLM alignment, using model behavior knowledge from benchmarks to create preference data, avoiding costly instance-wise supervision.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods rely on expensive and less interpretable instance-wise supervision, prompting the need for a scalable, cost-effective alternative.

Method: ZEBRA binarizes response pairs by evaluating model quality and similarity from benchmark performances, eliminating instance-level annotation.

Result: ZEBRA achieves alignment performance comparable to instance-supervised methods without manual or model-based labeling.

Conclusion: ZEBRA offers a scalable, controllable, and cost-effective solution for LLM alignment, reducing reliance on annotation.

Abstract: Recent efforts in LLM alignment have focused on constructing large-scale
preference datasets via human or Artificial Intelligence (AI) annotators.
However, such approaches rely on instance-wise supervision, incurring
substantial annotation cost and limited interpretability. In this paper, we
propose ZEBRA - a model behavior-wise zero-annotation framework that constructs
preference data by leveraging model behavior knowledge derived from benchmark
performances. ZEBRA binarizes response pairs by evaluating the quality and
similarity of their origin models, entirely bypassing instance-level
annotation. This allows scalable, controllable, and cost-effective alignment
data generation. Empirical results show that ZEBRA achieves alignment
performance comparable to instance-supervised methods, despite requiring no
manual or model-based labeling.

</details>


### [443] [MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts](https://arxiv.org/pdf/2502.20808)
*Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, Cheng-Lin Liu*

Main category: cs.AI

TL;DR: MV-MATH is a new dataset for evaluating Multimodal Large Language Models (MLLMs) in multi-visual math tasks, highlighting their challenges and performance gaps compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack multi-visual contexts, which are common in real-world math applications, necessitating a more comprehensive dataset.

Method: The authors introduce MV-MATH, a dataset of 2,009 multi-image math problems from K-12 scenarios, with detailed annotations and diverse question types.

Result: MLLMs perform poorly on multi-visual math tasks, showing a significant gap compared to human performance.

Conclusion: MV-MATH serves as a rigorous benchmark, revealing MLLMs' limitations and providing insights for improving their multi-visual reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have shown promising capabilities in
mathematical reasoning within visual contexts across various datasets. However,
most existing multimodal math benchmarks are limited to single-visual contexts,
which diverges from the multi-visual scenarios commonly encountered in
real-world mathematical applications. To address this gap, we introduce
MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical
problems. Each problem integrates multiple images interleaved with text,
derived from authentic K-12 scenarios, and enriched with detailed annotations.
MV-MATH includes multiple-choice, free-form, and multi-step questions, covering
11 subject areas across 3 difficulty levels, and serves as a comprehensive and
rigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual
contexts. Through extensive experimentation, we observe that MLLMs encounter
substantial challenges in multi-visual math tasks, with a considerable
performance gap relative to human capabilities on MV-MATH. Furthermore, we
analyze the performance and error patterns of various models, providing
insights into MLLMs' mathematical reasoning capabilities within multi-visual
settings.

</details>


### [444] [SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic](https://arxiv.org/pdf/2503.07996)
*Jikai Chen, Leilei Gan, Ziyu Zhao, Zechuan Wang, Dong Wang, Chenyi Zhuang*

Main category: cs.AI

TL;DR: The paper introduces SQLCriticBench and a clause-wise critique generation method to improve LLM-based Text-to-SQL systems, addressing syntax and semantic errors. It uses adaptive DPO for training and a cost-effective dataset pipeline, showing significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Existing refinement methods in LLM-based Text-to-SQL systems are ineffective, often introducing new errors and failing to correct semantic inaccuracies.

Method: Proposes a clause-wise critique generation task with SQLCriticBench, adaptive DPO training, and an automated dataset curation pipeline.

Result: SQLCritic model improves SQL accuracy on BIRD and Spider datasets and outperforms existing models in critique capabilities.

Conclusion: The approach effectively addresses limitations in current refinement methods, enhancing both accuracy and critique quality in Text-to-SQL systems.

Abstract: Existing refinement methods in LLM-based Text-to-SQL systems exhibit limited
effectiveness. They often introduce new errors during the self-correction
process and fail to detect and correct semantic inaccuracies. To address these
gaps, we first introduce a clause-wise critique generation task along with a
benchmark, SQLCriticBench, which performs fine-grained error localization
including both syntax and semantic errors at the clause level. Furthermore, we
introduce a variant of DPO for training our SQLCritic model, where the $\beta$
coefficient is adaptively changed according to the clause-level inconsistencies
between the preferred and dispreferred critiques. We also propose an
automatically training dataset curation pipeline which annotate clause-wise
critique at scale in a cost-effective way. Experiments demonstrate that the
SQLCritic model significantly improves SQL accuracy on the BIRD and Spider
datasets, and the results on SQLCriticBench further reveals its superior
critique capabilities compared to existing models.

</details>


### [445] [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/pdf/2503.10619)
*Andy Zhou, Ron Arel*

Main category: cs.AI

TL;DR: Tempest is a multi-turn adversarial framework that erodes LLM safety by exploiting incremental policy leaks through tree search, achieving high success rates on GPT models.


<details>
  <summary>Details</summary>
Motivation: To understand and demonstrate how minor concessions in LLM responses can accumulate into fully disallowed outputs over multiple turns, highlighting the need for robust multi-turn testing.

Method: Uses a breadth-first tree search to expand adversarial prompts at each turn, tracking and re-injecting policy leaks into subsequent queries.

Result: Achieves 100% success on GPT-3.5-turbo and 97% on GPT-4, outperforming baselines like Crescendo and GOAT with fewer queries.

Conclusion: Tempest reveals the gradual degradation of LLM safeguards, emphasizing the necessity for stronger multi-turn testing procedures.

Abstract: We introduce Tempest, a multi-turn adversarial framework that models the
gradual erosion of Large Language Model (LLM) safety through a tree search
perspective. Unlike single-turn jailbreaks that rely on one meticulously
engineered prompt, Tempest expands the conversation at each turn in a
breadth-first fashion, branching out multiple adversarial prompts that exploit
partial compliance from previous responses. By tracking these incremental
policy leaks and re-injecting them into subsequent queries, Tempest reveals how
minor concessions can accumulate into fully disallowed outputs. Evaluations on
the JailbreakBench dataset show that Tempest achieves a 100% success rate on
GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries
than baselines such as Crescendo or GOAT. This tree search methodology offers
an in-depth view of how model safeguards degrade over successive dialogue
turns, underscoring the urgency of robust multi-turn testing procedures for
language models.

</details>


### [446] [Systematic Parameter Decision in Approximate Model Counting](https://arxiv.org/pdf/2504.05874)
*Jinping Lei, Toru Takisaka, Junqiang Peng, Mingyu Xiao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes a novel approach to determining the internal parameters
of the hashing-based approximate model counting algorithm $\mathsf{ApproxMC}$.
In this problem, the chosen parameter values must ensure that
$\mathsf{ApproxMC}$ is Probably Approximately Correct (PAC), while also making
it as efficient as possible. The existing approach to this problem relies on
heuristics; in this paper, we solve this problem by formulating it as an
optimization problem that arises from generalizing $\mathsf{ApproxMC}$'s
correctness proof to arbitrary parameter values.
  Our approach separates the concerns of algorithm soundness and optimality,
allowing us to address the former without the need for repetitive case-by-case
argumentation, while establishing a clear framework for the latter.
Furthermore, after reduction, the resulting optimization problem takes on an
exceptionally simple form, enabling the use of a basic search algorithm and
providing insight into how parameter values affect algorithm performance.
Experimental results demonstrate that our optimized parameters improve the
runtime performance of the latest $\mathsf{ApproxMC}$ by a factor of 1.6 to
2.4, depending on the error tolerance.

</details>


### [447] [Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection](https://arxiv.org/pdf/2504.09440)
*MingShan Liu, Shi Bo, Jialing Fang*

Main category: cs.AI

TL;DR: A structured self-consistency framework improves LLMs' mathematical reasoning by enforcing consistency in intermediate steps and final outputs, reducing hallucinations and enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit strong mathematical reasoning but suffer from hallucinations and logical inconsistencies, especially in theorem proving, symbolic manipulation, and numerical computation.

Method: Introduces a structured self-consistency framework to enforce consistency across intermediate reasoning steps and final outputs.

Result: Significant improvements in proof validity, symbolic reasoning accuracy, and numerical stability, with reduced output variance.

Conclusion: Structured self-consistency enhances LLMs' mathematical reasoning, making AI-driven mathematics more reliable and interpretable.

Abstract: Large language models (LLMs) have demonstrated strong mathematical reasoning
capabilities but remain susceptible to hallucinations producing plausible yet
incorrect statements especially in theorem proving, symbolic manipulation, and
numerical computation. While self-consistency (SC) has been explored as a means
to improve factuality in LLMs, existing approaches primarily apply SC to
final-answer selection, neglecting the logical consistency of intermediate
reasoning steps. In this work, we introduce a structured self-consistency
framework designed to enhance the reliability of mathematical reasoning. Our
method enforces self-consistency across intermediate steps and final outputs,
reducing logical inconsistencies and hallucinations. We evaluate our approach
across three core mathematical tasks: theorem proving, symbolic transformation,
and numerical computation. Experimental results demonstrate that SC
significantly improves proof validity, symbolic reasoning accuracy, and
numerical stability while maintaining computational efficiency. Further
analysis reveals that structured self-consistency not only enhances
problem-solving accuracy but also reduces the variance of model-generated
outputs. These findings highlight self-consistency as a robust mechanism for
improving mathematical reasoning in LLMs, paving the way for more reliable and
interpretable AI-driven mathematics.

</details>


### [448] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/pdf/2504.14298)
*Xiucheng Wang, Zhongsheng Fang, Nan Cheng, Ruijin Sun, Zan Li, Xuemin, Shen*

Main category: cs.AI

TL;DR: The paper introduces RadioDiff-Inverse, a diffusion-enhanced Bayesian framework for constructing radio maps (RMs) using sparse, noisy measurements and coarse environmental data, achieving high accuracy without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing RM construction methods require precise environmental data and BS locations, which are often unavailable. The impact of noise in sparse measurements on RM accuracy is also unclear.

Method: The paper formulates RM construction as a Bayesian inverse problem, proposing RadioDiff-Inverse, which uses an unconditional generative diffusion model to learn the RM prior, enabling environmental structure perception.

Result: RadioDiff-Inverse achieves state-of-the-art accuracy in RM construction and environmental reconstruction, with robustness against noisy sparse sampling.

Conclusion: The proposed framework is training-free, leveraging pre-trained models, reducing costs, and improving RM construction in dynamic or privacy-sensitive environments.

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>


### [449] [An Empirical Study of LLM Reasoning Ability Under Strict Output Length Constraint](https://arxiv.org/pdf/2504.14350)
*Yi Sun, Han Wang, Jiaqiang Li, Jiacheng Liu, Xiangyu Li, Hao Wen, Yizhen Yuan, Huiwen Zheng, Yan Liang, Yuanchun Li, Yunxin Liu*

Main category: cs.AI

TL;DR: The paper investigates how Large Language Models (LLMs) perform under strict output length constraints, revealing that optimal model choices (e.g., size, prompt style) vary with budget limits.


<details>
  <summary>Details</summary>
Motivation: To understand if and how LLMs' reasoning abilities remain effective under real-world time and output length constraints.

Method: Empirical study testing 30 LLMs on reasoning datasets under varying output length budgets, analyzing accuracy correlations with model properties and latency.

Result: Findings show budget-aware reasoning differs from unconstrained scenarios, with optimal model choices shifting under different budgets.

Conclusion: The study provides practical guidance for deploying LLMs under latency constraints and highlights the need for budget-aware evaluations.

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making models think before answering,
they are able to achieve much higher accuracy with extra inference computation.
However, in many real-world scenarios, models are used under time constraints,
where an answer should be given within a certain output length. It is unclear
whether and how the reasoning ability of different LLMs remain effective under
strict constraints. We take a first look at this problem by conducting an
in-depth empirical study. Specifically, we test 30 LLMs on common reasoning
datasets under a wide range of output length budgets, and we analyze the
correlation between the inference accuracy and various properties including
model type, model size, prompt style, etc. We also consider the mappings
between token budgets and actual on-device latency budgets. The results have
demonstrated several interesting findings regarding the budget-aware LLM
reasoning ability that differ from the unconstrained situation, e.g. the
optimal choices of either model size or prompt style change under different
budgets. These findings offer timely evaluation to this area and practical
guidance for users to deploy LLMs under real-world latency constraints.

</details>


### [450] [AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning](https://arxiv.org/pdf/2504.14858)
*Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, Siqi Sun*

Main category: cs.AI

TL;DR: AlignRAG addresses reasoning misalignment in RAG systems by introducing a Critique-Driven Alignment framework with a retrieval-augmented Critic Language Model (CLM), improving factual reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Standard RAG pipelines often fail to ensure consistency between model reasoning and retrieved evidence, leading to factual inconsistencies.

Method: Proposes AlignRAG, an iterative framework with a contrastive critique synthesis mechanism and a retrieval-augmented CLM trained to detect and revise reasoning errors.

Result: Empirical evaluations show a 12.1% improvement over Self-Refine and outperforms a standard 72B-parameter CLM by 2.2%.

Conclusion: AlignRAG enhances factual reliability and robustness of RAG systems by aligning model reasoning with retrieved evidence.

Abstract: Retrieval-augmented generation (RAG) has become a widely adopted paradigm for
enabling knowledge-grounded large language models (LLMs). However, standard RAG
pipelines often fail to ensure that model reasoning remains consistent with the
evidence retrieved, leading to factual inconsistencies or unsupported
conclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning
and identify a central but underexplored problem: \textit{Reasoning
Misalignment}-the divergence between an LLM's internal reasoning trajectory and
the evidential constraints provided by retrieval. To address this issue, we
propose \textsc{AlignRAG}, a novel iterative framework grounded in
Critique-Driven Alignment (CDA). At the heart of \textsc{AlignRAG} lies a
\textit{contrastive critique synthesis} mechanism that generates
retrieval-sensitive critiques while mitigating self-bias. This mechanism trains
a dedicated retrieval-augmented \textit{Critic Language Model (CLM)} using
labeled critiques that distinguish between evidence-aligned and misaligned
reasoning. Alignment signals for supervision are obtained through
self-supervised or externally guided labeling strategies. The resulting CLM is
explicitly optimized for evidence sensitivity, enabling it to detect and revise
reasoning errors during inference without relying solely on self-generated
feedback. Empirical evaluations show that our 8B-parameter CLM improves
performance over the Self-Refine baseline by 12.1\% on out-of-domain tasks and
outperforms a standard 72B-parameter CLM by 2.2\%, while remaining compatible
with existing RAG architectures as a plug-and-play module. Overall, AlignRAG
offers a principled solution for aligning model reasoning with retrieved
evidence, substantially improving the factual reliability and robustness of RAG
systems.

</details>


### [451] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/pdf/2504.17531)
*Justus Flerlage, Ilja Behnke, Odej Kao*

Main category: cs.AI

TL;DR: The paper explores using LLMs like GPT-4o-mini to generate and execute workflows from user intentions, showing feasibility and proficiency in code-oriented tasks.


<details>
  <summary>Details</summary>
Motivation: Reassess user-device interaction due to AI advancements, enabling intent resolution via model-generated code for hybrid workflows.

Method: Prompt LLM with user intentions and a simplified API for a GUI-less OS, analyze generated code and execution.

Result: Demonstrates feasibility; GPT-4o-mini excels in generating code workflows aligned with user intentions.

Conclusion: LLMs can effectively bridge user intentions and implementation, enhancing hybrid workflows.

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code. This development represents a significant progression in
the realm of hybrid workflows, where human and artificial intelligence
collaborate to address user intentions, with the former responsible for
defining these intentions and the latter for implementing the solutions to
address them. In this paper, we investigate the feasibility of generating and
executing workflows through code generation that results from prompting an LLM
with a concrete user intention, and a simplified application programming
interface for a GUI-less operating system. We provide an in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate the general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [452] [Transformational Creativity in Science: A Graphical Theory](https://arxiv.org/pdf/2504.18687)
*Samuel Schapiro, Jonah Black, Lav R. Varshney*

Main category: cs.AI

TL;DR: A graphical theory of transformational scientific creativity is proposed, linking Boden's enabling constraints and Kuhn's paradigm shifts, with proofs on axiom modifications' transformative potential.


<details>
  <summary>Details</summary>
Motivation: To synthesize Boden's and Kuhn's theories into a graphical model for understanding transformational creativity in science.

Method: Develop a graphical model, analyze axiom modifications, and apply it to historical cases of transformational creativity.

Result: Proved that axiom changes have the highest transformative potential; historical cases fit the framework.

Conclusion: The graphical model effectively captures transformational creativity, aligning with theoretical and historical insights.

Abstract: Creative processes are typically divided into three types: combinatorial,
exploratory, and transformational. Here, we provide a graphical theory of
transformational scientific creativity, synthesizing Boden's insight that
transformational creativity arises from changes in the "enabling constraints"
of a conceptual space and Kuhn's structure of scientific revolutions as
resulting from paradigm shifts. We prove that modifications made to axioms of
our graphical model have the most transformative potential and then illustrate
how several historical instances of transformational creativity can be captured
by our framework.

</details>


### [453] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/pdf/2504.20930)
*Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie*

Main category: cs.AI

TL;DR: ChestX-Reasoner, a radiology diagnosis MLLM, improves diagnostic accuracy and reasoning by leveraging clinical report-derived reasoning chains and a two-stage training framework.


<details>
  <summary>Details</summary>
Motivation: Medical AI models often lack structured reasoning processes from clinical practice, which ChestX-Reasoner addresses by mimicking radiologists' step-by-step reasoning.

Method: The model uses a two-stage training framework (supervised fine-tuning and reinforcement learning) with process rewards, and is evaluated on RadRBench-CXR, a benchmark with 59K samples.

Result: ChestX-Reasoner outperforms existing MLLMs, achieving 16%, 5.9%, and 18% improvements in reasoning ability and 3.3%, 24%, and 27% in diagnostic accuracy.

Conclusion: The model advances medical reasoning in AI, with open-sourced resources to support further research.

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>


### [454] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/pdf/2504.21277)
*Guanghao Zhou, Panjia Qiu, Cen Chen, Jie Wang, Zheming Yang, Jian Xu, Minghui Qiu*

Main category: cs.AI

TL;DR: A review of RL-based reasoning for MLLMs, covering methods, rewards, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: Enhancing MLLMs' reasoning across modalities using RL, addressing challenges like sparse rewards and cross-modal inefficiency.

Method: Systematic review of RL paradigms (value-model-free and value-model-based), reward mechanisms, and reasoning trajectory optimization.

Result: Identifies key algorithmic designs, benchmarks, and limitations, proposing future research directions.

Conclusion: Provides a structured guide to RL-based multimodal reasoning, aiming to advance the field.

Abstract: The application of reinforcement learning (RL) to enhance the reasoning
capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly
advancing research area. While MLLMs extend Large Language Models (LLMs) to
handle diverse modalities such as vision, audio, and video, enabling robust
reasoning across multimodal inputs remains challenging. This paper provides a
systematic review of recent advances in RL-based reasoning for MLLMs, covering
key algorithmic designs, reward mechanism innovations, and practical
applications. We highlight two main RL paradigms, value-model-free and
value-model-based methods, and analyze how RL enhances reasoning abilities by
optimizing reasoning trajectories and aligning multimodal information.
Additionally, we provide an extensive overview of benchmark datasets,
evaluation protocols, and current limitations, and propose future research
directions to address challenges such as sparse rewards, inefficient
cross-modal reasoning, and real-world deployment constraints. Our goal is to
provide a comprehensive and structured guide to RL-based multimodal reasoning.

</details>


### [455] [Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/pdf/2504.21659)
*Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, Li Shen*

Main category: cs.AI

TL;DR: The paper introduces Ada-R1, a two-stage framework for adaptive and efficient reasoning in large language models, reducing inference costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models incur high inference overhead, and the benefits of long reasoning paths vary by problem. This motivates adaptive strategies to tailor reasoning depth.

Method: A hybrid reasoning model merges long and short CoT models, followed by bi-level preference training to select and optimize reasoning styles.

Result: Ada-R1 reduces reasoning length by over 50% on five mathematical datasets while maintaining accuracy, significantly cutting inference costs.

Conclusion: Adaptive reasoning strategies like Ada-R1 can optimize efficiency in large language models without sacrificing performance.

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
(Ada-R1) significantly reduces inference costs compared to other baseline
approaches, while maintaining performance. Notably, on five mathematical
datasets, the average length of reasoning is reduced by more than 50%,
highlighting the potential of adaptive strategies to optimize reasoning
efficiency in large language models. Our code is coming soon at
https://github.com/StarDewXXX/AdaR1

</details>


### [456] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/pdf/2505.13887)
*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V uses video to automate mobile task management, reducing manual effort and improving performance by 36%.


<details>
  <summary>Details</summary>
Motivation: The rise in mobile device usage requires better automation, but current AI frameworks lack operational expertise. Manual knowledge injection is inefficient.

Method: Mobile-Agent-V leverages video content to inject operational knowledge into automation, eliminating manual intervention.

Result: Mobile-Agent-V improves performance by 36% over existing methods, as validated by the Mobile-Knowledge benchmark.

Conclusion: Mobile-Agent-V offers an effortless and efficient solution for mobile automation, outperforming traditional approaches.

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [457] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/pdf/2505.14147)
*Xiong Jun Wu, Zhenduo Zhang, ZuJie Wen, Zhiqiang Zhang, Wang Ren, Lei Shi, Cai Chen, Deng Zhao, Dingnan Jin, Qing Cui, Jun Zhou*

Main category: cs.AI

TL;DR: SHARP is a method for synthesizing high-quality STEM problems for training large reasoning models, outperforming existing methods by ensuring verifiable, diverse, and challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating STEM problem sets are limited in quality and verifiability, hindering the advancement of large reasoning models in complex tasks.

Method: SHARP uses a three-phase framework (Alignment, Instantiation, Inference) and self-alignment principles to generate diverse, verifiable STEM problems, leveraging reinforcement learning with verifiable rewards.

Result: SHARP-augmented training significantly improves reasoning accuracy on benchmarks like GPQA, approaching expert-level performance.

Conclusion: SHARP provides an effective strategy and framework for enhancing large reasoning models' capabilities in STEM domains.

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [458] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/pdf/2505.14604)
*Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang*

Main category: cs.AI

TL;DR: The paper introduces Self-Braking Tuning (SBT), a framework to reduce redundant reasoning in large reasoning models (LRMs) by enabling self-regulation, cutting token use by 60% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LRMs like OpenAI o1 and DeepSeek-R1 show improved reasoning but suffer from redundant computations and overthinking. Existing solutions rely on external controls, which SBT aims to eliminate.

Method: SBT uses overthinking metrics and training signals for self-regulation, adaptive reasoning-length data, and a braking prompt to terminate reasoning appropriately.

Result: Experiments on math benchmarks (AIME, AMC, MATH500, GSM8K) show 60% fewer tokens used while matching unconstrained model accuracy.

Conclusion: SBT effectively reduces overthinking in LRMs through self-regulation, offering computational efficiency without sacrificing performance.

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [459] [GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples](https://arxiv.org/pdf/2505.14814)
*Harry Zhang, Kurt Partridge, Pai Zhu, Neng Chen, Hyun Jin Park, Dhruuv Agarwal, Quan Wang*

Main category: cs.SD

TL;DR: The paper proposes a method to generate adversarial examples near the decision boundary for Spoken Keyword Spotting (KWS) to improve model accuracy, showing a 61% AUC improvement on synthetic hard negatives.


<details>
  <summary>Details</summary>
Motivation: Boundary examples in KWS training data are scarce, limiting model performance. The paper aims to address this by generating adversarial examples near the decision boundary.

Method: The method involves systematic generation of adversarial examples using insertion/deletion/substitution edits on the keyword's graphemes.

Result: The technique improves AUC by 61% on synthetic hard negatives while maintaining performance on positives and ambient negative audio.

Conclusion: The proposed method effectively enhances KWS model accuracy by addressing the scarcity of boundary examples in training data.

Abstract: Spoken Keyword Spotting (KWS) is the task of distinguishing between the
presence and absence of a keyword in audio. The accuracy of a KWS model hinges
on its ability to correctly classify examples close to the keyword and
non-keyword boundary. These boundary examples are often scarce in training
data, limiting model performance. In this paper, we propose a method to
systematically generate adversarial examples close to the decision boundary by
making insertion/deletion/substitution edits on the keyword's graphemes. We
evaluate this technique on held-out data for a popular keyword and show that
the technique improves AUC on a dataset of synthetic hard negatives by 61%
while maintaining quality on positives and ambient negative audio data.

</details>


### [460] [Replay Attacks Against Audio Deepfake Detection](https://arxiv.org/pdf/2505.14862)
*Nicolas Müller, Piotr Kawa, Wei-Herng Choong, Adriana Stan, Aditya Tirumala Bukkapatnam, Karla Pizzi, Alexander Wagner, Philip Sperl*

Main category: cs.SD

TL;DR: Replay attacks can deceive audio deepfake detection by re-recording spoofed audio through various setups, making them appear authentic. The ReplayDF dataset demonstrates this vulnerability, showing significant performance drops in detection models.


<details>
  <summary>Details</summary>
Motivation: To investigate how replay attacks undermine audio deepfake detection and highlight vulnerabilities in current detection systems.

Method: Introduces ReplayDF, a dataset with diverse speaker-microphone combinations and acoustic conditions, and evaluates six detection models across five datasets.

Result: Detection models are highly vulnerable, with the best model's EER increasing from 4.7% to 18.2%. Retraining with adaptive RIR only slightly improves performance.

Conclusion: Replay attacks pose a serious threat to audio deepfake detection, and current models struggle to mitigate this vulnerability effectively.

Abstract: We show how replay attacks undermine audio deepfake detection: By playing and
re-recording deepfake audio through various speakers and microphones, we make
spoofed samples appear authentic to the detection model. To study this
phenomenon in more detail, we introduce ReplayDF, a dataset of recordings
derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations
across six languages and four TTS models. It includes diverse acoustic
conditions, some highly challenging for detection. Our analysis of six
open-source detection models across five datasets reveals significant
vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate
(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response
(RIR) retraining, performance remains compromised with an 11.0% EER. We release
ReplayDF for non-commercial research use.

</details>


### [461] [Discrete Audio Representations for Automated Audio Captioning](https://arxiv.org/pdf/2505.14989)
*Jingguang Tian, Haoqin Sun, Xinhui Hu, Xinkang Xu*

Main category: cs.SD

TL;DR: The paper explores audio tokenization for automated audio captioning (AAC), finding performance degradation with unsupervised tokens. A supervised tokenizer is introduced, improving AAC performance.


<details>
  <summary>Details</summary>
Motivation: The applicability of audio tokens (semantic and acoustic) in AAC is underexplored, prompting a systematic investigation.

Method: Comparative analysis of tokenization methods and introduction of a supervised audio tokenizer trained with an audio tagging objective.

Result: Supervised tokens outperform conventional tokens in AAC, as shown on the Clotho dataset.

Conclusion: Supervised audio tokenization enhances AAC performance by better capturing audio event information.

Abstract: Discrete audio representations, termed audio tokens, are broadly categorized
into semantic and acoustic tokens, typically generated through unsupervised
tokenization of continuous audio representations. However, their applicability
to automated audio captioning (AAC) remains underexplored. This paper
systematically investigates the viability of audio token-driven models for AAC
through comparative analyses of various tokenization methods. Our findings
reveal that audio tokenization leads to performance degradation in AAC models
compared to those that directly utilize continuous audio representations. To
address this issue, we introduce a supervised audio tokenizer trained with an
audio tagging objective. Unlike unsupervised tokenizers, which lack explicit
semantic understanding, the proposed tokenizer effectively captures audio event
information. Experiments conducted on the Clotho dataset demonstrate that the
proposed audio tokens outperform conventional audio tokens in the AAC task.

</details>


### [462] [AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars](https://arxiv.org/pdf/2505.15058)
*Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li*

Main category: cs.SD

TL;DR: AsynFusion is a novel framework using diffusion transformers to generate synchronized facial expressions and gestures for lifelike avatars, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack coordination between facial and gestural elements, leading to unnatural animations.

Method: Uses a dual-branch DiT architecture with a Cooperative Synchronization Module and Asynchronous LCM Sampling for efficient, high-quality synthesis.

Result: Achieves state-of-the-art performance in real-time, synchronized whole-body animations.

Conclusion: AsynFusion effectively addresses the coordination challenge, producing more natural and cohesive animations.

Abstract: Whole-body audio-driven avatar pose and expression generation is a critical
task for creating lifelike digital humans and enhancing the capabilities of
interactive virtual agents, with wide-ranging applications in virtual reality,
digital entertainment, and remote communication. Existing approaches often
generate audio-driven facial expressions and gestures independently, which
introduces a significant limitation: the lack of seamless coordination between
facial and gestural elements, resulting in less natural and cohesive
animations. To address this limitation, we propose AsynFusion, a novel
framework that leverages diffusion transformers to achieve harmonious
expression and gesture synthesis. The proposed method is built upon a
dual-branch DiT architecture, which enables the parallel generation of facial
expressions and gestures. Within the model, we introduce a Cooperative
Synchronization Module to facilitate bidirectional feature interaction between
the two modalities, and an Asynchronous LCM Sampling strategy to reduce
computational overhead while maintaining high-quality outputs. Extensive
experiments demonstrate that AsynFusion achieves state-of-the-art performance
in generating real-time, synchronized whole-body animations, consistently
outperforming existing methods in both quantitative and qualitative
evaluations.

</details>


### [463] [SHEET: A Multi-purpose Open-source Speech Human Evaluation Estimation Toolkit](https://arxiv.org/pdf/2505.15061)
*Wen-Chin Huang, Erica Cooper, Tomoki Toda*

Main category: cs.SD

TL;DR: SHEET is an open-source toolkit for subjective speech quality assessment, offering training scripts, multi-dataset support, and pre-trained models. It re-evaluated SSL-MOS, identifying a superior speech SSL model.


<details>
  <summary>Details</summary>
Motivation: To accelerate SSQA research by providing a versatile, data-driven toolkit for predicting human-labeled speech quality scores.

Method: SHEET uses deep neural networks for SSQA, supports multi-dataset and multi-model training, and includes pre-trained models. It re-evaluated SSL-MOS on BVCC and NISQA datasets.

Result: Identified an optimal speech SSL model outperforming SSL-MOS and matching state-of-the-art methods.

Conclusion: SHEET is a powerful toolkit for SSQA research, demonstrated by its successful re-evaluation and improvement of SSL-MOS.

Abstract: We introduce SHEET, a multi-purpose open-source toolkit designed to
accelerate subjective speech quality assessment (SSQA) research. SHEET stands
for the Speech Human Evaluation Estimation Toolkit, which focuses on
data-driven deep neural network-based models trained to predict human-labeled
quality scores of speech samples. SHEET provides comprehensive training and
evaluation scripts, multi-dataset and multi-model support, as well as
pre-trained models accessible via Torch Hub and HuggingFace Spaces. To
demonstrate its capabilities, we re-evaluated SSL-MOS, a speech self-supervised
learning (SSL)-based SSQA model widely used in recent scientific papers, on an
extensive list of speech SSL models. Experiments were conducted on two
representative SSQA datasets named BVCC and NISQA, and we identified the
optimal speech SSL model, whose performance surpassed the original SSL-MOS
implementation and was comparable to state-of-the-art methods.

</details>


### [464] [Hybrid Audio Detection Using Fine-Tuned Audio Spectrogram Transformers: A Dataset-Driven Evaluation of Mixed AI-Human Speech](https://arxiv.org/pdf/2505.15136)
*Kunyang Huang, Bin Hu*

Main category: cs.SD

TL;DR: The paper introduces a hybrid audio dataset and fine-tuned AST models to detect mixed human and AI-generated speech, achieving 97% accuracy, improving voice authentication security.


<details>
  <summary>Details</summary>
Motivation: Address the gap in detecting mixed human and AI-generated audio, a real-world security risk for voice authentication systems.

Method: Construct a hybrid audio dataset with human, AI-generated, cloned, and mixed samples; develop fine-tuned AST models for detection.

Result: The proposed model achieves 97% classification accuracy, outperforming existing baselines in mixed-audio detection.

Conclusion: Hybrid datasets and tailored models are crucial for enhancing the robustness of speech-based authentication systems.

Abstract: The rapid advancement of artificial intelligence (AI) has enabled
sophisticated audio generation and voice cloning technologies, posing
significant security risks for applications reliant on voice authentication.
While existing datasets and models primarily focus on distinguishing between
human and fully synthetic speech, real-world attacks often involve audio that
combines both genuine and cloned segments. To address this gap, we construct a
novel hybrid audio dataset incorporating human, AI-generated, cloned, and mixed
audio samples. We further propose fine-tuned Audio Spectrogram Transformer
(AST)-based models tailored for detecting these complex acoustic patterns.
Extensive experiments demonstrate that our approach significantly outperforms
existing baselines in mixed-audio detection, achieving 97\% classification
accuracy. Our findings highlight the importance of hybrid datasets and tailored
models in advancing the robustness of speech-based authentication systems.

</details>


### [465] [Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice Conversion Framework](https://arxiv.org/pdf/2505.15254)
*Kyungguen Byun, Jason Filos, Erik Visser, Sunkuk Moon*

Main category: cs.SD

TL;DR: A two-stage speech enhancement system combining generative speech restoration (GSR) and voice conversion (VC) achieves studio-level quality by first suppressing noise and then refining speech using clean speaker embeddings.


<details>
  <summary>Details</summary>
Motivation: To improve speech quality by leveraging VC models for restoration, despite their vulnerability to noise, by integrating a noise-resistant GSR model.

Method: The system uses a GSR model for noise suppression and speech restoration, followed by a VC model guided by clean speaker embeddings for further refinement.

Result: Achieves speech quality scores comparable to SOTA methods across multiple datasets.

Conclusion: The two-stage approach effectively combines speaker-agnostic restoration with VC to produce high-quality speech.

Abstract: We propose a speech enhancement system that combines speaker-agnostic speech
restoration with voice conversion (VC) to obtain a studio-level quality speech
signal. While voice conversion models are typically used to change speaker
characteristics, they can also serve as a means of speech restoration when the
target speaker is the same as the source speaker. However, since VC models are
vulnerable to noisy conditions, we have included a generative speech
restoration (GSR) model at the front end of our proposed system. The GSR model
performs noise suppression and restores speech damage incurred during that
process without knowledge about the target speaker. The VC stage then uses
guidance from clean speaker embeddings to further restore the output speech. By
employing this two-stage approach, we have achieved speech quality objective
metric scores comparable to state-of-the-art (SOTA) methods across multiple
datasets.

</details>


### [466] [Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN](https://arxiv.org/pdf/2505.15368)
*Yicheng Gu, Chaoren Wang, Zhizheng Wu, Lauri Juvela*

Main category: cs.SD

TL;DR: Neurodyne improves neural-network-based pitch manipulation by using adversarial representation learning and cycle-consistency training, achieving better synthesis quality and preserving singer identity.


<details>
  <summary>Details</summary>
Motivation: Current neural-network-based pitch-manipulation systems suffer from inaccurate feature disentanglement and lack of paired training data, limiting their performance.

Method: Neurodyne employs adversarial representation learning for pitch-independent latent representations and cycle-consistency training to implicitly create paired data.

Result: Experiments show improved synthesis quality in global-key and template-based pitch manipulation while retaining the original singer identity.

Conclusion: Neurodyne effectively addresses the limitations of existing systems, enhancing pitch manipulation in music production.

Abstract: Pitch manipulation is the process of producers adjusting the pitch of an
audio segment to a specific key and intonation, which is essential in music
production. Neural-network-based pitch-manipulation systems have been popular
in recent years due to their superior synthesis quality compared to classical
DSP methods. However, their performance is still limited due to their
inaccurate feature disentanglement using source-filter models and the lack of
paired in- and out-of-tune training data. This work proposes Neurodyne to
address these issues. Specifically, Neurodyne uses adversarial representation
learning to learn a pitch-independent latent representation to avoid inaccurate
disentanglement and cycle-consistency training to create paired training data
implicitly. Experimental results on global-key and template-based pitch
manipulation demonstrate the effectiveness of the proposed system, marking
improved synthesis quality while maintaining the original singer identity.

</details>


### [467] [Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding](https://arxiv.org/pdf/2505.15380)
*Zijian Lin, Yang Zhang, Yougen Yuan, Yuming Yan, Jinjiang Liu, Zhiyong Wu, Pengfei Hu, Qun Yu*

Main category: cs.SD

TL;DR: SSD accelerates autoregressive speech synthesis by using a draft model for parallel verification, achieving 1.4x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Sequential token prediction in autoregressive models causes high latency, limiting deployment in speed-critical scenarios.

Method: Proposes SSD, a framework using a lightweight draft model to generate candidate tokens, verified in parallel by the target model.

Result: SSD achieves 1.4x speedup over conventional decoding while maintaining high fidelity and naturalness.

Conclusion: SSD effectively accelerates speech synthesis without compromising perceptual quality.

Abstract: Modern autoregressive speech synthesis models leveraging language models have
demonstrated remarkable performance. However, the sequential nature of next
token prediction in these models leads to significant latency, hindering their
deployment in scenarios where inference speed is critical. In this work, we
propose Speech Speculative Decoding (SSD), a novel framework for autoregressive
speech synthesis acceleration. Specifically, our method employs a lightweight
draft model to generate candidate token sequences, which are subsequently
verified in parallel by the target model using the proposed SSD framework.
Experimental results demonstrate that SSD achieves a significant speedup of
1.4x compared with conventional autoregressive decoding, while maintaining high
fidelity and naturalness. Subjective evaluations further validate the
effectiveness of SSD in preserving the perceptual quality of the target model
while accelerating inference.

</details>


### [468] [Unified Microphone Conversion: Many-to-Many Device Mapping via Feature-wise Linear Modulation](https://arxiv.org/pdf/2410.18322)
*Myeonghoon Ryu, Hongseok Oh, Suji Lee, Han Park*

Main category: cs.SD

TL;DR: Unified Microphone Conversion uses a generative framework with frequency response conditioning to improve sound event classification scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability limitations of prior CycleGAN-based methods for device variability in sound event classification.

Method: A unified generative framework conditioned on frequency response data, using Feature-wise Linear Modulation and synthetic frequency response differences for many-to-many device mappings.

Result: Outperforms state-of-the-art by 2.6% and reduces variability by 0.8% in macro-average F1 score.

Conclusion: The proposed framework effectively enhances scalability and performance for sound event classification against device variability.

Abstract: We present Unified Microphone Conversion, a unified generative framework
designed to bolster sound event classification (SEC) systems against device
variability. While our prior CycleGAN-based methods effectively simulate device
characteristics, they require separate models for each device pair, limiting
scalability. Our approach overcomes this constraint by conditioning the
generator on frequency response data, enabling many-to-many device mappings
through unpaired training. We integrate frequency-response information via
Feature-wise Linear Modulation, further enhancing scalability. Additionally,
incorporating synthetic frequency response differences improves the
applicability of our framework for real-world application. Experimental results
show that our method outperforms the state-of-the-art by 2.6% and reduces
variability by 0.8% in macro-average F1 score.

</details>


### [469] [Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning](https://arxiv.org/pdf/2505.15402)
*Junchuan Zhao, Xintong Wang, Ye Wang*

Main category: cs.SD

TL;DR: A voice conversion model using VALLE-X framework with a prosody-aware audio codec encoder (PACE) improves prosody control and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Leverage in-context learning for speaker adaptation and enhance prosody control in voice conversion.

Method: Integrate PACE module into VALLE-X framework to isolate and refine prosody.

Result: Outperforms baseline VC systems in prosody preservation, timbre consistency, and naturalness.

Conclusion: The proposed VC model with PACE offers superior prosody control and performance.

Abstract: Recent advances in discrete audio codecs have significantly improved speech
representation modeling, while codec language models have enabled in-context
learning for zero-shot speech synthesis. Inspired by this, we propose a voice
conversion (VC) model within the VALLE-X framework, leveraging its strong
in-context learning capabilities for speaker adaptation. To enhance prosody
control, we introduce a prosody-aware audio codec encoder (PACE) module, which
isolates and refines prosody from other sources, improving expressiveness and
control. By integrating PACE into our VC model, we achieve greater flexibility
in prosody manipulation while preserving speaker timbre. Experimental
evaluation results demonstrate that our approach outperforms baseline VC
systems in prosody preservation, timbre consistency, and overall naturalness,
surpassing baseline VC systems.

</details>


### [470] [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/pdf/2505.15406)
*Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li, Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang, Chenxi Wang, Guangxian Ouyang, Zhenhao Chen, Xiuying Chen*

Main category: cs.SD

TL;DR: AJailBench is introduced as the first benchmark to evaluate jailbreak vulnerabilities in Large Audio Language Models (LAMs), revealing their lack of robustness against adversarial audio prompts.


<details>
  <summary>Details</summary>
Motivation: Current research lacks systematic evaluation of LAM safety, particularly against jailbreak attacks, due to the temporal and semantic nature of speech.

Method: AJailBench-Base dataset of adversarial audio prompts is created, followed by dynamic adversarial variants using the Audio Perturbation Toolkit (APT) with semantic consistency constraints.

Result: Leading LAMs show inconsistent robustness, with small, semantically preserved perturbations significantly reducing safety performance.

Conclusion: The study highlights the need for more robust and semantically aware defense mechanisms in LAMs.

Abstract: The rise of Large Audio Language Models (LAMs) brings both potential and
risks, as their audio outputs may contain harmful or unethical content.
However, current research lacks a systematic, quantitative evaluation of LAM
safety especially against jailbreak attacks, which are challenging due to the
temporal and semantic nature of speech. To bridge this gap, we introduce
AJailBench, the first benchmark specifically designed to evaluate jailbreak
vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of
1,495 adversarial audio prompts spanning 10 policy-violating categories,
converted from textual jailbreak attacks using realistic text to speech
synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and
reveal that none exhibit consistent robustness across attacks. To further
strengthen jailbreak testing and simulate more realistic attack conditions, we
propose a method to generate dynamic adversarial variants. Our Audio
Perturbation Toolkit (APT) applies targeted distortions across time, frequency,
and amplitude domains. To preserve the original jailbreak intent, we enforce a
semantic consistency constraint and employ Bayesian optimization to efficiently
search for perturbations that are both subtle and highly effective. This
results in AJailBench-APT, an extended dataset of optimized adversarial audio
samples. Our findings demonstrate that even small, semantically preserved
perturbations can significantly reduce the safety performance of leading LAMs,
underscoring the need for more robust and semantically aware defense
mechanisms.

</details>


### [471] [Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes](https://arxiv.org/pdf/2505.15559)
*Zixun Guo, Simon Dixon*

Main category: cs.SD

TL;DR: Moonbeam is a transformer-based model for symbolic music, pretrained on extensive MIDI data. It introduces novel tokenization and attention mechanisms, excelling in music understanding and generation tasks.


<details>
  <summary>Details</summary>
Motivation: To create a foundation model for symbolic music that incorporates domain-specific biases and outperforms existing models in accuracy and generation quality.

Method: Pretrained on 81.6K hours of MIDI data using a novel tokenization method and Multidimensional Relative Attention (MRA). Two finetuning architectures are proposed for understanding and generation tasks.

Result: Outperforms other models in accuracy and F1 score on classification tasks and surpasses a strong baseline in conditional music generation.

Conclusion: Moonbeam is a robust model for symbolic music, offering superior performance and open-sourced resources for further research.

Abstract: Moonbeam is a transformer-based foundation model for symbolic music,
pretrained on a large and diverse collection of MIDI data totaling 81.6K hours
of music and 18 billion tokens. Moonbeam incorporates music-domain inductive
biases by capturing both absolute and relative musical attributes through the
introduction of a novel domain-knowledge-inspired tokenization method and
Multidimensional Relative Attention (MRA), which captures relative music
information without additional trainable parameters. Leveraging the pretrained
Moonbeam, we propose 2 finetuning architectures with full anticipatory
capabilities, targeting 2 categories of downstream tasks: symbolic music
understanding and conditional music generation (including music infilling). Our
model outperforms other large-scale pretrained music models in most cases in
terms of accuracy and F1 score across 3 downstream music classification tasks
on 4 datasets. Moreover, our finetuned conditional music generation model
outperforms a strong transformer baseline with a REMI-like tokenizer. We
open-source the code, pretrained model, and generated samples on Github.

</details>


### [472] [VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning](https://arxiv.org/pdf/2505.12332)
*Qianyue Hu, Junyan Wu, Wei Lu, Xiangyang Luo*

Main category: cs.SD

TL;DR: VoiceCloak is a proactive defense framework designed to disrupt unauthorized voice cloning using diffusion models by introducing adversarial perturbations to obfuscate speaker identity and degrade output quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models (DMs) pose risks of malicious misuse in voice cloning (VC). Existing defenses are incompatible with DMs, necessitating a new framework.

Method: VoiceCloak disrupts VC by targeting speaker identity via distorted embeddings and conditional guidance processes, and degrades quality using score magnitude amplification and noise-guided semantic corruption.

Result: VoiceCloak achieves a high defense success rate against unauthorized diffusion-based VC.

Conclusion: VoiceCloak effectively bridges the gap in defending against DM-based VC misuse.

Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice
cloning (VC), while they also increase the risk of malicious misuse. Existing
proactive defenses designed for traditional VC models aim to disrupt the
forgery process, but they have been proven incompatible with DMs due to the
intricate generative mechanisms of diffusion. To bridge this gap, we introduce
VoiceCloak, a multi-dimensional proactive defense framework with the goal of
obfuscating speaker identity and degrading perceptual quality in potential
unauthorized VC. To achieve these goals, we conduct a focused analysis to
identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt
the cloning process by introducing adversarial perturbations into the reference
audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets
speaker identity by distorting representation learning embeddings to maximize
identity variation, which is guided by auditory perception principles.
Additionally, VoiceCloak disrupts crucial conditional guidance processes,
particularly attention context, thereby preventing the alignment of vocal
characteristics that are essential for achieving convincing cloning. Then, to
address the second objective, VoiceCloak introduces score magnitude
amplification to actively steer the reverse trajectory away from the generation
of high-quality speech. Noise-guided semantic corruption is further employed to
disrupt structural speech semantics captured by DMs, degrading output quality.
Extensive experiments highlight VoiceCloak's outstanding defense success rate
against unauthorized diffusion-based voice cloning.

</details>


### [473] [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/pdf/2505.15772)
*Cheng Yifan, Zhang Ruoyi, Shi Jiatong*

Main category: cs.SD

TL;DR: MIKU-PAL is an automated pipeline for extracting high-consistency emotional speech from unlabeled videos, achieving human-level accuracy and superior consistency at lower cost.


<details>
  <summary>Details</summary>
Motivation: Challenges in acquiring large-scale, consistent emotional speech data for synthesis.

Method: Uses face detection, tracking, and a multimodal large language model (MLLM) for emotion analysis.

Result: Human-level accuracy (68.5% on MELD), high consistency (0.93 Fleiss kappa), and 83% human-validated rationality. Released MIKU-EmoBench dataset (131.2 hours).

Conclusion: MIKU-PAL provides efficient, high-quality emotional speech annotation, enabling fine-grained emotion analysis and dataset creation.

Abstract: Acquiring large-scale emotional speech data with strong consistency remains a
challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated
multimodal pipeline for extracting high-consistency emotional speech from
unlabeled video data. Leveraging face detection and tracking algorithms, we
developed an automatic emotion analysis system using a multimodal large
language model (MLLM). Our results demonstrate that MIKU-PAL can achieve
human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss
kappa score) while being much cheaper and faster than human annotation. With
the high-quality, flexible, and consistent annotation from MIKU-PAL, we can
annotate fine-grained speech emotion categories of up to 26 types, validated by
human annotators with 83% rationality ratings. Based on our proposed system, we
further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2
hours) as a new benchmark for emotional text-to-speech and visual voice
cloning.

</details>


### [474] [MatchDance: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis](https://arxiv.org/pdf/2505.14222)
*Kaixing Yang, Xulong Tang, Yuxuan Hu, Jiahao Yang, Hongyan Liu, Qinnan Zhang, Jun He, Zhaoxin Fan*

Main category: cs.SD

TL;DR: MatchDance is a novel framework for music-to-dance generation, enhancing choreographic consistency via a two-stage latent representation approach.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack choreographic consistency in music-to-dance generation, limiting their practical application.

Method: MatchDance uses a two-stage design: (1) KDQS for latent representation with kinematic-dynamic constraints, and (2) HMDGS for music-to-dance mapping via a Mamba-Transformer hybrid.

Result: State-of-the-art performance on the FineDance dataset, validated by comprehensive metrics.

Conclusion: MatchDance addresses choreographic consistency effectively, offering a robust solution for music-to-dance generation.

Abstract: Music-to-dance generation represents a challenging yet pivotal task at the
intersection of choreography, virtual reality, and creative content generation.
Despite its significance, existing methods face substantial limitation in
achieving choreographic consistency. To address the challenge, we propose
MatchDance, a novel framework for music-to-dance generation that constructs a
latent representation to enhance choreographic consistency. MatchDance employs
a two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),
which encodes dance motions into a latent representation by Finite Scalar
Quantization (FSQ) with kinematic-dynamic constraints and reconstructs them
with high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),
which uses a Mamba-Transformer hybrid architecture to map music into the latent
representation, followed by the KDQS decoder to generate 3D dance motions.
Additionally, a music-dance retrieval framework and comprehensive metrics are
introduced for evaluation. Extensive experiments on the FineDance dataset
demonstrate state-of-the-art performance. Code will be released upon
acceptance.

</details>


### [475] [Solid State Bus-Comp: A Large-Scale and Diverse Dataset for Dynamic Range Compressor Virtual Analog Modeling](https://arxiv.org/pdf/2504.04589)
*Yicheng Gu, Runsong Zhang, Lauri Juvela, Zhizheng Wu*

Main category: cs.SD

TL;DR: The paper introduces Solid State Bus-Comp, a large-scale dataset for modeling the SSL 500 G-Bus compressor, addressing data limitations in neural-network-based VA modeling.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in neural-network-based VA modeling of Dynamic Range Compressors by providing a diverse and extensive dataset.

Method: Manually collected 175 unmastered songs, recorded compressed audio in 220 parameter combinations, and conducted benchmark experiments with various models.

Result: Created a 2528-hour dataset with diverse audio characteristics and demonstrated its effectiveness through ablation studies.

Conclusion: The dataset enhances VA modeling of compressors, with potential applications in music production.

Abstract: Virtual Analog (VA) modeling aims to simulate the behavior of hardware
circuits via algorithms to replicate their tone digitally. Dynamic Range
Compressor (DRC) is an audio processing module that controls the dynamics of a
track by reducing and amplifying the volumes of loud and quiet sounds, which is
essential in music production. In recent years, neural-network-based VA
modeling has shown great potential in producing high-fidelity models. However,
due to the lack of data quantity and diversity, their generalization ability in
different parameter settings and input sounds is still limited. To tackle this
problem, we present Solid State Bus-Comp, the first large-scale and diverse
dataset for modeling the classical VCA compressor -- SSL 500 G-Bus.
Specifically, we manually collected 175 unmastered songs from the Cambridge
Multitrack Library. We recorded the compressed audio in 220 parameter
combinations, resulting in an extensive 2528-hour dataset with diverse genres,
instruments, tempos, and keys. Moreover, to facilitate the use of our proposed
dataset, we conducted benchmark experiments in various open-sourced black-box
and grey-box models, as well as white-box plugins. We also conducted ablation
studies in different data subsets to illustrate the effectiveness of the
improved data diversity and quantity. The dataset and demos are on our project
page: https://www.yichenggu.com/SolidStateBusComp/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [476] [Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics](https://arxiv.org/pdf/2505.14700)
*Rômulo Damasclin Chaves dos Santos, Jorge Henrique de Oliveira Sales*

Main category: cs.LG

TL;DR: A new class of neural network operators combining symmetrized activation functions, fractional derivatives, and stochastic noise is introduced for problems with memory and randomness. Theoretical foundations and practical applications are provided.


<details>
  <summary>Details</summary>
Motivation: To address problems where memory effects and randomness are central, such as turbulent fluid flows, by developing a robust neural network framework.

Method: Merge symmetrized activation functions, Caputo-type fractional derivatives, and Itô-type noise to create neural operators. Prove Voronovskaya-type theorems for asymptotic behavior, convergence, and consistency.

Result: Theoretical guarantees for approximation quality, with successful application to fractional Navier-Stokes equations. Operators are effective for complex systems with memory and randomness.

Conclusion: The framework blends neural networks, fractional calculus, and stochastic analysis, offering new tools for modeling turbulence and multiscale processes with strong analytical support.

Abstract: In this work, we introduce a new class of neural network operators designed
to handle problems where memory effects and randomness play a central role. In
this work, we introduce a new class of neural network operators designed to
handle problems where memory effects and randomness play a central role. These
operators merge symmetrized activation functions, Caputo-type fractional
derivatives, and stochastic perturbations introduced via It\^o type noise. The
result is a powerful framework capable of approximating functions that evolve
over time with both long-term memory and uncertain dynamics. We develop the
mathematical foundations of these operators, proving three key theorems of
Voronovskaya type. These results describe the asymptotic behavior of the
operators, their convergence in the mean-square sense, and their consistency
under fractional regularity assumptions. All estimates explicitly account for
the influence of the memory parameter $\alpha$ and the noise level $\sigma$. As
a practical application, we apply the proposed theory to the fractional
Navier-Stokes equations with stochastic forcing, a model often used to describe
turbulence in fluid flows with memory. Our approach provides theoretical
guarantees for the approximation quality and suggests that these neural
operators can serve as effective tools in the analysis and simulation of
complex systems. By blending ideas from neural networks, fractional calculus,
and stochastic analysis, this research opens new perspectives for modeling
turbulent phenomena and other multiscale processes where memory and randomness
are fundamental. The results lay the groundwork for hybrid learning-based
methods with strong analytical backing.

</details>


### [477] [The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents](https://arxiv.org/pdf/2505.14727)
*Mohammad Rubyet Islam*

Main category: cs.LG

TL;DR: The paper presents a five-stage taxonomy for the evolution of alpha-return strategies, from manual methods to AI-driven systems, emphasizing system-level integration and challenges like interpretability and compliance.


<details>
  <summary>Details</summary>
Motivation: To map the transformation of alpha-return strategies from intuition-based to AI-powered systems, addressing gaps in prior surveys by focusing on system-level advancements.

Method: Introduces a five-stage taxonomy (manual strategies to LLM-powered agents) and reviews representation learning, multimodal data fusion, and tool-augmented LLM agents.

Result: Highlights the shift to context-aware financial agents and identifies key challenges (interpretability, data fragility, governance).

Conclusion: The taxonomy provides a framework for evaluating and responsibly developing next-gen alpha systems.

Abstract: The pursuit of alpha returns that exceed market benchmarks has undergone a
profound transformation, evolving from intuition-driven investing to
autonomous, AI powered systems. This paper introduces a comprehensive five
stage taxonomy that traces this progression across manual strategies,
statistical models, classical machine learning, deep learning, and agentic
architectures powered by large language models (LLMs). Unlike prior surveys
focused narrowly on modeling techniques, this review adopts a system level
lens, integrating advances in representation learning, multimodal data fusion,
and tool augmented LLM agents. The strategic shift from static predictors to
contextaware financial agents capable of real time reasoning, scenario
simulation, and cross modal decision making is emphasized. Key challenges in
interpretability, data fragility, governance, and regulatory compliance areas
critical to production deployment are examined. The proposed taxonomy offers a
unified framework for evaluating maturity, aligning infrastructure, and guiding
the responsible development of next generation alpha systems.

</details>


### [478] [The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute](https://arxiv.org/pdf/2505.14733)
*Yunho Jin, Gu-Yeon Wei, David Brooks*

Main category: cs.LG

TL;DR: Test-time compute (TTC) improves accuracy-energy efficiency in LLMs, outperforming traditional model scaling, especially in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Address diminishing returns and high energy demands of scaling LLMs by exploring TTC as an alternative.

Method: Empirical analysis comparing TTC with model scaling, focusing on accuracy-energy trade-offs and task complexity.

Result: TTC achieves better efficiency, particularly in complex reasoning, and adapts well to query complexity.

Conclusion: TTC offers a sustainable, adaptable, and cost-effective approach for deploying future language models.

Abstract: Scaling large language models (LLMs) has driven significant advancements, yet
it faces diminishing returns and escalating energy demands. This work
introduces test-time compute (TTC)-allocating additional computational
resources during inference-as a compelling complement to conventional scaling
strategies. Specifically, we investigate whether employing TTC can achieve
superior accuracy-energy trade-offs compared to simply increasing model size.
Our empirical analysis reveals that TTC surpasses traditional model scaling in
accuracy/energy efficiency, with notable gains in tasks demanding complex
reasoning rather than mere factual recall. Further, we identify a critical
interaction between TTC performance and output sequence length, demonstrating
that strategically adjusting compute resources at inference time according to
query complexity can substantially enhance efficiency. Our findings advocate
for TTC as a promising direction, enabling more sustainable, accurate, and
adaptable deployment of future language models without incurring additional
pretraining costs.

</details>


### [479] [Leveraging Multivariate Long-Term History Representation for Time Series Forecasting](https://arxiv.org/pdf/2505.14737)
*Huiliang Zhang, Di Wu, Arnaud Zinflou, Stephane Dellacherie, Mouhamadou Makhtar Dione, Benoit Boulet*

Main category: cs.LG

TL;DR: The paper introduces LMHR, a framework enhancing STGNNs for MTS forecasting by addressing long-term spatial-temporal dependencies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing STGNNs for MTS forecasting overlook long-term spatial-temporal correlations, limiting accuracy.

Method: LMHR uses LHEncoder for long-term history encoding, HRetriever for spatial-temporal dependency modeling, and TAggregator for representation fusion.

Result: LMHR outperforms typical STGNNs by 10.72% and state-of-the-art methods by 4.12%, with a 9.8% improvement on rapidly changing patterns.

Conclusion: LMHR effectively addresses long-term dependencies in MTS forecasting, achieving superior performance.

Abstract: Multivariate Time Series (MTS) forecasting has a wide range of applications
in both industry and academia. Recent advances in Spatial-Temporal Graph Neural
Network (STGNN) have achieved great progress in modelling spatial-temporal
correlations. Limited by computational complexity, most STGNNs for MTS
forecasting focus primarily on short-term and local spatial-temporal
dependencies. Although some recent methods attempt to incorporate univariate
history into modeling, they still overlook crucial long-term spatial-temporal
similarities and correlations across MTS, which are essential for accurate
forecasting. To fill this gap, we propose a framework called the Long-term
Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.
Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively
encode the long-term history into segment-level contextual representations and
reduce point-level noise. A non-parametric Hierarchical Representation
Retriever (HRetriever) is designed to include the spatial information in the
long-term spatial-temporal dependency modelling and pick out the most valuable
representations with no additional training. A Transformer-based Aggregator
(TAggregator) selectively fuses the sparsely retrieved contextual
representations based on the ranking positional embedding efficiently.
Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%
on the average prediction horizons and state-of-the-art methods by 4.12% on
several real-world datasets. Additionally, it consistently improves prediction
accuracy by 9.8% on the top 10% of rapidly changing patterns across the
datasets.

</details>


### [480] [An Initial Introduction to Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2405.06161)
*Christopher Amato*

Main category: cs.LG

TL;DR: The paper introduces cooperative multi-agent reinforcement learning (MARL), categorizing methods into CTE, CTDE, and DTE, and discusses key concepts, methods, and open questions.


<details>
  <summary>Details</summary>
Motivation: To provide a clear understanding of cooperative MARL, its settings, and common methods, while addressing misconceptions and open questions.

Method: Categorizes MARL approaches into CTE, CTDE, and DTE, and reviews key methods like VDN, QMIX, MADDPG, and others.

Result: A comprehensive overview of cooperative MARL, highlighting its main concepts, methods, and unresolved challenges.

Conclusion: The paper serves as an introductory guide to cooperative MARL, clarifying its landscape and pointing to future research directions.

Abstract: Multi-agent reinforcement learning (MARL) has exploded in popularity in
recent years. While numerous approaches have been developed, they can be
broadly categorized into three main types: centralized training and execution
(CTE), centralized training for decentralized execution (CTDE), and
decentralized training and execution (DTE). CTE methods assume centralization
during training and execution (e.g., with fast, free, and perfect
communication) and have the most information during execution. CTDE methods are
the most common, as they leverage centralized information during training while
enabling decentralized execution -- using only information available to that
agent during execution. Decentralized training and execution methods make the
fewest assumptions and are often simple to implement.
  This text is an introduction to cooperative MARL -- MARL in which all agents
share a single, joint reward. It is meant to explain the setting, basic
concepts, and common methods for the CTE, CTDE, and DTE settings. It does not
cover all work in cooperative MARL as the area is quite extensive. I have
included work that I believe is important for understanding the main concepts
in the area and apologize to those that I have omitted. Topics include simple
applications of single-agent methods to CTE as well as some more scalable
methods that exploit the multi-agent structure, independent Q-learning and
policy gradient methods and their extensions, as well as value function
factorization methods including the well-known VDN, QMIX, and QPLEX approaches,
and centralized critic methods including MADDPG, COMA, and MAPPO. I also
discuss common misconceptions, the relationship between different approaches,
and some open questions.

</details>


### [481] [Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs](https://arxiv.org/pdf/2505.14739)
*Heiko Oppel, Andreas Spilz, Michael Munz*

Main category: cs.LG

TL;DR: Denoising diffusion models generate synthetic sensor signals but struggle with quality estimation. The paper proposes adapting similarity metrics to monitor training, reducing epochs without performance loss.


<details>
  <summary>Details</summary>
Motivation: The randomness in diffusion models and their loss functions makes data quality estimation challenging.

Method: Examine and adapt similarity metrics to monitor training and synthesis, fine-tuning for classification tasks.

Result: Reduced training epochs significantly without compromising classification performance.

Conclusion: Optimized training saves resources and time for generative models.

Abstract: Denoising diffusion probabilistic models are able to generate synthetic
sensor signals. The training process of such a model is controlled by a loss
function which measures the difference between the noise that was added in the
forward process and the noise that was predicted by the diffusion model. This
enables the generation of realistic data. However, the randomness within the
process and the loss function itself makes it difficult to estimate the quality
of the data. Therefore, we examine multiple similarity metrics and adapt an
existing metric to overcome this issue by monitoring the training and
synthetisation process using those metrics. The adapted metric can even be
fine-tuned on the input data to comply with the requirements of an underlying
classification task. We were able to significantly reduce the amount of
training epochs without a performance reduction in the classification task. An
optimized training process not only saves resources, but also reduces the time
for training generative models.

</details>


### [482] [Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism](https://arxiv.org/pdf/2505.14741)
*Kunyun Wang, Bohan Li, Kai Yu, Minyi Guo, Jieru Zhao*

Main category: cs.LG

TL;DR: ParaStep is a parallelization method for diffusion models that reduces inference latency by exploiting step-wise similarity, achieving significant speedups without quality loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face high inference latency due to sequential denoising, and existing parallelization methods incur heavy communication overhead.

Method: Proposes ParaStep, a reuse-then-predict mechanism with lightweight step-wise communication to parallelize diffusion inference.

Result: Achieves speedups of 3.88× (SVD), 2.43× (CogVideoX-2b), and 6.56× (AudioLDM2-large) while maintaining quality.

Conclusion: ParaStep is a scalable, communication-efficient solution for accelerating diffusion models, especially in bandwidth-limited settings.

Abstract: Diffusion models have emerged as a powerful class of generative models across
various modalities, including image, video, and audio synthesis. However, their
deployment is often limited by significant inference latency, primarily due to
the inherently sequential nature of the denoising process. While existing
parallelization strategies attempt to accelerate inference by distributing
computation across multiple devices, they typically incur high communication
overhead, hindering deployment on commercial hardware. To address this
challenge, we propose \textbf{ParaStep}, a novel parallelization method based
on a reuse-then-predict mechanism that parallelizes diffusion inference by
exploiting similarity between adjacent denoising steps. Unlike prior approaches
that rely on layer-wise or stage-wise communication, ParaStep employs
lightweight, step-wise communication, substantially reducing overhead. ParaStep
achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD,
\textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on
AudioLDM2-large, while maintaining generation quality. These results highlight
ParaStep as a scalable and communication-efficient solution for accelerating
diffusion inference, particularly in bandwidth-constrained environments.

</details>


### [483] [Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2412.00661)
*Emile Anand, Ishani Karmarkar, Guannan Qu*

Main category: cs.LG

TL;DR: Proposes SUBSAMPLE-MFQ, a polynomial-time MARL algorithm for systems with $n$ agents, achieving near-optimal policy convergence independent of $n$.


<details>
  <summary>Details</summary>
Motivation: Addresses the exponential growth of joint state-action spaces in MARL and the challenge of balancing global decision-making with local agent interactions.

Method: Introduces SUBSAMPLE-MFQ, a decentralized randomized policy that learns a policy in polynomial time for any $k \leq n$ subsampled agents.

Result: Proves the learned policy converges to the optimal policy at a rate of $\tilde{O}(1/\sqrt{k})$, independent of $n$.

Conclusion: SUBSAMPLE-MFQ efficiently scales MARL by focusing on subsampled agents, offering a practical solution for large-scale systems.

Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL)
is fundamentally challenging because the size of the joint state and action
spaces grows exponentially in the number of agents. These difficulties are
exacerbated when balancing sequential global decision-making with local agent
interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$
($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning)
and a decentralized randomized policy for a system with $n$ agents. For any
$k\leq n$, our algorithm learns a policy for the system in time polynomial in
$k$. We prove that this learned policy converges to the optimal policy on the
order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$
increases. In particular, this bound is independent of the number of agents
$n$.

</details>


### [484] [Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](https://arxiv.org/pdf/2505.14742)
*Hong Huang, Dapeng Wu*

Main category: cs.LG

TL;DR: Quaff is a quantized parameter-efficient fine-tuning framework for LLMs, addressing activation outliers via the Outlier Spatial Stability Hypothesis (OSSH), achieving efficiency, performance, and deployability.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on resource-constrained devices is hindered by computational/memory demands of fine-tuning, with existing quantization methods struggling to balance performance and overhead.

Method: Proposes OSSH (activation outlier channels retain stable spatial positions) and Quaff, which optimizes low-precision activations via targeted momentum scaling, suppressing outliers in invariant channels.

Result: Quaff reduces latency by 1.73x, saves 30% memory, and improves accuracy by 0.6% on Phi-3, enabling fine-tuning on consumer-grade GPUs.

Conclusion: Quaff democratizes personalized LLM deployment by reconciling efficiency, performance, and deployability without sacrificing utility.

Abstract: Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While quantization offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in quantized fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing quantization errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.

</details>


### [485] [Explainable Prediction of the Mechanical Properties of Composites with CNNs](https://arxiv.org/pdf/2505.14745)
*Varun Raaghav, Dimitrios Bikos, Antonio Rago, Francesca Toni, Maria Charalambides*

Main category: cs.LG

TL;DR: The paper proposes using CNNs with XAI methods to predict composites' mechanical properties, outperforming traditional FE modelling and simpler AI models in accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: FE modelling is computationally expensive, and existing AI approaches lack accuracy, focus on limited properties, and are not transparent.

Method: Custom CNNs trained on FE-generated data predict Young's modulus and yield strength, validated against ResNet-34. XAI methods (SHAP, Integrated Gradients) explain predictions.

Result: The CNN approach achieves high accuracy and identifies critical geometrical features, ensuring trustworthiness.

Conclusion: CNNs with XAI offer a reliable, efficient alternative to FE modelling for predicting composites' mechanical properties.

Abstract: Composites are amongst the most important materials manufactured today, as
evidenced by their use in countless applications. In order to establish the
suitability of composites in specific applications, finite element (FE)
modelling, a numerical method based on partial differential equations, is the
industry standard for assessing their mechanical properties. However, FE
modelling is exceptionally costly from a computational viewpoint, a limitation
which has led to efforts towards applying AI models to this task. However, in
these approaches: the chosen model architectures were rudimentary, feed-forward
neural networks giving limited accuracy; the studies focus on predicting
elastic mechanical properties, without considering material strength limits;
and the models lacked transparency, hindering trustworthiness by users. In this
paper, we show that convolutional neural networks (CNNs) equipped with methods
from explainable AI (XAI) can be successfully deployed to solve this problem.
Our approach uses customised CNNs trained on a dataset we generate using
transverse tension tests in FE modelling to predict composites' mechanical
properties, i.e., Young's modulus and yield strength. We show empirically that
our approach achieves high accuracy, outperforming a baseline, ResNet-34, in
estimating the mechanical properties. We then use SHAP and Integrated
Gradients, two post-hoc XAI methods, to explain the predictions, showing that
the CNNs use the critical geometrical features that influence the composites'
behaviour, thus allowing engineers to verify that the models are trustworthy by
representing the science of composites.

</details>


### [486] [Cooperative Causal GraphSAGE](https://arxiv.org/pdf/2505.14748)
*Zaifa Xue, Tao Zhang, Tuo Xu, Huaixin Liang, Le Gao*

Main category: cs.LG

TL;DR: CoCa-GraphSAGE enhances Causal GraphSAGE by integrating cooperative game theory, improving robustness through cooperative causal sampling (CoCa-sampling) and Shapley values.


<details>
  <summary>Details</summary>
Motivation: Causal GraphSAGE neglects cooperative relationships among sampling nodes, limiting robustness.

Method: Proposes CoCa-GraphSAGE, combining cooperative game theory with causal inference, using Shapley values for cooperative causal sampling.

Result: Outperforms under perturbations with comparable classification performance, showing improved robustness.

Conclusion: CoCa-GraphSAGE successfully integrates cooperative relationships, enhancing stability and robustness in node embeddings.

Abstract: GraphSAGE is a widely used graph neural network. The introduction of causal
inference has improved its robust performance and named as Causal GraphSAGE.
However, Causal GraphSAGE focuses on measuring causal weighting among
individual nodes, but neglecting the cooperative relationships among sampling
nodes as a whole. To address this issue, this paper proposes Cooperative Causal
GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal
GraphSAGE. Initially, a cooperative causal structure model is constructed in
the case of cooperation based on the graph structure. Subsequently, Cooperative
Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley
values to calculate the cooperative contribution based on causal weights of the
nodes sets. CoCa-sampling guides the selection of nodes with significant
cooperative causal effects during the neighborhood sampling process, thus
integrating the selected neighborhood features under cooperative relationships,
which takes the sampled nodes as a whole and generates more stable target node
embeddings. Experiments on publicly available datasets show that the proposed
method has comparable classification performance to the compared methods and
outperforms under perturbations, demonstrating the robustness improvement by
CoCa-sampling.

</details>


### [487] [Self Distillation via Iterative Constructive Perturbations](https://arxiv.org/pdf/2505.14751)
*Maheak Dave, Aniket Kumar Singh, Aryan Pareek, Harshita Jha, Debasis Chaudhuri, Manish Pratap Singh*

Main category: cs.LG

TL;DR: A novel cyclic optimization framework using Iterative Constructive Perturbation (ICP) to balance performance and generalization in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing performance and generalization in deep neural networks by rethinking traditional training paradigms.

Method: Proposes ICP to iteratively perturb input data using the model's loss, creating enhanced representations. Combines this with self-distillation to improve intermediate features.

Result: Mitigates performance bottlenecks and shows significant improvements across training variations.

Conclusion: The framework effectively bridges the gap between fitting and generalization, enhancing neural network performance.

Abstract: Deep Neural Networks have achieved remarkable achievements across various
domains, however balancing performance and generalization still remains a
challenge while training these networks. In this paper, we propose a novel
framework that uses a cyclic optimization strategy to concurrently optimize the
model and its input data for better training, rethinking the traditional
training paradigm. Central to our approach is Iterative Constructive
Perturbation (ICP), which leverages the model's loss to iteratively perturb the
input, progressively constructing an enhanced representation over some
refinement steps. This ICP input is then fed back into the model to produce
improved intermediate features, which serve as a target in a self-distillation
framework against the original features. By alternately altering the model's
parameters to the data and the data to the model, our method effectively
addresses the gap between fitting and generalization, leading to enhanced
performance. Extensive experiments demonstrate that our approach not only
mitigates common performance bottlenecks in neural networks but also
demonstrates significant improvements across training variations.

</details>


### [488] [Large Language Models for Data Synthesis](https://arxiv.org/pdf/2505.14752)
*Yihong Tang, Menglin Kong, Lijun Sun*

Main category: cs.LG

TL;DR: LLMSynthor is a framework using LLMs for efficient, structure-aware synthetic data generation, ensuring statistical alignment with real-world data.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of classical synthetic data methods and inefficient LLM-based sampling by creating a flexible, high-dimensional prior for accurate data synthesis.

Method: LLMSynthor transforms LLMs into nonparametric copula simulators, uses LLM Proposal Sampling for efficient grounded proposals, and iteratively aligns synthetic and real data via summary statistics.

Result: LLMSynthor produces synthetic data with high statistical fidelity, practical utility, and adaptability across diverse domains like e-commerce and mobility.

Conclusion: LLMSynthor is a versatile tool for synthetic data generation, applicable in economics, social science, and urban studies.

Abstract: Generating synthetic data that faithfully captures the statistical structure
of real-world distributions is a fundamental challenge in data modeling.
Classical approaches often depend on strong parametric assumptions or manual
structural design and struggle in high-dimensional or heterogeneous domains.
Recent progress in Large Language Models (LLMs) reveals their potential as
flexible, high-dimensional priors over real-world distributions. However, when
applied to data synthesis, standard LLM-based sampling is inefficient,
constrained by fixed context limits, and fails to ensure statistical alignment.
Given this, we introduce LLMSynthor, a general framework for data synthesis
that transforms LLMs into structure-aware simulators guided by distributional
feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for
modeling high-order dependencies and introduces LLM Proposal Sampling to
generate grounded proposal distributions that improve sampling efficiency
without requiring rejection. By minimizing discrepancies in the summary
statistics space, the iterative synthesis loop aligns real and synthetic data
while gradually uncovering and refining the latent generative structure. We
evaluate LLMSynthor in both controlled and real-world settings using
heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,
population, and mobility) that encompass both structured and unstructured
formats. The synthetic data produced by LLMSynthor shows high statistical
fidelity, practical utility, and cross-data adaptability, positioning it as a
valuable tool across economics, social science, urban studies, and beyond.

</details>


### [489] [$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization](https://arxiv.org/pdf/2505.14756)
*Chih-Yu Chang, Milad Azvar, Chinedum Okwudire, Raed Al Kontar*

Main category: cs.LG

TL;DR: LLINBO combines LLMs with Bayesian optimization for black-box function optimization, balancing early exploration (LLMs) and exploitation (statistical models).


<details>
  <summary>Details</summary>
Motivation: LLMs lack explicit surrogate modeling and uncertainty calibration, risking reliability in optimization.

Method: Proposes LLINBO, integrating LLMs with Gaussian Processes, with three collaboration mechanisms.

Result: Theoretical guarantees for the hybrid approach and a real-life 3D printing proof-of-concept.

Conclusion: LLINBO effectively leverages LLMs' contextual reasoning while ensuring reliability through statistical models.

Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used
for optimizing expensive black-box functions. Recently, Large Language Models
(LLMs) have shown remarkable adaptability in low-data regimes, making them
promising tools for black-box optimization by leveraging contextual knowledge
to propose high-quality query points. However, relying solely on LLMs as
optimization agents introduces risks due to their lack of explicit surrogate
modeling and calibrated uncertainty, as well as their inherently opaque
internal mechanisms. This structural opacity makes it difficult to characterize
or control the exploration-exploitation trade-off, ultimately undermining
theoretical tractability and reliability. To address this, we propose LLINBO:
LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with
statistical surrogate experts (e.g., Gaussian Processes (GP)). The core
philosophy is to leverage contextual reasoning strengths of LLMs for early
exploration, while relying on principled statistical models to guide efficient
exploitation. Specifically, we introduce three mechanisms that enable this
collaboration and establish their theoretical guarantees. We end the paper with
a real-life proof-of-concept in the context of 3D printing. The code to
reproduce the results can be found at
https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.

</details>


### [490] [Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding](https://arxiv.org/pdf/2505.14765)
*Orhun Vural, Bunyamin Ozaydin, Khalid Y. Aram, James Booth, Brittany F. Lindsey, Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: Deep learning models forecast ED boarding patients 6 hours ahead using non-clinical data, with N-BEATSx performing best.


<details>
  <summary>Details</summary>
Motivation: To support proactive operational decision-making in emergency departments by predicting boarding patient counts without clinical data.

Method: Collected data from ED tracking, inpatient census, weather, holidays, and events. Used ResNetPlus, TSTPlus, TSiTPlus, and N-BEATSx with hyperparameter tuning.

Result: N-BEATSx achieved MAE of 2.10, MSE of 7.08, RMSE of 2.66, and R² of 0.95, maintaining accuracy during high boarding periods.

Conclusion: Accurate forecasting is possible without clinical data, and additional features enhance stability. The framework aids in mitigating ED overcrowding.

Abstract: This study develops deep learning models to forecast the number of patients
in the emergency department (ED) boarding phase six hours in advance, aiming to
support proactive operational decision-making using only non-clinical,
operational, and contextual features. Data were collected from five sources: ED
tracking systems, inpatient census records, weather reports, federal holiday
calendars, and local event schedules. After feature engineering, the data were
aggregated at an hourly level, cleaned, and merged into a unified dataset for
model training. Several time series deep learning models, including ResNetPlus,
TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using
Optuna and grid search for hyperparameter tuning. The average ED boarding count
was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best
performance, with a mean absolute error of 2.10, mean squared error of 7.08,
root mean squared error of 2.66, and a coefficient of determination of 0.95.
The model maintained stable accuracy even during periods of extremely high
boarding counts, defined as values exceeding one, two, or three standard
deviations above the mean. Results show that accurate six-hour-ahead forecasts
are achievable without using patient-level clinical data. While strong
performance was observed even with a basic feature set, the inclusion of
additional features improved prediction stability under extreme conditions.
This framework offers a practical and generalizable approach for hospital
systems to anticipate boarding levels and help mitigate ED overcrowding.

</details>


### [491] [This Time is Different: An Observability Perspective on Time Series Foundation Models](https://arxiv.org/pdf/2505.14766)
*Ben Cohen, Emaad Khwaja, Youssef Doubli, Salahidine Lemaachi, Chris Lettieri, Charles Masson, Hugo Miccinilli, Elise Ramé, Qiqi Ren, Afshin Rostamizadeh, Jean Ogier du Terrail, Anna-Monica Toon, Kan Wang, Stephan Xie, David Asker, Ameet Talwalkar, Othmane Abou-Amal*

Main category: cs.LG

TL;DR: Toto is a 151M-parameter time series forecasting model with a decoder-only architecture, trained on a large mixed dataset. It outperforms others on the new BOOM benchmark and established benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multivariate observability time series data with a scalable, high-performance foundation model.

Method: Uses a decoder-only architecture and trains on a mix of observability, open, and synthetic data (4-10x larger than competitors). Introduces BOOM, a 350M-observation benchmark.

Result: Achieves state-of-the-art performance on BOOM and general benchmarks.

Conclusion: Toto and BOOM are open-sourced, offering tools for advanced time series forecasting.

Abstract: We introduce Toto, a time series forecasting foundation model with 151
million parameters. Toto uses a modern decoder-only architecture coupled with
architectural innovations designed to account for specific challenges found in
multivariate observability time series data. Toto's pre-training corpus is a
mixture of observability data, open datasets, and synthetic data, and is
4-10$\times$ larger than those of leading time series foundation models.
Additionally, we introduce BOOM, a large-scale benchmark consisting of 350
million observations across 2,807 real-world time series. For both Toto and
BOOM, we source observability data exclusively from Datadog's own telemetry and
internal observability metrics. Extensive evaluations demonstrate that Toto
achieves state-of-the-art performance on both BOOM and on established general
purpose time series forecasting benchmarks. Toto's model weights, inference
code, and evaluation scripts, as well as BOOM's data and evaluation code, are
all available as open source under the Apache 2.0 License available at
https://huggingface.co/Datadog/Toto-Open-Base-1.0 and
https://github.com/DataDog/toto.

</details>


### [492] [KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches](https://arxiv.org/pdf/2505.14777)
*Mingquan Feng, Yixin Huang, Yifan Fu, Shaobo Wang, Junchi Yan*

Main category: cs.LG

TL;DR: KO (Kinetics-inspired Optimizer) is a novel neural optimizer based on kinetic theory and PDEs, outperforming traditional methods like Adam and SGD in accuracy while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing neural network optimizers rely on heuristic adaptations of gradient-based methods, lacking theoretical grounding. KO addresses this by drawing inspiration from kinetic theory and PDEs.

Method: KO models parameter updates using a numerical scheme for the Boltzmann transport equation (BTE), simulating particle collisions to promote parameter diversity and prevent condensation.

Result: KO consistently outperforms baseline optimizers (Adam, SGD) in tasks like image and text classification, improving accuracy without increasing computational cost.

Conclusion: KO provides a physics-driven, theoretically grounded alternative to heuristic optimizers, demonstrating superior performance in diverse tasks.

Abstract: The design of optimization algorithms for neural networks remains a critical
challenge, with most existing methods relying on heuristic adaptations of
gradient-based approaches. This paper introduces KO (Kinetics-inspired
Optimizer), a novel neural optimizer inspired by kinetic theory and partial
differential equation (PDE) simulations. We reimagine the training dynamics of
network parameters as the evolution of a particle system governed by kinetic
principles, where parameter updates are simulated via a numerical scheme for
the Boltzmann transport equation (BTE) that models stochastic particle
collisions. This physics-driven approach inherently promotes parameter
diversity during optimization, mitigating the phenomenon of parameter
condensation, i.e. collapse of network parameters into low-dimensional
subspaces, through mechanisms analogous to thermal diffusion in physical
systems. We analyze this property, establishing both a mathematical proof and a
physical interpretation. Extensive experiments on image classification
(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks
demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,
SGD), achieving accuracy improvements while computation cost remains
comparable.

</details>


### [493] [Text embedding models can be great data engineers](https://arxiv.org/pdf/2505.14802)
*Iman Kazemian, Paritosh Ramanan, Murat Yildirim*

Main category: cs.LG

TL;DR: ADEPT automates data engineering pipelines using text embeddings, outperforming traditional methods in predictive performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Data engineering pipelines are costly and time-consuming, requiring domain expertise. ADEPT aims to automate these tasks efficiently.

Method: ADEPT uses text embeddings for raw time series data and applies a variational information bottleneck to reduce entropy variance.

Result: ADEPT outperforms existing benchmarks in healthcare, finance, science, and IoT, handling issues like missing data and irregular timestamps.

Conclusion: ADEPT offers a scalable, automated alternative to conventional data pipelines, enhancing efficiency in data science applications.

Abstract: Data engineering pipelines are essential - albeit costly - components of
predictive analytics frameworks requiring significant engineering time and
domain expertise for carrying out tasks such as data ingestion, preprocessing,
feature extraction, and feature engineering. In this paper, we propose ADEPT,
an automated data engineering pipeline via text embeddings. At the core of the
ADEPT framework is a simple yet powerful idea that the entropy of embeddings
corresponding to textually dense raw format representation of time series can
be intuitively viewed as equivalent (or in many cases superior) to that of
numerically dense vector representations obtained by data engineering
pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text
embeddings to represent the diverse data sources, and (ii) constructs a
variational information bottleneck criteria to mitigate entropy variance in
text embeddings of time series data. ADEPT provides an end-to-end automated
implementation of predictive models that offers superior predictive performance
despite issues such as missing data, ill-formed records, improper or corrupted
data formats and irregular timestamps. Through exhaustive experiments, we show
that the ADEPT outperforms the best existing benchmarks in a diverse set of
datasets from large-scale applications across healthcare, finance, science and
industrial internet of things. Our results show that ADEPT can potentially
leapfrog many conventional data pipeline steps thereby paving the way for
efficient and scalable automation pathways for diverse data science
applications.

</details>


### [494] [SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis](https://arxiv.org/pdf/2505.14803)
*Yu Liu, Weiyao Tao, Tong Xia, Simon Knight, Tingting Zhu*

Main category: cs.LG

TL;DR: SurvUnc is a model-agnostic framework for post-hoc uncertainty quantification in survival models, improving interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: Current survival models lack reliable uncertainty quantification, limiting their trustworthiness in critical applications like healthcare.

Method: SurvUnc uses an anchor-based learning strategy integrating concordance knowledge into meta-model optimization for uncertainty estimation.

Result: Experiments on four datasets and five models show SurvUnc's superiority in selective prediction, misprediction detection, and out-of-domain detection.

Conclusion: SurvUnc enhances survival model reliability, enabling more trustworthy predictions in real-world applications.

Abstract: Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.

</details>


### [495] [Imitation Learning via Focused Satisficing](https://arxiv.org/pdf/2505.14820)
*Rushit N. Shah, Nikolaos Agadakos, Synthia Sasulski, Ali Farajzadeh, Sanjiban Choudhury, Brian Ziebart*

Main category: cs.LG

TL;DR: The paper introduces a satisficing approach to imitation learning, focusing on surpassing demonstrator aspiration levels without explicitly learning them, improving imitation quality and acceptability.


<details>
  <summary>Details</summary>
Motivation: Human demonstrations often reflect satisficing behavior (acceptable but not optimal), which traditional imitation learning overlooks.

Method: Uses a margin-based objective in deep reinforcement learning to surpass demonstrator aspiration levels on unseen demonstrations.

Result: Outperforms existing methods in imitating high-quality portions of demonstrations, ensuring higher acceptability and competitive returns.

Conclusion: The satisficing approach effectively improves imitation learning by focusing on surpassing demonstrator aspirations, enhancing policy quality.

Abstract: Imitation learning often assumes that demonstrations are close to optimal
according to some fixed, but unknown, cost function. However, according to
satisficing theory, humans often choose acceptable behavior based on their
personal (and potentially dynamic) levels of aspiration, rather than achieving
(near-) optimality. For example, a lunar lander demonstration that successfully
lands without crashing might be acceptable to a novice despite being slow or
jerky. Using a margin-based objective to guide deep reinforcement learning, our
focused satisficing approach to imitation learning seeks a policy that
surpasses the demonstrator's aspiration levels -- defined over trajectories or
portions of trajectories -- on unseen demonstrations without explicitly
learning those aspirations. We show experimentally that this focuses the policy
to imitate the highest quality (portions of) demonstrations better than
existing imitation learning methods, providing much higher rates of guaranteed
acceptability to the demonstrator, and competitive true returns on a range of
environments.

</details>


### [496] [Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation](https://arxiv.org/pdf/2505.14821)
*Runze Zhao, Yue Yu, Adams Yiyue Zhu, Chen Yang, Dongruo Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continuous-time reinforcement learning (CTRL) provides a principled framework
for sequential decision-making in environments where interactions evolve
continuously over time. Despite its empirical success, the theoretical
understanding of CTRL remains limited, especially in settings with general
function approximation. In this work, we propose a model-based CTRL algorithm
that achieves both sample and computational efficiency. Our approach leverages
optimism-based confidence sets to establish the first sample complexity
guarantee for CTRL with general function approximation, showing that a
near-optimal policy can be learned with a suboptimality gap of
$\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$
measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the
distributional Eluder dimensions of the reward and dynamic functions,
respectively, capturing the complexity of general function approximation in
reinforcement learning. Moreover, we introduce structured policy updates and an
alternative measurement strategy that significantly reduce the number of policy
updates and rollouts while maintaining competitive sample efficiency. We
implemented experiments to backup our proposed algorithms on continuous control
tasks and diffusion model fine-tuning, demonstrating comparable performance
with significantly fewer policy updates and rollouts.

</details>


### [497] [Assimilative Causal Inference](https://arxiv.org/pdf/2505.14825)
*Marios Andreou, Nan Chen, Erik Bollt*

Main category: cs.LG

TL;DR: A new causal inference framework, ACI, uses Bayesian data assimilation to identify dynamic causal relationships and their evolution, scalable to high-dimensional systems and applicable to incomplete datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack the ability to capture dynamic, instantaneous causal relationships in high-dimensional systems or with incomplete data.

Method: ACI solves an inverse problem via Bayesian data assimilation, tracing causes from effects and assessing uncertainty reduction.

Result: ACI dynamically identifies causal roles, scales to high dimensions, and works with short or incomplete data, even without observing candidate causes.

Conclusion: ACI is a versatile and effective framework for causal inference in complex systems, outperforming traditional methods.

Abstract: Causal inference determines cause-and-effect relationships between variables
and has broad applications across disciplines. Traditional time-series methods
often reveal causal links only in a time-averaged sense, while ensemble-based
information transfer approaches detect the time evolution of short-term causal
relationships but are typically limited to low-dimensional systems. In this
paper, a new causal inference framework, called assimilative causal inference
(ACI), is developed. Fundamentally different from the state-of-the-art methods,
ACI uses a dynamical system and a single realization of a subset of the state
variables to identify instantaneous causal relationships and the dynamic
evolution of the associated causal influence range (CIR). Instead of
quantifying how causes influence effects as done traditionally, ACI solves an
inverse problem via Bayesian data assimilation, thus tracing causes backward
from observed effects with an implicit Bayesian hypothesis. Causality is
determined by assessing whether incorporating the information of the effect
variables reduces the uncertainty in recovering the potential cause variables.
ACI has several desirable features. First, it captures the dynamic interplay of
variables, where their roles as causes and effects can shift repeatedly over
time. Second, a mathematically justified objective criterion determines the CIR
without empirical thresholds. Third, ACI is scalable to high-dimensional
problems by leveraging computationally efficient Bayesian data assimilation
techniques. Finally, ACI applies to short time series and incomplete datasets.
Notably, ACI does not require observations of candidate causes, which is a key
advantage since potential drivers are often unknown or unmeasured. The
effectiveness of ACI is demonstrated by complex dynamical systems showcasing
intermittency and extreme events.

</details>


### [498] [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/pdf/2505.14826)
*Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton*

Main category: cs.LG

TL;DR: The paper proposes a method to improve supervised fine-tuning (SFT) of large language models (LLMs) by selecting the most informative training examples, maximizing information gain via the Hessian of the log-likelihood.


<details>
  <summary>Details</summary>
Motivation: To enhance the statistical efficiency of SFT for LLMs by optimizing the selection of training examples within a fixed computational budget.

Method: Selects informative examples by maximizing information gain, measured using the Hessian of the log-likelihood. Approximates this efficiently by linearizing the LLM at the last layer with multinomial logistic regression models.

Result: The approach is computationally efficient, analyzable, and performs well empirically across several problems, supported by quantitative results and LLM evaluations.

Conclusion: The proposed method effectively improves SFT efficiency by focusing on the most informative training examples, backed by empirical success.

Abstract: Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.

</details>


### [499] [Deep Koopman operator framework for causal discovery in nonlinear dynamical systems](https://arxiv.org/pdf/2505.14828)
*Juan Nathaniel, Carla Roesch, Jatan Buch, Derek DeSantis, Adam Rupe, Kara Lamb, Pierre Gentine*

Main category: cs.LG

TL;DR: Kausal is a novel causal discovery algorithm using deep Koopman operator theory to address nonlinear dynamics, outperforming existing methods like Granger causality.


<details>
  <summary>Details</summary>
Motivation: To improve causal discovery in nonlinear systems with complex feedback, timescale mixing, and nonstationarity, where traditional methods fail.

Method: Leverages deep learning to infer optimal observables and evaluates causal relationships in a reproducing kernel Hilbert space.

Result: Demonstrates superior performance in discovering causal signals, validated with El Niño-Southern Oscillation data.

Conclusion: Kausal effectively addresses nonlinear causal discovery, with real-world applicability and open-source availability.

Abstract: We use a deep Koopman operator-theoretic formalism to develop a novel causal
discovery algorithm, Kausal. Causal discovery aims to identify cause-effect
mechanisms for better scientific understanding, explainable decision-making,
and more accurate modeling. Standard statistical frameworks, such as Granger
causality, lack the ability to quantify causal relationships in nonlinear
dynamics due to the presence of complex feedback mechanisms, timescale mixing,
and nonstationarity. This presents a challenge in studying many real-world
systems, such as the Earth's climate. Meanwhile, Koopman operator methods have
emerged as a promising tool for approximating nonlinear dynamics in a linear
space of observables. In Kausal, we propose to leverage this powerful idea for
causal analysis where optimal observables are inferred using deep learning.
Causal estimates are then evaluated in a reproducing kernel Hilbert space, and
defined as the distance between the marginal dynamics of the effect and the
joint dynamics of the cause-effect observables. Our numerical experiments
demonstrate Kausal's superior ability in discovering and characterizing causal
signals compared to existing approaches of prescribed observables. Lastly, we
extend our analysis to observations of El Ni\~no-Southern Oscillation
highlighting our algorithm's applicability to real-world phenomena. Our code is
available at https://github.com/juannat7/kausal.

</details>


### [500] [Subquadratic Algorithms and Hardness for Attention with Any Temperature](https://arxiv.org/pdf/2505.14840)
*Shreya Gupta, Boyang Huang, Barna Saha, Yinzhan Xu, Christopher Ye*

Main category: cs.LG

TL;DR: The paper addresses the inefficiency of standard Attention computation in Transformers, proposing subquadratic algorithms for large entry sizes and low-rank matrices, while also proving limits on further improvements under complexity hypotheses.


<details>
  <summary>Details</summary>
Motivation: The quadratic time complexity of standard Attention computation is a bottleneck, especially for large contexts. The paper aims to identify conditions for efficient Attention computation without restrictive assumptions on input values or temperature.

Method: The authors develop subquadratic algorithms for Attention with large entry sizes and low-rank matrices, and analyze their performance under the Strong Exponential Time Hypothesis (SETH).

Result: They achieve subquadratic time complexity for Attention with large entries and low-rank matrices, and show that further improvements are unlikely under SETH.

Conclusion: Efficient Attention computation is possible under specific conditions, but fundamental limits exist, especially for high-dimensional or large-scale inputs.

Abstract: Despite the popularity of the Transformer architecture, the standard
algorithm for computing Attention suffers from quadratic time complexity in
context length $n$. Alman and Song [NeurIPS 2023] showed that when the head
dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only
if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute
values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$).
Equivalently, subquadratic Attention is possible if and only if the softmax is
applied with high temperature for $d=\Theta(\log n)$. Running times of these
algorithms depend exponentially on $B$ and thus they do not lead to even a
polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed
efficiently without strong assumptions on temperature? Are there fast attention
algorithms that scale polylogarithmically with entry size $B$? In this work, we
resolve this question and characterize when fast Attention for arbitrary
temperatures is possible. First, for all constant $d = O(1)$, we give the first
subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm
for Attention with large $B$. Our result holds even for matrices with large
head dimension if they have low rank. In this regime, we also give a similar
running time for Attention gradient computation, and therefore for the full LLM
training process. Furthermore, we show that any substantial improvement on our
algorithm is unlikely. In particular, we show that even when $d =
2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under
$\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the
standard algorithm is optimal under popular fine-grained complexity
assumptions.

</details>


### [501] [A self-regulated convolutional neural network for classifying variable stars](https://arxiv.org/pdf/2505.14877)
*Francisco Pérez-Galarce, Jorge Martínez-Palomera, Karim Pichara, Pablo Huijse, Márcio Catelan*

Main category: cs.LG

TL;DR: A self-regulated training process using synthetic data improves variable star classification by reducing biases in datasets.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for variable star classification suffer from biases due to limited or unrepresentative training data, leading to unreliable results.

Method: Proposes a self-regulated training process with a physics-enhanced variational autoencoder to generate synthetic light curves, dynamically interacting with a classifier to reduce biases.

Result: The method outperforms traditional training, showing statistically significant improvements in classification accuracy on biased datasets.

Conclusion: The self-regulated approach enhances classifier reliability by addressing data biases, offering a promising solution for variable star classification.

Abstract: Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.

</details>


### [502] [An active learning framework for multi-group mean estimation](https://arxiv.org/pdf/2505.14882)
*Abdellah Aznag, Rachel Cummings, Adam N. Elmachtoub*

Main category: cs.LG

TL;DR: The paper proposes Variance-UCB, an algorithm for dynamically collecting data across multiple groups to minimize collective noise in mean estimation, improving upon existing bounds.


<details>
  <summary>Details</summary>
Motivation: To address fair and efficient data collection in dynamic settings like online platforms or clinical trials, ensuring reasonable noise levels in group mean estimates.

Method: Uses an active learning framework with bandit feedback, updating mean and variance estimates sequentially and selecting groups via Variance-UCB to minimize collective noise.

Result: Variance-UCB provides improved regret bounds and adapts to various distributions and objectives, outperforming existing methods.

Conclusion: The proposed algorithm and framework offer significant advancements in dynamic data collection for fair and accurate group mean estimation.

Abstract: We study a fundamental learning problem over multiple groups with unknown
data distributions, where an analyst would like to learn the mean of each
group. Moreover, we want to ensure that this data is collected in a relatively
fair manner such that the noise of the estimate of each group is reasonable. In
particular, we focus on settings where data are collected dynamically, which is
important in adaptive experimentation for online platforms or adaptive clinical
trials for healthcare. In our model, we employ an active learning framework to
sequentially collect samples with bandit feedback, observing a sample in each
period from the chosen group. After observing a sample, the analyst updates
their estimate of the mean and variance of that group and chooses the next
group accordingly. The analyst's objective is to dynamically collect samples to
minimize the collective noise of the estimators, measured by the norm of the
vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups
according to an upper confidence bound on the variance estimate. We provide a
general theoretical framework for providing efficient bounds on learning from
any underlying distribution where the variances can be estimated reasonably.
This framework yields upper bounds on regret that improve significantly upon
all existing bounds, as well as a collection of new results for different
objectives and distributions than those previously studied.

</details>


### [503] [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/pdf/2505.14884)
*Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy*

Main category: cs.LG

TL;DR: Polar Sparsity improves LLM inference by focusing sparsity on Attention layers, achieving 2.2× speedups without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Accelerating LLM inference for high throughput and low latency in real-world deployments.

Method: Introduces Polar Sparsity, shifting sparsity focus to Attention layers, and develops efficient GPU kernels for selective computations.

Result: Achieves up to 2.2× speedups for models like OPT and LLaMA-2/3 across batch sizes and sequence lengths.

Conclusion: Polar Sparsity scales contextual sparsity to large batches, enabling practical, high-throughput LLM deployment.

Abstract: Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.

</details>


### [504] [Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis](https://arxiv.org/pdf/2505.14896)
*Hootan Mahmoodiyan, Maryam Ahang, Mostafa Abbasi, Homayoun Najjaran*

Main category: cs.LG

TL;DR: A feature-weighted domain adaptation technique (MCW) improves fault diagnosis in power transformers by addressing distribution shifts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional DGA methods and direct ML model transfers fail due to inconsistent conditions and distribution shifts in transformer data.

Method: Proposes MCW, combining MMD and CORAL with feature-specific weighting using K-S statistics to prioritize features with larger discrepancies.

Result: MCW achieves 7.9% and 2.2% improvements over Fine-Tuning and MMD-CORAL, respectively, and shows robustness across sample sizes.

Conclusion: MCW effectively addresses domain adaptation challenges in transformer fault diagnosis, enhancing reliability and accuracy.

Abstract: Ensuring the reliable operation of power transformers is critical to grid
stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but
traditional methods rely on heuristic rules, which may lead to inconsistent
results. Machine learning (ML)-based approaches have improved diagnostic
accuracy; however, power transformers operate under varying conditions, and
differences in transformer type, environmental factors, and operational
settings create distribution shifts in diagnostic data. Consequently, direct
model transfer between transformers often fails, making techniques for domain
adaptation a necessity. To tackle this issue, this work proposes a
feature-weighted domain adaptation technique that combines Maximum Mean
Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific
weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign
adaptable weights, prioritizing features with larger distributional
discrepancies and thereby improving source and target domain alignment.
Experimental evaluations on datasets for power transformers demonstrate the
effectiveness of the proposed method, which achieves a 7.9% improvement over
Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it
outperforms both techniques across various training sample sizes, confirming
its robustness for domain adaptation.

</details>


### [505] [Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction](https://arxiv.org/pdf/2505.14897)
*Ali Mohajerzarrinkelk, Maryam Ahang, Mehran Zoravar, Mostafa Abbasi, Homayoun Najjaran*

Main category: cs.LG

TL;DR: A novel framework combining wavelet-based denoising, WPD, and MCSFormer improves RUL estimation for rolling bearings, outperforming state-of-the-art models with better noise resistance and safety focus.


<details>
  <summary>Details</summary>
Motivation: Accurate RUL estimation is critical to prevent failures and enhance safety in industrial systems, but challenges like noise and degradation trends complicate the task.

Method: The framework uses wavelet-based denoising, WPD, and a customized MCSFormer with attention mechanisms and a unique loss function for early/late prediction differentiation.

Result: MCSFormer achieved 41-69% lower MAE than competitors in intra-condition tests and superior generalization in cross-condition tests, with a 6.3% improvement in scoring.

Conclusion: MCSFormer is a robust, generalizable, and safety-focused tool for predictive maintenance in industrial applications.

Abstract: Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is
an important consideration to avoid unexpected failures, reduce downtime, and
promote safety and efficiency in industrial systems. Complications in
degradation trends, noise presence, and the necessity to detect faults in
advance make estimation of RUL a challenging task. This paper introduces a
novel framework that combines wavelet-based denoising method, Wavelet Packet
Decomposition (WPD), and a customized multi-channel Swin Transformer model
(MCSFormer) to address these problems. With attention mechanisms incorporated
for feature fusion, the model is designed to learn global and local degradation
patterns utilizing hierarchical representations for enhancing predictive
performance. Additionally, a customized loss function is developed as a key
distinction of this work to differentiate between early and late predictions,
prioritizing accurate early detection and minimizing the high operation risks
of late predictions. The proposed model was evaluated with the PRONOSTIA
dataset using three experiments. Intra-condition experiments demonstrated that
MCSFormer outperformed state-of-the-art models, including the Adaptive
Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on
average across different operating conditions, respectively. In terms of
cross-condition testing, it achieved superior generalization under varying
operating conditions compared to the adapted ViT and Swin Transformer. Lastly,
the custom loss function effectively reduced late predictions, as evidenced in
a 6.3% improvement in the scoring metric while maintaining competitive overall
performance. The model's robust noise resistance, generalization capability,
and focus on safety make MCSFormer a trustworthy and effective predictive
maintenance tool in industrial applications.

</details>


### [506] [When to retrain a machine learning model](https://arxiv.org/pdf/2505.14903)
*Regol Florence, Schwinn Leo, Sprague Kyle, Coates Mark, Markovich Thomas*

Main category: cs.LG

TL;DR: The paper addresses the challenge of deciding when to retrain machine learning models due to evolving data, proposing an uncertainty-based method that outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: The difficulty in deciding when to retrain models due to limited information, unknown distribution shifts, and unclear cost trade-offs.

Method: An uncertainty-based method that forecasts model performance evolution using a bounded metric.

Result: The proposed method consistently outperforms existing baselines on 7 classification datasets.

Conclusion: The method provides a principled solution to the retraining problem, addressing key practical challenges.

Abstract: A significant challenge in maintaining real-world machine learning models is
responding to the continuous and unpredictable evolution of data. Most
practitioners are faced with the difficult question: when should I retrain or
update my machine learning model? This seemingly straightforward problem is
particularly challenging for three reasons: 1) decisions must be made based on
very limited information - we usually have access to only a few examples, 2)
the nature, extent, and impact of the distribution shift are unknown, and 3) it
involves specifying a cost ratio between retraining and poor performance, which
can be hard to characterize. Existing works address certain aspects of this
problem, but none offer a comprehensive solution. Distribution shift detection
falls short as it cannot account for the cost trade-off; the scarcity of the
data, paired with its unusual structure, makes it a poor fit for existing
offline reinforcement learning methods, and the online learning formulation
overlooks key practical considerations. To address this, we present a
principled formulation of the retraining problem and propose an
uncertainty-based method that makes decisions by continually forecasting the
evolution of model performance evaluated with a bounded metric. Our experiments
addressing classification tasks show that the method consistently outperforms
existing baselines on 7 datasets.

</details>


### [507] [TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction](https://arxiv.org/pdf/2505.14919)
*Frederik Wenkel, Wilson Tu, Cassandra Masschelein, Hamed Shirzad, Cian Eastwood, Shawn T. Whitfield, Ihab Bendidi, Craig Russell, Liam Hodgson, Yassir El Mesbahi, Jiarui Ding, Marta M. Fay, Berton Earnshaw, Emmanuel Noutahi, Alisandra K. Denton*

Main category: cs.LG

TL;DR: TxPert leverages knowledge graphs to predict cellular responses to unseen genetic perturbations, improving OOD prediction in single/double perturbations and cell lines.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of cellular responses to genetic perturbations is crucial for disease understanding and therapy design, but exhaustive exploration is impractical.

Method: TxPert uses multiple biological knowledge graphs to predict transcriptional responses in OOD scenarios, with analysis of graph impact, model architecture, and data.

Result: TxPert achieves state-of-the-art performance in predicting responses to unseen perturbations and cell lines.

Conclusion: The work introduces TxPert, a robust method for OOD prediction, and establishes a benchmarking framework for perturbation modeling.

Abstract: Accurately predicting cellular responses to genetic perturbations is
essential for understanding disease mechanisms and designing effective
therapies. Yet exhaustively exploring the space of possible perturbations
(e.g., multi-gene perturbations or across tissues and cell types) is
prohibitively expensive, motivating methods that can generalize to unseen
conditions. In this work, we explore how knowledge graphs of gene-gene
relationships can improve out-of-distribution (OOD) prediction across three
challenging settings: unseen single perturbations; unseen double perturbations;
and unseen cell lines. In particular, we present: (i) TxPert, a new
state-of-the-art method that leverages multiple biological knowledge networks
to predict transcriptional responses under OOD scenarios; (ii) an in-depth
analysis demonstrating the impact of graphs, model architecture, and data on
performance; and (iii) an expanded benchmarking framework that strengthens
evaluation standards for perturbation modeling.

</details>


### [508] [Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities](https://arxiv.org/pdf/2505.14943)
*Ross Nordby*

Main category: cs.LG

TL;DR: The paper introduces 'soft prompts' to measure conditional distance between language models and target behaviors, aiding in latent capability discovery and evaluation.


<details>
  <summary>Details</summary>
Motivation: To evaluate and understand latent capabilities of language models, especially for automated red teaming and assessing concerning behaviors in future models.

Method: Uses optimized input embeddings (soft prompts) as a metric, demonstrated in natural language, chess, and pathfinding. Extends to generalized conditional soft prompts for task evaluations.

Result: Demonstrates the effectiveness of soft prompts in evaluating model capabilities across diverse tasks.

Conclusion: Soft prompts provide scalable, quantitative feedback for latent capability discovery, useful for future model evaluations.

Abstract: To help evaluate and understand the latent capabilities of language models,
this paper introduces an approach using optimized input embeddings, or 'soft
prompts,' as a metric of conditional distance between a model and a target
behavior. The technique aims to facilitate latent capability discovery as a
part of automated red teaming/evaluation suites and to provide quantitative
feedback about the accessibility of potentially concerning behaviors in a way
that may scale to powerful future models, including those which may otherwise
be capable of deceptive alignment. An evaluation framework using soft prompts
is demonstrated in natural language, chess, and pathfinding, and the technique
is extended with generalized conditional soft prompts to aid in constructing
task evaluations.

</details>


### [509] [Foundations of Unknown-aware Machine Learning](https://arxiv.org/pdf/2505.14933)
*Xuefeng Du*

Main category: cs.LG

TL;DR: The thesis addresses reliability and safety in AI by developing frameworks for handling distributional uncertainty and unknown classes, introducing methods like VOS, NPOS, and SAL, and extending solutions to foundation models like LLMs.


<details>
  <summary>Details</summary>
Motivation: To tackle overconfidence and failure modes in AI models, especially under distribution shifts and unknown inputs, ensuring safer deployment.

Method: Proposes unknown-aware learning, outlier synthesis (VOS, NPOS, DREAM-OOD), and SAL framework for OOD detection. Extends to foundation models with tools like HaloScope and MLLMGuard.

Result: Demonstrates improved reliability and safety in models, with formal guarantees for handling unforeseen inputs and malicious prompts.

Conclusion: Unknown-aware learning is a promising paradigm for enhancing AI reliability with minimal human intervention.

Abstract: Ensuring the reliability and safety of machine learning models in open-world
deployment is a central challenge in AI safety. This thesis develops both
algorithmic and theoretical foundations to address key reliability issues
arising from distributional uncertainty and unknown classes, from standard
neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM),
assume no distribution shift between training and inference, often leading to
overconfident predictions on out-of-distribution (OOD) inputs. This thesis
introduces novel frameworks that jointly optimize for in-distribution accuracy
and reliability to unseen data. A core contribution is the development of an
unknown-aware learning framework that enables models to recognize and handle
novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to
generate informative unknowns during training. Building on this, we present
SAL, a theoretical and algorithmic framework that leverages unlabeled
in-the-wild data to enhance OOD detection under realistic deployment
conditions. These methods demonstrate that abundant unlabeled data can be
harnessed to recognize and adapt to unforeseen inputs, providing formal
reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop
HaloScope for hallucination detection in LLMs, MLLMGuard for defending against
malicious prompts in multimodal models, and data cleaning methods to denoise
human feedback used for better alignment. These tools target failure modes that
threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new
paradigm, and we hope it can advance the reliability of AI systems with minimal
human efforts.

</details>


### [510] [The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models](https://arxiv.org/pdf/2505.14964)
*Dave Cook, Tim Klawa*

Main category: cs.LG

TL;DR: Smart-sizing, a data strategy focusing on label diversity and model-guided selection, outperforms traditional methods with 20-40% of curated data, improving rare-class recall and edge-case generalization.


<details>
  <summary>Details</summary>
Motivation: AI systems in high-consequence domains need efficient data strategies to detect rare events under resource constraints, avoiding redundancy and noise from traditional annotation.

Method: Introduces smart-sizing via Adaptive Label Optimization (ALO), combining pre-labeling triage, annotator disagreement analysis, and iterative feedback to prioritize impactful labels.

Result: Models trained on 20-40% of curated data match or exceed full-data baselines, especially in rare-class recall and edge-case generalization. Latent labeling errors are highlighted as a distortion risk.

Conclusion: Smart-sizing aligns annotation with mission outcomes, enabling robust models with fewer labels and supporting efficient AI development for high-stakes applications.

Abstract: AI systems in high-consequence domains such as defense, intelligence, and
disaster response must detect rare, high-impact events while operating under
tight resource constraints. Traditional annotation strategies that prioritize
label volume over informational value introduce redundancy and noise, limiting
model generalization. This paper introduces smart-sizing, a training data
strategy that emphasizes label diversity, model-guided selection, and marginal
utility-based stopping. We implement this through Adaptive Label Optimization
(ALO), combining pre-labeling triage, annotator disagreement analysis, and
iterative feedback to prioritize labels that meaningfully improve model
performance. Experiments show that models trained on 20 to 40 percent of
curated data can match or exceed full-data baselines, particularly in
rare-class recall and edge-case generalization. We also demonstrate how latent
labeling errors embedded in training and validation sets can distort
evaluation, underscoring the need for embedded audit tools and
performance-aware governance. Smart-sizing reframes annotation as a
feedback-driven process aligned with mission outcomes, enabling more robust
models with fewer labels and supporting efficient AI development pipelines for
frontier models and operational systems.

</details>


### [511] [Unlearning Algorithmic Biases over Graphs](https://arxiv.org/pdf/2505.14945)
*O. Deniz Kose, Gonzalo Mateos, Yanning Shen*

Main category: cs.LG

TL;DR: The paper proposes a training-free graph unlearning method for bias mitigation in ML models, using a single-step Newton update. It offers certified fairness guarantees and outperforms retraining methods in utility-complexity trade-offs.


<details>
  <summary>Details</summary>
Motivation: The right to be forgotten and bias amplification in graph data motivate the need for efficient, certified unlearning strategies to mitigate bias without retraining.

Method: Develops a training-free unlearning procedure via a Newton update on model weights, with fairness-aware nodal feature unlearning and structural unlearning methods based on bias analyses.

Result: Experimental results show effective bias mitigation with minimal impact on utility, outperforming retraining from scratch.

Conclusion: The proposed unlearning strategy provides a lightweight, certified solution for bias mitigation in graph ML models, with strong utility-complexity trade-offs.

Abstract: The growing enforcement of the right to be forgotten regulations has
propelled recent advances in certified (graph) unlearning strategies to comply
with data removal requests from deployed machine learning (ML) models.
Motivated by the well-documented bias amplification predicament inherent to
graph data, here we take a fresh look at graph unlearning and leverage it as a
bias mitigation tool. Given a pre-trained graph ML model, we develop a
training-free unlearning procedure that offers certifiable bias mitigation via
a single-step Newton update on the model weights. This way, we contribute a
computationally lightweight alternative to the prevalent training- and
optimization-based fairness enhancement approaches, with quantifiable
performance guarantees. We first develop a novel fairness-aware nodal feature
unlearning strategy along with refined certified unlearning bounds for this
setting, whose impact extends beyond the realm of graph unlearning. We then
design structural unlearning methods endowed with principled selection
mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning
these judiciously selected elements can mitigate algorithmic biases with
minimal impact on downstream utility (e.g., node classification accuracy).
Experimental results over real networks corroborate the bias mitigation
efficacy of our unlearning strategies, and delineate markedly favorable
utility-complexity trade-offs relative to retraining from scratch using
augmented graph data obtained via removals.

</details>


### [512] [Anomaly Detection Based on Critical Paths for Deep Neural Networks](https://arxiv.org/pdf/2505.14967)
*Fangzhen Zhao, Chenyi Zhang, Naipeng Dong, Ming Li, Jinxiao Shan*

Main category: cs.LG

TL;DR: The paper proposes a method to extract critical paths from DNNs for anomaly detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: DNNs are hard to interpret and defend. Extracting paths to understand their decision-making process is promising for anomaly detection.

Method: Identify critical paths via genetic evolution, ensemble results using random subspace sampling and voting.

Result: Outperforms state-of-the-art methods and detects a broad range of anomalies accurately.

Conclusion: The approach effectively interprets DNNs and improves anomaly detection.

Abstract: Deep neural networks (DNNs) are notoriously hard to understand and difficult
to defend. Extracting representative paths (including the neuron activation
values and the connections between neurons) from DNNs using software
engineering approaches has recently shown to be a promising approach in
interpreting the decision making process of blackbox DNNs, as the extracted
paths are often effective in capturing essential features. With this in mind,
this work investigates a novel approach that extracts critical paths from DNNs
and subsequently applies the extracted paths for the anomaly detection task,
based on the observation that outliers and adversarial inputs do not usually
induce the same activation pattern on those paths as normal (in-distribution)
inputs.
  In our approach, we first identify critical detection paths via genetic
evolution and mutation. Since different paths in a DNN often capture different
features for the same target class, we ensemble detection results from multiple
paths by integrating random subspace sampling and a voting mechanism. Compared
with state-of-the-art methods, our experimental results suggest that our method
not only outperforms them, but it is also suitable for the detection of a broad
range of anomaly types with high accuracy.

</details>


### [513] [Privacy Preserving Conversion Modeling in Data Clean Room](https://arxiv.org/pdf/2505.14959)
*Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, Behnam Rezaei*

Main category: cs.LG

TL;DR: A novel framework for privacy-preserving CVR prediction in online advertising, using batch-level gradients, adapter-based tuning, and de-biasing techniques to balance performance and privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in CVR prediction due to privacy concerns and advertiser reluctance to share sensitive data, while maintaining model accuracy.

Method: Proposes a framework with batch-level aggregated gradients, adapter-based fine-tuning, gradient compression, and de-biasing under label differential privacy.

Result: Achieves competitive ROCAUC performance, reduces communication costs, and complies with privacy requirements.

Conclusion: Sets a new standard for privacy-preserving, high-performance CVR prediction in digital advertising.

Abstract: In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.

</details>


### [514] [STree: Speculative Tree Decoding for Hybrid State-Space Models](https://arxiv.org/pdf/2505.14969)
*Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto*

Main category: cs.LG

TL;DR: The paper introduces a scalable algorithm for tree-based speculative decoding in state-space models (SSMs) and hybrid architectures, improving efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of SSMs and hybrid models by enabling tree-based speculative decoding, which current SSMs lack.

Method: Proposes an algorithm leveraging accumulated state transition matrices for tree-based speculative decoding with minimal overhead.

Result: Outperforms vanilla speculative decoding on SSMs across three benchmarks, showing potential for further speed improvements.

Conclusion: The work opens new opportunities for faster SSM and hybrid model inference, with code to be released upon acceptance.

Abstract: Speculative decoding is a technique to leverage hardware concurrency to
improve the efficiency of large-scale autoregressive (AR) Transformer models by
enabling multiple steps of token generation in a single forward pass.
State-space models (SSMs) are already more efficient than AR Transformers,
since their state summarizes all past data with no need to cache or re-process
tokens in the sliding window context. However, their state can also comprise
thousands of tokens; so, speculative decoding has recently been extended to
SSMs. Existing approaches, however, do not leverage the tree-based verification
methods, since current SSMs lack the means to compute a token tree efficiently.
We propose the first scalable algorithm to perform tree-based speculative
decoding in state-space models (SSMs) and hybrid architectures of SSMs and
Transformer layers. We exploit the structure of accumulated state transition
matrices to facilitate tree-based speculative decoding with minimal overhead to
current SSM state update implementations. With the algorithm, we describe a
hardware-aware implementation that improves naive application of AR Transformer
tree-based speculative decoding methods to SSMs. Furthermore, we outperform
vanilla speculative decoding with SSMs even with a baseline drafting model and
tree structure on three different benchmarks, opening up opportunities for
further speed up with SSM and hybrid model inference. Code will be released
upon paper acceptance.

</details>


### [515] [Flattening Hierarchies with Policy Bootstrapping](https://arxiv.org/pdf/2505.14975)
*John L. Zhou, Jonathan C. Kao*

Main category: cs.LG

TL;DR: The paper introduces a flat goal-conditioned RL algorithm using subgoal-conditioned policies and advantage-weighted importance sampling, eliminating the need for hierarchical structures and scaling well in high-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of scaling offline goal-conditioned RL (GCRL) to long horizons due to sparse rewards and discounting, while avoiding the complexity of hierarchical methods.

Method: Proposes a flat goal-conditioned policy trained by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling, avoiding generative models for subgoals.

Result: The method matches or outperforms state-of-the-art offline GCRL algorithms on locomotion and manipulation tasks, scaling to complex, long-horizon tasks.

Conclusion: The approach simplifies scaling GCRL by removing hierarchical complexity and demonstrates strong performance in high-dimensional control tasks.

Abstract: Offline goal-conditioned reinforcement learning (GCRL) is a promising
approach for pretraining generalist policies on large datasets of reward-free
trajectories, akin to the self-supervised objectives used to train foundation
models for computer vision and natural language processing. However, scaling
GCRL to longer horizons remains challenging due to the combination of sparse
rewards and discounting, which obscures the comparative advantages of primitive
actions with respect to distant goals. Hierarchical RL methods achieve strong
empirical results on long-horizon goal-reaching tasks, but their reliance on
modular, timescale-specific policies and subgoal generation introduces
significant additional complexity and hinders scaling to high-dimensional goal
spaces. In this work, we introduce an algorithm to train a flat
(non-hierarchical) goal-conditioned policy by bootstrapping on
subgoal-conditioned policies with advantage-weighted importance sampling. Our
approach eliminates the need for a generative model over the (sub)goal space,
which we find is key for scaling to high-dimensional control in large state
spaces. We further show that existing hierarchical and bootstrapping-based
approaches correspond to specific design choices within our derivation. Across
a comprehensive suite of state- and pixel-based locomotion and manipulation
benchmarks, our method matches or surpasses state-of-the-art offline GCRL
algorithms and scales to complex, long-horizon tasks where prior approaches
fail.

</details>


### [516] [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/pdf/2505.14999)
*Eric Hanchen Jiang, Haozheng Luo, Shengyuan Pang, Xiaomin Li, Zhenting Qi, Hengli Li, Cheng-Fu Yang, Zongyu Lin, Xinfeng Li, Hao Xu, Kai-Wei Chang, Ying Nian Wu*

Main category: cs.LG

TL;DR: EORM is a lightweight post hoc verifier using Energy-Based Models to improve LLM reasoning reliability by ranking solutions with energy scores, avoiding costly sampling.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of unreliable reasoning in LLMs, especially in mathematical tasks, without extensive computational costs.

Method: EORM trains reward models using outcome labels to assign energy scores to Chain of Thought solutions, ranking them for correctness.

Result: Achieves high accuracy (90.7% on GSM8k, 63.7% on MATH) with Llama 3 8B, matching brute force sampling performance.

Conclusion: EORM efficiently enhances LLM reasoning reliability through streamlined verification, leveraging outcome labels for correctness.

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.

</details>


### [517] [Know When to Abstain: Optimal Selective Classification with Likelihood Ratios](https://arxiv.org/pdf/2505.15008)
*Alvin Heng, Harold Soh*

Main category: cs.LG

TL;DR: The paper revisits optimal selection functions for selective classification using the Neyman-Pearson lemma, proposing new methods that outperform baselines, especially under covariate shift.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of predictive models by allowing them to abstain from uncertain predictions, particularly under covariate shift.

Method: Uses the Neyman-Pearson lemma to design likelihood ratio-based selection functions, unifying existing baselines and proposing new approaches.

Result: Proposed methods consistently outperform baselines in vision and language tasks, including supervised learning and vision-language models.

Conclusion: Likelihood ratio-based selection is robust for improving selective classification under covariate shifts.

Abstract: Selective classification enhances the reliability of predictive models by
allowing them to abstain from making uncertain predictions. In this work, we
revisit the design of optimal selection functions through the lens of the
Neyman--Pearson lemma, a classical result in statistics that characterizes the
optimal rejection rule as a likelihood ratio test. We show that this
perspective not only unifies the behavior of several post-hoc selection
baselines, but also motivates new approaches to selective classification which
we propose here. A central focus of our work is the setting of covariate shift,
where the input distribution at test time differs from that at training. This
realistic and challenging scenario remains relatively underexplored in the
context of selective classification. We evaluate our proposed methods across a
range of vision and language tasks, including both supervised learning and
vision-language models. Our experiments demonstrate that our
Neyman--Pearson-informed methods consistently outperform existing baselines,
indicating that likelihood ratio-based selection offers a robust mechanism for
improving selective classification under covariate shifts. Our code is publicly
available at https://github.com/clear-nus/sc-likelihood-ratios.

</details>


### [518] [One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks](https://arxiv.org/pdf/2505.15009)
*Quan Nguyen, Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: The paper proves that certain one-layer transformers achieve Bayes-optimal performance in noiseless/noisy in-context reasoning, with linear convergence and generalization guarantees.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding the convergence rates, generalization abilities, and finite-sample behaviors of one-layer transformers in next-token prediction tasks.

Method: Analyzes one-layer transformers with linear and ReLU attention, trained via gradient descent, using finite-sample analysis.

Result: Shows linear convergence to Bayes risk and generalization to unseen samples, aligning with empirical observations.

Conclusion: Theoretical and empirical evidence confirms the effectiveness of these transformers in in-context reasoning tasks.

Abstract: We study the approximation capabilities and on-convergence behaviors of
one-layer transformers on the noiseless and noisy in-context reasoning of
next-token prediction. Existing theoretical results focus on understanding the
in-context reasoning behaviors for either the first gradient step or when the
number of samples is infinite. Furthermore, no convergence rates nor
generalization abilities were known. Our work addresses these gaps by showing
that there exists a class of one-layer transformers that are provably
Bayes-optimal with both linear and ReLU attention. When being trained with
gradient descent, we show via a finite-sample analysis that the expected loss
of these transformers converges at linear rate to the Bayes risk. Moreover, we
prove that the trained models generalize to unseen samples as well as exhibit
learning behaviors that were empirically observed in previous works. Our
theoretical findings are further supported by extensive empirical validations.

</details>


### [519] [Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing](https://arxiv.org/pdf/2505.15015)
*Longlong Li, Cunquan Qu, Guanghui Wang*

Main category: cs.LG

TL;DR: MSH-GNN introduces feature-wise adaptive message passing using harmonic projections and multi-frequency encodings, outperforming state-of-the-art GNNs in graph and node classification.


<details>
  <summary>Details</summary>
Motivation: Conventional GNNs lack fine-grained, direction-specific feature relevance, limiting their ability to capture structural patterns.

Method: MSH-GNN dynamically projects neighbor features onto frequency-sensitive directions, modulated by learnable sinusoidal encodings, and uses frequency-aware attention pooling.

Result: MSH-GNN outperforms state-of-the-art models in graph and node classification, excelling in tasks with structural asymmetries and high-frequency modulations.

Conclusion: MSH-GNN's harmonic projections and multi-scale approach enhance graph discrimination, matching the expressive power of the 1-WL test.

Abstract: Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as
holistic vectors, lacking the ability to identify fine-grained,
direction-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic
Graph Neural Network), a novel architecture that performs feature-wise adaptive
message passing through node-specific harmonic projections. For each node,
MSH-GNN dynamically projects neighbor features onto frequency-sensitive
directions determined by the target node's own representation. These
projections are further modulated using learnable sinusoidal encodings at
multiple frequencies, enabling the model to capture both smooth and oscillatory
structural patterns across scales. A frequency-aware attention pooling
mechanism is introduced to emphasize spectrally and structurally salient nodes
during readout. Theoretically, we prove that MSH-GNN approximates
shift-invariant kernels and matches the expressive power of the
1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms
state-of-the-art models on a wide range of graph and node classification tasks.
Furthermore, in challenging classification settings involving joint variations
in graph topology and spectral frequency, MSH-GNN excels at capturing
structural asymmetries and high-frequency modulations, enabling more accurate
graph discrimination.

</details>


### [520] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/pdf/2505.15034)
*Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S. Boning, Dina Katabi*

Main category: cs.LG

TL;DR: Tango is a framework using RL to jointly train an LLM generator and a generative verifier, improving robustness and generalization without process-level annotations.


<details>
  <summary>Details</summary>
Motivation: Current RL post-training methods for LLMs use fixed or discriminative verifiers, leading to reward hacking and poor generalization.

Method: Tango concurrently trains an LLM generator and a generative verifier via RL, using outcome-level rewards.

Result: Tango achieves state-of-the-art results on math and reasoning benchmarks, with significant improvements on difficult problems.

Conclusion: Tango's co-evolving generator and verifier framework enhances robustness and generalization, outperforming existing methods.

Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [521] [Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC](https://arxiv.org/pdf/2505.15030)
*Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan*

Main category: cs.LG

TL;DR: The paper introduces a methodology for evaluating on-device LLMs, revealing insights on performance, quantization, and power consumption for edge deployment.


<details>
  <summary>Details</summary>
Motivation: To address performance limitations of on-device LLMs due to reduced capacity and compression, ensuring efficient deployment on edge devices.

Method: A systematic evaluation of models (0.5B to 14B parameters) and seven PTQ methods on commodity laptops, focusing on system-level metrics, effective BPW, and power consumption.

Result: Key findings include near-linear scaling with BPW, a practical threshold at ~3.5 BPW, marginal accuracy loss with low BPW, and power consumption trends favoring computation over memory operations.

Conclusion: The study provides practical guidelines for optimizing LLM deployment on edge devices, balancing performance and resource constraints.

Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training quantization (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
low-bit quantization consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.

</details>


### [522] [RLBenchNet: The Right Network for the Right Reinforcement Learning Task](https://arxiv.org/pdf/2505.15040)
*Ivan Smirnov, Shangding Gu*

Main category: cs.LG

TL;DR: The paper evaluates neural network architectures in RL tasks, identifying strengths and limitations of MLPs, LSTMs, GRUs, Mamba models, and Transformers for different task types.


<details>
  <summary>Details</summary>
Motivation: To systematically compare neural network architectures in RL to guide architecture selection based on task requirements and computational constraints.

Method: Comprehensive evaluation of MLPs, LSTMs, GRUs, Mamba models, and Transformers across continuous control, discrete decision-making, and memory-based environments.

Result: MLPs excel in continuous control; LSTMs/GRUs perform well in partially observable tasks; Mamba models offer high throughput; Mamba-2 and Transformers solve memory-intensive tasks efficiently.

Conclusion: The study provides actionable insights for selecting RL architectures tailored to task characteristics and computational limits.

Abstract: Reinforcement learning (RL) has seen significant advancements through the
application of various neural network architectures. In this study, we
systematically investigate the performance of several neural networks in RL
tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),
Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit
(GRU). Through comprehensive evaluation across continuous control, discrete
decision-making, and memory-based environments, we identify
architecture-specific strengths and limitations. Our results reveal that: (1)
MLPs excel in fully observable continuous control tasks, providing an optimal
balance of performance and efficiency; (2) recurrent architectures like LSTM
and GRU offer robust performance in partially observable environments with
moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput
compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable
performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2
successfully solve the most challenging memory-intensive tasks, with Mamba-2
requiring 8x less memory than Transformer-XL. These findings provide insights
for researchers and practitioners, enabling more informed architecture
selection based on specific task characteristics and computational constraints.
Code is available at: https://github.com/SafeRL-Lab/RLBenchNet

</details>


### [523] [PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration](https://arxiv.org/pdf/2505.15047)
*Yingming Pu, Tao Lin, Hongyu Chen*

Main category: cs.LG

TL;DR: PiFlow is an information-theoretical framework for automated scientific discovery, improving efficiency and solution quality by systematically reducing uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems lack rationality constraints, leading to aimless hypothesizing and poor hypothesis-evidence linkage.

Method: PiFlow treats scientific discovery as a structured uncertainty reduction problem guided by principles like scientific laws.

Result: PiFlow achieves a 73.55% increase in AUC and 94.06% better solution quality compared to vanilla agent systems.

Conclusion: PiFlow is a plug-and-play method that enhances AI-driven research efficiency and robustness.

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.

</details>


### [524] [Generalization Through Growth: Hidden Dynamics Controls Depth Dependence](https://arxiv.org/pdf/2505.15064)
*Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda*

Main category: cs.LG

TL;DR: The paper presents a framework for generalization bounds in arbitrary pseudo-metric spaces, isolating depth contributions and revealing a geometric dichotomy between polynomial and exponential growth rates.


<details>
  <summary>Details</summary>
Motivation: To unify and extend generalization bounds beyond Euclidean inputs and specific architectures, addressing depth dependence and dynamical systems in deep learning.

Method: A framework using continuous hidden maps and output maps in pseudo-metric spaces, analyzing word-ball growth and Gromov's theorem to classify dynamics.

Result: Generalization bound $O(\sqrt{(\alpha + \log \beta(k))/n})$ isolates depth effects, with polynomial/exponential growth linked to nilpotent/expanding dynamics.

Conclusion: The work decouples specification from implementation, offering architecture-agnostic guarantees applicable to modern deep-learning paradigms.

Abstract: Recent theory has reduced the depth dependence of generalization bounds from
exponential to polynomial and even depth-independent rates, yet these results
remain tied to specific architectures and Euclidean inputs. We present a
unified framework for arbitrary \blue{pseudo-metric} spaces in which a
depth-\(k\) network is the composition of continuous hidden maps
\(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to
\mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$
isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of
the semigroup generated by the hidden layers. By Gromov's theorem polynomial
(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)
dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$
(sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further
provide covering-number estimates showing that expanding dynamics yield an
exponential parameter saving via compositional expressivity. Our results
decouple specification from implementation, offering architecture-agnostic and
dynamical-systems-aware guarantees applicable to modern deep-learning paradigms
such as test-time inference and diffusion models.

</details>


### [525] [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/pdf/2505.15080)
*Sergey Pankov, Georges Harik*

Main category: cs.LG

TL;DR: Proposes a method to reduce backpropagation computation in transformers by stochastically cutting gradient flow through low-impact attention weights, achieving linear complexity.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic compute complexity of attention in transformers for long sequences by exploiting the sparsity of attention weights.

Method: Introduces a probabilistic rule to cut backpropagation through most attention weights, limiting interactions per token to a parameter c, reducing compute from O(n²) to O(nc).

Result: Empirical results show cutting 99% of attention gradient flow (c ~20-30) increases gradient variance by only ~1% for n ~2000.

Conclusion: The method efficiently reduces backpropagation cost in transformers for long sequences, making backward pass negligible compared to forward pass.

Abstract: It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.

</details>


### [526] [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/pdf/2505.15072)
*Xin Zhou, Weiqing Wang, Francisco J. Baldán, Wray Buntine, Christoph Bergmeir*

Main category: cs.LG

TL;DR: MoTime introduces multimodal datasets for time series forecasting, showing external modalities improve performance, especially in cold-start scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on unimodal time series, but multimodal data is increasingly available. MoTime addresses this gap by providing datasets with paired temporal signals and external modalities.

Method: MoTime offers datasets pairing time series with text, metadata, and images, evaluating modality utility in common and cold-start forecasting scenarios.

Result: External modalities improve forecasting performance, with notable benefits for short series and cold-start scenarios, though impact varies by data characteristics.

Conclusion: MoTime supports future research with publicly available datasets, promoting comprehensive benchmarks in multimodal time series forecasting.

Abstract: While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.

</details>


### [527] [Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features](https://arxiv.org/pdf/2505.15083)
*Jeremy Qin*

Main category: cs.LG

TL;DR: The paper extends a bi-level transparency framework for time series forecasting by incorporating exogenous time series features, maintaining interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate and transparent time series forecasting is critical in healthcare for clinical decision-making.

Method: The approach integrates exogenous time series features with static features, decomposing them into interpretable trends and properties.

Result: Experiments on synthetic datasets show the method remains predictive while preserving interpretability and robustness.

Conclusion: This work advances robust and interpretable time series forecasting models, with code available for reproducibility.

Abstract: Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW

</details>


### [528] [Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories](https://arxiv.org/pdf/2505.15076)
*Nanxu Gong, Sixun Dong, Haoyue Bai, Xinyuan Wang, Wangyang Ying, Yanjie Fu*

Main category: cs.LG

TL;DR: The paper proposes a unified agentic framework (MAGS) combining feature selection and generation using multi-agent reinforcement learning to improve AI model performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat feature selection and generation separately, lacking balance between reducing redundancy and adding meaningful dimensions.

Method: Develops MAGS, a multi-agent system with selector, generator, and router agents, using in-context learning and offline PPO reinforcement fine-tuning.

Result: The framework consistently achieves superior task performance by intelligently orchestrating feature selection and generation.

Conclusion: MAGS effectively unifies feature selection and generation, demonstrating improved AI model performance.

Abstract: As a widely-used and practical tool, feature engineering transforms raw data
into discriminative features to advance AI model performance. However, existing
methods usually apply feature selection and generation separately, failing to
strive a balance between reducing redundancy and adding meaningful dimensions.
To fill this gap, we propose an agentic feature augmentation concept, where the
unification of feature generation and selection is modeled as agentic teaming
and planning. Specifically, we develop a Multi-Agent System with Long and
Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant
features, a generator agent to produce informative new dimensions, and a router
agent that strategically coordinates their actions. We leverage in-context
learning with short-term memory for immediate feedback refinement and long-term
memory for globally optimal guidance. Additionally, we employ offline Proximal
Policy Optimization (PPO) reinforcement fine-tuning to train the router agent
for effective decision-making to navigate a vast discrete feature space.
Extensive experiments demonstrate that this unified agentic framework
consistently achieves superior task performance by intelligently orchestrating
feature selection and generation.

</details>


### [529] [Cost-aware LLM-based Online Dataset Annotation](https://arxiv.org/pdf/2505.15101)
*Eray Can Elumar, Cem Tekin, Osman Yagan*

Main category: cs.LG

TL;DR: CaMVo is an online framework for efficient LLM-based dataset annotation, reducing costs while maintaining accuracy by adaptively selecting LLMs and using weighted majority voting.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of majority voting in LLM-based dataset labeling while maintaining reliability.

Method: CaMVo uses contextual embeddings, LinUCB-based selection, and Bayesian confidence estimation to adaptively choose LLMs and aggregate responses via weighted majority voting.

Result: CaMVo matches or outperforms full majority voting in accuracy on MMLU and IMDB datasets while significantly cutting costs.

Conclusion: CaMVo is a practical, cost-efficient solution for dynamic dataset annotation.

Abstract: Recent advances in large language models (LLMs) have enabled automated
dataset labeling with minimal human supervision. While majority voting across
multiple LLMs can improve label reliability by mitigating individual model
biases, it incurs high computational costs due to repeated querying. In this
work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),
for efficient and accurate LLM-based dataset annotation. CaMVo adaptively
selects a subset of LLMs for each data instance based on contextual embeddings,
balancing confidence and cost without requiring pre-training or ground-truth
labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator
over confidence scores, CaMVo estimates a lower bound on labeling accuracy for
each LLM and aggregates responses through weighted majority voting. Our
empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates
that CaMVo achieves comparable or superior accuracy to full majority voting
while significantly reducing labeling costs. This establishes CaMVo as a
practical and robust solution for cost-efficient annotation in dynamic labeling
environments.

</details>


### [530] [Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives](https://arxiv.org/pdf/2505.15103)
*Zihu Wang, Boxun Xu, Hejia Geng, Peng Li*

Main category: cs.LG

TL;DR: Khan-GCL enhances graph contrastive learning by using Kolmogorov-Arnold Networks (KAN) for better encoders and generating semantically meaningful hard negatives.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional GCL: MLP-based encoders' restricted capacity and suboptimal negative samples.

Method: Integrates KAN into GCL encoders and develops techniques for generating semantically meaningful hard negatives.

Result: Achieves state-of-the-art performance across datasets and tasks.

Conclusion: Khan-GCL improves GCL by enhancing encoder capacity and optimizing negative sample generation.

Abstract: Graph contrastive learning (GCL) has demonstrated great promise for learning
generalizable graph representations from unlabeled data. However, conventional
GCL approaches face two critical limitations: (1) the restricted expressive
capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal
negative samples that either from random augmentations-failing to provide
effective 'hard negatives'-or generated hard negatives without addressing the
semantic distinctions crucial for discriminating graph data. To this end, we
propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold
Network (KAN) into the GCL encoder architecture, substantially enhancing its
representational capacity. Furthermore, we exploit the rich information
embedded within KAN coefficient parameters to develop two novel critical
feature identification techniques that enable the generation of semantically
meaningful hard negative samples for each graph representation. These
strategically constructed hard negatives guide the encoder to learn more
discriminative features by emphasizing critical semantic differences between
graphs. Extensive experiments demonstrate that our approach achieves
state-of-the-art performance compared to existing GCL methods across a variety
of datasets and tasks.

</details>


### [531] [Graph Foundation Models: A Comprehensive Survey](https://arxiv.org/pdf/2505.15116)
*Zehong Wang, Zheyuan Liu, Tianyi Ma, Jiazheng Li, Zheyuan Zhang, Xingbo Fu, Yiyang Li, Zhengqing Yuan, Wei Song, Yijun Ma, Qingkai Zeng, Xiusi Chen, Jianan Zhao, Jundong Li, Meng Jiang, Pietro Lio, Nitesh Chawla, Chuxu Zhang, Yanfang Ye*

Main category: cs.LG

TL;DR: A survey on Graph Foundation Models (GFMs) aiming to bring scalable, general-purpose intelligence to graph-structured data, covering backbone architectures, pretraining strategies, and adaptation mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of extending foundation models to non-Euclidean graph data and enable broad transfer across graph-centric tasks.

Method: Modular framework with three components: backbone architectures, pretraining strategies, and adaptation mechanisms, categorized by generalization scope (universal, task-specific, domain-specific).

Result: Comprehensive overview of GFMs, including key innovations, theoretical insights, and challenges like structural alignment and scalability.

Conclusion: GFMs are foundational for open-ended reasoning over structured data, with the survey guiding future research in this evolving field.

Abstract: Graph-structured data pervades domains such as social networks, biological
systems, knowledge graphs, and recommender systems. While foundation models
have transformed natural language processing, vision, and multimodal learning
through large-scale pretraining and generalization, extending these
capabilities to graphs -- characterized by non-Euclidean structures and complex
relational semantics -- poses unique challenges and opens new opportunities. To
this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose
intelligence to structured data, enabling broad transfer across graph-centric
tasks and domains. This survey provides a comprehensive overview of GFMs,
unifying diverse efforts under a modular framework comprising three key
components: backbone architectures, pretraining strategies, and adaptation
mechanisms. We categorize GFMs by their generalization scope -- universal,
task-specific, and domain-specific -- and review representative methods, key
innovations, and theoretical insights within each category. Beyond methodology,
we examine theoretical foundations including transferability and emergent
capabilities, and highlight key challenges such as structural alignment,
heterogeneity, scalability, and evaluation. Positioned at the intersection of
graph learning and general-purpose AI, GFMs are poised to become foundational
infrastructure for open-ended reasoning over structured data. This survey
consolidates current progress and outlines future directions to guide research
in this rapidly evolving field. Resources are available at
https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.

</details>


### [532] [Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models](https://arxiv.org/pdf/2505.15130)
*Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, Ramtin Pedarsani*

Main category: cs.LG

TL;DR: AdvCLIP-LoRA enhances adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings, balancing robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP are vulnerable to adversarial attacks, and existing PEFT methods lack robustness.

Method: Formulates adversarial fine-tuning as a minimax optimization problem with theoretical guarantees.

Result: Improves robustness against attacks (e.g., FGSM, PGD) without significant loss in clean accuracy.

Conclusion: AdvCLIP-LoRA is a practical, robust adaptation method for VLMs in resource-constrained scenarios.

Abstract: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance
in cross-modal tasks through large-scale contrastive pre-training. To adapt
these large transformer-based models efficiently for downstream tasks,
Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as
scalable alternatives to full fine-tuning, especially in few-shot scenarios.
However, like traditional deep neural networks, VLMs are highly vulnerable to
adversarial attacks, where imperceptible perturbations can significantly
degrade model performance. Adversarial training remains the most effective
strategy for improving model robustness in PEFT. In this work, we propose
AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial
robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method
formulates adversarial fine-tuning as a minimax optimization problem and
provides theoretical guarantees for convergence under smoothness and
nonconvex-strong-concavity assumptions. Empirical results across eight datasets
using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly
improves robustness against common adversarial attacks (e.g., FGSM, PGD),
without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA
as a practical and theoretically grounded approach for robust adaptation of
VLMs in resource-constrained settings.

</details>


### [533] [The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning](https://arxiv.org/pdf/2505.15134)
*Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, Hao Peng*

Main category: cs.LG

TL;DR: Entropy minimization (EM) improves LLM performance on math, physics, and coding tasks without labeled data, using three methods: EM-FT, EM-RL, and EM-INF. EM-RL matches or outperforms labeled-data baselines, while EM-INF rivals proprietary models like GPT-4o efficiently.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that LLMs' reasoning capabilities can be enhanced solely through entropy minimization, eliminating the need for labeled data or parameter updates.

Method: Three approaches: (1) EM-FT (token-level entropy minimization), (2) EM-RL (reinforcement learning with entropy reward), (3) EM-INF (inference-time logit adjustment).

Result: EM-RL matches/exceeds labeled-data baselines (e.g., GRPO, RLOO). EM-INF enables Qwen-32B to rival GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on SciCode, with 3x efficiency gains.

Conclusion: Entropy minimization alone can unlock LLMs' latent reasoning abilities, offering a label-free, efficient alternative to traditional methods.

Abstract: Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.

</details>


### [534] [Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm](https://arxiv.org/pdf/2505.15138)
*Yang Xu, Swetha Ganesh, Washim Uddin Mondal, Qinbo Bai, Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates infinite-horizon average reward Constrained Markov
Decision Processes (CMDPs) with general parametrization. We propose a
Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints
while ensuring a high convergence rate. In particular, our algorithm achieves
global convergence and constraint violation rates of
$\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing
time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge
of $\tau_{\mathrm{mix}}$, the achievable rates change to
$\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq
\tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results
match the theoretical lower bound for Markov Decision Processes and establish a
new benchmark in the theoretical exploration of average reward CMDPs.

</details>


### [535] [EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression](https://arxiv.org/pdf/2505.15140)
*Tong Cheng, Fu Jie, Xinpeng Ling, Huifa Li, Zhili Chen*

Main category: cs.LG

TL;DR: The paper introduces EC-LDA, a novel attack method in Federated Graph Learning (FGL) that improves label distribution inference by compressing node embeddings, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks in FGL, where servers can infer client data, the paper focuses on label distribution attacks (LDAs) and their effectiveness.

Method: The study analyzes the relationship between LDA effectiveness and node embedding variance, proposing EC-LDA to enhance attacks by compressing embeddings.

Result: EC-LDA outperforms state-of-the-art LDAs on node classification and link prediction tasks across six datasets, achieving optimal metrics in CoraFull and LastFM.

Conclusion: EC-LDA is highly effective for label distribution inference in FGL, even under differential privacy protection, highlighting privacy vulnerabilities.

Abstract: Graph Neural Networks (GNNs) have been widely used for graph analysis.
Federated Graph Learning (FGL) is an emerging learning framework to
collaboratively train graph data from various clients. However, since clients
are required to upload model parameters to the server in each round, this
provides the server with an opportunity to infer each client's data privacy. In
this paper, we focus on label distribution attacks(LDAs) that aim to infer the
label distributions of the clients' local data. We take the first step to
attack client's label distributions in FGL. Firstly, we observe that the
effectiveness of LDA is closely related to the variance of node embeddings in
GNNs. Next, we analyze the relation between them and we propose a new attack
named EC-LDA, which significantly improves the attack effectiveness by
compressing node embeddings. Thirdly, extensive experiments on node
classification and link prediction tasks across six widely used graph datasets
show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal
values under both Cos-sim and JS-div evaluation metrics in the CoraFull and
LastFM datasets. Finally, we explore the robustness of EC-LDA under
differential privacy protection.

</details>


### [536] [BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms](https://arxiv.org/pdf/2505.15141)
*Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent Y. F. Tan, Zhuoran Yang*

Main category: cs.LG

TL;DR: The paper introduces BanditSpec, a training-free online learning framework for adaptive hyperparameter selection in speculative decoding of LLMs, using bandit algorithms (UCBSpec and EXP3Spec) to optimize performance.


<details>
  <summary>Details</summary>
Motivation: To improve speculative decoding in LLMs by dynamically selecting hyperparameters during text generation, avoiding fixed or offline-trained configurations.

Method: Formulates hyperparameter selection as a Multi-Armed Bandit problem, proposing BanditSpec with UCBSpec and EXP3Spec algorithms, analyzed via stopping time regret.

Result: UCBSpec's regret is proven optimal; experiments with LLaMA3 and Qwen2 show throughput close to oracle best hyperparameters in diverse scenarios.

Conclusion: BanditSpec effectively adapts hyperparameters for speculative decoding, outperforming existing methods and nearing optimal performance.

Abstract: Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.

</details>


### [537] [Filtering Learning Histories Enhances In-Context Reinforcement Learning](https://arxiv.org/pdf/2505.15143)
*Weiqin Chen, Xinjie Zhang, Dharmashankar Subramanian, Santiago Paternain*

Main category: cs.LG

TL;DR: The paper introduces Learning History Filtering (LHF), a dataset preprocessing method to enhance in-context reinforcement learning (ICRL) by filtering and reweighting suboptimal learning histories.


<details>
  <summary>Details</summary>
Motivation: Transformer models in ICRL often inherit suboptimal behaviors from source algorithms/datasets. The goal is to mitigate this by improving dataset quality.

Method: Proposes LHF, which reweights and filters learning histories based on improvement and stability, compatible with SOTA ICRL algorithms.

Result: LHF improves performance across benchmarks, especially in noisy data scenarios, and works robustly with varying hyperparameters.

Conclusion: LHF effectively addresses suboptimality in ICRL through dataset preprocessing, demonstrating significant performance gains.

Abstract: Transformer models (TMs) have exhibited remarkable in-context reinforcement
learning (ICRL) capabilities, allowing them to generalize to and improve in
previously unseen environments without re-training or fine-tuning. This is
typically accomplished by imitating the complete learning histories of a source
RL algorithm over a substantial amount of pretraining environments, which,
however, may transfer suboptimal behaviors inherited from the source
algorithm/dataset. Therefore, in this work, we address the issue of inheriting
suboptimality from the perspective of dataset preprocessing. Motivated by the
success of the weighted empirical risk minimization, we propose a simple yet
effective approach, learning history filtering (LHF), to enhance ICRL by
reweighting and filtering the learning histories based on their improvement and
stability characteristics. To the best of our knowledge, LHF is the first
approach to avoid source suboptimality by dataset preprocessing, and can be
combined with the current state-of-the-art (SOTA) ICRL algorithms. We
substantiate the effectiveness of LHF through a series of experiments conducted
on the well-known ICRL benchmarks, encompassing both discrete environments and
continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,
DPT, DICP) as the backbones. LHF exhibits robust performance across a variety
of suboptimal scenarios, as well as under varying hyperparameters and sampling
strategies. Notably, the superior performance of LHF becomes more pronounced in
the presence of noisy data, indicating the significance of filtering learning
histories.

</details>


### [538] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/pdf/2505.15201)
*Christian Walder, Deep Karkhanis*

Main category: cs.LG

TL;DR: PKPO optimizes pass@k performance by transforming rewards, enabling better exploration and solving harder problems.


<details>
  <summary>Details</summary>
Motivation: Current RL methods prioritize pass@1, limiting diversity and collective utility of samples. PKPO addresses this by optimizing for pass@k.

Method: PKPO introduces low-variance unbiased estimators for pass@k and its gradient, applying a stable transformation to rewards.

Result: PKPO improves pass@k performance, solves harder problems, and maintains strong pass@1 results when annealing k.

Conclusion: PKPO unblocks learning on challenging tasks by prioritizing joint sample utility, enhancing exploration and performance.

Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [539] [Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation](https://arxiv.org/pdf/2505.15152)
*Nanxu Gong, Zijun Li, Sixun Dong, Haoyue Bai, Wangyang Ying, Xinyuan Wang, Yanjie Fu*

Main category: cs.LG

TL;DR: DIFFT introduces a reward-guided generative approach for feature transformation, combining VAE and LDM to overcome limitations of discrete and continuous search methods, achieving superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing feature transformation methods face challenges like combinatorial explosion in discrete search and sensitivity to initialization in continuous search, limiting their effectiveness.

Method: DIFFT uses a VAE to learn a latent space for feature sets, navigates it with an LDM guided by a performance evaluator, and decodes embeddings into discrete features with a semi-autoregressive decoder.

Result: DIFFT outperforms state-of-the-art baselines in accuracy and robustness across 14 datasets, with lower training and inference times.

Conclusion: DIFFT's hybrid approach of global learning and targeted optimization effectively addresses limitations of traditional FT methods, offering a scalable and efficient solution.

Abstract: Feature Transformation (FT) crafts new features from original ones via
mathematical operations to enhance dataset expressiveness for downstream
models. However, existing FT methods exhibit critical limitations: discrete
search struggles with enormous combinatorial spaces, impeding practical use;
and continuous search, being highly sensitive to initialization and step sizes,
often becomes trapped in local optima, restricting global exploration. To
overcome these limitations, DIFFT redefines FT as a reward-guided generative
task. It first learns a compact and expressive latent space for feature sets
using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then
navigates this space to generate high-quality feature embeddings, its
trajectory guided by a performance evaluator towards task-specific optima. This
synthesis of global distribution learning (from LDM) and targeted optimization
(reward guidance) produces potent embeddings, which a novel semi-autoregressive
decoder efficiently converts into structured, discrete features, preserving
intra-feature dependencies while allowing parallel inter-feature generation.
Extensive experiments on 14 benchmark datasets show DIFFT consistently
outperforms state-of-the-art baselines in predictive accuracy and robustness,
with significantly lower training and inference times.

</details>


### [540] [Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss](https://arxiv.org/pdf/2505.15174)
*Bo-Han Lai, Pin-Han Huang, Bo-Han Kung, Shang-Tse Chen*

Main category: cs.LG

TL;DR: The paper introduces BRONet, a Lipschitz neural network with a novel BRO layer and annealing-based loss, achieving state-of-the-art certified robustness.


<details>
  <summary>Details</summary>
Motivation: Enhancing the expressiveness and robustness of Lipschitz neural networks for better certified robustness in deep learning.

Method: Proposes a Block Reflector Orthogonal (BRO) layer and a new annealing-based loss function to improve Lipschitz models.

Result: BRONet outperforms existing baselines on CIFAR-10/100, Tiny-ImageNet, and ImageNet.

Conclusion: The BRO layer and annealing loss effectively improve Lipschitz networks, validated by extensive experiments.

Abstract: Lipschitz neural networks are well-known for providing certified robustness
in deep learning. In this paper, we present a novel, efficient Block Reflector
Orthogonal (BRO) layer that enhances the capability of orthogonal layers on
constructing more expressive Lipschitz neural architectures. In addition, by
theoretically analyzing the nature of Lipschitz neural networks, we introduce a
new loss function that employs an annealing mechanism to increase margin for
most data points. This enables Lipschitz models to provide better certified
robustness. By employing our BRO layer and loss function, we design BRONet - a
simple yet effective Lipschitz neural network that achieves state-of-the-art
certified robustness. Extensive experiments and empirical analysis on
CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms
existing baselines. The implementation is available at
\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.

</details>


### [541] [SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps](https://arxiv.org/pdf/2505.15177)
*Jiawei Gu, Ziyue Qiao, Zechao Li*

Main category: cs.LG

TL;DR: SpecGap is a post-hoc OOD detection method for graphs, leveraging spectral gaps in Laplacian eigenvalues to distinguish ID and OOD samples.


<details>
  <summary>Details</summary>
Motivation: OOD samples show anomalous spectral gaps, motivating a spectral-based approach for detection.

Method: SpecGap adjusts features by subtracting the component linked to the second-largest eigenvalue, scaled by the spectral gap.

Result: Achieves state-of-the-art performance on benchmarks, supported by ablation studies and theoretical analysis.

Conclusion: SpecGap is a parameter-free, easily integrable solution for OOD detection in graph neural networks.

Abstract: The task of graph-level out-of-distribution (OOD) detection is crucial for
deploying graph neural networks in real-world settings. In this paper, we
observe a significant difference in the relationship between the largest and
second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and
OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps
(the difference between the largest and second-largest eigenvalues)}. This
observation motivates us to propose SpecGap, an effective post-hoc approach for
OOD detection on graphs. SpecGap adjusts features by subtracting the component
associated with the second-largest eigenvalue, scaled by the spectral gap, from
the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right)
\mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art
performance across multiple benchmark datasets. We present extensive ablation
studies and comprehensive theoretical analyses to support our empirical
results. As a parameter-free post-hoc method, SpecGap can be easily integrated
into existing graph neural network models without requiring any additional
training or model modification.

</details>


### [542] [Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers](https://arxiv.org/pdf/2505.15239)
*Peter Súkeník, Christoph H. Lampert, Marco Mondelli*

Main category: cs.LG

TL;DR: The paper analyzes neural collapse in modern architectures like transformers and ResNets, proving their global optima are approximately collapsed, with tighter approximation as depth increases.


<details>
  <summary>Details</summary>
Motivation: To bridge gaps in understanding neural collapse by focusing on data-aware regimes and modern architectures, beyond data-agnostic models or multi-layer perceptrons.

Method: Theoretical analysis of deep regularized transformers and ResNets with LayerNorm, trained with cross-entropy or mean squared error loss, and experimental validation on vision and language datasets.

Result: Global optima of these architectures are approximately collapsed, with tighter collapse as depth increases.

Conclusion: Neural collapse becomes more prominent with depth, validating the theoretical reduction of these architectures to unconstrained feature models.

Abstract: The empirical emergence of neural collapse -- a surprising symmetry in the
feature representations of the training data in the penultimate layer of deep
neural networks -- has spurred a line of theoretical research aimed at its
understanding. However, existing work focuses on data-agnostic models or, when
data structure is taken into account, it remains limited to multi-layer
perceptrons. Our paper fills both these gaps by analyzing modern architectures
in a data-aware regime: we prove that global optima of deep regularized
transformers and residual networks (ResNets) with LayerNorm trained with cross
entropy or mean squared error loss are approximately collapsed, and the
approximation gets tighter as the depth grows. More generally, we formally
reduce any end-to-end large-depth ResNet or transformer training into an
equivalent unconstrained features model, thus justifying its wide use in the
literature even beyond data-agnostic settings. Our theoretical results are
supported by experiments on computer vision and language datasets showing that,
as the depth grows, neural collapse indeed becomes more prominent.

</details>


### [543] [A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning](https://arxiv.org/pdf/2505.15178)
*Zhehao Huang, Xinwen Cheng, Jie Zhang, Jinghao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang*

Main category: cs.LG

TL;DR: The paper introduces a unified Continual Learning-Unlearning (CLU) framework, combining continual learning and machine unlearning via KL divergence optimization, addressing stability-plasticity with a manifold constraint and fast-slow weight adaptation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continual learning (knowledge acquisition) and machine unlearning (data removal) by revealing their intrinsic connection and providing a unified solution.

Method: Proposes a KL divergence-based optimization framework with four gradient components, a remain-preserved manifold constraint, and fast-slow weight adaptation for second-order optimization.

Result: UG-CLU effectively balances learning, unlearning, and knowledge stability across datasets and models, supporting task-agnostic scenarios.

Conclusion: The framework offers a theoretical and methodological foundation for dynamic, compliant intelligent systems.

Abstract: Recent advancements in deep models have highlighted the need for intelligent
systems that combine continual learning (CL) for knowledge acquisition with
machine unlearning (MU) for data removal, forming the Continual
Learning-Unlearning (CLU) paradigm. While existing work treats CL and MU as
separate processes, we reveal their intrinsic connection through a unified
optimization framework based on Kullback-Leibler divergence minimization. This
framework decomposes gradient updates for approximate CLU into four components:
learning new knowledge, unlearning targeted data, preserving existing
knowledge, and modulation via weight saliency. A critical challenge lies in
balancing knowledge update and retention during sequential learning-unlearning
cycles. To resolve this stability-plasticity dilemma, we introduce a
remain-preserved manifold constraint to induce a remaining Hessian compensation
for CLU iterations. A fast-slow weight adaptation mechanism is designed to
efficiently approximate the second-order optimization direction, combined with
adaptive weighting coefficients and a balanced weight saliency mask, proposing
a unified implementation framework for gradient-based CLU. Furthermore, we
pioneer task-agnostic CLU scenarios that support fine-grained unlearning at the
cross-task category and random sample levels beyond the traditional task-aware
setups. Experiments demonstrate that the proposed UG-CLU framework effectively
coordinates incremental learning, precise unlearning, and knowledge stability
across multiple datasets and model architectures, providing a theoretical
foundation and methodological support for dynamic, compliant intelligent
systems.

</details>


### [544] [NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration](https://arxiv.org/pdf/2505.15180)
*Jiawei Gu, Ziyue Qiao, Xiao Luo*

Main category: cs.LG

TL;DR: NeuBM (Neutral Bias Mitigation) is a novel method to reduce model bias in GNNs by dynamically calibrating inputs using a neutral graph, improving performance for underrepresented classes without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: GNNs often suffer from bias due to class imbalance, leading to unfair predictions for minority classes. NeuBM aims to address this issue.

Method: NeuBM uses a dynamically updated neutral graph to estimate and correct model biases by adjusting logits, seamlessly integrating into existing GNNs.

Result: Experiments show NeuBM enhances balanced accuracy and recall for minority classes, especially in severe imbalance and limited data scenarios.

Conclusion: NeuBM effectively mitigates bias in GNNs, balancing predictions and feature representations, with strong theoretical and empirical support.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance across various
domains, yet they often struggle with model bias, particularly in the presence
of class imbalance. This bias can lead to suboptimal performance and unfair
predictions, especially for underrepresented classes. We introduce NeuBM
(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs
through neutral input calibration. NeuBM leverages a dynamically updated
neutral graph to estimate and correct the inherent biases of the model. By
subtracting the logits obtained from the neutral graph from those of the input
graph, NeuBM effectively recalibrates the model's predictions, reducing bias
across different classes. Our method integrates seamlessly into existing GNN
architectures and training procedures, requiring minimal computational
overhead. Extensive experiments on multiple benchmark datasets demonstrate that
NeuBM significantly improves the balanced accuracy and recall of minority
classes, while maintaining strong overall performance. The effectiveness of
NeuBM is particularly pronounced in scenarios with severe class imbalance and
limited labeled data, where traditional methods often struggle. We provide
theoretical insights into how NeuBM achieves bias mitigation, relating it to
the concept of representation balancing. Our analysis reveals that NeuBM not
only adjusts the final predictions but also influences the learning of balanced
feature representations throughout the network.

</details>


### [545] [Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification](https://arxiv.org/pdf/2505.15250)
*Suping Xu, Lin Shang, Keyu Liu, Hengrong Ju, Xibei Yang, Witold Pedrycz*

Main category: cs.LG

TL;DR: MAFRFS improves FRFS by linking uncertainty reduction to classification performance, focusing on class compactness and separation, outperforming six state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing FRFS methods prioritize uncertainty reduction but overlook its impact on classification performance.

Method: Proposes MAFRFS, a framework integrating class compactness and separation to enhance feature selection.

Result: MAFRFS outperforms FRFS and six other algorithms on 15 datasets, showing scalability and effectiveness.

Conclusion: MAFRFS successfully bridges uncertainty reduction and classification performance, offering superior feature selection.

Abstract: Fuzzy rough feature selection (FRFS) is an effective means of addressing the
curse of dimensionality in high-dimensional data. By removing redundant and
irrelevant features, FRFS helps mitigate classifier overfitting, enhance
generalization performance, and lessen computational overhead. However, most
existing FRFS algorithms primarily focus on reducing uncertainty in pattern
classification, neglecting that lower uncertainty does not necessarily result
in improved classification performance, despite it commonly being regarded as a
key indicator of feature selection effectiveness in the FRFS literature. To
bridge uncertainty characterization and pattern classification, we propose a
Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers
both the compactness and separation of label classes. MAFRFS effectively
reduces uncertainty in pattern classification tasks, while guiding the feature
selection towards more separable and discriminative label class structures.
Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly
scalable and more effective than FRFS. The algorithms developed using MAFRFS
outperform six state-of-the-art feature selection algorithms.

</details>


### [546] [Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing](https://arxiv.org/pdf/2505.15195)
*Adel Javanmard, Rudrajit Das, Alessandro Epasto, Vahab Mirrokni*

Main category: cs.LG

TL;DR: The paper proposes a principled framework for optimally combining model predictions and noisy labels in iterative retraining for binary classification, using AMP to derive a Bayes optimal aggregator function.


<details>
  <summary>Details</summary>
Motivation: To address the open question of how to optimally combine model predictions and noisy labels during retraining to improve performance.

Method: Develops a framework based on approximate message passing (AMP) to analyze iterative retraining for GMM and GLM settings, deriving the Bayes optimal aggregator function.

Result: The derived aggregator function minimizes prediction error when used for retraining, with performance quantified over multiple rounds. A practical version for linear probing outperforms baselines in high noise.

Conclusion: The paper provides a theoretically grounded and practical solution for optimal retraining in binary classification, validated in high-noise scenarios.

Abstract: Retraining a model using its own predictions together with the original,
potentially noisy labels is a well-known strategy for improving the model
performance. While prior works have demonstrated the benefits of specific
heuristic retraining schemes, the question of how to optimally combine the
model's predictions and the provided labels remains largely open. This paper
addresses this fundamental question for binary classification tasks. We develop
a principled framework based on approximate message passing (AMP) to analyze
iterative retraining procedures for two ground truth settings: Gaussian mixture
model (GMM) and generalized linear model (GLM). Our main contribution is the
derivation of the Bayes optimal aggregator function to combine the current
model's predictions and the given labels, which when used to retrain the same
model, minimizes its prediction error. We also quantify the performance of this
optimal retraining strategy over multiple rounds. We complement our theoretical
results by proposing a practically usable version of the theoretically-optimal
aggregator function for linear probing with the cross-entropy loss, and
demonstrate its superiority over baseline methods in the high label noise
regime.

</details>


### [547] [Scaling Diffusion Transformers Efficiently via $μ$P](https://arxiv.org/pdf/2505.15270)
*Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li*

Main category: cs.LG

TL;DR: The paper generalizes Maximal Update Parametrization (μP) to diffusion Transformers, proving its alignment with vanilla Transformers and demonstrating robust hyperparameter transferability, leading to faster convergence and reduced tuning costs.


<details>
  <summary>Details</summary>
Motivation: The high cost of hyperparameter tuning for large-scale diffusion Transformers motivates the need for a scalable and efficient framework like μP, which has shown success in vanilla Transformers.

Method: The authors generalize μP to diffusion Transformers (DiT, U-ViT, PixArt-α, MMDiT), rigorously proving its alignment with vanilla Transformers and validating it through large-scale experiments.

Result: DiT-μP achieves 2.9x faster convergence, and scaled models (PixArt-α and MMDiT) outperform baselines with minimal tuning costs (5.5% and 3% of original costs, respectively).

Conclusion: μP is established as a principled and efficient framework for scaling diffusion Transformers, reducing tuning costs while maintaining performance.

Abstract: Diffusion Transformers have emerged as the foundation for vision generative
models, but their scalability is limited by the high cost of hyperparameter
(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P)
was proposed for vanilla Transformers, which enables stable HP transfer from
small to large language models, and dramatically reduces tuning costs. However,
it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion
Transformers, which differ architecturally and objectively. In this work, we
generalize standard $\mu$P to diffusion Transformers and validate its
effectiveness through large-scale experiments. First, we rigorously prove that
$\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,
PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer,
enabling the direct application of existing $\mu$P methodologies. Leveraging
this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP
transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate
achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we
validate the effectiveness of $\mu$P on text-to-image generation by scaling
PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,
models under $\mu$P outperform their respective baselines while requiring small
tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of
consumption by human experts for MMDiT-18B. These results establish $\mu$P as a
principled and efficient framework for scaling diffusion Transformers.

</details>


### [548] [Group Distributionally Robust Optimization with Flexible Sample Queries](https://arxiv.org/pdf/2505.15212)
*Haomin Bai, Dingzhi Yu, Shuai Li, Haipeng Luo, Lijun Zhang*

Main category: cs.LG

TL;DR: The paper introduces a flexible sample query method for Group Distributionally Robust Optimization (GDRO), addressing dynamic sample sizes. It proposes a two-player game framework and a novel PLA algorithm, achieving high-probability regret and optimization error bounds.


<details>
  <summary>Details</summary>
Motivation: Existing GDRO algorithms are limited to fixed sample sizes (1 or m), failing to adapt to dynamic scenarios. This work aims to overcome this by enabling flexible sample queries.

Method: The problem is framed as a two-player game involving online convex optimization and prediction with limited advice (PLA). A novel PLA algorithm is introduced, with loss estimators and follow-the-regularized-leader updates.

Result: The proposed GDRO algorithm achieves a high-probability optimization error bound of O(1/t √(∑(m/r_j log m))), with consistent sample complexity O(m log m / ε²).

Conclusion: The method successfully handles varying sample sizes, matching existing bounds for fixed cases (r=1 or m), and is validated on synthetic and real-world datasets.

Abstract: Group distributionally robust optimization (GDRO) aims to develop models that
perform well across $m$ distributions simultaneously. Existing GDRO algorithms
can only process a fixed number of samples per iteration, either 1 or $m$, and
therefore can not support scenarios where the sample size varies dynamically.
To address this limitation, we investigate GDRO with flexible sample queries
and cast it as a two-player game: one player solves an online convex
optimization problem, while the other tackles a prediction with limited advice
(PLA) problem. Within such a game, we propose a novel PLA algorithm,
constructing appropriate loss estimators for cases where the sample size is
either 1 or not, and updating the decision using follow-the-regularized-leader.
Then, we establish the first high-probability regret bound for non-oblivious
PLA. Building upon the above approach, we develop a GDRO algorithm that allows
an arbitrary and varying sample size per round, achieving a high-probability
optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t
\frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$.
This result demonstrates that the optimization error decreases as the number of
samples increases and implies a consistent sample complexity of $O(m\log
(m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing
bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary
and real-world multi-class datasets.

</details>


### [549] [KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning](https://arxiv.org/pdf/2505.15213)
*Sampanna Yashwant Kahu*

Main category: cs.LG

TL;DR: This paper explores using deep learning (LSTM) to predict task sequences in Linux's CFS scheduler, aiming to improve adaptability. It introduces a new dataset and evaluates the model's feasibility for kernel integration.


<details>
  <summary>Details</summary>
Motivation: To enhance the Linux kernel's CFS scheduler by leveraging deep learning for better adaptability and performance across diverse workloads.

Method: Generated a real-world CFS scheduling dataset and trained an LSTM network to predict the next scheduled task.

Result: Demonstrated the feasibility of using LSTM for task prediction, with potential for integration into the kernel.

Conclusion: The study opens doors for data-driven improvements in kernel scheduling, with practical implications and provided source code for reproducibility.

Abstract: Efficient task scheduling is paramount in the Linux kernel, where the
Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance
high utilization with interactive responsiveness. This research pioneers the
use of deep learning techniques to predict the sequence of tasks selected by
CFS, aiming to evaluate the feasibility of a more generalized and potentially
more adaptive task scheduler for diverse workloads. Our core contributions are
twofold: first, the systematic generation and curation of a novel scheduling
dataset from a running Linux kernel, capturing real-world CFS behavior; and
second, the development, training, and evaluation of a Long Short-Term Memory
(LSTM) network designed to accurately forecast the next task to be scheduled.
This paper further discusses the practical pathways and implications of
integrating such a predictive model into the kernel's scheduling framework. The
findings and methodologies presented herein open avenues for data-driven
advancements in kernel scheduling, with the full source code provided for
reproducibility and further exploration.

</details>


### [550] [LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models](https://arxiv.org/pdf/2505.15293)
*Qianyue Hao, Yiwen Song, Qingmin Liao, Jian Yuan, Yong Li*

Main category: cs.LG

TL;DR: LLM-Explorer uses large language models to adaptively generate task-specific exploration strategies in RL, improving performance by up to 37.27%.


<details>
  <summary>Details</summary>
Motivation: Existing RL exploration methods are rigid and task-agnostic, lacking adaptability to real-time learning status.

Method: LLM-Explorer samples agent trajectories, prompts LLMs to analyze learning status, and generates adaptive exploration distributions.

Result: Achieves up to 37.27% performance improvement on Atari and MuJoCo benchmarks.

Conclusion: LLM-Explorer is a versatile, plug-in module that enhances RL exploration by leveraging LLMs for dynamic, task-specific strategies.

Abstract: Policy exploration is critical in reinforcement learning (RL), where existing
approaches include greedy, Gaussian process, etc. However, these approaches
utilize preset stochastic processes and are indiscriminately applied in all
kinds of RL tasks without considering task-specific features that influence
policy exploration. Moreover, during RL training, the evolution of such
stochastic processes is rigid, which typically only incorporates a decay in the
variance, failing to adjust flexibly according to the agent's real-time
learning status. Inspired by the analyzing and reasoning capability of large
language models (LLMs), we design LLM-Explorer to adaptively generate
task-specific exploration strategies with LLMs, enhancing the policy
exploration in RL. In our design, we sample the learning trajectory of the
agent during the RL training in a given task and prompt the LLM to analyze the
agent's current policy learning status and then generate a probability
distribution for future policy exploration. Updating the probability
distribution periodically, we derive a stochastic process specialized for the
particular task and dynamically adjusted to adapt to the learning process. Our
design is a plug-in module compatible with various widely applied RL
algorithms, including the DQN series, DDPG, TD3, and any possible variants
developed based on them. Through extensive experiments on the Atari and MuJoCo
benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy
exploration, achieving an average performance improvement up to 37.27%. Our
code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for
reproducibility.

</details>


### [551] [Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2505.15228)
*Mathew Vanherreweghe, Lirandë Pira, Patrick Rebentrost*

Main category: cs.LG

TL;DR: CP-KAN combines Chebyshev polynomials and QUBO for efficient degree selection, improving computational tractability and performance in regression tasks.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of degree selection in neural networks and enhance efficiency in regression tasks with limited data.

Method: Reformulates degree selection as a QUBO task, reducing complexity, and uses Chebyshev polynomial basis functions for robustness and regularization.

Result: CP-KAN shows competitive performance in regression, especially in data-efficient and numerically stable scenarios, with theoretical ties to financial time series.

Conclusion: CP-KAN offers a computationally efficient and robust solution for regression tasks, validated across multiple domains.

Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a
neural architecture combining Chebyshev polynomial basis functions and
quadratic unconstrained binary optimization (QUBO). Our primary contribution
involves reformulating the degree selection problem as a QUBO task, reducing
the complexity from $O(D^N)$ to a single optimization step per layer. This
approach enables efficient degree selection across neurons while maintaining
computational tractability. The architecture performs well in regression tasks
with limited data, showing good robustness to input scales and natural
regularization properties from its polynomial basis. Additionally, theoretical
analysis establishes connections between CP-KAN's performance and properties of
financial time series. Our empirical validation across multiple domains
demonstrates competitive performance compared to several traditional
architectures tested, especially in scenarios where data efficiency and
numerical stability are important. Our implementation, including strategies for
managing computational overhead in larger networks is available in
Ref.~\citep{cpkan_implementation}.

</details>


### [552] [Laplace Sample Information: Data Informativeness Through a Bayesian Lens](https://arxiv.org/pdf/2505.15303)
*Johannes Kaiser, Kristian Schwethelm, Daniel Rueckert, Georgios Kaissis*

Main category: cs.LG

TL;DR: The paper introduces Laplace Sample Information (LSI), a measure of sample informativeness in deep learning, leveraging Bayesian approximation and KL divergence. LSI effectively orders data, detects mislabels, and assesses dataset difficulty, showing efficiency and transferability.


<details>
  <summary>Details</summary>
Motivation: Improving model efficiency and accuracy by identifying and removing redundant or harmful samples in datasets.

Method: Proposes LSI, a Bayesian approximation-based measure using KL divergence to quantify sample-induced changes in parameter distribution.

Result: LSI successfully orders data by typicality, detects mislabeled samples, measures class-wise informativeness, and evaluates dataset difficulty across image and text data in supervised and unsupervised settings.

Conclusion: LSI is a versatile, efficient, and transferable tool for assessing sample informativeness in deep learning.

Abstract: Accurately estimating the informativeness of individual samples in a dataset
is an important objective in deep learning, as it can guide sample selection,
which can improve model efficiency and accuracy by removing redundant or
potentially harmful samples. We propose Laplace Sample Information (LSI)
measure of sample informativeness grounded in information theory widely
applicable across model architectures and learning settings. LSI leverages a
Bayesian approximation to the weight posterior and the KL divergence to measure
the change in the parameter distribution induced by a sample of interest from
the dataset. We experimentally show that LSI is effective in ordering the data
with respect to typicality, detecting mislabeled samples, measuring class-wise
informativeness, and assessing dataset difficulty. We demonstrate these
capabilities of LSI on image and text data in supervised and unsupervised
settings. Moreover, we show that LSI can be computed efficiently through probes
and transfers well to the training of large models.

</details>


### [553] [Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions](https://arxiv.org/pdf/2505.15231)
*Kabir V. Dabholkar, Omri Barak*

Main category: cs.LG

TL;DR: A numerical framework combining Koopman Theory and Deep Neural Networks is introduced to characterize separatrices in high-dimensional dynamical systems, with applications in synthetic, ecological, and neural network models.


<details>
  <summary>Details</summary>
Motivation: Existing tools struggle to describe separatrices in high-dimensional systems, limiting understanding of boundaries between stable states in natural systems like neural circuits.

Method: The framework approximates Koopman Eigenfunctions (KEFs) with real positive eigenvalues, which vanish at separatrices, and uses optimization to locate them.

Result: The method successfully identifies separatrices in synthetic, ecological, and neural network models and designs optimal perturbations for system shifts.

Conclusion: The approach provides a practical tool for analyzing high-dimensional systems and has potential applications in neuroscience, such as optogenetic stimulation.

Abstract: Many natural systems, including neural circuits involved in decision making,
can be modeled as high-dimensional dynamical systems with multiple stable
states. While existing analytical tools primarily describe behavior near stable
equilibria, characterizing separatrices -- the manifolds that delineate
boundaries between different basins of attraction -- remains challenging,
particularly in high-dimensional settings. Here, we introduce a numerical
framework leveraging Koopman Theory combined with Deep Neural Networks to
effectively characterize separatrices. Specifically, we approximate Koopman
Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish
precisely at the separatrices. Utilizing these scalar KEFs, optimization
methods efficiently locate separatrices even in complex systems. We demonstrate
our approach on synthetic benchmarks, ecological network models, and recurrent
neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate
the practical utility of our method by designing optimal perturbations that can
shift systems across separatrices, enabling predictions relevant to optogenetic
stimulation experiments in neuroscience.

</details>


### [554] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/pdf/2505.15306)
*Yiwen Song, Qianyue Hao, Qingmin Liao, Jian Yuan, Yong Li*

Main category: cs.LG

TL;DR: LLM-Ens enhances RL model ensembles by using LLMs for task-specific semantic understanding, dynamically selecting the best agent for each situation, improving performance by up to 20.9%.


<details>
  <summary>Details</summary>
Motivation: Training effective RL agents is challenging due to tuning complexities (algorithm, hyperparameters, seeds). Existing ensemble methods lack adaptability.

Method: LLM-Ens categorizes task states into 'situations' using LLMs, analyzes agent strengths per situation, and dynamically switches agents during inference.

Result: LLM-Ens outperforms baselines by up to 20.9% on the Atari benchmark.

Conclusion: LLM-Ens offers a dynamic, adaptable approach to RL ensembles, leveraging LLMs for task-specific optimization.

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


### [555] [Reliable Vertical Federated Learning in 5G Core Network Architecture](https://arxiv.org/pdf/2505.15244)
*Mohamad Mestoukirdi, Mourad Khanfouci*

Main category: cs.LG

TL;DR: A new algorithm improves model generalization in Vertical Federated Learning (VFL) under client reliability constraints in 5G Core Networks by optimizing feature splits and local models.


<details>
  <summary>Details</summary>
Motivation: VFL performance degrades due to reliability issues in Network Data Analytics Functions (NWDAFs) caused by resource constraints and operational overhead in 5G Core Networks.

Method: Optimizes vertical feature splits among clients and centrally defines local models based on reliability metrics.

Result: Empirical evaluation shows improved performance over traditional baseline methods.

Conclusion: The proposed algorithm effectively mitigates generalization loss in VFL under client reliability constraints.

Abstract: This work proposes a new algorithm to mitigate model generalization loss in
Vertical Federated Learning (VFL) operating under client reliability
constraints within 5G Core Networks (CNs). Recently studied and endorsed by
3GPP, VFL enables collaborative and load-balanced model training and inference
across the CN. However, the performance of VFL significantly degrades when the
Network Data Analytics Functions (NWDAFs) - which serve as primary clients for
VFL model training and inference - experience reliability issues stemming from
resource constraints and operational overhead. Unlike edge environments, CN
environments adopt fundamentally different data management strategies,
characterized by more centralized data orchestration capabilities. This
presents opportunities to implement better distributed solutions that take full
advantage of the CN data handling flexibility. Leveraging this flexibility, we
propose a method that optimizes the vertical feature split among clients while
centrally defining their local models based on reliability metrics. Our
empirical evaluation demonstrates the effectiveness of our proposed algorithm,
showing improved performance over traditional baseline methods.

</details>


### [556] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/pdf/2505.15311)
*Yurun Yuan, Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie*

Main category: cs.LG

TL;DR: TBRM, a value-based RL method for LLMs, outperforms policy-based methods like PPO and GRPO with lower overhead.


<details>
  <summary>Details</summary>
Motivation: Policy-based RL dominates LLM reasoning, but value-based approaches are underexplored. TBRM revisits Bellman Residual Minimization for LLMs.

Method: TBRM optimizes a trajectory-level Bellman objective using LLM logits as Q-values, eliminating critics, importance-sampling, and clipping.

Result: TBRM outperforms PPO and GRPO on math-reasoning benchmarks with comparable or lower computational overhead.

Conclusion: Value-based RL, like TBRM, is a principled and efficient alternative for enhancing LLM reasoning.

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [557] [Mitigating Spurious Correlations with Causal Logit Perturbation](https://arxiv.org/pdf/2505.15246)
*Xiaoling Zhou, Wei Ye, Rui Xie, Shikun Zhang*

Main category: cs.LG

TL;DR: The paper introduces a Causal Logit Perturbation (CLP) framework to mitigate spurious correlations in deep learning by training classifiers with causal logit perturbations.


<details>
  <summary>Details</summary>
Motivation: Addressing non-robustness in deep learning caused by reliance on spurious correlations.

Method: Uses a perturbation network to generate sample-wise logit perturbations, optimized via meta-learning and augmented with human causal knowledge.

Result: CLP achieves state-of-the-art performance in biased learning scenarios and redirects model attention to causal attributes.

Conclusion: The CLP framework effectively dismantles spurious associations and improves model robustness.

Abstract: Deep learning has seen widespread success in various domains such as science,
industry, and society. However, it is acknowledged that certain approaches
suffer from non-robustness, relying on spurious correlations for predictions.
Addressing these limitations is of paramount importance, necessitating the
development of methods that can disentangle spurious correlations. {This study
attempts to implement causal models via logit perturbations and introduces a
novel Causal Logit Perturbation (CLP) framework to train classifiers with
generated causal logit perturbations for individual samples, thereby mitigating
the spurious associations between non-causal attributes (i.e., image
backgrounds) and classes.} {Our framework employs a} perturbation network to
generate sample-wise logit perturbations using a series of training
characteristics of samples as inputs. The whole framework is optimized by an
online meta-learning-based learning algorithm and leverages human causal
knowledge by augmenting metadata in both counterfactual and factual manners.
Empirical evaluations on four typical biased learning scenarios, including
long-tail learning, noisy label learning, generalized long-tail learning, and
subpopulation shift learning, demonstrate that CLP consistently achieves
state-of-the-art performance. Moreover, visualization results support the
effectiveness of the generated causal perturbations in redirecting model
attention towards causal image attributes and dismantling spurious
associations.

</details>


### [558] [Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets](https://arxiv.org/pdf/2505.15251)
*Idriss Malek, Abhijit Sharma, Salem Lahlou*

Main category: cs.LG

TL;DR: LGGFN improves GFlowNets by using loss-guided exploration to avoid mode collapse and discover diverse solutions faster.


<details>
  <summary>Details</summary>
Motivation: GFlowNets often suffer from mode collapse, limiting their ability to find diverse solutions. Existing methods rely on heuristics, which may not be optimal.

Method: LGGFN introduces an auxiliary GFlowNet driven by the main model's training loss, focusing exploration on high-loss regions.

Result: LGGFN outperforms baselines, discovering 40x more unique modes and reducing exploration error by ~99% in sequence generation.

Conclusion: LGGFN effectively addresses mode collapse and enhances exploration efficiency in GFlowNets.

Abstract: Although Generative Flow Networks (GFlowNets) are designed to capture
multiple modes of a reward function, they often suffer from mode collapse in
practice, getting trapped in early discovered modes and requiring prolonged
training to find diverse solutions. Existing exploration techniques may rely on
heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel
approach where an auxiliary GFlowNet's exploration is directly driven by the
main GFlowNet's training loss. By prioritizing trajectories where the main
model exhibits high loss, LGGFN focuses sampling on poorly understood regions
of the state space. This targeted exploration significantly accelerates the
discovery of diverse, high-reward samples. Empirically, across various
benchmarks including grid environments, structured sequence generation, and
Bayesian structure learning, LGGFN consistently enhances exploration efficiency
and sample diversity compared to baselines. For instance, on a challenging
sequence generation task, it discovered over 40 times more unique valid modes
while simultaneously reducing the exploration error metric by approximately
99\%.

</details>


### [559] [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/pdf/2505.15259)
*Hyunseok Lee, Jeonghoon Kim, Beomjun Kim, Jihoon Tack, Chansong Jo, Jaehong Lee, Cheonbok Park, Sookyo In, Jinwoo Shin, Kang Min Yoo*

Main category: cs.LG

TL;DR: ReGUIDE is a framework for efficient web grounding in MLLMs, using self-generated reasoning and spatial-aware criticism to improve accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Accurate localization of GUI elements is challenging, and prior methods rely on large datasets. ReGUIDE aims to improve efficiency.

Method: ReGUIDE uses online reinforcement learning for self-generated reasoning and spatial-aware criticism. It also employs test-time scaling for inference.

Result: ReGUIDE outperforms baselines with significantly fewer training samples (e.g., 0.2% of data).

Conclusion: ReGUIDE advances web grounding performance efficiently, demonstrating the potential of reasoning and spatial awareness in MLLMs.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).

</details>


### [560] [Hadamax Encoding: Elevating Performance in Model-Free Atari](https://arxiv.org/pdf/2505.15345)
*Jacob E. Kooi, Zhao Yang, Vincent François-Lavet*

Main category: cs.LG

TL;DR: The paper introduces the Hadamax encoder, a novel architecture for pixel-based model-free RL, achieving state-of-the-art performance in Atari-57 with an 80% gain over vanilla PQN.


<details>
  <summary>Details</summary>
Motivation: Despite neural networks' impact in ML, RL architectures remain simple due to marginal performance gains. This work aims to improve RL performance with a new encoder.

Method: The Hadamax encoder uses max-pooling of Hadamard products between GELU-activated parallel hidden layers, integrated with the PQN algorithm.

Result: Hadamax-PQN achieves an 80% performance gain over vanilla PQN and surpasses Rainbow-DQN in the Atari-57 benchmark.

Conclusion: The Hadamax encoder is a significant advancement for model-free RL, offering reproducible state-of-the-art results.

Abstract: Neural network architectures have a large impact in machine learning. In
reinforcement learning, network architectures have remained notably simple, as
changes often lead to small gains in performance. This work introduces a novel
encoder architecture for pixel-based model-free reinforcement learning. The
Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves
state-of-the-art performance by max-pooling Hadamard products between
GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the
Hadamax encoder achieves state-of-the-art model-free performance in the
Atari-57 benchmark. Specifically, without applying any algorithmic
hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain
over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,
the full code is available on
\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.

</details>


### [561] [Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations](https://arxiv.org/pdf/2505.15284)
*Kun Fang, Qinghua Tao, Mingzhen He, Kexin Lv, Runze Yang, Haibo Hu, Xiaolin Huang, Jie Yang, Longbin Cao*

Main category: cs.LG

TL;DR: The paper proposes a KPCA-based method for OoD detection by learning a discriminative non-linear subspace, addressing kernel selection and computational challenges.


<details>
  <summary>Details</summary>
Motivation: To improve OoD detection by exploiting disparities between InD and OoD data through non-linear feature subspaces.

Method: Uses KPCA to learn a discriminative subspace, introduces a Cosine-Gaussian kernel, and employs approximation techniques for efficiency.

Result: Achieves effective OoD detection with improved efficacy and efficiency.

Conclusion: The method provides new insights into non-linear subspaces for OoD detection and practical solutions for kernel design and computation.

Abstract: Out-of-Distribution (OoD) detection is vital for the reliability of deep
neural networks, the key of which lies in effectively characterizing the
disparities between OoD and In-Distribution (InD) data. In this work, such
disparities are exploited through a fresh perspective of non-linear feature
subspace. That is, a discriminative non-linear subspace is learned from InD
features to capture representative patterns of InD, while informative patterns
of OoD features cannot be well captured in such a subspace due to their
different distribution. Grounded on this perspective, we exploit the deviations
of InD and OoD features in such a non-linear subspace for effective OoD
detection. To be specific, we leverage the framework of Kernel Principal
Component Analysis (KPCA) to attain the discriminative non-linear subspace and
deploy the reconstruction error on such subspace to distinguish InD and OoD
data. Two challenges emerge: (i) the learning of an effective non-linear
subspace, i.e., the selection of kernel function in KPCA, and (ii) the
computation of the kernel matrix with large-scale InD data. For the former, we
reveal two vital non-linear patterns that closely relate to the InD-OoD
disparity, leading to the establishment of a Cosine-Gaussian kernel for
constructing the subspace. For the latter, we introduce two techniques to
approximate the Cosine-Gaussian kernel with significantly cheap computations.
In particular, our approximation is further tailored by incorporating the InD
data confidence, which is demonstrated to promote the learning of
discriminative subspaces for OoD data. Our study presents new insights into the
non-linear feature subspace for OoD detection and contributes practical
explorations on the associated kernel design and efficient computations,
yielding a KPCA detection method with distinctively improved efficacy and
efficiency.

</details>


### [562] [Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting](https://arxiv.org/pdf/2505.15312)
*Yuxuan Shu, Vasileios Lampos*

Main category: cs.LG

TL;DR: Sonnet, a novel architecture using learnable wavelet transformations and spectral analysis, outperforms baselines in multivariable time series forecasting by leveraging Multivariable Coherence Attention (MVCA).


<details>
  <summary>Details</summary>
Motivation: Naive transformer applications struggle with complex variable relationships over time, prompting the need for a better method.

Method: Sonnet combines learnable wavelet transformations, spectral analysis via the Koopman operator, and MVCA to model variable dependencies.

Result: Sonnet achieves the best performance in 34 out of 47 tasks, reducing MAE by 1.1% on average. MVCA alone reduces MAE by 10.7% in challenging tasks.

Conclusion: Sonnet and MVCA effectively address transformer limitations, improving forecasting accuracy.

Abstract: Multivariable time series forecasting methods can integrate information from
exogenous variables, leading to significant prediction accuracy gains.
Transformer architecture has been widely applied in various time series
forecasting models due to its ability to capture long-range sequential
dependencies. However, a na\"ive application of transformers often struggles to
effectively model complex relationships among variables over time. To mitigate
against this, we propose a novel architecture, namely the Spectral Operator
Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to
the input and incorporates spectral analysis using the Koopman operator. Its
predictive skill relies on the Multivariable Coherence Attention (MVCA), an
operation that leverages spectral coherence to model variable dependencies. Our
empirical analysis shows that Sonnet yields the best performance on $34$ out of
$47$ forecasting tasks with an average mean absolute error (MAE) reduction of
$1.1\%$ against the most competitive baseline (different per task). We further
show that MVCA -- when put in place of the na\"ive attention used in various
deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$
on average in the most challenging forecasting tasks.

</details>


### [563] [Guided Policy Optimization under Partial Observability](https://arxiv.org/pdf/2505.15418)
*Yueheng Li, Guangming Xie, Zongqing Lu*

Main category: cs.LG

TL;DR: Guided Policy Optimization (GPO) co-trains a guider and learner to leverage privileged information in RL, achieving optimality comparable to direct RL and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of RL in partially observable environments by effectively leveraging additional information (e.g., simulations) to enhance training.

Method: Introduces GPO, a framework where a guider uses privileged information while aligning with a learner's policy trained via imitation learning.

Result: Theoretical and empirical results show GPO achieves near-optimal performance, excelling in tasks like continuous control with partial observability and noise.

Conclusion: GPO overcomes limitations of existing approaches, demonstrating strong performance in complex RL tasks.

Abstract: Reinforcement Learning (RL) in partially observable environments poses
significant challenges due to the complexity of learning under uncertainty.
While additional information, such as that available in simulations, can
enhance training, effectively leveraging it remains an open problem. To address
this, we introduce Guided Policy Optimization (GPO), a framework that co-trains
a guider and a learner. The guider takes advantage of privileged information
while ensuring alignment with the learner's policy that is primarily trained
via imitation learning. We theoretically demonstrate that this learning scheme
achieves optimality comparable to direct RL, thereby overcoming key limitations
inherent in existing approaches. Empirical evaluations show strong performance
of GPO across various tasks, including continuous control with partial
observability and noise, and memory-based challenges, significantly
outperforming existing methods.

</details>


### [564] [Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows](https://arxiv.org/pdf/2505.15329)
*Anqiao Ouyang, Hongyi Ke, Qi Wang*

Main category: cs.LG

TL;DR: FINE, an invertible neural encoder, outperforms classical linear methods and conventional deep autoencoders in learning compact, interpretable representations of physics datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage invertible neural architectures for compactness, interpretability, and information-preserving properties in physics datasets.

Method: Combines invertible monotonic activation functions with reversible filter structures, extended via Invertible ResNets, and includes a Fourier truncation step for dimensionality reduction.

Result: FINE achieves better reconstruction accuracy than DFT, POD, and CNN-based autoencoders, using smaller models with superior interpretability.

Conclusion: Invertible networks with spectral truncation are promising for compact, interpretable physics dataset representations and symmetry-aware learning.

Abstract: Invertible neural architectures have recently attracted attention for their
compactness, interpretability, and information-preserving properties. In this
work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines
invertible monotonic activation functions with reversible filter structures,
and could be extended using Invertible ResNets. This architecture is examined
in learning low-dimensional representations of one-dimensional nonlinear wave
interactions and exact circular translation symmetry. Dimensionality is
preserved across layers, except for a Fourier truncation step in the latent
space, which enables dimensionality reduction while maintaining shift
equivariance and interpretability. Our results demonstrate that FINE
significantly outperforms classical linear methods such as Discrete Fourier
Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves
reconstruction accuracy better than conventional deep autoencoders with
convolutional layers (CNN) - while using substantially smaller models and
offering superior physical interpretability. These findings suggest that
invertible single-neuron networks, when combined with spectral truncation,
offer a promising framework for learning compact and interpretable
representations of physics datasets, and symmetry-aware representation learning
in physics-informed machine learning.

</details>


### [565] [SSR: Speculative Parallel Scaling Reasoning in Test-time](https://arxiv.org/pdf/2505.15340)
*Yuanlin Chu, Bo Wang, Xiang Liu, Hong Chen, Aiwei Liu, Xuming Hu*

Main category: cs.LG

TL;DR: SSR (Speculative Parallel Scaling Reasoning) is a training-free framework that improves efficiency and accuracy in multi-step mathematical reasoning for LLMs by using speculative decoding and selective parallel modules.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational overhead and efficiency-accuracy trade-off in multi-step mathematical reasoning with LLMs, particularly for test-time scaling methods.

Method: SSR combines a Selective Parallel Module (SPM) for identifying promising reasoning strategies and Step-level Speculative Decoding (SSD) for efficient draft-target collaboration.

Result: SSR achieves significant gains: 13.84% higher pass@1 accuracy on LiveMathBench with 80.5% FLOPs, and 30% compute reduction on MATH-500 without accuracy loss.

Conclusion: SSR effectively balances efficiency and accuracy in LLM-based mathematical reasoning, offering a practical solution for scaling methods.

Abstract: Large language models (LLMs) have achieved impressive results on multi-step
mathematical reasoning, yet at the cost of high computational overhead. This
challenge is particularly acute for test-time scaling methods such as parallel
decoding, which increase answer diversity but scale poorly in efficiency. To
address this efficiency-accuracy trade-off, we propose SSR (Speculative
Parallel Scaling Reasoning), a training-free framework that leverages a key
insight: by introducing speculative decoding at the step level, we can
accelerate reasoning without sacrificing correctness. SSR integrates two
components: a Selective Parallel Module (SPM) that identifies a small set of
promising reasoning strategies via model-internal scoring, and Step-level
Speculative Decoding (SSD), which enables efficient draft-target collaboration
for fine-grained reasoning acceleration. Experiments on three mathematical
benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR
achieves strong gains over baselines. For instance, on LiveMathBench, SSR
improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the
baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in
accuracy.

</details>


### [566] [Human in the Loop Adaptive Optimization for Improved Time Series Forecasting](https://arxiv.org/pdf/2505.15354)
*Malik Tiomoko, Hamza Cherkaoui, Giuseppe Paolo, Zhang Yili, Yu Meng, Zhang Keli, Hafiz Tiomoko Ali*

Main category: cs.LG

TL;DR: A post-training adaptive optimization framework improves forecast accuracy in time series models without retraining, using reinforcement learning, contextual bandits, or genetic algorithms. It supports human-in-the-loop guidance via natural language.


<details>
  <summary>Details</summary>
Motivation: Address systematic errors in time series forecasting models, especially in critical domains like energy, finance, and healthcare, without requiring retraining or architectural changes.

Method: Lightweight, model-agnostic framework applying expressive transformations (e.g., affine corrections) optimized via reinforcement learning, contextual bandits, or genetic algorithms. Includes optional human guidance via natural language.

Result: Consistent accuracy gains across benchmarks (electricity, weather, traffic) with minimal computational overhead.

Conclusion: The framework offers a practical, interpretable, and extensible solution for improving forecasting systems.

Abstract: Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.

</details>


### [567] [Distributionally Robust Federated Learning with Client Drift Minimization](https://arxiv.org/pdf/2505.15371)
*Mounssif Krouka, Chaouki Ben Issaid, Mehdi Bennis*

Main category: cs.LG

TL;DR: DRDM is a federated learning algorithm combining DRO and dynamic regularization to improve fairness and efficiency in heterogeneous environments, reducing communication rounds and energy costs.


<details>
  <summary>Details</summary>
Motivation: Address unfair and inefficient model performance in FL due to non-IID data across clients.

Method: Combines distributionally robust optimization (DRO) with dynamic regularization to mitigate client drift, framed as a min-max problem for worst-case client performance.

Result: Improves worst-case test accuracy with fewer communication rounds; adapts local updates for minimal energy cost.

Conclusion: DRDM is effective for robust and fair FL in heterogeneous settings, with practical benefits in communication and energy efficiency.

Abstract: Federated learning (FL) faces critical challenges, particularly in
heterogeneous environments where non-independent and identically distributed
data across clients can lead to unfair and inefficient model performance. In
this work, we introduce \textit{DRDM}, a novel algorithm that addresses these
issues by combining a distributionally robust optimization (DRO) framework with
dynamic regularization to mitigate client drift. \textit{DRDM} frames the
training as a min-max optimization problem aimed at maximizing performance for
the worst-case client, thereby promoting robustness and fairness. This robust
objective is optimized through an algorithm leveraging dynamic regularization
and efficient local updates, which significantly reduces the required number of
communication rounds. Moreover, we provide a theoretical convergence analysis
for convex smooth objectives under partial participation. Extensive experiments
on three benchmark datasets, covering various model architectures and data
heterogeneity levels, demonstrate that \textit{DRDM} significantly improves
worst-case test accuracy while requiring fewer communication rounds than
existing state-of-the-art baselines. Furthermore, we analyze the impact of
signal-to-noise ratio (SNR) and bandwidth on the energy consumption of
participating clients, demonstrating that the number of local update steps can
be adaptively selected to achieve a target worst-case test accuracy with
minimal total energy cost across diverse communication environments.

</details>


### [568] [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/pdf/2505.15433)
*Beni Egressy, Jan Stühmer*

Main category: cs.LG

TL;DR: The paper introduces Set-LLM, a method to eliminate order sensitivity in LLMs by adapting their architecture for permutation-invariant processing of set-text inputs.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from observed vulnerabilities in LLMs, such as order bias and inconsistent answers when options are reordered, impacting applications like multiple-choice QA and automated evaluation.

Method: Set-LLM modifies pretrained LLMs with a new attention mask and positional encodings designed for sets, ensuring permutation invariance. Theoretical proofs and experiments validate its effectiveness.

Result: Set-LLM achieves comparable or improved performance, maintains runtime efficiency, and eliminates order sensitivity.

Conclusion: Set-LLM successfully addresses order sensitivity in LLMs, enhancing their robustness for practical applications.

Abstract: While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.

</details>


### [569] [InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference](https://arxiv.org/pdf/2505.15391)
*Duncan Bart, Bruno Endres Forlin, Ana-Lucia Varbanescu, Marco Ottavi, Kuan-Hsun Chen*

Main category: cs.LG

TL;DR: InTreeger is an end-to-end framework for generating integer-only, architecture-agnostic C implementations of tree-based ML models, improving inference latency and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing quantization-induced errors in integer quantization for resource-constrained devices.

Method: InTreeger takes a training dataset and outputs an integer-only C implementation of tree-based models without precision loss.

Result: Improved inference latency across ARM, x86, and RISC-V architectures and better energy efficiency compared to floating-point implementations.

Conclusion: Integer-only inference is advantageous for energy- and area-constrained devices, enabling deployment on ultra-low power platforms.

Abstract: Integer quantization has emerged as a critical technique to facilitate
deployment on resource-constrained devices. Although they do reduce the
complexity of the learning models, their inference performance is often prone
to quantization-induced errors. To this end, we introduce InTreeger: an
end-to-end framework that takes a training dataset as input, and outputs an
architecture-agnostic integer-only C implementation of tree-based machine
learning model, without loss of precision. This framework enables anyone, even
those without prior experience in machine learning, to generate a highly
optimized integer-only classification model that can run on any hardware simply
by providing an input dataset and target variable. We evaluated our generated
implementations across three different architectures (ARM, x86, and RISC-V),
resulting in significant improvements in inference latency. In addition, we
show the energy efficiency compared to typical decision tree implementations
that rely on floating-point arithmetic. The results underscore the advantages
of integer-only inference, making it particularly suitable for energy- and
area-constrained devices such as embedded systems and edge computing platforms,
while also enabling the execution of decision trees on existing ultra-low power
devices.

</details>


### [570] [HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations](https://arxiv.org/pdf/2505.15405)
*Martin Carrasco, Guillermo Bernardez, Marco Montagna, Nina Miolane, Lev Telyatnikov*

Main category: cs.LG

TL;DR: HOPSE is a scalable, message-passing-free framework for Topological Deep Learning, addressing the limitations of existing methods by using Hasse graph decompositions.


<details>
  <summary>Details</summary>
Motivation: Existing TDL methods face scalability issues due to combinatorial explosion and complexity in message-passing.

Method: HOPSE avoids message-passing by using Hasse graph decompositions for efficient encodings over higher-order domains.

Result: HOPSE scales linearly, matches or outperforms state-of-the-art, and achieves up to 7x speedups.

Conclusion: HOPSE offers a scalable and efficient alternative for TDL, enabling broader applications.

Abstract: While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. To overcome these limitations, we propose HOPSE (Higher-Order
Positional and Structural Encoder) -- a \emph{message passing-free} framework
that uses Hasse graph decompositions to derive efficient and expressive
encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales
linearly with dataset size while preserving expressive power and permutation
equivariance. Experiments on molecular, expressivity and topological benchmarks
show that HOPSE matches or surpasses state-of-the-art performance while
achieving up to 7 $times$ speedups over HOMP-based models, opening a new path
for scalable TDL.

</details>


### [571] [Efficient Differentiable Approximation of Generalized Low-rank Regularization](https://arxiv.org/pdf/2505.15407)
*Naiqi Li, Yuqiu Xie, Peiyuan Liu, Tao Dai, Yong Jiang, Shu-Tao Xia*

Main category: cs.LG

TL;DR: The paper proposes a differentiable approximation for generalized low-rank regularization (LRR), addressing challenges like NP-hard optimization and non-differentiable SVD operations. It enables plug-and-play LRR in loss functions and offers efficient GPU implementation.


<details>
  <summary>Details</summary>
Motivation: LRR is widely used but hard to optimize due to NP-hard rank function and non-differentiable SVD operations. Existing relaxations are inefficient or impractical for gradient-based methods.

Method: The authors introduce a differentiable approximation of generalized LRR, compatible with various norms (e.g., nuclear, Schatten-$p$). It supports GPU-friendly operations and plug-and-play integration into loss functions.

Result: The method shows rapid reduction in bias and variance with increased sample size and iterations. Experiments demonstrate its versatility and efficiency across tasks.

Conclusion: The proposed differentiable LRR approximation is efficient, versatile, and practical for gradient-based optimization, with proven convergence and broad applicability.

Abstract: Low-rank regularization (LRR) has been widely applied in various machine
learning tasks, but the associated optimization is challenging. Directly
optimizing the rank function under constraints is NP-hard in general. To
overcome this difficulty, various relaxations of the rank function were
studied. However, optimization of these relaxed LRRs typically depends on
singular value decomposition, which is a time-consuming and nondifferentiable
operator that cannot be optimized with gradient-based techniques. To address
these challenges, in this paper we propose an efficient differentiable
approximation of the generalized LRR. The considered LRR form subsumes many
popular choices like the nuclear norm, the Schatten-$p$ norm, and various
nonconvex relaxations. Our method enables LRR terms to be appended to loss
functions in a plug-and-play fashion, and the GPU-friendly operations enable
efficient and convenient implementation. Furthermore, convergence analysis is
presented, which rigorously shows that both the bias and the variance of our
rank estimator rapidly reduce with increased sample size and iteration steps.
In the experimental study, the proposed method is applied to various tasks,
which demonstrates its versatility and efficiency. Code is available at
https://github.com/naiqili/EDLRR.

</details>


### [572] [SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding](https://arxiv.org/pdf/2505.15423)
*Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski*

Main category: cs.LG

TL;DR: SplitWise enhances stepwise regression by using shallow decision trees to transform numeric predictors into binary features when it improves model fit, balancing interpretability and nonlinearity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing nonlinear relationships in regression while maintaining interpretability.

Method: Adaptively transforms numeric predictors into binary features using shallow decision trees, evaluated by AIC or BIC.

Result: Produces more parsimonious and generalizable models than traditional stepwise and penalized regression.

Conclusion: SplitWise effectively balances interpretability and flexibility in regression modeling.

Abstract: Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.

</details>


### [573] [Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes](https://arxiv.org/pdf/2505.15496)
*Hossein Zakerinia, Christoph H. Lampert*

Main category: cs.LG

TL;DR: New fast-rate generalization bounds for multi-task and meta-learning in unbalanced settings, where tasks have varying training set sizes, are presented. These bounds are computable, interpretable, and outperform previous guarantees.


<details>
  <summary>Details</summary>
Motivation: Address the gap in fast-rate bounds for unbalanced multi-task learning, a common real-world scenario, and explore its unique statistical properties.

Method: Develop numerically computable and interpretable fast-rate bounds for unbalanced settings, comparing them to previous balanced-case bounds.

Result: The new bounds provide stronger guarantees and reveal distinct statistical properties in unbalanced settings, including two meaningful definitions of multi-task risk.

Conclusion: The work advances understanding of unbalanced multi-task learning, offering practical bounds and highlighting conceptual differences from balanced scenarios.

Abstract: We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.

</details>


### [574] [Certified Neural Approximations of Nonlinear Dynamics](https://arxiv.org/pdf/2505.15497)
*Frederik Baymler Mathiesen, Nikolaus Vertovec, Francesco Fabiano, Luca Laurenti, Alessandro Abate*

Main category: cs.LG

TL;DR: A novel adaptive verification method for neural approximations of dynamical systems provides formal error bounds, outperforming state-of-the-art and enabling safe use in safety-critical contexts.


<details>
  <summary>Details</summary>
Motivation: Neural networks can approximate nonlinear dynamical systems but require formal bounds on their accuracy for safety-critical applications.

Method: Proposes an adaptive, parallelizable verification method using certified first-order models to derive formal error bounds.

Result: Demonstrates effectiveness and scalability on benchmarks, outperforming existing methods, and applies successfully to neural network compression and Koopman operator learning.

Conclusion: The method offers a reliable way to ensure neural approximations are safe for use in critical applications, with broad applicability.

Abstract: Neural networks hold great potential to act as approximate models of
nonlinear dynamical systems, with the resulting neural approximations enabling
verification and control of such systems. However, in safety-critical contexts,
the use of neural approximations requires formal bounds on their closeness to
the underlying system. To address this fundamental challenge, we propose a
novel, adaptive, and parallelizable verification method based on certified
first-order models. Our approach provides formal error bounds on the neural
approximations of dynamical systems, allowing them to be safely employed as
surrogates by interpreting the error bound as bounded disturbances acting on
the approximated dynamics. We demonstrate the effectiveness and scalability of
our method on a range of established benchmarks from the literature, showing
that it outperforms the state-of-the-art. Furthermore, we highlight the
flexibility of our framework by applying it to two novel scenarios not
previously explored in this context: neural network compression and an
autoencoder-based deep learning architecture for learning Koopman operators,
both yielding compelling results.

</details>


### [575] [Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning](https://arxiv.org/pdf/2505.15507)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: A novel algebraic framework for multi-dimensional compositional embeddings with directional non-commutative monoidal operators, unifying classical sequence-modeling paradigms.


<details>
  <summary>Details</summary>
Motivation: To generalize classical sequence-modeling approaches (e.g., SSMs, transformers) into a unified multi-dimensional framework with theoretical consistency.

Method: Introduces axis-specific composition operators (circ_i) ensuring associativity per axis and global interchangeability, compatible with modern ML architectures.

Result: The framework recovers classical paradigms (affine transformations, self-attention, SSMs) as 1D cases and supports higher-dimensional, structure-aware operations.

Conclusion: The theoretical framework has potential applications in deep learning (e.g., positional encodings, image embeddings) but requires empirical validation in future work.

Abstract: We introduce a new algebraic structure for multi-dimensional compositional
embeddings, built on directional non-commutative monoidal operators. The core
contribution of this work is this novel framework, which exhibits appealing
theoretical properties (associativity along each dimension and an interchange
law ensuring global consistency) while remaining compatible with modern machine
learning architectures. Our construction defines a distinct composition
operator circ_i for each axis i, ensuring associative combination along each
axis without imposing global commutativity. Importantly, all axis-specific
operators commute with one another, enforcing a global interchange law that
enables consistent crossaxis compositions. This is, to our knowledge, the first
approach that provides a common foundation that generalizes classical
sequence-modeling paradigms (e.g., structured state-space models (SSMs) and
transformer self-attention) to a unified multi-dimensional framework. For
example, specific one-dimensional instances of our framework can recover the
familiar affine transformation algebra, vanilla self-attention, and the
SSM-style recurrence. The higher-dimensional generalizations naturally support
recursive, structure-aware operations in embedding spaces. We outline several
potential applications unlocked by this structure-including structured
positional encodings in Transformers, directional image embeddings, and
symbolic modeling of sequences or grids-indicating that it could inform future
deep learning model designs. We formally establish the algebraic properties of
our framework and discuss efficient implementations. Finally, as our focus is
theoretical, we include no experiments here and defer empirical validation to
future work, which we plan to undertake.

</details>


### [576] [NOMAD Projection](https://arxiv.org/pdf/2505.15511)
*Brandon Duderstadt, Zach Nussbaum, Laurens van der Maaten*

Main category: cs.LG

TL;DR: NOMAD Projection is a scalable, GPU-accelerated method for unstructured data visualization, outperforming traditional techniques like t-SNE and UMAP in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional visualization methods (t-SNE, UMAP) struggle with the scale of modern AI datasets, hindering explainability.

Method: Introduces NOMAD Projection, a nonlinear dimensionality reduction method optimized for multi-GPU training, with theoretical ties to InfoNC-t-SNE loss.

Result: NOMAD outperforms state-of-the-art methods in speed and performance, demonstrated by mapping Multilingual Wikipedia.

Conclusion: NOMAD Projection addresses scalability challenges in AI data visualization, offering a practical solution for large datasets.

Abstract: The rapid adoption of generative AI has driven an explosion in the size of
datasets consumed and produced by AI models. Traditional methods for
unstructured data visualization, such as t-SNE and UMAP, have not kept up with
the pace of dataset scaling. This presents a significant challenge for AI
explainability, which relies on methods such as t-SNE and UMAP for exploratory
data analysis. In this paper, we introduce Negative Or Mean Affinity
Discrimination (NOMAD) Projection, the first method for unstructured data
visualization via nonlinear dimensionality reduction that can run on multiple
GPUs at train time. We provide theory that situates NOMAD Projection as an
approximate upper bound on the InfoNC-t-SNE loss, and empirical results that
demonstrate NOMAD Projection's superior performance and speed profile compared
to existing state-of-the-art methods. We demonstrate the scalability of NOMAD
Projection by computing the first complete data map of Multilingual Wikipedia.

</details>


### [577] [AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization](https://arxiv.org/pdf/2505.15514)
*Soham Sane*

Main category: cs.LG

TL;DR: AM-PPO enhances PPO by dynamically modulating advantage estimates using a non-linear scaling mechanism, improving stability and performance in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Raw advantage signals in PPO can be noisy and unstable, hindering optimal learning. AM-PPO aims to address these issues.

Method: AM-PPO uses an alpha controller and tanh-based gating to dynamically adjust advantage scaling based on statistical properties.

Result: AM-PPO outperforms PPO in continuous control benchmarks, offering better rewards and reduced clipping needs.

Conclusion: Advantage modulation is a promising technique for improving reinforcement learning optimization.

Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning
algorithm that heavily relies on accurate advantage estimates for stable and
efficient training. However, raw advantage signals can exhibit significant
variance, noise, and scale-related issues, impeding optimal learning
performance. To address this challenge, we introduce Advantage Modulation PPO
(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage
estimates using a dynamic, non-linear scaling mechanism. This adaptive
modulation employs an alpha controller that dynamically adjusts the scaling
factor based on evolving statistical properties of the advantage signals, such
as their norm, variance, and a predefined target saturation level. By
incorporating a tanh-based gating function driven by these adaptively scaled
advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates
and improve the conditioning of the policy gradient landscape. Crucially, this
modulation also influences value function training by providing consistent and
adaptively conditioned learning targets. Empirical evaluations across standard
continuous control benchmarks demonstrate that AM-PPO achieves superior reward
trajectories, exhibits sustained learning progression, and significantly
reduces the clipping required by adaptive optimizers. These findings underscore
the potential of advantage modulation as a broadly applicable technique for
enhancing reinforcement learning optimization.

</details>


### [578] [Explainable embeddings with Distance Explainer](https://arxiv.org/pdf/2505.15516)
*Christiaan Meijer, E. G. Patrick Bos*

Main category: cs.LG

TL;DR: Distance Explainer is a novel XAI method for interpreting embedded vector spaces by explaining distances between data points using saliency-based techniques.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability methods for embedded spaces where dimensions represent complex abstractions.

Method: Adapts RISE's saliency-based techniques with selective masking and distance-ranked mask filtering to explain distances between embedded points.

Result: Effective in identifying features contributing to similarity/dissimilarity in cross-modal embeddings, with high robustness and consistency.

Conclusion: Fills a critical gap in XAI, improving transparency and trustworthiness in deep learning applications using embedded spaces.

Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.

</details>


### [579] [A Temporal Difference Method for Stochastic Continuous Dynamics](https://arxiv.org/pdf/2505.15544)
*Haruki Settai, Naoya Takeishi, Takehisa Yairi*

Main category: cs.LG

TL;DR: A model-free reinforcement learning approach targeting the HJB equation is proposed, overcoming the need for known dynamics and bridging stochastic optimal control with RL.


<details>
  <summary>Details</summary>
Motivation: Existing HJB-based RL methods require known dynamics, limiting their applicability. This work aims to remove this constraint.

Method: Proposes a model-free temporal difference method targeting the HJB equation without needing explicit dynamics.

Result: Demonstrates advantages over transition kernel-based methods, both theoretically and empirically.

Conclusion: The approach bridges stochastic optimal control and model-free RL, offering broader applicability.

Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.

</details>


### [580] [Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning](https://arxiv.org/pdf/2505.15547)
*Adrian Arnaiz-Rodriguez, Federico Errica*

Main category: cs.LG

TL;DR: The paper critiques common beliefs in graph machine learning, highlighting ambiguities around oversmoothing, oversquashing, homophily-heterophily, and long-range tasks, and encourages clearer research directions.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities and misunderstandings in graph machine learning research, particularly around message-passing's limitations and benefits.

Method: The authors analyze and challenge commonly accepted beliefs using simple counterexamples and critical thinking.

Result: Identifies ambiguities in research problems and promotes distinct but interconnected research directions.

Conclusion: Encourages clearer distinctions between issues and fosters focused research to address them effectively.

Abstract: After a renaissance phase in which researchers revisited the message-passing
paradigm through the lens of deep learning, the graph machine learning
community shifted its attention towards a deeper and practical understanding of
message-passing's benefits and limitations. In this position paper, we notice
how the fast pace of progress around the topics of oversmoothing and
oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came
with the consolidation of commonly accepted beliefs and assumptions that are
not always true nor easy to distinguish from each other. We argue that this has
led to ambiguities around the investigated problems, preventing researchers
from focusing on and addressing precise research questions while causing a good
amount of misunderstandings. Our contribution wants to make such common beliefs
explicit and encourage critical thinking around these topics, supported by
simple but noteworthy counterexamples. The hope is to clarify the distinction
between the different issues and promote separate but intertwined research
directions to address them.

</details>


### [581] [Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution](https://arxiv.org/pdf/2505.15548)
*Suvadeep Hajra*

Main category: cs.LG

TL;DR: The paper identifies training instability in transformers due to self-attention's limited short-range dependency capture, proposes LS-attention to stabilize training, and shows improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Transformers suffer from training instability, often due to self-attention's inability to handle short-range dependencies, leading to logit explosion and destabilized training.

Method: Proposes Long Short-attention (LS-attention), decomposing self-attention into local (short-range) and global (long-range) heads to mitigate logit growth.

Result: LS-attention reduces validation perplexity significantly (to nearly 2/5 of one method), matches another method with 1/20 GPU hours, and cuts inference latency by up to 36%.

Conclusion: LS-attention effectively addresses training instability in transformers, offering improved stability, efficiency, and performance.

Abstract: Transformer language models have driven significant progress across various
fields, including natural language processing and computer vision. A central
component of these models is the self-attention (SA) mechanism, which learns
rich vector representations of tokens by modeling their relationships with
others in a sequence. However, despite extensive research, transformers
continue to suffer from training instability -- often manifesting as spikes or
divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited
ability to capture short-range dependencies, especially in tasks like language
modeling, where almost every token heavily relies on its nearby neighbors. This
limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing
training. To address this, we propose decomposing the SA into local
(short-range) and global (long-range) attention heads. This decomposed
attention, referred to as Long Short-attention (LS-attention), mitigates logit
explosion and results in more stable training compared to an equivalent
multi-head self-attention (MHSA). Empirical comparisons with two alternative
training stabilization methods show that LS-attention reduces the validation
perplexity to nearly 2/5 of that achieved by one method and reaches a similar
perplexity as the other method using only 1/20 of the GPU hours. Additionally,
our experiments demonstrate that LS-attention reduces inference latency by up
to 36% compared to a state-of-the-art implementation of equivalent MHSA.

</details>


### [582] [Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection](https://arxiv.org/pdf/2505.15560)
*Julian Oelhaf, Georg Kordowich, Changhun Kim, Paula Andrea Perez-Toro, Andreas Maier, Johann Jager, Siming Bayer*

Main category: cs.LG

TL;DR: The paper proposes a framework to evaluate how data sparsity affects ML-based fault detection (FD) and fault line identification (FLI) in power grids, showing FD remains robust while FLI is more sensitive.


<details>
  <summary>Details</summary>
Motivation: Germany's shift to renewable energy requires reliable grid monitoring, but data sparsity challenges ML-based protection methods like FD and FLI.

Method: The study simulates data sparsity scenarios, assesses their impact on ML models, and applies this to an existing framework.

Result: FD performance stays strong (F1-score 0.999 ± 0.000) even with 50x less data, while FLI drops by 55.61% for missing voltage and 9.73% for communication failures.

Conclusion: The framework provides insights for optimizing ML models in grid protection, improving FD efficiency and FLI resilience.

Abstract: Germany's transition to a renewable energy-based power system is reshaping
grid operations, requiring advanced monitoring and control to manage
decentralized generation. Machine learning (ML) has emerged as a powerful tool
for power system protection, particularly for fault detection (FD) and fault
line identification (FLI) in transmission grids. However, ML model reliability
depends on data quality and availability. Data sparsity resulting from sensor
failures, communication disruptions, or reduced sampling rates poses a
challenge to ML-based FD and FLI. Yet, its impact has not been systematically
validated prior to this work. In response, we propose a framework to assess the
impact of data sparsity on ML-based FD and FLI performance. We simulate
realistic data sparsity scenarios, evaluate their impact, derive quantitative
insights, and demonstrate the effectiveness of this evaluation strategy by
applying it to an existing ML-based framework. Results show the ML model
remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after
a 50x data reduction. In contrast, FLI is more sensitive, with performance
decreasing by 55.61% for missing voltage measurements and 9.73% due to
communication failures at critical network points. These findings offer
actionable insights for optimizing ML models for real-world grid protection.
This enables more efficient FD and supports targeted improvements in FLI.

</details>


### [583] [Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers](https://arxiv.org/pdf/2505.15570)
*Marko Tuononen, Duy Vu, Dani Korpi, Vesa Starck, Ville Hautamäki*

Main category: cs.LG

TL;DR: The paper introduces Neural Activation Pattern (NAP) for clustering full-layer activations to uncover distributed concepts, improving normalization and clustering methods. It finds SNR as a key learned factor in radio models and enhances generalization.


<details>
  <summary>Details</summary>
Motivation: To move beyond individual neuron analysis and discover distributed layer-wide patterns in neural networks, improving interpretability and troubleshooting.

Method: Proposes NAP methodology with improved normalization, distribution estimation, distance metrics, and cluster selection, applied to visual and radio models.

Result: In radio models, no distinct concepts emerged; instead, a continuous activation manifold shaped by SNR was observed, validating its physical plausibility. Enhanced NAP improved generalization.

Conclusion: Clustering design and activation manifolds are crucial for interpreting neural networks, with NAP improvements aiding generalization and validation.

Abstract: Concept discovery in neural networks often targets individual neurons or
human-interpretable features, overlooking distributed layer-wide patterns. We
study the Neural Activation Pattern (NAP) methodology, which clusters
full-layer activation distributions to identify such layer-level concepts.
Applied to visual object recognition and radio receiver models, we propose
improved normalization, distribution estimation, distance metrics, and varied
cluster selection. In the radio receiver model, distinct concepts did not
emerge; instead, a continuous activation manifold shaped by Signal-to-Noise
Ratio (SNR) was observed -- highlighting SNR as a key learned factor,
consistent with classical receiver behavior and supporting physical
plausibility. Our enhancements to NAP improved in-distribution vs.
out-of-distribution separation, suggesting better generalization and indirectly
validating clustering quality. These results underscore the importance of
clustering design and activation manifolds in interpreting and troubleshooting
neural network behavior.

</details>


### [584] [Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback](https://arxiv.org/pdf/2505.15572)
*Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Haifeng Chen, Yanjie Fu*

Main category: cs.LG

TL;DR: A reinforcement learning-based framework enhances foundation models for generating accurate and interpretable mathematical equations from data, improving domain adaptability and mathematical semantics.


<details>
  <summary>Details</summary>
Motivation: Existing methods (genetic programming, deep learning, and foundation models) are inefficient or lack domain-specific effectiveness and mathematical accuracy for the Data2Eqn task.

Method: Proposes a reinforcement learning-based finetuning framework to optimize a pretrained model's generation policy using reward signals from downstream numerical fitness.

Result: The method improves accuracy and robustness in equation generation under complex data distributions.

Conclusion: The framework effectively adapts foundation models to domain-specific tasks, generating mathematically meaningful equations.

Abstract: The data-to-equation (Data2Eqn) task aims to discover interpretable
mathematical equations that map observed values to labels, offering physical
insights and broad applicability across academic and industrial domains.
Genetic programming and traditional deep learning-based approaches suffer from
search inefficiency and poor generalization on small task-specific datasets.
Foundation models showed promise in this area, but existing approaches suffer
from: 1) They are pretrained on general-purpose data distributions, making them
less effective for domain-specific tasks; and 2) their training objectives
focus on token-level alignment, overlooking mathematical semantics, which can
lead to inaccurate equations. To address these issues, we aim to enhance the
domain adaptability of foundation models for Data2Eqn tasks. In this work, we
propose a reinforcement learning-based finetuning framework that directly
optimizes the generation policy of a pretrained model through reward signals
derived from downstream numerical fitness. Our method allows the model to adapt
to specific and complex data distributions and generate mathematically
meaningful equations. Extensive experiments demonstrate that our approach
improves both the accuracy and robustness of equation generation under complex
distributions.

</details>


### [585] [Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions](https://arxiv.org/pdf/2505.15579)
*Hossein Zakerinia, Jonathan Scott, Christoph H. Lampert*

Main category: cs.LG

TL;DR: FLowDUP is a novel method for personalized federated learning that generates personalized models using only unlabeled data, with efficient communication and computation.


<details>
  <summary>Details</summary>
Motivation: Existing personalized federated learning methods require labeled data for training, which FLowDUP addresses by enabling unlabeled data usage.

Method: FLowDUP generates personalized models via a forward pass with unlabeled data, leveraging a low-dimensional subspace for efficiency, and is guided by a transductive multi-task PAC-Bayesian generalization bound.

Result: FLowDUP shows strong empirical performance across datasets with heterogeneous clients, validated by ablation studies.

Conclusion: FLowDUP effectively enables personalized federated learning with unlabeled data, supported by theory and experiments.

Abstract: Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.

</details>


### [586] [World Models as Reference Trajectories for Rapid Motor Adaptation](https://arxiv.org/pdf/2505.15589)
*Carlos Stein Brito, Daniel McNamee*

Main category: cs.LG

TL;DR: A dual control framework, Reflexive World Models (RWM), is introduced for rapid adaptation in changing dynamics, combining long-term RL with fast latent control.


<details>
  <summary>Details</summary>
Motivation: Addressing performance degradation in learned control policies when system dynamics change unexpectedly.

Method: Uses world model predictions as implicit reference trajectories, separating control into long-term RL and rapid latent execution.

Result: Achieves faster adaptation with low computational cost and near-optimal performance compared to model-based RL baselines.

Conclusion: Provides a principled approach for maintaining performance in high-dimensional continuous control tasks under varying dynamics.

Abstract: Deploying learned control policies in real-world environments poses a
fundamental challenge. When system dynamics change unexpectedly, performance
degrades until models are retrained on new data. We introduce Reflexive World
Models (RWM), a dual control framework that uses world model predictions as
implicit reference trajectories for rapid adaptation. Our method separates the
control problem into long-term reward maximization through reinforcement
learning and robust motor execution through rapid latent control. This dual
architecture achieves significantly faster adaptation with low online
computational cost compared to model-based RL baselines, while maintaining
near-optimal performance. The approach combines the benefits of flexible policy
learning through reinforcement learning with rapid error correction
capabilities, providing a principled approach to maintaining performance in
high-dimensional continuous control tasks under varying dynamics.

</details>


### [587] [Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off](https://arxiv.org/pdf/2505.15594)
*Yury Belousov, Brian Pulfer, Vitaliy Kinakh, Slava Voloshynovskiy*

Main category: cs.LG

TL;DR: The paper examines the effectiveness of Diffusion Denoised Smoothing for adversarial robustness in foundation models, revealing trade-offs between performance and protection.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored effectiveness of Diffusion Denoised Smoothing beyond classification tasks and assess its robustness against adversarial attacks.

Method: Analyzed three datasets with four downstream tasks under three adversarial attack algorithms, testing high-noise and low-noise diffusion denoising settings.

Result: High-noise diffusion degrades performance by up to 57%, while low-noise preserves performance but lacks robustness. A novel attack targeting diffusion was introduced.

Conclusion: The trade-off between adversarial robustness and performance remains unresolved, highlighting challenges in current defense methods.

Abstract: While foundation models demonstrate impressive performance across various
tasks, they remain vulnerable to adversarial inputs. Current research explores
various approaches to enhance model robustness, with Diffusion Denoised
Smoothing emerging as a particularly promising technique. This method employs a
pretrained diffusion model to preprocess inputs before model inference. Yet,
its effectiveness remains largely unexplored beyond classification. We aim to
address this gap by analyzing three datasets with four distinct downstream
tasks under three different adversarial attack algorithms. Our findings reveal
that while foundation models maintain resilience against conventional
transformations, applying high-noise diffusion denoising to clean images
without any distortions significantly degrades performance by as high as 57%.
Low-noise diffusion settings preserve performance but fail to provide adequate
protection across all attack types. Moreover, we introduce a novel attack
strategy specifically targeting the diffusion process itself, capable of
circumventing defenses in the low-noise regime. Our results suggest that the
trade-off between adversarial robustness and performance remains a challenge to
be addressed.

</details>


### [588] [Deep Learning for Continuous-time Stochastic Control with Jumps](https://arxiv.org/pdf/2505.15602)
*Patrick Cheridito, Jean-Loup Dupret, Donatien Hainaut*

Main category: cs.LG

TL;DR: A model-based deep-learning method for solving finite-horizon continuous-time stochastic control problems with jumps, using two neural networks for policy and value function approximation.


<details>
  <summary>Details</summary>
Motivation: To address complex, high-dimensional stochastic control tasks by leveraging deep learning and dynamic programming principles.

Method: Iterative training of two neural networks (policy and value function) using objectives derived from the Hamilton-Jacobi-Bellman equation.

Result: Empirical evaluations show the approach is accurate and scalable for high-dimensional problems.

Conclusion: The method effectively solves complex stochastic control tasks by combining deep learning with dynamic programming.

Abstract: In this paper, we introduce a model-based deep-learning approach to solve
finite-horizon continuous-time stochastic control problems with jumps. We
iteratively train two neural networks: one to represent the optimal policy and
the other to approximate the value function. Leveraging a continuous-time
version of the dynamic programming principle, we derive two different training
objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the
networks capture the underlying stochastic dynamics. Empirical evaluations on
different problems illustrate the accuracy and scalability of our approach,
demonstrating its effectiveness in solving complex, high-dimensional stochastic
control tasks.

</details>


### [589] [Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI](https://arxiv.org/pdf/2505.15622)
*Pietro Bartoli, Christian Veronesi, Andrea Giudici, David Siorpaes, Diana Trojaniello, Franco Zappa*

Main category: cs.LG

TL;DR: The paper introduces a benchmarking methodology for TinyML on MCUs, integrating energy and latency measurements across three execution phases, and demonstrates its effectiveness on the STM32N6 MCU.


<details>
  <summary>Details</summary>
Motivation: The need for efficient performance evaluation of TinyML on resource-constrained devices like MCUs, given diverse architectures and application scenarios.

Method: A benchmarking setup measuring energy and latency in pre-inference, inference, and post-inference phases, with automated testing for statistical relevance.

Result: Reducing core voltage and clock frequency improves pre- and post-processing efficiency without significantly impacting network execution.

Conclusion: The methodology enables cross-platform comparisons and quantifies efficiency variations across hardware implementations.

Abstract: The rise of IoT has increased the need for on-edge machine learning, with
TinyML emerging as a promising solution for resource-constrained devices such
as MCU. However, evaluating their performance remains challenging due to
diverse architectures and application scenarios. Current solutions have many
non-negligible limitations. This work introduces an alternative benchmarking
methodology that integrates energy and latency measurements while
distinguishing three execution phases pre-inference, inference, and
post-inference. Additionally, the setup ensures that the device operates
without being powered by an external measurement unit, while automated testing
can be leveraged to enhance statistical significance. To evaluate our setup, we
tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two
configurations were considered: high-performance and Low-power. The variation
of the EDP was analyzed separately for each phase, providing insights into the
impact of hardware configurations on energy efficiency. Each model was tested
1000 times to ensure statistically relevant results. Our findings demonstrate
that reducing the core voltage and clock frequency improve the efficiency of
pre- and post-processing without significantly affecting network execution
performance. This approach can also be used for cross-platform comparisons to
determine the most efficient inference platform and to quantify how pre- and
post-processing overhead varies across different hardware implementations.

</details>


### [590] [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/pdf/2505.15624)
*H. V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Munachiso Nwadike, Kentaro Inui*

Main category: cs.LG

TL;DR: The paper explores grokking in neural networks, identifying embeddings as key to delayed generalization. It reveals two mechanisms—embedding update dynamics and bilinear coupling—and proposes solutions like frequency-aware sampling and adaptive learning rates to improve grokking and Transformer optimization.


<details>
  <summary>Details</summary>
Motivation: To understand the underexplored components driving grokking (delayed generalization) in neural networks, particularly the role of embeddings.

Method: Analyzes embedding dynamics and bilinear coupling, proposes frequency-aware sampling and adaptive learning rates based on loss landscape curvature.

Result: Identifies embedding stagnation and bilinear coupling as key grokking mechanisms. Adaptive learning rates and balanced sampling mitigate these issues, improving convergence.

Conclusion: Embeddings are central to grokking. The proposed methods enhance grokking dynamics and address broader challenges in Transformer optimization.

Abstract: Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.

</details>


### [591] [Aligning Explanations with Human Communication](https://arxiv.org/pdf/2505.15626)
*Jacopo Teneggi, Zhenzhen Wang, Paul H. Yi, Tianmin Shu, Jeremias Sulam*

Main category: cs.LG

TL;DR: The paper introduces listener-adaptive explanations in machine learning, improving interpretability by tailoring explanations to user understanding, validated through image classification tasks and a user study.


<details>
  <summary>Details</summary>
Motivation: Current explainability methods lack consideration for the user's communicative context, leading to explanations that may not be understood by all audiences.

Method: An iterative procedure based on pragmatic reasoning and rational speech act generates explanations using pairwise preferences between candidates, without needing a listener model.

Result: Improved alignment between explanations and listener preferences in image classification tasks and increased communicative utility in a user study.

Conclusion: Listener-adaptive explanations enhance interpretability by adapting to user understanding, validated by empirical results.

Abstract: Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.

</details>


### [592] [Second-Order Convergence in Private Stochastic Non-Convex Optimization](https://arxiv.org/pdf/2505.15647)
*Youming Tao, Zuyuan Zhang, Dongxiao Yu, Xiuzhen Cheng, Falko Dressler, Di Wang*

Main category: cs.LG

TL;DR: The paper proposes a perturbed stochastic gradient descent (PSGD) framework to address limitations in differentially private (DP) stochastic non-convex optimization, improving convergence rates and eliminating the need for auxiliary private model selection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for finding second-order stationary points (SOSP) in DP non-convex optimization suffer from inaccurate convergence rates and reliance on private model selection, which harms utility, especially in distributed settings.

Method: The authors introduce a PSGD framework using Gaussian noise injection and gradient oracles, leveraging model drift distance for saddle point escape. They also use the adaptive DP-SPIDER estimator for improved convergence.

Result: The proposed method corrects prior convergence error rates and extends to distributed learning with heterogeneous data, providing formal guarantees for DP-SOSP. Numerical experiments validate its efficacy.

Conclusion: The framework offers practical benefits by avoiding private selection procedures and improving convergence, with demonstrated effectiveness in real-world applications.

Abstract: We investigate the problem of finding second-order stationary points (SOSP)
in differentially private (DP) stochastic non-convex optimization. Existing
methods suffer from two key limitations: (i) inaccurate convergence error rate
due to overlooking gradient variance in the saddle point escape analysis, and
(ii) dependence on auxiliary private model selection procedures for identifying
DP-SOSP, which can significantly impair utility, particularly in distributed
settings. To address these issues, we propose a generic perturbed stochastic
gradient descent (PSGD) framework built upon Gaussian noise injection and
general gradient oracles. A core innovation of our framework is using model
drift distance to determine whether PSGD escapes saddle points, ensuring
convergence to approximate local minima without relying on second-order
information or additional DP-SOSP identification. By leveraging the adaptive
DP-SPIDER estimator as a specific gradient oracle, we develop a new DP
algorithm that rectifies the convergence error rates reported in prior work. We
further extend this algorithm to distributed learning with arbitrarily
heterogeneous data, providing the first formal guarantees for finding DP-SOSP
in such settings. Our analysis also highlights the detrimental impacts of
private selection procedures in distributed learning under high-dimensional
models, underscoring the practical benefits of our design. Numerical
experiments on real-world datasets validate the efficacy of our approach.

</details>


### [593] [Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks](https://arxiv.org/pdf/2505.15631)
*Nick Kocher, Christian Wassermann, Leona Hennig, Jonas Seng, Holger Hoos, Kristian Kersting, Marius Lindauer, Matthias Müller*

Main category: cs.LG

TL;DR: The paper addresses energy consumption in Neural Architecture Search (NAS) by proposing energy-aware benchmarking principles and analyzing their impact, highlighting issues with GPU measurement APIs and suggesting improvements for accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce the high energy consumption in NAS by developing reliable energy-aware benchmarking methods.

Method: Proposes three design principles for energy-aware benchmarks: reliable power measurements, wide GPU usage range, and holistic cost reporting. Analyzes EA-HAS-Bench using these principles.

Result: Finds that GPU measurement APIs like Nvidia SMI can lead to faulty low-power estimations. Shows narrow GPU usage range and proposes calibration experiments to improve accuracy.

Conclusion: Highlights key considerations for energy-aware benchmarking and provides guidelines for designing novel benchmarks, improving measurement accuracy.

Abstract: Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.

</details>


### [594] [LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought](https://arxiv.org/pdf/2505.15657)
*Cheng Yan, Felix Mohr, Tom Viering*

Main category: cs.LG

TL;DR: Learning curves are less well-behaved (monotone and convex) than previously thought, with 14% showing ill-behavior. LCDB 1.1 highlights this issue and its impact on tasks like model selection.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that learning curves are well-behaved and study their actual behavior using a large-scale database.

Method: Constructed LCDB 1.1, a high-resolution learning curves database, and used statistically rigorous methods to analyze curve behavior.

Result: 14% of learning curves exhibit significant ill-behavior, with certain learners more prone to it. Feature scaling rarely resolves the issue.

Conclusion: Ill-behavior in learning curves poses challenges for tasks like model selection, making LCDB 1.1 a valuable benchmark for future research.

Abstract: Sample-wise learning curves plot performance versus training set size. They
are useful for studying scaling laws and speeding up hyperparameter tuning and
model selection. Learning curves are often assumed to be well-behaved: monotone
(i.e. improving with more data) and convex. By constructing the Learning Curves
Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning
curves, we show that learning curves are less often well-behaved than
previously thought. Using statistically rigorous methods, we observe
significant ill-behavior in approximately 14% of the learning curves, almost
twice as much as in previous estimates. We also identify which learners are to
blame and show that specific learners are more ill-behaved than others.
Additionally, we demonstrate that different feature scalings rarely resolve
ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such
as learning curve fitting and model selection, and find it poses significant
challenges, underscoring the relevance and potential of LCDB 1.1 as a
challenging benchmark for future research.

</details>


### [595] [Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes](https://arxiv.org/pdf/2505.15638)
*Daniel Waxman, Fernando Llorente, Petar M. Djurić*

Main category: cs.LG

TL;DR: The paper introduces Online Bayesian Stacking (OBS), a method for adaptively combining Bayesian models in online learning, outperforming Bayesian model averaging (BMA) in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning optimal combinations of Bayesian models in an online, continual learning setting, while reinterpreting existing methods like BMA and Bayesian stacking.

Method: Proposes OBS, which optimizes the log-score over predictive distributions, connecting it to portfolio selection for efficient algorithms and regret analysis.

Result: OBS outperforms online BMA in specific scenarios, with theoretical and empirical validation.

Conclusion: Provides principled guidance on choosing between OBS and online BMA, bridging Bayesian ensemble learning with portfolio selection theory.

Abstract: We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.

</details>


### [596] [Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima](https://arxiv.org/pdf/2505.15643)
*Lan V. Truong*

Main category: cs.LG

TL;DR: The paper revisits the Track-and-Stop algorithm for best-arm identification in multi-armed bandits with multiple optimal arms, proposing a modified stopping rule for instance-optimality.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient understanding of the Track-and-Stop algorithm's performance when multiple optimal arms exist.

Method: Proposes a modified stopping rule for the Track-and-Stop strategy and introduces a new information-theoretic lower bound accounting for multiple optima.

Result: The modified stopping rule tightly matches the new lower bound, ensuring instance-optimality.

Conclusion: The work successfully extends the Track-and-Stop algorithm's optimality to cases with multiple optimal arms.

Abstract: We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.

</details>


### [597] [Learning Small Decision Trees with Few Outliers: A Parameterized Perspective](https://arxiv.org/pdf/2505.15648)
*Harmender Gahlawat, Meirav Zehavi*

Main category: cs.LG

TL;DR: The paper explores the parameterized complexity of generalized Decision Tree Learning, focusing on minimizing tree size or depth while allowing limited disagreements with given data. It proves W[1]-hardness for certain parameters and shows fixed-parameter tractability when including the disagreement parameter. Kernelization results are also discussed.


<details>
  <summary>Details</summary>
Motivation: Decision trees are widely used in machine learning, but constructing small trees efficiently is challenging. Understanding the parameterized complexity of such problems helps in developing better algorithms.

Method: The study generalizes Decision Tree Learning to include a tolerance for disagreements (t). It analyzes two problems, DTSO (minimizing size) and DTDO (minimizing depth), proving hardness and tractability under different parameterizations. Kernelization techniques are also examined.

Result: DTSO and DTDO are W[1]-hard when parameterized by s+δ_max and d+δ_max, respectively. However, they become FPT when including the parameter t. Kernelization yields both positive and negative results.

Conclusion: The work provides insights into the complexity of decision tree learning under disagreement constraints, highlighting conditions for tractability and limitations in kernelization.

Abstract: Decision trees are a fundamental tool in machine learning for representing,
classifying, and generalizing data. It is desirable to construct ``small''
decision trees, by minimizing either the \textit{size} ($s$) or the
\textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the
parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot
of attention. We consider a generalization of \textsc{Decision Tree Learning}
where given a \textit{classification instance} $E$ and an integer $t$, the task
is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$
examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the
goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We
first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when
parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where
$\delta_{max}$ is the maximum number of features in which two differently
labeled examples can differ. We complement this result by showing that these
problems become \textsc{FPT} if we include the parameter $t$. We also consider
the kernelization complexity of these problems and establish several positive
and negative results for both \textsc{DTSO} and \textsc{DTDO}.

</details>


### [598] [A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO](https://arxiv.org/pdf/2505.15694)
*Xingyu Zhou, Yulian Wu, Francesco Orabona*

Main category: cs.LG

TL;DR: The paper analyzes the impact of noisy labels in offline alignment, focusing on privacy and adversarial robustness under linear models, comparing LTC and CTL scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand how noisy labels affect offline alignment, especially the interaction between privacy (local differential privacy) and adversarial corruption.

Method: A reduction framework converts the offline alignment problem into logistic regression parameter estimation under linear modeling assumptions.

Result: LTC (privatization before corruption) is more challenging than CTL (corruption before privatization) in offline alignment.

Conclusion: The study provides insights into offline alignment under noisy labels and advances theoretical understanding of privacy-only or corruption-only scenarios.

Abstract: In this paper, we theoretically investigate the effects of noisy labels in
offline alignment, with a focus on the interplay between privacy and robustness
against adversarial corruption. Specifically, under linear modeling
assumptions, we present a unified analysis covering both reinforcement learning
from human feedback (RLHF) and direct preference optimization (DPO) under
different privacy-corruption scenarios, such as Local differential
privacy-then-Corruption (LTC), where human preference labels are privatized
before being corrupted by an adversary, and Corruption-then-Local differential
privacy (CTL), where labels are corrupted before privacy protection. Our
analysis leverages a reduction framework that reduces the offline alignment
problem under linear modeling assumptions to parameter estimation in logistic
regression. This framework allows us to establish an interesting separation
result between LTC and CTL, demonstrating that LTC presents a greater challenge
than CTL in offline alignment, even under linear models. As important
by-products, our findings also advance the state-of-the-art theoretical results
in offline alignment under privacy-only or corruption-only scenarios.

</details>


### [599] [Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms](https://arxiv.org/pdf/2505.15661)
*Sina Mohammad-Taheri, Matthew J. Colbrook, Simone Brugiapaglia*

Main category: cs.LG

TL;DR: The paper introduces differentiable versions of Orthogonal Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT) using soft permutation matrices, enabling their integration into neural networks.


<details>
  <summary>Details</summary>
Motivation: Gradient-based learning requires differentiability, but greedy sparse recovery algorithms like OMP and IHT rely on non-differentiable operations, limiting their use in neural networks.

Method: Proposes Soft-OMP and Soft-IHT by approximating non-differentiable argsort with soft permutation matrices derived from softsort.

Result: Theoretical and numerical results show Soft-OMP and Soft-IHT effectively approximate OMP and IHT, enabling trainable OMP- and IHT-Net architectures.

Conclusion: The approach bridges sparse recovery and neural networks, allowing structured sparse recovery and latent sparsity pattern extraction.

Abstract: Gradient-based learning imposes (deep) neural networks to be differentiable
at all steps. This includes model-based architectures constructed by unrolling
iterations of an iterative algorithm onto layers of a neural network, known as
algorithm unrolling. However, greedy sparse recovery algorithms depend on the
non-differentiable argsort operator, which hinders their integration into
neural networks. In this paper, we address this challenge in Orthogonal
Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular
representative algorithms in this class. We propose permutation-based variants
of these algorithms and approximate permutation matrices using "soft"
permutation matrices derived from softsort, a continuous relaxation of argsort.
We demonstrate -- both theoretically and numerically -- that Soft-OMP and
Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible
with neural network training, effectively approximate these algorithms with a
controllable degree of accuracy. This leads to the development of OMP- and
IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,
respectively. Finally, by choosing weights as "structure-aware" trainable
parameters, we connect our approach to structured sparse recovery and
demonstrate its ability to extract latent sparsity patterns from data.

</details>


### [600] [Graph Conditional Flow Matching for Relational Data Generation](https://arxiv.org/pdf/2505.15668)
*Davide Scassola, Sebastiano Saccani, Luca Bortolussi*

Main category: cs.LG

TL;DR: Proposes a generative model for relational data using flow matching and graph neural networks to handle complex structures and dependencies.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-table data lack flexibility and expressiveness, struggling with complex relational structures and dependencies.

Method: Uses a deep generative model with flow matching and graph neural networks to generate relational dataset content based on foreign-key relationships.

Result: Achieves state-of-the-art performance in synthetic data fidelity on benchmark datasets.

Conclusion: The method is flexible and expressive, effectively capturing complex relational structures and dependencies.

Abstract: Data synthesis is gaining momentum as a privacy-enhancing technology. While
single-table tabular data generation has seen considerable progress, current
methods for multi-table data often lack the flexibility and expressiveness
needed to capture complex relational structures. In particular, they struggle
with long-range dependencies and complex foreign-key relationships, such as
tables with multiple parent tables or multiple types of links between the same
pair of tables. We propose a generative model for relational data that
generates the content of a relational dataset given the graph formed by the
foreign-key relationships. We do this by learning a deep generative model of
the content of the whole relational database by flow matching, where the neural
network trained to denoise records leverages a graph neural network to obtain
information from connected records. Our method is flexible, as it can support
relational datasets with complex structures, and expressive, as the generation
of each record can be influenced by any other record within the same connected
component. We evaluate our method on several benchmark datasets and show that
it achieves state-of-the-art performance in terms of synthetic data fidelity.

</details>


### [601] [A packing lemma for VCN${}_k$-dimension and learning high-dimensional data](https://arxiv.org/pdf/2505.15688)
*Leonardo N. Coregliano, Maryanthe Malliaris*

Main category: cs.LG

TL;DR: The paper completes the characterization of non-partite, non-agnostic high-arity PAC learnability by linking it to a high-arity Haussler packing property and finiteness of VCN$_k$-dimension.


<details>
  <summary>Details</summary>
Motivation: To resolve the open problem of characterizing non-partite, non-agnostic high-arity PAC learnability left in prior work.

Method: Direct proofs showing classic PAC learnability implies the Haussler packing property, which lifts to high-arity settings.

Result: Non-partite, non-agnostic high-arity PAC learnability implies a high-arity Haussler packing property and finite VCN$_k$-dimension.

Conclusion: The work fully characterizes high-arity PAC learnability, closing the gap left in earlier research.

Abstract: Recently, the authors introduced the theory of high-arity PAC learning, which
is well-suited for learning graphs, hypergraphs and relational structures. In
the same initial work, the authors proved a high-arity analogue of the
Fundamental Theorem of Statistical Learning that almost completely
characterizes all notions of high-arity PAC learning in terms of a
combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)
$k$-dimension, leaving as an open problem only the characterization of
non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite
non-agnostic high-arity PAC learnability implies a high-arity version of the
Haussler packing property, which in turn implies finiteness of
VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC
learnability implies classic Haussler packing property, which in turn implies
finite Natarajan dimension and noticing that these direct proofs nicely lift to
high-arity.

</details>


### [602] [Privacy-Preserving Conformal Prediction Under Local Differential Privacy](https://arxiv.org/pdf/2505.15721)
*Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet*

Main category: cs.LG

TL;DR: The paper proposes two LDP-based methods for conformal prediction in privacy-sensitive scenarios, ensuring robust coverage without clean labels.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in conformal prediction when labels are perturbed and the aggregator is untrusted.

Method: Two approaches: 1) users provide perturbed labels via k-ary randomized response; 2) users add noise to conformity scores via binary search response.

Result: Finite-sample coverage guarantees are proven, with robust performance under severe randomization.

Conclusion: The methods unify strong privacy with predictive uncertainty control, suitable for sensitive applications like medical imaging or LLM queries.

Abstract: Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.

</details>


### [603] [Higher-order Structure Boosts Link Prediction on Temporal Graphs](https://arxiv.org/pdf/2505.15746)
*Jingzhe Liu, Zhigang Hua, Yan Xie, Bingheng Li, Harry Shomer, Yu Song, Kaveh Hassani, Jiliang Tang*

Main category: cs.LG

TL;DR: A Higher-order Temporal Graph Neural Network (HTGN) is proposed to address the limitations of existing TGNNs by incorporating hypergraph representations, improving efficiency and capturing higher-order structures in temporal graphs.


<details>
  <summary>Details</summary>
Motivation: Existing TGNNs overlook higher-order structures and face efficiency bottlenecks, limiting their expressive power for modeling real-world temporal graphs.

Method: HTGN integrates hypergraph representations, identifies higher-order structures, and aggregates edge features into hyperedges to reduce memory costs.

Result: HTGN outperforms existing methods in dynamic link prediction and reduces memory costs by up to 50%.

Conclusion: HTGN effectively captures higher-order interactions and improves efficiency, making it a superior choice for temporal graph learning.

Abstract: Temporal Graph Neural Networks (TGNNs) have gained growing attention for
modeling and predicting structures in temporal graphs. However, existing TGNNs
primarily focus on pairwise interactions while overlooking higher-order
structures that are integral to link formation and evolution in real-world
temporal graphs. Meanwhile, these models often suffer from efficiency
bottlenecks, further limiting their expressive power. To tackle these
challenges, we propose a Higher-order structure Temporal Graph Neural Network,
which incorporates hypergraph representations into temporal graph learning. In
particular, we develop an algorithm to identify the underlying higher-order
structures, enhancing the model's ability to capture the group interactions.
Furthermore, by aggregating multiple edge features into hyperedge
representations, HTGN effectively reduces memory cost during training. We
theoretically demonstrate the enhanced expressiveness of our approach and
validate its effectiveness and efficiency through extensive experiments on
various real-world temporal graphs. Experimental results show that HTGN
achieves superior performance on dynamic link prediction while reducing memory
costs by up to 50\% compared to existing methods.

</details>


### [604] [Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs](https://arxiv.org/pdf/2505.15747)
*Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase*

Main category: cs.LG

TL;DR: A novel framework integrates fragmented multi-modal Alzheimer's disease (AD) data using LLMs and knowledge graphs, enabling population-level analysis without matched patient IDs. It reveals new correlations and hypotheses, validated across datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional AD research requires matched patient IDs for multi-modal data integration, limiting the use of fragmented datasets. This framework aims to overcome this barrier.

Method: The approach uses statistical analysis to identify significant features in MRI, gene expression, biomarkers, EEG, and clinical data, connects them in a knowledge graph, and employs LLMs to analyze correlations and generate hypotheses.

Result: The framework identified novel relationships (e.g., metabolic risk factors linked to tau protein via neuroinflammation) and unexpected EEG-gene expression correlations, validated with consistent effect sizes (variance <15%) and expert review (Cohen's k=0.82).

Conclusion: The framework enables conceptual-level integration of fragmented AD data, generating testable hypotheses and advancing understanding of AD pathology without requiring patient ID matching.

Abstract: We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.

</details>


### [605] [Improving planning and MBRL with temporally-extended actions](https://arxiv.org/pdf/2505.15754)
*Palash Chatterjee, Roni Khardon*

Main category: cs.LG

TL;DR: The paper proposes controlling continuous decision timescales directly by treating action duration as an optimization variable, improving planning efficiency and performance in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Discrete time dynamics in continuous systems require small simulation steps, leading to computationally demanding planning and reduced performance.

Method: Use temporally-extended actions and optimize action duration alongside standard action variables, integrating it into model-based reinforcement learning (MBRL).

Result: The approach speeds up simulation, reduces compounding errors, and enables solutions to previously unsolvable problems.

Conclusion: Directly optimizing action durations enhances planning efficiency and performance in both planning and MBRL settings.

Abstract: Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.

</details>


### [606] [Projection-Based Correction for Enhancing Deep Inverse Networks](https://arxiv.org/pdf/2505.15777)
*Jorge Bacca*

Main category: cs.LG

TL;DR: A projection-based correction method is introduced to ensure deep inverse networks adhere to physical constraints, improving reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often violate physical constraints in inverse problems, limiting their reliability.

Method: A projection step is applied to initial estimates from deep networks to enforce solution validity.

Result: The method improves reconstruction accuracy across various inverse problems and architectures.

Conclusion: The projection-based correction enhances deep inverse networks by ensuring physical consistency.

Abstract: Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.

</details>


### [607] [Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning](https://arxiv.org/pdf/2505.15782)
*Pedro P. Santos, Alberto Sardinha, Francisco S. Melo*

Main category: cs.LG

TL;DR: First approach for solving infinite-horizon discounted GUMDPs in the single-trial regime, leveraging policy optimization and Monte-Carlo tree search for superior performance.


<details>
  <summary>Details</summary>
Motivation: Address the gap in solving GUMDPs when performance is evaluated on a single trajectory, a previously unexplored scenario.

Method: Investigates policy optimization classes, formulates an equivalent MDP, and uses Monte-Carlo tree search for online planning.

Result: Demonstrates superior performance compared to baselines in experimental evaluations.

Conclusion: The approach effectively solves GUMDPs in the single-trial regime, combining theoretical insights with practical algorithms.

Abstract: In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.

</details>


### [608] [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/pdf/2505.15784)
*Jun Wan, Lingrui Mei*

Main category: cs.LG

TL;DR: The paper connects LLM architectures to Algorithmic Information Theory (AIT), explaining training as program length optimization and next-token prediction as Solomonoff induction. It unifies in-context learning, few-shot learning, and scaling laws, and proposes a method for few-shot example selection based on predictive confidence, showing performance gains.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous theoretical framework explaining LLM behaviors, bridging the gap between fragmented theories and empirical success.

Method: Establishes formal links between LLMs and AIT, proving training approximates Solomonoff prior and next-token prediction implements Solomonoff induction. Introduces a principled few-shot example selection method.

Result: Theoretical framework explains in-context learning, few-shot learning, and scaling laws. Experimental validation shows improved performance for few-shot learning with low-confidence examples.

Conclusion: The work unifies LLM theory and practice, offering explanatory power and actionable insights for future model development.

Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.

</details>


### [609] [Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates](https://arxiv.org/pdf/2505.15788)
*Zahra Khatti, Daniel P. Robinson, Frank E. Curtis*

Main category: cs.LG

TL;DR: A new fair supervised ML strategy using smooth nonconvex surrogates and hard constraints for tractable, fair models with minimal tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness in ML by improving approximation of unfairness measures and avoiding regularization complexities.

Method: Introduces smooth nonconvex surrogates for Heaviside functions and employs hard constraints for unfairness tolerances.

Result: Ensures fair prediction models with tractable optimization and minimal tuning.

Conclusion: Proposed strategy effectively balances fairness and practicality in supervised learning.

Abstract: A new strategy for fair supervised machine learning is proposed. The main
advantages of the proposed strategy as compared to others in the literature are
as follows. (a) We introduce a new smooth nonconvex surrogate to approximate
the Heaviside functions involved in discontinuous unfairness measures. The
surrogate is based on smoothing methods from the optimization literature, and
is new for the fair supervised learning literature. The surrogate is a tight
approximation which ensures the trained prediction models are fair, as opposed
to other (e.g., convex) surrogates that can fail to lead to a fair prediction
model in practice. (b) Rather than rely on regularizers (that lead to
optimization problems that are difficult to solve) and corresponding
regularization parameters (that can be expensive to tune), we propose a
strategy that employs hard constraints so that specific tolerances for
unfairness can be enforced without the complications associated with the use of
regularization. (c)~Our proposed strategy readily allows for constraints on
multiple (potentially conflicting) unfairness measures at the same time.
Multiple measures can be considered with a regularization approach, but at the
cost of having even more difficult optimization problems to solve and further
expense for tuning. By contrast, through hard constraints, our strategy leads
to optimization models that can be solved tractably with minimal tuning.

</details>


### [610] [Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning](https://arxiv.org/pdf/2505.15798)
*Taehoon Kim, Henry Gouk, Minyoung Kim, Timothy Hospedales*

Main category: cs.LG

TL;DR: The paper connects model fusion methods with generalization certificates, showing minor adjustments to existing strategies can provide non-trivial guarantees, even with small datasets.


<details>
  <summary>Details</summary>
Motivation: To certify the IID generalization ability of deep networks for high-stakes AI applications, where non-vacuous guarantees are challenging.

Method: Focuses on data-driven learning via model fusion (not fine-tuning) to achieve tiny, size-independent generalization gaps.

Result: Demonstrates non-trivial generalization guarantees with as few as 100 examples, using models like VIT-B and mistral-7B.

Conclusion: Enables certification of existing systems and opens new research directions bridging practice and theory.

Abstract: Certifying the IID generalisation ability of deep networks is the first of
many requirements for trusting AI in high-stakes applications from medicine to
security. However, when instantiating generalisation bounds for deep networks
it remains challenging to obtain non-vacuous guarantees, especially when
applying contemporary large models on the small scale data prevalent in such
high-stakes fields. In this paper, we draw a novel connection between a family
of learning methods based on model fusion and generalisation certificates, and
surprisingly show that with minor adjustment several existing learning
strategies already provide non-trivial generalisation guarantees. Essentially,
by focusing on data-driven learning of downstream tasks by fusion rather than
fine-tuning, the certified generalisation gap becomes tiny and independent of
the base network size, facilitating its certification. Our results show for the
first time non-trivial generalisation guarantees for learning with as low as
100 examples, while using vision models such as VIT-B and language models such
as mistral-7B. This observation is significant as it has immediate implications
for facilitating the certification of existing systems as trustworthy, and
opens up new directions for research at the intersection of practice and
theory.

</details>


### [611] [A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation](https://arxiv.org/pdf/2505.15802)
*Sarah E. Wessinger, Leslie N. Smith, Jacob Gull, Jonathan Gehman, Zachary Beever, Andrew J. Kammerer*

Main category: cs.LG

TL;DR: Deep neural networks offer a faster alternative to traditional parabolic equation simulations for estimating pattern propagation factors in radar signal propagation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods are computationally expensive, limiting practical applications in radar technology deployment.

Method: Image-to-image translation generators using deep neural networks to predict pattern propagation factors from modified refractivity data.

Result: Deep neural networks can reasonably predict pattern propagation factors across multiple frequencies.

Conclusion: Deep learning provides a viable, efficient alternative to traditional simulations for radar signal propagation analysis.

Abstract: Accurately estimating the refractive environment over multiple frequencies
within the marine atmospheric boundary layer is crucial for the effective
deployment of radar technologies. Traditional parabolic equation simulations,
while effective, can be computationally expensive and time-intensive, limiting
their practical application. This communication explores a novel approach using
deep neural networks to estimate the pattern propagation factor, a critical
parameter for characterizing environmental impacts on signal propagation.
Image-to-image translation generators designed to ingest modified refractivity
data and generate predictions of pattern propagation factors over the same
domain were developed. Findings demonstrate that deep neural networks can be
trained to analyze multiple frequencies and reasonably predict the pattern
propagation factor, offering an alternative to traditional methods.

</details>


### [612] [Adaptive Estimation and Learning under Temporal Distribution Shift](https://arxiv.org/pdf/2505.15803)
*Dheeraj Baby, Yifei Tang, Hieu Duy Nguyen, Yu-Xiang Wang, Rohit Pyati*

Main category: cs.LG

TL;DR: The paper proposes a wavelet soft-thresholding estimator for optimal groundtruth estimation under temporal distribution shift, generalizing prior work and validating results with experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating groundtruth under temporal distribution shift without prior knowledge of shift levels.

Method: Wavelet soft-thresholding estimator, linking non-stationarity to wavelet-domain sparsity.

Result: Optimal estimation error bounds and validated theoretical findings via numerical experiments. Applications include sparsity-aware risk bounds and efficient training objectives.

Conclusion: The method generalizes prior research, connects to classical signal processing, and uncovers novel algorithms for total-variation denoising.

Abstract: In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
sparsity in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive sparsity-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.

</details>


### [613] [Neural Conditional Transport Maps](https://arxiv.org/pdf/2505.15808)
*Carlos Rodriguez-Pardo, Leonardo Chiani, Emanuele Borgonovo, Massimo Tavoni*

Main category: cs.LG

TL;DR: A neural framework for learning conditional optimal transport maps, using a hypernetwork to generate adaptive mappings, outperforming baselines and applied to global sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: To advance conditional optimal transport by enabling simultaneous processing of categorical and continuous variables, improving applications in generative modeling and explainability.

Method: Uses a hypernetwork to generate transport layer parameters based on conditioning variables, creating adaptive mappings.

Result: Outperforms baseline methods, demonstrated through ablation studies and applied successfully to global sensitivity analysis.

Conclusion: Advances conditional optimal transport, broadening its application to complex, high-dimensional domains.

Abstract: We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.

</details>


### [614] [On the creation of narrow AI: hierarchy and nonlocality of neural network skills](https://arxiv.org/pdf/2505.15811)
*Eric J. Michaud, Asher Parker-Sartori, Max Tegmark*

Main category: cs.LG

TL;DR: The paper explores challenges in creating narrow AI systems, focusing on training from scratch and skill transfer from large models.


<details>
  <summary>Details</summary>
Motivation: Efficiency and safety in AI can benefit from specialized narrow models, but challenges exist in training and transferring skills.

Method: Experiments on synthetic tasks and pruning-based methods to transfer skills from large to small models.

Result: Training on broad data accelerates learning hierarchical skills, and pruning outperforms distillation for skill transfer.

Conclusion: Narrow AI systems can be optimized through broad training and pruning, but skill localization remains a challenge.

Abstract: We study the problem of creating strong, yet narrow, AI systems. While recent
AI progress has been driven by the training of large general-purpose foundation
models, the creation of smaller models specialized for narrow domains could be
valuable for both efficiency and safety. In this work, we explore two
challenges involved in creating such systems, having to do with basic
properties of how neural networks learn and structure their representations.
The first challenge regards when it is possible to train narrow models from
scratch. Through experiments on a synthetic task, we find that it is sometimes
necessary to train networks on a wide distribution of data to learn certain
narrow skills within that distribution. This effect arises when skills depend
on each other hierarchically, and training on a broad distribution introduces a
curriculum which substantially accelerates learning. The second challenge
regards how to transfer particular skills from large general models into small
specialized models. We find that model skills are often not perfectly localized
to a particular set of prunable components. However, we find that methods based
on pruning can still outperform distillation. We investigate the use of a
regularization objective to align desired skills with prunable components while
unlearning unnecessary skills.

</details>


### [615] [Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex](https://arxiv.org/pdf/2505.15813)
*Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo*

Main category: cs.LG

TL;DR: BraInCoRL uses in-context learning with transformers to predict neural responses from few-shot examples, outperforming existing methods and generalizing well to new datasets.


<details>
  <summary>Details</summary>
Motivation: The need for large-scale fMRI datasets limits generalizability of visual cortex models. BraInCoRL addresses this by enabling predictions from few-shot examples without additional finetuning.

Method: A transformer architecture is used to condition on in-context image stimuli and voxel activations, optimizing for in-context learning during training.

Result: BraInCoRL outperforms existing methods in low-data regimes, generalizes to new datasets, and improves interpretability of neural signals.

Conclusion: BraInCoRL offers a scalable, interpretable, and generalizable solution for modeling higher visual cortex responses.

Abstract: Understanding functional representations within higher visual cortex is a
fundamental question in computational neuroscience. While artificial neural
networks pretrained on large-scale datasets exhibit striking representational
alignment with human neural responses, learning image-computable models of
visual cortex relies on individual-level, large-scale fMRI datasets. The
necessity for expensive, time-intensive, and often impractical data acquisition
limits the generalizability of encoders to new subjects and stimuli. BraInCoRL
uses in-context learning to predict voxelwise neural responses from few-shot
examples without any additional finetuning for novel subjects and stimuli. We
leverage a transformer architecture that can flexibly condition on a variable
number of in-context image stimuli, learning an inductive bias over multiple
subjects. During training, we explicitly optimize the model for in-context
learning. By jointly conditioning on image features and voxel activations, our
model learns to directly generate better performing voxelwise models of higher
visual cortex. We demonstrate that BraInCoRL consistently outperforms existing
voxelwise encoder designs in a low-data regime when evaluated on entirely novel
images, while also exhibiting strong test-time scaling behavior. The model also
generalizes to an entirely new visual fMRI dataset, which uses different
subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates
better interpretability of neural signals in higher visual cortex by attending
to semantically relevant stimuli. Finally, we show that our framework enables
interpretable mappings from natural language queries to voxel selectivity.

</details>


### [616] [HurriCast: Synthetic Tropical Cyclone Track Generation for Hurricane Forecasting](https://arxiv.org/pdf/2309.07174)
*Shouwei Gao, Meiyan Gao, Yuepeng Li, Wenqian Dong*

Main category: cs.LG

TL;DR: A hybrid method combining ARIMA, K-MEANS, and Autoencoder generates synthetic tropical cyclone tracks for risk assessment, aiding disaster preparedness and policy-making.


<details>
  <summary>Details</summary>
Motivation: To improve risk assessment and disaster preparedness by generating plausible synthetic TC tracks for insurance, governments, and policymakers.

Method: A hybrid approach using ARIMA, K-MEANS, and Autoencoder on historical HURDAT2 data to model TC behaviors and project future trajectories.

Result: The method efficiently captures historical TC patterns and provides reliable future projections for risk analysis.

Conclusion: The approach validates reliability and offers valuable insights for disaster management, insurance, and policy formulation.

Abstract: The generation of synthetic tropical cyclone(TC) tracks for risk assessment
is a critical application of preparedness for the impacts of climate change and
disaster relief, particularly in North America. Insurance companies use these
synthetic tracks to estimate the potential risks and financial impacts of
future TCs. For governments and policymakers, understanding the potential
impacts of TCs helps in developing effective emergency response strategies,
updating building codes, and prioritizing investments in resilience and
mitigation projects. In this study, many hypothetical but plausible TC
scenarios are created based on historical TC data HURDAT2 (HURricane DATA 2nd
generation). A hybrid methodology, combining the ARIMA and K-MEANS methods with
Autoencoder, is employed to capture better historical TC behaviors and project
future trajectories and intensities. It demonstrates an efficient and reliable
in the field of climate modeling and risk assessment. By effectively capturing
past hurricane patterns and providing detailed future projections, this
approach not only validates the reliability of this method but also offers
crucial insights for a range of applications, from disaster preparedness and
emergency management to insurance risk analysis and policy formulation.

</details>


### [617] [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/pdf/2402.12264)
*Oleksandr Balabanov, Hampus Linander*

Main category: cs.LG

TL;DR: The paper explores uncertainty quantification in fine-tuned LLMs using low-rank adaptation ensembles, analyzing knowledge retention and adaptation in fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand what fine-tuned models learn, forget, and how to trust their predictions, which is currently unclear.

Method: Uses low-rank adaptation ensembles for computationally efficient posterior approximations, tested on Mistral-7b with three multiple-choice datasets.

Result: Identifies unexpected knowledge retention during fine-tuning, even in overfitting scenarios, and assesses dataset complexity and adaptation balance.

Conclusion: Provides insights into fine-tuning dynamics, highlighting retained knowledge and effective uncertainty quantification.

Abstract: Fine-tuning large language models can improve task specific performance,
although a general understanding of what the fine-tuned model has learned,
forgotten and how to trust its predictions is still missing. We derive
principled uncertainty quantification for fine-tuned LLMs with posterior
approximations using computationally efficient low-rank adaptation ensembles.
We analyze three common multiple-choice datasets using low-rank adaptation
ensembles based on Mistral-7b, and draw quantitative and qualitative
conclusions on their perceived complexity and balance between retained prior
knowledge and domain specific adaptation during and after fine-tuning. We
identify unexpected retention of acquired knowledge during fine-tuning in the
overfitting regime.

</details>


### [618] [Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning](https://arxiv.org/pdf/2404.05894)
*Andrew Holliday, Ahmed El-Geneidy, Gregory Dudek*

Main category: cs.LG

TL;DR: Using deep reinforcement learning to train a graph neural net for heuristics in transit network planning improves results on benchmarks and real-world cases.


<details>
  <summary>Details</summary>
Motivation: Public transit route planning is complex; metaheuristic algorithms rely on heuristics, which impact result quality.

Method: Deep reinforcement learning trains a graph neural net to provide heuristics for an evolutionary algorithm.

Result: Improved performance on synthetic cities (70+ nodes) and Mumford benchmark; real-world Laval network saw 52% and 25% improvements, with 19% cost savings.

Conclusion: Neural heuristics enhance transit network planning, offering significant real-world benefits.

Abstract: Planning a network of public transit routes is a challenging optimization
problem. Metaheuristic algorithms search through the space of possible transit
networks by applying heuristics that randomly alter routes in a network. The
design of these heuristics has a major impact on the quality of the result. In
this paper, we use deep reinforcement learning to train a graph neural net to
provide heuristics for an evolutionary algorithm. These neural heuristics
improve the algorithm's results on benchmark synthetic cities with 70 nodes or
more, and achieve new state-of-the-art results on the challenging Mumford
benchmark. They also improve upon a simulation of the real transit network in
the city of Laval, Canada, by 52% and 25% on two key metrics, and offer cost
savings of up to 19% over the city's existing transit network.

</details>


### [619] [DPO: A Differential and Pointwise Control Approach to Reinforcement Learning](https://arxiv.org/pdf/2404.15617)
*Minh Nguyen, Chandrajit Bajaj*

Main category: cs.LG

TL;DR: Differential RL introduces a physics-embedded framework for RL in continuous spaces, improving efficiency and consistency via a differential dual formulation. DPO, its algorithm, outperforms standard RL with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing poor sample efficiency and lack of physical consistency in RL for continuous state-action spaces in scientific computing.

Method: Differential RL reformulates RL via a differential dual formulation, embedding physics priors. DPO, a pointwise algorithm, refines local movement operators for efficiency.

Result: Theoretical convergence guarantees and a regret bound of $O(K^{5/6})$. Empirically, DPO outperforms standard RL in tasks like surface modeling and molecular dynamics.

Conclusion: Differential RL and DPO offer a physics-consistent, efficient alternative to standard RL in scientific computing.

Abstract: Reinforcement learning (RL) in continuous state-action spaces remains
challenging in scientific computing due to poor sample efficiency and lack of
pathwise physical consistency. We introduce Differential Reinforcement Learning
(Differential RL), a novel framework that reformulates RL from a
continuous-time control perspective via a differential dual formulation. This
induces a Hamiltonian structure that embeds physics priors and ensures
consistent trajectories without requiring explicit constraints. To implement
Differential RL, we develop Differential Policy Optimization (DPO), a
pointwise, stage-wise algorithm that refines local movement operators along the
trajectory for improved sample efficiency and dynamic alignment. We establish
pointwise convergence guarantees, a property not available in standard RL, and
derive a competitive theoretical regret bound of $O(K^{5/6})$. Empirically, DPO
outperforms standard RL baselines on representative scientific computing tasks,
including surface modeling, grid control, and molecular dynamics, under
low-data and physics-constrained conditions.

</details>


### [620] [DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)
*Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, Liwei Wang*

Main category: cs.LG

TL;DR: The paper introduces Reinforced Token Optimization (RTO), a framework for RLHF that models token-wise rewards, outperforming PPO and other methods.


<details>
  <summary>Details</summary>
Motivation: Address sub-optimal PPO implementations in RLHF by capturing fine-grained token-level information for better alignment.

Method: Model RLHF as an MDP, learn token-wise rewards from preference data, and integrate DPO with PPO for policy optimization.

Result: RTO outperforms PPO by 7.5 points on AlpacaEval 2 and 4.1 points on Arena-Hard.

Conclusion: RTO is a sample-efficient, superior alternative to PPO for RLHF, with practical benefits demonstrated in benchmarks.

Abstract: In the classical Reinforcement Learning from Human Feedback (RLHF) framework,
Proximal Policy Optimization (PPO) is employed to learn from sparse,
sentence-level rewards -- a challenging scenario in traditional deep
reinforcement learning. Despite the great successes of PPO in the alignment of
large language models, its open-source implementation is still largely
sub-optimal. To address these issues, we introduce a framework that models RLHF
problems as a Markov decision process (MDP), enabling the capture of
fine-grained token-wise information. Under this framework, we introduce an
algorithm Reinforced Token Optimization (\texttt{RTO}), which learns the
token-wise reward function from preference data and performs policy
optimization based on this learned token-wise reward signal. Theoretically,
\texttt{RTO} is proven to have the capability of finding the near-optimal
policy sample-efficiently. For its practical implementation, \texttt{RTO}
innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,
originally derived from sparse sentence rewards, surprisingly provides us with
a token-wise characterization of response quality, which is seamlessly
incorporated into our subsequent PPO training stage. Extensive experiments
demonstrate that \texttt{RTO} performs better than PPO and other direct
preference learning algorithms. In particular, RTO outperforms PPO by 7.5
points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code
and models are available at
\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.

</details>


### [621] [Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing](https://arxiv.org/pdf/2405.11783)
*Shinyoung Kang, Jihan Kim*

Main category: cs.LG

TL;DR: The study uses quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties, achieving high classification accuracies and demonstrating potential for quantum computing in materials design.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of QNLP for designing MOFs with specific properties, leveraging quantum computing to navigate the complex MOF landscape.

Method: Analyzed 450 hypothetical MOF structures with 3 topologies, 10 metal nodes, and 15 ligands, categorizing them into four classes. Compared QNLP models (bag-of-words, DisCoCat, sequence-based) using IBM Qiskit simulator.

Result: Bag-of-words model performed best (88.6% and 78.0% validation accuracy for binary tasks). Multi-class models achieved 92% and 80% test accuracy for pore volume and CO2 Henry's constant, respectively. MOF generation accuracies were 93.5% and 87%.

Conclusion: The study demonstrates QNLP's promise for MOF design, offering a novel quantum computing approach despite limited MOF coverage.

Abstract: In this study, we explore the potential of using quantum natural language
processing (QNLP) to inverse design metal-organic frameworks (MOFs) with
targeted properties. Specifically, by analyzing 450 hypothetical MOF structures
consisting of 3 topologies, 10 metal nodes and 15 organic ligands, we
categorize these structures into four distinct classes for pore volume and
$CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the
bag-of-words, DisCoCat (Distributional Compositional Categorical), and
sequence-based models) to identify the most effective approach to process the
MOF dataset. Using a classical simulator provided by the IBM Qiskit, the
bag-of-words model is identified to be the optimum model, achieving validation
accuracies of 88.6% and 78.0% for binary classification tasks on pore volume
and $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class
classification models tailored to the probabilistic nature of quantum circuits,
with average test accuracies of 92% and 80% across different classes for pore
volume and $CO_{2}$ Henry's constant datasets. Finally, the performance of
generating MOF with target properties showed accuracies of 93.5% for pore
volume and 87% for $CO_{2}$ Henry's constant, respectively. Although our
investigation covers only a fraction of the vast MOF search space, it marks a
promising first step towards using quantum computing for materials design,
offering a new perspective through which to explore the complex landscape of
MOFs.

</details>


### [622] [Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception](https://arxiv.org/pdf/2407.11104)
*Phillip Mueller, Lars Mikelsons*

Main category: cs.LG

TL;DR: The paper explores why Deep Generative Models (DGMs) are not widely used in product design synthesis, analyzes various DGM families, and provides guidance for engineers to choose the best method.


<details>
  <summary>Details</summary>
Motivation: To understand the limited adoption of DGMs in product design and identify requirements for their successful integration.

Method: Systematic analysis of DGM families (VAE, GAN, Diffusion, Transformer, Radiance Field) to evaluate their suitability for product design.

Result: Insights into strengths, weaknesses, and applicability of DGMs for product design, aiding decision-making for engineers.

Conclusion: The paper offers a roadmap for leveraging DGMs in product design, addressing current challenges and proposing solutions.

Abstract: The synthesis of product design concepts stands at the crux of early-phase
development processes for technical products, traditionally posing an intricate
interdisciplinary challenge. The application of deep learning methods,
particularly Deep Generative Models (DGMs), holds the promise of automating and
streamlining manual iterations and therefore introducing heightened levels of
innovation and efficiency. However, DGMs have yet to be widely adopted into the
synthesis of product design concepts. This paper aims to explore the reasons
behind this limited application and derive the requirements for successful
integration of these technologies. We systematically analyze DGM-families (VAE,
GAN, Diffusion, Transformer, Radiance Field), assessing their strengths,
weaknesses, and general applicability for product design conception. Our
objective is to provide insights that simplify the decision-making process for
engineers, helping them determine which method might be most effective for
their specific challenges. Recognizing the rapid evolution of this field, we
hope that our analysis contributes to a fundamental understanding and guides
practitioners towards the most promising approaches. This work seeks not only
to illuminate current challenges but also to propose potential solutions,
thereby offering a clear roadmap for leveraging DGMs in the realm of product
design conception.

</details>


### [623] [Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions](https://arxiv.org/pdf/2210.09168)
*Frida Marie Viset, Rudy Helmons, Manon Kok*

Main category: cs.LG

TL;DR: A recursive Gaussian process (GP) mapping algorithm using local basis functions is proposed for scalable online mapping in unknown environments, reducing computational complexity in SLAM tasks.


<details>
  <summary>Details</summary>
Motivation: To address the increasing computational demands of GP mapping algorithms as mapped areas expand, especially in SLAM applications without GNSS signals.

Method: Uses local basis functions in an information filter for spatial scalability, restricting computations to localized subsets around prediction points.

Result: The algorithm reduces computational complexity and is faster than existing methods for large mapped areas with many measurements.

Conclusion: The proposed recursive GP mapping algorithm is scalable and efficient, making it suitable for integration into SLAM systems like magnetic field SLAM.

Abstract: When an agent, person, vehicle or robot is moving through an unknown
environment without GNSS signals, online mapping of nonlinear terrains can be
used to improve position estimates when the agent returns to a previously
mapped area. Mapping algorithms using online Gaussian process (GP) regression
are commonly integrated in algorithms for simultaneous localisation and mapping
(SLAM). However, GP mapping algorithms have increasing computational demands as
the mapped area expands relative to spatial field variations. This is due to
the need for estimating an increasing amount of map parameters as the area of
the map grows. Contrary to this, we propose a recursive GP mapping estimation
algorithm which uses local basis functions in an information filter to achieve
spatial scalability. Our proposed approximation employs a global grid of finite
support basis functions but restricts computations to a localized subset around
each prediction point. As our proposed algorithm is recursive, it can naturally
be incorporated into existing algorithms that uses Gaussian process maps for
SLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF)
for magnetic field SLAM reduces the overall computational complexity of the
algorithm. We show experimentally that our algorithm is faster than existing
methods when the mapped area is large and the map is based on many
measurements, both for recursive mapping tasks and for magnetic field SLAM.

</details>


### [624] [BiSSL: Enhancing the Alignment Between Self-Supervised Pretraining and Downstream Fine-Tuning via Bilevel Optimization](https://arxiv.org/pdf/2410.02387)
*Gustav Wagner Zakarias, Lars Kai Hansen, Zheng-Hua Tan*

Main category: cs.LG

TL;DR: BiSSL is a bilevel training framework improving alignment of self-supervised pretrained models with downstream tasks before fine-tuning, enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Self-supervised pretrained models often misalign with downstream tasks, limiting fine-tuning effectiveness.

Method: BiSSL introduces a bilevel optimization problem combining pretext and downstream objectives, acting as an intermediate training stage.

Result: BiSSL significantly improves accuracy on 12 downstream image classification datasets and object detection.

Conclusion: BiSSL enhances alignment between pretraining and fine-tuning, leading to better downstream task performance.

Abstract: Models initialized from self-supervised pretraining may suffer from poor
alignment with downstream tasks, reducing the extent to which subsequent
fine-tuning can adapt pretrained features toward downstream objectives. To
mitigate this, we introduce BiSSL, a novel bilevel training framework that
enhances the alignment of self-supervised pretrained models with downstream
tasks prior to fine-tuning. BiSSL acts as an intermediate training stage
conducted after conventional self-supervised pretraining and is tasked with
solving a bilevel optimization problem that incorporates the pretext and
downstream training objectives in its lower- and upper-level objectives,
respectively. This approach explicitly models the interdependence between the
pretraining and fine-tuning stages within the conventional self-supervised
learning pipeline, facilitating enhanced information sharing between them that
ultimately leads to a model initialization better aligned with the downstream
task. We propose a general training algorithm for BiSSL that is compatible with
a broad range of pretext and downstream tasks. Using SimCLR and Bootstrap Your
Own Latent to pretrain ResNet-50 backbones on the ImageNet dataset, we
demonstrate that our proposed framework significantly improves accuracy on the
vast majority of 12 downstream image classification datasets, as well as on
object detection. Exploratory analyses alongside investigative experiments
further provide compelling evidence that BiSSL enhances downstream alignment.

</details>


### [625] [An Empirical Bayes Analysis of Object Trajectory Representation Models](https://arxiv.org/pdf/2211.01696)
*Yue Yao, Daniel Goehring, Joerg Reichardt*

Main category: cs.LG

TL;DR: Linear trajectory models are effective for motion prediction in autonomous driving, balancing simplicity and accuracy.


<details>
  <summary>Details</summary>
Motivation: To analyze the expressive power and bias of linear models for real-world trajectories in autonomous driving.

Method: Empirical analysis of model complexity vs. fit error, using large-scale datasets to estimate noise and priors for regularization.

Result: Linear models accurately represent real-world trajectories with moderate complexity.

Conclusion: Linear trajectory models are feasible for future motion prediction systems due to their mathematical advantages.

Abstract: Linear trajectory models provide mathematical advantages to autonomous
driving applications such as motion prediction. However, linear models'
expressive power and bias for real-world trajectories have not been thoroughly
analyzed. We present an in-depth empirical analysis of the trade-off between
model complexity and fit error in modelling object trajectories. We analyze
vehicle, cyclist, and pedestrian trajectories. Our methodology estimates
observation noise and prior distributions over model parameters from several
large-scale datasets. Incorporating these priors can then regularize prediction
models. Our results show that linear models do represent real-world
trajectories with high fidelity at very moderate model complexity. This
suggests the feasibility of using linear trajectory models in future motion
prediction systems with inherent mathematical advantages.

</details>


### [626] [ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning](https://arxiv.org/pdf/2308.04964)
*Giuseppe Floris, Christian Scano, Biagio Montaruli, Luca Demetrio, Andrea Valenza, Luca Compagna, Davide Ariu, Luca Piras, Davide Balzarotti, Battista Biggio*

Main category: cs.LG

TL;DR: The paper proposes ModSec-AdvLearn, a machine learning-based approach to optimize OWASP CRS configurations for SQLi attack detection, improving detection rates and robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current manual configurations of OWASP CRS for WAFs are suboptimal in balancing detection and false alarm rates and lack robustness against adversarial SQLi attacks.

Method: Uses machine learning to automate rule selection and weight optimization, and adversarial training to enhance robustness.

Result: ModSec-AdvLearn increases detection rates by up to 30%, reduces false alarms, discards 50% of CRS rules, and improves robustness by 85%.

Conclusion: The approach significantly advances WAF effectiveness and robustness, with open-source code available for implementation.

Abstract: Many Web Application Firewalls (WAFs) leverage the OWASP CRS to block
incoming malicious requests. The CRS consists of different sets of rules
designed by domain experts to detect well-known web attack patterns. Both the
set of rules and the weights used to combine them are manually defined,
yielding four different default configurations of the CRS. In this work, we
focus on the detection of SQLi attacks, and show that the manual configurations
of the CRS typically yield a suboptimal trade-off between detection and false
alarm rates. Furthermore, we show that these configurations are not robust to
adversarial SQLi attacks, i.e., carefully-crafted attacks that iteratively
refine the malicious SQLi payload by querying the target WAF to bypass
detection. To overcome these limitations, we propose (i) using machine learning
to automate the selection of the set of rules to be combined along with their
weights, i.e., customizing the CRS configuration based on the monitored web
services; and (ii) leveraging adversarial training to significantly improve its
robustness to adversarial SQLi manipulations. Our experiments, conducted using
the well-known open-source ModSecurity WAF equipped with the CRS rules, show
that our approach, named ModSec-AdvLearn, can (i) increase the detection rate
up to 30%, while retaining negligible false alarm rates and discarding up to
50% of the CRS rules; and (ii) improve robustness against adversarial SQLi
attacks up to 85%, marking a significant stride toward designing more effective
and robust WAFs. We release our open-source code at
https://github.com/pralab/modsec-advlearn.

</details>


### [627] [EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions](https://arxiv.org/pdf/2410.03779)
*Huayu Deng, Xiangming Zhu, Yunbo Wang, Xiaokang Yang*

Main category: cs.LG

TL;DR: EvoMesh is a differentiable framework that learns adaptive graph hierarchies and physical dynamics, outperforming fixed-hierarchy methods in physical simulations.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical graph methods for physical simulation use fixed, manually designed structures, limiting adaptability to dynamic systems.

Method: EvoMesh introduces anisotropic message passing and learns node selection probabilities adaptively based on physical inputs.

Result: EvoMesh significantly outperforms fixed-hierarchy networks on five benchmark datasets.

Conclusion: EvoMesh enhances flexibility and long-range dependency capture in physical simulations, demonstrating superior performance.

Abstract: Graph neural networks have been a powerful tool for mesh-based physical
simulation. To efficiently model large-scale systems, existing methods mainly
employ hierarchical graph structures to capture multi-scale node relations.
However, these graph hierarchies are typically manually designed and fixed,
limiting their ability to adapt to the evolving dynamics of complex physical
systems. We propose EvoMesh, a fully differentiable framework that jointly
learns graph hierarchies and physical dynamics, adaptively guided by physical
inputs. EvoMesh introduces anisotropic message passing, which enables
direction-specific aggregation of dynamic features between nodes within each
hierarchy, while simultaneously learning node selection probabilities for the
next hierarchical level based on physical context. This design creates more
flexible message shortcuts and enhances the model's capacity to capture
long-range dependencies. Extensive experiments on five benchmark physical
simulation datasets show that EvoMesh outperforms recent fixed-hierarchy
message passing networks by large margins. The project page is available at
https://hbell99.github.io/evo-mesh/.

</details>


### [628] [Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference](https://arxiv.org/pdf/2403.04082)
*Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine*

Main category: cs.LG

TL;DR: The paper presents a method for probabilistic inference in high-dimensional time series data using contrastive learning, showing that learned representations follow a Gauss-Markov chain, enabling efficient prediction and planning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of answering probabilistic inference questions (e.g., future prediction, past analysis) in high-dimensional time series data.

Method: A variant of contrastive learning is applied to time series data, proving that the learned representations follow a Gaussian distribution and form a Gauss-Markov chain, simplifying inference tasks.

Result: Theoretical proofs and numerical simulations on tasks up to 46 dimensions validate that the method enables compact, closed-form solutions for inference.

Conclusion: Temporal contrastive learning provides a scalable and efficient framework for probabilistic inference in high-dimensional time series data.

Abstract: Given time series data, how can we answer questions like "what will happen in
the future?" and "how did we get here?" These sorts of probabilistic inference
questions are challenging when observations are high-dimensional. In this
paper, we show how these questions can have compact, closed form solutions in
terms of learned representations. The key idea is to apply a variant of
contrastive learning to time series data. Prior work already shows that the
representations learned by contrastive learning encode a probability ratio. By
extending prior work to show that the marginal distribution over
representations is Gaussian, we can then prove that joint distribution of
representations is also Gaussian. Taken together, these results show that
representations learned via temporal contrastive learning follow a Gauss-Markov
chain, a graphical model where inference (e.g., prediction, planning) over
representations corresponds to inverting a low-dimensional matrix. In one
special case, inferring intermediate representations will be equivalent to
interpolating between the learned representations. We validate our theory using
numerical simulations on tasks up to 46-dimensions.

</details>


### [629] [Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders](https://arxiv.org/pdf/2410.06981)
*Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez*

Main category: cs.LG

TL;DR: The paper explores the Analogous Feature Universality hypothesis, suggesting that SAE feature spaces in different LLMs are similar under rotation-invariant transformations, enabling cross-model interpretability techniques.


<details>
  <summary>Details</summary>
Motivation: To generalize mechanistic interpretability techniques across LLMs by validating the universality of SAE feature spaces.

Method: Pairs SAE features across models via activation correlation and measures spatial similarities using representational similarity measures.

Result: High similarities in SAE feature spaces across various LLMs, supporting the universality hypothesis.

Conclusion: Evidence for Analogous Feature Universality suggests interpretability techniques like steering vectors can be transferred across models via transformations.

Abstract: The Universality Hypothesis in large language models (LLMs) claims that
different models converge towards similar concept representations in their
latent spaces. Providing evidence for this hypothesis would enable researchers
to exploit universal properties, facilitating the generalization of mechanistic
interpretability techniques across models. Previous works studied if LLMs
learned the same features, which are internal representations that activate on
specific concepts. Since comparing features across LLMs is challenging due to
polysemanticity, in which LLM neurons often correspond to multiple unrelated
features rather than to distinct concepts, sparse autoencoders (SAEs) have been
employed to disentangle LLM neurons into SAE features corresponding to distinct
concepts. In this paper, we introduce a new variation of the universality
hypothesis called Analogous Feature Universality: we hypothesize that even if
SAEs across different models learn different feature representations, the
spaces spanned by SAE features are similar, such that one SAE space is similar
to another SAE space under rotation-invariant transformations. Evidence for
this hypothesis would imply that interpretability techniques related to latent
spaces, such as steering vectors, may be transferred across models via certain
transformations. To investigate this hypothesis, we first pair SAE features
across different models via activation correlation, and then measure spatial
relation similarities between paired features via representational similarity
measures, which transform spaces into representations that reveal hidden
relational similarities. Our experiments demonstrate high similarities for SAE
feature spaces across various LLMs, providing evidence for feature space
universality.

</details>


### [630] [Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability](https://arxiv.org/pdf/2403.09548)
*João Manoel Herrera Pinheiro, Marcelo Becker*

Main category: cs.LG

TL;DR: The study evaluates boosting algorithms (AdaBoost, XGBoost, CatBoost, LightGBM) for breast cancer prediction, focusing on recall, ROC-AUC, and reducing false negatives, achieving high AUC (>99.41%).


<details>
  <summary>Details</summary>
Motivation: Early detection of breast cancer is crucial, and while accuracy is often prioritized, recall is vital for minimizing false negatives in medical predictions.

Method: Used UCI dataset, applied boosting algorithms with Optuna for hyperparameter tuning and SHAP for model interpretability.

Result: Improved AUC and recall for all models, with final AUC exceeding 99.41% and reduced false negatives for AdaBoost and LightGBM.

Conclusion: Boosting algorithms, especially with hyperparameter optimization, are effective for breast cancer prediction, with high recall and AUC, supporting early diagnosis.

Abstract: Cancer is one of the diseases that kill the most women in the world, with
breast cancer being responsible for the highest number of cancer cases and
consequently deaths. However, it can be prevented by early detection and,
consequently, early treatment. Any development for detection or perdition this
kind of cancer is important for a better healthy life. Many studies focus on a
model with high accuracy in cancer prediction, but sometimes accuracy alone may
not always be a reliable metric. This study implies an investigative approach
to studying the performance of different machine learning algorithms based on
boosting to predict breast cancer focusing on the recall metric. Boosting
machine learning algorithms has been proven to be an effective tool for
detecting medical diseases. The dataset of the University of California, Irvine
(UCI) repository has been utilized to train and test the model classifier that
contains their attributes. The main objective of this study is to use
state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and
LightGBM to predict and diagnose breast cancer and to find the most effective
metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study
is the first to use these four boosting algorithms with Optuna, a library for
hyperparameter optimization, and the SHAP method to improve the
interpretability of our model, which can be used as a support to identify and
predict breast cancer. We were able to improve AUC or recall for all the models
and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more
than 99.41\% for all models.

</details>


### [631] [Parameter Efficient Fine-tuning via Explained Variance Adaptation](https://arxiv.org/pdf/2410.07170)
*Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter*

Main category: cs.LG

TL;DR: EVA is a new initialization scheme for LoRA that maximizes gradient signal and accelerates fine-tuning by using activation variance directions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA initialization strategies lack provable maximization of gradient signal, hindering fast adaptation.

Method: EVA uses incremental SVD on activation vectors to select directions with most variance, enabling adaptive ranks and fewer parameters.

Result: EVA achieves faster convergence, higher performance, and reduced trainable parameters across tasks like language, image, and reinforcement learning.

Conclusion: EVA is a superior initialization method for LoRA, improving fine-tuning efficiency and performance.

Abstract: Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned for a specific downstream task. The most common fine-tuning method
is to update pretrained weights via low-rank adaptation (LoRA). Existing
initialization strategies for LoRA often rely on singular value decompositions
(SVD) of gradients or weight matrices. However, they do not provably maximize
the expected gradient signal, which is critical for fast adaptation. To this
end, we introduce Explained Variance Adaptation (EVA), an initialization scheme
that uses the directions capturing the most activation variance, provably
maximizing the expected gradient signal and accelerating fine-tuning. EVA
performs incremental SVD on minibatches of activation vectors and selects the
right-singular vectors for initialization once they converged. Further, by
selecting the directions that capture the most activation-variance for a given
rank budget, EVA accommodates adaptive ranks that reduce the number of
trainable parameters, while maintaining or improving downstream performance. We
apply EVA to a variety of fine-tuning tasks as language generation and
understanding, image classification, and reinforcement learning. EVA exhibits
faster convergence than competitors and achieves the highest average score
across a multitude of tasks per domain while reducing the number of trainable
parameters through rank redistribution.

</details>


### [632] [Efficient Deep Learning with Decorrelated Backpropagation](https://arxiv.org/pdf/2405.02385)
*Sander Dalm, Joshua Offergeld, Nasir Ahmad, Marcel van Gerven*

Main category: cs.LG

TL;DR: Decorrelated backpropagation speeds up deep neural network training, achieving over two-fold speed-up and higher accuracy compared to traditional backpropagation.


<details>
  <summary>Details</summary>
Motivation: Training deep neural networks (DNNs) at scale is computationally expensive and has a high carbon footprint. Input decorrelation could improve efficiency, but challenges in enforcing stable decorrelation have limited its impact.

Method: A novel algorithm induces network-wide input decorrelation with minimal computational overhead, combined with optimizations for efficient training.

Result: The method achieves over two-fold speed-up and higher test accuracy when training deep networks, including a 50-layer ResNet model.

Conclusion: Decorrelated backpropagation offers promising prospects for efficient large-scale deep learning.

Abstract: The backpropagation algorithm remains the dominant and most successful method
for training deep neural networks (DNNs). At the same time, training DNNs at
scale comes at a significant computational cost and therefore a high carbon
footprint. Converging evidence suggests that input decorrelation may speed up
deep learning. However, to date, this has not yet translated into substantial
improvements in training efficiency in large-scale DNNs. This is mainly caused
by the challenge of enforcing fast and stable network-wide decorrelation. Here,
we show for the first time that much more efficient training of deep
convolutional neural networks is feasible by embracing decorrelated
backpropagation as a mechanism for learning. To achieve this goal we made use
of a novel algorithm which induces network-wide input decorrelation using
minimal computational overhead. By combining this algorithm with careful
optimizations, we achieve a more than two-fold speed-up and higher test
accuracy compared to backpropagation when training several deep networks up to
a 50-layer ResNet model. This demonstrates that decorrelation provides exciting
prospects for efficient deep learning at scale.

</details>


### [633] [Towards Real-world Debiasing: Rethinking Evaluation, Challenge, and Solution](https://arxiv.org/pdf/2405.15240)
*Peng Kuang, Zhibo Wang, Zhixuan Chu, Jingyi Wang, Kui Ren*

Main category: cs.LG

TL;DR: The paper critiques existing debiasing benchmarks for lacking realism in representing real-world biases and proposes a framework (RDBench) and method (DiD) to address this gap.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing benchmarks often fail to accurately reflect real-world biases, limiting their practical applicability.

Method: The authors propose a fine-grained framework to analyze biased distributions, introduce two novel real-world-inspired biases, and develop a debiasing method (DiD) for scenarios without bias labels.

Result: The proposed RDBench framework and DiD method are validated on 8 datasets, showing effectiveness in handling real-world biases.

Conclusion: The work highlights the need for realistic bias representation in benchmarks and offers practical solutions for debiasing in real-world settings.

Abstract: Spurious correlations in training data significantly hinder the
generalization capability of machine learning models when faced with
distribution shifts, leading to the proposition of numberous debiasing methods.
However, it remains to be asked: \textit{Do existing benchmarks for debiasing
really represent biases in the real world?} Recent works attempt to address
such concerns by sampling from real-world data (instead of synthesizing)
according to some predefined biased distributions to ensure the realism of
individual samples. However, the realism of the biased distribution is more
critical yet challenging and underexplored due to the complexity of real-world
bias distributions. To tackle the problem, we propose a fine-grained framework
for analyzing biased distributions, based on which we empirically and
theoretically identify key characteristics of biased distributions in the real
world that are poorly represented by existing benchmarks. Towards applicable
debiasing in the real world, we further introduce two novel real-world-inspired
biases to bridge this gap and build a systematic evaluation framework for
real-world debiasing, RDBench\footnote{RDBench: Code to be released.
Preliminary version in supplementary material for anonimized review.}.
Furthermore, focusing on the practical setting of debiasing w/o bias label, we
find real-world biases pose a novel \textit{Sparse bias capturing} challenge to
the existing paradigm. We propose a simple yet effective approach named Debias
in Destruction (DiD), to address the challenge, whose effectiveness is
validated with extensive experiments on 8 datasets of various biased
distributions.

</details>


### [634] [Unsupervised detection of semantic correlations in big data](https://arxiv.org/pdf/2411.02126)
*Santiago Acevedo, Alex Rodriguez, Alessandro Laio*

Main category: cs.LG

TL;DR: A method to detect semantic correlations in high-dimensional binary data by estimating its intrinsic dimension, applicable to big data analysis and tested on model systems and deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Real-world data often involves large, correlated feature vectors with semantic roles, which this work aims to quantify and analyze.

Method: Estimate the binary intrinsic dimension of datasets to measure semantic complexity, using an algorithm robust to high dimensionality.

Result: The method successfully identifies phase transitions in magnetic systems and detects semantic correlations in images and text within deep neural networks.

Conclusion: The proposed approach effectively quantifies semantic complexity in high-dimensional data, offering practical utility in big data analysis.

Abstract: In real-world data, information is stored in extremely large feature vectors.
These variables are typically correlated due to complex interactions involving
many features simultaneously. Such correlations qualitatively correspond to
semantic roles and are naturally recognized by both the human brain and
artificial neural networks. This recognition enables, for instance, the
prediction of missing parts of an image or text based on their context. We
present a method to detect these correlations in high-dimensional data
represented as binary numbers. We estimate the binary intrinsic dimension of a
dataset, which quantifies the minimum number of independent coordinates needed
to describe the data, and is therefore a proxy of semantic complexity. The
proposed algorithm is largely insensitive to the so-called curse of
dimensionality, and can therefore be used in big data analysis. We test this
approach identifying phase transitions in model magnetic systems and we then
apply it to the detection of semantic correlations of images and text inside
deep neural networks.

</details>


### [635] [A Trajectory-Based Bayesian Approach to Multi-Objective Hyperparameter Optimization with Epoch-Aware Trade-Offs](https://arxiv.org/pdf/2405.15303)
*Wenyu Wang, Zheyi Fan, Szu Hui Ng*

Main category: cs.LG

TL;DR: The paper proposes a multi-objective hyperparameter optimization method that treats training epochs as a decision variable, enabling early identification of trade-offs and improving tuning efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook trade-offs emerging early in training (e.g., overfitting), focusing only on final model performance.

Method: Introduces a trajectory-based multi-objective Bayesian optimization algorithm with a novel acquisition function and early stopping mechanism.

Result: Experiments show the algorithm effectively identifies trade-offs and enhances tuning efficiency.

Conclusion: The approach bridges the gap in utilizing iterative learning insights for better hyperparameter optimization.

Abstract: Training machine learning models inherently involves a resource-intensive and
noisy iterative learning procedure that allows epoch-wise monitoring of the
model performance. However, the insights gained from the iterative learning
procedure typically remain underutilized in multi-objective hyperparameter
optimization scenarios. Despite the limited research in this area, existing
methods commonly identify the trade-offs only at the end of model training,
overlooking the fact that trade-offs can emerge at earlier epochs in cases such
as overfitting. To bridge this gap, we propose an enhanced multi-objective
hyperparameter optimization problem that treats the number of training epochs
as a decision variable, rather than merely an auxiliary parameter, to account
for trade-offs at an earlier training stage. To solve this problem and
accommodate its iterative learning, we then present a trajectory-based
multi-objective Bayesian optimization algorithm characterized by two features:
1) a novel acquisition function that captures the improvement along the
predictive trajectory of model performances over epochs for any hyperparameter
setting and 2) a multi-objective early stopping mechanism that determines when
to terminate the training to maximize epoch efficiency. Experiments on
synthetic simulations and hyperparameter tuning benchmarks demonstrate that our
algorithm can effectively identify the desirable trade-offs while improving
tuning efficiency.

</details>


### [636] [Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers](https://arxiv.org/pdf/2411.15370)
*Gautham Vasan, Mohamed Elsayed, Alireza Azimi, Jiamin He, Fahim Shariar, Colin Bellinger, Martha White, A. Rupam Mahmood*

Main category: cs.LG

TL;DR: AVG is a novel incremental deep policy gradient method that addresses instability in incremental learning, achieving performance comparable to batch methods and enabling effective deep reinforcement learning on real robots.


<details>
  <summary>Details</summary>
Motivation: Existing deep policy gradient methods require large replay buffers or batch updates, making them unsuitable for resource-limited real systems. They fail catastrophically with small buffers or incremental learning.

Method: Proposes Action Value Gradient (AVG) with normalization and scaling techniques to stabilize incremental learning.

Result: AVG learns effectively in incremental settings, matching batch methods' performance in simulations and enabling successful real-robot applications.

Conclusion: AVG is a breakthrough for incremental deep reinforcement learning, making it viable for real-world robotic systems with limited resources.

Abstract: Modern deep policy gradient methods achieve effective performance on
simulated robotic tasks, but they all require large replay buffers or expensive
batch updates, or both, making them incompatible for real systems with
resource-limited computers. We show that these methods fail catastrophically
when limited to small replay buffers or during incremental learning, where
updates only use the most recent sample without batch updates or a replay
buffer. We propose a novel incremental deep policy gradient method -- Action
Value Gradient (AVG) and a set of normalization and scaling techniques to
address the challenges of instability in incremental learning. On robotic
simulation benchmarks, we show that AVG is the only incremental method that
learns effectively, often achieving final performance comparable to batch
policy gradient methods. This advancement enabled us to show for the first time
effective deep reinforcement learning with real robots using only incremental
updates, employing a robotic manipulator and a mobile robot.

</details>


### [637] [DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform](https://arxiv.org/pdf/2406.01781)
*Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio*

Main category: cs.LG

TL;DR: The paper introduces DEFT, a method for conditional generation using Doob's h-transform, unifying existing approaches and achieving faster, state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unifying framework for conditional sampling in diffusion models and issues like hyperparameter sensitivity and high training costs.

Method: Uses Doob's h-transform to unify methods and proposes DEFT, which fine-tunes a small network for conditional generation while keeping the unconditional model fixed.

Result: DEFT achieves 1.6× speedups, best perceptual quality on images, and outperforms baselines in reconstruction and protein motif scaffolding.

Conclusion: DEFT provides an efficient, unified approach for conditional generation with superior performance and speed.

Abstract: Generative modelling paradigms based on denoising diffusion processes have
emerged as a leading candidate for conditional sampling in inverse problems. In
many real-world applications, we often have access to large, expensively
trained unconditional diffusion models, which we aim to exploit for improving
conditional sampling. Most recent approaches are motivated heuristically and
lack a unifying framework, obscuring connections between them. Further, they
often suffer from issues such as being very sensitive to hyperparameters, being
expensive to train or needing access to weights hidden behind a closed API. In
this work, we unify conditional training and sampling using the mathematically
well-understood Doob's h-transform. This new perspective allows us to unify
many existing methods under a common umbrella. Under this framework, we propose
DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional
generation that simply fine-tunes a very small network to quickly learn the
conditional $h$-transform, while keeping the larger unconditional network
unchanged. DEFT is much faster than existing baselines while achieving
state-of-the-art performance across a variety of linear and non-linear
benchmarks. On image reconstruction tasks, we achieve speedups of up to
1.6$\times$, while having the best perceptual quality on natural images and
reconstruction performance on medical images. Further, we also provide initial
experiments on protein motif scaffolding and outperform reconstruction guidance
methods.

</details>


### [638] [Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks](https://arxiv.org/pdf/2406.09262)
*Spencer Young, Porter Jenkins, Longchao Da, Jeff Dotson, Hua Wei*

Main category: cs.LG

TL;DR: DDPN is proposed for flexible discrete predictive distributions, outperforming existing models in accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discrete regression tasks produce flawed predictive distributions or lack flexibility, especially for complex inputs.

Method: Introduces Deep Double Poisson Network (DDPN) and a training technique to balance mean fit and probabilistic calibration.

Result: DDPN outperforms existing discrete models, matches Gaussian NLL networks, and excels in out-of-distribution detection.

Conclusion: DDPN is versatile and effective for various count regression tasks, offering superior performance and flexibility.

Abstract: Neural networks that can produce accurate, input-conditional uncertainty
representations are critical for real-world applications. Recent progress on
heteroscedastic continuous regression has shown great promise for calibrated
uncertainty quantification on complex tasks, like image regression. However,
when these methods are applied to discrete regression tasks, such as crowd
counting, ratings prediction, or inventory estimation, they tend to produce
predictive distributions with numerous pathologies. Moreover, discrete models
based on the Generalized Linear Model (GLM) framework either cannot process
complex input or are not fully heterosedastic. To address these issues we
propose the Deep Double Poisson Network (DDPN). In contrast to networks trained
to minimize Gaussian negative log likelihood (NLL), discrete network
parameterizations (i.e., Poisson, Negative binomial), and GLMs, DDPN can
produce discrete predictive distributions of arbitrary flexibility.
Additionally, we propose a technique to tune the prioritization of mean fit and
probabilistic calibration during training. We show DDPN 1) vastly outperforms
existing discrete models; 2) meets or exceeds the accuracy and flexibility of
networks trained with Gaussian NLL; 3) produces proper predictive distributions
over discrete counts; and 4) exhibits superior out-of-distribution detection.
DDPN can easily be applied to a variety of count regression datasets including
tabular, image, point cloud, and text data.

</details>


### [639] [Hitchhiker's guide on the relation of Energy-Based Models with other generative models, sampling and statistical physics: a comprehensive review](https://arxiv.org/pdf/2406.13661)
*Davide Carbone*

Main category: cs.LG

TL;DR: A review explaining Energy-Based Models (EBMs) for physicists, connecting them to other generative models, sampling techniques, and statistical mechanics concepts.


<details>
  <summary>Details</summary>
Motivation: To clarify the complex interconnections between EBMs and other generative models for physicists, leveraging statistical mechanics principles.

Method: Explores EBMs' connections to other models, sampling techniques like MCMC, and recent training advancements.

Result: Provides a comprehensive understanding of EBMs, their parallels with statistical mechanics, and improved training methodologies.

Conclusion: EBMs offer a powerful framework for generative modeling, with strong ties to physics, and recent advancements enhance their performance.

Abstract: Energy-Based Models have emerged as a powerful framework in the realm of
generative modeling, offering a unique perspective that aligns closely with
principles of statistical mechanics. This review aims to provide physicists
with a comprehensive understanding of EBMs, delineating their connection to
other generative models such as Generative Adversarial Networks, Variational
Autoencoders, and Normalizing Flows. We explore the sampling techniques crucial
for EBMs, including Markov Chain Monte Carlo (MCMC) methods, and draw parallels
between EBM concepts and statistical mechanics, highlighting the significance
of energy functions and partition functions. Furthermore, we delve into recent
training methodologies for EBMs, covering recent advancements and their
implications for enhanced model performance and efficiency. This review is
designed to clarify the often complex interconnections between these models,
which can be challenging due to the diverse communities working on the topic.

</details>


### [640] [Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies](https://arxiv.org/pdf/2412.18296)
*Qi Liu, Wanjing Ma*

Main category: cs.LG

TL;DR: The study examines the impact of data corruption (missing/noisy data) on ML models, evaluates mitigation strategies (imputation, dataset enlargement), and identifies performance patterns and trade-offs.


<details>
  <summary>Details</summary>
Motivation: Data corruption is a major challenge in real-world ML, affecting model performance. This study aims to understand its effects and explore solutions.

Method: Two experimental setups: NLP-SL (supervised learning for NLP tasks) and Signal-RL (deep RL for traffic signal optimization). Analyzed corruption levels, imputation methods, and dataset enlargement.

Result: Performance follows a diminishing return curve. Noisy data is worse than missing data. Imputation has trade-offs, and dataset enlargement helps but doesn't fully resolve corruption. 30% of data is critical for performance.

Conclusion: Findings guide robust ML system development, emphasizing preprocessing, imputation strategies, and data collection practices in noisy environments.

Abstract: Data corruption, including missing and noisy data, poses significant
challenges in real-world machine learning. This study investigates the effects
of data corruption on model performance and explores strategies to mitigate
these effects through two experimental setups: supervised learning with NLP
tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization
(Signal-RL). We analyze the relationship between data corruption levels and
model performance, evaluate the effectiveness of data imputation methods, and
assess the utility of enlarging datasets to address data corruption.
  Our results show that model performance under data corruption follows a
diminishing return curve, modeled by the exponential function. Missing data,
while detrimental, is less harmful than noisy data, which causes severe
performance degradation and training instability, particularly in sequential
decision-making tasks like Signal-RL. Imputation strategies involve a
trade-off: they recover missing information but may introduce noise. Their
effectiveness depends on imputation accuracy and corruption ratio. We identify
distinct regions in the imputation advantage heatmap, including an "imputation
advantageous corner" and an "imputation disadvantageous edge" and classify
tasks as "noise-sensitive" or "noise-insensitive" based on their decision
boundaries.
  Furthermore, we find that increasing dataset size mitigates but cannot fully
overcome the effects of data corruption. The marginal utility of additional
data diminishes as corruption increases. An empirical rule emerges:
approximately 30% of the data is critical for determining performance, while
the remaining 70% has minimal impact.
  These findings provide actionable insights into data preprocessing,
imputation strategies, and data collection practices, guiding the development
of robust machine learning systems in noisy environments.

</details>


### [641] [Parameter-Efficient Fine-Tuning via Circular Convolution](https://arxiv.org/pdf/2407.19342)
*Aochuan Chen, Jiashun Cheng, Zijing Liu, Ziqi Gao, Fugee Tsung, Yu Li, Jia Li*

Main category: cs.LG

TL;DR: C³A outperforms LoRA in fine-tuning tasks by achieving high-rank adaptation while maintaining computational and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: LoRA's low-rank characteristic may limit performance despite its efficiency. C³A aims to enhance performance without sacrificing efficiency.

Method: Proposes Circular Convolution Adaptation (C³A) for high-rank adaptation, improving performance while retaining computational and memory efficiency.

Result: C³A consistently outperforms LoRA and its variants in various fine-tuning tasks.

Conclusion: C³A is a superior alternative to LoRA, offering better performance and efficiency.

Abstract: Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large
foundation models, leveraging low-rank matrices $\mathbf{A}$ and $\mathbf{B}$
to represent weight changes (i.e., $\Delta \mathbf{W} = \mathbf{B}
\mathbf{A}$). This method reduces trainable parameters and mitigates heavy
memory consumption associated with full delta matrices by sequentially
multiplying $\mathbf{A}$ and $\mathbf{B}$ with the activation. Despite its
success, the intrinsic low-rank characteristic may limit its performance.
Although several variants have been proposed to address this issue, they often
overlook the crucial computational and memory efficiency brought by LoRA. In
this paper, we propose Circular Convolution Adaptation (C$^3$A), which not only
achieves high-rank adaptation with enhanced performance but also excels in both
computational power and memory utilization. Extensive experiments demonstrate
that C$^3$A consistently outperforms LoRA and its variants across various
fine-tuning tasks.

</details>


### [642] [Predictive Learning in Energy-based Models with Attractor Structures](https://arxiv.org/pdf/2501.13997)
*Xingsi Dong, Xiangyuan Peng, Si Wu*

Main category: cs.LG

TL;DR: The paper introduces an energy-based model (EBM) to explain neural prediction, learning, and inference, demonstrating efficacy in diverse scenarios and matching machine learning performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in biologically plausible models for neural prediction, leveraging advances in machine learning.

Method: Uses a hierarchical EBM with a continuous attractor neural network for memory, tested in scenarios like eye movement and environmental changes.

Result: The model accurately predicts trained and unseen environments, performing comparably to machine learning methods.

Conclusion: The study advances understanding of neural prediction, offering a biologically plausible framework.

Abstract: Predictive models are highly advanced in understanding the mechanisms of
brain function. Recent advances in machine learning further underscore the
power of prediction for optimal representation in learning. However, there
remains a gap in creating a biologically plausible model that explains how the
neural system achieves prediction. In this paper, we introduce a framework that
employs an energy-based model (EBM) to capture the nuanced processes of
predicting observation after action within the neural system, encompassing
prediction, learning, and inference. We implement the EBM with a hierarchical
structure and integrate a continuous attractor neural network for memory,
constructing a biologically plausible model. In experimental evaluations, our
model demonstrates efficacy across diverse scenarios. The range of actions
includes eye movement, motion in environments, head turning, and static
observation while the environment changes. Our model not only makes accurate
predictions for environments it was trained on, but also provides reasonable
predictions for unseen environments, matching the performances of machine
learning methods in multiple tasks. We hope that this study contributes to a
deep understanding of how the neural system performs prediction.

</details>


### [643] [Neural Entropy](https://arxiv.org/pdf/2409.03817)
*Akhil Premkumar*

Main category: cs.LG

TL;DR: The paper explores the link between deep learning and information theory using diffusion models, introducing 'neural entropy' to quantify information efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand how diffusion models store and restore information during training, bridging deep learning and information theory.

Method: Introduces 'neural entropy,' a measure related to the entropy produced by diffusion, and tests it on simple image diffusion models.

Result: Diffusion models are highly efficient at compressing structured data, as revealed by neural entropy measurements.

Conclusion: Neural entropy provides a valuable metric for analyzing the efficiency of diffusion models in information storage and restoration.

Abstract: We explore the connection between deep learning and information theory
through the paradigm of diffusion models. A diffusion model converts noise into
structured data by reinstating, imperfectly, information that is erased when
data was diffused to noise. This information is stored in a neural network
during training. We quantify this information by introducing a measure called
neural entropy, which is related to the total entropy produced by diffusion.
Neural entropy is a function of not just the data distribution, but also the
diffusive process itself. Measurements of neural entropy on a few simple image
diffusion models reveal that they are extremely efficient at compressing large
ensembles of structured data.

</details>


### [644] [Automated Data Augmentation for Few-Shot Time Series Forecasting: A Reinforcement Learning Approach Guided by a Model Zoo](https://arxiv.org/pdf/2409.06282)
*Haochen Yuan, Yutong Wang, Yihong Chen, Yunbo Wang, Xiaokang Yang*

Main category: cs.LG

TL;DR: ReAugment uses RL for time series data augmentation, targeting overfit-prone samples to enhance diversity and improve forecasting in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited high-quality training data in few-shot time series forecasting.

Method: Maintains a model zoo to identify overfit-prone samples, then uses RL to adaptively augment these samples.

Result: Validated effectiveness across base models, improving performance in standard and few-shot forecasting tasks.

Conclusion: ReAugment successfully leverages RL for targeted data augmentation, enhancing model robustness and diversity.

Abstract: Time series forecasting, particularly in few-shot learning scenarios, is
challenging due to the limited availability of high-quality training data. To
address this, we present a pilot study on using reinforcement learning (RL) for
time series data augmentation. Our method, ReAugment, tackles three critical
questions: which parts of the training set should be augmented, how the
augmentation should be performed, and what advantages RL brings to the process.
Specifically, our approach maintains a forecasting model zoo, and by measuring
prediction diversity across the models, we identify samples with higher
probabilities for overfitting and use them as the anchor points for
augmentation. Leveraging RL, our method adaptively transforms the overfit-prone
samples into new data that not only enhances training set diversity but also
directs the augmented data to target regions where the forecasting models are
prone to overfitting. We validate the effectiveness of ReAugment across a wide
range of base models, showing its advantages in both standard time series
forecasting and few-shot learning tasks.

</details>


### [645] [Distributed Conformal Prediction via Message Passing](https://arxiv.org/pdf/2501.14544)
*Haifeng Wen, Hong Xing, Osvaldo Simeone*

Main category: cs.LG

TL;DR: The paper proposes two decentralized methods, Q-DCP and H-DCP, for post-hoc calibration of pre-trained models using conformal prediction in settings with limited local data and communication constraints.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliable inference in safety-critical domains like healthcare, especially in decentralized settings with limited calibration data and communication.

Method: Q-DCP uses distributed quantile regression with smoothing and regularization, while H-DCP employs consensus-based histogram estimation.

Result: Experiments explore trade-offs in hyperparameter tuning, communication, coverage guarantees, and prediction set sizes across network topologies.

Conclusion: The proposed methods enable reliable decentralized conformal prediction, with code available for further use.

Abstract: Post-hoc calibration of pre-trained models is critical for ensuring reliable
inference, especially in safety-critical domains such as healthcare. Conformal
Prediction (CP) offers a robust post-hoc calibration framework, providing
distribution-free statistical coverage guarantees for prediction sets by
leveraging held-out datasets. In this work, we address a decentralized setting
where each device has limited calibration data and can communicate only with
its neighbors over an arbitrary graph topology. We propose two
message-passing-based approaches for achieving reliable inference via CP:
quantile-based distributed conformal prediction (Q-DCP) and histogram-based
distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile
regression enhanced with tailored smoothing and regularization terms to
accelerate convergence, while H-DCP uses a consensus-based histogram estimation
approach. Through extensive experiments, we investigate the trade-offs between
hyperparameter tuning requirements, communication overhead, coverage
guarantees, and prediction set sizes across different network topologies. The
code of our work is released on:
https://github.com/HaifengWen/Distributed-Conformal-Prediction.

</details>


### [646] [FedGraph: A Research Library and Benchmark for Federated Graph Learning](https://arxiv.org/pdf/2410.06340)
*Yuhang Yao, Yuan Li, Xinyi Fan, Junhao Li, Kay Liu, Weizhao Jin, Yu Yang, Srivatsan Ravi, Philip S. Yu, Carlee Joe-Wong*

Main category: cs.LG

TL;DR: FedGraph is a federated graph learning library addressing system performance and privacy, supporting encrypted low-rank communication and scalability for large graphs.


<details>
  <summary>Details</summary>
Motivation: Existing federated graph learning (FGL) algorithms focus on accuracy but neglect system performance, which is critical for real-world deployment.

Method: FedGraph integrates homomorphic encryption, supports scalable deployment, and introduces a low-rank communication scheme for efficiency.

Result: FedGraph benchmarks FGL algorithms on three tasks, demonstrating scalability to 100M-node graphs and efficient encrypted communication.

Conclusion: FedGraph is the first efficient FGL framework to combine performance evaluation, privacy, and scalability, setting a benchmark for future FGL system design.

Abstract: Federated graph learning is an emerging field with significant practical
challenges. While algorithms have been proposed to improve the accuracy of
training graph neural networks, such as node classification on federated
graphs, the system performance is often overlooked, despite it is crucial for
real-world deployment. To bridge this gap, we introduce FedGraph, a research
library designed for practical distributed training and comprehensive
benchmarking of FGL algorithms. FedGraph supports a range of state-of-the-art
graph learning methods and includes a monitoring class that evaluates system
performance, with a particular focus on communication and computation costs
during training. Unlike existing federated learning platforms, FedGraph
natively integrates homomorphic encryption to enhance privacy preservation and
supports scalable deployment across multiple physical machines with
system-level performance evaluation to guide the system design of future
algorithms. To enhance efficiency and privacy, we propose a low-rank
communication scheme for algorithms like FedGCN that require pre-training
communication, accelerating both the pre-training and training phases.
Extensive experiments benchmark different FGL algorithms on three major graph
learning tasks and demonstrate FedGraph as the first efficient FGL framework to
support encrypted low-rank communication and scale to graphs with 100 million
nodes.

</details>


### [647] [Adaptive Width Neural Networks](https://arxiv.org/pdf/2501.15889)
*Federico Errica, Henrik Christiansen, Viktor Zaverkin, Mathias Niepert, Francesco Alesiani*

Main category: cs.LG

TL;DR: The paper introduces a technique to dynamically learn the width of neural network layers during training, eliminating the need for hyper-parameter tuning. It jointly optimizes width and parameters via backpropagation, adapting to task difficulty and enabling efficient network truncation or compression.


<details>
  <summary>Details</summary>
Motivation: Challenges the reliance on hyper-parameter tuning for neural network width selection, especially in large-scale models where tuning is impractical.

Method: Proposes a technique that jointly optimizes layer width and parameters using simple backpropagation, without alternate optimization or gradient heuristics.

Result: Demonstrates adaptability across various data domains, enabling dynamic network truncation or compression with minimal performance loss.

Conclusion: Offers a scalable alternative for width learning in large models, addressing inefficiencies of hyper-parameter tuning.

Abstract: For almost 70 years, researchers have mostly relied on hyper-parameter tuning
to select the width of neural networks' layers. This paper challenges the
status quo by introducing an easy-to-use technique to learn an unbounded width
of a neural network's layer during training. The technique does not rely on
alternate optimization nor hand-crafted gradient heuristics; rather, it jointly
optimizes the width and the parameters of each layer via simple
backpropagation. We apply the technique to a broad range of data domains such
as tables, images, text, sequences, and graphs, showing how the width adapts to
the task's difficulty. The method imposes a soft ordering of importance among
neurons, by which it also is possible to truncate the trained network at
virtually zero cost, achieving a smooth trade-off between performance and
compute resources in a structured way. Alternatively, one can dynamically
compress the network with no performance degradation. In light of recent
foundation models trained on large datasets, believed to require billions of
parameters and where hyper-parameter tuning is unfeasible due to humongous
training costs, our approach stands as a viable alternative for width learning.

</details>


### [648] [Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation](https://arxiv.org/pdf/2411.02001)
*Satoki Ishikawa, Rio Yokota, Ryo Karakida*

Main category: cs.LG

TL;DR: The paper introduces the maximal update parameterization ($\mu$P) for local learning methods like predictive coding (PC) and target propagation (TP), enabling hyperparameter transfer across models and revealing unique properties compared to backpropagation.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and instability of local learning algorithms, which often require additional hyperparameters due to their locality.

Method: Introduces $\mu$P in the infinite-width limit for PC and TP, analyzing deep linear networks to study gradient behaviors.

Result: $\mu$P allows hyperparameter transfer and reveals PC's gradients interpolate between first-order and Gauss-Newton-like gradients, while TP favors feature learning.

Conclusion: $\mu$P provides theoretical insights into local learning, showing its potential advantages over backpropagation in certain settings.

Abstract: Local learning, which trains a network through layer-wise local targets and
losses, has been studied as an alternative to backpropagation (BP) in neural
computation. However, its algorithms often become more complex or require
additional hyperparameters because of the locality, making it challenging to
identify desirable settings in which the algorithm progresses in a stable
manner. To provide theoretical and quantitative insights, we introduce the
maximal update parameterization ($\mu$P) in the infinite-width limit for two
representative designs of local targets: predictive coding (PC) and target
propagation (TP). We verified that $\mu$P enables hyperparameter transfer
across models of different widths. Furthermore, our analysis revealed unique
and intriguing properties of $\mu$P that are not present in conventional BP. By
analyzing deep linear networks, we found that PC's gradients interpolate
between first-order and Gauss-Newton-like gradients, depending on the
parameterization. We demonstrate that, in specific standard settings, PC in the
infinite-width limit behaves more similarly to the first-order gradient. For
TP, even with the standard scaling of the last layer, which differs from
classical $\mu$P, its local loss optimization favors the feature learning
regime over the kernel regime.

</details>


### [649] [Diffusion-based Method for Satellite Pattern-of-Life Identification](https://arxiv.org/pdf/2412.10814)
*Yongchao Ye, Xinting Zhu, Xuejin Shen, Xiaoyu Chen, Lishuai Li, S. Joe Qin*

Main category: cs.LG

TL;DR: A diffusion-based method for satellite pattern-of-life (PoL) identification is proposed, overcoming limitations of prior domain expertise-dependent methods by achieving high accuracy and robustness across varying sampling rates.


<details>
  <summary>Details</summary>
Motivation: Existing PoL identification methods are limited by dependency on domain expertise and sensitivity to sampling rate variations, necessitating a more generalizable solution.

Method: The proposed method uses a diffusion model with a multivariate time-series encoder for end-to-end PoL identification, eliminating the need for manual refinement or domain-specific knowledge.

Result: The diffusion-based method achieves high identification quality and robustness, even with reduced sampling rates, outperforming prior approaches.

Conclusion: The diffusion-based PoL identification method offers a practical, generalizable solution for satellite behavior monitoring, with potential for broader mission deployment.

Abstract: Satellite pattern-of-life (PoL) identification is crucial for space safety
and satellite monitoring, involving the analysis of typical satellite behaviors
such as station-keeping, drift, etc. However, existing PoL identification
methods remain underdeveloped due to the complexity of aerospace systems,
variability in satellite behaviors, and fluctuating observation sampling rates.
In a first attempt, we developed a domain expertise-informed machine learning
method (Expert-ML) to combine satellite orbital movement knowledge and machine
learning models. The Expert-ML method achieved high accuracy results in
simulation data and real-world data with normal sampling rate. However, this
approach lacks of generality as it requires domain expertise and its
performance degraded significantly when data sampling rate varied. To achieve
generality, we propose a novel diffusion-based PoL identification method.
Distinct from prior approaches, the proposed method leverages a diffusion model
to achieve end-to-end identification without manual refinement or
domain-specific knowledge. Specifically, we employ a multivariate time-series
encoder to capture hidden representations of satellite positional data. The
encoded features are subsequently incorporated as conditional information in
the denoising process to generate PoL labels. Through experimentation across
real-world satellite settings, our proposed diffusion-based method demonstrates
its high identification quality and provides a robust solution even with
reduced data sampling rates, indicating its great potential in practical
satellite behavior pattern identification, tracking and related mission
deployment.

</details>


### [650] [How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence](https://arxiv.org/pdf/2502.00678)
*Hyeong Kyu Choi, Maxim Khanov, Hongxin Wei, Yixuan Li*

Main category: cs.LG

TL;DR: The paper introduces Kernel Divergence Score (KDS) to measure dataset contamination, ensuring reliable model evaluations by comparing kernel similarity matrices before and after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Dataset contamination inflates performance metrics, undermining model evaluation reliability. A method to measure contamination is needed to ensure evaluations reflect generalization, not memorization.

Method: Proposes KDS, which computes divergence between kernel similarity matrices of sample embeddings before and after fine-tuning, leveraging the differential impact of fine-tuning on seen vs. unseen samples.

Result: KDS shows near-perfect correlation with contamination levels and outperforms baselines in controlled experiments. Ablation studies confirm its reliability and design choices.

Conclusion: KDS effectively measures dataset contamination, providing a robust framework for reliable model evaluation across diverse datasets.

Abstract: Dataset contamination, where evaluation datasets overlap with pre-training
corpora, inflates performance metrics and undermines the reliability of model
evaluations. Measuring dataset contamination thus becomes essential to ensure
that performance evaluations genuinely reflect a model's ability to generalize
to unseen data, rather than relying on memorized examples. To address this
problem, we propose Kernel Divergence Score (KDS), a novel method that
evaluates dataset contamination by computing the divergence between the kernel
similarity matrix of sample embeddings, before and after fine-tuning on the
benchmark dataset. Leveraging the insight that fine-tuning affects unseen
samples more significantly than seen ones, KDS provides a reliable measure of
contamination. Through extensive experiments on controlled contamination
scenarios, KDS demonstrates a near-perfect correlation with contamination
levels and outperforms existing baselines. Additionally, we perform
comprehensive ablation studies to analyze the impact of key design choices,
providing deeper insights into the components and effectiveness of KDS. These
ablations highlight the importance of leveraging fine-grained kernel-based
information and confirm the reliability of the proposed framework across
diverse datasets and settings. Code is released in
https://github.com/deeplearning-wisc/kernel-divergence-score.

</details>


### [651] [Uncertainty quantification for improving radiomic-based models in radiation pneumonitis prediction](https://arxiv.org/pdf/2412.19511)
*Chanon Puttanawarut, Romen Samuel Wabina, Nat Sirirutbunkajorn*

Main category: cs.LG

TL;DR: Machine learning models with radiomic and dosiomic features, enhanced by uncertainty quantification methods, improve radiation pneumonitis prediction, supporting clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: To enhance predictive accuracy and calibration of machine learning models for radiation pneumonitis by integrating uncertainty quantification methods.

Method: Evaluated four models (logistic regression, SVM, XGBoost, random forest) using dosimetric, dosiomic, and radiomic features, applying uncertainty quantification methods like Platt scaling and conformal prediction. Performance assessed via AUROC, AUPRC, and calibration error.

Result: Logistic regression with conformal prediction achieved highest AUROC (0.75), while XGBoost with conformal prediction had highest AUPRC (0.82). Radiomic and dosiomic features improved performance.

Conclusion: Uncertainty quantification enhances model reliability for radiation pneumonitis prediction, aiding clinical decisions.

Abstract: Background: Radiation pneumonitis is a side effect of thoracic radiation
therapy. Recently, machine learning models with radiomic features have improved
radiation pneumonitis prediction by capturing spatial information. To further
support clinical decision-making, this study explores the role of post hoc
uncertainty quantification methods in enhancing model uncertainty estimate.
Methods: We retrospectively analyzed a cohort of 101 esophageal cancer
patients. This study evaluated four machine learning models: logistic
regression, support vector machines, extreme gradient boosting, and random
forest, using 15 dosimetric, 79 dosiomic, and 237 radiomic features to predict
radiation pneumonitis. We applied uncertainty quantification methods, including
Platt scaling, isotonic regression, Venn-ABERS predictor, and conformal
prediction, to quantify uncertainty. Model performance was assessed through an
area under the receiver operating characteristic curve (AUROC), area under the
precision-recall curve (AUPRC), and adaptive calibration error using
leave-one-out cross-validation. Results: Highest AUROC is achieved by the
logistic regression model with the conformal prediction method (AUROC
0.75+-0.01, AUPRC 0.74+-0.01) at a certainty cut point of 0.8. Highest AUPRC of
0.82+-0.02 (with AUROC of 0.67+-0.04) achieved by The extreme gradient boosting
model with conformal prediction at the 0.9 certainty threshold. Radiomic and
dosiomic features improve both discriminative and calibration performance.
Conclusions: Integrating uncertainty quantification into machine learning
models with radiomic and dosiomic features may improve both predictive accuracy
and calibration, supporting more reliable clinical decision-making. The
findings emphasize the value of uncertainty quantification methods in enhancing
applicability of predictive models for radiation pneumonitis in healthcare
settings.

</details>


### [652] [Learning Fused State Representations for Control from Multi-View Observations](https://arxiv.org/pdf/2502.01316)
*Zeyu Wang, Yao-Hui Li, Xin Li, Hongyu Zang, Romain Laroche, Riashat Islam*

Main category: cs.LG

TL;DR: MFSC introduces bisimulation metric learning and a mask-based auxiliary task to improve multi-view reinforcement learning, outperforming existing methods even with interference or missing views.


<details>
  <summary>Details</summary>
Motivation: MVRL aims to enhance agent perception with multi-view observations, but learning task-relevant representations is challenging due to redundancy, distractions, or missing views.

Method: MFSC integrates bisimulation metric learning and a mask/latent reconstruction auxiliary task to leverage shared information and handle missing views.

Result: MFSC outperforms existing MVRL methods, maintaining high performance in scenarios with interference or missing views.

Conclusion: MFSC effectively addresses MVRL challenges by combining bisimulation learning and robust auxiliary tasks, demonstrating superior performance.

Abstract: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with
multi-view observations, enabling them to perceive environment with greater
effectiveness and precision. Recent advancements in MVRL focus on extracting
latent representations from multiview observations and leveraging them in
control tasks. However, it is not straightforward to learn compact and
task-relevant representations, particularly in the presence of redundancy,
distracting information, or missing views. In this paper, we propose Multi-view
Fusion State for Control (MFSC), firstly incorporating bisimulation metric
learning into MVRL to learn task-relevant representations. Furthermore, we
propose a multiview-based mask and latent reconstruction auxiliary task that
exploits shared information across views and improves MFSC's robustness in
missing views by introducing a mask token. Extensive experimental results
demonstrate that our method outperforms existing approaches in MVRL tasks. Even
in more realistic scenarios with interference or missing views, MFSC
consistently maintains high performance.

</details>


### [653] [Monotonic Learning in the PAC Framework: A New Perspective](https://arxiv.org/pdf/2501.05493)
*Ming Li, Chenyi Zhang, Qin Li*

Main category: cs.LG

TL;DR: The paper proves that monotone learning is achievable under specific conditions using PAC theory, validating it with experiments.


<details>
  <summary>Details</summary>
Motivation: Recent studies challenge the conventional wisdom of monotone learning, revealing gaps in understanding generalization in machine learning. Addressing these gaps is crucial for advancing theoretical foundations.

Method: The authors use PAC learning theory to construct a theoretical risk distribution and rigorously prove its monotonicity with increasing sample sizes. They identify two scenarios for deterministic ERM-based algorithms to be monotone.

Result: Experiments on classical learning problems validate that the generalization error's monotonicity is guaranteed, with the theoretical risk upper bound monotonically converging to 0.

Conclusion: The work provides theoretical and empirical evidence supporting monotone learning under specific conditions, contributing to the understanding of generalization in machine learning.

Abstract: Monotone learning describes learning processes in which expected performance
consistently improves as the amount of training data increases. However, recent
studies challenge this conventional wisdom, revealing significant gaps in the
understanding of generalization in machine learning. Addressing these gaps is
crucial for advancing the theoretical foundations of the field. In this work,
we utilize Probably Approximately Correct (PAC) learning theory to construct a
theoretical risk distribution that approximates a learning algorithm's actual
performance. We rigorously prove that this theoretical distribution exhibits
monotonicity as sample sizes increase. We identify two scenarios under which
deterministic algorithms based on Empirical Risk Minimization (ERM) are
monotone: (1) the hypothesis space is finite, or (2) the hypothesis space has
finite VC-dimension. Experiments on two classical learning problems validate
our findings by demonstrating that the monotonicity of the algorithms'
generalization error is guaranteed, as its theoretical risk upper bound
monotonically converges to 0.

</details>


### [654] [Learning with Differentially Private (Sliced) Wasserstein Gradients](https://arxiv.org/pdf/2502.01701)
*David Rodríguez-Vítores, Clément Lalanne, Jean-Michel Loubes*

Main category: cs.LG

TL;DR: A novel framework for private optimization using Wasserstein distances, with strong privacy guarantees and minimal utility loss, applied in deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the need for privacy-preserving machine learning, especially for objectives relying on Wasserstein distances, while maintaining accuracy.

Method: Develops a deep learning approach with gradient and activations clipping, and extends privacy accounting to Wasserstein-based objectives.

Result: The framework effectively balances accuracy and privacy, suitable for large-scale private training.

Conclusion: Theoretically sound solution for privacy-preserving tasks involving optimal transport distances like Wasserstein or sliced-Wasserstein.

Abstract: In this work, we introduce a novel framework for privately optimizing
objectives that rely on Wasserstein distances between data-dependent empirical
measures. Our main theoretical contribution is, based on an explicit
formulation of the Wasserstein gradient in a fully discrete setting, a control
on the sensitivity of this gradient to individual data points, allowing strong
privacy guarantees at minimal utility cost. Building on these insights, we
develop a deep learning approach that incorporates gradient and activations
clipping, originally designed for DP training of problems with a finite-sum
structure. We further demonstrate that privacy accounting methods extend to
Wasserstein-based objectives, facilitating large-scale private training.
Empirical results confirm that our framework effectively balances accuracy and
privacy, offering a theoretically sound solution for privacy-preserving machine
learning tasks relying on optimal transport distances such as Wasserstein
distance or sliced-Wasserstein distance.

</details>


### [655] [Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport](https://arxiv.org/pdf/2501.15549)
*Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic*

Main category: cs.LG

TL;DR: A novel method for handling categorical variables in counterfactual fairness by converting them into compositional data and transporting them within a probabilistic simplex.


<details>
  <summary>Details</summary>
Motivation: Existing optimal transport-based methods for counterfactual fairness are opaque and struggle with categorical variables.

Method: Convert categorical variables to compositional data and transport them in a probabilistic simplex.

Result: Demonstrated applicability and effectiveness on real-world data.

Conclusion: The approach addresses a key challenge but has limitations.

Abstract: Recently, optimal transport-based approaches have gained attention for
deriving counterfactuals, e.g., to quantify algorithmic discrimination.
However, in the general multivariate setting, these methods are often opaque
and difficult to interpret. To address this, alternative methodologies have
been proposed, using causal graphs combined with iterative quantile regressions
(Ple\v{c}ko and Meinshausen (2020)) or sequential transport (Fernandes Machado
et al. (2025)) to examine fairness at the individual level, often referred to
as ``counterfactual fairness.'' Despite these advancements, transporting
categorical variables remains a significant challenge in practical applications
with real datasets. In this paper, we propose a novel approach to address this
issue. Our method involves (1) converting categorical variables into
compositional data and (2) transporting these compositions within the
probabilistic simplex of $\mathbb{R}^d$. We demonstrate the applicability and
effectiveness of this approach through an illustration on real-world data, and
discuss limitations.

</details>


### [656] [From Low Intrinsic Dimensionality to Non-Vacuous Generalization Bounds in Deep Multi-Task Learning](https://arxiv.org/pdf/2501.19067)
*Hossein Zakerinia, Dorsa Ghobadi, Christoph H. Lampert*

Main category: cs.LG

TL;DR: Deep learning models generalize well due to low intrinsic dimensionality. This paper confirms this in multi-task learning, proposing a method to parametrize networks in low-dimensional space, achieving high accuracy with fewer parameters and non-vacuous generalization bounds.


<details>
  <summary>Details</summary>
Motivation: To explain and leverage the phenomenon of deep learning models generalizing well despite high ambient dimensionality by focusing on their low intrinsic dimensionality, particularly in multi-task learning.

Method: Introduces a method to parametrize multi-task networks directly in low-dimensional space using random expansions, combined with weight compression and PAC-Bayesian reasoning.

Result: High-accuracy multi-task solutions are achieved with fewer parameters than single-task learning, and non-vacuous generalization bounds for deep multi-task networks are derived.

Conclusion: The study confirms the role of low intrinsic dimensionality in generalization and provides practical methods and theoretical bounds for multi-task deep learning.

Abstract: Deep learning methods are known to generalize well from training to future
data, even in an overparametrized regime, where they could easily overfit. One
explanation for this phenomenon is that even when their *ambient
dimensionality*, (i.e. the number of parameters) is large, the models'
*intrinsic dimensionality* is small; specifically, their learning takes place
in a small subspace of all possible weight configurations. In this work, we
confirm this phenomenon in the setting of *deep multi-task learning*. We
introduce a method to parametrize multi-task network directly in the
low-dimensional space, facilitated by the use of *random expansions*
techniques. We then show that high-accuracy multi-task solutions can be found
with much smaller intrinsic dimensionality (fewer free parameters) than what
single-task learning requires. Subsequently, we show that the low-dimensional
representations in combination with *weight compression* and *PAC-Bayesian*
reasoning lead to the *first non-vacuous generalization bounds* for deep
multi-task networks.

</details>


### [657] [Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics](https://arxiv.org/pdf/2502.03654)
*Indrashis Das, Mahmoud Safari, Steven Adriaensen, Frank Hutter*

Main category: cs.LG

TL;DR: The paper introduces GoLU, a novel self-gated activation function, outperforming existing methods like GELU and Swish in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of ReLU and its variants, particularly the dying neuron problem, by proposing a more effective activation function.

Method: Defines GoLU as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, leveraging the Gompertz function's asymmetry for better latent space variance reduction.

Result: GoLU shows superior performance in tasks like Image Classification, Language Modeling, and more, compared to state-of-the-art activations.

Conclusion: GoLU is established as a robust alternative to existing activation functions, offering improved performance and stability.

Abstract: Activation functions are fundamental elements of deep learning architectures
as they significantly influence training dynamics. ReLU, while widely used, is
prone to the dying neuron problem, which has been mitigated by variants such as
LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently,
self-gated activations like GELU and Swish have emerged as state-of-the-art
alternatives, leveraging their smoothness to ensure stable gradient flow and
prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit
(GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x
\, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU
activation leverages the right-skewed asymmetry in the Gompertz function to
reduce variance in the latent space more effectively compared to GELU and
Swish, while preserving robust gradient flow. Extensive experiments across
diverse tasks, including Image Classification, Language Modeling, Semantic
Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight
GoLU's superior performance relative to state-of-the-art activation functions,
establishing GoLU as a robust alternative to existing activation functions.

</details>


### [658] [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/pdf/2502.01068)
*Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim*

Main category: cs.LG

TL;DR: FastKV is a KV cache compression method for LLMs that reduces latency and improves efficiency without sacrificing accuracy, achieving significant speed and throughput gains.


<details>
  <summary>Details</summary>
Motivation: Large KV caches in LLMs burden computational efficiency and memory; existing compression methods focus on memory reduction but lack latency improvements.

Method: FastKV uses Token-Selective Propagation (TSP) to preserve full context in early layers and selectively propagate information in later layers, minimizing redundant computation.

Result: FastKV improves TTFT by 1.97× and throughput by 4.82× while maintaining accuracy within 1% of the baseline.

Conclusion: FastKV effectively balances efficiency and accuracy, offering a practical solution for long-context inference in LLMs.

Abstract: While large language models (LLMs) excel at handling long-context sequences,
they require substantial key-value (KV) caches to store contextual information,
which can heavily burden computational efficiency and memory usage. Previous
efforts to compress these KV caches primarily focused on reducing memory
demands but were limited in enhancing latency. To address this issue, we
introduce FastKV, a KV cache compression method designed to reduce latency for
long-context inference. FastKV improves processing speed while preserving
accuracy by adopting Token-Selective Propagation (TSP). This approach preserves
full-context information in early layers of LLMs and selectively propagates
only a portion of this information in later layers. This design enables FastKV
to minimize redundant computation without sacrificing contextual fidelity. Our
experimental results show that FastKV achieves up to 1.97$\times$ and
4.82$\times$ improvements in time-to-first-token (TTFT) and throughput,
respectively, compared to baseline without KV cache compression. Moreover,
FastKV successfully maintains accuracy within 1\% of the baseline on
long-context benchmarks. Our code is available at
https://github.com/dongwonjo/FastKV.

</details>


### [659] [Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective](https://arxiv.org/pdf/2502.04591)
*Kaicheng Zhang, Piero Deidda, Desmond Higham, Francesco Tudisco*

Main category: cs.LG

TL;DR: The paper addresses oversmoothing in GNNs, critiques traditional metrics like Dirichlet energy, and proposes rank-based metrics for better reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for oversmoothing in GNNs are limited and unreliable in practical scenarios, prompting the need for a more robust alternative.

Method: The authors propose measuring oversmoothing via the numerical or effective rank of feature representations, supported by theoretical analysis.

Result: Rank-based metrics reliably capture oversmoothing, aligning with performance drops, unlike energy-based metrics.

Conclusion: Rank-based metrics offer a superior and theoretically grounded approach to quantify oversmoothing in GNNs.

Abstract: Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as
the number of layers increases, node embeddings become increasingly similar,
and model performance drops sharply. Traditionally, oversmoothing has been
quantified using metrics that measure the similarity of neighbouring node
features, such as the Dirichlet energy. While these metrics are related to
oversmoothing, we argue they have critical limitations and fail to reliably
capture oversmoothing in realistic scenarios. For instance, they provide
meaningful insights only for very deep networks and under somewhat strict
conditions on the norm of network weights and feature representations. As an
alternative, we propose measuring oversmoothing by examining the numerical or
effective rank of the feature representations. We provide theoretical support
for this approach, demonstrating that the numerical rank of feature
representations converges to one for a broad family of nonlinear activation
functions under the assumption of nonnegative trained weights. To the best of
our knowledge, this is the first result that proves the occurrence of
oversmoothing in the nonlinear setting without assumptions on the boundedness
of the weight matrices. Along with the theoretical findings, we provide
extensive numerical evaluation across diverse graph architectures. Our results
show that rank-based metrics consistently capture oversmoothing, whereas
energy-based metrics often fail. Notably, we reveal that a significant drop in
the rank aligns closely with performance degradation, even in scenarios where
energy metrics remain unchanged.

</details>


### [660] [Sparse Data Generation Using Diffusion Models](https://arxiv.org/pdf/2502.02448)
*Phil Ostheimer, Mayank Nagda, Jean Radig, Carl Herrmann, Stephan Mandt, Marius Kloft, Sophie Fellenz*

Main category: cs.LG

TL;DR: SDD is a new method for generating high-fidelity synthetic sparse data by modeling sparsity with Sparsity Bits, validated across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Sparse data is common but challenging to synthesize accurately.

Method: SDD extends diffusion models with Sparsity Bits to explicitly represent zeros.

Result: SDD achieves high fidelity in sparsity representation and data quality.

Conclusion: SDD effectively addresses the challenge of generating synthetic sparse data.

Abstract: Sparse data is ubiquitous, appearing in numerous domains, from economics and
recommender systems to astronomy and biomedical sciences. However, efficiently
generating high-fidelity synthetic sparse data remains a significant challenge.
We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse
data. SDD extends continuous state-space diffusion models with an explicit
representation of exact zeros by modeling sparsity through the introduction of
Sparsity Bits. Empirical validation in various domains, including two
scientific applications in physics and biology, demonstrates that SDD achieves
high fidelity in representing data sparsity while preserving the quality of the
generated data.

</details>


### [661] [Rethinking Link Prediction for Directed Graphs](https://arxiv.org/pdf/2502.05724)
*Mingguo He, Yuhe Guo, Yanping Zheng, Zhewei Wei, Stephan Günnemann, Xiaokui Xiao*

Main category: cs.LG

TL;DR: The paper proposes a unified framework to evaluate directed link prediction methods, introduces DirLinkBench for benchmarking, and presents SDGAE, a novel method achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of thorough analysis and effective benchmarks for directed link prediction methods.

Method: Proposes a unified framework for assessing expressiveness, introduces DirLinkBench, and develops SDGAE, a Spectral Directed Graph Auto-Encoder.

Result: DiGAE outperforms baselines, and SDGAE achieves state-of-the-art performance on DirLinkBench.

Conclusion: Highlights key factors in directed link prediction and identifies open challenges, with SDGAE as a promising solution.

Abstract: Link prediction for directed graphs is a crucial task with diverse real-world
applications. Recent advances in embedding methods and Graph Neural Networks
(GNNs) have shown promising improvements. However, these methods often lack a
thorough analysis of their expressiveness and suffer from effective benchmarks
for a fair evaluation. In this paper, we propose a unified framework to assess
the expressiveness of existing methods, highlighting the impact of dual
embeddings and decoder design on directed link prediction performance. To
address limitations in current benchmark setups, we introduce DirLinkBench, a
robust new benchmark with comprehensive coverage, standardized evaluation, and
modular extensibility. The results on DirLinkBench show that current methods
struggle to achieve strong performance, while DiGAE outperforms other baselines
overall. We further revisit DiGAE theoretically, showing its graph convolution
aligns with GCN on an undirected bipartite graph. Inspired by these insights,
we propose a novel Spectral Directed Graph Auto-Encoder SDGAE that achieves
state-of-the-art average performance on DirLinkBench. Finally, we analyze key
factors influencing directed link prediction and highlight open challenges in
this field.

</details>


### [662] [Scaling laws in wearable human activity recognition](https://arxiv.org/pdf/2502.03364)
*Tom Hoddes, Alex Bijamov, Saket Joshi, Daniel Roggen, Ali Etemad, Robert Harle, David Racz*

Main category: cs.LG

TL;DR: The paper establishes scaling laws for human activity recognition (HAR) from wearable sensors, showing power law relationships between pre-training loss, data volume, and model size. Diversity of data (more users) improves performance more than data per user. Results generalize across three HAR benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of established scaling laws in HAR, unlike in language and vision, and to guide more principled model design by linking data volume and model capacity.

Method: Conducted an exhaustive grid search on pre-training data volume and Transformer architectures to derive scaling laws. Evaluated on three HAR benchmark datasets.

Result: Pre-training loss scales with data and model size via power laws. Diversity (more users) outperforms data per user. Results validated on UCI HAR, WISDM Phone, and WISDM Watch datasets.

Conclusion: Scaling laws for HAR are established, emphasizing data diversity. Suggests revisiting prior works with adequate model capacities.

Abstract: Many deep architectures and self-supervised pre-training techniques have been
proposed for human activity recognition (HAR) from wearable multimodal sensors.
Scaling laws have the potential to help move towards more principled design by
linking model capacity with pre-training data volume. Yet, scaling laws have
not been established for HAR to the same extent as in language and vision. By
conducting an exhaustive grid search on both amount of pre-training data and
Transformer architectures, we establish the first known scaling laws for HAR.
We show that pre-training loss scales with a power law relationship to amount
of data and parameter count and that increasing the number of users in a
dataset results in a steeper improvement in performance than increasing data
per user, indicating that diversity of pre-training data is important, which
contrasts to some previously reported findings in self-supervised HAR. We show
that these scaling laws translate to downstream performance improvements on
three HAR benchmark datasets of postures, modes of locomotion and activities of
daily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some
previously published works should be revisited in light of these scaling laws
with more adequate model capacities.

</details>


### [663] [Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution](https://arxiv.org/pdf/2502.06809)
*Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, Peizhong Ju, A. B. Siddique*

Main category: cs.LG

TL;DR: The paper introduces NeuronLens, a range-based framework for interpreting and manipulating neuron activations in LLMs, addressing polysemanticity and improving precision.


<details>
  <summary>Details</summary>
Motivation: Understanding LLM internal mechanisms is key for trust and utility, but current neuron-to-concept mappings fail due to polysemanticity.

Method: Analyzed encoder and decoder-based LLMs, observed Gaussian-like activation distributions, and proposed NeuronLens for range-based interpretation.

Result: NeuronLens reduces unintended interference and enables precise concept manipulation, outperforming neuron attribution methods.

Conclusion: Range-based interpretation (NeuronLens) offers a more accurate and effective approach for LLM interpretability and control.

Abstract: Interpreting the internal mechanisms of large language models (LLMs) is
crucial for improving their trustworthiness and utility. Prior work has
primarily focused on mapping individual neurons to discrete semantic concepts.
However, such mappings struggle to handle the inherent polysemanticity in LLMs,
where individual neurons encode multiple, distinct concepts. Through a
comprehensive analysis of both encoder and decoder-based LLMs across diverse
datasets, we observe that even highly salient neurons, identified via various
attribution techniques for specific semantic concepts, consistently exhibit
polysemantic behavior. Importantly, activation magnitudes for fine-grained
concepts follow distinct, often Gaussian-like distributions with minimal
overlap. This observation motivates a shift from neuron attribution to
range-based interpretation. We hypothesize that interpreting and manipulating
neuron activation ranges would enable more precise interpretability and
targeted interventions in LLMs. To validate our hypothesis, we introduce
NeuronLens, a novel range-based interpretation and manipulation framework that
provides a finer view of neuron activation distributions to localize concept
attribution within a neuron. Extensive empirical evaluations demonstrate that
NeuronLens significantly reduces unintended interference, while maintaining
precise manipulation of targeted concepts, outperforming neuron attribution.

</details>


### [664] [From Features to Transformers: Redefining Ranking for Scalable Impact](https://arxiv.org/pdf/2502.03417)
*Fedor Borisyuk, Lars Hertel, Ganesh Parameswaran, Gaurav Srivastava, Sudarshan Srinivasa Ramanujam, Borja Ocejo, Peng Du, Andrei Akterskii, Neil Daftary, Shao Tang, Daqi Sun, Qiang Charles Xiao, Deepesh Nathani, Mohit Kothari, Yun Dai, Guoyao Li, Aman Gupta*

Main category: cs.LG

TL;DR: LiGR is a transformer-based ranking framework by LinkedIn, reducing manual feature engineering, validating scaling laws, and improving diversity via set-wise attention.


<details>
  <summary>Details</summary>
Motivation: To advance ranking systems by integrating transformer architectures, reducing reliance on manual features, and improving efficiency and diversity.

Method: Modified transformer with learned normalization and set-wise attention to user history and items, enabling joint scoring and efficient inference.

Result: Outperforms prior systems with fewer features, scales well with larger models/data, and improves diversity. Efficient serving techniques are validated.

Conclusion: LiGR successfully integrates transformers for ranking, achieving breakthroughs in feature reduction, scalability, and diversity while enabling efficient production deployment.

Abstract: We present LiGR, a large-scale ranking framework developed at LinkedIn that
brings state-of-the-art transformer-based modeling architectures into
production. We introduce a modified transformer architecture that incorporates
learned normalization and simultaneous set-wise attention to user history and
ranked items. This architecture enables several breakthrough achievements,
including: (1) the deprecation of most manually designed feature engineering,
outperforming the prior state-of-the-art system using only few features
(compared to hundreds in the baseline), (2) validation of the scaling law for
ranking systems, showing improved performance with larger models, more training
data, and longer context sequences, and (3) simultaneous joint scoring of items
in a set-wise manner, leading to automated improvements in diversity. To enable
efficient serving of large ranking models, we describe techniques to scale
inference effectively using single-pass processing of user history and set-wise
attention. We also summarize key insights from various ablation studies and A/B
tests, highlighting the most impactful technical approaches.

</details>


### [665] [Memory Is Not the Bottleneck: Cost-Efficient Continual Learning via Weight Space Consolidation](https://arxiv.org/pdf/2502.07274)
*Dongkyu Cho, Taesup Moon, Rumi Chunara, Kyunghyun Cho, Sungmin Cha*

Main category: cs.LG

TL;DR: The paper re-examines continual learning (CL) in modern settings with abundant memory, finding that stability improves but plasticity declines. It proposes Weight Space Consolidation, a lightweight method outperforming state-of-the-art approaches at lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Modern computing environments prioritize GPU time over memory, challenging traditional CL assumptions. The paper explores CL with sufficient memory to retain past data.

Method: Proposes Weight Space Consolidation, using rank-based parameter resets for plasticity and weight averaging for stability.

Result: Outperforms baselines in class-incremental learning and continual instruction tuning, reducing training costs by 66-75%.

Conclusion: Challenges CL norms, offering a cost-efficient baseline for memory-abundant settings.

Abstract: Continual learning (CL) has traditionally emphasized minimizing exemplar
memory usage, assuming that memory is the primary bottleneck. However, in
modern computing environments-particularly those involving large foundation
models-memory is inexpensive and abundant, while GPU time constitutes the main
cost. This paper re-examines CL under a more realistic setting with sufficient
exemplar memory, where the system can retain a representative portion of past
data. We find that, under this regime, stability improves due to reduced
forgetting, but plasticity diminishes as the model becomes biased toward prior
tasks and struggles to adapt to new ones. Notably, even simple baselines like
naive replay can match or exceed the performance of state-of-the-art methods at
a fraction of the computational cost. Building on this insight, we propose a
lightweight yet effective method called Weight Space Consolidation, which
directly operates in the model's weight space via two core mechanisms: (1)
rank-based parameter resets to recover plasticity, and (2) weight averaging to
enhance stability. Our approach outperforms strong baselines across
class-incremental learning with image classifiers and continual instruction
tuning with large language models, while requiring only one-third to one-fourth
of the training cost. These findings challenge long-standing CL assumptions and
establish a new, cost-efficient baseline for real-world continual learning
systems where exemplar memory is no longer the limiting factor.

</details>


### [666] [Reinforcement Learning on Dyads to Enhance Medication Adherence](https://arxiv.org/pdf/2502.06835)
*Ziping Xu, Hinal Jajal, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Alexandra M. Psihogios, Pei-Yao Hung, Susan Murphy*

Main category: cs.LG

TL;DR: A MARL-based digital intervention improves medication adherence in AYAs post-HCT by personalizing dyad-focused support.


<details>
  <summary>Details</summary>
Motivation: Medication adherence is challenging for AYAs post-HCT due to individual and interpersonal barriers, necessitating personalized interventions.

Method: A Multi-Agent Reinforcement Learning (MARL) framework personalizes a three-component digital intervention for dyads (AYAs and care partners).

Result: The MARL approach improves medication adherence by ~3% compared to random intervention delivery in a simulated environment.

Conclusion: The MARL framework shows promise for enhancing adherence and will be further tested in a clinical trial.

Abstract: Medication adherence is critical for the recovery of adolescents and young
adults (AYAs) who have undergone hematopoietic cell transplantation (HCT).
However, maintaining adherence is challenging for AYAs after hospital
discharge, who experience both individual (e.g. physical and emotional
symptoms) and interpersonal barriers (e.g., relational difficulties with their
care partner, who is often involved in medication management). To optimize the
effectiveness of a three-component digital intervention targeting both members
of the dyad as well as their relationship, we propose a novel Multi-Agent
Reinforcement Learning (MARL) approach to personalize the delivery of
interventions. By incorporating the domain knowledge, the MARL framework, where
each agent is responsible for the delivery of one intervention component,
allows for faster learning compared with a flattened agent. Evaluation using a
dyadic simulator environment, based on real clinical data, shows a significant
improvement in medication adherence (approximately 3%) compared to purely
random intervention delivery. The effectiveness of this approach will be
further evaluated in an upcoming trial.

</details>


### [667] [Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcement Learning](https://arxiv.org/pdf/2502.07949)
*Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao*

Main category: cs.LG

TL;DR: VSC-RL improves learning efficiency for VLM agents in complex tasks by reformulating decision-making as a variational subgoal-conditioned RL problem with a new optimization objective (SGC-ELBO).


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for VLM agents struggle with inefficiencies in complex, real-world tasks due to sparse rewards and long-horizon dependencies.

Method: VSC-RL introduces a variational subgoal-conditioned RL framework with SGC-ELBO, optimizing subgoal-conditioned returns and minimizing divergence from a reference policy.

Result: VSC-RL outperforms SOTA methods in benchmarks like mobile device and web control tasks, showing superior learning efficiency and performance.

Conclusion: VSC-RL effectively addresses inefficiencies in RL for VLM agents, offering a robust solution for complex decision-making tasks.

Abstract: State-of-the-art (SOTA) reinforcement learning (RL) methods have enabled
vision-language model (VLM) agents to learn from interaction with online
environments without human supervision. However, these methods often struggle
with learning inefficiencies when applied to complex, real-world
decision-making tasks with sparse rewards and long-horizon dependencies. We
propose a novel framework, Variational Subgoal-Conditioned Reinforcement
Learning (VSC-RL), advancing the VLM agents in resolving challenging
decision-making tasks. Fundamentally distinct from existing methods, VSC-RL
reformulates the decision-making problem as a variational subgoal-conditioned
RL problem with the newly derived optimization objective, Subgoal Evidence
Lower BOund (SGC-ELBO), which comprises two key components: (a) maximizing the
subgoal-conditioned return, and (b) minimizing the divergence from a reference
goal-conditioned policy. We theoretically and empirically demonstrate that the
VSC-RL can efficiently improve the learning efficiency without compromising
performance guarantees. Across a diverse set of challenging benchmarks,
including mobile device and web control tasks, VSC-RL consistently outperforms
existing SOTA methods, achieving superior learning efficiency and performance.

</details>


### [668] [Optimizing Asynchronous Federated Learning: A~Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency](https://arxiv.org/pdf/2502.08206)
*Abdelkrim Alahyane, Céline Comte, Matthieu Jonckheere, Éric Moulines*

Main category: cs.LG

TL;DR: The paper analyzes asynchronous federated learning (FL) to address scalability issues caused by stragglers. It optimizes design choices like concurrency and routing, balancing gradient staleness and system throughput, improving accuracy by 10-30%.


<details>
  <summary>Details</summary>
Motivation: Synchronous FL suffers from the straggler effect, limiting scalability. Asynchronous FL (e.g., FedAsync) offers a solution, but its design choices' impact is not well understood. This work aims to model and optimize these choices.

Method: Uses stochastic modeling to analyze asynchronous FL, focusing on concurrency and routing. Introduces a discrete variant of Little's law to quantify staleness and derives a tractable upper-bound for a new optimization metric.

Result: Proves a closed-form expression for relative delay (staleness) and introduces a new metric balancing staleness and throughput. Numerical results show 10-30% accuracy improvement.

Conclusion: Optimizing asynchronous FL requires balancing staleness and throughput. The proposed metrics and methods significantly enhance accuracy, demonstrating practical benefits.

Abstract: Synchronous federated learning (FL) scales poorly with the number of clients
due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync
address this limitation by enabling asynchronous communication between clients
and the central server. In this work, we rely on stochastic modeling and
analysis to better understand the impact of design choices in asynchronous FL
algorithms, such as the concurrency level and routing probabilities, and we
leverage this knowledge to optimize loss. Compared to most existing studies, we
account for the joint impact of heterogeneous and variable service speeds and
heterogeneous datasets at the clients. We characterize in particular a
fundamental trade-off for optimizing asynchronous FL: minimizing gradient
estimation errors by avoiding model parameter staleness, while also speeding up
the system by increasing the throughput of model updates. Our two main
contributions can be summarized as follows. First, we prove a discrete variant
of Little's law to derive a closed-form expression for relative delay, a metric
that quantifies staleness. This allows us to efficiently minimize the average
loss per model update, which has been the gold standard in literature to date.
Second, we observe that naively optimizing this metric leads us to slow down
the system drastically by overemphazing staleness at the detriment of
throughput. This motivates us to introduce an alternative metric that also
takes system speed into account, for which we derive a tractable upper-bound
that can be minimized numerically. Extensive numerical results show that these
optimizations enhance accuracy by 10% to 30%.

</details>


### [669] [Probing Semantic Routing in Large Mixture-of-Expert Models](https://arxiv.org/pdf/2502.10928)
*Matthew Lyle Olson, Neale Ratzlaff, Musashi Hinck, Man Luo, Sungduk Yu, Chendi Xue, Vasudev Lal*

Main category: cs.LG

TL;DR: The paper investigates if expert routing in large MoE models is influenced by input semantics, finding clear evidence of semantic routing.


<details>
  <summary>Details</summary>
Motivation: To understand whether expert routing in large MoE models is semantically driven, given prior work on functional differentiation.

Method: Two controlled experiments: comparing activations on sentence pairs with shared target words in same/different senses, and substituting target words with similar/dissimilar alternatives.

Result: Statistically significant evidence of semantic routing in large MoE models.

Conclusion: Large MoE models exhibit semantic routing, influenced by input semantics.

Abstract: In the past year, large (>100B parameter) mixture-of-expert (MoE) models have
become increasingly common in the open domain. While their advantages are often
framed in terms of efficiency, prior work has also explored functional
differentiation through routing behavior. We investigate whether expert routing
in large MoE models is influenced by the semantics of the inputs. To test this,
we design two controlled experiments. First, we compare activations on sentence
pairs with a shared target word used in the same or different senses. Second,
we fix context and substitute the target word with semantically similar or
dissimilar alternatives. Comparing expert overlap across these conditions
reveals clear, statistically significant evidence of semantic routing in large
MoE models.

</details>


### [670] [Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data](https://arxiv.org/pdf/2502.09981)
*Harsh Poonia, Felix Divo, Kristian Kersting, Devendra Singh Dhami*

Main category: cs.LG

TL;DR: Proposes GC-xLSTM, a method combining Granger causality with xLSTM to improve long-range causal detection in time series.


<details>
  <summary>Details</summary>
Motivation: Granger causality struggles with long-range dependencies in time series; GC-xLSTM aims to address this.

Method: Uses xLSTM with dynamic loss penalty for sparsity and joint optimization to robustly recover Granger causal relations.

Result: Demonstrated efficacy on six diverse datasets.

Conclusion: GC-xLSTM effectively improves long-range Granger causal detection.

Abstract: Causality in time series can be difficult to determine, especially in the
presence of non-linear dependencies. The concept of Granger causality helps
analyze potential relationships between variables, thereby offering a method to
determine whether one time series can predict - Granger cause - future values
of another. Although successful, Granger causal methods still struggle with
capturing long-range relations between variables. To this end, we leverage the
recently successful Extended Long Short-Term Memory (xLSTM) architecture and
propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between
the time series components by using a novel dynamic loss penalty on the initial
projection. Specifically, we adaptively improve the model and identify sparsity
candidates. Our joint optimization procedure then ensures that the Granger
causal relations are recovered robustly. Our experimental evaluation on six
diverse datasets demonstrates the overall efficacy of our proposed GC-xLSTM
model.

</details>


### [671] [GiFT: Gibbs Fine-Tuning for Code Generation](https://arxiv.org/pdf/2502.11466)
*Haochen Li, Wanjin Feng, Xin Zhou, Zhiqi Shen*

Main category: cs.LG

TL;DR: GiFT is a self-training method for LLMs in code generation, using Gibbs sampling to draw data from the marginal distribution of the joint description-code space, improving performance over conditional sampling.


<details>
  <summary>Details</summary>
Motivation: Conditional sampling in self-training underrepresents the full description-code space, leading to biases.

Method: GiFT employs Gibbs sampling to draw self-generated data from the marginal distribution and uses perplexity-based code selection to address long-tail imbalances.

Result: GiFT outperforms conditional sampling, especially on challenging benchmarks, as shown in evaluations across two LLMs and four datasets.

Conclusion: GiFT mitigates biases in self-training and enhances LLM performance in code generation.

Abstract: Training Large Language Models (LLMs) with synthetic data is a prevalent
practice in code generation. A key approach is self-training, where LLMs are
iteratively trained on self-generated correct code snippets. In this case, the
self-generated codes are drawn from a conditional distribution, conditioned on
a specific seed description. However, the seed description is not the only
valid representation that aligns with its intended meaning. With all valid
descriptions and codes forming a joint space, codes drawn from the conditional
distribution would lead to an underrepresentation of the full description-code
space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training
method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn
from the marginal distribution of the joint space, thereby mitigating the
biases inherent in conditional sampling. We provide a theoretical analysis
demonstrating the potential benefits of fine-tuning LLMs with code derived from
the marginal distribution. Furthermore, we propose a perplexity-based code
selection method to mitigate the imbalanced long-tail distribution of the
self-generated codes. Empirical evaluation of two LLMs across four datasets
demonstrates that GiFT achieves superior performance, particularly on more
challenging benchmarks. Source code is available at
https://github.com/Alex-HaochenLi/GiFT.

</details>


### [672] [LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities](https://arxiv.org/pdf/2502.12128)
*Florian Sestak, Artur Toshev, Andreas Fürst, Günter Klambauer, Andreas Mayr, Johannes Brandstetter*

Main category: cs.LG

TL;DR: LaM-SLidE introduces identifier representations (IDs) to enable traceability of entities in latent space modeling of dynamical systems, bridging the gap between efficiency and entity tracking.


<details>
  <summary>Details</summary>
Motivation: Dynamical systems, like chemical molecules or human behavior, involve interactions and traceability of entities, which traditional latent space models struggle to address.

Method: LaM-SLidE uses identifier representations (IDs) to retrieve entity properties and composition from latent representations, combining efficiency with traceability.

Result: LaM-SLidE outperforms in speed, accuracy, and generalizability across various domains.

Conclusion: The approach successfully integrates traceability with latent space modeling, advancing generative models for dynamical systems.

Abstract: Generative models are spearheading recent progress in deep learning,
showcasing strong promise for trajectory sampling in dynamical systems as well.
However, whereas latent space modeling paradigms have transformed image and
video generation, similar approaches are more difficult for most dynamical
systems. Such systems -- from chemical molecule structures to collective human
behavior -- are described by interactions of entities, making them inherently
linked to connectivity patterns, entity conservation, and the traceability of
entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial
Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping
the traceability of individual entities in a latent system representation, and
(2) leveraging the efficiency and scalability of recent advances in image and
video generation, where pre-trained encoder and decoder enable generative
modeling directly in latent space. The core idea of LaM-SLidE is the
introduction of identifier representations (IDs) that enable the retrieval of
entity properties and entity composition from latent system representations,
thus fostering traceability. Experimentally, across different domains, we show
that LaM-SLidE performs favorably in terms of speed, accuracy, and
generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .

</details>


### [673] [Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis](https://arxiv.org/pdf/2502.13178)
*Jiaqi Zhao, Ming Wang, Miao Zhang, Yuzhang Shang, Xuebo Liu, Yaowei Wang, Min Zhang, Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces a benchmark for Post-training Quantization (PTQ) in large language models (LLMs), analyzing PTQ strategies' strengths and trade-offs among model size, performance, and bitwidth.


<details>
  <summary>Details</summary>
Motivation: Current PTQ research lacks detailed analysis of strategy superiority and applicable scenarios, and overlooks trade-offs between model size, performance, and quantization bitwidth.

Method: The authors propose a taxonomy for PTQ methods, conduct extensive experiments across models of varying sizes, bitwidths, architectures, and modalities, and analyze results.

Result: Compensation-based PTQ shows cross-architecture robustness, and low-bit PTQ for ultra-large models needs reevaluation. A combination of compensation and other PTQ strategies achieves state-of-the-art robustness.

Conclusion: The benchmark provides practical recommendations for LLM deployment and future PTQ research, with findings shared in a public repository.

Abstract: Post-training Quantization (PTQ) technique has been extensively adopted for
large language models (LLMs) compression owing to its efficiency and low
resource requirement. However, current research lacks a in-depth analysis of
the superior and applicable scenarios of each PTQ strategy. In addition,
existing algorithms focus primarily on performance, overlooking the trade-off
among model size, performance, and quantization bitwidth. To mitigate these
confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,
in order to support our benchmark, we propose a comprehensive taxonomy for
existing mainstream methods by scrutinizing their computational strategies
(e.g., optimization-based, compensation-based, etc.). Then, we conduct
extensive experiments with the baseline within each class, covering models with
various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),
architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and
VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on
the results, we summarize the superior of each PTQ strategy and
modelsize-bitwidth trade-off considering the performance. For example, our
benchmark reveals that compensation-based technique demonstrates outstanding
cross-architecture robustness and extremely low-bit PTQ for ultra large models
should be reexamined. Finally, we further accordingly claim that a practical
combination of compensation and other PTQ strategy can achieve SOTA various
robustness. We believe that our benchmark will provide valuable recommendations
for the deployment of LLMs and future research on PTQ approaches.We conduct an
repository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.

</details>


### [674] [Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification](https://arxiv.org/pdf/2502.16627)
*Arshia Kermani, Ehsan Zeraatkar, Habib Irani*

Main category: cs.LG

TL;DR: The paper explores optimization techniques like pruning and quantization for transformer models in time series classification, showing significant energy and speed improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational demands of transformer models in time series classification to enable energy-efficient deployment.

Method: Systematic investigation of structured pruning and quantization methods, tested on three datasets (RefrigerationDevices, ElectricDevices, PLAID).

Result: Static quantization cuts energy use by 29.14% without performance loss; L1 pruning boosts inference speed by 63% with slight accuracy drop.

Conclusion: Optimization strategies like pruning and quantization are effective for efficient transformer deployment in resource-limited settings.

Abstract: The increasing computational demands of transformer models in time series
classification necessitate effective optimization strategies for
energy-efficient deployment. Our study presents a systematic investigation of
optimization techniques, focusing on structured pruning and quantization
methods for transformer architectures. Through extensive experimentation on
three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we
quantitatively evaluate model performance and energy efficiency across
different transformer configurations. Our experimental results demonstrate that
static quantization reduces energy consumption by 29.14% while maintaining
classification performance, and L1 pruning achieves a 63% improvement in
inference speed with minimal accuracy degradation. Our findings provide
valuable insights into the effectiveness of optimization strategies for
transformer-based time series classification, establishing a foundation for
efficient model deployment in resource-constrained environments.

</details>


### [675] [Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence](https://arxiv.org/pdf/2502.17028)
*Wenzhe Yin, Zehao Xiao, Pan Zhou, Shujian Yu, Jiayi Shen, Jan-Jakob Sonke, Efstratios Gavves*

Main category: cs.LG

TL;DR: CS-Aligner improves multimodal alignment by integrating Cauchy-Schwarz divergence with mutual information, addressing InfoNCE's limitations and enhancing flexibility.


<details>
  <summary>Details</summary>
Motivation: Overcoming InfoNCE's suboptimal alignment and modality gaps in multimodal tasks like cross-modal generation and retrieval.

Method: Proposes CS-Aligner, a framework combining Cauchy-Schwarz divergence and mutual information for distributional vision-language alignment.

Result: Achieves tighter and more precise alignment, leveraging unpaired data and token-level representations.

Conclusion: CS-Aligner outperforms in vision-language alignment, validated by text-to-image generation and cross-modality retrieval tasks.

Abstract: Multimodal alignment is crucial for various downstream tasks such as
cross-modal generation and retrieval. Previous multimodal approaches like CLIP
utilize InfoNCE to maximize mutual information, primarily aligning pairwise
samples across modalities while overlooking distributional differences. In
addition, InfoNCE has inherent conflict in terms of alignment and uniformity in
multimodality, leading to suboptimal alignment with modality gaps. To overcome
the limitations, we propose CS-Aligner, a novel framework that performs
distributional vision-language alignment by integrating Cauchy-Schwarz (CS)
divergence with mutual information. CS-Aligner captures both the global
distribution information of each modality and the pairwise semantic
relationships. We find that the CS divergence seamlessly addresses the
InfoNCE's alignment-uniformity conflict and serves complementary roles with
InfoNCE, yielding tighter and more precise alignment. Moreover, by introducing
distributional alignment, CS-Aligner enables incorporating additional
information from unpaired data and token-level representations, enhancing
flexible and fine-grained alignment in practice. Experiments on text-to-image
generation and cross-modality retrieval tasks demonstrate the effectiveness of
our method on vision-language alignment.

</details>


### [676] [Large Language Models are Powerful Electronic Health Record Encoders](https://arxiv.org/pdf/2502.17403)
*Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild*

Main category: cs.LG

TL;DR: General-purpose LLMs can effectively encode EHRs for clinical prediction, matching or outperforming specialized models like CLMBR-T-Base across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: EHRs are complex and heterogeneous, challenging traditional ML. Domain-specific models face data access and consistency issues.

Method: Convert structured EHR data to markdown text, replacing codes with natural language descriptions, and use LLMs for encoding.

Result: LLM-based embeddings match or surpass CLMBR-T-Base on 15 EHRSHOT tasks and generalize well on UK Biobank data.

Conclusion: General-purpose LLMs offer a scalable, generalizable alternative to domain-specific EHR models.

Abstract: Electronic Health Records (EHRs) offer considerable potential for clinical
prediction, but their complexity and heterogeneity present significant
challenges for traditional machine learning methods. Recently, domain-specific
EHR foundation models trained on large volumes of unlabeled EHR data have shown
improved predictive accuracy and generalization. However, their development is
constrained by limited access to diverse, high-quality datasets, and by
inconsistencies in coding standards and clinical practices. In this study, we
explore the use of general-purpose Large Language Models (LLMs) to encode EHR
into high-dimensional representations for downstream clinical prediction tasks.
We convert structured EHR data into markdown-formatted plain text documents by
replacing medical codes with natural language descriptions. This enables the
use of LLMs and their extensive semantic understanding and generalization
capabilities as effective encoders of EHRs without requiring access to private
medical training data. We show that LLM-based embeddings can often match or
even surpass the performance of a specialized EHR foundation model,
CLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To
demonstrate generalizability, we further evaluate the approach on the UK
Biobank (UKB) cohort, a population distinct from that used to train
CLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior
performance for disease onset, hospitalization, and mortality prediction,
highlighting robustness to shifts in patient populations. Our findings suggest
that repurposed general-purpose LLMs for EHR encoding provide a scalable and
generalizable alternative to domain-specific models for clinical prediction.

</details>


### [677] [Sharper Risk Bound for Multi-Task Learning with Multi-Graph Dependent Data](https://arxiv.org/pdf/2502.18167)
*Xiao Shao, Guoqiang Wu*

Main category: cs.LG

TL;DR: The paper improves the risk bound in multi-task learning (MTL) with graph-dependent data from $O(\frac{1}{\sqrt{n}})$ to $O(\frac{\log n}{n})$ using a new Bennett-type inequality and analytical framework.


<details>
  <summary>Details</summary>
Motivation: Existing generalization analyses for MTL with graph-dependent data yield sub-optimal risk bounds, prompting the need for sharper concentration inequalities.

Method: The paper introduces a new Bennett-type inequality, a Talagrand-type inequality, and a local fractional Rademacher complexity framework to enhance generalization analyses.

Result: A sharper risk bound of $O(\frac{\log n}{n})$ is achieved, validated by applications like Macro-AUC optimization and experiments.

Conclusion: The theoretical advancements provide superior generalization bounds for MTL with graph-dependent data, outperforming prior work.

Abstract: In multi-task learning (MTL) with each task involving graph-dependent data,
existing generalization analyses yield a \emph{sub-optimal} risk bound of
$O(\frac{1}{\sqrt{n}})$, where $n$ is the number of training samples of each
task. However, to improve the risk bound is technically challenging, which is
attributed to the lack of a foundational sharper concentration inequality for
multi-graph dependent random variables. To fill up this gap, this paper
proposes a new Bennett-type inequality, enabling the derivation of a sharper
risk bound of $O(\frac{\log n}{n})$. Technically, building on the proposed
Bennett-type inequality, we propose a new Talagrand-type inequality for the
empirical process, and further develop a new analytical framework of the local
fractional Rademacher complexity to enhance generalization analyses in MTL with
multi-graph dependent data. Finally, we apply the theoretical advancements to
applications such as Macro-AUC optimization, illustrating the superiority of
our theoretical results over prior work, which is also verified by experimental
results.

</details>


### [678] [Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining](https://arxiv.org/pdf/2503.04715)
*Houyi Li, Wenzhen Zheng, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang*

Main category: cs.LG

TL;DR: The paper identifies universal scaling laws for hyperparameters in LLMs, showing power-law relationships for learning rate and batch size, and introduces a plug-and-play tool for optimal hyperparameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hyperparameter optimization in LLMs, which is critical for their effective deployment.

Method: Extensive empirical studies involving grid searches across diverse configurations to discover scaling laws and analyze the optimization landscape.

Result: Optimal hyperparameters follow power-law relationships, with a convex optimization landscape. The tool achieves near-global optimal performance (0.09% deviation).

Conclusion: The study provides robust scaling laws applicable across diverse model shapes and data distributions, supported by extensive computational resources.

Abstract: The impressive capabilities of Large Language Models (LLMs) across diverse
tasks are now well-established, yet their effective deployment necessitates
careful hyperparameter optimization. Through extensive empirical studies
involving grid searches across diverse configurations, we discover universal
scaling laws governing these hyperparameters: optimal learning rate follows a
power-law relationship with both model parameters and data sizes, while optimal
batch size scales primarily with data sizes. Our analysis reveals a convex
optimization landscape for hyperparameters under fixed models and data size
conditions. This convexity implies an optimal hyperparameter plateau. We
contribute a universal, plug-and-play optimal hyperparameter tool for the
community. Its estimated values on the test set are merely 0.09% away from the
globally optimal LLM performance found via an exhaustive search. These laws
demonstrate remarkable robustness across variations in model sparsity, training
data distribution, and model shape. To our best known, this is the first work
that unifies different model shapes and structures, such as Mixture-of-Experts
models and dense transformers, as well as establishes optimal hyperparameter
scaling laws across diverse data distributions. This exhaustive optimization
process demands substantial computational resources, utilizing nearly one
million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and
hyperparameters from scratch and consuming approximately 100 trillion tokens in
total. To facilitate reproducibility and further research, we will
progressively release all loss measurements and model checkpoints through our
designated repository https://step-law.github.io/

</details>


### [679] [Teaching Metric Distance to Autoregressive Multimodal Foundational Models](https://arxiv.org/pdf/2503.02379)
*Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu*

Main category: cs.LG

TL;DR: DIST2Loss is a distance-aware framework for training autoregressive models to preserve metric relationships among tokens, improving performance in multimodal tasks, especially in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Tokens in large language models increasingly represent metric relationships (e.g., in math or multimodal tasks) rather than just linguistic meaning. Existing models lack mechanisms to preserve these relationships during token generation.

Method: DIST2Loss transforms continuous exponential family distributions (based on distance metrics) into discrete optimization targets compatible with autoregressive models, ensuring meaningful distance relationships are learned.

Result: Empirical evaluations show performance gains in visual grounding, robotic manipulation, generative reward modeling, and image generation, particularly in low-data regimes.

Conclusion: DIST2Loss effectively trains models to preserve metric relationships among tokens, enhancing performance in diverse multimodal applications, especially under resource constraints.

Abstract: As large language models expand beyond natural language to domains such as
mathematics, multimodal understanding, and embodied agents, tokens increasingly
reflect metric relationships rather than purely linguistic meaning. We
introduce DIST2Loss, a distance-aware framework designed to train
autoregressive discrete models by leveraging predefined distance relationships
among output tokens. At its core, DIST2Loss transforms continuous exponential
family distributions derived from inherent distance metrics into discrete,
categorical optimization targets compatible with the models' architectures.
This approach enables the models to learn and preserve meaningful distance
relationships during token generation while maintaining compatibility with
existing architectures. Empirical evaluations show consistent performance gains
in diverse multimodal applications, including visual grounding, robotic
manipulation, generative reward modeling, and image generation using
vector-quantized features. These improvements are most notable in low-data
regimes, demonstrating DIST2Loss's strength under resource constraints.

</details>


### [680] [Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation](https://arxiv.org/pdf/2503.07578)
*Tianyu Chen, Yasi Zhang, Zhendong Wang, Ying Nian Wu, Oscar Leong, Mingyuan Zhou*

Main category: cs.LG

TL;DR: DSD trains generative models from noisy data by pretraining a diffusion model on corrupted samples and distilling it into a one-step generator, improving sample quality and applicability in low-quality data settings.


<details>
  <summary>Details</summary>
Motivation: Diffusion models rely on high-quality data, limiting their use in domains with scarce or costly clean data.

Method: DSD pretrains a diffusion model on noisy data, then distills it into a one-step generator for refined outputs.

Result: DSD consistently improves generative performance across noise levels and datasets, with theoretical insights on implicit regularization.

Conclusion: DSD reframes score distillation as a tool for both efficiency and quality enhancement in low-quality data scenarios.

Abstract: Diffusion models have achieved remarkable success in generating
high-resolution, realistic images across diverse natural distributions.
However, their performance heavily relies on high-quality training data, making
it challenging to learn meaningful distributions from corrupted samples. This
limitation restricts their applicability in scientific domains where clean data
is scarce or costly to obtain. In this work, we introduce denoising score
distillation (DSD), a surprisingly effective and novel approach for training
high-quality generative models from low-quality data. DSD first pretrains a
diffusion model exclusively on noisy, corrupted samples and then distills it
into a one-step generator capable of producing refined, clean outputs. While
score distillation is traditionally viewed as a method to accelerate diffusion
models, we show that it can also significantly enhance sample quality,
particularly when starting from a degraded teacher model. Across varying noise
levels and datasets, DSD consistently improves generative performancewe
summarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical
insights showing that, in a linear model setting, DSD identifies the eigenspace
of the clean data distributions covariance matrix, implicitly regularizing the
generator. This perspective reframes score distillation as not only a tool for
efficiency but also a mechanism for improving generative models, particularly
in low-quality data settings.

</details>


### [681] [TRACE: Time SeRies PArameter EffiCient FinE-tuning](https://arxiv.org/pdf/2503.16991)
*Yuze Li, Wei Zhu*

Main category: cs.LG

TL;DR: TRACE is a parameter-efficient fine-tuning method for time series foundation models, addressing challenges like data variability and temporal adaptation with innovations like Gated DSIC and reconstructed prediction heads.


<details>
  <summary>Details</summary>
Motivation: Time series data vary in frequency, channels, and lengths, requiring tailored fine-tuning for tasks like long-term forecasting. Existing methods like LoRA need adaptation for temporal characteristics.

Method: TRACE introduces Gated DSIC for unbiased LoRA module importance selection and reconstructed prediction heads for long-term forecasting, reducing parameters while maintaining performance.

Result: Experiments show TRACE outperforms common fine-tuning and achieves comparable/superior performance to linear probing heads with fewer parameters.

Conclusion: TRACE is validated as effective across diverse datasets and tasks, including forecasting and anomaly detection.

Abstract: We propose an efficient fine-tuning method for time series foundation models,
termed TRACE: Time Series Parameter Efficient Fine-tuning. While pretrained
time series foundation models are gaining popularity, they face the following
challenges: (1) Unlike natural language tasks, time series data vary in
frequency, channel numbers, historical/prediction lengths. For long-term
forecasting tasks in particular, tailored fine-tuning can significantly enhance
performance.(2) Existing parameter-efficient tuning methods like LoRA remain
applicable but require adaptation to temporal characteristics.
  To address these challenges, our TRACE framework introduces two key
innovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Calculation),
an unbiased LoRA module importance selection mechanism that ensures conditional
parameter consistency before and after masking. Experiments demonstrate that
Gated DSIC outperforms common fine-tuning. (2) Reconstructed prediction heads
for long-term forecasting tasks, which achieve comparable or superior
performance to linear probing heads while drastically reducing parameter
counts.
  Extensive experiments on long-/short-term forecasting, anomaly detection and
natural language tasks across diverse datasets, coupled with ablation studies,
validate the effectiveness of our method.

</details>


### [682] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/pdf/2505.04046)
*Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, Dezhong Peng*

Main category: cs.LG

TL;DR: RDML addresses adversarial unreliability in multi-view learning by disentangling clean and adversarial parts, recalibrating features, and using evidential attention.


<details>
  <summary>Details</summary>
Motivation: Existing trusted multi-view learning assumes secure data, but adversarial perturbations in safety-sensitive applications cause adversarial unreliability.

Method: RDML uses evidential disentanglement, feature recalibration, and view-level evidential attention to mitigate adversarial impacts.

Result: RDML outperforms state-of-the-art methods in multi-view classification under adversarial attacks.

Conclusion: RDML effectively enhances the reliability of multi-view learning in adversarial settings.

Abstract: Trustworthy multi-view learning has attracted extensive attention because
evidence learning can provide reliable uncertainty estimation to enhance the
credibility of multi-view predictions. Existing trusted multi-view learning
methods implicitly assume that multi-view data is secure. However, in
safety-sensitive applications such as autonomous driving and security
monitoring, multi-view data often faces threats from adversarial perturbations,
thereby deceiving or disrupting multi-view models. This inevitably leads to the
adversarial unreliability problem (AUP) in trusted multi-view learning. To
overcome this tricky problem, we propose a novel multi-view learning framework,
namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we
first propose evidential disentanglement learning to decompose each view into
clean and adversarial parts under the guidance of corresponding evidences,
which is extracted by a pretrained evidence extractor. Then, we employ the
feature recalibration module to mitigate the negative impact of adversarial
perturbations and extract potential informative features from them. Finally, to
further ignore the irreparable adversarial interferences, a view-level
evidential attention mechanism is designed. Extensive experiments on multi-view
classification tasks with adversarial attacks show that RDML outperforms the
state-of-the-art methods by a relatively large margin. Our code is available at
https://github.com/Willy1005/2025-IJCAI-RDML.

</details>


### [683] [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370)
*Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward Suh, Prateek Mittal*

Main category: cs.LG

TL;DR: Thinking Intervention enhances LLMs by guiding reasoning steps, improving performance in tasks like instruction following and safety alignment.


<details>
  <summary>Details</summary>
Motivation: To leverage intermediate reasoning steps in LLMs for finer control over model behavior.

Method: Proposes Thinking Intervention, a paradigm to guide reasoning by inserting or revising specific thinking tokens.

Result: Significant improvements: 6.7% accuracy in instruction following, 15.4% in instruction hierarchies, 40.0% refusal rate for unsafe prompts.

Conclusion: Thinking Intervention offers a promising approach for controlling reasoning in LLMs.

Abstract: Reasoning-enhanced large language models (LLMs) explicitly generate
intermediate reasoning steps prior to generating final answers, helping the
model excel in complex problem-solving. In this paper, we demonstrate that this
emerging generation framework offers a unique opportunity for more fine-grained
control over model behavior. We propose Thinking Intervention, a novel paradigm
designed to explicitly guide the internal reasoning processes of LLMs by
strategically inserting or revising specific thinking tokens. We find that the
Thinking Intervention paradigm enhances the capabilities of reasoning models
across a wide range of tasks, including instruction following on IFEval and
Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and
SorryBench. Our results demonstrate that Thinking Intervention significantly
outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains
in instruction-following scenarios, 15.4% improvements in reasoning about
instruction hierarchies, and a 40.0% increase in refusal rates for unsafe
prompts using open-source DeepSeek R1 models. Overall, our work opens a
promising new research avenue for controlling reasoning LLMs.

</details>


### [684] [Plain Transformers Can be Powerful Graph Learners](https://arxiv.org/pdf/2504.12588)
*Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Philip H. S. Torr, Mark Coates*

Main category: cs.LG

TL;DR: PPGT shows plain Transformers can excel in graph learning with three simple modifications: simplified $L_2$ attention, adaptive normalization, and MLP-based positional encoding.


<details>
  <summary>Details</summary>
Motivation: To prove plain Transformers can be effective for graph learning without complex architectural changes, enabling easier adoption of training advances.

Method: Three minimal modifications: (1) simplified $L_2$ attention, (2) adaptive root-mean-square normalization, (3) MLP-based positional encoding.

Result: PPGT outperforms complex competitors like subgraph GNNs and higher-order GNNs in empirical benchmarks and real-world datasets.

Conclusion: Plain Transformers, with minor tweaks, are powerful graph learners, simplifying adoption and maintaining performance.

Abstract: Transformers have attained outstanding performance across various modalities,
owing to their simple but powerful scaled-dot-product (SDP) attention
mechanisms. Researchers have attempted to migrate Transformers to graph
learning, but most advanced Graph Transformers (GTs) have strayed far from
plain Transformers, exhibiting major architectural differences either by
integrating message-passing or incorporating sophisticated attention
mechanisms. These divergences hinder the easy adoption of training advances for
Transformers developed in other domains. Contrary to previous GTs, this work
demonstrates that the plain Transformer architecture can be a powerful graph
learner. To achieve this, we propose to incorporate three simple, minimal, and
easy-to-implement modifications to the plain Transformer architecture to
construct our Powerful Plain Graph Transformers (PPGT): (1) simplified $L_2$
attention for measuring the magnitude closeness among tokens; (2) adaptive
root-mean-square normalization to preserve token magnitude information; and (3)
a simple MLP-based stem for graph positional encoding. Consistent with its
theoretical expressivity, PPGT demonstrates noteworthy realized expressivity on
the empirical graph expressivity benchmark, comparing favorably to more
complicated competitors such as subgraph GNNs and higher-order GNNs. Its
outstanding empirical performance across various graph datasets also justifies
the practical effectiveness of PPGT.

</details>


### [685] [Structural Inference: Interpreting Small Language Models with Susceptibilities](https://arxiv.org/pdf/2504.18274)
*Garrett Baker, George Wang, Jesse Hoogland, Daniel Murfet*

Main category: cs.LG

TL;DR: A linear response framework for interpretability treats neural networks as Bayesian systems, measuring changes in posterior expectations due to data perturbations.


<details>
  <summary>Details</summary>
Motivation: To provide interpretability by analyzing how small data perturbations affect neural network components.

Method: Uses a Bayesian statistical mechanical approach, estimating susceptibilities with local SGLD samples and token-level attributions.

Result: Identifies functional modules (e.g., multigram and induction heads) in a transformer via low-rank response matrices.

Conclusion: The framework offers efficient, interpretable analysis of neural network behavior under data shifts.

Abstract: We develop a linear response framework for interpretability that treats a
neural network as a Bayesian statistical mechanical system. A small
perturbation of the data distribution, for example shifting the Pile toward
GitHub or legal text, induces a first-order change in the posterior expectation
of an observable localized on a chosen component of the network. The resulting
susceptibility can be estimated efficiently with local SGLD samples and
factorizes into signed, per-token contributions that serve as attribution
scores. We combine these susceptibilities into a response matrix whose low-rank
structure separates functional modules such as multigram and induction heads in
a 3M-parameter transformer.

</details>


### [686] [PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking](https://arxiv.org/pdf/2505.01700)
*Yize Jiang, Xinze Li, Yuanyuan Zhang, Jin Han, Youjun Xu, Ayush Pandit, Zaixi Zhang, Mengdi Wang, Mengyang Wang, Chong Liu, Guang Yang, Yejin Choi, Wu-Jun Li, Tianfan Fu, Fang Wu, Junhong Liu*

Main category: cs.LG

TL;DR: PoseX is an open-source benchmark for evaluating protein-ligand docking methods, covering self-docking and cross-docking, with a curated dataset, 23 docking methods, and post-processing relaxation. AI methods outperform physics-based ones, and combining AI with physics-based post-processing improves results.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of self-docking in real applications and the inefficiency of heavy frameworks in existing docking studies.

Method: Curated a dataset (718 self-docking, 1,312 cross-docking entries), incorporated 23 docking methods (physics-based, AI docking, AI co-folding), developed a relaxation method for post-processing, and built a leaderboard.

Result: AI methods outperformed physics-based ones; relaxation alleviated clashes; AI co-folding had chirality issues (except Boltz-1x); specifying binding pockets improved performance.

Conclusion: Combining AI modeling with physics-based post-processing yields excellent performance, and leveraging pocket information enhances docking, especially for AI co-folding methods.

Abstract: Existing protein-ligand docking studies typically focus on the self-docking
scenario, which is less practical in real applications. Moreover, some studies
involve heavy frameworks requiring extensive training, posing challenges for
convenient and efficient assessment of docking methods. To fill these gaps, we
design PoseX, an open-source benchmark to evaluate both self-docking and
cross-docking, enabling a practical and comprehensive assessment of algorithmic
advances. Specifically, we curated a novel dataset comprising 718 entries for
self-docking and 1,312 entries for cross-docking; second, we incorporated 23
docking methods in three methodological categories, including physics-based
methods (e.g., Schr\"odinger Glide), AI docking methods (e.g., DiffDock) and AI
co-folding methods (e.g., AlphaFold3); third, we developed a relaxation method
for post-processing to minimize conformational energy and refine binding poses;
fourth, we built a leaderboard to rank submitted models in real-time. We
derived some key insights and conclusions from extensive experiments: (1) AI
approaches have consistently outperformed physics-based methods in overall
docking success rate. (2) Most intra- and intermolecular clashes of AI
approaches can be greatly alleviated with relaxation, which means combining AI
modeling with physics-based post-processing could achieve excellent
performance. (3) AI co-folding methods exhibit ligand chirality issues, except
for Boltz-1x, which introduced physics-inspired potentials to fix
hallucinations, suggesting modeling on stereochemistry improves the structural
plausibility markedly. (4) Specifying binding pockets significantly promotes
docking performance, indicating that pocket information can be leveraged
adequately, particularly for AI co-folding methods, in future modeling efforts.
The code, dataset, and leaderboard are released at
https://github.com/CataAI/PoseX.

</details>


### [687] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/pdf/2505.03230)
*Yue Chen, Hui Kang, Jiahui Li, Geng Sun, Boxiong Wang, Jiacheng Wang, Cong Liang, Shuang Liang, Dusit Niyato*

Main category: cs.LG

TL;DR: A UAV-assisted MEC system with directional antennas is proposed for SWIPT in 6G IoT networks, addressing energy and computational trade-offs via a bi-objective optimization and an improved SAC algorithm.


<details>
  <summary>Details</summary>
Motivation: Challenges in SWIPT for 6G IoT in remote/disaster areas with no ground infrastructure motivate the need for UAV-assisted solutions.

Method: Formulates a bi-objective optimization problem as an MDP and solves it using an improved SAC algorithm with action simplification.

Result: Outperforms baselines in energy efficiency and computational performance, with strong generalization in complex environments.

Conclusion: The proposed method effectively balances energy and computational needs, validated by simulations.

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [688] [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/pdf/2505.05315)
*Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, Caiming Xiong*

Main category: cs.LG

TL;DR: Elastic Reasoning is a framework for scalable chain-of-thought reasoning, splitting it into thinking and solution phases with independent budgets, improving reliability under tight constraints.


<details>
  <summary>Details</summary>
Motivation: LRMs' uncontrolled output lengths hinder real-world deployment due to strict inference-time budgets.

Method: Separates reasoning into thinking and solution phases with budget allocation, uses budget-constrained rollout strategy for training.

Result: Robust performance under strict constraints, lower training cost, and more concise reasoning even in unconstrained settings.

Conclusion: Elastic Reasoning offers a scalable, efficient solution for constrained reasoning tasks.

Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex
tasks by generating extended chains of thought (CoT). However, their
uncontrolled output lengths pose significant challenges for real-world
deployment, where inference-time budgets on tokens, latency, or compute are
strictly constrained. We propose Elastic Reasoning, a novel framework for
scalable chain of thoughts that explicitly separates reasoning into two
phases--thinking and solution--with independently allocated budgets. At test
time, Elastic Reasoning prioritizes the completeness of solution segments,
significantly improving reliability under tight resource constraints. To train
models that are robust to truncated thinking, we introduce a lightweight
budget-constrained rollout strategy, integrated into GRPO, which teaches the
model to reason adaptively when the thinking process is cut short and
generalizes effectively to unseen budget constraints without additional
training. Empirical results on mathematical (AIME, MATH500) and programming
(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning
performs robustly under strict budget constraints, while incurring
significantly lower training cost than baseline methods. Remarkably, our
approach also produces more concise and efficient reasoning even in
unconstrained settings. Our code has been made available at
https://github.com/SalesforceAIResearch/Elastic-Reasoning.

</details>


### [689] [PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of Neural Networks](https://arxiv.org/pdf/2505.06520)
*Xuran Li, Jingyi Wang, Xiaohan Yuan, Peixin Zhang, Zhan Qin, Zhibo Wang, Kui Ren*

Main category: cs.LG

TL;DR: A novel neural network unlearning method uses lightweight 'patches' to forget specific data, offering efficiency and verifiability.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for data removal (right to be forgotten) without costly retraining or verification challenges.

Method: Strategic application of minimal patches to the model for targeted forgetting, with iterative selection for larger data sets.

Result: Effective unlearning with preserved model performance, competitive efficiency, and low memory use.

Conclusion: The approach provides a practical, certifiable solution for data removal in neural networks.

Abstract: It is often desirable to remove (a.k.a. unlearn) a specific part of the
training data from a trained neural network model. A typical application
scenario is to protect the data holder's right to be forgotten, which has been
promoted by many recent regulation rules. Existing unlearning methods involve
training alternative models with remaining data, which may be costly and
challenging to verify from the data holder or a thirdparty auditor's
perspective. In this work, we provide a new angle and propose a novel
unlearning approach by imposing carefully crafted "patch" on the original
neural network to achieve targeted "forgetting" of the requested data to
delete. Specifically, inspired by the research line of neural network repair,
we propose to strategically seek a lightweight minimum "patch" for unlearning a
given data point with certifiable guarantee. Furthermore, to unlearn a
considerable amount of data points (or an entire class), we propose to
iteratively select a small subset of representative data points to unlearn,
which achieves the effect of unlearning the whole set. Extensive experiments on
multiple categorical datasets demonstrates our approach's effectiveness,
achieving measurable unlearning while preserving the model's performance and
being competitive in efficiency and memory consumption compared to various
baseline methods.

</details>


### [690] [FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers](https://arxiv.org/pdf/2505.06858)
*Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Zhenzhe Zhang, Tianchen Zhu, Shanghang Zhang, Jianxin Li*

Main category: cs.LG

TL;DR: FreqMoE improves Fourier Neural Operators (FNO) by focusing on low-frequency signals first, then extending to high frequencies, achieving higher accuracy with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and signal loss in high-frequency regions of FNOs for solving PDEs.

Method: Progressive training framework (FreqMoE) that learns low-frequency weights first, then uses a sparse upward-cycling strategy to extend to high frequencies.

Result: Up to 16.6% accuracy improvement with 47.32x parameter reduction; stable long-term predictions and generalization.

Conclusion: FreqMoE establishes a new paradigm for solving PDEs by pretraining on low frequencies and fine-tuning on high frequencies.

Abstract: Fourier Neural Operators (FNO) have emerged as promising solutions for
efficiently solving partial differential equations (PDEs) by learning
infinite-dimensional function mappings through frequency domain
transformations. However, the sparsity of high-frequency signals limits
computational efficiency for high-dimensional inputs, and fixed-pattern
truncation often causes high-frequency signal loss, reducing performance in
scenarios such as high-resolution inputs or long-term predictions. To address
these challenges, we propose FreqMoE, an efficient and progressive training
framework that exploits the dependency of high-frequency signals on
low-frequency components. The model first learns low-frequency weights and then
applies a sparse upward-cycling strategy to construct a mixture of experts
(MoE) in the frequency domain, effectively extending the learned weights to
high-frequency regions. Experiments on both regular and irregular grid PDEs
demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using
merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,
the approach demonstrates remarkable stability in long-term predictions and
generalizes seamlessly to various FNO variants and grid structures,
establishing a new ``Low frequency Pretraining, High frequency Fine-tuning''
paradigm for solving PDEs.

</details>


### [691] [Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL](https://arxiv.org/pdf/2505.08179)
*Zhikun Tao, Gang Xiong, He Fang, Zhen Shen, Yunjun Han, Qing-Shan Jia*

Main category: cs.LG

TL;DR: FASP is a novel offline safe RL framework addressing long-horizon safety and OOD challenges using H-J reachability and pessimistic Q-value estimation.


<details>
  <summary>Details</summary>
Motivation: Existing OSRL methods lack long-term safety guarantees and struggle with OOD states/actions, limiting real-world applicability.

Method: Uses H-J reachability for safety labels, trains a CVAE and safety classifier, and employs pessimistic Q-value estimation to avoid unsafe actions.

Result: FASP outperforms state-of-the-art algorithms in safety on DSRL benchmarks.

Conclusion: FASP ensures long-horizon safety and handles OOD challenges effectively, making it suitable for safety-critical applications.

Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying
policies from pre-collected datasets, offers a promising avenue for deploying
RL in safety-critical real-world domains such as robotics. However, the
majority of existing approaches emphasize only short-term safety, neglecting
long-horizon considerations. Consequently, they may violate safety constraints
and fail to ensure sustained protection during online deployment. Moreover, the
learned policies often struggle to handle states and actions that are not
present or out-of-distribution(OOD) from the offline dataset, and exhibit
limited sample efficiency. To address these challenges, we propose a novel
framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based
Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis
to generate reliable safety labels, which serve as supervisory signals for
training both a conditional variational autoencoder (CVAE) and a safety
classifier. This approach not only ensures high sampling efficiency but also
provides rigorous long-horizon safety guarantees. Furthermore, we utilize
pessimistic estimation methods to estimate the Q-value of reward and cost,
which mitigates the extrapolation errors induces by OOD actions, and penalize
unsafe actions to enabled the agent to proactively avoid high-risk behaviors.
Moreover, we theoretically prove the validity of this pessimistic estimation.
Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm
achieves competitive performance across multiple experimental tasks,
particularly outperforming state-of-the-art algorithms in terms of safety.

</details>


### [692] [Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning](https://arxiv.org/pdf/2505.07527)
*Hu Wang, Congbo Ma, Ian Reid, Mohammad Yaqub*

Main category: cs.LG

TL;DR: KRPO enhances GRPO by using Kalman filtering for adaptive advantage normalization, improving stability and performance in noisy reward environments.


<details>
  <summary>Details</summary>
Motivation: GRPO's naive batch mean baseline can introduce bias in noisy reward settings, motivating the need for a more adaptive approach.

Method: KRPO employs lightweight Kalman filtering to dynamically estimate reward mean and variance, replacing GRPO's batch mean baseline.

Result: KRPO improves stability and performance in math question answering and reasoning tasks, outperforming GRPO.

Conclusion: KRPO offers a simple, effective enhancement to GRPO, adapting better to dynamic reward signals without additional parameters.

Abstract: Reward baseline is important for Reinforcement Learning (RL) algorithms to
reduce variance in policy gradient estimates. Recently, for language modeling,
Group Relative Policy Optimization (GRPO) is proposed to compute the advantage
for each output by subtracting the mean reward, as the baseline, for all
outputs in the group. However, it can lead to inaccurate advantage estimates in
environments with highly noisy rewards, potentially introducing bias. In this
work, we propose a model, called Kalman Filter Enhanced Group Relative Policy
Optimization (KRPO), by using lightweight Kalman filtering to dynamically
estimate the latent reward mean and variance. This filtering technique replaces
the naive batch mean baseline, enabling more adaptive advantage normalization.
Our method does not require additional learned parameters over GRPO. This
approach offers a simple yet effective way to incorporate multiple outputs of
GRPO into advantage estimation, improving policy optimization in settings where
highly dynamic reward signals are difficult to model for language models.
Through accuracy and rewards obtained from math question answering and
reasoning, we show that using a more adaptive advantage estimation model, KRPO
can improve the stability and performance of GRPO. The code is available at
https://github.com/billhhh/KRPO_LLMs_RL.

</details>


### [693] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/pdf/2505.09847)
*Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei*

Main category: cs.LG

TL;DR: A principled approach called Causal Predictive Optimization and Generation (CPOG) is introduced for sales optimization, combining causal ML, constraint optimization, contextual bandit, and Generative AI, with proven success at LinkedIn.


<details>
  <summary>Details</summary>
Motivation: Optimizing the sales process is crucial for B2B success, requiring a systematic and AI-driven approach.

Method: CPOG involves three layers: causal ML for prediction, constraint optimization and contextual bandit for optimization, and Generative AI with feedback for serving.

Result: The system demonstrated significant improvements over legacy systems at LinkedIn.

Conclusion: CPOG offers a scalable and effective framework for sales optimization, with broad applicability in business AI.

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [694] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/pdf/2505.10259)
*Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload improves LLM inference efficiency on resource-constrained devices by integrating speculative decoding into offloading, boosting GPU utilization and throughput.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in GPU utilization and memory impact during LLM inference on devices with limited resources.

Method: Proposes SpecOffload, which embeds speculative decoding into offloading, using latent GPU resources for a draft model and optimizing tensor placement.

Result: Achieves 4.49x higher GPU core utilization and 2.54x better inference throughput compared to baselines.

Conclusion: SpecOffload effectively enhances inference efficiency with minimal additional cost, offering a practical solution for resource-constrained devices.

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload-public .

</details>


### [695] [Training NTK to Generalize with KARE](https://arxiv.org/pdf/2505.11347)
*Johannes Schwab, Bryan Kelly, Semyon Malamud, Teng Andrea Xu*

Main category: cs.LG

TL;DR: Explicitly training the Neural Tangent Kernel (NTK) using Kernel Alignment Risk Estimator (KARE) outperforms traditional DNN training and DNN-induced NTK, suggesting a shift from implicit to explicit kernel learning.


<details>
  <summary>Details</summary>
Motivation: The NTK of a trained DNN often matches or exceeds the DNN's performance, implying implicit kernel learning during training. This paper explores explicit NTK optimization to improve generalization.

Method: Proposes optimizing the NTK explicitly using KARE to minimize generalization error, instead of traditional empirical risk minimization in DNNs.

Result: Simulations and experiments show NTKs trained with KARE outperform original DNNs and DNN-induced NTKs, challenging DNN dominance.

Conclusion: Explicit NTK training is a form of over-parametrized feature learning, offering a viable alternative to traditional DNN optimization.

Abstract: The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.

</details>


### [696] [Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning](https://arxiv.org/pdf/2505.11578)
*Peimian Du, Jiabin Liu, Xiaowei Jin, Mengwang Zuo, Hui Li*

Main category: cs.LG

TL;DR: The paper introduces HMT-PF, a hybrid Mamba-Transformer model for spatiotemporal physical field generation, using physics-informed fine-tuning to reduce equation discrepancies.


<details>
  <summary>Details</summary>
Motivation: Addressing physical equation discrepancies in data-driven models for spatiotemporal field generation.

Method: Develops HMT-PF with a hybrid Mamba-Transformer architecture and physics-informed fine-tuning, using unstructured grid inputs and self-supervised learning.

Result: The model effectively reduces physical errors and maintains field characteristics, validated by MSE-R evaluation.

Conclusion: HMT-PF successfully improves physical consistency in spatiotemporal field generation.

Abstract: This research confronts the challenge of substantial physical equation
discrepancies encountered in the generation of spatiotemporal physical fields
through data-driven trained models. A spatiotemporal physical field generation
model, named HMT-PF, is developed based on the hybrid Mamba-Transformer
architecture, incorporating unstructured grid information as input. A
fine-tuning block, enhanced with physical information, is introduced to
effectively reduce the physical equation discrepancies. The physical equation
residuals are computed through a point query mechanism for efficient gradient
evaluation, then encoded into latent space for refinement. The fine-tuning
process employs a self-supervised learning approach to achieve physical
consistency while maintaining essential field characteristics. Results show
that the hybrid Mamba-Transformer model achieves good performance in generating
spatiotemporal fields, while the physics-informed fine-tuning mechanism further
reduces significant physical errors effectively. A MSE-R evaluation method is
developed to assess the accuracy and realism of physical field generation.

</details>


### [697] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2505.11415)
*Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai*

Main category: cs.LG

TL;DR: MoE-CAP is a benchmark for MoE systems, highlighting the trade-offs between Cost, Accuracy, and Performance (CAP) due to hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the CAP trade-offs in MoE systems, complicating deployment decisions.

Method: Introduces MoE-CAP benchmark, CAP Radar Diagram, and sparsity-aware metrics (S-MBU, S-MFU) for accurate performance benchmarking.

Result: Optimal CAP balance is hard; MoE systems often sacrifice one dimension for the other two.

Conclusion: MoE-CAP and new metrics aid in better benchmarking and deployment decisions for MoE systems.

Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [698] [Multi-head Temporal Latent Attention](https://arxiv.org/pdf/2505.13544)
*Keqi Deng, Philip C. Woodland*

Main category: cs.LG

TL;DR: MTLA reduces KV cache size in Transformer self-attention by compressing it along the temporal dimension, improving inference speed and memory usage without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The linear growth of the KV cache with sequence length in Transformer self-attention hinders inference efficiency, prompting the need for compression methods.

Method: MTLA uses a hyper-network to merge temporally adjacent KV cache vectors and introduces a stride-aware causal mask for training-inference consistency.

Result: MTLA achieves competitive performance across tasks (e.g., 5.3x speedup and 8.3x memory reduction in speech translation) compared to standard MHA.

Conclusion: MTLA effectively addresses KV cache inefficiency, offering significant improvements in inference speed and memory usage while maintaining task performance.

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [699] [UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models](https://arxiv.org/pdf/2505.11654)
*Yuhang Liu, Yingxue Zhang, Xin Zhang, Ling Tian, Yanhua Li, Jun Luo*

Main category: cs.LG

TL;DR: UrbanMind is a spatial-temporal LLM framework for urban dynamics prediction, combining multifaceted fusion masking, semantic-aware prompting, and test time adaptation to achieve high accuracy and robust generalization.


<details>
  <summary>Details</summary>
Motivation: Existing neural network and LLM-based methods lack generalization and struggle with multifaceted spatial-temporal data integration and distributional shifts.

Method: UrbanMind uses Muffin-MAE (a masked autoencoder with specialized masking), semantic-aware prompting, and a test time adaptation mechanism.

Result: Outperforms state-of-the-art baselines in accuracy and generalization, even in zero-shot settings.

Conclusion: UrbanMind bridges the gap in urban dynamics prediction, offering reliable and generalizable solutions.

Abstract: Understanding and predicting urban dynamics is crucial for managing
transportation systems, optimizing urban planning, and enhancing public
services. While neural network-based approaches have achieved success, they
often rely on task-specific architectures and large volumes of data, limiting
their ability to generalize across diverse urban scenarios. Meanwhile, Large
Language Models (LLMs) offer strong reasoning and generalization capabilities,
yet their application to spatial-temporal urban dynamics remains underexplored.
Existing LLM-based methods struggle to effectively integrate multifaceted
spatial-temporal data and fail to address distributional shifts between
training and testing data, limiting their predictive reliability in real-world
applications. To bridge this gap, we propose UrbanMind, a novel
spatial-temporal LLM framework for multifaceted urban dynamics prediction that
ensures both accurate forecasting and robust generalization. At its core,
UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with
specialized masking strategies that capture intricate spatial-temporal
dependencies and intercorrelations among multifaceted urban dynamics.
Additionally, we design a semantic-aware prompting and fine-tuning strategy
that encodes spatial-temporal contextual details into prompts, enhancing LLMs'
ability to reason over spatial-temporal patterns. To further improve
generalization, we introduce a test time adaptation mechanism with a test data
reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by
reconstructing LLM-generated embeddings. Extensive experiments on real-world
urban datasets across multiple cities demonstrate that UrbanMind consistently
outperforms state-of-the-art baselines, achieving high accuracy and robust
generalization, even in zero-shot settings.

</details>


### [700] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/pdf/2505.13938)
*Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri*

Main category: cs.LG

TL;DR: ${\rm C{\small LEVER}}$ is a benchmark for verified code generation in Lean, featuring 161 problems with verified specifications and implementations. It avoids common pitfalls like test-case supervision and leaky specifications, and evaluates state-of-the-art methods, which struggle with full verification.


<details>
  <summary>Details</summary>
Motivation: To provide a high-quality, curated benchmark for end-to-end verified code generation in Lean, addressing limitations of prior benchmarks like test-case supervision and leaky specifications.

Method: The benchmark consists of 161 problems, each requiring generating a specification and a Lean implementation that satisfies it. Outputs are verified using Lean's type checker. Several few-shot and agentic approaches based on state-of-the-art language models are evaluated.

Result: State-of-the-art methods struggle to achieve full verification, highlighting the benchmark's challenge for program synthesis and formal reasoning.

Conclusion: ${\rm C{\small LEVER}}$ establishes a challenging frontier for verified code generation, with potential for advancing program synthesis and formal reasoning.

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [701] [LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models](https://arxiv.org/pdf/2505.11772)
*Ryan Chen, Youngmin Ko, Zeyu Zhang, Catherine Cho, Sunny Chung, Mauro Giuffré, Dennis L. Shung, Bradly C. Stadie*

Main category: cs.LG

TL;DR: LAMP (Linear Attribution Mapping Probe) is a method to analyze black-box language models by fitting a locally linear surrogate to their self-reported explanations, revealing how these explanations influence predictions.


<details>
  <summary>Details</summary>
Motivation: To understand how reliably language models map their stated reasons to predictions and to audit proprietary models without needing internal access.

Method: LAMP treats the model's self-reported explanations as a coordinate system and fits a locally linear surrogate to link these to the model's output.

Result: LAMP shows that many LLMs have locally linear decision landscapes, correlating with human judgments on explanation quality and expert assessments.

Conclusion: LAMP is a practical, lightweight framework for auditing proprietary models and assessing consistency between model behavior and explanations.

Abstract: We introduce LAMP (Linear Attribution Mapping Probe), a method that shines
light onto a black-box language model's decision surface and studies how
reliably a model maps its stated reasons to its predictions through a locally
linear model approximating the decision surface. LAMP treats the model's own
self-reported explanations as a coordinate system and fits a locally linear
surrogate that links those weights to the model's output. By doing so, it
reveals which stated factors steer the model's decisions, and by how much. We
apply LAMP to three tasks: sentiment analysis, controversial-topic detection,
and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs
exhibit locally linear decision landscapes. In addition, these surfaces
correlate with human judgments on explanation quality and, on a clinical
case-file data set, aligns with expert assessments. Since LAMP operates without
requiring access to model gradients, logits, or internal activations, it serves
as a practical and lightweight framework for auditing proprietary language
models, and enabling assessment of whether a model behaves consistently with
the explanations it provides.

</details>


### [702] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/pdf/2505.13989)
*Yanzhe Wen, Xunkai Li, Qi Zhang, Zhu Lei, Guang Zeng, Rong-Hua Li, Guoren Wang*

Main category: cs.LG

TL;DR: OGA is an LLM-based framework for TAG learning, addressing data uncertainty in open-world scenarios by combining adaptive label traceability and a graph label annotator.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle data uncertainty in open-world TAG learning, particularly with limited labeling and unknown-class nodes.

Method: OGA integrates adaptive label traceability (semantics + topology) for unknown-class rejection and includes a graph label annotator for model updates.

Result: Experiments show OGA is effective and practical.

Conclusion: OGA successfully addresses limitations in open-world TAG learning by leveraging LLMs and adaptive techniques.

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [703] [Embedding principle of homogeneous neural network for classification problem](https://arxiv.org/pdf/2505.12419)
*Jiahan Zhang, Yaoyu Zhang, Tao Luo*

Main category: cs.LG

TL;DR: The paper explores how KKT points in homogeneous neural networks relate across different widths via neuron splitting, proving an embedding principle and linking it to gradient flow dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the structural connections and optimization landscape of homogeneous neural networks, especially how KKT points behave under neuron splitting.

Method: Introduces the KKT point embedding principle, proving it for two-layer and deep networks, and connects it to gradient flow training dynamics.

Result: KKT points of smaller networks can be embedded into larger ones via neuron splitting, and gradient flow preserves this alignment dynamically.

Conclusion: The findings provide insights into network width, parameter redundancy, and solution structures in homogeneous networks of varying sizes.

Abstract: Understanding the convergence points and optimization landscape of neural
networks is crucial, particularly for homogeneous networks where
Karush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often
characterize solutions. This paper investigates the relationship between such
KKT points across networks of different widths generated via neuron splitting.
We introduce and formalize the \textbf{KKT point embedding principle},
establishing that KKT points of a homogeneous network's max-margin problem
($P_{\Phi}$) can be embedded into the KKT points of a larger network's problem
($P_{\tilde{\Phi}}$) via specific linear isometric transformations
corresponding to neuron splitting. We rigorously prove this principle holds for
neuron splitting in both two-layer and deep homogeneous networks. Furthermore,
we connect this static embedding to the dynamics of gradient flow training with
smooth losses. We demonstrate that trajectories initiated from appropriately
mapped points remain mapped throughout training and that the resulting
$\omega$-limit sets of directions are correspondingly mapped ($T(L(\theta(0)))
= L(\boldsymbol{\eta}(0))$), thereby preserving the alignment with KKT
directions dynamically when directional convergence occurs. Our findings offer
insights into the effects of network width, parameter redundancy, and the
structural connections between solutions found via optimization in homogeneous
networks of varying sizes.

</details>


### [704] [A Path to Universal Neural Cellular Automata](https://arxiv.org/pdf/2505.13058)
*Gabriel Béna, Maxence Faldor, Dan F. M. Goodman, Antoine Cully*

Main category: cs.LG

TL;DR: The paper explores neural cellular automata's potential for universal computation in continuous domains, demonstrating success in training computational primitives and emulating a neural network for MNIST classification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether continuous neural cellular automata can achieve universal computation, leveraging gradient descent for rule learning.

Method: Introduces a model, objective functions, and training strategies to guide neural cellular automata toward universal computation in continuous settings.

Result: Successfully trained computational primitives (e.g., matrix operations) and emulated a neural network for MNIST classification within the automata state.

Conclusion: A foundational step toward analog general-purpose computers, with implications for understanding universal computation and automating complex behavior discovery.

Abstract: Cellular automata have long been celebrated for their ability to generate
complex behaviors from simple, local rules, with well-known discrete models
like Conway's Game of Life proven capable of universal computation. Recent
advancements have extended cellular automata into continuous domains, raising
the question of whether these systems retain the capacity for universal
computation. In parallel, neural cellular automata have emerged as a powerful
paradigm where rules are learned via gradient descent rather than manually
designed. This work explores the potential of neural cellular automata to
develop a continuous Universal Cellular Automaton through training by gradient
descent. We introduce a cellular automaton model, objective functions and
training strategies to guide neural cellular automata toward universal
computation in a continuous setting. Our experiments demonstrate the successful
training of fundamental computational primitives - such as matrix
multiplication and transposition - culminating in the emulation of a neural
network solving the MNIST digit classification task directly within the
cellular automata state. These results represent a foundational step toward
realizing analog general-purpose computers, with implications for understanding
universal computation in continuous dynamics and advancing the automated
discovery of complex cellular automata behaviors via machine learning.

</details>


### [705] [Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning](https://arxiv.org/pdf/2505.13317)
*Song-Lin Lv, Rui Zhu, Yu-Feng Li, Lan-Zhe Guo*

Main category: cs.LG

TL;DR: The paper compares semi-supervised learning (SSL) and pre-trained models (VLMs) in low-labeled-data scenarios, finding VLMs generally outperform SSL except in low-resolution or unstructured data. It proposes Few-shot SSL for fair comparison and suggests integrating pre-trained knowledge into SSL.


<details>
  <summary>Details</summary>
Motivation: To determine whether to use unlabeled data (SSL) or pre-trained models (VLMs) when labeled data is scarce in target tasks.

Method: Proposes Few-shot SSL, a framework for fair comparison between SSL and VLMs by controlling labeled data. Uses pre-trained Vision-Language Models (VLMs) as representatives.

Result: VLMs outperform SSL in most cases, except for low-resolution or semantically unclear data.

Conclusion: Future SSL research should compare with pre-trained models and explore integration (e.g., enhancing pseudo-labeling). A unified framework is released for reproducibility.

Abstract: Semi-supervised learning (SSL) alleviates the cost of data labeling process
by exploiting unlabeled data, and has achieved promising results on various
tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm
has garnered significant attention in recent years, and exploiting pre-trained
models could also reduce the requirement of labeled data in downstream tasks.
Therefore, a question naturally occurs: \emph{When the labeled data is scarce
in the target tasks, should we exploit unlabeled data or pre-trained models?}
To answer this question, we select pre-trained Vision-Language Models (VLMs) as
representative pretrain-finetuning instances and propose \textit{Few-shot SSL}
-- a framework that enables fair comparison between these two paradigms by
controlling the amount of labeled data used. Extensive experiments across
various settings demonstrate that pre-trained VLMs generally outperform SSL
methods in nearly all cases, except when the data has low resolution or lacks
clear semantic structure. Therefore, we encourage future SSL research to
compare with pre-trained models and explore deeper integration, such as using
pre-trained knowledge to enhance pseudo-labeling. To support future research,
we release our unified reproduction and evaluation framework. Codes are
available
\href{https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566
}{here}.

</details>


### [706] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/pdf/2505.14170)
*Chen Zhang, Weixin Bu, Zeyi Ren, Zhengwu Liu, Yik-Chung Wu, Ngai Wong*

Main category: cs.LG

TL;DR: GraNT improves GCN training efficiency by selecting optimal graph-property pairs, reducing training time significantly without losing generalization.


<details>
  <summary>Details</summary>
Motivation: Learning graph properties is costly for GCNs; GraNT aims to enhance efficiency by reinterpreting learning through nonparametric teaching.

Method: GraNT selects a subset of graph-property pairs to train GCNs, using functional gradient descent in a nonparametric framework.

Result: GraNT reduces training time by 36.62% to 47.30% across tasks while maintaining performance.

Conclusion: GraNT effectively speeds up GCN training by aligning teaching with nonparametric learning, offering practical efficiency gains.

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [707] [Object-centric Processes with Structured Data and Exact Synchronization (Extended Version)](https://arxiv.org/pdf/2505.15409)
*Alessandro Gianola, Marco Montali, Sarah Winkler*

Main category: cs.MA

TL;DR: The paper introduces Data-aware Object-centric Petri Nets with Identifiers (DOPIDs) to address limitations in existing process formalisms by incorporating structured data manipulation and synchronization.


<details>
  <summary>Details</summary>
Motivation: Existing formalisms fail to combine object identity tracking, complex datatypes, dependency handling, and synchronization. OPIDs partially address this but lack data value semantics.

Method: Extends OPIDs to DOPIDs with structured data manipulation and synchronization. Uses SMT for conformance checking and data-aware alignments.

Result: DOPIDs successfully integrate data values and synchronization, enabling operational use.

Conclusion: DOPIDs provide a more expressive and practical framework for modeling real-world processes with data-rich objects.

Abstract: Real-world processes often involve interdependent objects that also carry
data values, such as integers, reals, or strings. However, existing process
formalisms fall short to combine key modeling features, such as tracking object
identities, supporting complex datatypes, handling dependencies among them, and
object-aware synchronization. Object-centric Petri nets with identifiers
(OPIDs) partially address these needs but treat objects as unstructured
identifiers (e.g., order and item IDs), overlooking the rich semantics of
complex data values (e.g., item prices or other attributes). To overcome these
limitations, we introduce data-aware OPIDs (DOPIDs), a framework that strictly
extends OPIDs by incorporating structured data manipulation capabilities, and
full synchronization mechanisms. In spite of the expressiveness of the model,
we show that it can be made operational: Specifically, we define a novel
conformance checking approach leveraging satisfiability modulo theories (SMT)
to compute data-aware object-centric alignments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [708] [Relationship Analysis of Image-Text Pair in SNS Posts](https://arxiv.org/pdf/2505.15629)
*Takuto Nabeoka, Yijun Duan, Qiang Ma*

Main category: cs.MM

TL;DR: A graph-based method classifies image-text pairs in SNS into similar and complementary relationships using CLIP embeddings, clustering, and a GCN.


<details>
  <summary>Details</summary>
Motivation: To improve analysis of image-text relationships in SNS beyond just similarity, addressing prior limitations.

Method: Uses CLIP for embeddings, clustering, constructs an ITRC-Line Graph, and employs a GCN for classification.

Result: Demonstrates effectiveness on a public dataset.

Conclusion: The proposed method successfully classifies image-text pairs into distinct relationships.

Abstract: Social networking services (SNS) contain vast amounts of image-text posts,
necessitating effective analysis of their relationships for improved
information retrieval. This study addresses the classification of image-text
pairs in SNS, overcoming prior limitations in distinguishing relationships
beyond similarity. We propose a graph-based method to classify image-text pairs
into similar and complementary relationships. Our approach first embeds images
and text using CLIP, followed by clustering. Next, we construct an Image-Text
Relationship Clustering Line Graph (ITRC-Line Graph), where clusters serve as
nodes. Finally, edges and nodes are swapped in a pseudo-graph representation. A
Graph Convolutional Network (GCN) then learns node and edge representations,
which are fused with the original embeddings for final classification.
Experimental results on a publicly available dataset demonstrate the
effectiveness of our method.

</details>


### [709] [CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment](https://arxiv.org/pdf/2505.01237)
*Edson Araujo, Andrew Rouditchenko, Yuan Gong, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Leonid Karlinsky, Rogerio Feris, James R. Glass, Hilde Kuehne*

Main category: cs.MM

TL;DR: CAV-MAE Sync improves audio-visual learning by aligning audio temporally with video, separating contrastive and reconstruction objectives, and using register tokens for better localization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained temporal alignment and conflicting optimization goals in audio-visual learning.

Method: Proposes CAV-MAE Sync, treating audio as a temporal sequence, separating objectives with global tokens, and using register tokens for localization.

Result: Achieves state-of-the-art performance on AudioSet, VGG Sound, and ADE20K Sound for retrieval, classification, and localization.

Conclusion: CAV-MAE Sync is a simple yet effective extension of CAV-MAE, addressing key challenges and outperforming complex architectures.

Abstract: Recent advances in audio-visual learning have shown promising results in
learning representations across modalities. However, most approaches rely on
global audio representations that fail to capture fine-grained temporal
correspondences with visual frames. Additionally, existing methods often
struggle with conflicting optimization objectives when trying to jointly learn
reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync
as a simple yet effective extension of the original CAV-MAE framework for
self-supervised audio-visual learning. We address three key challenges: First,
we tackle the granularity mismatch between modalities by treating audio as a
temporal sequence aligned with video frames, rather than using global
representations. Second, we resolve conflicting optimization goals by
separating contrastive and reconstruction objectives through dedicated global
tokens. Third, we improve spatial localization by introducing learnable
register tokens that reduce semantic load on patch tokens. We evaluate the
proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on
zero-shot retrieval, classification and localization tasks demonstrating
state-of-the-art performance and outperforming more complex architectures.

</details>


### [710] [Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model](https://arxiv.org/pdf/2505.13062)
*Yong Ren, Chenxing Li, Le Xu, Hao Gu, Duzhen Zhang, Yujie Chen, Manjie Xu, Ruibo Fu, Shan Yang, Dong Yu*

Main category: cs.MM

TL;DR: The paper introduces the SVAD task to explore multimodal reasoning in VLMs for inferring audio from silent videos, proposing a CoT-based fine-tuning strategy to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current VT2A methods lack audio description capabilities during inference, prompting the need for exploring modal-mismatch reasoning in VLMs.

Method: A CoT-AudioCaps dataset is created, and a Chain-of-Thought-based supervised fine-tuning strategy is proposed to enhance VLMs' reasoning for SVAD.

Result: The method significantly improves VLMs' modal-mismatch reasoning for SVAD and addresses audio description challenges in VT2A tasks.

Conclusion: The proposed approach effectively enhances VLMs' reasoning for audio inference from silent videos, bridging gaps in current VT2A methods.

Abstract: Humans can intuitively infer sounds from silent videos, but whether
multimodal large language models can perform modal-mismatch reasoning without
accessing target modalities remains relatively unexplored. Current
text-assisted-video-to-audio (VT2A) methods excel in video foley tasks but
struggle to acquire audio descriptions during inference. We introduce the task
of Reasoning Audio Descriptions from Silent Videos (SVAD) to address this
challenge and investigate vision-language models' (VLMs) capabilities on this
task. To further enhance the VLMs' reasoning capacity for the SVAD task, we
construct a CoT-AudioCaps dataset and propose a Chain-of-Thought-based
supervised fine-tuning strategy. Experiments on SVAD and subsequent VT2A tasks
demonstrate our method's effectiveness in two key aspects: significantly
improving VLMs' modal-mismatch reasoning for SVAD and effectively addressing
the challenge of acquiring audio descriptions during VT2A inference.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [711] [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/pdf/2505.14723)
*Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam*

Main category: eess.AS

TL;DR: QUADS is a unified framework for Spoken Language Understanding (SLU) that combines distillation and quantization, achieving high efficiency and accuracy in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat distillation and quantization separately, leading to suboptimal compression. QUADS aims to unify these processes for better performance.

Method: QUADS uses multi-stage training with a pre-tuned model to optimize both distillation and quantization, enhancing adaptability to low-bit regimes.

Result: QUADS achieves 71.13% accuracy on SLURP and 99.20% on FSC, with minor accuracy loss (up to 5.56%) and significant efficiency gains (60-73x fewer GMACs, 83-700x smaller model size).

Conclusion: QUADS is a robust and efficient solution for real-world SLU applications, especially in resource-constrained environments.

Abstract: Spoken Language Understanding (SLU) systems must balance performance and
efficiency, particularly in resource-constrained environments. Existing methods
apply distillation and quantization separately, leading to suboptimal
compression as distillation ignores quantization constraints. We propose QUADS,
a unified framework that optimizes both through multi-stage training with a
pre-tuned model, enhancing adaptability to low-bit regimes while maintaining
accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with
only minor degradations of up to 5.56\% compared to state-of-the-art models.
Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and
model size by 83--700$\times$, demonstrating strong robustness under extreme
quantization. These results establish QUADS as a highly efficient solution for
real-world, resource-constrained SLU applications.

</details>


### [712] [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/pdf/2505.14910)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao*

Main category: eess.AS

TL;DR: TCSinger 2 improves multilingual zero-shot singing voice synthesis with style transfer and control, addressing boundary and prompt limitations of existing models.


<details>
  <summary>Details</summary>
Motivation: Existing SVS models rely heavily on annotations, lack robustness in zero-shot scenarios, and have poor transitions and style control.

Method: TCSinger 2 uses three modules: Blurred Boundary Content Encoder for smooth transitions, Custom Audio Encoder for aligned representations, and Flow-based Custom Transformer for quality and style.

Result: TCSinger 2 outperforms baselines in subjective and objective metrics across tasks.

Conclusion: TCSinger 2 advances multilingual zero-shot SVS with better transitions, style control, and synthesis quality.

Abstract: Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.

</details>


### [713] [EASY: Emotion-aware Speaker Anonymization via Factorized Distillation](https://arxiv.org/pdf/2505.15004)
*Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie*

Main category: eess.AS

TL;DR: EASY is an emotion-aware speaker anonymization framework that disentangles speaker identity, linguistic content, and emotional representation, outperforming baselines in privacy protection while preserving emotion and content.


<details>
  <summary>Details</summary>
Motivation: Existing speaker anonymization systems neglect emotional state preservation, focusing only on linguistic content and speaker identity.

Method: EASY uses sequential disentanglement and factorized distillation to model speaker identity, linguistic content, and emotional representation in distinct subspaces.

Result: EASY outperforms baseline systems on VoicePrivacy Challenge datasets, effectively protecting privacy while maintaining linguistic content and emotional state.

Conclusion: EASY successfully addresses the gap in emotion preservation in speaker anonymization, offering enhanced privacy without compromising emotional or linguistic fidelity.

Abstract: Emotion plays a significant role in speech interaction, conveyed through
tone, pitch, and rhythm, enabling the expression of feelings and intentions
beyond words to create a more personalized experience. However, most existing
speaker anonymization systems employ parallel disentanglement methods, which
only separate speech into linguistic content and speaker identity, often
neglecting the preservation of the original emotional state. In this study, we
introduce EASY, an emotion-aware speaker anonymization framework. EASY employs
a novel sequential disentanglement process to disentangle speaker identity,
linguistic content, and emotional representation, modeling each speech
attribute in distinct subspaces through a factorized distillation approach. By
independently constraining speaker identity and emotional representation, EASY
minimizes information leakage, enhancing privacy protection while preserving
original linguistic content and emotional state. Experimental results on the
VoicePrivacy Challenge official datasets demonstrate that our proposed approach
outperforms all baseline systems, effectively protecting speaker privacy while
maintaining linguistic content and emotional state.

</details>


### [714] [Towards Pre-training an Effective Respiratory Audio Foundation Model](https://arxiv.org/pdf/2505.15307)
*Daisuke Niizumi, Daiki Takeuchi, Masahiro Yasuda, Binh Thien Nguyen, Yasunori Ohishi, Noboru Harada*

Main category: eess.AS

TL;DR: The study explores better pre-training practices for respiratory sounds, finding general audio datasets (AudioSet) more effective than respiratory-specific ones. Combining datasets and preserving frequency-wise information improves performance, achieving a new state-of-the-art on the OPERA benchmark.


<details>
  <summary>Details</summary>
Motivation: To verify the effectiveness of conventional pre-training schemes on small, non-diverse respiratory audio datasets and improve respiratory audio foundation models.

Method: Comparison of various pre-trained audio models, including those pre-trained on AudioSet and respiratory sounds, and further pre-training by combining datasets while preserving frequency-wise information.

Result: AudioSet pre-trained models outperform respiratory-specific ones. Combining datasets and preserving frequency information enhances performance, setting a new state-of-the-art on the OPERA benchmark.

Conclusion: General audio datasets like AudioSet are more effective for pre-training respiratory audio models, and combining datasets with proper feature aggregation improves results, advancing respiratory audio foundation models.

Abstract: Recent advancements in foundation models have sparked interest in respiratory
audio foundation models. However, the effectiveness of applying conventional
pre-training schemes to datasets that are small-sized and lack diversity has
not been sufficiently verified. This study aims to explore better pre-training
practices for respiratory sounds by comparing numerous pre-trained audio
models. Our investigation reveals that models pre-trained on AudioSet, a
general audio dataset, are more effective than the models specifically
pre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory
sound datasets for further pre-training enhances performance, and preserving
the frequency-wise information when aggregating features is vital. Along with
more insights found in the experiments, we establish a new state-of-the-art for
the OPERA benchmark, contributing to advancing respiratory audio foundation
models. Our code is available online at
https://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.

</details>


### [715] [Analysis of ABC Frontend Audio Systems for the NIST-SRE24](https://arxiv.org/pdf/2505.15320)
*Sara Barahona, Anna Silnova, Ladislav Mošner, Junyi Peng, Oldřich Plchot, Johan Rohdin, Lin Zhang, Jiangyu Han, Petr Palka, Federico Landini, Lukáš Burget, Themos Stafylakis, Sandro Cumani, Dominik Boboš, Miroslav Hlavaček, Martin Kodovsky, Tomáš Pavlíček*

Main category: eess.AS

TL;DR: Analysis of embedding extractors for NIST SRE 2024, comparing fixed and open training conditions, with ResNet, ReDimNet, and XLS-R architectures. VoxBlink2 dataset showed strong performance.


<details>
  <summary>Details</summary>
Motivation: To develop optimal speaker embedding extractors for conversational telephone speech under NIST SRE 2024 constraints, exploring various architectures and training conditions.

Method: Evaluated ResNet variants, ReDimNet, and XLS-R models under fixed (telephone recordings only) and open (additional public data) conditions, using VoxBlink2 for open training.

Result: VoxBlink2-trained models demonstrated robust performance, providing practical guidelines for state-of-the-art speaker recognition frontends.

Conclusion: The study offers effective architectures and training strategies for speaker embedding extractors, with VoxBlink2 proving valuable in open conditions.

Abstract: We present a comprehensive analysis of the embedding extractors (frontends)
developed by the ABC team for the audio track of NIST SRE 2024. We follow the
two scenarios imposed by NIST: using only a provided set of telephone
recordings for training (fixed) or adding publicly available data (open
condition). Under these constraints, we develop the best possible speaker
embedding extractors for the pre-dominant conversational telephone speech (CTS)
domain. We explored architectures based on ResNet with different pooling
mechanisms, recently introduced ReDimNet architecture, as well as a system
based on the XLS-R model, which represents the family of large pre-trained
self-supervised models. In open condition, we train on VoxBlink2 dataset,
containing 110 thousand speakers across multiple languages. We observed a good
performance and robustness of VoxBlink-trained models, and our experiments show
practical recipes for developing state-of-the-art frontends for speaker
recognition.

</details>


### [716] [On the Relevance of Clinical Assessment Tasks for the Automatic Detection of Parkinson's Disease Medication State from Speech](https://arxiv.org/pdf/2505.15378)
*David Gimeno-Gómez, Rubén Solera-Ureña, Anna Pompili, Carlos-D. Martínez-Hinarejos, Rita Cardoso, Isabel Guimarães, Joaquim Ferreira, Alberto Abad*

Main category: eess.AS

TL;DR: The paper proposes a speaker-independent method using self-supervised speech representations to identify Parkinson's disease medication states, achieving an 88.2% F1-score.


<details>
  <summary>Details</summary>
Motivation: To assist clinicians in monitoring PD patients and personalizing treatments by using speech as a non-invasive biomarker.

Method: Uses self-supervised speech representations, focusing on prosody and continuous speech, outperforming traditional machine learning and knowledge-based acoustic descriptors.

Result: Achieves an F1-score of 88.2% in distinguishing medication states, demonstrating the effectiveness of speech as a biomarker.

Conclusion: The approach may simplify clinical monitoring and reduce patient effort in voice recordings.

Abstract: The automatic identification of medication states of Parkinson's disease (PD)
patients can assist clinicians in monitoring and scheduling personalized
treatments, as well as studying the effects of medication in alleviating the
motor symptoms that characterize the disease. This paper explores speech as a
non-invasive and accessible biomarker for identifying PD medication states,
introducing a novel approach that addresses this task from a
speaker-independent perspective. While traditional machine learning models
achieve competitive results, self-supervised speech representations prove
essential for optimal performance, significantly surpassing knowledge-based
acoustic descriptors. Experiments across diverse speech assessment tasks
highlight the relevance of prosody and continuous speech in distinguishing
medication states, reaching an F1-score of 88.2%. These findings may streamline
clinicians' work and reduce patient effort in voice recordings.

</details>


### [717] [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/pdf/2505.15667)
*Nicholas Sanders, Yuanchao Li, Korin Richmond, Simon King*

Main category: eess.AS

TL;DR: SVCs quantize speech at distinct linguistic units, improving prosodic and paralinguistic retention without inefficient bitrate increases.


<details>
  <summary>Details</summary>
Motivation: Current quantization in SSL speech models loses prosodic and paralinguistic info, and increasing codebook size is inefficient.

Method: Proposed Segmentation-Variant Codebooks (SVCs) quantize speech at frame, phone, word, and utterance levels, using segment-specific discrete features.

Result: SVCs better preserve prosodic/paralinguistic info, and pooling before discretization retains segment-level info. Resynthesis shows improved style and quality.

Conclusion: SVCs are effective for retaining speech nuances while maintaining intelligibility and efficiency.

Abstract: Quantization in SSL speech models (e.g., HuBERT) improves compression and
performance in tasks like language modeling, resynthesis, and text-to-speech
but often discards prosodic and paralinguistic information (e.g., emotion,
prominence). While increasing codebook size mitigates some loss, it
inefficiently raises bitrates. We propose Segmentation-Variant Codebooks
(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,
utterance), factorizing it into multiple streams of segment-specific discrete
features. Our results show that SVCs are significantly more effective at
preserving prosodic and paralinguistic information across probing tasks.
Additionally, we find that pooling before rather than after discretization
better retains segment-level information. Resynthesis experiments further
confirm improved style realization and slightly improved quality while
preserving intelligibility.

</details>


### [718] [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/pdf/2505.15773)
*Yu-Xiang Luo, Yi-Cheng Lin, Ming-To Chuang, Jia-Hung Chen, I-Ning Tsai, Pei Xing Kiew, Yueh-Hsuan Huang, Chien-Feng Liu, Yu-Chen Chen, Bo-Han Feng, Wenze Ren, Hung-yi Lee*

Main category: eess.AS

TL;DR: ToxicTone is a new dataset for detecting toxic speech in Mandarin audio, combining linguistic, acoustic, and emotional features, outperforming text-only models.


<details>
  <summary>Details</summary>
Motivation: The lack of annotated datasets for toxic speech in spoken Mandarin, including prosodic and cultural cues, leaves this area underexplored.

Method: Introduces ToxicTone dataset with detailed annotations and a multimodal framework integrating acoustic, linguistic, and emotional features.

Result: The proposed approach outperforms text-only and baseline models, highlighting the importance of speech-specific cues.

Conclusion: Speech-specific cues are essential for detecting hidden toxic expressions in Mandarin audio, as demonstrated by ToxicTone.

Abstract: Despite extensive research on toxic speech detection in text, a critical gap
remains in handling spoken Mandarin audio. The lack of annotated datasets that
capture the unique prosodic cues and culturally specific expressions in
Mandarin leaves spoken toxicity underexplored. To address this, we introduce
ToxicTone -- the largest public dataset of its kind -- featuring detailed
annotations that distinguish both forms of toxicity (e.g., profanity, bullying)
and sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,
sourced from diverse real-world audio and organized into 13 topical categories,
mirrors authentic communication scenarios. We also propose a multimodal
detection framework that integrates acoustic, linguistic, and emotional
features using state-of-the-art speech and emotion encoders. Extensive
experiments show our approach outperforms text-only and baseline models,
underscoring the essential role of speech-specific cues in revealing hidden
toxic expressions.

</details>


### [719] [Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach](https://arxiv.org/pdf/2505.14336)
*Umberto Cappellazzo, Minsu Kim, Stavros Petridis, Daniele Falavigna, Alessio Brutti*

Main category: eess.AS

TL;DR: Llama-SMoP is an efficient Multimodal LLM for AVSR, using Sparse Mixture of Projectors (SMoP) to maintain performance with smaller LLMs, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: High computational costs of integrating LLMs into AVSR hinder deployment in resource-constrained settings.

Method: Proposes Llama-SMoP with SMoP module, using sparsely-gated MoE projectors and three configurations (best: DEDR).

Result: Llama-SMoP DEDR achieves superior performance on ASR, VSR, and AVSR tasks, with effective expert activation and noise robustness.

Conclusion: Llama-SMoP offers a scalable, efficient solution for AVSR, balancing performance and computational cost.

Abstract: Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.

</details>


### [720] [LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec](https://arxiv.org/pdf/2410.15764)
*Yiwei Guo, Zhihan Li, Chenpeng Du, Hankun Wang, Xie Chen, Kai Yu*

Main category: eess.AS

TL;DR: LSCodec is a low-bitrate, speaker-decoupled discrete speech codec using multi-stage unsupervised training and speaker perturbation, outperforming baselines in intelligibility and quality.


<details>
  <summary>Details</summary>
Motivation: Discrete speech tokens have high bitrates and redundant timbre, limiting their potential for speech generation models.

Method: LSCodec uses a multi-stage unsupervised training framework with speaker perturbation, creating a continuous bottleneck followed by vector quantization for speaker decoupling, and a token vocoder for acoustic refinement.

Result: LSCodec achieves superior intelligibility and audio quality with a single codebook and smaller vocabulary. Speaker disentanglement is proven effective.

Conclusion: LSCodec's training framework and speaker decoupling ability make it a promising solution for efficient speech generation.

Abstract: Although discrete speech tokens have exhibited strong potential for language
model-based speech generation, their high bitrates and redundant timbre
information restrict the development of such models. In this work, we propose
LSCodec, a discrete speech codec that has both low bitrate and speaker
decoupling ability. LSCodec adopts a multi-stage unsupervised training
framework with a speaker perturbation technique. A continuous information
bottleneck is first established, followed by vector quantization that produces
a discrete speaker-decoupled space. A discrete token vocoder finally refines
acoustic details from LSCodec. By reconstruction evaluations, LSCodec
demonstrates superior intelligibility and audio quality with only a single
codebook and smaller vocabulary size than baselines. Voice conversion and
speaker probing experiments prove the excellent speaker disentanglement of
LSCodec, and ablation study verifies the effectiveness of the proposed training
framework.

</details>


### [721] [From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based Methodology](https://arxiv.org/pdf/2412.17778)
*Haoyang Li, Yuchen Hu, Chen Chen, Sabato Marco Siniscalchi, Songting Liu, Eng Siong Chng*

Main category: eess.AS

TL;DR: GR-KAN, a variant of KAN, improves speech enhancement (SE) by replacing dense layers in DNNs, achieving better performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Conventional activation functions in DNN-based SE lack expressiveness for high-fidelity enhancement, prompting the exploration of GR-KAN.

Method: GR-KAN layers replace dense layers in MP-SENet (T-F domain) and adapt activations in 1D CNN layers of Demucs (time-domain).

Result: GR-KAN reduces parameters by 4x and improves PESQ by 0.1 on Voicebank-DEMAND, outperforming KAN and MLP.

Conclusion: GR-KAN is a scalable and effective alternative for SE, demonstrating consistent improvements in both time- and T-F domains.

Abstract: Deep neural network (DNN)-based speech enhancement (SE) usually uses
conventional activation functions, which lack the expressiveness to capture
complex multiscale structures needed for high-fidelity SE. Group-Rational KAN
(GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN's
expressiveness while improving scalability on complex tasks. We adapt GR-KAN to
existing DNN-based SE by replacing dense layers with GR-KAN layers in the
time-frequency (T-F) domain MP-SENet and adapting GR-KAN's activations into the
1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that
GR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In
contrast, KAN, facing scalability issues, outperforms MLP on a small-scale
signal modeling task but fails to improve MP-SENet. We demonstrate the first
successful use of KAN-based methods for consistent improvement in both time-
and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE.

</details>


### [722] [Enhancing Intelligibility for Generative Target Speech Extraction via Joint Optimization with Target Speaker ASR](https://arxiv.org/pdf/2501.14477)
*Hao Ma, Rujin Chen, Xiao-Lei Zhang, Ju Liu, Xuelong Li*

Main category: eess.AS

TL;DR: A generative TSE framework using the Whisper model improves both intelligibility and perceptual quality in target speech extraction.


<details>
  <summary>Details</summary>
Motivation: Existing TSE methods suffer from quality issues (over-/under-suppression) or neglect intelligibility.

Method: Integrates semantic modeling from Whisper with flow-based acoustic modeling.

Result: Outperforms existing generative and discriminative baselines in benchmarks.

Conclusion: The proposed framework achieves superior intelligibility and perceptual quality.

Abstract: Target speech extraction (TSE) isolates the speech of a specific speaker from
a multi-talker overlapped speech mixture. Most existing TSE models rely on
discriminative methods, typically predicting a time-frequency spectrogram mask
for the target speech. However, imperfections in these masks often result in
over-/under-suppression of target/non-target speech, degrading perceptual
quality. Generative methods, by contrast, re-synthesize target speech based on
the mixture and target speaker cues, achieving superior perceptual quality.
Nevertheless, these methods often overlook speech intelligibility, leading to
alterations or loss of semantic content in the re-synthesized speech. Inspired
by the Whisper model's success in target speaker ASR, we propose a generative
TSE framework based on the pre-trained Whisper model to address the above
issues. This framework integrates semantic modeling with flow-based acoustic
modeling to achieve both high intelligibility and perceptual quality. Results
from multiple benchmarks demonstrate that the proposed method outperforms
existing generative and discriminative baselines. We present speech samples on
https://aisaka0v0.github.io/GenerativeTSE_demo/.

</details>


### [723] [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/pdf/2501.16344)
*Rajath Rao, Adithya Ganesan, Oscar Kjell, Jonah Luby, Akshay Raghavan, Scott Feltman, Whitney Ringwald, Ryan L. Boyd, Benjamin Luft, Camilo Ruggero, Neville Ryant, Roman Kotov, H. Andrew Schwartz*

Main category: eess.AS

TL;DR: WhiSPA improves speech encoding by aligning Whisper's latent space with semantic and psychological embeddings, eliminating the need for an additional text-based LM.


<details>
  <summary>Details</summary>
Motivation: Current speech encoding relies on text-based LMs despite SotA speech-to-text models having internal LMs. This work aims to enhance the internal LM to avoid redundancy.

Method: WhiSPA uses contrastive loss with a language model embedding as a teacher, aligning Whisper's latent space with SBERT and psychological embeddings (emotion, personality).

Result: WhiSPA outperforms current speech encoders, reducing error by 73.4% (self-supervised) and 83.8% (downstream psychological tasks).

Conclusion: WhiSPA shows rich psychological representations can be achieved without additional text LMs, improving efficiency in speech encoding.

Abstract: Current speech encoding pipelines often rely on an additional text-based LM
to get robust representations of human communication, even though SotA
speech-to-text models often have a LM within. This work proposes an approach to
improve the LM within an audio model such that the subsequent text-LM is
unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological
Alignment), which leverages a novel audio training objective: contrastive loss
with a language model embedding as a teacher. Using over 500k speech segments
from mental health audio interviews, we evaluate the utility of aligning
Whisper's latent space with semantic representations from a text autoencoder
(SBERT) and lexically derived embeddings of basic psychological dimensions:
emotion and personality. Over self-supervised affective tasks and downstream
psychological tasks, WhiSPA surpasses current speech encoders, achieving an
average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates
that it is not always necessary to run a subsequent text LM on speech-to-text
output in order to get a rich psychological representation of human
communication.

</details>


### [724] [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](https://arxiv.org/pdf/2502.10447)
*Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun*

Main category: eess.AS

TL;DR: MoHAVE introduces a scalable AVSR framework using Mixture-of-Experts (MoE) to enhance efficiency and adaptability in noisy environments.


<details>
  <summary>Details</summary>
Motivation: Existing AVSR systems face scalability and computational efficiency challenges.

Method: MoHAVE employs a sparse MoE architecture with hierarchical gating for dynamic expert group activation.

Result: Achieves state-of-the-art performance on LRS3 and MuAViC benchmarks.

Conclusion: MoHAVE sets a new standard for scalable and robust AVSR systems.

Abstract: Audio-visual speech recognition (AVSR) has become critical for enhancing
speech recognition in noisy environments by integrating both auditory and
visual modalities. However, existing AVSR systems struggle to scale up without
compromising computational efficiency. In this study, we introduce MoHAVE
(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework
designed to address these scalability constraints. By leveraging a
Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific
expert groups, ensuring dynamic adaptation to various audio-visual inputs with
minimal computational overhead. Key contributions of MoHAVE include: (1) a
sparse MoE framework that efficiently scales AVSR model capacity, (2) a
hierarchical gating mechanism that dynamically utilizes the expert groups based
on input context, enhancing adaptability and robustness, and (3) remarkable
performance across robust AVSR benchmarks, including LRS3 and MuAViC
transcription and translation tasks, setting a new standard for scalable speech
recognition systems.

</details>


### [725] [MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement](https://arxiv.org/pdf/2505.13029)
*Nan Xu, Zhaolong Huang, Xiaonan Zhi*

Main category: eess.AS

TL;DR: MDDM, a Multi-view Discriminative enhanced Diffusion-based Model, improves speech enhancement by combining discriminative and generative approaches, reducing distortions and computational costs.


<details>
  <summary>Details</summary>
Motivation: Address limitations of previous methods (speech distortions, high computational cost) by integrating discriminative and generative modeling.

Method: Uses multi-view features (time, frequency, noise) in a discriminative network to generate a preliminary spectrogram, refined via diffusion sampling.

Result: Achieves competitive performance with fewer sampling steps, validated on public and real-world datasets.

Conclusion: MDDM effectively balances quality and efficiency in speech enhancement.

Abstract: With the development of deep learning, speech enhancement has been greatly
optimized in terms of speech quality. Previous methods typically focus on the
discriminative supervised learning or generative modeling, which tends to
introduce speech distortions or high computational cost. In this paper, we
propose MDDM, a Multi-view Discriminative enhanced Diffusion-based Model.
Specifically, we take the features of three domains (time, frequency and noise)
as inputs of a discriminative prediction network, generating the preliminary
spectrogram. Then, the discriminative output can be converted to clean speech
by several inference sampling steps. Due to the intersection of the
distributions between discriminative output and clean target, the smaller
sampling steps can achieve the competitive performance compared to other
diffusion-based methods. Experiments conducted on a public dataset and a
realworld dataset validate the effectiveness of MDDM, either on subjective or
objective metric.

</details>


### [726] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/pdf/2505.14449)
*Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces an Implicit Demography Inference (IDI) module to address fairness in Speech Emotion Recognition (SER) without explicit demographic labels, improving fairness metrics significantly with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Fairness in SER is underexplored, and existing methods rely on hard-to-obtain demographic labels. The study aims to mitigate bias without explicit labels.

Method: The IDI module uses pseudo-labeling from a pre-trained model and unsupervised k-means clustering to infer demographics implicitly.

Result: Pseudo-labeling IDI improves fairness by over 33% with <3% accuracy drop; unsupervised IDI improves fairness by 26% with <4% accuracy drop. It mitigates race and age disparities.

Conclusion: The IDI module effectively enhances fairness in SER without explicit labels, making it suitable for privacy-sensitive scenarios.

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [727] [A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis](https://arxiv.org/pdf/2505.14715)
*Muhammad Zubair, Muzammil Hussai, Mousa Ahmad Al-Bashrawi, Malika Bendechache, Muhammad Owais*

Main category: eess.IV

TL;DR: The paper reviews multi-modal medical image fusion (MMIF), its evolution, methodologies, and clinical applications, comparing traditional and deep learning-based techniques, and discusses challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: MMIF enhances diagnostic precision and clinical decision-making by integrating diverse medical imaging data, improving accuracy in diagnosis, lesion detection, and segmentation.

Method: The review surveys traditional fusion methods (pixel-, feature-, decision-level) and modern techniques (deep learning, generative models, transformers), comparing their robustness, efficiency, and interpretability.

Result: MMIF significantly improves diagnostic accuracy and therapeutic outcomes in oncology, neurology, and cardiology, but faces challenges like data privacy, computational complexity, and clinical workflow integration.

Conclusion: Future research should focus on explainable AI, federated learning, real-time systems, and standardization to address current limitations and broaden MMIF adoption.

Abstract: Multi-modal medical image fusion (MMIF) is increasingly recognized as an
essential technique for enhancing diagnostic precision and facilitating
effective clinical decision-making within computer-aided diagnosis systems.
MMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create
detailed, clinically useful images of patient anatomy and pathology. These
integrated representations significantly advance diagnostic accuracy, lesion
detection, and segmentation. This comprehensive review meticulously surveys the
evolution, methodologies, algorithms, current advancements, and clinical
applications of MMIF. We present a critical comparative analysis of traditional
fusion approaches, including pixel-, feature-, and decision-level methods, and
delves into recent advancements driven by deep learning, generative models, and
transformer-based architectures. A critical comparative analysis is presented
between these conventional methods and contemporary techniques, highlighting
differences in robustness, computational efficiency, and interpretability. The
article addresses extensive clinical applications across oncology, neurology,
and cardiology, demonstrating MMIF's vital role in precision medicine through
improved patient-specific therapeutic outcomes. Moreover, the review thoroughly
investigates the persistent challenges affecting MMIF's broad adoption,
including issues related to data privacy, heterogeneity, computational
complexity, interpretability of AI-driven algorithms, and integration within
clinical workflows. It also identifies significant future research avenues,
such as the integration of explainable AI, adoption of privacy-preserving
federated learning frameworks, development of real-time fusion systems, and
standardization efforts for regulatory compliance.

</details>


### [728] [A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis](https://arxiv.org/pdf/2505.14716)
*Sahil Tomar, Rajeshwar Tripathi, Sandeep Kumar*

Main category: eess.IV

TL;DR: A hybrid quantum-classical pipeline for bone fracture detection achieves 99% accuracy, reducing feature extraction time by 82%.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies and errors in traditional X-ray interpretation and the resource-heavy demands of existing ML/DL solutions.

Method: Uses PCA for dimensionality reduction and a 4-qubit quantum amplitude encoding circuit for feature enrichment, combining PCA and quantum features for classification.

Result: 99% accuracy on a public X-ray dataset, matching state-of-the-art models while cutting feature extraction time by 82%.

Conclusion: The hybrid pipeline offers a resource-efficient, high-accuracy alternative for bone fracture detection.

Abstract: Bone fractures are a leading cause of morbidity and disability worldwide,
imposing significant clinical and economic burdens on healthcare systems.
Traditional X ray interpretation is time consuming and error prone, while
existing machine learning and deep learning solutions often demand extensive
feature engineering, large, annotated datasets, and high computational
resources. To address these challenges, a distributed hybrid quantum classical
pipeline is proposed that first applies Principal Component Analysis (PCA) for
dimensionality reduction and then leverages a 4 qubit quantum amplitude
encoding circuit for feature enrichment. By fusing eight PCA derived features
with eight quantum enhanced features into a 16 dimensional vector and then
classifying with different machine learning models achieving 99% accuracy using
a public multi region X ray dataset on par with state of the art transfer
learning models while reducing feature extraction time by 82%.

</details>


### [729] [Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks](https://arxiv.org/pdf/2505.14717)
*Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Chen Jiang, Tan Pan, Xingmeng Zhang, Cenyu Liu, Zeyun Miao, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Yichi Zhang, Wenbo Zhang, Fengping Zhu, Limei Han, Yuan Qi, Chensen Lin, Yuan Cheng*

Main category: eess.IV

TL;DR: A large-scale, high-fidelity aneurysm CFD dataset was created to enable efficient machine learning algorithms for hemodynamic studies, addressing computational limitations of conventional CFD methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing intracranial aneurysm (IA) risk lack clarity on hemodynamic influences, and conventional CFD is too computationally intensive for large-scale or real-time use.

Method: The study synthesized 10,660 3D aneurysm shapes from 427 real geometries, validated by neurosurgeons, and performed CFD computations under eight flow conditions, generating 85,280 flow dynamics data points.

Result: The dataset includes segmentation masks and a benchmark for flow parameter estimation, supporting multimodal data tasks and advancing aneurysm research.

Conclusion: This dataset promotes data-driven approaches in biofluids, biomedical engineering, and clinical risk assessment, with code and data publicly available.

Abstract: Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in
approximately 5\% of the general population. Their rupture may lead to high
mortality. Current methods for assessing IA risk focus on morphological and
patient-specific factors, but the hemodynamic influences on IA development and
rupture remain unclear. While accurate for hemodynamic studies, conventional
computational fluid dynamics (CFD) methods are computationally intensive,
hindering their deployment in large-scale or real-time clinical applications.
To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD
dataset to facilitate the development of efficient machine learning algorithms
for such applications. Based on 427 real aneurysm geometries, we synthesized
10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The
authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD
computations were performed on each shape under eight steady-state mass flow
conditions, generating a total of 85,280 blood flow dynamics data covering key
parameters. Furthermore, the dataset includes segmentation masks, which can
support tasks that use images, point clouds or other multimodal data as input.
Additionally, we introduced a benchmark for estimating flow parameters to
assess current modeling methods. This dataset aims to advance aneurysm research
and promote data-driven approaches in biofluids, biomedical engineering, and
clinical risk assessment. The code and dataset are available at:
https://github.com/Xigui-Li/Aneumo.

</details>


### [730] [MedBLIP: Fine-tuning BLIP for Medical Image Captioning](https://arxiv.org/pdf/2505.14726)
*Manshi Limbu, Diwita Banerjee*

Main category: eess.IV

TL;DR: Fine-tuning BLIP on the ROCO dataset improves radiology image captioning, outperforming zero-shot and other models, with decoder-only fine-tuning offering a balance between performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models (VLMs) like BLIP, BLIP2, Gemini, and ViT-GPT2 perform well on natural images but produce generic captions for medical images, necessitating domain-specific adaptation.

Method: Fine-tuned BLIP on the ROCO dataset, compared against zero-shot BLIP, BLIP-2 variants, and ViT-GPT2. Evaluated performance via quantitative/qualitative metrics, visualized cross-attention maps, and conducted ablation studies on encoder/decoder fine-tuning.

Result: Domain-specific fine-tuning significantly improves performance. Decoder-only fine-tuning reduces training time by 5% with strong results, but full fine-tuning yields the best overall performance.

Conclusion: Targeted adaptation is crucial for medical applications, with decoder-only fine-tuning providing a practical trade-off, though full fine-tuning remains optimal.

Abstract: Medical image captioning is a challenging task that requires generating
clinically accurate and semantically meaningful descriptions of radiology
images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini
and ViT-GPT2 show strong performance on natural image datasets, they often
produce generic or imprecise captions when applied to specialized medical
domains. In this project, we explore the effectiveness of fine-tuning the BLIP
model on the ROCO dataset for improved radiology captioning. We compare the
fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and
a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific
fine-tuning on BLIP significantly improves performance across both quantitative
and qualitative evaluation metrics. We also visualize decoder cross-attention
maps to assess interpretability and conduct an ablation study to evaluate the
contributions of encoder-only and decoder-only fine-tuning. Our findings
highlight the importance of targeted adaptation for medical applications and
suggest that decoder-only fine-tuning (encoder-frozen) offers a strong
performance baseline with 5% lower training time than full fine-tuning, while
full model fine-tuning still yields the best results overall.

</details>


### [731] [LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction](https://arxiv.org/pdf/2505.14747)
*Fatemeh Chajaei, Hossein Bagheri*

Main category: eess.IV

TL;DR: The study evaluates LiDAR data for 3D building reconstruction at LOD1 using deep learning models, finding U-Net3+ and Attention U-Net most effective. Segmentation accuracy impacts 3D model quality and morphological feature estimation.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D building reconstruction at LOD1 is vital for urban planning, environmental studies, and transportation design.

Method: Four deep semantic segmentation models (U-Net, Attention U-Net, U-Net3+, DeepLabV3+) were applied to LiDAR data for building footprint extraction and height estimation.

Result: U-Net3+ and Attention U-Net performed best (IoU 0.833 and 0.814). Segmentation accuracy significantly influenced 3D model quality and morphological feature accuracy.

Conclusion: U-Net3+ with 90th percentile and median measures provides accurate height estimation and morphological feature extraction for LOD1 models.

Abstract: Three-dimensional reconstruction of buildings, particularly at Level of
Detail 1 (LOD1), plays a crucial role in various applications such as urban
planning, urban environmental studies, and designing optimized transportation
networks. This study focuses on assessing the potential of LiDAR data for
accurate 3D building reconstruction at LOD1 and extracting morphological
features from these models. Four deep semantic segmentation models, U-Net,
Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning
to extract building footprints from LiDAR data. The results showed that U-Net3+
and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and
0.814, respectively. Various statistical measures, including maximum, range,
mode, median, and the 90th percentile, were used to estimate building heights,
resulting in the generation of 3D models at LOD1. As the main contribution of
the research, the impact of segmentation accuracy on the quality of 3D building
modeling and the accuracy of morphological features like building area and
external wall surface area was investigated. The results showed that the
accuracy of building identification (segmentation performance) significantly
affects the 3D model quality and the estimation of morphological features,
depending on the height calculation method. Overall, the UNet3+ method,
utilizing the 90th percentile and median measures, leads to accurate height
estimation of buildings and the extraction of morphological features.

</details>


### [732] [TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/pdf/2505.14753)
*Mengzhu Wang, Jiao Li, Shanshan Wang, Long Lan, Huibin Tan, Liang Yang, Guoli Yang*

Main category: eess.IV

TL;DR: TransMedSeg introduces a transferable semantic framework for semi-supervised medical image segmentation, outperforming existing methods by aligning domain-invariant semantics through a novel augmentation module.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods for medical images overlook transferable semantic relationships across domains and modalities, limiting their effectiveness.

Method: TransMedSeg uses a Transferable Semantic Augmentation (TSA) module to align domain-invariant semantics via cross-domain distribution matching and intra-domain structural preservation, leveraging a lightweight memory module for implicit semantic transformation.

Result: Extensive experiments show TransMedSeg outperforms existing semi-supervised methods in medical image segmentation.

Conclusion: TransMedSeg establishes a new direction for transferable representation learning in medical image analysis, enhancing feature representations without explicit data generation.

Abstract: Semi-supervised learning (SSL) has achieved significant progress in medical
image segmentation (SSMIS) through effective utilization of limited labeled
data. While current SSL methods for medical images predominantly rely on
consistency regularization and pseudo-labeling, they often overlook
transferable semantic relationships across different clinical domains and
imaging modalities. To address this, we propose TransMedSeg, a novel
transferable semantic framework for semi-supervised medical image segmentation.
Our approach introduces a Transferable Semantic Augmentation (TSA) module,
which implicitly enhances feature representations by aligning domain-invariant
semantics through cross-domain distribution matching and intra-domain
structural preservation. Specifically, TransMedSeg constructs a unified feature
space where teacher network features are adaptively augmented towards student
network semantics via a lightweight memory module, enabling implicit semantic
transformation without explicit data generation. Interestingly, this
augmentation is implicitly realized through an expected transferable
cross-entropy loss computed over the augmented teacher distribution. An upper
bound of the expected loss is theoretically derived and minimized during
training, incurring negligible computational overhead. Extensive experiments on
medical image datasets demonstrate that TransMedSeg outperforms existing
semi-supervised methods, establishing a new direction for transferable
representation learning in medical image analysis.

</details>


### [733] [Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking](https://arxiv.org/pdf/2505.14754)
*Andrey Alexandrov, Giovanni Acampora, Giovanni De Lellis, Antonia Di Crescenzo, Chiara Errico, Daria Morozova, Valeri Tioukov, Autilia Vittiello*

Main category: eess.IV

TL;DR: A deep learning method using CNNs achieves 40nm axial localization accuracy from dual-focal plane images, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: Accurate axial particle tracking is challenging in microscopy, especially for high precision.

Method: Uses CNNs to analyze dual-focal plane images without predefined models.

Result: Achieves 40nm axial localization accuracy, six times better than single-focal plane methods.

Conclusion: Demonstrates machine learning's potential to convert complex data into precise information for diverse scientific applications.

Abstract: Accurately tracking particles and determining their position along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
positions from dual-focal plane images without relying on predefined models.
Our method achieves an axial localization accuracy of 40 nanometers - six times
better than traditional single-focal plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.

</details>


### [734] [Virtual Fluoroscopy for Interventional Guidance using Magnetic Tracking](https://arxiv.org/pdf/2505.14854)
*Shuwei Xing, Inaara Ahmed-Fazal, Utsav Pardasani, Uditha Jayarathne, Scott Illsley, Aaron Fenster, Terry M. Peters, Elvis C. S. Chen*

Main category: eess.IV

TL;DR: Virtual fluoroscopy with magnetic tracking improves depth perception and reduces radiation exposure in interventions, achieving high accuracy in simulated images.


<details>
  <summary>Details</summary>
Motivation: Conventional fluoroscopy lacks depth perception and increases radiation exposure. Magnetic tracking, though promising, is underused due to interference issues.

Method: Developed a virtual fluoroscopy workflow with automatic fluoro-CT registration and C-arm modeling for accurate pose calculation and image generation.

Result: Simulated fluoroscopic images matched real ones closely (1.55 mm error). Needle tip error in phantom insertion was 3.42 mm.

Conclusion: Virtual fluoroscopy with magnetic tracking enhances navigation and understanding of X-ray imaging, improving efficiency.

Abstract: Purpose: In conventional fluoroscopy-guided interventions, the 2D projective
nature of X-ray imaging limits depth perception and leads to prolonged
radiation exposure. Virtual fluoroscopy, combined with spatially tracked
surgical instruments, is a promising strategy to mitigate these limitations.
While magnetic tracking shows unique advantages, particularly in tracking
flexible instruments, it remains under-explored due to interference from
ferromagnetic materials in the C-arm room. This work proposes a virtual
fluoroscopy workflow by effectively integrating magnetic tracking, and
demonstrates its clinical efficacy. Methods: An automatic virtual fluoroscopy
workflow was developed using a radiolucent tabletop field generator prototype.
Specifically, we developed a fluoro-CT registration approach with automatic
2D-3D shared landmark correspondence to establish the C-arm-patient
relationship, along with a general C-arm modelling approach to calculate
desired poses and generate corresponding virtual fluoroscopic images. Results:
Testing on a dataset with views ranging from RAO 90 degrees to LAO 90 degrees,
simulated fluoroscopic images showed visually imperceptible differences from
the real ones, achieving a mean target projection distance error of 1.55 mm. An
endoleak phantom insertion experiment highlighted the effectiveness of
simulating multiplanar views with real-time instrument overlays, achieving a
mean needle tip error of 3.42 mm. Conclusions: Results demonstrated the
efficacy of virtual fluoroscopy integrated with magnetic tracking, improving
depth perception during navigation. The broad capture range of virtual
fluoroscopy showed promise in improving the users understanding of X-ray
imaging principles, facilitating more efficient image acquisition.

</details>


### [735] [Super-Resolution Optical Coherence Tomography Using Diffusion Model-Based Plug-and-Play Priors](https://arxiv.org/pdf/2505.14916)
*Yaning Wang, Jinglun Yu, Wenhan Guo, Yu Sun, Jin U. Kang*

Main category: eess.IV

TL;DR: A plug-and-play diffusion model (PnP-DM) for OCT super-resolution improves image quality from sparse measurements, outperforming 2D-UNet baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance high-fidelity OCT imaging for clinical use by reconstructing high-quality images from under-sampled data.

Method: Combines a diffusion prior with Markov chain Monte Carlo sampling for efficient posterior inference, trained on deep learning-based up-sampled pairs.

Result: PnP-DM produces sharper structures and better noise suppression than 2D-UNet, validated on in vivo and ex vivo models.

Conclusion: The framework advances high-speed OCT imaging, offering clinical benefits.

Abstract: We propose an OCT super-resolution framework based on a plug-and-play
diffusion model (PnP-DM) to reconstruct high-quality images from sparse
measurements (OCT B-mode corneal images). Our method formulates reconstruction
as an inverse problem, combining a diffusion prior with Markov chain Monte
Carlo sampling for efficient posterior inference. We collect high-speed
under-sampled B-mode corneal images and apply a deep learning-based up-sampling
pipeline to build realistic training pairs. Evaluations on in vivo and ex vivo
fish-eye corneal models show that PnP-DM outperforms conventional 2D-UNet
baselines, producing sharper structures and better noise suppression. This
approach advances high-fidelity OCT imaging in high-speed acquisition for
clinical applications.

</details>


### [736] [Rate-Accuracy Bounds in Visual Coding for Machines](https://arxiv.org/pdf/2505.14980)
*Ivan V. Bajić*

Main category: eess.IV

TL;DR: The paper explores rate-accuracy bounds for visual coding for machines, revealing significant gaps between current methods and theoretical limits.


<details>
  <summary>Details</summary>
Motivation: The rise of automated analysis of visual signals (e.g., images, videos) in applications like traffic monitoring and autonomous driving necessitates efficient compression strategies for analysis, not reconstruction.

Method: The paper draws parallels with lossy coding of discrete memoryless sources to derive rate-accuracy bounds for visual coding problems.

Result: Current methods are 1-3 orders of magnitude away from theoretical bounds in bitrate efficiency for achieving accuracy.

Conclusion: There is substantial room for improvement in visual coding for machines to bridge the gap between theory and practice.

Abstract: Increasingly, visual signals such as images, videos and point clouds are
being captured solely for the purpose of automated analysis by computer vision
models. Applications include traffic monitoring, robotics, autonomous driving,
smart home, and many others. This trend has led to the need to develop
compression strategies for these signals for the purpose of analysis rather
than reconstruction, an area often referred to as "coding for machines." By
drawing parallels with lossy coding of a discrete memoryless source, in this
paper we derive rate-accuracy bounds on several popular problems in visual
coding for machines, and compare these with state-of-the-art results from the
literature. The comparison shows that the current results are at least an order
of magnitude -- and in some cases two or three orders of magnitude -- away from
the theoretical bounds in terms of the bitrate needed to achieve a certain
level of accuracy. This, in turn, means that there is much room for improvement
in the current methods for visual coding for machines.

</details>


### [737] [Rate-Distortion Optimization with Non-Reference Metrics for UGC Compression](https://arxiv.org/pdf/2505.15003)
*Samuel Fernández-Menduiña, Xin Xiong, Eduardo Pavez, Antonio Ortega, Neil Birkbeck, Balu Adsumilli*

Main category: eess.IV

TL;DR: The paper proposes a method to optimize video encoding for noisy UGC by linearizing non-reference metrics (NRMs) and using SSE regularization, achieving significant bitrate savings.


<details>
  <summary>Details</summary>
Motivation: Conventional codecs using full-reference metrics (FRMs) perform poorly on noisy UGC, preserving artifacts. NRMs are better but computationally impractical for RDO.

Method: Linearizes NRMs around uncompressed video, enabling block-wise bit allocation with SSE regularization to avoid large deviations.

Result: Achieves over 30% bitrate savings compared to SSE-RDO, with minimal encoder complexity increase and no decoder overhead.

Conclusion: The proposed method effectively optimizes UGC encoding using NRMs, balancing quality and computational efficiency.

Abstract: Service providers must encode a large volume of noisy videos to meet the
demand for user-generated content (UGC) in online video-sharing platforms.
However, low-quality UGC challenges conventional codecs based on
rate-distortion optimization (RDO) with full-reference metrics (FRMs). While
effective for pristine videos, FRMs drive codecs to preserve artifacts when the
input is degraded, resulting in suboptimal compression. A more suitable
approach used to assess UGC quality is based on non-reference metrics (NRMs).
However, RDO with NRMs as a measure of distortion requires an iterative
workflow of encoding, decoding, and metric evaluation, which is computationally
impractical. This paper overcomes this limitation by linearizing the NRM around
the uncompressed video. The resulting cost function enables block-wise bit
allocation in the transform domain by estimating the alignment of the
quantization error with the gradient of the NRM. To avoid large deviations from
the input, we add sum of squared errors (SSE) regularization. We derive
expressions for both the SSE regularization parameter and the Lagrangian, akin
to the relationship used for SSE-RDO. Experiments with images and videos show
bitrate savings of more than 30\% over SSE-RDO using the target NRM, with no
decoder complexity overhead and minimal encoder complexity increase.

</details>


### [738] [Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models](https://arxiv.org/pdf/2505.15057)
*Frederic Wang, Jonathan I. Tamir*

Main category: eess.IV

TL;DR: A novel alternating minimization framework using a diffusion model to correct motion artifacts in MRI by reconstructing k-space data, even with severe undersampling.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in MRI degrade diagnostic quality, especially in dynamic imaging, necessitating robust correction methods.

Method: Uses a coarse-to-fine diffusion model for joint reconstruction and motion correction, prioritizing lower frequencies for better motion estimation.

Result: Effective on real-world cardiac MRI and simulated deformations, even with 64x undersampling, and adaptable to various sampling patterns and protocols.

Conclusion: The method provides a versatile and effective solution for motion artifact correction in MRI, enhancing diagnostic utility.

Abstract: Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts
due to the extended acquisition times required for k-space sampling. These
artifacts can compromise diagnostic utility, particularly for dynamic imaging.
We propose a novel alternating minimization framework that leverages a bespoke
diffusion model to jointly reconstruct and correct non-rigid motion-corrupted
k-space data. The diffusion model uses a coarse-to-fine denoising strategy to
capture large overall motion and reconstruct the lower frequencies of the image
first, providing a better inductive bias for motion estimation than that of
standard diffusion models. We demonstrate the performance of our approach on
both real-world cine cardiac MRI datasets and complex simulated rigid and
non-rigid deformations, even when each motion state is undersampled by a factor
of 64x. Additionally, our method is agnostic to sampling patterns, anatomical
variations, and MRI scanning protocols, as long as some low frequency
components are sampled during each motion state.

</details>


### [739] [Joint Optimization of Primary and Secondary Transforms Using Rate-Distortion Optimized Transform Design](https://arxiv.org/pdf/2505.15104)
*Darukeesan Pakiyarajah, Eduardo Pavez, Antonio Ortega, Debargha Mukherjee, Onur Guleryuz, Keng-Shih Lu, Kruthika Koratti Sivakumar*

Main category: eess.IV

TL;DR: The paper proposes a framework for jointly optimizing primary and secondary transforms in video coding, showing improved efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address computational complexities of non-separable transforms in video coding by combining separable primary and non-separable secondary transforms.

Method: Jointly design primary (path-graph model) and secondary (non-separable) transforms using rate-distortion optimized clustering.

Result: The joint clustering method reduces RD cost, and optimized SPGT outperforms separable KLTs in coding efficiency.

Conclusion: The proposed framework enhances video coding efficiency by jointly optimizing transforms, validated with AVM data.

Abstract: Data-dependent transforms are increasingly being incorporated into
next-generation video coding systems such as AVM, a codec under development by
the Alliance for Open Media (AOM), and VVC. To circumvent the computational
complexities associated with implementing non-separable data-dependent
transforms, combinations of separable primary transforms and non-separable
secondary transforms have been studied and integrated into video coding
standards. These codecs often utilize rate-distortion optimized transforms
(RDOT) to ensure that the new transforms complement existing transforms like
the DCT and the ADST. In this work, we propose an optimization framework for
jointly designing primary and secondary transforms from data through a
rate-distortion optimized clustering. Primary transforms are assumed to follow
a path-graph model, while secondary transforms are non-separable. We
empirically evaluate our proposed approach using AVM residual data and
demonstrate that 1) the joint clustering method achieves lower total RD cost in
the RDOT design framework, and 2) jointly optimized separable path-graph
transforms (SPGT) provide better coding efficiency compared to separable KLTs
obtained from the same data.

</details>


### [740] [Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images](https://arxiv.org/pdf/2505.15120)
*Muniba Noreen, Furqan Shaukat*

Main category: eess.IV

TL;DR: A self-supervised learning method, LungNodule-SSM, using DINOv2 and transformers, achieves 98.37% accuracy for lung nodule detection without annotated data.


<details>
  <summary>Details</summary>
Motivation: Early lung nodule detection is vital for improving cancer outcomes, but annotated medical data is scarce. Self-supervised learning can leverage unlabeled data for robust CAD systems.

Method: Two-stage approach: pre-train DINOv2 on unlabeled CT scans for feature learning, then fine-tune with transformers for nodule detection and classification.

Result: Achieved 98.37% accuracy on LUNA 16 dataset, outperforming SOTA methods.

Conclusion: LungNodule-SSM is effective for lung nodule detection, demonstrating the potential of self-supervised learning in medical imaging.

Abstract: Lung cancer remains among the deadliest types of cancer in recent decades,
and early lung nodule detection is crucial for improving patient outcomes. The
limited availability of annotated medical imaging data remains a bottleneck in
developing accurate computer-aided diagnosis (CAD) systems. Self-supervised
learning can help leverage large amounts of unlabeled data to develop more
robust CAD systems. With the recent advent of transformer-based architecture
and their ability to generalize to unseen tasks, there has been an effort
within the healthcare community to adapt them to various medical downstream
tasks. Thus, we propose a novel "LungNodule-SSM" method, which utilizes
selfsupervised learning with DINOv2 as a backbone to enhance lung nodule
detection and classification without annotated data. Our methodology has two
stages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn
robust feature representations, then secondly, these features are fine-tuned
using transformer-based architectures for lesionlevel detection and accurate
lung nodule diagnosis. The proposed method has been evaluated on the
challenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA
methods. Our experimental results show the superiority of our proposed method
with an accuracy of 98.37%, explaining its effectiveness in lung nodule
detection. The source code, datasets, and pre-processed data can be accessed
using the
link:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main

</details>


### [741] [Physics-Guided Multi-View Graph Neural Network for Schizophrenia Classification via Structural-Functional Coupling](https://arxiv.org/pdf/2505.15135)
*Badhan Mazumder, Ayush Kanyal, Lei Wu, Vince D. Calhoun, Dong Hye Ye*

Main category: eess.IV

TL;DR: A novel physics-guided deep learning framework integrates structural (SC) and functional connectivity (FC) to improve understanding and classification of schizophrenia (SZ).


<details>
  <summary>Details</summary>
Motivation: Traditional methods often ignore SC-FC interrelationships due to limited FC data, hindering comprehension of cognitive impairments in SZ.

Method: Proposes a physics-guided deep learning framework using neural oscillation models and multi-view GNNs for SC-FC fusion and classification.

Result: Improved performance on a clinical dataset, demonstrating robustness.

Conclusion: The framework effectively integrates SC and FC, enhancing SZ classification and understanding of brain connectivity disruptions.

Abstract: Clinical studies reveal disruptions in brain structural connectivity (SC) and
functional connectivity (FC) in neuropsychiatric disorders such as
schizophrenia (SZ). Traditional approaches might rely solely on SC due to
limited functional data availability, hindering comprehension of cognitive and
behavioral impairments in individuals with SZ by neglecting the intricate SC-FC
interrelationship. To tackle the challenge, we propose a novel physics-guided
deep learning framework that leverages a neural oscillation model to describe
the dynamics of a collection of interconnected neural oscillators, which
operate via nerve fibers dispersed across the brain's structure. Our proposed
framework utilizes SC to simultaneously generate FC by learning SC-FC coupling
from a system dynamics perspective. Additionally, it employs a novel multi-view
graph neural network (GNN) with a joint loss to perform correlation-based SC-FC
fusion and classification of individuals with SZ. Experiments conducted on a
clinical dataset exhibited improved performance, demonstrating the robustness
of our proposed approach.

</details>


### [742] [SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning](https://arxiv.org/pdf/2505.15234)
*Saqib Qamar, Mohd Fazil, Parvez Ahmad, Ghulam Muhammad*

Main category: eess.IV

TL;DR: SAMA-UNet introduces a novel architecture for medical image segmentation, combining Self-Adaptive Mamba-like Aggregated Attention (SAMA) and Causal-Resonance Multi-Scale Module (CR-MSM) to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with computational inefficiencies and balancing local and global features in medical image segmentation.

Method: SAMA-UNet uses SAMA blocks for dynamic feature prioritization and CR-MSM for enhanced information flow in U-shaped architectures.

Result: Outperforms CNN, Transformer, and Mamba-based methods in segmentation accuracy on MRI, CT, and endoscopy images.

Conclusion: SAMA-UNet offers a promising solution for efficient and accurate medical image segmentation.

Abstract: Medical image segmentation plays an important role in various clinical
applications, but existing models often struggle with the computational
inefficiencies and challenges posed by complex medical data. State Space
Sequence Models (SSMs) have demonstrated promise in modeling long-range
dependencies with linear computational complexity, yet their application in
medical image segmentation remains hindered by incompatibilities with image
tokens and autoregressive assumptions. Moreover, it is difficult to achieve a
balance in capturing both local fine-grained information and global semantic
dependencies. To address these challenges, we introduce SAMA-UNet, a novel
architecture for medical image segmentation. A key innovation is the
Self-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates
contextual self-attention with dynamic weight modulation to prioritise the most
relevant features based on local and global contexts. This approach reduces
computational complexity and improves the representation of complex image
features across multiple scales. We also suggest the Causal-Resonance
Multi-Scale Module (CR-MSM), which enhances the flow of information between the
encoder and decoder by using causal resonance learning. This mechanism allows
the model to automatically adjust feature resolution and causal dependencies
across scales, leading to better semantic alignment between the low-level and
high-level features in U-shaped architectures. Experiments on MRI, CT, and
endoscopy images show that SAMA-UNet performs better in segmentation accuracy
than current methods using CNN, Transformer, and Mamba. The implementation is
publicly available at GitHub.

</details>


### [743] [X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography](https://arxiv.org/pdf/2505.15235)
*Yifan Liu, Wuyang Li, Weihao Yu, Chenxin Li, Alexandre Alahi, Max Meng, Yixuan Yuan*

Main category: eess.IV

TL;DR: X-GRM is a transformer-based model for 3D CT reconstruction from sparse X-ray projections, using Voxel-based Gaussian Splatting (VoxGS) and trained on a large dataset (ReconX-15K).


<details>
  <summary>Details</summary>
Motivation: Existing CT reconstruction methods suffer from small models, inflexible volume representation, and limited training data.

Method: X-GRM uses a scalable transformer to encode sparse X-ray inputs and decodes them into VoxGS for efficient volume extraction and rendering.

Result: The model achieves high-quality reconstructions from diverse X-ray inputs, including in-domain and out-domain projections.

Conclusion: X-GRM advances CT reconstruction with its large-scale architecture, flexible representation, and extensive training data.

Abstract: Computed Tomography serves as an indispensable tool in clinical workflows,
providing non-invasive visualization of internal anatomical structures.
Existing CT reconstruction works are limited to small-capacity model
architecture, inflexible volume representation, and small-scale training data.
In this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large
feedforward model for reconstructing 3D CT from sparse-view 2D X-ray
projections. X-GRM employs a scalable transformer-based architecture to encode
an arbitrary number of sparse X-ray inputs, where tokens from different views
are integrated efficiently. Then, tokens are decoded into a new volume
representation, named Voxel-based Gaussian Splatting (VoxGS), which enables
efficient CT volume extraction and differentiable X-ray rendering. To support
the training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction
dataset containing around 15,000 CT/X-ray pairs across diverse organs,
including the chest, abdomen, pelvis, and tooth etc. This combination of a
high-capacity model, flexible volume representation, and large-scale training
data empowers our model to produce high-quality reconstructions from various
testing inputs, including in-domain and out-domain X-ray projections. Project
Page: https://github.com/CUHK-AIM-Group/X-GRM.

</details>


### [744] [Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction](https://arxiv.org/pdf/2505.15285)
*Fengting Zhang, Boxu Liang, Qinghao Liu, Min Liu, Xiang Chen, Yaonan Wang*

Main category: eess.IV

TL;DR: ATMRN improves mesh reconstruction by generating adaptive templates from images, outperforming fixed-template methods with a 0.267mm accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh reconstruction uses fixed templates, ignoring anatomical variations and reducing fidelity.

Method: Proposes ATMRN, which creates adaptive templates from images for deformation, validated on OASIS dataset cortical MR images.

Result: Achieves 0.267mm average symmetric surface distance for cortical structures.

Conclusion: ATMRN is generic and adaptable to other modalities and anatomical structures.

Abstract: Mesh reconstruction is a cornerstone process across various applications,
including in-silico trials, digital twins, surgical planning, and navigation.
Recent advancements in deep learning have notably enhanced mesh reconstruction
speeds. Yet, traditional methods predominantly rely on deforming a standardised
template mesh for individual subjects, which overlooks the unique anatomical
variations between them, and may compromise the fidelity of the
reconstructions. In this paper, we propose an adaptive-template-based mesh
reconstruction network (ATMRN), which generates adaptive templates from the
given images for the subsequent deformation, moving beyond the constraints of a
singular, fixed template. Our approach, validated on cortical magnetic
resonance (MR) images from the OASIS dataset, sets a new benchmark in
voxel-to-cortex mesh reconstruction, achieving an average symmetric surface
distance of 0.267mm across four cortical structures. Our proposed method is
generic and can be easily transferred to other image modalities and anatomical
structures.

</details>


### [745] [Linear Convergence of Plug-and-Play Algorithms with Kernel Denoisers](https://arxiv.org/pdf/2505.15318)
*Arghya Sinha, Bhartendu Kumar, Chirayu D. Athalye, Kunal N. Chaudhury*

Main category: eess.IV

TL;DR: The paper analyzes the convergence of Plug-and-Play (PnP) methods using kernel denoisers for linear inverse problems, extending results to nonsymmetric denoisers and providing quantitative bounds on convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified framework for global linear convergence of PnP methods with both symmetric and nonsymmetric kernel denoisers, and to quantify convergence rates for specific tasks like inpainting, deblurring, and superresolution.

Method: Develops a theoretical framework to prove global linear convergence for PnP iterates with kernel denoisers, leveraging the contraction mapping theorem, and derives quantitative bounds on the contraction factor.

Result: Establishes global linear convergence for PnP with nonsymmetric kernel denoisers and provides explicit convergence rates for inpainting, deblurring, and superresolution, validated by numerical experiments.

Conclusion: The work successfully extends convergence guarantees to nonsymmetric denoisers and offers practical insights into the performance of PnP methods for various image reconstruction tasks.

Abstract: The use of denoisers for image reconstruction has shown significant
potential, especially for the Plug-and-Play (PnP) framework. In PnP, a powerful
denoiser is used as an implicit regularizer in proximal algorithms such as ISTA
and ADMM. The focus of this work is on the convergence of PnP iterates for
linear inverse problems using kernel denoisers. It was shown in prior work that
the update operator in standard PnP is contractive for symmetric kernel
denoisers under appropriate conditions on the denoiser and the linear forward
operator. Consequently, we could establish global linear convergence of the
iterates using the contraction mapping theorem. In this work, we develop a
unified framework to establish global linear convergence for symmetric and
nonsymmetric kernel denoisers. Additionally, we derive quantitative bounds on
the contraction factor (convergence rate) for inpainting, deblurring, and
superresolution. We present numerical results to validate our theoretical
findings.

</details>


### [746] [Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart](https://arxiv.org/pdf/2505.15488)
*Shubhrangshu Debsarkar, Bijoy Kundu*

Main category: eess.IV

TL;DR: A study on dynamic FDG PET imaging in rats developed a 15-parameter model for estimating blood input function and kinetic rate constants, then improved it with an LSTM network for semi-automated prediction, achieving a 56.4% reduction in error.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of manual annotations and parameter determination in dynamic FDG PET imaging, the study aims to automate and improve accuracy in estimating blood input functions and kinetic rate constants.

Method: Developed a 15-parameter model for dynamic FDG PET imaging, then used semi-automated segmentation and an LSTM network to predict blood input functions. Cross-validation and midpoint interpolation were employed to enhance performance.

Result: The LSTM-based model with midpoint interpolation achieved a 56.4% improvement in Mean Squared Error (MSE) compared to previous methods.

Conclusion: The study successfully demonstrated the potential of LSTM networks to automate and improve accuracy in dynamic FDG PET imaging, overcoming manual limitations and sparse data challenges.

Abstract: Dynamic FDG PET imaging study of n = 52 rats including 26 control
Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats
(SHR) were performed using a Siemens microPET and Albira trimodal scanner
longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual
output model correcting for spill over contamination and partial volume effects
with peak fitting cost functions was developed for simultaneous estimation of
model corrected blood input function (MCIF) and kinetic rate constants for
dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are
its dependence on manual annotations for the Image Derived Input Function
(IDIF) and manual determination of crucial model parameters to compute MCIF. To
overcome these limitations, we performed semi-automated segmentation and then
formulated a Long-Short-Term Memory (LSTM) cell network to train and predict
MCIF in test data using a concatenation of IDIFs and myocardial inputs and
compared them with reference-modeled MCIF. Thresholding along 2D plane slices
with two thresholds, with T1 representing high-intensity myocardium, and T2
representing lower-intensity rings, was used to segment the area of the LV
blood pool. The resultant IDIF and myocardial TACs were used to compute the
corresponding reference (model) MCIF for all data sets. The segmented IDIF and
the myocardium formed the input for the LSTM network. A k-fold cross validation
structure with a 33:8:11 split and 5 folds was utilized to create the model and
evaluate the performance of the LSTM network for all datasets. To overcome the
sparseness of data as time steps increase, midpoint interpolation was utilized
to increase the density of datapoints beyond time = 10 minutes. The model
utilizing midpoint interpolation was able to achieve a 56.4% improvement over
previous Mean Squared Error (MSE).

</details>


### [747] [Deep Learning Enabled Segmentation, Classification and Risk Assessment of Cervical Cancer](https://arxiv.org/pdf/2505.15505)
*Abdul Samad Shaik, Shashaank Mattur Aswatha, Rahul Jashvantbhai Pandya*

Main category: eess.IV

TL;DR: A novel DL model for cervical cancer detection using Pap smear images achieves high accuracy with fewer parameters, combining segmentation, classification, and risk assessment.


<details>
  <summary>Details</summary>
Motivation: Early detection of cervical cancer via Pap smears is critical, but existing methods are resource-intensive. This study aims to improve efficiency and accuracy.

Method: Proposed a Multi-Resolution Fusion Deep Convolutional Network for handling varying image resolutions, with multi-task learning for segmentation and classification.

Result: Achieved 90% classification accuracy, 0.83 IoU score, and comparable performance to state-of-the-art models with 85x fewer parameters than VGG-19.

Conclusion: The model is efficient and effective for cervical cancer detection, offering a probabilistic risk assessment for prognosis.

Abstract: Cervical cancer, the fourth leading cause of cancer in women globally,
requires early detection through Pap smear tests to identify precancerous
changes and prevent disease progression. In this study, we performed a focused
analysis by segmenting the cellular boundaries and drawing bounding boxes to
isolate the cancer cells. A novel Deep Learning (DL) architecture, the
``Multi-Resolution Fusion Deep Convolutional Network", was proposed to
effectively handle images with varying resolutions and aspect ratios, with its
efficacy showcased using the SIPaKMeD dataset. The performance of this DL model
was observed to be similar to the state-of-the-art models, with accuracy
variations of a mere 2\% to 3\%, achieved using just 1.7 million learnable
parameters, which is approximately 85 times less than the VGG-19 model.
Furthermore, we introduced a multi-task learning technique that simultaneously
performs segmentation and classification tasks and begets an Intersection over
Union score of 0.83 and a classification accuracy of 90\%. The final stage of
the workflow employs a probabilistic approach for risk assessment, extracting
feature vectors to predict the likelihood of normal cells progressing to
malignant states, which can be utilized for the prognosis of cervical cancer.

</details>


### [748] [Combining Progressive Image Compression and Random Access in DNA Data Storage](https://arxiv.org/pdf/2505.15632)
*Xavier Pic, Raja Appuswamy*

Main category: eess.IV

TL;DR: PIC-DNA is a JPEG2000-based progressive image coder for DNA storage, enabling random access and adaptive read-cost for efficient retrieval of specific images from oligo pools.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost and reliability challenges in DNA data storage, particularly for retrieving specific files from oligo pools without sequencing all data.

Method: Introduces PIC-DNA, a novel JPEG2000-based coder with built-in random access and progressive decoding to optimize read-cost and quality.

Result: PIC-DNA improves read-cost efficiency by enabling targeted retrieval and adaptive decoding based on user constraints.

Conclusion: PIC-DNA offers a scalable solution for DNA storage by integrating random access and progressive decoding, addressing key bottlenecks in the field.

Abstract: The exponential increase in storage demand and low lifespan of data storage
devices has resulted in long-term archival and preservation emerging as a
critical bottlenecks in data storage. In order to meet this demand, researchers
are now investigating novel forms of data storage media. The high density, long
lifespan and low energy needs of synthetic DNA make it a promising candidate
for long-term data archival. However, current DNA data storage technologies are
facing challenges with respect to cost (writing data to DNA is expensive) and
reliability (reading and writing data is error prone). Thus, data compression
and error correction are crucial to scale DNA storage. Additionally, the DNA
molecules encoding several files are very often stored in the same place,
called an oligo pool. For this reason, without random access solutions, it is
relatively impractical to decode a specific file from the pool, because all the
oligos from all the files need to first be sequenced, which greatly
deteriorates the read cost. This paper introduces PIC-DNA - a novel
JPEG2000-based progressive image coder adapted to DNA data storage. This coder
directly includes a random access process in its coding system, allowing for
the retrieval of a specific image from a pool of oligos encoding several
images. The progressive decoder can dynamically adapt the read cost according
to the user's cost and quality constraints at decoding time. Both the random
access and progressive decoding greatly improve on the read-cost of image
coders adapted to DNA.

</details>


### [749] [Generalizing Medical Image Representations via Quaternion Wavelet Networks](https://arxiv.org/pdf/2310.10224)
*Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello*

Main category: eess.IV

TL;DR: QUAVE is a generalizable, data- and task-agnostic framework for medical image analysis, using quaternion wavelet transforms to extract and weigh salient features, improving neural network performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of methodological standards and domain shifts in medical imaging by creating a flexible framework for feature extraction.

Method: QUAVE uses quaternion wavelet transforms to extract sub-bands (low/high-frequency features) and weighs them for input into neural models, replacing standard data samples.

Result: Extensive experiments show QUAVE improves performance in tasks like reconstruction, segmentation, and modality translation, and works with real/quaternion-valued models.

Conclusion: QUAVE is effective, generalizable, and robust to domain shifts, offering a versatile solution for medical image analysis.

Abstract: Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios
and robust to domain shifts. The full code is available at:
https://github.com/ispamm/QWT.

</details>


### [750] [Variable Rate Learned Wavelet Video Coding using Temporal Layer Adaptivity](https://arxiv.org/pdf/2410.15873)
*Anna Meyer, André Kaup*

Main category: eess.IV

TL;DR: The paper introduces variable rate support and quality adaptation for learned wavelet video coders, achieving -32% bitrate savings compared to baseline MCTF models.


<details>
  <summary>Details</summary>
Motivation: To enhance coding efficiency and scalability in learned wavelet video coders by introducing variable rate support and quality adaptation mechanisms.

Method: Proposes a multi-stage training strategy for handling multiple temporal layers and integrates variable rate support with quality adaptation.

Result: Achieves at least -32% Bj{\o}ntegaard Delta bitrate savings over baseline learned MCTF models.

Conclusion: The proposed extensions significantly improve coding efficiency, with code made publicly available for reproducibility.

Abstract: Learned wavelet video coders provide an explainable framework by performing
discrete wavelet transforms in temporal, horizontal, and vertical dimensions.
With a temporal transform based on motion-compensated temporal filtering
(MCTF), spatial and temporal scalability is obtained. In this paper, we
introduce variable rate support and a mechanism for quality adaption to
different temporal layers for a higher coding efficiency. Moreover, we propose
a multi-stage training strategy that allows training with multiple temporal
layers. Our experiments demonstrate Bj{\o}ntegaard Delta bitrate savings of at
least -32% compared to a learned MCTF model without these extensions. Training
and inference code is available at: https://github.com/FAU-LMS/Learned-pMCTF.

</details>


### [751] [Aggregation Schemes for Single-Vector WSI Representation Learning in Digital Pathology](https://arxiv.org/pdf/2501.17822)
*Sobhan Hemati, Ghazal Alabtah, Saghir Alfasly, H. R. Tizhoosh*

Main category: eess.IV

TL;DR: The paper evaluates various set representation learning techniques for aggregating patch embeddings in Whole Slide Images (WSIs) to improve WSI search performance.


<details>
  <summary>Details</summary>
Motivation: Efficiently integrating WSIs in computational pathology requires assigning a single high-quality feature vector (embedding) to each WSI, but their gigapixel nature makes direct processing infeasible.

Method: The study compares multiple aggregation techniques (e.g., pooling, Deep Sets, Memory networks, Focal attention, GMM Fisher Vector) and benchmarks them against a non-aggregating approach using patch embeddings.

Result: The performance of these methods is evaluated on WSIs from four primary sites (bladder, breast, kidney, colon) in TCGA.

Conclusion: The paper provides insights into the effectiveness of different set representation learning techniques for WSI retrieval.

Abstract: A crucial step to efficiently integrate Whole Slide Images (WSIs) in
computational pathology is assigning a single high-quality feature vector,
i.e., one embedding, to each WSI. With the existence of many pre-trained deep
neural networks and the emergence of foundation models, extracting embeddings
for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,
given their high resolution and gigapixel nature, inputting them into existing
GPUs as a single image is not feasible. As a result, WSIs are usually split
into many patches. Feeding each patch to a pre-trained model, each WSI can then
be represented by a set of patches, hence, a set of embeddings. Hence, in such
a setup, WSI representation learning reduces to set representation learning
where for each WSI we have access to a set of patch embeddings. To obtain a
single embedding from a set of patch embeddings for each WSI, multiple
set-based learning schemes have been proposed in the literature. In this paper,
we evaluate the WSI search performance of multiple recently developed
aggregation techniques (mainly set representation learning techniques)
including simple average or max pooling operations, Deep Sets, Memory networks,
Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse
and binary Fisher Vector on four different primary sites including bladder,
breast, kidney, and Colon from TCGA. Further, we benchmark the search
performance of these methods against the median of minimum distances of patch
embeddings, a non-aggregating approach used for WSI retrieval.

</details>


### [752] [Diff-Unfolding: A Model-Based Score Learning Framework for Inverse Problems](https://arxiv.org/pdf/2505.11393)
*Yuanhao Wang, Shirin Shoushtari, Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: Diff-Unfolding is a framework for learning posterior score functions in conditional diffusion models, integrating physical measurement operators into a modular network for improved generalization and performance in inverse problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling image priors for inverse problems by incorporating measurement operators into diffusion models, enabling generalization without retraining.

Method: Diff-Unfolding unrolls an optimization scheme, decoupling the measurement model from the learned image prior, and derives the posterior score from a composite optimization formulation.

Result: Achieves state-of-the-art performance in image restoration and MRI, with up to 2 dB PSNR improvement, 22.7% LPIPS reduction, and efficient inference (0.63 seconds per image).

Conclusion: Diff-Unfolding is a practical, efficient, and high-performing solution for inverse problems, validated by theoretical and experimental results.

Abstract: Diffusion models are extensively used for modeling image priors for inverse
problems. We introduce \emph{Diff-Unfolding}, a principled framework for
learning posterior score functions of \emph{conditional diffusion models} by
explicitly incorporating the physical measurement operator into a modular
network architecture. Diff-Unfolding formulates posterior score learning as the
training of an unrolled optimization scheme, where the measurement model is
decoupled from the learned image prior. This design allows our method to
generalize across inverse problems at inference time by simply replacing the
forward operator without retraining. We theoretically justify our unrolling
approach by showing that the posterior score can be derived from a composite
model-based optimization formulation. Extensive experiments on image
restoration and accelerated MRI show that Diff-Unfolding achieves
state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS
by $22.7\%$, while being both compact (47M parameters) and efficient (0.72
seconds per $256 \times 256$ image). An optimized C++/LibTorch implementation
further reduces inference time to 0.63 seconds, underscoring the practicality
of our approach.

</details>
