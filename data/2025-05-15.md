<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 90]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.LG](#cs.LG) [Total: 84]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/pdf/2505.08828)
*Eduardo Araujo Oliveira, Madhavi Mohoni, Sonsoles LÃ³pez-Pernas, Mohammed Saqr*

Main category: cs.CL

TL;DR: The paper explores using authorship verification (AV) to measure AI assistance in academic writing, promoting transparency and student development. It develops an adapted AV method, tests it on expanded datasets, and demonstrates its effectiveness in distinguishing human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To address challenges in measuring human-AI collaboration in education, focusing on transparency, interpretability, and student growth rather than punitive measures.

Method: Three-stage approach: dataset selection/expansion (including LLM-generated texts), AV method development (Feature Vector Difference), and systematic evaluation across scenarios.

Result: The AV classifier effectively identifies stylometric discrepancies, measures human-AI collaboration, and aids educators in academic integrity investigations.

Conclusion: The work advances AV technology, providing insights into AI-driven academic writing dynamics and supporting ethical AI use in education.

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [2] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/pdf/2505.08891)
*Daeun Hwang, Samuel Shields, Alex Calderwood, Shi Johnson-Bey, Michael Mateas, Noah Wardrip-Fruin, Edward F. Melcer*

Main category: cs.CL

TL;DR: The paper compares static and dynamic narrative versions of an educational game, Academical, showing dynamic narratives enhance engagement but pose design challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of dynamic narratives on learner motivation, leveraging AI advances for adaptive educational content.

Method: Comparison of a static branching plot version and a dynamic plot-sequencing version of Academical, an educational game on research ethics.

Result: Dynamic narratives improve engagement through responsive content and varied choices, though balancing pedagogy with narrative dynamics is challenging.

Conclusion: The study highlights AI-driven dynamic narratives' potential in education, with design implications for future work.

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [3] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/pdf/2505.08996)
*Adele E Goldberg, Supantho Rakshit, Jennifer Hu, Kyle Mahowald*

Main category: cs.CL

TL;DR: The paper revisits claims of LLMs underperforming humans in comprehension, showing human performance drops without rereading, while models like GPT-4 and Falcon-180B-Chat outperform humans. GPT-01 achieves perfect accuracy, and both humans and models struggle with reciprocal actions. The study highlights flaws in prior evaluations and suggests LLMs may not be inherently weaker than humans.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that LLMs are weaker than humans in language comprehension by re-evaluating human and model performance under more naturalistic conditions.

Method: A preregistered study comparing human responses with and without rereading, using the same stimuli as prior work, and analyzing model performance (Falcon-180B-Chat, GPT-4, GPT-01) and human accuracy. Additional analyses included log probabilities, recoding responses, and grammaticality ratings.

Result: Human accuracy dropped to 73% without rereading, below Falcon-180B-Chat (76%) and GPT-4 (81%). GPT-01 achieved perfect accuracy. Both humans and models struggled with reciprocal actions. GPT-4o aligned with naive or expert judgments based on prompts.

Conclusion: The study reveals systematic underestimation of LLM performance and flaws in prior evaluations, challenging the notion that current models are inherently weaker than humans in comprehension.

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [4] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/pdf/2505.09005)
*Nicole Cuneo, Eleanor Graves, Supantho Rakshit, Adele E. Goldberg*

Main category: cs.CL

TL;DR: GPT-4 demonstrates reliable metalinguistic skills in understanding information structure and acceptability in English, replicating human-like judgments and revealing a causal relationship between prominence and LDD acceptability.


<details>
  <summary>Details</summary>
Motivation: To assess whether LMs like GPT-4 can capture subtle linguistic relationships, specifically the connection between information structure and acceptability in long-distance dependency constructions.

Method: Probing GPT-4 on tasks involving information structure judgments and acceptability ratings, including zero-shot, explicit tasks and controlled manipulations of prominence.

Result: GPT-4 replicated human-like interactions between information structure and acceptability, showing a causal effect of prominence on LDD acceptability.

Conclusion: GPT-4's performance suggests a tight relationship between natural and model-generated English, highlighting the need for further exploration of LM linguistic capabilities.

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [5] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/pdf/2505.09039)
*Jingfeng Chen, Raghuveer Thirukovalluru, Junlin Wang, Kaiwei Luo, Bhuwan Dhingra*

Main category: cs.CL

TL;DR: ACPO is a self-supervised method to reduce factoid hallucinations in LLMs by leveraging atomic consistency signals, outperforming supervised baselines without external resources.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate plausible but incorrect answers (factoid hallucinations). Existing alignment methods rely on external models or knowledge bases, which may not be accessible.

Method: ACPO uses atomic consistency signals (agreement of facts across stochastic responses) to identify high- and low-quality data pairs for self-supervised model alignment.

Result: ACPO outperforms FactAlign by 1.95 points on LongFact and BioGen datasets, improving factual accuracy without external supervision.

Conclusion: ACPO offers a scalable, efficient solution for enhancing factual reliability in LLMs without dependency on external models or knowledge bases.

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [6] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/pdf/2505.09056)
*Brandon Smith, Mohamed Reda Bouadjenek, Tahsin Alamgir Kheya, Phillip Dawson, Sunil Aryal*

Main category: cs.CL

TL;DR: The paper investigates output similarity, variability, and ethical implications of 12 LLMs, revealing differences in style, similarity, and bias.


<details>
  <summary>Details</summary>
Motivation: To understand how similar and varied LLM outputs are, and how they compare ethically across models.

Method: Analyzed ~3M texts from 12 LLMs using 5,000 diverse prompts.

Result: Same-model outputs are more similar than human texts; GPT-4 is more varied; some models show better ethical standards.

Conclusion: Findings provide insights for LLM development and ethical evaluation.

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [7] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/pdf/2505.09068)
*Jennifer Haase, Paul H. P. Hanel, Sebastian Pokutta*

Main category: cs.CL

TL;DR: S-DAT is a scalable, multilingual framework for automated divergent thinking assessment, overcoming limitations of traditional methods by using language models and semantic distance.


<details>
  <summary>Details</summary>
Motivation: Traditional creativity assessments are labor-intensive, language-specific, and subjective, hindering scalability and cross-cultural applicability.

Method: S-DAT leverages large language models and multilingual embeddings to compute semantic distance as a proxy for divergent thinking.

Result: Evaluated across 11 languages, S-DAT shows robust, consistent scoring, convergent validity with DT measures, and discriminant validity with convergent thinking.

Conclusion: S-DAT enables inclusive, global-scale creativity research, offering a fairer, more comprehensive tool for cognitive flexibility evaluation.

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [8] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/pdf/2505.09082)
*Sophie Zhang, Zhiming Lin*

Main category: cs.CL

TL;DR: CEC-Zero is a reinforcement learning framework for LLMs to self-correct Chinese text errors without supervision, improving accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Address reliability and generalization challenges in LLMs for Chinese Spelling Correction (CSC).

Method: Proposes CEC-Zero, an RL framework integrating LLMs' generative power for autonomous error strategy learning without external supervision.

Result: RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization.

Conclusion: CEC-Zero offers a scalable, reliable solution for Chinese NLP applications and sets a new paradigm for self-improving language models.

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [9] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/pdf/2505.09269)
*Ulrich Frank, Pierre Maier*

Main category: cs.CL

TL;DR: A new UML modeling tool integrates class and object diagrams, supports object execution, and enhances teaching and software architecture.


<details>
  <summary>Details</summary>
Motivation: To advance conventional UML tools by enabling integration of diagrams and object execution, benefiting both software development and education.

Method: Design and implementation of a UML tool with integrated class/object diagrams and object execution capabilities.

Result: The tool supports new software architectures and improves learning experiences in teaching.

Conclusion: The project demonstrates how research can yield valuable side results, exemplified by this innovative UML tool.

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [10] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/pdf/2505.09286)
*Jiin Park, Misuk Kim*

Main category: cs.CL

TL;DR: The paper proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection in online reviews, achieving high performance with automatically generated labels.


<details>
  <summary>Details</summary>
Motivation: Existing studies are limited by domain/language specificity and reliance on supervised learning, which requires large labeled datasets.

Method: The framework uses clustering for aspect extraction and aspect-aware embedding vectors with negative sampling. Pretrained language models are fine-tuned to evaluate label quality.

Result: The framework outperforms large language models in consistency and scalability, with human evaluation confirming label quality comparable to manual labels.

Conclusion: The study demonstrates a robust, adaptable approach for multi-aspect labeling, with future work aimed at review summarization and AI integration.

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [11] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/pdf/2505.09595)
*Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir*

Main category: cs.CL

TL;DR: WorldView-Bench is introduced to evaluate Global Cultural Inclusivity (GCI) in LLMs, addressing Western-centric biases through multiplexity principles and multi-agent systems, achieving significant improvements in cultural balance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs reinforce Western-centric norms, limiting global cultural plurality. Existing benchmarks fail to capture this bias, necessitating a new approach.

Method: WorldView-Bench uses free-form generative evaluation and multiplexity principles, implemented via contextual prompts and multi-agent systems.

Result: MAS-Implemented Multiplex LLMs increased Perspectives Distribution Score entropy from 13% to 94%, with improved sentiment (67.7%) and cultural balance.

Conclusion: Multiplex-aware AI evaluation can mitigate cultural bias, promoting more inclusive and ethically aligned LLMs.

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [12] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/pdf/2505.09316)
*Hongjin Qian, Zheng Liu*

Main category: cs.CL

TL;DR: InForage, a reinforcement learning framework, enhances LLMs by enabling adaptive retrieval during inference, outperforming traditional static methods in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of static retrieval-augmented LLMs for complex, evolving tasks by leveraging dynamic, adaptive retrieval inspired by Information Foraging Theory.

Method: Proposes InForage, a reinforcement learning framework that rewards intermediate retrieval quality, using a human-guided dataset for training.

Result: Superior performance in general QA, multi-hop reasoning, and real-time web QA tasks compared to baseline methods.

Conclusion: InForage effectively builds adaptive, efficient reasoning agents for complex information needs.

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [13] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/pdf/2505.09338)
*Jingcheng Niu, Xingdi Yuan, Tong Wang, Hamidreza Saghir, Amir H. Abdi*

Main category: cs.CL

TL;DR: The paper introduces "contextual entrainment," a phenomenon where language models (LMs) favor tokens from the input context, regardless of relevance. It identifies "entrainment heads" as the cause and shows how disabling them reduces distraction.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate how LMs get distracted by irrelevant context, providing a mechanistic perspective.

Method: Uses statistical analysis and a novel differentiable masking method to identify "entrainment heads" responsible for contextual entrainment. Tests counterfactual vs. factual prompts.

Result: LMs disproportionately favor context tokens. Disabling entrainment heads reduces this effect, improving model focus.

Conclusion: Contextual entrainment is a mechanistic but semantically modulated phenomenon. Identifying and mitigating it advances LM analysis and distraction reduction.

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [14] [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388)
*An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu*

Main category: cs.CL

TL;DR: Qwen3 is the latest version of the Qwen model family, featuring advanced performance, efficiency, and multilingual support. It integrates thinking and non-thinking modes, introduces a thinking budget mechanism, and reduces computational costs for smaller models. It outperforms benchmarks and expands multilingual support to 119 languages.


<details>
  <summary>Details</summary>
Motivation: To enhance performance, efficiency, and multilingual capabilities in large language models while eliminating the need for model switching and optimizing resource allocation.

Method: Qwen3 includes dense and MoE architectures, integrates thinking and non-thinking modes, and employs a thinking budget mechanism for adaptive resource allocation.

Result: Achieves state-of-the-art results in benchmarks like code generation and mathematical reasoning, supports 119 languages, and reduces computational costs for smaller models.

Conclusion: Qwen3 advances LLM capabilities with unified modes, efficient resource use, and broad multilingual support, making it accessible under Apache 2.0 for community-driven development.

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [15] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/pdf/2505.09407)
*Subrit Dikshit, Ritu Tiwari, Priyank Jain*

Main category: cs.CL

TL;DR: QEDACVC proposes a quantum computing-based approach for multilingual machine translation, achieving 82% accuracy on the OPUS dataset.


<details>
  <summary>Details</summary>
Motivation: Existing translation services rely on classical computing; QEDACVC explores quantum computing for improved multilingual translation.

Method: Uses quantum encoder-decoder architecture with quantum convolution, pooling, variational circuits, and attention mechanisms.

Result: Achieves 82% accuracy on English, French, German, and Hindi translations.

Conclusion: QEDACVC demonstrates the potential of quantum computing for multilingual machine translation.

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [16] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/pdf/2505.09519)
*Zongqian Li, Yixuan Su, Nigel Collier*

Main category: cs.CL

TL;DR: PT-MoE integrates matrix decomposition and MoE routing for efficient prompt tuning, outperforming PT and LoRA in QA and math tasks with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing counter-intuitive phenomena in PEFT methods, such as router integration in PT not universally improving performance, and leveraging modular PT for better efficiency.

Method: Proposes PT-MoE, combining matrix decomposition with MoE routing for efficient prompt tuning.

Result: Achieves state-of-the-art performance in QA (1.49 F1 over PT) and math tasks (10.75 accuracy over PT), using 25% fewer parameters than LoRA.

Conclusion: PT-MoE's integration of decomposition and MoE enables cross-task consistency and generalization, offering insights for future PEFT methods.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [17] [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/pdf/2402.01383)
*Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan*

Main category: cs.CL

TL;DR: A survey on LLM-based NLG evaluation methods, discussing their taxonomy, pros, cons, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Traditional NLG evaluation metrics (e.g., n-gram overlap) are inadequate, and LLMs like ChatGPT show promise for improving evaluation.

Method: Taxonomy of LLM-based NLG evaluation methods: metrics from LLMs, prompting, fine-tuning, and human-LLM collaboration.

Result: LLMs offer diverse evaluation approaches but have trade-offs in effectiveness and practicality.

Conclusion: Open problems remain; future research should address these to advance LLM-based NLG evaluation.

Abstract: Evaluating natural language generation (NLG) is a vital but challenging
problem in natural language processing. Traditional evaluation metrics mainly
capturing content (e.g. n-gram) overlap between system outputs and references
are far from satisfactory, and large language models (LLMs) such as ChatGPT
have demonstrated great potential in NLG evaluation in recent years. Various
automatic evaluation methods based on LLMs have been proposed, including
metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM
collaborative evaluation. In this survey, we first give a taxonomy of LLM-based
NLG evaluation methods, and discuss their pros and cons, respectively. Lastly,
we discuss several open problems in this area and point out future research
directions.

</details>


### [18] [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/pdf/2404.03080)
*Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang*

Main category: cs.CL

TL;DR: The paper introduces the Materials Knowledge Graph (MKG), leveraging AI and NLP to organize materials science literature into structured data, reducing reliance on traditional experimental methods.


<details>
  <summary>Details</summary>
Motivation: The dispersed nature of materials science knowledge and inefficiencies in traditional methods hinder rapid innovation. AI integration offers a solution but requires precise data handling.

Method: MKG uses NLP and large language models to extract and organize research into structured triples (162,605 nodes, 731,772 edges) with a designed ontology.

Result: MKG enhances data usability, enables efficient link prediction, and reduces dependency on costly experimental approaches.

Conclusion: MKG streamlines materials research and paves the way for advanced science knowledge graphs.

Abstract: Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges to the efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques integrated with large language models to extract
and systematically organize a decade's worth of high-quality research into
structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes
information into comprehensive labels such as Name, Formula, and Application,
structured around a meticulously designed ontology, thus enhancing data
usability and integration. By implementing network-based algorithms, MKG not
only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.

</details>


### [19] [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/pdf/2410.04526)
*Siqiao Xue, Xiaojing Li, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei*

Main category: cs.CL

TL;DR: FAMMA is an open-source benchmark for financial multilingual multimodal QA, challenging LLMs with complex reasoning questions. It includes two versions (Basic and LivePro) and shows that training on reasoning trajectories improves performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' abilities in answering complex financial questions requiring advanced knowledge and multilingual, multimodal understanding.

Method: Created FAMMA-Basic (1,945 questions from textbooks/exams) and FAMMA-LivePro (103 expert-created questions). Experiments tested LLMs like GPT-o1 and DeepSeek-R1, and fine-tuned Qwen models using reasoning trajectories.

Result: FAMMA poses a significant challenge to LLMs. Training on reasoning trajectories improved performance on FAMMA-LivePro.

Conclusion: FAMMA is a valuable benchmark for evaluating and improving LLMs in financial multilingual multimodal QA, with released resources for further research.

Abstract: In this paper, we introduce FAMMA, an open-source benchmark for
\underline{f}in\underline{a}ncial \underline{m}ultilingual
\underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims
to evaluate the abilities of large language models (LLMs) in answering complex
reasoning questions that require advanced financial knowledge. The benchmark
has two versions: FAMMA-Basic consists of 1,945 questions extracted from
university textbooks and exams, along with human-annotated answers and
rationales; FAMMA-LivePro consists of 103 novel questions created by human
domain experts, with answers and rationales held out from the public for a
contamination-free evaluation. These questions cover advanced knowledge of 8
major subfields in finance (e.g., corporate finance, derivatives, and portfolio
management). Some are in Chinese or French, while a majority of them are in
English. Each question has some non-text data such as charts, diagrams, or
tables. Our experiments reveal that FAMMA poses a significant challenge on
LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,
we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,
and fine-tuned a series of open-source Qwen models using this reasoning data.
We found that training a model on these reasoning trajectories can
significantly improve its performance on FAMMA-LivePro. We released our
leaderboard, data, code, and trained models at
https://famma-bench.github.io/famma/.

</details>


### [20] [P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs](https://arxiv.org/pdf/2411.09116)
*Yidan Zhang, Yu Wan, Boyi Deng, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: The paper introduces P-MMEval, a comprehensive multilingual multitask benchmark to evaluate LLMs beyond traditional NLP tasks, offering insights into performance across models, tasks, and languages.


<details>
  <summary>Details</summary>
Motivation: To address the limited scope of previous LLM assessments by creating a benchmark that covers diverse multilingual tasks and provides consistent language coverage.

Method: Developed P-MMEval, a large-scale benchmark with fundamental and specialized datasets, parallel samples, and extensive experiments on multilingual models.

Result: Evaluated model performances across tasks, languages, and factors like model size and prompts, and examined English-to-other-language knowledge transfer.

Conclusion: P-MMEval provides valuable insights for future research, with the dataset publicly available for further exploration.

Abstract: Recent advancements in large language models (LLMs) showcase varied
multilingual capabilities across tasks like translation, code generation, and
reasoning. Previous assessments often limited their scope to fundamental
natural language processing (NLP) or isolated capability-specific tasks. To
alleviate this drawback, we aim to present a comprehensive multilingual
multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark
covering effective fundamental and capability-specialized datasets.
Furthermore, P-MMEval delivers consistent language coverage across various
datasets and provides parallel samples. Finally, we conduct extensive
experiments on representative multilingual model series to compare performances
across models and tasks, explore the relationship between multilingual
performances and factors such as tasks, model sizes, languages, and prompts,
and examine the effectiveness of knowledge transfer from English to other
languages. The resulting insights are intended to offer valuable guidance for
future research. The dataset is available at
https://huggingface.co/datasets/Qwen/P-MMEval.

</details>


### [21] [Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples](https://arxiv.org/pdf/2502.09650)
*Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu*

Main category: cs.CL

TL;DR: The paper challenges the assumption that more clean data always improves LLM alignment, proposing that overly difficult examples hinder performance. Selective DPO, filtering such examples, boosts alignment by 9-16%.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked mismatch between model capacity and example difficulty in LLM alignment, which can degrade performance.

Method: Introduces Selective DPO, which filters overly difficult examples based on model capacity, validated through systematic experiments.

Result: Shows that overly difficult examples degrade performance, and Selective DPO improves alignment by 9-16% in win rates.

Conclusion: Aligning data difficulty with model capacity is crucial for effective LLM alignment, offering a transformative strategy.

Abstract: The alignment of large language models (LLMs) often assumes that using more
clean data yields better outcomes, overlooking the match between model capacity
and example difficulty. Challenging this, we propose a new principle:
Preference data vary in difficulty, and overly difficult examples hinder
alignment, by exceeding the model's capacity. Through systematic
experimentation, we validate this principle with three key findings: (1)
preference examples vary in difficulty, as evidenced by consistent learning
orders across alignment runs; (2) overly difficult examples significantly
degrade performance across four LLMs and two datasets; and (3) the capacity of
a model dictates its threshold for handling difficult examples, underscoring a
critical relationship between data selection and model capacity. Building on
this principle, we introduce Selective DPO, which filters out overly difficult
examples. This simple adjustment improves alignment performance by 9-16% in win
rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a
series of DPO variants with different algorithmic adjustments. Together, these
results illuminate the importance of aligning data difficulty with model
capacity, offering a transformative perspective for improving alignment
strategies in LLMs. Code is available at
https://github.com/glorgao/SelectiveDPO.

</details>


### [22] [PropNet: a White-Box and Human-Like Network for Sentence Representation](https://arxiv.org/pdf/2502.10725)
*Fei Yang*

Main category: cs.CL

TL;DR: PropNet is a white-box, human-like sentence representation network inspired by cognitive science, addressing interpretability gaps in Transformer-based models, though it lags in performance on STS tasks.


<details>
  <summary>Details</summary>
Motivation: To address the black-box nature and interpretability issues of Transformer-based sentence embedding models, which raise concerns about bias, trust, and safety.

Method: Proposes PropNet, a hierarchical network based on propositions in sentences, inspired by cognitive science.

Result: PropNet shows a performance gap compared to SOTA models in STS tasks but offers interpretability and insights into human cognitive processes.

Conclusion: PropNet provides a foundation for inherently interpretable sentence representation, with potential for future improvements.

Abstract: Transformer-based embedding methods have dominated the field of sentence
representation in recent years. Although they have achieved remarkable
performance on NLP missions, such as semantic textual similarity (STS) tasks,
their black-box nature and large-data-driven training style have raised
concerns, including issues related to bias, trust, and safety. Many efforts
have been made to improve the interpretability of embedding models, but these
problems have not been fundamentally resolved. To achieve inherent
interpretability, we propose a purely white-box and human-like sentence
representation network, PropNet. Inspired by findings from cognitive science,
PropNet constructs a hierarchical network based on the propositions contained
in a sentence. While experiments indicate that PropNet has a significant gap
compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies
reveal substantial room for improvement. Additionally, PropNet enables us to
analyze and understand the human cognitive processes underlying STS benchmarks.

</details>


### [23] [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/pdf/2505.07890)
*Kutay ErtÃ¼rk, Furkan AltÄ±nÄ±ÅÄ±k, Ä°rem SarÄ±altÄ±n, Ãmer Nezih Gerek*

Main category: cs.CL

TL;DR: TSLFormer is a lightweight Turkish Sign Language recognition model using 3D joint positions for efficient, real-time sign translation.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and robust sign language recognition by treating gestures as ordered sequences, leveraging linguistic similarities.

Method: Uses 3D joint positions from Mediapipe, models sign language as sequence-to-sequence translation with transformers.

Result: Achieves competitive performance on AUTSL dataset (36k samples, 227 words) with minimal computational cost.

Conclusion: Joint-based input enables real-time, mobile assistive systems for hearing-impaired users.

Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence
translation, inspired by the linguistic nature of sign languages and the
success of transformers in natural language processing. Since TSLFormer uses
the self-attention mechanism, it effectively captures temporal co-occurrence
within gesture sequences and highlights meaningful motion patterns as words
unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different
words, TSLFormer achieves competitive performance with minimal computational
cost. These results show that joint-based input is sufficient for enabling
real-time, mobile, and assistive communication systems for hearing-impaired
individuals.

</details>


### [24] [Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference](https://arxiv.org/pdf/2503.10652)
*Han Wang, Jacek Pawlak, Aruna Sivakumar*

Main category: cs.CL

TL;DR: The study explores using LLMs to simulate consumer choices in energy-related SP surveys, finding limited practical accuracy but potential for scalability and qualitative insights.


<details>
  <summary>Details</summary>
Motivation: Traditional SP survey methods are costly and time-consuming, prompting interest in LLMs for simulating consumer choices efficiently.

Method: Test scenarios assessed LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) for simulation performance, focusing on prompt design, reasoning methods, and integration with choice models.

Result: LLMs achieve above-random accuracy (DeepSeek-R1 highest at 77%) but remain insufficient for practical use. Reasoning LLMs show qualitative potential in data analysis.

Conclusion: Pre-trained LLMs offer scalability but need refinement. Future work should focus on prompt design, reasoning, and fine-tuning.

Abstract: Survey research plays a crucial role in studies by capturing consumer
preferences and informing policy decisions. Stated preference (SP) surveys help
researchers understand how individuals make trade-offs in hypothetical,
potentially futuristic, scenarios. However, traditional methods are costly,
time-consuming, and affected by respondent fatigue and ethical constraints.
Large language models (LLMs) have shown remarkable capabilities in generating
human-like responses, prompting interest in their use in survey research. This
study investigates LLMs for simulating consumer choices in energy-related SP
surveys and explores their integration into data collection and analysis
workflows. Test scenarios were designed to assess the simulation performance of
several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and
aggregated levels, considering prompt design, in-context learning (ICL),
chain-of-thought (CoT) reasoning, model types, integration with traditional
choice models, and potential biases. While LLMs achieve accuracy above random
guessing, performance remains insufficient for practical simulation use.
Cloud-based LLMs do not consistently outperform smaller local models.
DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms
non-reasoning LLMs in accuracy, factor identification, and choice distribution
alignment. Previous SP choices are the most effective input; longer prompts
with more factors reduce accuracy. Mixed logit models can support LLM prompt
refinement. Reasoning LLMs show potential in data analysis by indicating factor
significance, offering a qualitative complement to statistical models. Despite
limitations, pre-trained LLMs offer scalability and require minimal historical
data. Future work should refine prompts, further explore CoT reasoning, and
investigate fine-tuning techniques.

</details>


### [25] [Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark](https://arxiv.org/pdf/2503.17599)
*Zheqing Li, Yiying Yang, Jiping Lang, Wenhao Jiang, Yuhang Zhao, Shuang Li, Dingqian Wang, Zhu Lin, Xuanna Li, Yuze Tang, Jiexian Qiu, Xiaolin Lu, Hongji Yu, Shuang Chen, Yuhua Bi, Xiaofei Zeng, Yixian Chen, Junrong Chen, Lin Yao*

Main category: cs.CL

TL;DR: The paper proposes GPBench, a novel evaluation framework to assess LLMs' capability as general practitioners, finding current models inadequate without human oversight.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world clinical alignment, leaving LLMs' GP competency uncertain.

Method: Introduces GPBench, a benchmark with expert-annotated data, and evaluates ten LLMs.

Result: Current LLMs are not ready for GP deployment without human oversight.

Conclusion: Further optimization tailored to GP responsibilities is needed.

Abstract: Large Language Models (LLMs) have demonstrated considerable potential in
general practice. However, existing benchmarks and evaluation frameworks
primarily depend on exam-style or simplified question-answer formats, lacking a
competency-based structure aligned with the real-world clinical
responsibilities encountered in general practice. Consequently, the extent to
which LLMs can reliably fulfill the duties of general practitioners (GPs)
remains uncertain. In this work, we propose a novel evaluation framework to
assess the capability of LLMs to function as GPs. Based on this framework, we
introduce a general practice benchmark (GPBench), whose data are meticulously
annotated by domain experts in accordance with routine clinical practice
standards. We evaluate ten state-of-the-art LLMs and analyze their
competencies. Our findings indicate that current LLMs are not yet ready for
deployment in such settings without human oversight, and further optimization
specifically tailored to the daily responsibilities of GPs is essential.

</details>


### [26] [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/pdf/2503.21696)
*Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper introduces Embodied Reasoner, a model for interactive embodied tasks, outperforming advanced visual reasoning models like OpenAI o1 and Claude-3.7.


<details>
  <summary>Details</summary>
Motivation: Existing deep thinking models excel in mathematical and coding tasks but lack exploration in embodied domains requiring continuous interaction.

Method: The model uses a three-stage training pipeline: imitation learning, self-exploration, and self-correction, trained on 9.3k Observation-Thought-Action trajectories.

Result: The model outperforms OpenAI o1, o3-mini, and Claude-3.7 by +9%, 24%, and +13%, with fewer repeated searches and logical inconsistencies.

Conclusion: Embodied Reasoner excels in complex long-horizon tasks and real-world environments, demonstrating superior reasoning and interaction capabilities.

Abstract: Recent advances in deep thinking models have demonstrated remarkable
reasoning capabilities on mathematical and coding tasks. However, their
effectiveness in embodied domains which require continuous interaction with
environments through image action interleaved trajectories remains largely
-unexplored. We present Embodied Reasoner, a model that extends o1 style
reasoning to interactive embodied search tasks. Unlike mathematical reasoning
that relies primarily on logical deduction, embodied scenarios demand spatial
understanding, temporal reasoning, and ongoing self-reflection based on
interaction history. To address these challenges, we synthesize 9.3k coherent
Observation-Thought-Action trajectories containing 64k interactive images and
90k diverse thinking processes (analysis, spatial reasoning, reflection,
planning, and verification). We develop a three-stage training pipeline that
progressively enhances the model's capabilities through imitation learning,
self-exploration via rejection sampling, and self-correction through reflection
tuning. The evaluation shows that our model significantly outperforms those
advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and
Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer
repeated searches and logical inconsistencies, with particular advantages in
complex long-horizon tasks. Real-world environments also show our superiority
while exhibiting fewer repeated searches and logical inconsistency cases.

</details>


### [27] [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/pdf/2503.21813)
*Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang*

Main category: cs.CL

TL;DR: A new benchmark dataset, OAEI-LLM-T, is introduced to address hallucinations in LLM-based ontology matching, categorizing them into two main types and six subcategories for evaluation and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs pose challenges for ontology matching tasks, necessitating a dedicated dataset to study and mitigate them.

Method: The dataset is derived from seven TBox datasets in OAEI, capturing hallucinations from ten LLMs, and categorizing them systematically.

Result: The dataset enables the creation of an LLM leaderboard for OM tasks and supports fine-tuning LLMs to reduce hallucinations.

Conclusion: OAEI-LLM-T provides a valuable resource for improving LLM performance in ontology matching by addressing hallucination issues.

Abstract: Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.

</details>


### [28] [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/pdf/2503.24293)
*Hayley Ross, Kathryn Davidson, Najoung Kim*

Main category: cs.CL

TL;DR: The paper explores whether analogical reasoning, rather than composition, can explain how humans and LLMs generalize to novel adjective-noun combinations. While analogy works for many cases, some require composition.


<details>
  <summary>Details</summary>
Motivation: To determine if generalization in humans and LLMs for novel adjective-noun combinations relies on analogy or composition.

Method: (1) Build a model of analogical reasoning using lexical similarity. (2) Conduct human experiments on analogical reasoning.

Result: Analogy works for many cases, but some novel combinations require composition, as both humans and LLMs show convergent inferences not explainable by analogy.

Conclusion: Generalization in humans and LLMs cannot be fully reduced to analogy; composition is likely involved.

Abstract: Recent work (Ross et al., 2025, 2024) has argued that the ability of humans
and LLMs respectively to generalize to novel adjective-noun combinations shows
that they each have access to a compositional mechanism to determine the
phrase's meaning and derive inferences. We study whether these inferences can
instead be derived by analogy to known inferences, without need for
composition. We investigate this by (1) building a model of analogical
reasoning using similarity over lexical items, and (2) asking human
participants to reason by analogy. While we find that this strategy works well
for a large proportion of the dataset of Ross et al. (2025), there are novel
combinations for which both humans and LLMs derive convergent inferences but
which are not well handled by analogy. We thus conclude that the mechanism
humans and LLMs use to generalize in these cases cannot be fully reduced to
analogy, and likely involves composition.

</details>


### [29] [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/pdf/2504.04717)
*Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman*

Main category: cs.CL

TL;DR: A survey on evaluating and enhancing multi-turn interactions in LLMs, covering benchmarks, methodologies, and future directions.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require sophisticated multi-turn interactions, but current LLMs excel mainly in single-turn tasks.

Method: Systematic review of benchmarks, datasets, and enhancement methods (model-centric, external integration, agent-based).

Result: Organized current benchmarks and reviewed methodologies to improve multi-turn interactions in LLMs.

Conclusion: Identifies open challenges and proposes future research directions for robust multi-turn LLM interactions.

Abstract: Recent advancements in large language models (LLMs) have revolutionized their
ability to handle single-turn tasks, yet real-world applications demand
sophisticated multi-turn interactions. This survey provides a comprehensive
review of recent advancements in evaluating and enhancing multi-turn
interactions in LLMs. Focusing on task-specific scenarios, from instruction
following in diverse domains such as math and coding to complex conversational
engagements in roleplay, healthcare, education, and even adversarial jailbreak
settings, we systematically examine the challenges of maintaining context,
coherence, fairness, and responsiveness over prolonged dialogues. The paper
organizes current benchmarks and datasets into coherent categories that reflect
the evolving landscape of multi-turn dialogue evaluation. In addition, we
review a range of enhancement methodologies under multi-turn settings,
including model-centric strategies (contextual learning, supervised
fine-tuning, reinforcement learning, and new architectures), external
integration approaches (memory-augmented, retrieval-based methods, and
knowledge graph), and agent-based techniques for collaborative interactions.
Finally, we discuss open challenges and propose future directions for research
to further advance the robustness and effectiveness of multi-turn interactions
in LLMs. Related resources and papers are available at
https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.

</details>


### [30] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/pdf/2505.00949)
*Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung*

Main category: cs.CL

TL;DR: The Llama-Nemotron series offers open-source, heterogeneous reasoning models with competitive performance, superior efficiency, and a dynamic reasoning toggle. Three sizes are available, trained using neural architecture search, knowledge distillation, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To provide high-performance, efficient reasoning models with open licensing for enterprise use and support open research.

Method: Training involves neural architecture search, knowledge distillation, continued pretraining, and a reasoning-focused post-training stage (supervised fine-tuning and large-scale reinforcement learning).

Result: Models (Nano, Super, Ultra) perform competitively with state-of-the-art models while offering better throughput and memory efficiency.

Conclusion: Llama-Nemotron models advance open-source reasoning capabilities with practical features and released resources for community development.

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [31] [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/pdf/2505.05084)
*Xiaowei Zhu, Yubing Ren, Yanan Cao, Xixun Lin, Fang Fang, Yangxi Li*

Main category: cs.CL

TL;DR: The paper proposes a Zero-Shot Machine-Generated Text Detection Framework using Multiscaled Conformal Prediction (MCP) to balance FPR constraints and detection performance, validated by the RealDet dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing societal risks from high false positive rates (FPRs) in existing detection methods for large language models.

Method: Leverages Conformal Prediction (CP) and introduces MCP to enforce FPR constraints while improving detection. Uses the RealDet dataset for realistic calibration.

Result: MCP effectively constrains FPRs, enhances detection performance, and increases robustness against adversarial attacks.

Conclusion: MCP offers a balanced solution for mitigating misuse risks of large language models by improving detection without compromising FPR constraints.

Abstract: The rapid advancement of large language models has raised significant
concerns regarding their potential misuse by malicious actors. As a result,
developing effective detectors to mitigate these risks has become a critical
priority. However, most existing detection methods focus excessively on
detection accuracy, often neglecting the societal risks posed by high false
positive rates (FPRs). This paper addresses this issue by leveraging Conformal
Prediction (CP), which effectively constrains the upper bound of FPRs. While
directly applying CP constrains FPRs, it also leads to a significant reduction
in detection performance. To overcome this trade-off, this paper proposes a
Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal
Prediction (MCP), which both enforces the FPR constraint and improves detection
performance. This paper also introduces RealDet, a high-quality dataset that
spans a wide range of domains, ensuring realistic calibration and enabling
superior detection performance when combined with MCP. Empirical evaluations
demonstrate that MCP effectively constrains FPRs, significantly enhances
detection performance, and increases robustness against adversarial attacks
across multiple detectors and datasets.

</details>


### [32] [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/pdf/2505.08037)
*Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi*

Main category: cs.CL

TL;DR: TiSpell is a semi-masked model for multi-level Tibetan spelling correction, outperforming baselines by leveraging data augmentation and a unified approach for character- and syllable-level errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack integration for multi-level correction and lack tailored datasets for Tibetan spelling errors.

Method: Proposes data augmentation with unlabeled text and introduces TiSpell, a semi-masked model for correcting both character- and syllable-level errors.

Result: TiSpell outperforms baselines and matches state-of-the-art performance on simulated and real-world data.

Conclusion: TiSpell is effective for multi-level Tibetan spelling correction, addressing gaps in existing methods.

Abstract: Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.

</details>


### [33] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/pdf/2505.08167)
*Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang*

Main category: cs.CL

TL;DR: A novel training method combining bidirectional chains of thought and a reward mechanism improves domain-specific LLMs, addressing bias and knowledge issues in ICH data.


<details>
  <summary>Details</summary>
Motivation: Challenges like bias, incorrect knowledge inheritance, and catastrophic forgetting in fine-tuning LLMs with ICH data necessitate a robust training approach.

Method: Proposes bidirectional chains of thought (forward and reverse reasoning) and a reward mechanism for optimizing decision-making and output quality.

Result: Outperforms baseline methods (0-shot, step-by-step reasoning, etc.) in accuracy, Bleu-4, and Rouge-L scores, and shows generalizability across domains.

Conclusion: The method is adaptable and effective for diverse domains, offering a promising approach for future LLM training.

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [34] [Hakim: Farsi Text Embedding Model](https://arxiv.org/pdf/2505.08435)
*Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman*

Main category: cs.CL

TL;DR: Hakim, a new Persian text embedding model, outperforms existing methods by 8.5% on FaMTEB, introduces new datasets, and supports chatbot and RAG applications.


<details>
  <summary>Details</summary>
Motivation: Persian is underrepresented in text embedding research, prompting the development of Hakim to address this gap.

Method: Hakim leverages a BERT-based baseline and RetroMAE for retrieval tasks, supported by three new datasets (Corpesia, Pairsia-sup, Pairsia-unsup).

Result: Hakim achieves state-of-the-art performance on Persian NLP tasks and excels in retrieval applications.

Conclusion: Hakim advances Persian language understanding and sets a new benchmark for future research.

Abstract: Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/pdf/2505.09615)
*Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, FranÃ§ois Germain, Michael Jeffrey Jones, Moitreya Chatterjee*

Main category: cs.CV

TL;DR: UWAV improves AVVP by addressing pseudo-label limitations with uncertainty weighting and feature mixup, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The high cost of detailed annotations for AVVP tasks and the limitations of weakly-supervised methods (e.g., lack of inter-segment dependencies and label bias) motivate the need for a better approach.

Method: Proposes UWAV, which incorporates uncertainty-weighted pseudo-labels and feature mixup regularization to enhance training.

Result: UWAV outperforms existing methods on AVVP tasks across multiple metrics and datasets.

Conclusion: UWAV effectively addresses weaknesses in weakly-supervised AVVP, demonstrating superior performance and generalizability.

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [36] [HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment](https://arxiv.org/pdf/2412.01986)
*Armin Shafiee Sarvestani, Sheyang Tang, Zhou Wang*

Main category: cs.CV

TL;DR: HybridMQA is a hybrid full-reference colored mesh quality assessment framework combining model-based and projection-based methods to better capture texture-geometry interactions.


<details>
  <summary>Details</summary>
Motivation: Current MQA models fail to adequately capture interactions between texture and 3D geometry, limiting their effectiveness.

Method: HybridMQA integrates graph learning for 3D representations and a novel feature rendering process for 2D projections, using cross-attention to explore geometry-texture interactions.

Result: Extensive experiments show HybridMQA outperforms existing methods by effectively leveraging geometry-texture interactions.

Conclusion: HybridMQA provides a comprehensive solution for mesh quality assessment, with superior performance and public implementation.

Abstract: Mesh quality assessment (MQA) models play a critical role in the design,
optimization, and evaluation of mesh operation systems in a wide variety of
applications. Current MQA models, whether model-based methods using
topology-aware features or projection-based approaches working on rendered 2D
projections, often fail to capture the intricate interactions between texture
and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid
full-reference colored MQA framework that integrates model-based and
projection-based approaches, capturing complex interactions between textural
information and 3D structures for enriched quality representations. Our method
employs graph learning to extract detailed 3D representations, which are then
projected to 2D using a novel feature rendering process that precisely aligns
them with colored projections. This enables the exploration of geometry-texture
interactions via cross-attention, producing comprehensive mesh quality
representations. Extensive experiments demonstrate HybridMQA's superior
performance across diverse datasets, highlighting its ability to effectively
leverage geometry-texture interactions for a thorough understanding of mesh
quality. Our implementation will be made publicly available.

</details>


### [37] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/pdf/2505.08800)
*Olivia Nocentini, Marta Lagomarsino, Gokhan Solak, Younggeol Cho, Qiyi Tong, Marta Lorenzini, Arash Ajoudani*

Main category: cs.CV

TL;DR: The paper introduces a DGNN-based online monitoring system for train driver fatigue, combining facial and skeletal features for improved accuracy, and presents a novel dataset including pathological conditions.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional alertness checks in railway safety by developing a more advanced, behavior-based monitoring system.

Method: Uses a Directed-Graph Neural Network (DGNN) to classify driver states (alert, not alert, pathological) and compares feature configurations (skeletal-only, facial-only, combined) through an ablation study.

Result: Combined facial and skeletal features achieve 80.88% accuracy in three-class classification and over 99% in binary alertness classification. A novel dataset with simulated pathological conditions is introduced.

Conclusion: The study advances railway safety by leveraging vision-based technologies for more accurate and comprehensive driver fatigue monitoring.

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [38] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/pdf/2505.08801)
*Md. Sakib Hassan Chowdhury, Md. Hafiz Ahamed, Bishowjit Paul, Sarafat Hussain Abhi, Abu Bakar Siddique, Md. Robius Sany*

Main category: cs.CV

TL;DR: Proposes OptiGait-LGBM, a skeletal model-based gait recognition method, addressing real-world challenges like uncontrolled environments and computational efficiency. Introduces RUET-GAIT dataset and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current gait recognition systems struggle with real-world, unconstrained data due to factors like varying illumination and non-overlapping camera views. No existing dataset tackles all these challenges.

Method: Uses skeletal joint landmarks to create a numerical dataset, minimizing memory usage. Develops OptiGait-LGBM model for gait classification, focusing on efficiency.

Result: Outperforms ensemble techniques (Random Forest, CatBoost) in accuracy, memory usage, and training time.

Conclusion: OptiGait-LGBM offers a low-cost, memory-efficient solution for real-world gait recognition, validated by the RUET-GAIT dataset.

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [39] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/pdf/2505.08808)
*Anqing Jiang, Jinhao Chai, Yu Gao, Yiru Wang, Yuwen Heng, Zhigang Sun, Hao Sun, Zezhong Zhao, Li Sun, Jian Zhou, Lijuan Zhu, Shugong Xu, Hao Zhao*

Main category: cs.CV

TL;DR: The paper enhances sparse representations for HD map construction, surpassing dense methods with a new network architecture, auxiliary tasks, and denoising, achieving state-of-the-art results on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Sparse representations lag behind dense ones in HD map construction due to lack of tailored designs, despite their efficiency. This work aims to bridge this gap.

Method: Introduces a network optimized for sparse feature extraction, a sparse-dense segmentation task, and a denoising module with physical priors.

Result: Achieves 55.5% to 68.9% mAP on nuScenes, with SparseMeXt-Large setting a new benchmark at 68.9% mAP and over 20 fps.

Conclusion: Sparse methods have untapped potential, challenging dense representations and redefining efficiency-performance trade-offs in HD map construction.

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [40] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/pdf/2505.08811)
*Shijie Lian, Ziyi Zhang, Laurence Tianruo Yang and, Mengyu Ren, Debin Liu, Hua Li*

Main category: cs.CV

TL;DR: TUGS is a method for underwater 3D scene reconstruction that addresses challenges like light propagation and water medium interactions, offering faster rendering and lower memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with modeling underwater light-object interactions and are computationally expensive, limiting practical use in robotic systems.

Method: TUGS uses tensorized higher-order Gaussians and a physics-based Adaptive Medium Estimation module to simulate light attenuation and backscatter effects.

Result: TUGS outperforms NeRF and GS-based methods in rendering quality, speed, and memory efficiency, validated on real-world datasets.

Conclusion: TUGS is efficient and suitable for memory-constrained underwater UAV applications due to its superior reconstruction quality and reduced parameters.

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [41] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/pdf/2505.08882)
*Ali Almakhluk, Uthman Baroudi, Yasser El-Alfy*

Main category: cs.CV

TL;DR: A system using Raspberry Pi, a camera, deep learning, and cloud services detects and classifies road damage (potholes, cracks) in real-time, alerts authorities and nearby vehicles, and aims to improve road safety.


<details>
  <summary>Details</summary>
Motivation: Road damage anomalies like potholes and cracks are major causes of accidents, necessitating a proactive solution to enhance transportation safety.

Method: The system employs Raspberry Pi, a camera module, deep learning models, and cloud services to detect, classify, and transmit road anomaly data, while also broadcasting warnings to nearby vehicles.

Result: The system successfully detects and classifies road damage in real-time, enabling timely warnings and actions to mitigate accidents.

Conclusion: This innovative solution proactively improves road safety by alerting authorities and drivers, reducing accidents caused by road hazards.

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [42] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/pdf/2505.08814)
*Wenkai Li, Xiaoqi Li, Yingjie Mao, Yishun Wang*

Main category: cs.CV

TL;DR: The paper investigates relationships between DNN depth, configuration, and four coverage metrics (primary functionality, boundary, hierarchy, structural) through empirical experiments on LeNet, VGG, and ResNet models. It also explores modified decision/condition coverage and dataset size, proposing future directions for DNN security testing.


<details>
  <summary>Details</summary>
Motivation: The lack of empirical research on neural network coverage metrics and their relationships with model depth and configuration drives this study.

Method: Empirical experiments using LeNet, VGG, and ResNet models (5-54 layers) to analyze coverage metrics and modified decision/condition coverage with dataset size.

Result: Findings reveal patterns between model depth, configuration, and coverage metrics, along with insights into modified decision/condition coverage and dataset size.

Conclusion: The study highlights the need for further research in DNN security testing, proposing three future directions to advance the field.

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [43] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/pdf/2505.08817)
*Camilo Carvajal Reyes, JoaquÃ­n Fontbona, Felipe Tobar*

Main category: cs.CV

TL;DR: The paper proposes an external-conditioning method (SFW sampler) to prevent unsafe content generation in SBMs, using CLIP for user-defined NSFW classes, with minimal impact on image quality.


<details>
  <summary>Details</summary>
Motivation: SBMs, while state-of-the-art for image synthesis, can generate unsafe content. Current prevention methods rely on model fine-tuning, prompting exploration of external conditioning for safer outputs.

Method: The SFW sampler uses Conditional Trajectory Correction guided by multimodal models (e.g., CLIP) to steer samples away from NSFW regions, allowing user-defined NSFW classes.

Result: Experiments on Stable Diffusion show the SFW sampler reduces explicit content effectively, competes with fine-tuning methods, and minimally affects image quality for non-NSFW samples.

Conclusion: The SFW sampler demonstrates the potential of model-agnostic conditioning for aligning SBMs and preventing unwanted content, with negligible quality trade-offs.

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [44] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/pdf/2505.09324)
*Lakshya Gupta, Imran N. Junejo*

Main category: cs.CV

TL;DR: The paper proposes a neural video codec using 2D Gaussian Splatting for real-time applications, improving encoding speed by 88%.


<details>
  <summary>Details</summary>
Motivation: Traditional video codecs lack adaptability and efficiency, while neural video codecs (NVC) offer better performance but face computational challenges in real-time applications.

Method: A region-of-interest (ROI) based neural video compression model using 2D Gaussian Splatting, with content-aware initialization and inter-frame redundancy reduction.

Result: The proposed method speeds up encoding time by 88% compared to previous Gaussian splatting-based image codecs.

Conclusion: The work demonstrates the feasibility of using Gaussian splatting for neural video codecs, offering potential for real-time applications like video conferencing.

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [45] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/pdf/2505.08833)
*Qingyi Wang, Yuebing Liang, Yunhan Zheng, Kaiyuan Xu, Jinhua Zhao, Shenhao Wang*

Main category: cs.CV

TL;DR: The paper adapts Stable Diffusion with ControlNet to generate realistic urban layouts from land-use descriptions, overcoming data limitations by linking satellite imagery with OpenStreetMap data. It demonstrates high-quality, diverse urban designs and evaluates prompt impacts, achieving strong performance metrics and positive qualitative feedback.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of producing realistic and practical urban designs at scale using generative AI, leveraging existing data and models.

Method: Adapts Stable Diffusion with ControlNet, using satellite imagery linked to OpenStreetMap data for land-use and constraints. Evaluates language prompts and control imagery impacts.

Result: Generates high-fidelity, diverse urban landscapes with strong FID and KID scores. Qualitative feedback shows preference over real images.

Conclusion: Establishes a benchmark for controlled urban imagery generation, showcasing generative AI's potential in urban planning workflows and public engagement.

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [46] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/pdf/2505.08910)
*Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji*

Main category: cs.CV

TL;DR: Maya is an open-source Multilingual Vision-Language Model (VLM) addressing performance gaps in low-resource languages and cultural contexts by introducing a multilingual dataset and model.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs excel in widely spoken languages but underperform in low-resource languages and diverse cultural settings.

Method: Developed Maya, a multilingual VLM, using a pretraining dataset in eight languages based on LLaVA.

Result: Enhanced cultural and linguistic comprehension in vision-language tasks for the supported languages.

Conclusion: Maya bridges the gap in multilingual and culturally diverse vision-language understanding, with open-source availability.

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [47] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/pdf/2505.09433)
*Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma*

Main category: cs.CV

TL;DR: SerLiC is a neural compression framework for LiDAR reflectance data, using serialization and Mamba for efficient modeling, achieving significant compression and speed.


<details>
  <summary>Details</summary>
Motivation: LiDAR reflectance attributes are underexplored in neural compression, despite their importance for downstream tasks.

Method: SerLiC serializes LiDAR point clouds into 1D sequences, tokenizes points with contextual representations, and uses Mamba for sequential modeling.

Result: SerLiC achieves over 2x volume reduction, outperforms state-of-the-art by 22% in compressed bits, and offers a lightweight version with >10 fps.

Conclusion: SerLiC effectively compresses LiDAR reflectance data with high efficiency, making it suitable for real-world applications.

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [48] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/pdf/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: The paper addresses challenges in crowd scene analysis, proposing self-supervised training for crowd counting and a spatiotemporal model for anomaly detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To tackle data-hungry deep learning models in crowd counting and improve anomaly detection in complex scenes.

Method: 1. Self-supervised training with Multi-Column CNN for crowd counting. 2. Spatiotemporal model (VGG19 + LSTM) with dense residual blocks for anomaly detection.

Result: Evaluated on ShanghaiTech, UCF-QNRF (MAE, MSE), Hockey Fight, and SCVD datasets, outperforming existing methods.

Conclusion: The proposed models effectively address challenges in crowd counting and anomaly detection, demonstrating superior performance.

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [49] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/pdf/2505.08971)
*Yangyi Chen, Hao Peng, Tong Zhang, Heng Ji*

Main category: cs.CV

TL;DR: PRIOR improves vision-language pre-training by prioritizing image-related tokens in next-token prediction, reducing noise and hallucination risks.


<details>
  <summary>Details</summary>
Motivation: Standard NTP in LVLMs fits noise due to unrelated caption tokens, increasing hallucination risks.

Method: PRIOR uses a text-only LLM to weight tokens, adjusting loss based on visual relevance.

Result: 19% and 8% relative improvements in benchmarks for LVLMs with and without visual encoders, respectively.

Conclusion: PRIOR outperforms NTP, shows better scaling, and reduces hallucination risks.

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [50] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/pdf/2505.09529)
*Mohamed Moustafa, Joseph Lemley, Peter Corcoran*

Main category: cs.CV

TL;DR: Event cameras capture facial data to reconstruct cardiac pulse signals using a CNN, achieving competitive heart rate accuracy compared to standard cameras.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of event cameras for low-latency, high-dynamic-range remote heart rate monitoring.

Method: A supervised CNN model processes 2D event stream representations to extract cardiac signals, evaluated by heart rate accuracy.

Result: Event cameras preserve cardiac info effectively, with RMSEs of 3.32 bpm (event frames) and outperforming standard cameras at higher FPS (2.54/2.13 bpm at 60/120 FPS).

Conclusion: Event cameras show promise for remote heart rate monitoring, offering advantages in dynamic range and temporal resolution.

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [51] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/pdf/2505.08854)
*Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, Zhengzhong Tu*

Main category: cs.CV

TL;DR: The survey explores GenAI's role in autonomous driving, covering generative models, applications, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving Level 5 autonomous driving by leveraging GenAI's capabilities in content creation and reasoning.

Method: The paper synthesizes principles of generative models (VAEs, GANs, Diffusion Models, LLMs) and their applications in autonomous driving, including synthetic data, decision-making, and digital twins.

Result: Identifies practical applications, challenges (e.g., generalization, safety, ethics), and proposes research plans for theoretical assurances and socio-technical impact.

Conclusion: The survey serves as a forward-looking guide for researchers, engineers, and policymakers on GenAI's integration into autonomous mobility.

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [52] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/pdf/2505.08886)
*Hamideh Khaleghpour, Brett McKinney*

Main category: cs.CV

TL;DR: AI-based diagnostic tool using neuro-fuzzy and colonial competition methods achieves 94% accuracy in distinguishing malignant skin lesions.


<details>
  <summary>Details</summary>
Motivation: Addressing the urgent need for advanced diagnostic aids due to rising skin cancer cases and limited clinical expertise.

Method: Combines image processing with machine learning (neuro-fuzzy and colonial competition) on dermoscopic images from the ISIC database.

Result: Achieved 94% accuracy on a dataset of 560 images.

Conclusion: Demonstrates potential for early melanoma detection, enhancing skin cancer diagnostics.

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [53] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/pdf/2505.08909)
*Deliang Wei, Peng Chen, Haobo Xu, Jiale Yao, Fang Li, Tieyong Zeng*

Main category: cs.CV

TL;DR: The paper introduces a cocoercive conservative (CoCo) denoiser to address limitations of PnP methods in Poisson inverse problems, improving denoising performance and ensuring convergence.


<details>
  <summary>Details</summary>
Motivation: Existing PnP methods require restrictive assumptions (strong convexity/smoothness and non-expansive denoisers) that fail in Poisson inverse problems and limit denoising performance.

Method: Proposes CoCo denoiser, trained with Hamiltonian and spectral regularization, ensuring it acts as a proximal operator of a weakly convex function.

Result: CoCo denoiser enables a restoration model with implicit weakly convex prior, with global convergence proven. Experiments show superior performance.

Conclusion: The CoCo denoiser overcomes prior limitations, offering improved denoising and convergence in PnP methods for Poisson inverse problems.

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [54] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/pdf/2505.08961)
*Yancheng Wang, Nebojsa Jojic, Yingzhen Yang*

Main category: cs.CV

TL;DR: The paper introduces the DCS-Attention module, a novel attention mechanism that selects informative channels for feature extraction, improving performance in person Re-ID tasks.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the Information Bottleneck principle, aiming to enhance feature discriminability in neural networks.

Method: The DCS-Attention module selects channels differentially, integrates with fixed or learnable backbones (DCS-FB/DCS-DNAS), and optimizes a variational IB loss.

Result: Experiments show DCS-Attention significantly boosts accuracy in person Re-ID benchmarks.

Conclusion: DCS-Attention effectively learns discriminative features, achieving state-of-the-art performance in Re-ID tasks.

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [55] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/pdf/2505.08999)
*Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, Mao-Li Wang*

Main category: cs.CV

TL;DR: Proposes AMGA, an adaptive meta-gradient adversarial attack method for visual tracking, enhancing attack transferability and effectiveness by integrating multi-model ensembles, meta-learning, momentum mechanisms, and Gaussian smoothing.


<details>
  <summary>Details</summary>
Motivation: Address security vulnerabilities in deep learning-based visual trackers by developing effective adversarial attacks to reveal weaknesses.

Method: Combines multi-model ensembles, meta-learning, momentum mechanisms, and Gaussian smoothing to optimize adversarial examples. Performs iterative white- and black-box attacks in diverse scenarios.

Result: AMGA significantly improves attack performance, transferability, and deception on datasets like OTB2015, LaSOT, and GOT-10k.

Conclusion: AMGA effectively bridges the gap between white- and black-box attacks, demonstrating superior performance in adversarial scenarios.

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [56] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/pdf/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: A multimodal deep learning framework improves caloric intake estimation for Type 2 diabetes management by combining CGM data, demographic/microbiome info, and meal images, outperforming baselines by 50%.


<details>
  <summary>Details</summary>
Motivation: Accurate caloric intake estimation is challenging for diabetes management, and existing tools like CGMs lack full nutritional profiling.

Method: Uses attention-based encoding for meal images, MLPs for CGM/microbiome data, and late fusion for joint reasoning.

Result: Achieves RMSRE of 0.2544, outperforming baselines by over 50%.

Conclusion: Multimodal sensing enhances dietary assessment tools for chronic disease management.

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [57] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/pdf/2505.09073)
*J. Brennan Peace, Shuowen Hu, Benjamin S. Riggan*

Main category: cs.CV

TL;DR: A novel domain adaptive framework improves facial recognition performance across large pose discrepancies by aligning 2D and 3D representations using shared attention and joint entropy regularization.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in facial recognition due to pose differences between enrollment and query images.

Method: Proposes a framework with shared attention mapping and joint entropy regularization to align 2D and 3D facial representations.

Result: Outperforms competitive methods on FaceScape and ARL-VTF datasets, with significant improvements in profile recognition accuracy.

Conclusion: The framework effectively enhances pose invariance in facial recognition, demonstrating superior performance on benchmark datasets.

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [58] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/pdf/2505.09092)
*Yuhang Wang, Abdulaziz Alhuraish, Shengming Yuan, Hao Zhou*

Main category: cs.CV

TL;DR: OpenLKA is the first open, large-scale dataset for evaluating and improving Lane Keeping Assist (LKA) systems, featuring 400 hours of driving data from 50+ vehicle models, multimodal inputs, and rich annotations.


<details>
  <summary>Details</summary>
Motivation: The real-world performance of LKA systems is underexplored due to proprietary systems and limited data access. OpenLKA aims to address this gap.

Method: The dataset includes CAN bus streams, dash-cam video, Openpilot outputs, and scene annotations, collected from road testing and community contributions.

Result: OpenLKA provides a comprehensive platform for benchmarking LKA performance, identifying safety-critical scenarios, and assessing road infrastructure readiness.

Conclusion: OpenLKA is publicly available, offering a valuable resource for advancing LKA research and autonomous driving readiness.

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [59] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/pdf/2505.09118)
*Dayong Liang, Changmeng Zheng, Zhiyuan Wen, Yi Cai, Xiao-Yong Wei, Qing Li*

Main category: cs.CV

TL;DR: The paper introduces ISGR, a framework to enhance VLMs' interaction reasoning by combining spatial and functional scene graphs, active reasoning, and long-term memory reinforcement.


<details>
  <summary>Details</summary>
Motivation: Traditional scene graphs lack interaction reasoning, limiting VLMs' ability to understand complex visual scenes.

Method: ISGR uses dual-stream graph construction, interaction queries, and long-term memory reinforcement learning.

Result: ISGR outperforms baselines on interaction-heavy benchmarks, especially in complex scene understanding.

Conclusion: ISGR effectively improves VLMs' interaction reasoning, offering a robust solution for complex scene understanding.

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [60] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/pdf/2505.09123)
*Guoying Liang, Su Yang*

Main category: cs.CV

TL;DR: The study introduces a framework to adapt the Segment Anything Model (SAM) for Camouflaged Object Detection (COD), achieving competitive results by using point promotions and a novel key point selection algorithm.


<details>
  <summary>Details</summary>
Motivation: To leverage big models like SAM for COD, overcoming previous claims that SAM is unsuitable for this task.

Method: Developed the Promotion Point Targeting Network (PPT-net) for multi-scale feature prediction and a Key Point Selection (KPS) algorithm for contrastive point promotions.

Result: Achieved plausible results on 3 datasets under 6 metrics, outperforming existing methods.

Conclusion: Demonstrates an effective off-the-shelf approach for COD using SAM, offering advantages in performance and reducing task complexity.

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [61] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/pdf/2505.09129)
*Wei Meng*

Main category: cs.CV

TL;DR: A lightweight anomaly detection framework using color features for surveillance videos in high-risk tactical missions, achieving effective threat identification under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying traditional deep learning models in unlabeled, data-sensitive video intelligence environments for high-risk security tasks.

Method: Fuses unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutations in key frames.

Result: Successfully identifies anomalous frames (e.g., high-energy light sources, target presence) in an African surveillance video without original data access.

Conclusion: The method is deployable and valuable for tactical applications; future work includes integrating graph neural networks and temporal modeling.

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [62] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/pdf/2505.09139)
*Lucas Choi, Ross Greer*

Main category: cs.CV

TL;DR: Automated prompt refinement for VLMs using CCAS improves object detection accuracy without extra training or labeled data.


<details>
  <summary>Details</summary>
Motivation: Address performance variability in VLMs due to prompt phrasing by automating prompt refinement.

Method: Generate diverse prompts via a large language model, filter them using CCAS (Contrastive Class Alignment Score) based on semantic alignment.

Result: Improves object detection accuracy for challenging categories by selecting high-precision prompts.

Conclusion: Offers a scalable, model-agnostic alternative to manual prompt engineering for VLMs.

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [63] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/pdf/2505.09140)
*Zechao Guan, Feng Yan, Shuai Du, Lin Ma, Qingshan Liu*

Main category: cs.CV

TL;DR: TopoDiT-3D, a topology-aware diffusion transformer, improves 3D point cloud generation by integrating global topological information and filtering redundant local features.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook global topological information like voids, which are crucial for shape consistency and complex geometries.

Method: Proposes TopoDiT-3D with a bottleneck structure using Perceiver Resampler to integrate topological information and filter redundant local features.

Result: Outperforms state-of-the-art models in visual quality, diversity, and training efficiency.

Conclusion: Highlights the importance of topological information and its synergy with local feature learning in 3D point cloud generation.

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [64] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/pdf/2505.09155)
*Yichen Shi, Zhuofu Tao, Yuhao Gao, Li Huang, Hongyang Wang, Zhiping Yu, Ting-Jung Lin, Lei He*

Main category: cs.CV

TL;DR: A novel net detection method for MLLMs improves schematic understanding by segmenting and recovering positional data, expanding the AMSnet dataset to AMSnet 2.0 with richer annotations.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with circuit schematics due to limited recognition and lack of high-quality training data. Existing methods like AMSnet rely on rigid heuristics and fail with complex/noisy schematics.

Method: Proposes a segmentation-based net detection mechanism for robustness and positional data recovery, enabling digital schematic reconstruction. Expands AMSnet to AMSnet 2.0 with diverse data.

Result: AMSnet 2.0 includes 2,686 circuits with schematic images, netlists, digital schematics, and positional info, a significant upgrade from AMSnet's 792 circuits with only SPICE netlists.

Conclusion: The new method enhances MLLM capabilities for schematic understanding, supported by a more comprehensive dataset (AMSnet 2.0).

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [65] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/pdf/2505.09168)
*Jianlin Sun, Xiaolin Fang, Juwei Guan, Dongdong Gui, Teqi Wang, Tongxin Zhu*

Main category: cs.CV

TL;DR: DRRNet introduces a four-stage architecture for Camouflage Object Detection, combining global and local features with a refinement pipeline to improve edge details and reduce background interference.


<details>
  <summary>Details</summary>
Motivation: Existing COD methods struggle with indistinct target-background similarities, losing edge details or being misled by similar backgrounds.

Method: DRRNet uses a four-stage pipeline: Omni-Context Feature Extraction, Local Detail Extraction, dual-representation fusion, and reverse refinement.

Result: DRRNet outperforms state-of-the-art methods on benchmark datasets by enhancing boundary continuity and suppressing noise.

Conclusion: DRRNet effectively addresses COD challenges by integrating global and local features with innovative refinement techniques.

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [66] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/pdf/2505.09178)
*Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang*

Main category: cs.CV

TL;DR: UniCAD is a unified, efficient, and modular architecture for multi-task medical image diagnosis, leveraging pre-trained vision models with minimal task-specific parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of an open-source CAD platform and the resource-intensive nature of developing multi-task diagnostic systems.

Method: Uses low-rank adaptation for efficiency and a modular plug-and-play design with frozen foundation models and task-specific experts.

Result: Outperforms existing methods in accuracy and deployment efficiency across 12 diverse medical datasets.

Conclusion: UniCAD provides an open-source platform for equitable and efficient research in medical image diagnosis.

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [67] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/pdf/2505.09188)
*Minjun Kim, Jaehyeon Choi, Jongkeun Lee, Wonjin Cho, U Kang*

Main category: cs.CV

TL;DR: This paper surveys Zero-shot Quantization (ZSQ), a method for reducing deep learning model size without real data, categorizing existing approaches and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional quantization requires training data, which is often unavailable due to privacy or regulatory constraints. ZSQ addresses this by enabling quantization without real data.

Method: The paper categorizes ZSQ methods based on data generation strategies, analyzing their motivations and core ideas.

Result: A comprehensive overview of ZSQ methods is provided, highlighting challenges and recent advancements.

Conclusion: The paper identifies future research directions to improve ZSQ, marking the first in-depth survey on the topic.

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [68] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/pdf/2505.09196)
*Tong Li, Lizhi Wang, Hansen Feng, Lin Zhu, Hua Huang*

Main category: cs.CV

TL;DR: The paper introduces the 'gene effect' in low-light image enhancement (LLIE), where random parameters sometimes outperform learned ones, limiting model performance. The authors propose Parameter Dynamic Evolution (PDE) to mitigate this effect by simulating biological gene mutation and recombination.


<details>
  <summary>Details</summary>
Motivation: The 'gene effect' hinders LLIE performance, as static parameters can become maladaptive. The study aims to address this by drawing inspiration from biological evolution.

Method: Proposes Parameter Dynamic Evolution (PDE), using parameter orthogonal generation to simulate gene recombination and mutation, adapting parameters dynamically for different images.

Result: Experiments confirm PDE's effectiveness in mitigating the gene effect and improving LLIE performance.

Conclusion: PDE successfully addresses the gene effect, enhancing LLIE by dynamically evolving parameters, with code to be publicly released.

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [69] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/pdf/2505.09251)
*Vineetha Joy, Aditya Anand, Nidhi, Anshuman Kumar, Amit Sethi, Hema Singh*

Main category: cs.CV

TL;DR: A CNN-based surrogate model accelerates EM response prediction for metasurface-based RAS, achieving high accuracy and reduced computational time.


<details>
  <summary>Details</summary>
Motivation: Overcome the computational intensity and time consumption of conventional EM design methods for metasurface-based RAS.

Method: Uses a CNN with Huber loss function to predict reflection characteristics of RAS.

Result: Achieved 99.9% cosine similarity and 0.001 MSE, with significant computational time reduction.

Conclusion: The surrogate model is efficient and accurate, suitable for practical applications in RAS design.

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [70] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/pdf/2505.09252)
*Yinuo Wang, Yue Zeng, Kai Chen, Cai Meng, Chao Pan, Zhouping Tang*

Main category: cs.CV

TL;DR: Zero-shot MLLMs underperform traditional deep learning models in ICH binary classification and subtyping, but offer better interpretability.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of zero-shot MLLMs vs. traditional deep learning in ICH classification and subtyping due to challenges like low contrast and blurring boundaries.

Method: Compared MLLMs (GPT-4o, Gemini 2.0 Flash, Claude 3.5 Sonnet V2) with deep learning models (ResNet50, Vision Transformer) using RSNA dataset of 192 NCCT volumes. Tasks included ICH presence, subtype classification, localization, and volume estimation.

Result: Traditional deep learning models outperformed MLLMs in binary classification and subtyping. Gemini 2.0 Flash scored poorly (macro-averaged precision: 0.41, F1: 0.31).

Conclusion: MLLMs lag in accuracy but improve interpretability. Future work aims to refine MLLMs for 3D medical image processing.

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [71] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/pdf/2505.09256)
*Jaemin Jung, Youngjoon Jang, Joon Son Chung*

Main category: cs.CV

TL;DR: Pose-TTA enhances face recognition by aligning faces at inference time without retraining, using a portrait animator and weighted feature aggregation to reduce distortion and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing face recognition methods require retraining for each dataset, which is effort-intensive. This paper aims to improve performance without additional training.

Method: Proposes Pose-TTA, which aligns faces at inference time using a portrait animator and weighted feature aggregation to handle synthetic data distortions.

Result: Pose-TTA consistently improves inference performance across diverse datasets and pre-trained models, with no need for retraining.

Conclusion: Pose-TTA is an effective, easy-to-integrate solution for enhancing face recognition without retraining, addressing pose-related challenges.

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [72] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/pdf/2505.09263)
*Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu*

Main category: cs.CV

TL;DR: Proposes AnoGen, a few-shot anomaly-driven generation method using diffusion models to create realistic anomalies, improving anomaly detection performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the semantic gap between synthetic and real-world anomalies by generating realistic anomalies from few samples.

Method: Three-stage approach: learning anomaly distribution, guiding diffusion models to generate anomalies, and training anomaly detection models with generated data.

Result: Improves AU-PR metrics by 5.8% and 1.5% for DRAEM and DesTSeg on segmentation tasks.

Conclusion: AnoGen effectively enhances anomaly detection performance by generating diverse and realistic anomalies.

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [73] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/pdf/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: OneNIP proposes a method using one normal image prompt to reconstruct normal features and restore anomalies, improving unified anomaly detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: Self-attention reconstruction models struggle with detecting anomalies due to perfect reconstruction of both normal and anomaly features and low-resolution latent space issues.

Method: OneNIP uses one normal image prompt for reconstruction and restoration, along with a supervised refiner for better anomaly segmentation.

Result: OneNIP outperforms previous methods on MVTec, BTAD, and VisA benchmarks.

Conclusion: OneNIP is a simple yet effective solution for enhancing anomaly detection and segmentation performance.

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [74] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/pdf/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: MetaUAS introduces a pure visual foundation model for universal anomaly segmentation, unifying it with change segmentation and leveraging synthetic data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on vision-language models, but visual representations are independent of language, prompting exploration of a pure visual alternative.

Method: Proposes MetaUAS, a one-prompt meta-learning framework trained on synthetic image pairs, with a soft feature alignment module for handling geometrical variations.

Result: MetaUAS outperforms zero-shot, few-shot, and full-shot anomaly segmentation methods without relying on language or special datasets.

Conclusion: MetaUAS achieves universal anomaly segmentation efficiently with one normal image prompt, training-free, and language-independent.

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [75] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/pdf/2505.09274)
*Fares Bougourzi, Abdenour Hadid*

Main category: cs.CV

TL;DR: A survey on advancements in medical image segmentation, focusing on Generative AI, Few-Shot Learning, Foundation Models, and Universal Models, addressing challenges like generalization and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation is critical but faces challenges like data accessibility, annotation complexity, and domain variability. Robust solutions are needed to improve healthcare applications.

Method: The paper reviews methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models, providing theoretical foundations and state-of-the-art techniques.

Result: The survey highlights promising solutions to segmentation challenges but notes limitations and unresolved issues.

Conclusion: Future research should focus on enhancing practicality and accessibility of segmentation models, with ongoing updates tracked via a GitHub repository.

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [76] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/pdf/2505.09306)
*Thijs L van der Plas, Stephen Law, Michael JO Pocock*

Main category: cs.CV

TL;DR: A new dataset and contrastive regularization method are introduced to predict butterfly species presence from satellite images, improving biodiversity monitoring accuracy.


<details>
  <summary>Details</summary>
Motivation: The demand for scalable biodiversity monitoring methods drives the use of remote sensing data, especially with the availability of citizen-science wildlife data.

Method: A Resnet-based model is optimized to predict multi-species presence from 4-band satellite images, enhanced by a soft, supervised contrastive regularization loss.

Result: The model outperforms baselines in high-biodiversity areas, and the contrastive regularization improves prediction accuracy.

Conclusion: The dataset and method advance the challenge of predicting species biodiversity from remote sensing, aiding efficient monitoring.

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [77] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/pdf/2505.09329)
*Jiarun Liu, Hong-Yu Zhou, Weijian Huang, Hao Yang, Dongning Song, Tao Tan, Yong Liang, Shanshan Wang*

Main category: cs.CV

TL;DR: The paper explores scaling behaviors in medical vision foundation models, introducing BioVFM-21M, a large-scale dataset, and BioVFM, a model outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Understanding scaling behaviors in medical imaging, which differs from natural data, to develop effective foundation models.

Method: Self-supervised learning with scaling across model sizes, training algorithms, data sizes, and imaging modalities.

Result: Scaling benefits vary by task; BioVFM outperforms state-of-the-art models on 12 medical benchmarks.

Conclusion: Scaling improves performance, but task characteristics, data diversity, pretraining methods, and efficiency are crucial.

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [78] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/pdf/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: MultiviewVLM is a vision-language model for unsupervised contrastive learning of facial emotions from 3D/4D data, using pseudo-labels and a joint embedding space to align multiview representations. It outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve unsupervised representation learning for facial emotions by leveraging multiview data and aligning emotional semantics without explicit supervision.

Method: The model integrates pseudo-labels from textual prompts, uses a joint embedding space for multiview alignment, and employs a novel contrastive learning strategy with stable pair sampling. A gradient-friendly loss function and distributed training optimization are also introduced.

Result: MultiviewVLM outperforms existing state-of-the-art methods and is adaptable to real-world applications with minimal modifications.

Conclusion: The proposed MultiviewVLM effectively learns facial emotion representations from multiview data, demonstrating superior performance and scalability.

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [79] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/pdf/2505.09358)
*Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler*

Main category: cs.CV

TL;DR: Marigold leverages pretrained latent diffusion models (e.g., Stable Diffusion) for dense image analysis tasks like depth estimation, achieving state-of-the-art zero-shot generalization with minimal architecture changes and small synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional pretraining relies on labeled datasets or self-supervised learning, but text-to-image generative models offer a new, rich source of visual knowledge. Marigold aims to harness this for dense image tasks.

Method: Marigold fine-tunes pretrained latent diffusion models with a small synthetic dataset, requiring minimal architectural changes and training on a single GPU.

Result: The model achieves state-of-the-art zero-shot generalization on tasks like monocular depth estimation and surface normals prediction.

Conclusion: Marigold demonstrates the potential of adapting generative models for dense image analysis, offering efficient and effective transfer learning.

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [80] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/pdf/2505.09368)
*Jenny Schmalfuss, Victor Oei, Lukas Mehl, Madlen Bartsch, Shashank Agnihotri, Margret Keuper, AndrÃ©s Bruhn*

Main category: cs.CV

TL;DR: RobustSpring introduces a dataset and benchmark to evaluate model robustness to image corruptions for optical flow, scene flow, and stereo vision, addressing the lack of focus on real-world perturbations in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks prioritize accuracy over robustness to real-world image corruptions, leaving model resilience unquantified.

Method: RobustSpring applies 20 diverse image corruptions to the Spring dataset, generating 20,000 corrupted images, and introduces a new corruption robustness metric.

Result: Initial benchmarks show that accurate models are not necessarily robust, and robustness varies by corruption type.

Conclusion: RobustSpring promotes robustness as a key metric, aiming to foster models that balance accuracy with resilience.

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [81] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/pdf/2505.09372)
*Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge*

Main category: cs.CV

TL;DR: MAKE is a Multi-Aspect Knowledge-Enhanced VLP framework for dermatology, improving zero-shot tasks by decomposing clinical narratives and aligning sub-texts with image features.


<details>
  <summary>Details</summary>
Motivation: Existing VLP models struggle with dermatology due to text length constraints and lack of structured texts, limiting their effectiveness.

Method: MAKE uses multi-aspect contrastive learning, fine-grained alignment, and diagnosis-guided weighting to enhance zero-shot dermatological tasks.

Result: MAKE outperforms state-of-the-art VLP models on eight datasets for skin disease classification, concept annotation, and cross-modal retrieval.

Conclusion: MAKE advances dermatological AI by addressing text limitations and improving multimodal integration, with code made publicly available.

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [82] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/pdf/2505.09379)
*Ali Rida Sahili, Najett Neji, Hedi Tabia*

Main category: cs.CV

TL;DR: A survey on text-driven motion generation, covering traditional and modern approaches, datasets, evaluation methods, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and intuitive way to generate human movements from natural language, benefiting fields like VR, gaming, and robotics.

Method: Categorizes modern text-to-motion methods architecturally (VAE-based, diffusion-based, hybrid) and by motion representation (discrete vs. continuous).

Result: Summarizes current progress, datasets, benchmarks, and identifies key challenges and limitations.

Conclusion: Highlights promising future directions and serves as a resource for advancing language-driven motion synthesis.

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [83] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/pdf/2505.09380)
*Qinghui Liu, Jon Nesvold, Hanna Raaum, Elakkyen Murugesu, Martin RÃ¸vang, Bradley J Maclntosh, Atle BjÃ¸rnerud, Karoline Skogen*

Main category: cs.CV

TL;DR: NeoMedSys, a radiology software platform, improved the performance of VIOLA-AI for intracranial hemorrhage detection through iterative refinements in real-world clinical settings.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying AI tools in radiology by enabling efficient deployment and refinement of AI models like VIOLA-AI.

Method: NeoMedSys integrated tools for AI model deployment, testing, and optimization with a web-based medical image viewer and annotation system. It was tested in clinical settings for TBI and stroke cases, with performance metrics tracked.

Result: VIOLA-AI showed significant improvements: sensitivity rose to 90.3% (from 79.2%), specificity to 89.3% (from 80.7%), and AUC to 0.949 (from 0.873).

Conclusion: NeoMedSys effectively enhances AI model performance through iterative refinements and real-time feedback, demonstrating its feasibility in clinical radiology.

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [84] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/pdf/2505.09385)
*Xiaoyang Yu, Xiaoming Wu, Xin Wang, Dongrun Li, Ming Yang, Peng Cheng*

Main category: cs.CV

TL;DR: FedSaaS improves federated semantic segmentation by addressing class-consistency issues through class exemplars and adversarial mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing federated semantic segmentation methods neglect fine-grained class relationships, causing ambiguities in class representation, especially under domain shift.

Method: Proposes FedSaaS, using class exemplars for local and global class representation, adversarial mechanisms for harmonization, and multilevel contrastive losses for consistency.

Result: Outperforms state-of-the-art methods, significantly improving segmentation accuracy and resolving class-consistency issues.

Conclusion: FedSaaS effectively addresses class-consistency in federated semantic segmentation, enhancing performance and representation alignment.

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [85] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/pdf/2505.09406)
*Yue Wen, Liang Song, Yijia Liu, Siting Zhu, Yanzi Miao, Lijun Han, Hesheng Wang*

Main category: cs.CV

TL;DR: FreeDriveRF reconstructs dynamic driving scenes using only sequential RGB images, eliminating the need for pose inputs, and improves dynamic modeling with semantic supervision and optical flow constraints.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic scene reconstruction in autonomous driving rely on accurate poses and multi-sensor data, increasing system complexity. FreeDriveRF aims to simplify this by using only RGB images.

Method: The method decouples dynamic and static parts early using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss with optical flow. It also uses estimated dynamic flow for pose optimization.

Result: Experiments on KITTI and Waymo datasets show superior performance in dynamic scene modeling.

Conclusion: FreeDriveRF effectively reconstructs dynamic scenes without pose inputs, addressing challenges like motion and occlusion, and outperforms existing methods.

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [86] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/pdf/2505.09413)
*Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen*

Main category: cs.CV

TL;DR: A novel point cloud rendering method predicts 2D Gaussians from point clouds, eliminating the need for categorical priors or dense inputs, and achieves SOTA performance with direct generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on categorical priors, dense point clouds, or refinements, limiting flexibility and generalization.

Method: Uses two identical modules with an entire-patch architecture to normalize and initialize Gaussians, then refines them with splitting decoders for sparse inputs.

Result: Achieves state-of-the-art performance and generalization across datasets without additional refinement.

Conclusion: The method effectively renders sparse point clouds using 2D Gaussians, offering superior performance and generalization.

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [87] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/pdf/2505.09435)
*Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang*

Main category: cs.CV

TL;DR: Endo-CLIP is a self-supervised framework enhancing CLIP for endoscopic image analysis, addressing challenges like non-informative backgrounds and complex terminology, and outperforms state-of-the-art methods in polyp detection and classification.


<details>
  <summary>Details</summary>
Motivation: Improving endoscopic image analysis by overcoming challenges like non-informative backgrounds, complex terminology, and ambiguous multi-lesion descriptions.

Method: Three-stage framework: cleansing (removing background frames), attunement (using large language models for fine-grained contrastive learning), and unification (patient-level cross-attention for multi-polyp ambiguities).

Result: Significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification.

Conclusion: Endo-CLIP enables more accurate and clinically relevant endoscopic analysis.

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [88] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/pdf/2505.09415)
*Hongyang Wang, Yichen Shi, Zhuofu Tao, Yuhao Gao, Liepiao Zhang, Xun Lin, Jun Feng, Xiaochen Yuan, Zitong Yu, Xiaochun Cao*

Main category: cs.CV

TL;DR: FaceShield is a multimodal large language model (MLLM) for face anti-spoofing (FAS), offering interpretability, reasoning, and attack detection, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing FAS methods lack interpretability and reasoning. MLLMs show promise but lack FAS-specific models and datasets.

Method: Proposes FaceShield with spoof-aware vision perception (SAVP) and prompt-guided vision token masking (PVTM) for generalization.

Result: FaceShield outperforms deep learning models and general MLLMs on four FAS tasks.

Conclusion: FaceShield advances FAS with interpretability, reasoning, and attack detection, supported by new datasets and methods.

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [89] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/pdf/2505.09422)
*Xiangyuan Peng, Yu Wang, Miao Tang, Bierzynski Kay, Lorenzo Servadei, Robert Wille*

Main category: cs.CV

TL;DR: MoRAL is a motion-aware fusion framework for 4D radar and LiDAR, improving 3D object detection by addressing radar misalignment and leveraging object dynamics.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal fusion methods neglect radar misalignment and underutilize dynamic information from 4D radar, limiting detection accuracy.

Method: Proposes MoRAL with a Motion-aware Radar Encoder (MRE) to correct misalignment and a Motion Attention Gated Fusion (MAGF) module to integrate radar motion features with LiDAR.

Result: Achieves 73.30% mAP overall and 88.68% in driving corridors, with top performance for pedestrians (69.67%) and cyclists (96.25%).

Conclusion: MoRAL enhances 3D object detection by effectively addressing radar misalignment and leveraging motion data, outperforming existing methods.

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [90] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/pdf/2505.09450)
*Yuelin Zhang, Qingpeng Ding, Long Lei, Yongxuan Feng, Raymond Shing-Yan Tang, Shing Shin Cheng*

Main category: cs.CV

TL;DR: MrTrack is a Mamba-based aspiration needle tracker for ultrasound-guided FNA biopsies, addressing rapid motion challenges with temporal context storage and retrieval, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound-guided FNA biopsies lack a needle tracker for rapid reciprocating motion, necessitating a robust solution like MrTrack.

Method: MrTrack uses a Mamba-based register mechanism to distill global context from historical search maps and retrieves temporal prompts for degraded vision features. A self-supervised loss ensures feature diversity.

Result: MrTrack outperforms state-of-the-art trackers in accuracy, robustness, and inference efficiency on motorized and manual aspiration datasets.

Conclusion: MrTrack effectively addresses rapid motion challenges in FNA biopsies, offering superior performance and efficiency.

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [91] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/pdf/2505.09466)
*Xi Chen, Shiyang Zhou, Muqi Huang, Jiaxu Feng, Yun Xiong, Kun Zhou, Biao Yang, Yuhui Zhang, Huishuai Bao, Sijia Peng, Chuan Li, Feng Shi*

Main category: cs.CV

TL;DR: The paper introduces $	ext{SaPE}^2$, a semantic-aware position encoding method for vision transformers, addressing limitations of traditional 1D position encodings in capturing image patch relationships.


<details>
  <summary>Details</summary>
Motivation: Existing position encoding methods, borrowed from NLP, fail to capture semantic relationships between image patches, hindering model generalization and equivariance.

Method: Proposes $	ext{SaPE}^2$, a 2D semantic-aware position encoding that dynamically adapts position representations based on local content.

Result: Enhances generalization across resolutions, improves translation equivariance, and better aggregates features for similar but distant patches.

Conclusion: $	ext{SaPE}^2$ bridges the gap between position encoding and perceptual similarity, boosting performance in computer vision tasks.

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [92] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/pdf/2505.09455)
*Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris*

Main category: cs.CV

TL;DR: The paper proposes a Transformer-based method to improve spatio-temporal action detection (STAD) in soccer by incorporating game-level context and denoising sequences, enhancing precision and recall.


<details>
  <summary>Details</summary>
Motivation: Current STAD methods lack contextual understanding, leading to false positives in high-recall, low-precision regimes for soccer analytics.

Method: Uses a Transformer-based encoder-decoder model to process noisy player-centric predictions alongside game state info, leveraging temporal context and team dynamics.

Result: Improves precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video.

Conclusion: The approach effectively leverages soccer's tactical regularities to enhance STAD, complementing existing pixel-based methods.

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [93] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/pdf/2505.09484)
*Yingjie Ma, Xun Lin, Zitong Yu, Xin Liu, Xiaochen Yuan, Weicheng Xie, Linlin Shen*

Main category: cs.CV

TL;DR: The paper introduces the MMDA framework for Face Anti-Spoofing (FAS), addressing generalization issues in multimodal methods through denoising, alignment, and attention mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal FAS methods face challenges like modality-specific biases and domain shifts, limiting generalization. The goal is to improve cross-modal alignment and noise suppression.

Method: The MMDA framework uses CLIP for zero-shot generalization, MD2A for noise mitigation, RS2 for flexible alignment, and U-DSA for representation adaptability.

Result: Experiments on four datasets show MMDA outperforms existing methods in cross-domain generalization and multimodal detection accuracy.

Conclusion: MMDA enhances FAS generalization and representation capabilities, with promising results and future code release.

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [94] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/pdf/2505.09498)
*Bo Zhang, Shuo Li, Runhe Tian, Yang Yang, Jixin Tang, Jinhao Zhou, Lin Ma*

Main category: cs.CV

TL;DR: Flash-VL 2B optimizes Vision-Language Models for real-time use, achieving high speed and accuracy through architectural and computational enhancements.


<details>
  <summary>Details</summary>
Motivation: To enable real-time applications of VLMs with ultra-low latency and high throughput without compromising accuracy.

Method: Uses architectural enhancements, token compression, data curation, training schemes, and implicit semantic stitching for efficient processing.

Result: Achieves state-of-the-art speed and accuracy on 11 VLM benchmarks.

Conclusion: Flash-VL 2B is a viable solution for resource-constrained and large-scale real-time applications.

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [95] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/pdf/2505.09528)
*Jeffrey Wen, Rizwan Ahmad, Philip Schniter*

Main category: cs.CV

TL;DR: The paper proposes a method to bound full-reference image quality (FRIQ) metrics without knowing the true image, using conformal prediction and approximate posterior sampling, validated on denoising and MRI tasks.


<details>
  <summary>Details</summary>
Motivation: In safety-critical applications like medical imaging, knowing the quality of recovered images is crucial to avoid misdiagnoses, but FRIQ metrics cannot be directly computed without the true image.

Method: Combines conformal prediction with approximate posterior sampling to construct guaranteed bounds on FRIQ metrics.

Result: Demonstrated effectiveness on image denoising and accelerated MRI, with code publicly available.

Conclusion: The approach provides reliable bounds on FRIQ metrics, aiding quality assessment in imaging inverse problems.

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [96] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/pdf/2505.09562)
*Nicola Marinello, Simen Cassiman, Jonas Heylen, Marc Proesmans, Luc Van Gool*

Main category: cs.CV

TL;DR: A novel framework for 3D panoptic scene completion is introduced, extending existing 3D semantic scene completion models with Object and Panoptic Modules.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles require dense 3D maps for planning, but current 3D panoptic scene completion is underexplored.

Method: Proposes Object and Panoptic Modules integrated with existing 3D occupancy and scene completion methods, leveraging occupancy benchmarks for learning object shapes.

Result: The framework enables differentiable learning of individual object shapes, enhancing 3D panoptic scene completion.

Conclusion: The approach advances 3D panoptic scene completion, crucial for autonomous vehicle decision-making.

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [97] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/pdf/2505.09568)
*Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu*

Main category: cs.CV

TL;DR: The paper introduces BLIP3-o, a unified multimodal model combining image understanding and generation using a diffusion transformer and sequential pretraining. It outperforms benchmarks and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: To explore optimal architectures and training strategies for unifying image understanding and generation, leveraging autoregressive and diffusion models.

Method: Uses a diffusion transformer for CLIP image features, sequential pretraining (understanding first, then generation), and a curated dataset (BLIP3o-60k) for instruction tuning.

Result: BLIP3-o achieves state-of-the-art performance in both image understanding and generation tasks.

Conclusion: The proposed approach is effective, scalable, and open-sourced to advance future research in unified multimodal models.

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [98] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/pdf/2505.09564)
*Anne-Marie Rickmann, Stephanie L. Thorn, Shawn S. Ahn, Supum Lee, Selen Uman, Taras Lysyy, Rachel Burns, Nicole Guerrera, Francis G. Spinale, Jason A. Burdick, Albert J. Sinusas, James S. Duncan*

Main category: cs.CV

TL;DR: The paper explores using foundation models for cardiac CT segmentation in porcine models via self-training, improving accuracy without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Pre-clinical cardiac imaging, especially in porcine models, lacks robust segmentation methods due to domain shifts from human data.

Method: A self-training approach iteratively refines pseudo-labels from foundation models, requiring no manual pig annotations.

Result: Self-training enhances segmentation accuracy and temporal consistency in porcine cardiac CT.

Conclusion: While promising, further improvements could involve advanced self-training strategies and exploring other foundation models or imaging technologies.

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [99] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/pdf/2505.09571)
*Guillermo Gomez-Trenado, Pablo Mesejo, Oscar CordÃ³n, StÃ©phane LathuiliÃ¨re*

Main category: cs.CV

TL;DR: SAGE introduces a novel image editing technique using pre-trained diffusion models, outperforming existing methods in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of computationally intensive or poor-quality image editing in text-to-image generation.

Method: Leverages DDIM algorithm with a self-attention guidance mechanism for efficient reconstruction of unedited regions.

Result: Outperforms competitors in user studies and quantitative analyses, ranking top in 7 out of 10 evaluations.

Conclusion: SAGE effectively tackles key image editing challenges, offering superior performance and user preference.

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [100] [Variational Visual Question Answering](https://arxiv.org/pdf/2505.09591)
*Tobias Jan Wieczorek, Nathalie Daun, Mohammad Emtiyaz Khan, Marcus Rohrbach*

Main category: cs.CV

TL;DR: A Variational VQA approach improves calibration and reliability in multimodal models, especially under OOD settings, outperforming AdamW fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing reliability concerns in multimodal VQA models, particularly their overconfidence and miscalibration in OOD scenarios.

Method: Proposes using IVON, a variational algorithm, to fine-tune vision-language models, yielding a posterior distribution over parameters.

Result: Reduces Expected Calibration Error by over 50%, increases Coverage by 4% vs. SOTA, and achieves 8% improvement in OOD settings.

Conclusion: Variational learning enhances the reliability of multimodal models, offering better calibration and performance under distribution shifts.

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [101] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/pdf/2505.09608)
*Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen*

Main category: cs.CV

TL;DR: A diffusion-based method for fine-grained control over light sources in images, outperforming existing relighting techniques.


<details>
  <summary>Details</summary>
Motivation: Existing relighting methods lack explicit control over light changes or require multiple input views.

Method: Fine-tunes a diffusion model on real and synthetic image pairs, leveraging light linearity for controlled changes.

Result: Achieves precise illumination control and outperforms existing methods in user preference.

Conclusion: The method provides effective, photorealistic light editing with explicit control.

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [102] [3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image](https://arxiv.org/pdf/2207.14425)
*Hao Wang, Wenhao Shen, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao*

Main category: cs.CV

TL;DR: The paper explores generating 3D cartoon faces from 2D GAN-generated human faces without 3D supervision, using StyleGAN's latent space for expression and pose manipulation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between 2D human faces and 3D cartoon avatars without requiring 3D annotations.

Method: Fine-tunes StyleGAN on cartoon datasets, manipulates latent codes for expressions and poses, and reconstructs 3D shapes from varied 2D images.

Result: Validated on three cartoon datasets, showing effective translation and manipulation.

Conclusion: The approach successfully generates and manipulates 3D cartoon faces without 3D supervision.

Abstract: In this paper, we investigate an open research task of generating 3D cartoon
face shapes from single 2D GAN generated human faces and without 3D
supervision, where we can also manipulate the facial expressions of the 3D
shapes. To this end, we discover the semantic meanings of StyleGAN latent
space, such that we are able to produce face images of various expressions,
poses, and lighting conditions by controlling the latent codes. Specifically,
we first finetune the pretrained StyleGAN face model on the cartoon datasets.
By feeding the same latent codes to face and cartoon generation models, we aim
to realize the translation from 2D human face images to cartoon styled avatars.
We then discover semantic directions of the GAN latent space, in an attempt to
change the facial expressions while preserving the original identity. As we do
not have any 3D annotations for cartoon faces, we manipulate the latent codes
to generate images with different poses and lighting conditions, such that we
can reconstruct the 3D cartoon face shapes. We validate the efficacy of our
method on three cartoon datasets qualitatively and quantitatively.

</details>


### [103] [EiHi Net: Out-of-Distribution Generalization Paradigm](https://arxiv.org/pdf/2209.14946)
*Qinglai Wei, Beiming Yuan, Diancheng Chen*

Main category: cs.CV

TL;DR: EiHi net improves OoD generalization in deep learning by dynamically establishing causal feature-label relationships and suppressing pseudo correlations, validated on the Nico dataset.


<details>
  <summary>Details</summary>
Motivation: Address the OoD generalization problem in deep learning by avoiding pseudo correlations between indecisive features and labels.

Method: Fuses SimCLR and VIC-Reg to dynamically establish causal relationships between features and labels, and employs a human-in-the-loop strategy to prune the representation space.

Result: Significant improvements on the Nico dataset without relying on domain information, outperforming SOTA methods.

Conclusion: EiHi net effectively enhances OoD generalization by focusing on causal relationships and suppressing pseudo correlations.

Abstract: This paper develops a new EiHi net to solve the out-of-distribution (OoD)
generalization problem in deep learning. EiHi net is a model learning paradigm
that can be blessed on any visual backbone. This paradigm can change the
previous learning method of the deep model, namely find out correlations
between inductive sample features and corresponding categories, which suffers
from pseudo correlations between indecisive features and labels. We fuse SimCLR
and VIC-Reg via explicitly and dynamically establishing the original - positive
- negative sample pair as a minimal learning element, the deep model
iteratively establishes a relationship close to the causal one between features
and labels, while suppressing pseudo correlations. To further validate the
proposed model, and strengthen the established causal relationships, we develop
a human-in-the-loop strategy, with few guidance samples, to prune the
representation space directly. Finally, it is shown that the developed EiHi net
makes significant improvements in the most difficult and typical OoD dataset
Nico, compared with the current SOTA results, without any domain ($e.g.$
background, irrelevant features) information.

</details>


### [104] [Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search](https://arxiv.org/pdf/2401.07378)
*Guangyu Meng, Ruyu Zhou, Liu Liu, Peixian Liang, Fang Liu, Danny Chen, Michael Niemier, X. Sharon Hu*

Main category: cs.CV

TL;DR: NNS-EMD is a novel method using Nearest Neighbor Search to approximate Earth Mover's Distance, offering high accuracy, low time complexity, and memory efficiency, with GPU acceleration for large datasets.


<details>
  <summary>Details</summary>
Motivation: Exact EMD is computationally intensive, hindering scalability, while existing approximate methods sacrifice accuracy or require tuning.

Method: NNS-EMD leverages Nearest Neighbor Search to reduce comparisons and enable parallel processing, with GPU vectorization for speed.

Result: NNS-EMD is 44x-135x faster than exact EMD, outperforming other approximate methods in accuracy, speed, and memory efficiency.

Conclusion: NNS-EMD is a scalable, efficient, and accurate alternative to exact and approximate EMD methods, validated in image tasks.

Abstract: Earth Mover's Distance (EMD) is an important similarity measure between two
distributions, used in computer vision and many other application domains.
However, its exact calculation is computationally and memory intensive, which
hinders its scalability and applicability for large-scale problems. Various
approximate EMD algorithms have been proposed to reduce computational costs,
but they suffer lower accuracy and may require additional memory usage or
manual parameter tuning. In this paper, we present a novel approach, NNS-EMD,
to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve
high accuracy, low time complexity, and high memory efficiency. The NNS
operation reduces the number of data points compared in each NNS iteration and
offers opportunities for parallel processing. We further accelerate NNS-EMD via
vectorization on GPU, which is especially beneficial for large datasets. We
compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD
algorithms on image classification and retrieval tasks. We also apply NNS-EMD
to calculate transport mapping and realize color transfer between images.
NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and
achieves superior accuracy, speedup, and memory efficiency over existing
approximate EMD methods.

</details>


### [105] [F$^3$Loc: Fusion and Filtering for Floorplan Localization](https://arxiv.org/pdf/2403.03370)
*Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys*

Main category: cs.CV

TL;DR: An efficient data-driven method for self-localization in floorplans, using a probabilistic model with observation and temporal filtering modules, outperforming state-of-the-art without requiring retraining or large image databases.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust, efficient self-localization in floorplans without relying on retraining or extensive image databases.

Method: A probabilistic model with observation (single/multiview modules for depth prediction) and temporal filtering modules, using ray-based representation.

Result: Operates in real-time on consumer hardware, outperforms state-of-the-art, and works without upright images.

Conclusion: The proposed method is efficient, robust, and superior to existing solutions for floorplan-based self-localization.

Abstract: In this paper we propose an efficient data-driven solution to
self-localization within a floorplan. Floorplan data is readily available,
long-term persistent and inherently robust to changes in the visual appearance.
Our method does not require retraining per map and location or demand a large
database of images of the area of interest. We propose a novel probabilistic
model consisting of an observation and a novel temporal filtering module.
Operating internally with an efficient ray-based representation, the
observation module consists of a single and a multiview module to predict
horizontal depth from images and fuses their results to benefit from advantages
offered by either methodology. Our method operates on conventional consumer
hardware and overcomes a common limitation of competing methods that often
demand upright images. Our full system meets real-time requirements, while
outperforming the state-of-the-art by a significant margin.

</details>


### [106] [ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement](https://arxiv.org/pdf/2407.19708)
*Ezequiel Perez-Zarate, Oscar Ramos-Soto, Chunxiao Liu, Diego Oliva, Marco Perez-Cisneros*

Main category: cs.CV

TL;DR: ALEN introduces an adaptive network for low-light image enhancement, using classification to decide between local or global illumination adjustment, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Low-light conditions degrade image quality, impacting applications like surveillance and autonomous driving. Current methods struggle with diverse real-world scenarios.

Method: ALEN uses LCNet for illuminance classification, SCNet for illumination estimation, and MCNet for color enhancement.

Result: ALEN shows superior performance in quantitative and qualitative assessments on public datasets.

Conclusion: ALEN advances low-light enhancement and benefits high-level vision tasks like semantic segmentation.

Abstract: Low-light image enhancement is an important task in computer vision,
essential for improving the visibility and quality of images captured in
non-optimal lighting conditions. Inadequate illumination can lead to
significant information loss and poor image quality, impacting various
applications such as surveillance. photography, or even autonomous driving. In
this regard, automated methods have been developed to automatically adjust
illumination in the image for a better visual perception. Current enhancement
techniques often use specific datasets to enhance low-light images, but still
present challenges when adapting to diverse real-world conditions, where
illumination degradation may be localized to specific regions. To address this
challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose
main approach is the use of a classification mechanism to determine whether
local or global illumination enhancement is required. Subsequently, estimator
networks adjust illumination based on this classification and simultaneously
enhance color fidelity. ALEN integrates the Light Classification Network
(LCNet) for illuminance categorization, complemented by the Single-Channel
Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of
illumination and color, respectively. Extensive experiments on publicly
available datasets for low-light conditions were carried out to underscore
ALEN's robust generalization capabilities, demonstrating superior performance
in both quantitative metrics and qualitative assessments when compared to
recent state-of-the-art methods. The ALEN not only enhances image quality in
terms of visual perception but also represents an advancement in high-level
vision tasks, such as semantic segmentation, as presented in this work. The
code of this method is available at https://github.com/xingyumex/ALEN

</details>


### [107] [BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for Better Evaluation under Visual Ambiguities](https://arxiv.org/pdf/2408.17297)
*Boris Meden, Asma Brazi, Fabrice Mayran de Chamisso, Steve Bourgeois, Vincent Lepetit*

Main category: cs.CV

TL;DR: The paper addresses the need for per-image 6D pose annotations to account for visual ambiguities in pose estimation, proposes an automatic re-annotation method, re-evaluates state-of-the-art methods, and introduces a benchmark for pose distribution methods.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation benchmarks overlook per-image visual ambiguities caused by object symmetries or occlusions, leading to inaccurate evaluations.

Method: An automatic method is proposed to re-annotate datasets with image-specific 6D pose distributions, considering object surface visibility. State-of-the-art methods are re-evaluated, and a precision/recall benchmark for pose distribution methods is introduced.

Result: Re-annotation improves ground truth accuracy, altering the ranking of existing methods. The new benchmark enables evaluation of pose distribution methods.

Conclusion: The work highlights the importance of per-image pose annotations and provides tools for better evaluation of 6D pose estimation methods.

Abstract: 6D pose estimation aims at determining the object pose that best explains the
camera observation. The unique solution for non-ambiguous objects can turn into
a multi-modal pose distribution for symmetrical objects or when occlusions of
symmetry-breaking elements happen, depending on the viewpoint. Currently, 6D
pose estimation methods are benchmarked on datasets that consider, for their
ground truth annotations, visual ambiguities as only related to global object
symmetries, whereas they should be defined per-image to account for the camera
viewpoint. We thus first propose an automatic method to re-annotate those
datasets with a 6D pose distribution specific to each image, taking into
account the object surface visibility in the image to correctly determine the
visual ambiguities. Second, given this improved ground truth, we re-evaluate
the state-of-the-art single pose methods and show that this greatly modifies
the ranking of these methods. Third, as some recent works focus on estimating
the complete set of solutions, we derive a precision/recall formulation to
evaluate them against our image-wise distribution ground truth, making it the
first benchmark for pose distribution methods on real images.

</details>


### [108] [One Homography is All You Need: IMM-based Joint Homography and Multiple Object State Estimation](https://arxiv.org/pdf/2409.02562)
*Paul Johannes Claasen, Johan Pieter de Villiers*

Main category: cs.CV

TL;DR: IMM-JHSE is a novel online MOT algorithm that improves tracking by jointly modeling homography and dynamics, outperforming other methods on several datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-object tracking (MOT) by reducing reliance on explicit camera motion compensation and improving robustness to motion away from the ground plane.

Method: Uses IMM filter for camera motion, combines static/dynamic models, and employs dynamic noise estimation. Association is done via mixed BIoU scores and Mahalanobis distances.

Result: Outperforms UCMCTrack, OC-SORT, C-BIoU, and ByteTrack on DanceTrack and KITTI-car datasets (HOTA increases by 2.64 and 2.11). Competitive on MOT17, MOT20, and KITTI-pedestrian.

Conclusion: IMM-JHSE is a robust and efficient MOT method, excelling in 2D tracking and showing promise against 3D methods.

Abstract: A novel online MOT algorithm, IMM Joint Homography State Estimation
(IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the
only additional 3D information, whereas other 3D MOT methods use regular 3D
measurements. By jointly modelling the homography matrix and its dynamics as
part of track state vectors, IMM-JHSE removes the explicit influence of camera
motion compensation techniques on predicted track position states, which was
prevalent in previous approaches. Expanding upon this, static and dynamic
camera motion models are combined using an IMM filter. A simple bounding box
motion model is used to predict bounding box positions to incorporate image
plane information. In addition to applying an IMM to camera motion, a
non-standard IMM approach is applied where bounding-box-based BIoU scores are
mixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to
perform association only, making IMM-JHSE robust to motion away from the ground
plane. Finally, IMM-JHSE makes use of dynamic process and measurement noise
estimation techniques. IMM-JHSE improves upon related techniques, including
UCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car
datasets, increasing HOTA by 2.64 and 2.11, respectively, while offering
competitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets.
Using publicly available detections, IMM-JHSE outperforms almost all other 2D
MOT methods and is outperformed only by 3D MOT methods -- some of which are
offline -- on the KITTI-car dataset. Compared to tracking-by-attention methods,
IMM-JHSE shows remarkably similar performance on the DanceTrack dataset and
outperforms them on the MOT17 dataset. The code is publicly available:
https://github.com/Paulkie99/imm-jhse.

</details>


### [109] [State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features](https://arxiv.org/pdf/2409.18769)
*George R. Nahass, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Nicholas Tomaras, Madison Cheung, Alex Palacios, Kevin Heinze, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi*

Main category: cs.CV

TL;DR: An automated segmentation pipeline for periorbital distances outperforms manual and existing automated methods, achieving state-of-the-art accuracy and robustness in disease classification under varied conditions.


<details>
  <summary>Details</summary>
Motivation: Manual measurement of periorbital distances is subjective and prone to variability, while existing automated methods are limited by strict imaging requirements and small datasets.

Method: Developed a segmentation pipeline trained on a domain-specific dataset, compared it with Segment Anything Model (SAM) and PeriorbitAI, and evaluated segmentation accuracy and classification performance using periorbital distances as features.

Result: The segmentation model achieved state-of-the-art accuracy, with error rates within intergrader variability. Classification models using periorbital distances matched or outperformed CNNs, especially under out-of-distribution conditions.

Conclusion: Segmentation-derived periorbital distances provide robust, explainable features for disease classification, generalizing better under domain shifts than CNN-based methods, and set a new benchmark for real-world deployment.

Abstract: Periorbital distances are critical markers for diagnosing and monitoring a
range of oculoplastic and craniofacial conditions. Manual measurement, however,
is subjective and prone to intergrader variability. Automated methods have been
developed but remain limited by standardized imaging requirements, small
datasets, and a narrow focus on individual measurements. We developed a
segmentation pipeline trained on a domain-specific dataset of healthy eyes and
compared its performance against the Segment Anything Model (SAM) and the prior
benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple
disease classes and imaging conditions. We further investigated the use of
predicted periorbital distances as features for disease classification under
in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow
classifiers, CNNs, and fusion models. Our segmentation model achieved
state-of-the-art accuracy across all datasets, with error rates within
intergrader variability and superior performance relative to SAM and
PeriorbitAI. In classification tasks, models trained on periorbital distances
matched CNN performance on ID data (77--78\% accuracy) and substantially
outperformed CNNs under OOD conditions (63--68\% accuracy vs. 14\%). Fusion
models achieved the highest ID accuracy (80\%) but were sensitive to degraded
CNN features under OOD shifts. Segmentation-derived periorbital distances
provide robust, explainable features for disease classification and generalize
better under domain shift than CNN image classifiers. These results establish a
new benchmark for periorbital distance prediction and highlight the potential
of anatomy-based AI pipelines for real-world deployment in oculoplastic and
craniofacial care.

</details>


### [110] [Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos](https://arxiv.org/pdf/2410.07795)
*Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt*

Main category: cs.CV

TL;DR: A novel method integrates physics models with kinematics observations using a neural Kalman-filtering approach to improve human motion capture from monocular videos, addressing temporal artifacts and enhancing physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Modern motion capture methods often produce temporal artifacts like jittery motion and struggle with smooth, physically plausible results. Current approaches rely on imperfect physical models and require extensive preprocessing.

Method: The proposed method uses a meta-PD controller to predict joint torques and reaction forces, followed by physics-based motion simulation. A recurrent neural network implements a Kalman filter to balance kinematics input and simulated motion.

Result: The approach outperforms state-of-the-art methods in physics-based human pose estimation, producing accurate global motion trajectories and physically plausible poses.

Conclusion: The neural Kalman-filtering method effectively balances kinematics and physics, improving motion capture quality and physical plausibility.

Abstract: Human motion capture from monocular videos has made significant progress in
recent years. However, modern approaches often produce temporal artifacts, e.g.
in form of jittery motion and struggle to achieve smooth and physically
plausible motions. Explicitly integrating physics, in form of internal forces
and exterior torques, helps alleviating these artifacts. Current
state-of-the-art approaches make use of an automatic PD controller to predict
torques and reaction forces in order to re-simulate the input kinematics, i.e.
the joint angles of a predefined skeleton. However, due to imperfect physical
models, these methods often require simplifying assumptions and extensive
preprocessing of the input kinematics to achieve good performance. To this end,
we propose a novel method to selectively incorporate the physics models with
the kinematics observations in an online setting, inspired by a neural
Kalman-filtering approach. We develop a control loop as a meta-PD controller to
predict internal joint torques and external reaction forces, followed by a
physics-based motion simulation. A recurrent neural network is introduced to
realize a Kalman filter that attentively balances the kinematics input and
simulated motion, resulting in an optimal-state dynamics prediction. We show
that this filtering step is crucial to provide an online supervision that helps
balancing the shortcoming of the respective input motions, thus being important
for not only capturing accurate global motion trajectories but also producing
physically plausible human poses. The proposed approach excels in the
physics-based human pose estimation task and demonstrates the physical
plausibility of the predictive dynamics, compared to state of the art. The code
is available on https://github.com/cuongle1206/OSDCap

</details>


### [111] [Reflecting Topology Consistency and Abnormality via Learnable Attentions for Airway Labeling](https://arxiv.org/pdf/2410.23854)
*Chenyu Li, Minghui Zhang, Chuyan Zhang, Yun Gu*

Main category: cs.CV

TL;DR: A novel method for automatic airway anatomical labeling improves accuracy and consistency by incorporating Soft Subtree Consistency (SSC) and Abnormal Branch Saliency (ABS) modules, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate airway labeling is vital for bronchoscopy but challenging due to anatomical variability. Inconsistent predictions from prior methods hinder clinical use.

Method: Proposes SSC for flexible topological feature aggregation and ABS to distinguish abnormal branches, preventing feature mixing.

Result: Achieves 91.4% segmental and 83.7% subsegmental accuracy, with improved topological consistency and reliability in deformed airways.

Conclusion: The method enhances labeling accuracy and consistency, especially in challenging cases, benefiting clinical applications.

Abstract: Accurate airway anatomical labeling is crucial for clinicians to identify and
navigate complex bronchial structures during bronchoscopy. Automatic airway
anatomical labeling is challenging due to significant individual variability
and anatomical variations. Previous methods are prone to generate inconsistent
predictions, which is harmful for preoperative planning and intraoperative
navigation. This paper aims to address these challenges by proposing a novel
method that enhances topological consistency and improves the detection of
abnormal airway branches. We propose a novel approach incorporating two
modules: the Soft Subtree Consistency (SSC) and the Abnormal Branch Saliency
(ABS). The SSC module constructs a soft subtree to capture clinically relevant
topological relationships, allowing for flexible feature aggregation within and
across subtrees. The ABS module facilitates the interaction between node
features and prototypes to distinguish abnormal branches, preventing the
erroneous aggregation of features between normal and abnormal nodes. Evaluated
on a challenging dataset characterized by severe airway distortion and atrophy,
our method achieves superior performance compared to state-of-the-art
approaches. Specifically, it attains a 91.4% accuracy at the segmental level
and an 83.7% accuracy at the subsegmental level, representing a 1.4% increase
in subsegmental accuracy and a 3.1% increase in topological consistency.
Notably, the method demonstrates reliable performance in cases with
disease-induced airway deformities, ensuring consistent and accurate labeling.

</details>


### [112] [MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day](https://arxiv.org/pdf/2412.05888)
*Donghang Lyu, Ruochen Gao, Marius Staring*

Main category: cs.CV

TL;DR: MCP-MedSAM is a lightweight medical image segmentation model based on SAM, designed for efficient training on a single GPU while improving performance through modality and content prompts.


<details>
  <summary>Details</summary>
Motivation: SAM's large size and high GPU requirements limit its scalability in medical applications. MCP-MedSAM addresses this by offering a lightweight, efficient alternative.

Method: Introduces modality and content prompts to enhance segmentation, uses a modality-based data sampling strategy, and trains on a single A100 GPU in one day.

Result: Achieves superior performance compared to top-ranking methods on a challenge dataset, with efficient training.

Conclusion: MCP-MedSAM is a scalable, high-performance solution for medical image segmentation, balancing efficiency and accuracy.

Abstract: Medical image segmentation involves partitioning medical images into
meaningful regions, with a focus on identifying anatomical structures and
lesions. It has broad applications in healthcare, and deep learning methods
have enabled significant advancements in automating this process. Recently, the
introduction of the Segmentation Anything Model (SAM), the first foundation
model for segmentation task, has prompted researchers to adapt it for the
medical domain to improve performance across various tasks. However, SAM's
large model size and high GPU requirements hinder its scalability and
development in the medical domain. In this work, we propose MCP-MedSAM, a
powerful and lightweight medical SAM model designed to be trainable on a single
A100 GPU with 40GB of memory within one day while delivering superior
segmentation performance. Recognizing the significant internal differences
between modalities and the need for direct segmentation target information
within bounding boxes, we introduce two kinds of prompts: the modality prompt
and the content prompt. After passing through the prompt encoder, their
embedding representations can further improve the segmentation performance by
incorporating more relevant information without adding significant training
overhead. Additionally, we adopt an effective modality-based data sampling
strategy to address data imbalance between modalities, ensuring more balanced
performance across all modalities. Our method was trained and evaluated using a
large-scale challenge dataset, compared to top-ranking methods on the challenge
leaderboard, MCP-MedSAM achieved superior performance while requiring only one
day of training on a single GPU. The code is publicly available at
\textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}

</details>


### [113] [MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification](https://arxiv.org/pdf/2502.07409)
*Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild*

Main category: cs.CV

TL;DR: The paper introduces a prompt learning method for few-shot pathology image classification, leveraging a vision-language model with multi-granular attention and optimal transport-based distance to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Challenges in whole slide pathology image classification due to gigapixel sizes and limited annotations hinder model generalization.

Method: Extends Prov-GigaPath into a vision-language model, aligns it with medical text encoders, and uses multi-granular attention and optimal transport-based distance for robustness.

Result: Outperforms competitors on lung, kidney, and breast pathology datasets, improving performance across architectures like CLIP, PLIP, and Prov-GigaPath integrated PLIP.

Conclusion: The proposed method effectively addresses few-shot pathology classification challenges, enhancing model generalization and robustness.

Abstract: Whole slide pathology image classification presents challenges due to
gigapixel image sizes and limited annotation labels, hindering model
generalization. This paper introduces a prompt learning method to adapt large
vision-language models for few-shot pathology classification. We first extend
the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology
image tiles, into a vision-language model by adding adaptors and aligning it
with medical text encoders via contrastive learning on 923K image-text pairs.
The model is then used to extract visual features and text embeddings from
few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike
prior methods that combine prompts with frozen features using prefix embeddings
or self-attention, we propose multi-granular attention that compares
interactions between learnable prompts with individual image patches and groups
of them. This approach improves the model's ability to capture both
fine-grained details and broader context, enhancing its recognition of complex
patterns across sub-regions. To further improve accuracy, we leverage
(unbalanced) optimal transport-based visual-text distance to secure model
robustness by mitigating perturbations that might occur during the data
augmentation process. Empirical experiments on lung, kidney, and breast
pathology modalities validate the effectiveness of our approach; thereby, we
surpass several of the latest competitors and consistently improve performance
across diverse architectures, including CLIP, PLIP, and Prov-GigaPath
integrated PLIP. We release our implementations and pre-trained models at this
MGPATH.

</details>


### [114] [An ocean front detection and tracking algorithm](https://arxiv.org/pdf/2502.15250)
*Yishuo Wang, Feng Zhou, Qicheng Meng, Muping Zhou, Zhijun Hu, Chengqing Zhang, Tianhao Zhao*

Main category: cs.CV

TL;DR: The paper proposes BFDT-MSA, a Bayesian framework for ocean front detection, addressing limitations like discontinuous outputs and over-detection with innovations like Bayesian decision mechanisms and morphological refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for ocean front detection have flaws like discontinuous outputs and reliance on single-threshold decisions, prompting the need for a more robust solution.

Method: BFDT-MSA integrates Bayesian decision mechanisms, morphological refinement algorithms, and metric space analysis for temporal front tracking.

Result: Validated on global SST data, BFDT-MSA reduces over-detection by 73% and improves intensity, continuity, and coherence.

Conclusion: The framework offers a reproducible, open-source solution for ocean front detection, advancing oceanographic research.

Abstract: Existing ocean front detection methods--including histogram-based variance
analysis, Lyapunov exponent, gradient thresholding, and machine
learning--suffer from critical limitations: discontinuous outputs,
over-detection, reliance on single-threshold decisions, and lack of open-source
implementations. To address these challenges, this paper proposes the Bayesian
Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA).
The framework introduces three innovations: (1) a Bayesian decision mechanism
that integrates gradient priors and field operators to eliminate manual
threshold sensitivity; (2) morphological refinement algorithms for merging
fragmented fronts, deleting spurious rings, and thinning frontal zones to
pixel-level accuracy; and (3) a novel metric space definition for temporal
front tracking, enabling systematic analysis of front evolution. Validated on
global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\%$
compared to histogram-based methods while achieving superior intensity
($0.16^\circ$C/km), continuity, and spatiotemporal coherence. The open-source
release bridges a critical gap in reproducible oceanographic research.

</details>


### [115] [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/pdf/2503.21776)
*Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue*

Main category: cs.CV

TL;DR: Video-R1 introduces a rule-based RL approach for video reasoning in MLLMs, addressing temporal modeling and data scarcity with T-GRPO and hybrid datasets, achieving superior performance over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To extend the R1 paradigm for video reasoning in MLLMs, overcoming challenges like temporal modeling and data scarcity.

Method: Proposes T-GRPO for temporal modeling and uses hybrid image-video datasets (Video-R1-CoT-165k and Video-R1-260k) for training.

Result: Significant improvements on benchmarks like VideoMMMU and VSI-Bench, with Video-R1-7B outperforming GPT-4o (37.1% accuracy on VSI-bench).

Conclusion: Video-R1 successfully adapts RL for video reasoning, demonstrating state-of-the-art performance and releasing all resources openly.

Abstract: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for incentivizing video
reasoning within multimodal large language models (MLLMs). However, directly
applying RL training with the GRPO algorithm to video reasoning presents two
primary challenges: (i) a lack of temporal modeling for video reasoning, and
(ii) the scarcity of high-quality video-reasoning data. To address these
issues, we first propose the T-GRPO algorithm, which encourages models to
utilize temporal information in videos for reasoning. Additionally, instead of
relying solely on video data, we incorporate high-quality image-reasoning data
into the training process. We have constructed two datasets: Video-R1-CoT-165k
for SFT cold start and Video-R1-260k for RL training, both comprising image and
video data. Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
code, models, and data are released in: https://github.com/tulerfeng/Video-R1.

</details>


### [116] [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/pdf/2504.14988)
*Hong-Tao Yu, Xiu-Shen Wei, Yuxin Peng, Serge Belongie*

Main category: cs.CV

TL;DR: The paper introduces FG-BMK, a fine-grained evaluation benchmark for Large Vision-Language Models (LVLMs), addressing gaps in assessing their performance on detailed image tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LVLMs lack focus on fine-grained image tasks, which are fundamental to computer vision.

Method: The authors create FG-BMK, a benchmark with 1.01M questions and 0.33M images, and evaluate 12 LVLMs/VLMs on semantic recognition and fine-grained feature representation.

Result: Key findings include insights into training paradigms, modality alignment, perturbation susceptibility, and fine-grained reasoning.

Conclusion: The study highlights LVLM limitations and provides guidance for future data and model improvements, with open-source code available.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
remarkable multimodal perception capabilities, garnering significant attention.
While numerous evaluation studies have emerged, assessing LVLMs both
holistically and on specialized tasks, fine-grained image tasks-fundamental to
computer vision-remain largely unexplored. To fill this gap, we introduce a
comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 1.01
million questions and 0.33 million images. Our evaluation systematically
examines LVLMs from both human-oriented and machine-oriented perspectives,
focusing on their semantic recognition and fine-grained feature representation
capabilities. Through extensive experiments on twelve representative
LVLMs/VLMs, we uncover key findings regarding the influence of training
paradigms, modality alignment, perturbation susceptibility, and fine-grained
category reasoning on task performance. This work provides critical insights
into the limitations of current LVLMs and offers guidance for future data
construction and model design in the development of more advanced LVLMs. Our
code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.

</details>


### [117] [GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction](https://arxiv.org/pdf/2505.02126)
*Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao*

Main category: cs.CV

TL;DR: GarmentGS uses dense point clouds to guide 3D Gaussian Splatting for fast, high-fidelity garment reconstruction, outperforming traditional methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D garment creation is time-consuming and labor-intensive. Gaussian Splatting offers potential but struggles with unstructured primitives for garment reconstruction.

Method: Introduces GarmentGS, leveraging dense point clouds to guide Gaussian primitives for accurate, non-watertight garment reconstruction and fast point cloud generation.

Result: Achieves garment reconstruction in 10 minutes (vs. hours), with high geometric accuracy and superior rendering.

Conclusion: GarmentGS enables fast, high-quality garment reconstruction with real-time rendering, advancing 3D garment modeling.

Abstract: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.

</details>


### [118] [HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation](https://arxiv.org/pdf/2505.06512)
*Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang*

Main category: cs.CV

TL;DR: HCMA improves text-to-image synthesis by combining global and local alignment for better semantic fidelity and spatial control.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack spatial control and semantic fidelity in complex scenes.

Method: HCMA integrates global (scene-level) and local (object-level) alignment modules into diffusion sampling.

Result: HCMA outperforms baselines, improving FID by 0.69 and CLIP Score by 0.0295.

Conclusion: HCMA effectively balances semantic accuracy and spatial control for grounded image generation.

Abstract: Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation.Our code
is available at https://github.com/hwang-cs-ime/HCMA

</details>


### [119] [G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition](https://arxiv.org/pdf/2505.08233)
*Santhoshkumar Peddi, Soham Bandyopadhyay, Debasis Samanta*

Main category: cs.CV

TL;DR: G-MSGINet is a unified framework for contactless fingerprint recognition, combining minutiae localization and identity embedding without complex preprocessing. It outperforms existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing fingerprint recognition methods rely on multi-branch architectures or complex preprocessing, limiting scalability and generalization. G-MSGINet aims to address these limitations.

Method: The framework uses GMSGI layers, integrating pixel-level involution, multi-scale kernel generation, and graph-based relational modeling. It refines features end-to-end without orientation supervision.

Result: G-MSGINet achieves F1-scores of 0.83Â±0.02, Rank-1 accuracies of 97.0%-99.1%, and an EER of 0.5%, outperforming prior methods with fewer parameters.

Conclusion: G-MSGINet is scalable and effective for real-world contactless fingerprint recognition, offering superior performance with reduced computational costs.

Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust
contactless fingerprint recognition that jointly performs minutiae localization
and identity embedding directly from raw input images. Existing approaches rely
on multi-branch architectures, orientation labels, or complex preprocessing
steps, which limit scalability and generalization across real-world acquisition
scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a
novel computational module that integrates grouped pixel-level involution,
dynamic multi-scale kernel generation, and graph-based relational modelling
into a single processing unit. Stacked GMSGI layers progressively refine both
local minutiae-sensitive features and global topological representations
through end-to-end optimization. The architecture eliminates explicit
orientation supervision and adapts graph connectivity directly from learned
kernel descriptors, thereby capturing meaningful structural relationships among
fingerprint regions without fixed heuristics. Extensive experiments on three
benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that
G-MSGINet consistently achieves minutiae F1-scores in the range of
$0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,
while maintaining an Equal Error Rate (EER) as low as 0.5%. These results
correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1
accuracy when compared to prior methods, using only 0.38 million parameters and
6.63 giga floating-point operations, which represents up to ten times fewer
parameters than competitive baselines. This highlights the scalability and
effectiveness of G-MSGINet in real-world contactless biometric recognition
scenarios.

</details>


### [120] [Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting](https://arxiv.org/pdf/2505.08527)
*Zheang Huai, Hui Tang, Yi Li, Zhuangzhuang Chen, Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces a Dual Feature Guided (DFG) auto-prompting approach for source-free domain adaptation (SFDA) in segmentation, leveraging the Segment Anything Model (SAM) to generate accurate bounding box prompts and outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to adapt a segmentation model trained in a source domain to a target domain without labeled target data, addressing the challenge of defective bounding boxes generated by existing SFDA methods due to domain gaps.

Method: The method involves a two-phase approach: (1) training the source model in a feature aggregation phase to adapt it to the target domain and prepare for box prompt search, and (2) using DFG to expand the box prompt guided by target model and SAM features, followed by pseudo-label refinement based on connectivity analysis.

Result: Experiments on 3D and 2D datasets show superior performance compared to conventional SFDA methods.

Conclusion: The DFG auto-prompting approach effectively addresses the domain gap issue in SFDA for segmentation, leveraging SAM to improve accuracy and outperforming existing methods.

Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a
model trained in the source domain to perform well in the target domain with
only the source model and unlabeled target data.Inspired by the recent success
of Segment Anything Model (SAM) which exhibits the generality of segmenting
images of various modalities and in different domains given human-annotated
prompts like bounding boxes or points, we for the first time explore the
potentials of Segment Anything Model for SFDA via automatedly finding an
accurate bounding box prompt. We find that the bounding boxes directly
generated with existing SFDA approaches are defective due to the domain gap.To
tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting
approach to search for the box prompt. Specifically, the source model is first
trained in a feature aggregation phase, which not only preliminarily adapts the
source model to the target domain but also builds a feature distribution
well-prepared for box prompt search. In the second phase, based on two feature
distribution observations, we gradually expand the box prompt with the guidance
of the target model feature and the SAM feature to handle the class-wise
clustered target features and the class-wise dispersed target features,
respectively. To remove the potentially enlarged false positive regions caused
by the over-confident prediction of the target model, the refined pseudo-labels
produced by SAM are further postprocessed based on connectivity analysis.
Experiments on 3D and 2D datasets indicate that our approach yields superior
performance compared to conventional methods. Code is available at
https://github.com/xmed-lab/DFG.

</details>


### [121] [The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning](https://arxiv.org/pdf/2505.08537)
*Mohamed Lamine Mekhalfi, Paul Chippendale, Fabio Poiesi, Samuele Bonecher, Gilberto Osler, Nicola Zancanella*

Main category: cs.CV

TL;DR: The paper explores using computer vision for real-time raspberry grading, introducing the RaspGrade dataset, and highlights challenges in classification due to color similarities and occlusion.


<details>
  <summary>Details</summary>
Motivation: To enable rapid, accurate, and non-invasive food quality assessment in industrial settings, specifically for raspberry grading.

Method: Utilized instance segmentation on the RaspGrade dataset to classify raspberries into five grades, addressing challenges like color similarities and occlusion.

Result: Accurate fruit-level masks were achieved, but classification of some grades was difficult due to color similarities and occlusion.

Conclusion: The RaspGrade dataset is publicly available, and while some grading challenges persist, the method shows promise for industrial food quality assessment.

Abstract: This research investigates the application of computer vision for rapid,
accurate, and non-invasive food quality assessment, focusing on the novel
challenge of real-time raspberry grading into five distinct classes within an
industrial environment as the fruits move along a conveyor belt. To address
this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and
meticulously annotated. Instance segmentation experiments revealed that
accurate fruit-level masks can be obtained; however, the classification of
certain raspberry grades presents challenges due to color similarities and
occlusion, while others are more readily distinguishable based on color. The
acquired and annotated RaspGrade dataset is accessible on Hugging Face at:
https://huggingface.co/datasets/FBK-TeV/RaspGrade.

</details>


### [122] [Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections](https://arxiv.org/pdf/2505.08568)
*Xiao Ni, Carsten Kuehnel, Xiaoyi Jiang*

Main category: cs.CV

TL;DR: A thermal detector-based traffic light system (YOLO-Thermal) is proposed to address limitations of RGB camera systems, focusing on mobility-impaired individuals, with improved robustness and privacy.


<details>
  <summary>Details</summary>
Motivation: Current RGB camera-based traffic light systems neglect mobility-impaired individuals and face challenges like poor visibility in adverse conditions and privacy concerns.

Method: Developed YOLO-Thermal, a modified YOLO architecture with advanced feature extraction and attention mechanisms, using the TD4PWMR thermal dataset.

Result: YOLO-Thermal outperforms existing detectors, and the system enhances barrier-free intersection accessibility.

Conclusion: The thermal-based system offers a robust, privacy-friendly solution for inclusive traffic management.

Abstract: Rapid advances in deep learning for computer vision have driven the adoption
of RGB camera-based adaptive traffic light systems to improve traffic safety
and pedestrian comfort. However, these systems often overlook the needs of
people with mobility restrictions. Moreover, the use of RGB cameras presents
significant challenges, including limited detection performance under adverse
weather or low-visibility conditions, as well as heightened privacy concerns.
To address these issues, we propose a fully automated, thermal detector-based
traffic light system that dynamically adjusts signal durations for individuals
with walking impairments or mobility burden and triggers the auditory signal
for visually impaired individuals, thereby advancing towards barrier-free
intersection for all users. To this end, we build the thermal dataset for
people with mobility restrictions (TD4PWMR), designed to capture diverse
pedestrian scenarios, particularly focusing on individuals with mobility aids
or mobility burden under varying environmental conditions, such as different
lighting, weather, and crowded urban settings. While thermal imaging offers
advantages in terms of privacy and robustness to adverse conditions, it also
introduces inherent hurdles for object detection due to its lack of color and
fine texture details and generally lower resolution of thermal images. To
overcome these limitations, we develop YOLO-Thermal, a novel variant of the
YOLO architecture that integrates advanced feature extraction and attention
mechanisms for enhanced detection accuracy and robustness in thermal imaging.
Experiments demonstrate that the proposed thermal detector outperforms existing
detectors, while the proposed traffic light system effectively enhances
barrier-free intersection. The source codes and dataset are available at
https://github.com/leon2014dresden/YOLO-THERMAL.

</details>


### [123] [WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks](https://arxiv.org/pdf/2505.08614)
*Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma*

Main category: cs.CV

TL;DR: WaveGuard is a watermarking framework using DT-CWT and SC-GNN to combat deepfake threats with robust, imperceptible watermarks.


<details>
  <summary>Details</summary>
Motivation: Addressing risks like privacy invasion and identity theft from deepfake technology.

Method: Embeds watermarks in high-frequency sub-bands via DT-CWT and uses SC-GNN for visual quality. Includes an attention module for precision.

Result: Outperforms state-of-the-art methods in robustness and visual quality on face swap and reenactment tasks.

Conclusion: WaveGuard effectively mitigates deepfake risks with superior performance.

Abstract: Deepfake technology poses increasing risks such as privacy invasion and
identity theft. To address these threats, we propose WaveGuard, a proactive
watermarking framework that enhances robustness and imperceptibility via
frequency-domain embedding and graph-based structural consistency.
Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree
Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph
Neural Network (SC-GNN) to preserve visual quality. We also design an attention
module to refine embedding precision. Experimental results on face swap and
reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art
methods in both robustness and visual quality. Code is available at
https://github.com/vpsg-research/WaveGuard.

</details>


### [124] [Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology](https://arxiv.org/pdf/2505.08765)
*Yatai Ji, Zhengqiu Zhu, Yong Zhao, Beidan Liu, Chen Gao, Yihao Zhao, Sihang Qiu, Yue Hu, Quanjun Yin, Yong Li*

Main category: cs.CV

TL;DR: CityAVOS is a benchmark dataset for UAV-based object search in urban environments, paired with PRPSearcher, a novel agentic method using MLLMs for improved perception, reasoning, and planning.


<details>
  <summary>Details</summary>
Motivation: Existing UAV object search methods struggle in complex urban settings due to redundant processing, similar object confusion, and exploration-exploitation trade-offs.

Method: PRPSearcher uses multi-modal LLMs to create three specialized maps (dynamic semantic, 3D cognitive, and 3D uncertainty) and employs denoising and IPT prompting for adaptive planning.

Result: PRPSearcher outperforms baselines (+37.69% success rate, +28.96% efficiency) but still lags behind human performance.

Conclusion: The work lays groundwork for future embodied search advancements, highlighting the need for better semantic reasoning and exploration.

Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require
Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target
objects using visual and textual cues without external guidance. Existing
approaches struggle in complex urban environments due to redundant semantic
processing, similar object distinction, and the exploration-exploitation
dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,
the first benchmark dataset for autonomous search of common urban objects. This
dataset comprises 2,420 tasks across six object categories with varying
difficulty levels, enabling comprehensive evaluation of UAV agents' search
capabilities. To solve the AVOS tasks, we also propose PRPSearcher
(Perception-Reasoning-Planning Searcher), a novel agentic method powered by
multi-modal large language models (MLLMs) that mimics human three-tier
cognition. Specifically, PRPSearcher constructs three specialized maps: an
object-centric dynamic semantic map enhancing spatial perception, a 3D
cognitive map based on semantic attraction values for target reasoning, and a
3D uncertainty map for balanced exploration-exploitation search. Also, our
approach incorporates a denoising mechanism to mitigate interference from
similar objects and utilizes an Inspiration Promote Thought (IPT) prompting
mechanism for adaptive action planning. Experimental results on CityAVOS
demonstrate that PRPSearcher surpasses existing baselines in both success rate
and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and
-46.40% NE). While promising, the performance gap compared to humans highlights
the need for better semantic reasoning and spatial exploration capabilities in
AVOS tasks. This work establishes a foundation for future advances in embodied
target search. Dataset and source code are available at
https://anonymous.4open.science/r/CityAVOS-3DF8.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [125] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/pdf/2505.08896)
*Pankaj Kumar, Aditya Mishra, Pranamesh Chakraborty, Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: A Deep Reinforcement Learning (DRL) approach for autonomous vehicle control at signalised intersections improves efficiency, safety, and comfort by optimizing acceleration/deceleration using DDPG and SAC algorithms.


<details>
  <summary>Details</summary>
Motivation: Developing autonomous vehicle control at signalised intersections is challenging due to complex decision-making, necessitating a robust and efficient strategy.

Method: The study uses DRL (DDPG and SAC) with a tailored reward function focusing on efficiency, decision-making during amber lights, and asymmetric acceleration/deceleration. Training combines real-world and simulated trajectories.

Result: RL models achieve lower distance headway and jerk than human drivers, maintaining safety. Both DDPG and SAC handle critical scenarios, with DDPG offering smoother actions.

Conclusion: DRL-based control at signalised intersections enhances traffic safety, efficiency, and comfort, demonstrating the viability of autonomous solutions.

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [126] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/pdf/2505.08905)
*Michael Majurski, Cynthia Matuszek*

Main category: cs.AI

TL;DR: The paper proposes automating the construction of fact-based synthetic benchmarks for evaluating language models (LMs) using grounding documents, reducing human effort and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Human effort in benchmark construction is limited and outpaced by LM advancements, making automated solutions necessary.

Method: Uses LMs to generate domain-specific evaluation questions from grounding documents (e.g., textbooks), creating synthetic benchmarks.

Result: The synthetic benchmarks correlate well with human-curated ones (Spearman 0.96, Pearson 0.79) and reveal strong performance of Gemma3 models.

Conclusion: Automated synthetic benchmarking is a scalable and effective alternative to human-curated evaluations for assessing LM capabilities.

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [127] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.08995)
*Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger*

Main category: cs.AI

TL;DR: A hierarchical multi-agent reinforcement learning framework is proposed for air combat simulations, addressing challenges like complex dynamics and large state-action spaces by splitting decision-making into low-level (unit control) and high-level (mission command) policies.


<details>
  <summary>Details</summary>
Motivation: To explore real-world defense scenarios cost-effectively and safely by identifying effective courses of action in simulated air combat with heterogeneous agents.

Method: Uses a two-level hierarchical approach: low-level policies for individual unit control and a high-level commander for mission-aligned macro commands. Training involves a curriculum for low-level policies and mission targets for the high-level commander.

Result: Empirical validation confirms the framework's effectiveness in handling complex multi-agent air combat scenarios.

Conclusion: The hierarchical structure successfully addresses challenges in multi-agent reinforcement learning for air combat, enabling efficient training and mission success.

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [128] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/pdf/2505.08988)
*Montaser Mohammedalamen, Michael Bowling*

Main category: cs.AI

TL;DR: This paper explores Monitored Markov Decision Processes (Mon-MDPs) with function approximation, addressing challenges in reward generalization and proposing a solution to mitigate overgeneralization.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often lack observable rewards, modeled as Mon-MDPs, but prior work is limited to simple cases. This work aims to bridge the gap between theory and practical applications.

Method: The study combines function approximation with a learned reward model to generalize from monitored to unmonitored states, proposing cautious policy optimization to address overgeneralization.

Result: The approach achieves near-optimal policies in unsolvable environments but identifies overgeneralization as a critical limitation.

Conclusion: The work advances Mon-MDP theory towards real-world applicability, highlighting the need for cautious reward extrapolation.

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [129] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/pdf/2505.09012)
*Bo Meng, Chenghao Xu, Yongli Zhu*

Main category: cs.AI

TL;DR: The paper addresses multi-stage cascading failures in power grids using reinforcement learning, validated on IEEE systems.


<details>
  <summary>Details</summary>
Motivation: Existing mitigation strategies overlook multi-stage cascading failures, which can cause severe disruptions.

Method: Treats the problem as a reinforcement learning task, using deterministic policy gradient for continuous actions.

Result: Effectiveness is validated on IEEE 14-bus and 118-bus systems.

Conclusion: The proposed approach successfully mitigates multi-stage cascading failures.

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [130] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/pdf/2505.09396)
*Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis*

Main category: cs.AI

TL;DR: The paper explores how LLM-based agents compare to human strategic reasoning in game-theoretic settings, testing three agent designs and finding that human-inspired structures improve alignment, but complexity doesn't linearly correlate with human-likeness.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has shifted AI research toward agentic systems, raising questions about their ability to replicate human strategic reasoning, especially in game-theoretic contexts.

Method: Three agent designs were evaluated: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Guessing games were used as a testbed, with obfuscated scenarios to assess generalization.

Result: Human-inspired cognitive structures improved LLM agents' alignment with human strategic behavior, but the relationship between design complexity and human-likeness was non-linear, depending on LLM capabilities.

Conclusion: While architectural enhancements can improve LLM agents' human-likeness, there are limits to simple augmentation, emphasizing the role of underlying LLM capabilities.

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [131] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/pdf/2505.09024)
*Aaron Baughman, Rahul Agarwal, Eduardo Morales, Gozde Akay*

Main category: cs.AI

TL;DR: Meta-prompting aligns LLM outputs with human expectations using agentic reinforcement learning, improving content quality for live events like the US Open 2024.


<details>
  <summary>Details</summary>
Motivation: To solve the Theory of Mind (ToM) alignment problem by ensuring AI-generated text matches human mental expectations.

Method: Uses an LLM as a Judge (LLMaaJ) to teach another LLM via in-context learning, optimizing text traits like factualness and novelty in a Hilbert vector space.

Result: Achieved 53.8% alignment with human expectations, improving content quality and coverage of tennis action.

Conclusion: The method successfully enhances AI-human collaboration in content creation, with applications in sports and entertainment.

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [132] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/pdf/2505.09029)
*Hazim Alzorgan, Abolfazl Razi*

Main category: cs.AI

TL;DR: MCBS combines beam search and Monte Carlo rollouts with TD3 to improve exploration and policy convergence, outperforming baselines like TD3, SAC, PPO, and A2C in continuous-control tasks.


<details>
  <summary>Details</summary>
Motivation: Basic noise-based exploration in actor-critic methods like TD3 can lead to suboptimal policy convergence, prompting the need for better exploration strategies.

Method: MCBS generates candidate actions around the policy's output and evaluates them via short-horizon rollouts, enhancing action selection.

Result: MCBS shows improved sample efficiency and performance, achieving 90% of max reward in 200k timesteps vs. 400k for the next best method.

Conclusion: MCBS enhances policy learning through structured look-ahead search while maintaining computational efficiency, with adaptive strategies for complex tasks.

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [133] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/pdf/2505.09031)
*Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy*

Main category: cs.AI

TL;DR: Combining CoT with RAG and self-verification reduces LLM hallucinations, improving factual accuracy and coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing the hallucination problem in LLMs for complex tasks by enhancing CoT with retrieval and self-checking methods.

Method: Evaluates CoT, CoT+RAG, self-consistency, and self-verification techniques to reduce hallucinations.

Result: Identifies the most effective approach for minimizing hallucinations while maintaining reasoning quality.

Conclusion: Combining CoT with RAG and self-verification is robust for reducing hallucinations in LLMs.

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [134] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/pdf/2505.09114)
*Minh Hoang Nguyen, Linh Le Pham Van, Thommen George Karimpanal, Sunil Gupta, Hung Le*

Main category: cs.AI

TL;DR: CRDT improves Decision Transformers by using counterfactual reasoning to handle suboptimal data and enhance decision-making in unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional Decision Transformers struggle with suboptimal or limited offline data, hindering performance. CRDT addresses this by leveraging counterfactual reasoning.

Method: CRDT generates and uses counterfactual experiences to enhance reasoning and decision-making without architectural changes.

Result: CRDT outperforms conventional DT in Atari and D4RL benchmarks, especially in limited-data and altered-dynamics scenarios, and enables trajectory stitching.

Conclusion: Counterfactual reasoning significantly boosts DT performance and generalization, showcasing its potential in reinforcement learning.

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [135] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/pdf/2505.09289)
*Pedro M. P. Curvo, Mara Dragomir, Salvador Torpes, Mohammadmahdi Rahimi*

Main category: cs.AI

TL;DR: The study validates and extends GovSim, a framework for assessing LLMs' cooperative decision-making, confirming large models like GPT-4-turbo outperform smaller ones. It explores new models, scenarios, and languages, showing adaptability and influence in heterogeneous systems.


<details>
  <summary>Details</summary>
Motivation: To validate and expand on GovSim's findings, assessing LLMs' cooperative abilities and testing their adaptability in diverse settings.

Method: Replicated key experiments, evaluated additional models (e.g., DeepSeek-V3, GPT-4o-mini), and introduced new scenarios (heterogeneous multi-agent systems, Japanese instructions, inverse environments).

Result: Large models achieve sustainable cooperation regardless of the universalization principle, while smaller models fail without it. The framework adapts to new models, languages, and scenarios, with high-performing models influencing lower-performing ones.

Conclusion: The study confirms GovSim's applicability to diverse cooperative tasks, highlighting the adaptability of LLMs and their potential for efficient cooperative AI systems.

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [136] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/pdf/2505.09341)
*EvÅ¾en Wybitul*

Main category: cs.AI

TL;DR: A framework for AI safety uses verified credentials and risk categories to control access, balancing utility and safety.


<details>
  <summary>Details</summary>
Motivation: Address the dual-use dilemma in AI safety, where requests can be harmless or harmful based on context.

Method: Proposes a conceptual framework with verified user credentials and risk-based classifiers, using gated expert modules for efficient risk detection.

Result: Enables granular governance: verified users access specialized knowledge, adversaries are blocked.

Conclusion: Reconciles AI utility with robust safety, addressing the dual-use dilemma.

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [137] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/pdf/2505.09412)
*Paul Kobialka, Lina Gerlach, Francesco Leofante, Erika ÃbrahÃ¡m, Silvia Lizeth Tapia Tarifa, Einar Broch Johnsen*

Main category: cs.AI

TL;DR: The paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to address sequential decision-making, focusing on minimal strategy changes to avoid undesired outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual methods in AI are limited to one-step decision-making, leaving a gap for sequential tasks like MDPs.

Method: The approach encodes counterfactual strategies as non-linear optimization problems and extends this to synthesize diverse strategies.

Result: Evaluation on four real-world datasets confirms the method's practical viability for sequential decision-making.

Conclusion: The paper successfully bridges the gap in counterfactual methods for MDPs, offering a viable solution for sequential tasks.

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [138] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/pdf/2505.09518)
*Maris F. L. Galesloot, Roman Andriushchenko, Milan ÄeÅ¡ka, Sebastian Junges, Nils Jansen*

Main category: cs.AI

TL;DR: The paper introduces HM-POMDPs for robust decision-making under uncertainty, combining formal verification and subgradient ascent to compute policies that generalize well and scale to large model sets.


<details>
  <summary>Details</summary>
Motivation: Optimal POMDP policies may lack robustness against environmental perturbations. HM-POMDPs address this by considering multiple potential models, ensuring policies perform well across all.

Method: Combines deductive formal verification (for worst-case POMDP identification) and subgradient ascent (for policy optimization).

Result: Produces more robust policies that generalize better to unseen POMDPs and scales to over 100,000 environments.

Conclusion: The approach effectively enhances robustness and scalability in HM-POMDPs, outperforming baselines.

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [139] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/pdf/2505.09614)
*Anthony GX-Chen, Dongyan Lin, Mandana Samiei, Doina Precup, Blake A. Richards, Rob Fergus, Kenneth Marino*

Main category: cs.AI

TL;DR: LMs struggle with conjunctive causal relationships, showing a 'disjunctive bias' similar to humans. A test-time sampling method improves their causal reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess LMs' ability to explore and infer causal relationships, crucial for robust decision-making.

Method: Using the 'Blicket Test' paradigm, LMs are evaluated on causal inference tasks. Performance is analyzed across models and prompting strategies.

Result: LMs exhibit a disjunctive bias, struggling with conjunctive relationships. This mirrors human adult reasoning. A sampling method reduces this bias.

Conclusion: LMs inherit human-like reasoning biases but can be improved with targeted methods, advancing their causal reasoning capabilities.

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [140] [An Analytical Emotion Framework of Rumour Threads on Social Media](https://arxiv.org/pdf/2502.16560)
*Rui Xing, Boyang Sun, Kun Zhang, Preslav Nakov, Timothy Baldwin, Jey Han Lau*

Main category: cs.AI

TL;DR: The paper analyzes the role of emotions in rumor vs. non-rumor threads in social media, revealing that rumors trigger more negative emotions and spread negativity, while non-rumors evoke positivity.


<details>
  <summary>Details</summary>
Motivation: To understand the emotional dynamics in rumor development, addressing gaps in literature that overlook comparative differences between rumors and non-rumors.

Method: A comprehensive analytical emotion framework with multi-aspect emotion detection, applied to existing rumor datasets for correlation and causal analysis.

Result: Rumors trigger negative emotions (anger, fear, pessimism), while non-rumors evoke positive ones. Emotions are contagious, with surprise bridging rumors and other emotions.

Conclusion: Emotions play a key role in rumor spread, with negativity linked to rumors and positivity to non-rumors, offering insights for better rumor management.

Abstract: Rumours in online social media pose significant risks to modern society,
motivating the need for better understanding of how they develop. We focus
specifically on the interface between emotion and rumours in threaded
discourses, building on the surprisingly sparse literature on the topic which
has largely focused on single aspect of emotions within the original rumour
posts themselves, and largely overlooked the comparative differences between
rumours and non-rumours. In this work, we take one step further to provide a
comprehensive analytical emotion framework with multi-aspect emotion detection,
contrasting rumour and non-rumour threads and provide both correlation and
causal analysis of emotions. We applied our framework on existing widely-used
rumour datasets to further understand the emotion dynamics in online social
media threads. Our framework reveals that rumours trigger more negative
emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive
ones. Emotions are contagious, rumours spread negativity, non-rumours spread
positivity. Causal analysis shows surprise bridges rumours and other emotions;
pessimism comes from sadness and fear, while optimism arises from joy and love.

</details>


### [141] [Learning to Be Cautious](https://arxiv.org/pdf/2110.15907)
*Montaser Mohammedalamen, Dustin Morrill, Alexander Sieusahai, Yash Satsangi, Michael Bowling*

Main category: cs.AI

TL;DR: The paper introduces an algorithm for reinforcement learning agents to autonomously learn cautious behavior in novel situations, avoiding the need for task-specific safety tuning.


<details>
  <summary>Details</summary>
Motivation: Current methods require embedding task-specific safety information, which is error-prone and burdensome. The goal is to develop agents that learn cautious behavior independently.

Method: The algorithm characterizes reward function uncertainty using a neural network ensemble and constructs robust policies via k-of-N counterfactual regret minimization (CFR).

Result: The system successfully learns cautious behavior in increasingly non-obvious tasks without task-specific safety tuning.

Conclusion: The approach demonstrates that autonomous learning of cautious behavior is feasible, offering a scalable and generalizable solution.

Abstract: A key challenge in the field of reinforcement learning is to develop agents
that behave cautiously in novel situations. It is generally impossible to
anticipate all situations that an autonomous system may face or what behavior
would best avoid bad outcomes. An agent that can learn to be cautious would
overcome this challenge by discovering for itself when and how to behave
cautiously. In contrast, current approaches typically embed task-specific
safety information or explicit cautious behaviors into the system, which is
error-prone and imposes extra burdens on practitioners. In this paper, we
present both a sequence of tasks where cautious behavior becomes increasingly
non-obvious, as well as an algorithm to demonstrate that it is possible for a
system to learn to be cautious. The essential features of our algorithm are
that it characterizes reward function uncertainty without task-specific safety
information and uses this uncertainty to construct a robust policy.
Specifically, we construct robust policies with a k-of-N counterfactual regret
minimization (CFR) subroutine given learned reward function uncertainty
represented by a neural network ensemble. These policies exhibit caution in
each of our tasks without any task-specific safety tuning.

</details>


### [142] [Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases](https://arxiv.org/pdf/2306.09138)
*Riccardo Zese, Evelina Lamma, Fabrizio Riguzzi*

Main category: cs.AI

TL;DR: The paper addresses inconsistency in Description Logics Knowledge Bases (KBs) by using the probabilistic semantics DISPONTE, enabling queries even with inconsistent KBs. It compares this approach to repair semantics.


<details>
  <summary>Details</summary>
Motivation: Inconsistency in KBs arises from diverse, dynamic Semantic Web sources, and classical reasoning fails to handle it.

Method: Uses DISPONTE probabilistic semantics, implemented in TRILL and BUNDLE reasoners, and compares it to repair semantics.

Result: Empirical tests validate the approach, showing it works with inconsistent KBs.

Conclusion: DISPONTE offers a viable alternative to classical debugging, allowing reasoning with inconsistent KBs.

Abstract: The necessity to manage inconsistency in Description Logics Knowledge Bases
(KBs) has come to the fore with the increasing importance gained by the
Semantic Web, where information comes from different sources that constantly
change their content and may contain contradictory descriptions when considered
either alone or together. Classical reasoning algorithms do not handle
inconsistent KBs, forcing the debugging of the KB in order to remove the
inconsistency. In this paper, we exploit an existing probabilistic semantics
called DISPONTE to overcome this problem and allow queries also in case of
inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE
and empirically tested the validity of our proposal. Moreover, we formally
compare the presented approach to that of the repair semantics, one of the most
established semantics when considering DL reasoning tasks.

</details>


### [143] [Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2411.04867)
*Satchit Chatterji, Erman Acar*

Main category: cs.AI

TL;DR: The paper introduces Shielded Multi-Agent Reinforcement Learning (SMARL), extending Probabilistic Logic Shields (PLS) to multi-agent settings for safer, norm-compliant outcomes.


<details>
  <summary>Details</summary>
Motivation: Addressing the unexplored generalizability of PLS to multi-agent RL, aiming to enhance safety and cooperation in decentralized environments.

Method: Proposes SMARL with (1) Probabilistic Logic Temporal Difference (PLTD) for Q-learning, (2) a shielded PPO method with safety guarantees, and (3) evaluation in game-theoretic benchmarks.

Result: Demonstrates fewer constraint violations and improved cooperation under normative constraints.

Conclusion: SMARL is effective for equilibrium selection, advancing safer, socially aligned multi-agent systems.

Abstract: Safe reinforcement learning (RL) is crucial for real-world applications, and
multi-agent interactions introduce additional safety challenges. While
Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce
safety in single-agent RL, their generalizability to multi-agent settings
remains unexplored. In this paper, we address this gap by conducting extensive
analyses of PLS within decentralized, multi-agent environments, and in doing
so, propose Shielded Multi-Agent Reinforcement Learning (SMARL) as a general
framework for steering MARL towards norm-compliant outcomes. Our key
contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD)
update for shielded, independent Q-learning, which incorporates probabilistic
constraints directly into the value update process; (2) a probabilistic logic
policy gradient method for shielded PPO with formal safety guarantees for MARL;
and (3) comprehensive evaluation across symmetric and asymmetrically shielded
$n$-player game-theoretic benchmarks, demonstrating fewer constraint violations
and significantly better cooperation under normative constraints. These results
position SMARL as an effective mechanism for equilibrium selection, paving the
way toward safer, socially aligned multi-agent systems.

</details>


### [144] [PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment](https://arxiv.org/pdf/2411.11681)
*Jiawei Li, Xinyue Liang, Junlong Zhang, Yizhe Yang, Chong Feng, Yang Gao*

Main category: cs.AI

TL;DR: PSPO-WRS, a novel process supervision method, improves reasoning in large language models by using nonlinear rewards and adjusted Weibull distribution, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current process supervision methods are ineffective, leading to logical errors and redundant reasoning in large language models.

Method: Proposes PSPO*, a paradigm for reward model training and policy optimization, and PSPO-WRS, which uses nonlinear rewards and Weibull distribution for reward shaping.

Result: PSPO-WRS outperforms mainstream models on six mathematical reasoning datasets.

Conclusion: Effective process supervision requires nonlinear rewards and consideration of reasoning chain length, as demonstrated by PSPO-WRS.

Abstract: Process supervision enhances the performance of large language models in
reasoning tasks by providing feedback at each step of chain-of-thought
reasoning. However, due to the lack of effective process supervision methods,
even advanced large language models are prone to logical errors and redundant
reasoning. We claim that the effectiveness of process supervision significantly
depends on both the accuracy and the length of reasoning chains. Moreover, we
identify that these factors exhibit a nonlinear relationship with the overall
reward score of the reasoning process. Inspired by these insights, we propose a
novel process supervision paradigm, PSPO*, which systematically outlines the
workflow from reward model training to policy optimization, and highlights the
importance of nonlinear rewards in process supervision. Based on PSPO*, we
develop the PSPO-WRS, which considers the number of reasoning steps in
determining reward scores and utilizes an adjusted Weibull distribution for
nonlinear reward shaping. Experimental results on six mathematical reasoning
datasets demonstrate that PSPO-WRS consistently outperforms current mainstream
models.

</details>


### [145] [Deontic Temporal Logic for Formal Verification of AI Ethics](https://arxiv.org/pdf/2501.05765)
*Priya T. V., Shrisha Rao*

Main category: cs.AI

TL;DR: The paper proposes a deontic logic-based formalization to define and verify ethical behavior in AI systems, focusing on fairness and explainability. It tests this on COMPAS and loan prediction systems, revealing ethical shortcomings.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for ethical AI behavior, the paper explores formal methods to specify and verify ethical compliance in AI systems.

Method: Uses deontic logic with temporal operators to formalize ethical requirements, applying automated theorem proving to real-world AI systems (COMPAS and loan prediction).

Result: Formal verification shows both systems fail key ethical properties, highlighting the method's effectiveness in identifying ethical issues.

Conclusion: The proposed formalization is effective for evaluating AI ethics, revealing real-world systems' ethical shortcomings.

Abstract: Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst
their increasing ubiquity and influence is a major concern the world over. The
use of formal methods in AI ethics is a possible crucial approach for
specifying and verifying the ethical behavior of AI systems. This paper
proposes a formalization based on deontic logic to define and evaluate the
ethical behavior of AI systems, focusing on system-level specifications,
contributing to this important goal. It introduces axioms and theorems to
capture ethical requirements related to fairness and explainability. The
formalization incorporates temporal operators to reason about the ethical
behavior of AI systems over time. The authors evaluate the effectiveness of
this formalization by assessing the ethics of the real-world COMPAS and loan
prediction AI systems. Various ethical properties of the COMPAS and loan
prediction systems are encoded using deontic logical formulas, allowing the use
of an automated theorem prover to verify whether these systems satisfy the
defined properties. The formal verification reveals that both systems fail to
fulfill certain key ethical properties related to fairness and
non-discrimination, demonstrating the effectiveness of the proposed
formalization in identifying potential ethical issues in real-world AI
applications.

</details>


### [146] [Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming](https://arxiv.org/pdf/2503.16371)
*Minori Narita, Ryo Kuroiwa, J. Christopher Beck*

Main category: cs.AI

TL;DR: Using RL to guide DIDP improves performance over standard DIDP and greedy heuristics.


<details>
  <summary>Details</summary>
Motivation: Combine RL's heuristic learning with DIDP's dynamic programming for better combinatorial optimization.

Method: Developed value-based (DQN) and policy-based (PPO) RL approaches to guide DIDP search.

Result: RL guidance outperforms standard DIDP and greedy heuristics in node expansions and runtime.

Conclusion: RL-based guidance enhances DIDP, proving effective in most benchmark domains.

Abstract: Domain-Independent Dynamic Programming (DIDP) is a state-space search
paradigm based on dynamic programming for combinatorial optimization. In its
current implementation, DIDP guides the search using user-defined dual bounds.
Reinforcement learning (RL) is increasingly being applied to combinatorial
optimization problems and shares several key structures with DP, being
represented by the Bellman equation and state-based transition systems. We
propose using reinforcement learning to obtain a heuristic function to guide
the search in DIDP. We develop two RL-based guidance approaches: value-based
guidance using Deep Q-Networks and policy-based guidance using Proximal Policy
Optimization. Our experiments indicate that RL-based guidance significantly
outperforms standard DIDP and problem-specific greedy heuristics with the same
number of node expansions. Further, despite longer node evaluation times, RL
guidance achieves better run-time performance than standard DIDP on three of
four benchmark domains.

</details>


### [147] [AdaWorld: Learning Adaptable World Models with Latent Actions](https://arxiv.org/pdf/2503.18938)
*Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan*

Main category: cs.AI

TL;DR: AdaWorld is a world model learning approach that uses self-supervised latent actions for efficient adaptation to novel environments with limited data.


<details>
  <summary>Details</summary>
Motivation: Existing world models require extensive action-labeled data and costly training, limiting adaptability to new environments.

Method: AdaWorld extracts latent actions from videos self-supervisedly and conditions an autoregressive world model on these actions.

Result: AdaWorld outperforms in simulation quality and visual planning across multiple environments.

Conclusion: AdaWorld enables efficient adaptation and transfer of world models with limited interactions.

Abstract: World models aim to learn action-controlled future prediction and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this limitation, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.

</details>


### [148] [UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning](https://arxiv.org/pdf/2503.21620)
*Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, Hongsheng Li*

Main category: cs.AI

TL;DR: UI-R1 is a framework using rule-based RL to enhance MLLMs for GUI tasks, showing significant accuracy improvements over base models and competitive performance against larger models.


<details>
  <summary>Details</summary>
Motivation: The under-explored application of rule-based RL in multi-modal domains, especially GUI agent tasks, motivates the development of UI-R1.

Method: UI-R1 employs rule-based action rewards and policy-based algorithms like GRPO, trained on a curated dataset of 136 challenging tasks.

Result: UI-R1-3B outperforms the base model with 22.1%, 6.0%, and 12.7% accuracy gains on benchmarks and competes with larger models.

Conclusion: Rule-based RL holds promise for advancing GUI understanding, with UI-R1 paving the way for future research in this domain.

Abstract: The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities
in LLMs through reinforcement learning (RL) with rule-based rewards. Despite
its success in language models, its application in multi-modal domains,
particularly in graphic user interface (GUI) agent tasks, remains
under-explored. To address this issue, we propose UI-R1, the first framework to
explore how rule-based RL can enhance the reasoning capabilities of multimodal
large language models (MLLMs) for GUI action prediction tasks. Specifically,
UI-R1 introduces a novel rule-based action reward, enabling model optimization
via policy-based algorithms such as Group Relative Policy Optimization (GRPO).
For efficient training, we curate a small yet high-quality dataset of 136
challenging tasks, encompassing five common action types on mobile devices.
Experimental results demonstrate that our proposed UI-R1-3B achieves
significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both
in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of
22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.
Furthermore, UI-R1-3B delivers competitive performance compared to larger
models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K
samples. We additionally develop an optimized version, UI-R1-E-3B, which
significantly improves both grounding efficiency and accuracy. These results
underscore the potential of rule-based reinforcement learning to advance GUI
understanding and control, paving the way for future research in this domain.
Code website: https://github.com/lll6gg/UI-R1.

</details>


### [149] [SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance](https://arxiv.org/pdf/2505.02306)
*Junfeng Jiao, Jihyung Park, Yiming Xu, Kristen Sussman, Lucy Atkinson*

Main category: cs.AI

TL;DR: SafeMate is an AI assistant designed to bridge the gap between institutional emergency knowledge and public accessibility, providing context-aware guidance during crises.


<details>
  <summary>Details</summary>
Motivation: Traditional emergency decision support systems are not user-friendly for non-experts, creating a barrier in emergency preparedness and response.

Method: SafeMate uses the Model Context Protocol (MCP) and FAISS with cosine similarity for dynamic document retrieval, checklist generation, and summarization.

Result: The system delivers accurate, context-aware guidance to general users in emergency scenarios.

Conclusion: SafeMate improves public accessibility to emergency protocols, enhancing preparedness and response.

Abstract: Despite the abundance of public safety documents and emergency protocols,
most individuals remain ill-equipped to interpret and act on such information
during crises. Traditional emergency decision support systems (EDSS) are
designed for professionals and rely heavily on static documents like PDFs or
SOPs, which are difficult for non-experts to navigate under stress. This gap
between institutional knowledge and public accessibility poses a critical
barrier to effective emergency preparedness and response. We introduce
SafeMate, a retrieval-augmented AI assistant that delivers accurate,
context-aware guidance to general users in both preparedness and active
emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate
dynamically routes user queries to tools for document retrieval, checklist
generation, and structured summarization. It uses FAISS with cosine similarity
to identify relevant content from trusted sources.

</details>


### [150] [CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging](https://arxiv.org/pdf/2505.06977)
*Wenju Sun, Qingyong Li, Yangli-ao Geng, Boyang Li*

Main category: cs.AI

TL;DR: CAT Merging is a training-free framework that trims conflict-prone components in task vectors to improve multi-task model merging.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Task Arithmetic suffer from knowledge conflicts during model merging, causing performance degradation.

Method: CAT Merging selectively trims conflict-prone components using strategies like projection for linear weights and masking for normalization layers.

Result: Achieves average accuracy improvements of 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

Conclusion: CAT Merging effectively suppresses knowledge conflicts, enhancing multi-task model merging performance.

Abstract: Multi-task model merging offers a promising paradigm for integrating multiple
expert models into a unified model without additional training. Existing
state-of-the-art techniques, such as Task Arithmetic and its variants, merge
models by accumulating task vectors -- the parameter differences between
pretrained and finetuned models. However, task vector accumulation is often
hindered by knowledge conflicts, leading to performance degradation. To address
this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel
training-free framework that selectively trims conflict-prone components from
the task vectors. CAT Merging introduces several parameter-specific strategies,
including projection for linear weights and masking for scaling and shifting
parameters in normalization layers. Extensive experiments on vision, language,
and vision-language tasks demonstrate that CAT Merging effectively suppresses
knowledge conflicts, achieving average accuracy improvements of up to 2.5%
(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

</details>


### [151] [RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models](https://arxiv.org/pdf/2505.07089)
*Hanzheng Dai, Yuanliang Li, Zhibo Zhang, Jun Yan*

Main category: cs.AI

TL;DR: RefPentester, a knowledge-informed self-reflective AutoPT framework, outperforms baseline GPT-4o by 16.7% in identifying vulnerabilities, addressing limitations like imbalanced knowledge and lack of learning from failures.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based AutoPT frameworks underperform due to imbalanced knowledge, short-sighted planning, and hallucinations, and lack mechanisms to learn from failures.

Method: Proposes RefPentester, a framework integrating a seven-state Stage Machine to guide PT stages, select tactics, and learn from failures.

Result: RefPentester outperforms GPT-4o by 16.7% in revealing credentials and shows superior success rates in PT stage transitions.

Conclusion: RefPentester effectively addresses AutoPT limitations, enhancing performance and adaptability in penetration testing.

Abstract: Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks
often underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sighted planning
in the planning process, and hallucinations during command generation. In
addition, the penetration testing (PT) process, with its trial-and-error
nature, is limited by existing frameworks that lack mechanisms to learn from
previous failed operations, restricting adaptive improvement of PT strategies.
To address these limitations, we propose a knowledge-informed self-reflective
PT framework powered by LLMs, called RefPentester, which is an AutoPT framework
designed to assist human operators in identifying the current stage of the PT
process, selecting appropriate tactic and technique for the stage, choosing
suggested action, providing step-by-step operational guidance, and learning
from previous failed operations. We also modeled the PT process as a
seven-state Stage Machine to integrate the proposed framework effectively. The
evaluation shows that RefPentester can successfully reveal credentials on Hack
The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across
PT stages, RefPentester also demonstrates superior success rates on PT stage
transitions.

</details>


### [152] [Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving](https://arxiv.org/pdf/2505.07773)
*Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang*

Main category: cs.AI

TL;DR: ZeroTIR trains LLMs with RL to autonomously generate and execute Python code for math tasks, showing predictable scaling in performance metrics like accuracy and code execution frequency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with precise mathematical reasoning, and understanding how RL can enable autonomous tool use (like code execution) is crucial.

Method: ZeroTIR uses RL from outcome-based rewards to train LLMs to generate and execute Python code without supervised examples. A decoupled code execution environment is implemented.

Result: Increased training steps correlate with higher code execution frequency, longer responses, and improved task accuracy. ZeroTIR outperforms non-tool baselines on math benchmarks.

Conclusion: ZeroTIR demonstrates a quantifiable relationship between RL training and tool-augmented reasoning, providing a benchmark for future studies.

Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [153] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/pdf/2505.09091)
*Zeeshan Ahmad, Shudi Bao, Meng Chen*

Main category: cs.SD

TL;DR: DPN-GAN introduces a novel GAN architecture with deformable periodic networks to improve audio generation quality by addressing mel-spectrogram limitations and mode collapse.


<details>
  <summary>Details</summary>
Motivation: Existing GANs for audio generation rely on bandwidth-limited mel-spectrograms, leading to resolution constraints and mode collapse.

Method: Proposes DPN-GAN with kernel-based periodic ReLU activation and deformable convolution for multi-resolution generation and enhanced discriminator.

Result: DPN-GAN outperforms state-of-the-art models on speech and music tasks, showing robustness on noisy and out-of-distribution data.

Conclusion: DPN-GAN advances audio generation by improving fidelity and adaptability, demonstrating superior performance across diverse datasets.

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [154] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/pdf/2505.09304)
*Luciano Sebastian Martinez-Rau, Quynh Nguyen Phuong Vu, Yuxuan Zhang, Bengt Oelmann, Sebastian Bader*

Main category: cs.SD

TL;DR: A lightweight method for continuous noise adaptation in keyword spotting (KWS) systems improves performance under real-world conditions with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Standard KWS systems degrade in real-world conditions; resilient systems are needed for dynamic adaptation but face deployment challenges on resource-constrained devices.

Method: Proposes a low-computation approach for noise adaptation of pretrained KWS neural networks using 1-shot learning and one epoch.

Result: Adapted models outperformed pretrained ones, especially at low SNRs (â¤18 dB), with accuracy improvements of 4.9% to 46.0%.

Conclusion: The method is effective and lightweight, suitable for resource-constrained devices.

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [155] [SingNet: Towards a Large-Scale, Diverse, and In-the-Wild Singing Voice Dataset](https://arxiv.org/pdf/2505.09325)
*Yicheng Gu, Chaoren Wang, Junan Zhang, Xueyao Zhang, Zihao Fang, Haorui He, Zhizheng Wu*

Main category: cs.SD

TL;DR: SingNet is a large-scale, diverse singing voice dataset addressing the bottleneck in singing voice applications like SVS and SVC. It includes 3000 hours of data, pre-trained SOTA models, and benchmark results.


<details>
  <summary>Details</summary>
Motivation: The lack of a diverse, large-scale singing voice dataset hinders progress in applications like Singing Voice Synthesis and Conversion.

Method: A data processing pipeline extracts training data from online sample packs and songs, forming 3000 hours of diverse singing voices. Pre-trained models on Wav2vec2, BigVGAN, and NSF-HiFiGAN are provided.

Result: Benchmark experiments on ALT, Neural Vocoder, and SVC demonstrate the dataset's effectiveness.

Conclusion: SingNet fills a critical gap in singing voice research, offering a valuable resource for future work.

Abstract: The lack of a publicly-available large-scale and diverse dataset has long
been a significant bottleneck for singing voice applications like Singing Voice
Synthesis (SVS) and Singing Voice Conversion (SVC). To tackle this problem, we
present SingNet, an extensive, diverse, and in-the-wild singing voice dataset.
Specifically, we propose a data processing pipeline to extract ready-to-use
training data from sample packs and songs on the internet, forming 3000 hours
of singing voices in various languages and styles. Furthermore, to facilitate
the use and demonstrate the effectiveness of SingNet, we pre-train and
open-source various state-of-the-art (SOTA) models on Wav2vec2, BigVGAN, and
NSF-HiFiGAN based on our collected singing voice data. We also conduct
benchmark experiments on Automatic Lyric Transcription (ALT), Neural Vocoder,
and Singing Voice Conversion (SVC). Audio demos are available at:
https://singnet-dataset.github.io/.

</details>


### [156] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/pdf/2505.09382)
*Zhengyan Sheng, Jinghao He, Liping Chen, Kong Aik Lee, Zhen-Hua Ling*

Main category: cs.SD

TL;DR: The VtaD 2025 challenge aims to explain voice timbre attributes using sensory descriptors by comparing two voices, culminating in a proposal at NCMMSC2025.


<details>
  <summary>Details</summary>
Motivation: To verbalize and compare human impressions of voice timbre using sensory descriptors like bright, coarse, soft, etc.

Method: Comparative analysis of voice timbre intensity within specific descriptor dimensions.

Result: A special proposal will be presented at NCMMSC2025 in Zhenjiang, China.

Conclusion: The challenge advances understanding of voice timbre through structured comparison and sensory descriptors.

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [157] [Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)
*Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian Luan*

Main category: cs.SD

TL;DR: RL enhances audio understanding in LLMs using GRPO, achieving 64.5% accuracy on MMAU Test-mini with minimal data.


<details>
  <summary>Details</summary>
Motivation: Audio modality is overlooked in RL-based multimodal tasks, prompting exploration in audio question answering (AQA).

Method: Applied GRPO algorithm to Qwen2-Audio-7B-Instruct for AQA, testing with 38k post-training samples.

Result: Achieved state-of-the-art performance (64.5% accuracy), outperforming SFT with minimal data.

Conclusion: RL is effective for LALMs, but further research is needed for deep reasoning and human-level performance.

Abstract: Recently, reinforcement learning (RL) has been shown to greatly enhance the
reasoning capabilities of large language models (LLMs), and RL-based approaches
have been progressively applied to visual multimodal tasks. However, the audio
modality has largely been overlooked in these developments. Thus, we conduct a
series of RL explorations in audio understanding and reasoning, specifically
focusing on the audio question answering (AQA) task. We leverage the group
relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and
our experiments demonstrated state-of-the-art performance on the MMAU Test-mini
benchmark, achieving an accuracy rate of 64.5%. The main findings in this
technical report are as follows: 1) The GRPO algorithm can be effectively
applied to large audio language models (LALMs), even when the model has only
8.2B parameters; 2) With only 38k post-training samples, RL significantly
outperforms supervised fine-tuning (SFT), indicating that RL-based approaches
can be effective without large datasets; 3) The explicit reasoning process has
not shown significant benefits for AQA tasks, and how to efficiently utilize
deep thinking remains an open question for further research; 4) LALMs still lag
far behind humans auditory-language reasoning, suggesting that the RL-based
approaches warrant further exploration. Our project is available at
https://github.com/xiaomi-research/r1-aqa and
https://huggingface.co/mispeech/r1-aqa.

</details>


### [158] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/pdf/2505.08175)
*Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons*

Main category: cs.SD

TL;DR: ARC post-training accelerates text-to-audio diffusion models without distillation, achieving ultra-fast inference times.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio systems suffer from high latency, limiting their practicality for creative applications.

Method: ARC post-training combines relativistic adversarial formulation with a contrastive discriminator objective to improve speed and prompt adherence.

Result: The model generates 12s of 44.1kHz stereo audio in 75ms on an H100 and 7s on a mobile edge-device, setting a speed record.

Conclusion: ARC post-training is a simple yet effective method for accelerating diffusion models, enabling real-time text-to-audio applications.

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [159] [Deconstructing Jazz Piano Style Using Machine Learning](https://arxiv.org/pdf/2504.05009)
*Huw Cheston, Reuben Bance, Peter M. C. Harrison*

Main category: cs.SD

TL;DR: The paper explores computational analysis of musical style using machine learning, focusing on jazz musicians. It introduces a multi-input model for analyzing melody, harmony, rhythm, and dynamics, achieving 94% accuracy in performer identification.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine-learning insights and the interests of music practitioners and critics by leveraging computational methods to study musical style.

Method: Supervised-learning models, including a novel multi-input architecture, are trained on a curated dataset of 84 hours of jazz recordings to analyze four musical domains separately.

Result: Achieves 94% accuracy in identifying 20 iconic jazz musicians and provides insights into music theory.

Conclusion: The work advances performer identification and music theory understanding, with open-source models and a web app for style exploration.

Abstract: Artistic style has been studied for centuries, and recent advances in machine
learning create new possibilities for understanding it computationally.
However, ensuring that machine-learning models produce insights aligned with
the interests of practitioners and critics remains a significant challenge.
Here, we focus on musical style, which benefits from a rich theoretical and
mathematical analysis tradition. We train a variety of supervised-learning
models to identify 20 iconic jazz musicians across a carefully curated dataset
of 84 hours of recordings, and interpret their decision-making processes. Our
models include a novel multi-input architecture that enables four musical
domains (melody, harmony, rhythm, and dynamics) to be analysed separately.
These models enable us to address fundamental questions in music theory and
also advance the state-of-the-art in music performer identification (94%
accuracy across 20 classes). We release open-source implementations of our
models and an accompanying web application for exploring musical styles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [160] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/pdf/2505.08792)
*Michelle Nashla Turcios, Alicia E. Boyd, Angela D. R. Smith, Brittany Johnson*

Main category: cs.LG

TL;DR: The paper highlights the misuse of intersectionality in ML, proposes a framework based on foundational scholarship (Crenshaw, Combahee, Collins), and evaluates its application in ML literature.


<details>
  <summary>Details</summary>
Motivation: Address the inadequate support of ML for societal identities and experiences, leveraging intersectionality for equitable tech outcomes.

Method: Develop a framework based on foundational intersectionality scholarship to evaluate ML literature.

Result: Identifies misalignments in how intersectionality is applied in ML.

Conclusion: A proper adoption of intersectionality can lead to more equitable ML solutions.

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [161] [Onboard Optimization and Learning: A Survey](https://arxiv.org/pdf/2505.08793)
*Monirul Islam Pavel, Siyi Hu, Mahardhika Pratama, Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: A survey on onboard learning in edge AI, addressing challenges like limited resources and security, and exploring methodologies for efficiency, speed, and privacy.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, low-latency, and privacy-preserving AI on resource-constrained edge devices.

Method: Examines techniques like model optimization, inference acceleration, collaborative learning, and hardware-software co-design.

Result: Identifies strategies for efficient, scalable, and secure AI deployment at the edge.

Conclusion: Onboard learning is key for robust edge AI, with ongoing advancements in optimization and decentralization.

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [162] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/pdf/2505.08977)
*Hossein Babaei, Mel White, Sina Alemohammad, Richard G. Baraniuk*

Main category: cs.LG

TL;DR: A generalized method for building State-Space Models (SSMs) with any frame or basis, extending beyond polynomials, is introduced.


<details>
  <summary>Details</summary>
Motivation: Current SSMs are limited to a few polynomial bases, restricting their potential.

Method: Proposes SaFARi, a framework for constructing SSMs with any frame or basis, not just polynomials.

Result: Enables an infinite diversity of SSM architectures, expanding beyond HiPPO.

Conclusion: SaFARi broadens the scope of SSMs, offering flexibility and new possibilities for long-range dependent data modeling.

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [163] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/pdf/2505.08795)
*Andres Anabalon, Hugo Garces, Julio Oliva, Jose Cifuentes*

Main category: cs.LG

TL;DR: A fast algorithm embeds hierarchical structures in 3D Minkowski spacetime, encoding data correlations in causal structure. Applied to WordNet, it perfectly embeds hierarchies, suggesting discrete data has a 3D geometric representation.


<details>
  <summary>Details</summary>
Motivation: To explore whether hierarchical structures can be perfectly embedded in 3D Minkowski spacetime, leveraging causal relationships for data correlation.

Method: Uses oriented token pairs (local hierarchical signals) without global symbolic structure, applied to WordNet for embedding hierarchies. Introduces causality-based retrieval.

Result: Perfect embeddings of WordNet's mammal sub-tree and a maximal unambiguous subset (82,115 nouns) achieved. Hierarchies are codified in geometry, reproducing ground-truth.

Conclusion: Hierarchical meaning is geometric, with deep connections to general relativity. Discrete data may universally have 3D geometric representations.

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [164] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/pdf/2505.08803)
*Zizhao Hu, Mohammad Rostami, Jesse Thomason*

Main category: cs.LG

TL;DR: The paper explores model collapse in multi-modal generative systems, revealing distinct behaviors and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To understand and address model collapse in realistic multi-modal AI scenarios, beyond single-modality models.

Method: Expands synthetic data training to multi-modal systems (VLMs, diffusion models) and recursive generate-train loops.

Result: Model collapse in multi-modal systems shows unique traits (e.g., better vision-language alignment) and can be mitigated by methods like increased decoding budgets.

Conclusion: Provides insights and guidelines for reducing model collapse in self-improving multi-agent AI systems and robust synthetic datasets.

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [165] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/pdf/2505.08823)
*Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock*

Main category: cs.LG

TL;DR: The paper introduces a method to stably fine-tune full-precision LLMs into ternary (2-bit) models using RMS normalization and a gradual quantization schedule, achieving comparable accuracy without added complexity.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are costly to deploy due to their scale. While quantization reduces costs, it often degrades accuracy, especially in the ternary regime. The goal is to make ultra-low-bit inference practical without sacrificing performance.

Method: The approach involves inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule to fine-tune full-precision checkpoints into ternary LLMs.

Result: The method matches or surpasses more complex knowledge-distillation pipelines on standard benchmarks, demonstrating that careful normalization can bridge the accuracy gap between ternary and full-precision models.

Conclusion: Careful normalization and a gradual quantization schedule can make ternary LLMs practical, offering significant memory and computation savings without compromising accuracy.

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [166] [Self Rewarding Self Improving](https://arxiv.org/pdf/2505.08827)
*Toby Simonds, Kevin Lopez, Akira Yoshiyama, Dominique Garmier*

Main category: cs.LG

TL;DR: Large language models (LLMs) can self-improve by judging their own solutions without reference answers, achieving performance gains and enabling reinforcement learning in new domains.


<details>
  <summary>Details</summary>
Motivation: To explore self-improvement in LLMs without relying on human-provided solutions or rewards, leveraging the asymmetry between generating and verifying solutions.

Method: Models self-judge solutions on tasks like Countdown puzzles and MIT Integration Bee problems, using synthetic question generation and reinforcement learning.

Result: An 8% improvement over baseline with Qwen 2.5 7B, surpassing GPT-4o on integration tasks, and enabling reinforcement learning in previously challenging domains.

Conclusion: Self-judging LLMs can provide effective reward signals, enabling self-directed learning and potentially accelerating progress in data-scarce or complex domains.

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [167] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/pdf/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: The paper proposes using a linear combination of accuracy and fairness metrics to evaluate predictive algorithms, addressing trade-offs between these goals.


<details>
  <summary>Details</summary>
Motivation: The rise of powerful AI predictive algorithms highlights the tension between accuracy and fairness, with no clear guidelines for balancing these goals or aggregating diverse metrics.

Method: The paper argues for a linear combination of accuracy and fairness metrics, leveraging Harsanyi's preference aggregation theory, and applies this to the COMPAS dataset.

Result: The approach provides a formal framework for evaluating predictive algorithms when both accuracy and fairness are valued.

Conclusion: A linear combination of metrics offers a principled way to manage accuracy-fairness trade-offs, supported by theoretical and empirical analysis.

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [168] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/pdf/2505.08846)
*Felix Marti-Perez, Brigt HÃ¥vardstun, CÃ¨sar Ferri, Carlos Monserrat, Jan Arne Telle*

Main category: cs.LG

TL;DR: Metrics for evaluating simplified time series in TSC interpretability show better results than using original data, especially for seasonal, non-stationary, or low-entropy series.


<details>
  <summary>Details</summary>
Motivation: Time series data are not intuitively understandable to humans, unlike text or images, necessitating simplifications for interpretability.

Method: Introduced metrics for complexity and loyalty of simplifications, evaluated four algorithms across various TSC methods and datasets.

Result: Simplifications outperform original time series for interpretability, particularly in seasonal, non-stationary, or low-entropy cases.

Conclusion: Simplified time series enhance TSC interpretability, especially for complex datasets.

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [169] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/pdf/2505.08915)
*Jialin Mao, Itay Griniasty, Yan Sun, Mark K. Transtrum, James P. Sethna, Pratik Chaudhari*

Main category: cs.LG

TL;DR: The paper analyzes the low-dimensional "hyper-ribbon-like" manifold in deep neural network training trajectories, focusing on linear networks. It identifies key factors controlling this geometry and extends the analysis to kernel machines and SGD-trained linear models.


<details>
  <summary>Details</summary>
Motivation: To understand the common low-dimensional manifold observed in training trajectories of diverse deep networks by analytically studying linear networks.

Method: Uses dynamical systems theory to analyze the manifold's geometry, focusing on input correlation eigenvalues, initial weight-output scale, and gradient descent steps.

Result: Characterizes phase boundaries for hyper-ribbon emergence and extends findings to kernel machines and SGD-trained linear models.

Conclusion: The study provides theoretical insights into the low-dimensional training dynamics of neural networks, applicable to broader machine learning models.

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [170] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/pdf/2505.08940)
*Jeremie Blanchard, Lisa Casino, Jordan Gierschendorf*

Main category: cs.LG

TL;DR: The paper explores machine learning for exoplanetary atmospheric spectral analysis, emphasizing generalization and uncertainty estimation, but notes limitations of tabular modeling and business-driven approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of characterizing exoplanetary atmospheres using spectral data, leveraging the NeurIPS 2024 Ariel Data Challenge and ESA's Ariel mission.

Method: A data-centric approach focusing on feature extraction, signal transformation, and heteroskedastic uncertainty modeling, tested via Gaussian Log-Likelihood (GLL) scores.

Result: Uncertainty estimation improved GLL scores by 11%, but tabular modeling and feature engineering showed inherent limitations.

Conclusion: The study highlights trade-offs between simplicity, interpretability, and generalization in astrophysical data analysis, questioning the suitability of business-driven approaches for such tasks.

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [171] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/pdf/2505.08941)
*Gavin Hull, Alex Bihlo*

Main category: cs.LG

TL;DR: ForeCite is a framework using pre-trained language models to predict future citation rates of academic papers, achieving a 27-point improvement over prior methods.


<details>
  <summary>Details</summary>
Motivation: Automating research evaluation and accelerating scientific progress by predicting citation rates.

Method: Appends pre-trained causal language models with a linear head for regression, tested on 900K+ biomedical papers.

Result: Achieves a test correlation of Ï = 0.826, with consistent gains across model sizes and data volumes.

Conclusion: Sets a new state-of-the-art for forecasting research influence, enabling automated evaluation of scientific contributions.

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [172] [GPML: Graph Processing for Machine Learning](https://arxiv.org/pdf/2505.08964)
*Majed Jaber, Julien Michel, Nicolas Boutry, Pierre Parrend*

Main category: cs.LG

TL;DR: GPML transforms network traffic into graphs for advanced anomaly detection and forensic analysis in dynamic networks.


<details>
  <summary>Details</summary>
Motivation: Addressing the rise of complex cyber-threats in dynamic networks requiring advanced detection tools.

Method: Uses graph representations of network traffic, supporting community and spectral metrics extraction.

Result: Enables real-time anomaly detection and historical forensic analysis.

Conclusion: GPML provides a robust, graph-based solution for modern cybersecurity challenges.

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [173] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/pdf/2505.08982)
*Jiachen Qian, Yang Zheng*

Main category: cs.LG

TL;DR: The paper proposes an improved online prediction method for unknown linear stochastic systems using exponential forgetting to balance regression models, achieving better accuracy and a sharper regret bound.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unknown systems suffer from imbalanced regression models, leading to overfitting and degraded prediction accuracy.

Method: The approach uses exponential forgetting to balance the regression model, improving the trade-off between regression and regularization errors.

Result: The method reduces accumulation error and achieves a sharper logarithmic regret bound of O(logÂ³ N).

Conclusion: Exponential forgetting effectively balances the regression model, enhancing prediction accuracy and providing a tighter regret bound.

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [174] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/pdf/2505.09003)
*Zeki Doruk Erden, Donia Gasmi, Boi Faltings*

Main category: cs.LG

TL;DR: The paper explores using autoencoders for continual learning in reinforcement learning, enabling task recognition and knowledge retention without external signals.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of continual learning in reinforcement learning, particularly in preserving and leveraging existing information without external task-change signals.

Method: Integrates policy optimization with familiarity autoencoders in an end-to-end system to detect new tasks and match environments to prior ones.

Result: Initial results show successful continual learning without external signals, with the system recognizing and learning new tasks while retaining past knowledge.

Conclusion: The approach shows promise for continual learning in reinforcement learning by autonomously managing task recognition and knowledge retrieval.

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [175] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/pdf/2505.09011)
*Antonio Candito, Matthew D Blackledge, Richard Holbrey, Nuria Porta, Ana Ribeiro, Fabio Zugni, Luca D'Erme, Francesca Castagnoli, Alina Dragan, Ricardo Donners, Christina Messiou, Nina Tunariu, Dow-Mu Koh*

Main category: cs.LG

TL;DR: An AI-driven tool for quantifying metastatic bone disease from WB-DWI scans using a combination of deep learning models and statistical methods, achieving high accuracy and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To provide a reproducible and efficient method for quantifying metastatic bone disease from WB-DWI scans, aiding clinical decision-making for APC patients.

Method: Combines a weakly-supervised Residual U-Net for bone isolation, a statistical framework for intensity normalization, and a shallow CNN for lesion detection, followed by biomarker extraction.

Result: Achieved 0.6 Dice score, <9% and <5% differences for log-TDV and median gADC, and 80.5% accuracy in treatment response assessment.

Conclusion: The software offers reliable and fast quantification of metastatic bone disease, supporting clinical monitoring and decision-making.

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [176] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/pdf/2505.09017)
*Bizhan Alipour Pijan, Serdar Bozdag*

Main category: cs.LG

TL;DR: The paper proposes DyGSSM, a method combining GCN and GRU for dynamic graph representation learning, using HiPPO-based SSM for temporal dependency management, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to simultaneously capture global and local structures in dynamic graphs and lack effective temporal dependency management during parameter updates.

Method: DyGSSM integrates GCN for local features and GRU for global features per snapshot, using cross-attention and HiPPO-based SSM for temporal updates.

Result: Outperforms baselines in 17/20 cases across five datasets.

Conclusion: DyGSSM effectively addresses limitations in dynamic graph representation learning by combining multi-view features and HiPPO-based SSM.

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [177] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/pdf/2505.09022)
*Annan Yu, N. Benjamin Erichson*

Main category: cs.LG

TL;DR: Mamba, an improved state space model (SSM), struggles with long-range tasks despite its design. The paper analyzes its limitations and proposes $	ext{B}_2	ext{S}_6$ to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Mamba's poor performance on long-range tasks contradicts its design for long-range dependencies, prompting a need for improvement.

Method: The paper analyzes Mamba's limitations in expressiveness, inductive bias, and training stability, then introduces $	ext{B}_2	ext{S}_6$, a modified S6 unit with block-wise selective dynamics and channel-specific bias.

Result: $	ext{B}_2	ext{S}_6$ outperforms S4 and S4D on Long-Range Arena tasks while maintaining Mamba's language modeling performance.

Conclusion: The proposed $	ext{B}_2	ext{S}_6$ addresses Mamba's weaknesses, improving its universality and versatility.

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [178] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/pdf/2505.09063)
*Khalid Rafiq, Wenjing Liao, Aditya G. Nair*

Main category: cs.LG

TL;DR: Flexi-VAE is a framework for efficient single-shot forecasting of nonlinear PDEs, avoiding iterative time-stepping while ensuring accuracy. It uses neural propagators in a VAE setting, with DCP outperforming PEP for long-term predictions. Validated on PDE benchmarks, it shows significant speedups and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency of iterative time-stepping in PDE forecasting and provide a scalable, interpretable surrogate model for high-fidelity simulations.

Method: Flexi-VAE employs a neural propagator in a VAE framework, comparing DCP and PEP strategies. It uses geometric diagnostics like Jacobian spectral analysis to ensure stable latent spaces.

Result: Achieves accurate forecasts for 1D and 2D PDEs, with 50x CPU and 90x GPU speedups over baselines. DCP enhances long-term generalization.

Conclusion: Flexi-VAE is a scalable, interpretable tool for accelerating PDE-driven simulations, with potential for higher-dimensional applications.

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [179] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/pdf/2505.09436)
*Raghav Garg, Kapil Sharma, Karan Gupta*

Main category: cs.LG

TL;DR: CXMArena is a synthetic benchmark for evaluating LLMs in CXM, addressing data scarcity and realism gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realism and fail to address operational CXM tasks, hindering LLM evaluation in contact centers.

Method: Developed a scalable LLM-powered pipeline to simulate CXM entities, including knowledge articles and conversations, with controlled noise and validation.

Result: CXMArena benchmarks show low accuracy (68%) and F1 scores (0.3) for state-of-the-art models, highlighting challenges.

Conclusion: CXMArena fills a critical gap in CXM evaluation, revealing the need for advanced solutions beyond conventional methods.

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [180] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/pdf/2505.09076)
*Berkay Guler, Hamid Jafarkhani*

Main category: cs.LG

TL;DR: AdaFortiTran improves OFDM channel estimation in fast-fading and low-SNR conditions using a hybrid CNN-transformer model with adaptive priors, achieving a 6 dB MSE reduction.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of deep learning models in fast-fading and low-SNR scenarios for OFDM systems.

Method: Combines convolutional layers for local correlations and a transformer encoder for global attention, integrating SNR, delay spread, and Doppler shift as priors. Uses residual connections and final convolutional layers for refinement.

Result: Achieves up to 6 dB MSE reduction and superior robustness across varying Doppler shifts, SNRs, and delay spreads.

Conclusion: AdaFortiTran is a compact, effective solution for challenging channel estimation environments.

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [181] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/pdf/2505.09085)
*Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan*

Main category: cs.LG

TL;DR: Brain-in-the-loop supervised learning enhances DNNs' cognitive abilities using human brain signals, improving performance in complex tasks like few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Despite DNN advancements, achieving human-like cognitive abilities (e.g., abstract reasoning) remains challenging. This study explores using brain signals to bridge this gap.

Method: Utilizes brain-in-the-loop supervised learning with human brain signals to transfer conceptual structures to DNNs.

Result: Enhanced DNN comprehension of abstract concepts, improved performance in few-shot/zero-shot learning, and interpretable representations.

Conclusion: Human-in-the-loop supervision can augment DNNs' cognitive abilities, advancing toward human-like AI systems.

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [182] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/pdf/2505.09089)
*Philipp Hess, Maximilian Gelbrecht, Christof SchÃ¶tz, Michael Aich, Yu Huang, Shangshang Yang, Niklas Boers*

Main category: cs.LG

TL;DR: A time-consistency discriminator is proposed to adapt pretrained image diffusion models for realistic spatiotemporal video generation, matching VDMs in performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Training video diffusion models (VDMs) from scratch is resource-intensive, limiting their broader use. This work aims to leverage existing image diffusion models for video generation.

Method: Introduces a time-consistency discriminator to guide sampling in pretrained image diffusion models, avoiding model extensions or finetuning.

Result: Matches VDMs in temporal consistency, improves uncertainty calibration, reduces biases, and enables stable long-term climate simulations.

Conclusion: The proposed discriminator efficiently adapts image diffusion models for video tasks, offering a practical alternative to VDMs.

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [183] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/pdf/2505.09106)
*Ya Liu, Kai Yang, Yu Zhu, Keying Yang, Haibo Zhao*

Main category: cs.LG

TL;DR: The paper introduces Argus, an asynchronous algorithm for decentralized federated bilevel learning in SAGIN, addressing non-convex and non-smooth problems in time-varying networks.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized and synchronous methods are unsuitable for SAGIN due to its infrastructureless and dynamic nature, necessitating a novel asynchronous approach.

Method: The proposed Argus algorithm enables networked agents (e.g., aerial vehicles) to perform asynchronous bilevel learning in time-varying networks, avoiding delays from stragglers.

Result: Theoretical analysis covers iteration, communication, and computational complexities, with numerical experiments validating Argus's effectiveness.

Conclusion: Argus successfully addresses the challenges of decentralized federated bilevel learning in SAGIN, offering a robust solution for dynamic environments.

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [184] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/pdf/2505.09113)
*Yingrong Wang, Anpeng Wu, Baohong Li, Ziyang Xiao, Ruoxuan Xiong, Qing Han, Kun Kuang*

Main category: cs.LG

TL;DR: The paper proposes a novel framework (DSIV-CFR) to address unmeasured confounders in sequential treatment effects using instrumental variables and negative controls, achieving strong performance in dynamic systems.


<details>
  <summary>Details</summary>
Motivation: The challenge of unmeasured confounders in sequential decision-making, which biases treatment effect estimation despite advanced causal methods like transformers.

Method: Introduces DSIV-CFR, leveraging instrumental variables (IVs) and negative controls (previous outcomes) to adjust for latent confounding via a generalized moment condition.

Result: Experiments on 4 datasets show significant performance in one- and multi-step prediction, enabling optimal treatment identification.

Conclusion: DSIV-CFR effectively addresses unmeasured confounding in sequential treatments, offering a robust solution for dynamic systems.

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [185] [Fair Clustering via Alignment](https://arxiv.org/pdf/2505.09131)
*Kunwoong Kim, Jihu Lee, Sangchul Park, Yongdai Kim*

Main category: cs.LG

TL;DR: A new fair clustering algorithm, FCA, balances fairness and utility by aligning data from protected groups and optimizing cluster centers, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fair clustering algorithms suffer from suboptimal utility or instability due to complexity or approximations.

Method: FCA decomposes the fair K-means objective, alternately aligning data from protected groups and optimizing cluster centers in the aligned space.

Result: FCA achieves superior fairness-utility trade-off and near-perfect fairness without instability.

Conclusion: FCA provides a practical, high-utility solution for fair clustering with theoretical guarantees.

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [186] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/pdf/2505.09134)
*Daniel Huang*

Main category: cs.LG

TL;DR: DSoftKI extends SoftKI to handle derivative observations, improving scalability and accuracy for high-dimensional tasks like molecular force field prediction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling Gaussian Processes (GPs) for datasets with full derivative observations, especially in high-dimensional settings.

Method: Extends SoftKI's softmax interpolation to incorporate directional orientation of interpolation points, enabling scalable kernel approximation with derivatives.

Result: DSoftKI shows accuracy and scalability on synthetic benchmarks and high-dimensional molecular force field prediction (100-1000 dimensions).

Conclusion: DSoftKI advances GP methods by efficiently handling derivative observations, enabling larger-scale applications.

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [187] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/pdf/2505.09160)
*Berkay Guler, Giovanni Geraci, Hamid Jafarkhani*

Main category: cs.LG

TL;DR: The paper introduces WiMAE and ContraWiMAE, transformer-based models for self-supervised wireless channel representation, outperforming existing methods in adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised learning methods for wireless channels borrow from text/image paradigms, neglecting unique wireless constraints. This work addresses the gap.

Method: WiMAE is a transformer-based encoder-decoder pretrained on multi-antenna data. ContraWiMAE adds contrastive learning to WiMAE via noise injection and multi-task training.

Result: Both models excel in unseen scenarios, with ContraWiMAE improving linear separability and adaptability. They outperform state-of-the-art models in performance and data efficiency.

Conclusion: WiMAE and ContraWiMAE set strong baselines for future self-supervised wireless channel representation research.

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [188] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/pdf/2505.09174)
*Xinyu You, Xiang Liu, Chuan-Shen Hu, Kelin Xia, Tze Chien Sum*

Main category: cs.LG

TL;DR: A novel method, QCformer, using quotient complexes and graph neural networks, improves prediction of hybrid perovskite properties.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional GNNs in capturing periodic structures and higher-order interactions in materials like HOIPs.

Method: Proposes a quotient complex representation and QCformer, a simplex-based Transformer, for material property prediction.

Result: QCformer outperforms state-of-the-art models in predicting HOIP properties.

Conclusion: QCformer and quotient complex representation offer a powerful tool for predictive modeling of perovskite materials.

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [189] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/pdf/2505.09175)
*Mohammad Ganjirad, Mahmoud Reza Delavar, Hossein Bagheri, Mohammad Mehdi Azizi*

Main category: cs.LG

TL;DR: A framework for prioritizing urban green space development in Tehran using socio-economic, environmental, and sensitivity indices, validated by microclimate simulation.


<details>
  <summary>Details</summary>
Motivation: To address insufficient green spaces in Tehran by leveraging diverse data sources and machine learning for effective prioritization.

Method: Used Google Earth Engine, WRF model, and machine learning (XGBoost, LightGBM, RF, Extra Trees) for vegetation classification and prioritization. RF performed best (94% accuracy).

Result: RF-generated prioritization map identified critical areas. Green roofs reduced air temperature by 0.67Â°C in these areas.

Conclusion: The framework aids urban planners in developing green spaces effectively, validated by temperature reduction.

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [190] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/pdf/2505.09214)
*Zhonghao Lyu, Ming Xiao, Jie Xu, Mikael Skoglund, Marco Di Renzo*

Main category: cs.LG

TL;DR: The paper proposes a pruning-aware co-inference scheme for large AI models (LAIMs) between edge devices and servers, optimizing pruning ratio, power, and computation to minimize distortion while meeting latency and energy constraints.


<details>
  <summary>Details</summary>
Motivation: The shift to edge-based inference for low-latency, privacy-preserving AI services drives the need for resource-efficient LAIM execution in wireless networks.

Method: The scheme prunes and partitions LAIMs into on-device and on-server sub-models, analyzes distortion bounds, and formulates a joint optimization problem for pruning ratio, power, and computation. An efficient algorithm solves the non-convex problem.

Result: Simulations show the proposed design effectively bounds output distortion, balances performance-latency-energy trade-offs, and outperforms benchmark schemes like fully on-device or on-server inference.

Conclusion: The split point and joint optimization of pruning and resources are critical for optimizing LAIM co-inference in resource-limited edge environments.

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [191] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/pdf/2505.09218)
*Alexander Tyurin, Danil Sivtsov*

Main category: cs.LG

TL;DR: Birch SGD is a framework for analyzing distributed SGD methods using computation trees, unifying convergence analysis and method design. It introduces eight new methods, some with optimal complexity, and reveals trade-offs in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for analyzing and designing distributed SGD methods, simplifying convergence analysis and method development.

Method: Represent SGD methods as weighted directed trees (computation trees) and analyze their geometry to derive convergence properties.

Result: Eight new methods designed, some with optimal complexity; insights on trade-offs between iteration rate, communication efficiency, and practical performance.

Conclusion: Birch SGD unifies understanding and design of distributed SGD methods, offering a foundation for efficient asynchronous and parallel optimization.

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [192] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/pdf/2505.09239)
*Faruk Alpay*

Main category: cs.LG

TL;DR: The paper introduces a stable and convex optimization method for the Information Bottleneck (IB) problem using entropy regularization, ensuring smooth representation learning across beta values.


<details>
  <summary>Details</summary>
Motivation: The IB method often suffers from unstable optimization and abrupt shifts near critical beta points, which this work aims to address.

Method: The proposed approach uses symbolic continuation and entropy-regularized trajectories to achieve convexity and uniqueness in the IB solution path.

Result: The method stabilizes representation learning, with proven convexity and uniqueness, and includes sensitivity analyses with robust uncertainty quantification.

Conclusion: The work provides a practical, open-source framework for stable IB optimization, ready for deployment and future extensions.

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [193] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/pdf/2505.09284)
*Panqi Chen, Yifan Sun, Lei Cheng, Yang Yang, Weichang Li, Yang Liu, Weiqing Liu, Jiang Bian, Shikai Fang*

Main category: cs.LG

TL;DR: SDIFT is a novel framework using sequential diffusion in functional Tucker space to model and reconstruct multidimensional physical dynamics from sparse, off-grid observations, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in reconstructing physical dynamics from sparse, irregular observations, which current diffusion-based methods struggle with due to their reliance on on-grid data.

Method: SDIFT employs a functional Tucker model for latent space representation and a sequential diffusion model with a temporally augmented UNet to denoise Gaussian process noise, generating core tensor sequences.

Result: Validated on astronomical, environmental, and molecular systems, SDIFT shows superior reconstruction accuracy and computational efficiency.

Conclusion: SDIFT effectively addresses the limitations of current methods, offering a robust solution for modeling physical dynamics from sparse observations.

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [194] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/pdf/2505.09287)
*Shunsuke Yoneda, Valdemar Å vÃ¡benskÃ½, Gen Li, Daisuke Deguchi, Atsushi Shimada*

Main category: cs.LG

TL;DR: The paper proposes a federated learning method with differential features to address privacy concerns in analyzing digital textbook logs across schools, achieving comparable performance to centralized models.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns limit the integration of confidential student data across schools, hindering the development of generalizable models for behavior analysis and performance prediction.

Method: Combines federated learning (to avoid centralizing data) and differential features (using relative values) to train models for predicting at-risk students.

Result: The method matched centralized learning in performance metrics (Top-n precision, nDCG, PR-AUC) and improved prediction with differential features. Early detection of at-risk students was also successful.

Conclusion: The proposed method effectively balances privacy and performance, enhancing generalizability and early prediction in educational data mining.

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [195] [On the Learning with Augmented Class via Forests](https://arxiv.org/pdf/2505.09294)
*Fan Xu, Wuyang Chen, Wei Gao*

Main category: cs.LG

TL;DR: The paper introduces LACForest, a method for learning with augmented classes in forests, using a new splitting criterion called augmented Gini impurity. It combines shallow forests and deep neural forests for improved performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of augmented classes appearing in testing data but not in training data, enhancing decision tree and forest methods.

Method: Develops LACForest with augmented Gini impurity for splitting, uses pseudo-labeled augmented instances, and integrates deep neural forests with a novel optimization objective.

Result: Theoretical convergence analysis and experiments confirm the effectiveness of LACForest.

Conclusion: LACForest successfully handles augmented classes and improves performance, with code publicly available.

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [196] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/pdf/2505.09308)
*George Andriopoulos, Soyuj Jung Basnet, Juan Guevara, Li Guo, Keith Ross*

Main category: cs.LG

TL;DR: The Unconstrained Feature Model (UFM) provides closed-form approximations for DNN performance, showing multi-task models outperform single-task ones under certain conditions, and whitening/normalizing targets reduces MSE when average variance is <1.


<details>
  <summary>Details</summary>
Motivation: To understand and improve neural multivariate regression in DNNs, particularly for imitation learning, robotics, and reinforcement learning.

Method: Leverages UFM to analyze multi-task vs. single-task models and the impact of whitening/normalizing regression targets.

Result: Multi-task models achieve lower training MSE than single-task models with comparable regularization; whitening/normalizing reduces MSE when target variance is <1.

Conclusion: UFM is a valuable tool for guiding DNN design and data pre-processing strategies.

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [197] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/pdf/2505.09331)
*Cunlai Pu, Fangrui Wu, Rajput Ramiz Sharafat, Guangzhao Dai, Xiangbo Shu*

Main category: cs.LG

TL;DR: Proposes MUST, a multi-scale structural-temporal model for link prediction in UAV networks, addressing sparsity and dynamics with GATs and LSTMs.


<details>
  <summary>Details</summary>
Motivation: Link prediction in UAV networks is challenging due to dynamic, sparse topologies and lack of route info, requiring methods beyond single-scale temporal dynamics.

Method: Uses GATs for multi-scale structural features (individual, community, network levels) and LSTMs for temporal dynamics, with a specialized loss function for sparsity.

Result: MUST outperforms existing methods in highly dynamic, sparse UAV networks, validated by simulation datasets.

Conclusion: MUST effectively captures structural-temporal patterns and addresses sparsity, achieving state-of-the-art link prediction in UAV networks.

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [198] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/pdf/2505.09344)
*Gabriel CortÃªs, Nuno LourenÃ§o, Paolo Romano, Penousal Machado*

Main category: cs.LG

TL;DR: GreenFactory is an ensemble of zero-cost proxies using a random forest regressor to predict model test accuracy, outperforming traditional methods in Neural Architecture Search.


<details>
  <summary>Details</summary>
Motivation: Traditional performance evaluation in Neural Architecture Search is resource-intensive, and existing zero-cost proxies lack generalization and accuracy prediction.

Method: Proposes GreenFactory, an ensemble of zero-cost proxies combined via a random forest regressor to directly predict test accuracy.

Result: Achieves high Kendall correlations (e.g., 0.907-0.945) on NATS-Bench datasets, demonstrating robust performance prediction.

Conclusion: GreenFactory reliably predicts model accuracy, addressing limitations of existing proxies and improving efficiency in architecture search.

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [199] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/pdf/2505.09354)
*Guangtai Wang, Chi-Man Vong, Jintao Huang*

Main category: cs.LG

TL;DR: The paper introduces CleanSE, a calibration strategy for partial label learning that leverages clean samples to improve disambiguation by enhancing candidate confidence and characterizing sample distributions.


<details>
  <summary>Details</summary>
Motivation: Existing disambiguation strategies in partial label learning neglect the supervision information from clean samples, which can guide and boost confidence in candidate labels.

Method: CleanSE combines differentiable count loss and K-Nearest-Neighbor to attribute higher significance to reliable candidates and uses clean samples to restrict label counts, improving sample distribution characterization.

Result: Experiments on synthetic and real-world datasets demonstrate CleanSE's applicability to state-of-the-art PLL methods and its performance enhancement.

Conclusion: CleanSE effectively utilizes clean samples to improve disambiguation in partial label learning, enhancing the performance of existing methods.

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [200] [FAS: Fast ANN-SNN Conversion for Spiking Large Language Models](https://arxiv.org/pdf/2502.04405)
*Long Chen, Xiaotian Song, Andy Song, BaDong Chen, Jiancheng Lv, Yanan Sun*

Main category: cs.LG

TL;DR: FAS introduces a fast ANN-SNN conversion strategy for Spiking LLMs, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Spiking LLMs suffer from performance degradation and high computational costs.

Method: FAS uses a two-stage approach: full-parameter fine-tuning and coarse-to-fine calibration.

Result: FAS achieves higher accuracy with reduced latency and energy consumption, outperforming OPT-7B.

Conclusion: FAS is an efficient and high-performing method for converting LLMs to Spiking LLMs.

Abstract: Spiking Large Language Models have been shown as a good alternative to LLMs
in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct
training and ANN-SNN conversion, often suffer from performance degradation and
relatively high computational costs. To address these issues, we propose a
novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking
LLMs in two stages. The first stage employs a full-parameter fine-tuning of
pre-trained models, so it does not need any direct training from scratch. The
second stage introduces a coarse-to-fine calibration method to reduce
conversion errors and improve accuracy. Experiments on both language and
vision-language tasks across four different scales of LLMs demonstrate that FAS
can achieve state-of-the-art performance yet with significantly reduced
inference latency and computational costs. Notably, FAS only takes eight
timesteps to achieve an accuracy of 3\% higher than that of the OPT-7B model,
while reducing energy consumption by 96.63\%. The source code is available at
https://github.com/lc783/FAS

</details>


### [201] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/pdf/2505.09361)
*Samir Moustafa, Nils M. Kriege, Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: MixQ-GNN introduces mixed precision quantization for GNNs, ensuring efficient inference without performance loss, achieving 5.5x and 5.1x reductions in bit operations for node and graph classification.


<details>
  <summary>Details</summary>
Motivation: The computational demands of GNNs require efficient methods to accelerate inference while maintaining performance.

Method: MixQ-GNN uses a theorem for quantized message passing and flexibly selects integer bit-widths for GNN components.

Result: Achieves 5.5x and 5.1x reductions in bit operations for node and graph classification compared to FP32.

Conclusion: MixQ-GNN effectively balances efficiency and performance in GNN inference through mixed precision quantization.

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [202] [Activation Steering in Neural Theorem Provers](https://arxiv.org/pdf/2502.15507)
*Shashank Kirtania*

Main category: cs.LG

TL;DR: Activation steering improves LLMs' theorem proving by guiding tactic selection, offering a lightweight alternative to fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with ranking correct tactics in proof assistants, hindering their theorem-proving effectiveness.

Method: Use activation steering to guide LLM responses during inference for better tactic selection.

Result: Activation steering enhances LLMs' theorem-proving capabilities without extensive fine-tuning.

Conclusion: Activation steering is a promising, resource-efficient method for improving LLMs in theorem proving.

Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.

</details>


### [203] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2505.09366)
*SeyedMojtaba Mohasel, Alireza Afzal Aghaei, Corey Pew*

Main category: cs.LG

TL;DR: The paper explores learnable activation functions in KANs for personalized lower-limb prosthesis control, comparing user-specific vs. pooled training data for turn intent prediction. Results show no significant improvement with learnable activation functions, but user-specific data outperformed pooled data for ML models.


<details>
  <summary>Details</summary>
Motivation: To enhance personalized control in lower-limb prostheses by evaluating learnable activation functions in KANs and the impact of training data (user-specific vs. pooled) on ML/DL performance.

Method: IMU data from amputees performing turns was used to train MLP, KAN, CNN, and FKAN models. Learnable activation functions in KAN/FKAN were compared to MLP/CNN, and training data impact was assessed.

Result: Learnable activation functions didn't improve performance significantly. User-specific data outperformed pooled data for ML models, but no difference was seen in DL models.

Conclusion: Learnable activation functions may benefit more complex tasks. Pooled training is viable for DL models in prosthesis control, reducing reliance on user-specific data.

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [204] [InductionBench: LLMs Fail in the Simplest Complexity Class](https://arxiv.org/pdf/2502.15823)
*Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang*

Main category: cs.LG

TL;DR: The paper introduces InductionBench, a benchmark to evaluate LLMs' inductive reasoning, finding current models struggle with basic tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on deductive reasoning, leaving inductive reasoning, crucial for scientific discovery, underexplored.

Method: Developed InductionBench to test LLMs' ability to infer rules from data.

Result: Advanced LLMs perform poorly on simple inductive tasks.

Conclusion: Current LLMs lack strong inductive reasoning capabilities, highlighting a gap for future research.

Abstract: Large language models (LLMs) have shown remarkable improvements in reasoning
and many existing benchmarks have been addressed by models such as o1 and o3
either fully or partially. However, a majority of these benchmarks emphasize
deductive reasoning, including mathematical and coding tasks in which rules
such as mathematical axioms or programming syntax are clearly defined, based on
which LLMs can plan and apply these rules to arrive at a solution. In contrast,
inductive reasoning, where one infers the underlying rules from observed data,
remains less explored. Such inductive processes lie at the heart of scientific
discovery, as they enable researchers to extract general principles from
empirical observations. To assess whether LLMs possess this capacity, we
introduce InductionBench, a new benchmark designed to evaluate the inductive
reasoning ability of LLMs. Our experimental findings reveal that even the most
advanced models available struggle to master the simplest complexity classes
within the subregular hierarchy of functions, highlighting a notable deficiency
in current LLMs' inductive reasoning capabilities. Coda and data are available
https://github.com/Wenyueh/inductive_reasoning_benchmark.

</details>


### [205] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/pdf/2505.09427)
*Achref Doula, Max MÃ¼hlÃ¤user, Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath is a modular framework that enhances LLM-based path planning in autonomous driving by integrating conformal prediction for safety guarantees, reducing uncertainty and collision rates.


<details>
  <summary>Details</summary>
Motivation: LLMs in autonomous driving can be overconfident or hallucinate, posing safety risks. SafePath addresses this by ensuring safety guarantees.

Method: SafePath operates in three stages: generating diverse candidate paths, filtering high-risk trajectories with conformal prediction, and selecting the safest path or delegating to humans.

Result: SafePath reduces planning uncertainty by 77% and collision rates by up to 70%, proving its effectiveness.

Conclusion: SafePath successfully balances autonomy and safety in LLM-driven path planning, with tunable human delegation for uncertainty.

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [206] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses](https://arxiv.org/pdf/2505.09432)
*Yuzhou Cao, Han Bao, Lei Feng, Bo An*

Main category: cs.LG

TL;DR: The paper addresses the trade-off between smoothness and linear regret bounds in surrogate losses, proposing a method to construct convex smooth surrogates with linear regret bounds for discrete target losses.


<details>
  <summary>Details</summary>
Motivation: The community believes in a trade-off between smoothness and linear regret bounds in surrogate losses, which may degrade optimization and estimation properties. The paper aims to overcome this dilemma.

Method: The authors construct convex smooth surrogate losses using Fenchel-Young losses based on convolutional negentropy, equivalent to infimal convolution of generalized negentropy and target Bayes risk.

Result: The method maintains a linear surrogate regret bound while ensuring smoothness and provides a consistent estimator of class probability.

Conclusion: The work demonstrates how convex analysis can enhance optimization and statistical efficiency in risk minimization.

Abstract: Surrogate regret bounds bridge the gap between the convergence rates of
surrogate and target losses, with linear bounds favorable for their lossless
regret transfer. While convex smooth surrogate losses are appealing in
particular due to the efficient estimation and optimization, the existence of a
trade-off between the smoothness and linear regret bound has been believed in
the community. That being said, the better optimization and estimation
properties of convex smooth surrogate losses may inevitably deteriorate after
undergoing the regret transfer onto a target loss. We overcome this dilemma for
arbitrary discrete target losses by constructing a convex smooth surrogate
loss, which entails a linear surrogate regret bound composed with a tailored
prediction link. The construction is based on Fenchel-Young losses generated by
the convolutional negentropy, which are equivalent to the infimal convolution
of a generalized negentropy and the target Bayes risk. Consequently, the
infimal convolution enables us to derive a smooth loss while maintaining the
surrogate regret bound linear. We additionally benefit from the infimal
convolution to have a consistent estimator of the underlying class probability.
Our results are overall a novel demonstration of how convex analysis penetrates
into optimization and statistical efficiency in risk minimization.

</details>


### [207] [Variational Rank Reduction Autoencoder](https://arxiv.org/pdf/2505.09458)
*Jad Mounayer, Alicia Tierz, Jerome Tomezyk, Chady Ghnatios, Francisco Chinesta*

Main category: cs.LG

TL;DR: VRRAEs combine RRAEs' deterministic regularization with VAEs' probabilistic generative abilities, outperforming both in generation tasks and reducing posterior collapse.


<details>
  <summary>Details</summary>
Motivation: To merge the strengths of RRAEs (deterministic regularization via SVD) and VAEs (probabilistic generative abilities) for improved performance.

Method: VRRAEs sample RRAEs' latent space and regularize with KL divergence, leveraging SVD-induced regularization.

Result: VRRAEs outperform RRAEs and VAEs in generation tasks (MNIST, CelebA, CIFAR-10) and resist posterior collapse.

Conclusion: VRRAEs effectively combine deterministic and probabilistic approaches, enhancing generative performance and robustness.

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [208] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/pdf/2505.09486)
*Seyed Roozbeh Razavi Rohani, Khashayar Khajavi, Wesley Chung, Mo Chen, Sharan Vaswani*

Main category: cs.LG

TL;DR: AdaLin dynamically adapts neuron activation functions to mitigate plasticity loss in deep neural networks, improving performance on benchmarks without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Loss of plasticity hinders continual learning in non-stationary settings; deep linear networks show resilience, inspiring AdaLin.

Method: AdaLin uses learnable parameters and gating to inject linearity into activation functions based on gradient flow.

Result: AdaLin improves performance on benchmarks like MNIST, CIFAR-10, and CIFAR-100, and aids in reinforcement learning.

Conclusion: Neuron-level adaptation is key to mitigating plasticity loss, as shown by AdaLin's success across tasks.

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [209] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/pdf/2505.09500)
*Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell*

Main category: cs.LG

TL;DR: The paper investigates the brittleness of post-training methods in language models and proposes Layered Unlearning (LU) to improve robustness against adversarial relearning.


<details>
  <summary>Details</summary>
Motivation: To understand how post-training methods modify model behavior and why these modifications are brittle, often bypassed by prompt engineering or relearning.

Method: Designs an unlearning algorithm, Layered Unlearning (LU), which incrementally unlearns subsets of data to limit relearning recovery. Evaluated using synthetic and LLM experiments.

Result: LU enhances robustness to adversarial relearning across various unlearning methods.

Conclusion: LU advances machine unlearning techniques and sheds light on the impact of post-training updates.

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [210] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/pdf/2505.09503)
*Patrik Kenfack, Samira Ebrahimi Kaho, Ulrich AÃ¯vodji*

Main category: cs.LG

TL;DR: The paper explores fairness in tabular in-context learning (ICL), proposing preprocessing strategies to mitigate bias, with uncertainty-based demonstration selection proving most effective.


<details>
  <summary>Details</summary>
Motivation: To understand and address biases in tabular ICL, positioning it as a fair alternative to traditional methods.

Method: Investigates three preprocessing strategies: correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection.

Result: Uncertainty-based demonstration selection consistently improves group fairness in predictions.

Conclusion: Tabular ICL can be made fairer with preprocessing, particularly uncertainty-based demonstration selection, enhancing its viability as a competitive method.

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [211] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/pdf/2505.09572)
*Julian Kranz, Davide Gallon, Steffen Dereich, Arnulf Jentzen*

Main category: cs.LG

TL;DR: The paper analyzes gradient flows in fully connected neural networks with common activation functions, proving convergence or divergence behavior and identifying thresholds for loss convergence.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of gradient flows in neural networks and their convergence properties under various conditions.

Method: Theoretical analysis using o-minimal structures and numerical experiments to validate findings.

Result: Gradient flows either converge to critical points or diverge to infinity, with loss converging to an asymptotic value. A threshold exists for initialization ensuring convergence to optimal loss.

Conclusion: Gradient flows with good initialization diverge to infinity, and theoretical results align with numerical and real-world observations.

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [212] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/pdf/2505.09586)
*Yipeng Zhang, Longlong Li, Kelin Xia*

Main category: cs.LG

TL;DR: The paper introduces Rhomboid Tiling (RT) clustering and RTPool, a hierarchical graph clustering pooling model, to enhance GNNs by leveraging geometric information, outperforming 21 competitors on 7 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing GNN pooling methods rely on graph connectivity, missing rich geometric features in geometric graphs. RT clustering addresses this gap.

Method: Proposes RT clustering for geometric feature extraction and RTPool, a hierarchical pooling model for graph classification.

Result: RTPool outperforms 21 state-of-the-art methods across 7 benchmark datasets.

Conclusion: RT clustering and RTPool effectively capture geometric structures, enhancing GNN performance for graph classification.

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [213] [Online Isolation Forest](https://arxiv.org/pdf/2505.09593)
*Filippo Leveni, Guilherme Weigert Cassales, Bernhard Pfahringer, Albert Bifet, Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest is a streaming anomaly detection method that outperforms existing online and offline techniques in efficiency, making it ideal for real-time applications like cybersecurity.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods are impractical for streaming data due to memory constraints and reliance on retraining.

Method: Proposes Online-iForest, designed for streaming conditions to track evolving data processes without retraining.

Result: Matches online alternatives and rivals offline methods, excelling in efficiency for fast anomaly detection.

Conclusion: Online-iForest is a highly efficient solution for real-time anomaly detection in critical applications.

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [214] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/pdf/2505.09602)
*David Khachaturov, Robert Mullins*

Main category: cs.LG

TL;DR: ASF is a lightweight, model-agnostic defense against adversarial suffix attacks on LLMs, reducing attack efficacy to below 4% without compromising normal performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreak attacks like adversarial suffixes, and existing defenses are limited or ineffective.

Method: Introduces Adversarial Suffix Filtering (ASF), a pipeline to detect and filter adversarial suffixes in prompts.

Result: ASF reduces attack efficacy to below 4% in both black-box and white-box settings, with minimal impact on normal model performance.

Conclusion: ASF is an effective, lightweight solution for protecting LLMs against adversarial suffix attacks.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [215] [DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series](https://arxiv.org/pdf/2404.11269)
*Zahra Zamanzadeh Darban, Yiyuan Yang, Geoffrey I. Webb, Charu C. Aggarwal, Qingsong Wen, Shirui Pan, Mahsa Salehi*

Main category: cs.LG

TL;DR: DACAD combines unsupervised domain adaptation with contrastive learning for time series anomaly detection, addressing inconsistent anomalous classes and improving adaptability.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled data in TSAD and the assumption of consistent anomalous classes in existing UDA methods limit their effectiveness.

Method: DACAD uses anomaly injection, supervised contrastive loss for the source domain, self-supervised contrastive triplet loss for the target domain, and a Center-based Entropy Classifier (CEC).

Result: DACAD outperforms existing methods in transferring knowledge across domains and handling limited labeled data.

Conclusion: DACAD is a robust solution for TSAD, enhancing generalization and adaptability across domains with inconsistent anomalous classes.

Abstract: In time series anomaly detection (TSAD), the scarcity of labeled data poses a
challenge to the development of accurate models. Unsupervised domain adaptation
(UDA) offers a solution by leveraging labeled data from a related domain to
detect anomalies in an unlabeled target domain. However, existing UDA methods
assume consistent anomalous classes across domains. To address this limitation,
we propose a novel Domain Adaptation Contrastive learning model for Anomaly
Detection in multivariate time series (DACAD), combining UDA with contrastive
learning. DACAD utilizes an anomaly injection mechanism that enhances
generalization across unseen anomalous classes, improving adaptability and
robustness. Additionally, our model employs supervised contrastive loss for the
source domain and self-supervised contrastive triplet loss for the target
domain, ensuring comprehensive feature representation learning and
domain-invariant feature extraction. Finally, an effective Center-based Entropy
Classifier (CEC) accurately learns normal boundaries in the source domain.
Extensive evaluations on multiple real-world datasets and a synthetic dataset
highlight DACAD's superior performance in transferring knowledge across domains
and mitigating the challenge of limited labeled data in TSAD.

</details>


### [216] [Learning Traffic Anomalies from Generative Models on Real-Time Observations](https://arxiv.org/pdf/2502.01391)
*Fotis I. Giasemis, Alexandros Sopasakis*

Main category: cs.LG

TL;DR: STGAN framework detects traffic anomalies with high precision using spatiotemporal data from Gothenburg traffic cameras.


<details>
  <summary>Details</summary>
Motivation: Accurate traffic anomaly detection is vital for urban traffic management and congestion reduction.

Method: Combines Graph Neural Networks and LSTM in STGAN to analyze spatial and temporal traffic data from 42 cameras.

Result: High precision and low false positives in detecting anomalies like signal interruptions and weather effects.

Conclusion: STGAN effectively identifies traffic anomalies, aiding urban traffic management.

Abstract: Accurate detection of traffic anomalies is crucial for effective urban
traffic management and congestion mitigation. We use the Spatiotemporal
Generative Adversarial Network (STGAN) framework combining Graph Neural
Networks and Long Short-Term Memory networks to capture complex spatial and
temporal dependencies in traffic data. We apply STGAN to real-time,
minute-by-minute observations from 42 traffic cameras across Gothenburg,
Sweden, collected over several months in 2020. The images are processed to
compute a flow metric representing vehicle density, which serves as input for
the model. Training is conducted on data from April to November 2020, and
validation is performed on a separate dataset from November 14 to 23, 2020. Our
results demonstrate that the model effectively detects traffic anomalies with
high precision and low false positive rates. The detected anomalies include
camera signal interruptions, visual artifacts, and extreme weather conditions
affecting traffic flow.

</details>


### [217] [A General Graph Spectral Wavelet Convolution via Chebyshev Order Decomposition](https://arxiv.org/pdf/2405.13806)
*Nian Liu, Xiaoxin He, Thomas Laurent, Francesco Di Giovanni, Michael M. Bronstein, Xavier Bresson*

Main category: cs.LG

TL;DR: WaveGC, a wavelet-based graph convolution network, improves spectral graph convolution by integrating multi-resolution bases and a matrix-valued kernel, outperforming existing methods in flexibility and performance.


<details>
  <summary>Details</summary>
Motivation: Existing spectral graph convolution methods lack flexibility for large spatial ranges and spectral function capacity, limiting their effectiveness.

Method: WaveGC combines multi-resolution spectral bases and a matrix-valued filter kernel, using Chebyshev polynomials to learn general graph wavelets while satisfying admissibility criteria.

Result: Numerical experiments show WaveGC consistently improves performance in both short-range and long-range tasks.

Conclusion: WaveGC offers superior filtering flexibility and effectiveness, making it a robust solution for diverse graph signal processing scenarios.

Abstract: Spectral graph convolution, an important tool of data filtering on graphs,
relies on two essential decisions: selecting spectral bases for signal
transformation and parameterizing the kernel for frequency analysis. While
recent techniques mainly focus on standard Fourier transform and vector-valued
spectral functions, they fall short in flexibility to model signal
distributions over large spatial ranges, and capacity of spectral function. In
this paper, we present a novel wavelet-based graph convolution network, namely
WaveGC, which integrates multi-resolution spectral bases and a matrix-valued
filter kernel. Theoretically, we establish that WaveGC can effectively capture
and decouple short-range and long-range information, providing superior
filtering flexibility, surpassing existing graph wavelet neural networks. To
instantiate WaveGC, we introduce a novel technique for learning general graph
wavelets by separately combining odd and even terms of Chebyshev polynomials.
This approach strictly satisfies wavelet admissibility criteria. Our numerical
experiments showcase the consistent improvements in both short-range and
long-range tasks. This underscores the effectiveness of the proposed model in
handling different scenarios. Our code is available at
https://github.com/liun-online/WaveGC.

</details>


### [218] [Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling](https://arxiv.org/pdf/2504.10612)
*Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze*

Main category: cs.LG

TL;DR: Energy Matching combines flow-based models with EBM flexibility, improving generation fidelity and regularization for inverse problems.


<details>
  <summary>Details</summary>
Motivation: Address limitations of generative models in handling partial observations and priors, leveraging EBM strengths.

Method: Proposes Energy Matching, using a scalar field to guide samples from noise to data, incorporating entropic energy for Boltzmann equilibrium.

Result: Outperforms EBMs on CIFAR-10 and ImageNet, supports diverse mode exploration in protein generation.

Conclusion: Simplifies EBM framework, advancing generative modeling capabilities for broader adoption.

Abstract: The most widely used generative models map noise and data distributions by
matching flows or scores. However, they struggle to incorporate partial
observations and additional priors--something energy-based models (EBMs) handle
elegantly by simply adding corresponding scalar energy terms. We address this
issue by proposing Energy Matching, a framework that endows flow-based
approaches with the flexibility of EBMs. Far from the data manifold, samples
move along curl-free, optimal transport paths from noise to data. As they
approach the data manifold, an entropic energy term guides the system into a
Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in
terms of fidelity, while retaining simulation-free training of transport-based
approaches away from the data manifold. Furthermore, we leverage the method's
flexibility to introduce an interaction energy that supports diverse mode
exploration, which we demonstrate in a controlled protein-generation setting.
Our approach focuses on learning a scalar potential energy--without
time-conditioning, auxiliary generators, or additional networks--which marks a
significant departure from recent EBM methods. We believe that this simplified
framework significantly advances EBMs capabilities and paves the way for their
wider adoption in generative modeling across diverse domains.

</details>


### [219] [Accelerated Stochastic Min-Max Optimization Based on Bias-corrected Momentum](https://arxiv.org/pdf/2406.13041)
*Haoyuan Cai, Sulaiman A. Alghunaim, Ali H. Sayed*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Lower-bound analyses for nonconvex strongly-concave minimax optimization
problems have shown that stochastic first-order algorithms require at least
$\mathcal{O}(\varepsilon^{-4})$ oracle complexity to find an
$\varepsilon$-stationary point. Some works indicate that this complexity can be
improved to $\mathcal{O}(\varepsilon^{-3})$ when the loss gradient is Lipschitz
continuous. The question of achieving enhanced convergence rates under distinct
conditions, remains unresolved. In this work, we address this question for
optimization problems that are nonconvex in the minimization variable and
strongly concave or Polyak-Lojasiewicz (PL) in the maximization variable. We
introduce novel bias-corrected momentum algorithms utilizing efficient
Hessian-vector products. We establish convergence conditions and demonstrate a
lower iteration complexity of $\mathcal{O}(\varepsilon^{-3})$ for the proposed
algorithms. The effectiveness of the method is validated through applications
to robust logistic regression using real-world datasets.

</details>


### [220] [Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models](https://arxiv.org/pdf/2408.10368)
*Yuntao Wu, Jiayuan Guo, Goutham Gopalakrishna, Zissis Poulos*

Main category: cs.LG

TL;DR: Deep-MacroFin is a deep learning framework for solving high-dimensional PDEs in economics, offering efficiency and ease of use.


<details>
  <summary>Details</summary>
Motivation: Addressing computational challenges in solving high-dimensional PDEs in continuous-time economics.

Method: Uses Multi-Layer Perceptrons and Kolmogorov-Arnold Networks, optimized with HJB equations and algebraic constraints.

Result: Achieves 5Ã less memory and 40Ã fewer FLOPs in 100D problems, and solves 50D economic models.

Conclusion: Deep-MacroFin provides an efficient, scalable solution for high-dimensional PDEs in economics.

Abstract: In this paper, we present Deep-MacroFin, a comprehensive framework designed
to solve partial differential equations, with a particular focus on models in
continuous time economics. This framework leverages deep learning
methodologies, including Multi-Layer Perceptrons and the newly developed
Kolmogorov-Arnold Networks. It is optimized using economic information
encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic
equations. The application of neural networks holds the promise of accurately
resolving high-dimensional problems with fewer computational demands and
limitations compared to other numerical methods. This framework can be readily
adapted for systems of partial differential equations in high dimensions.
Importantly, it offers a more efficient (5$\times$ less CUDA memory and
40$\times$ fewer FLOPs in 100D problems) and user-friendly implementation than
existing libraries. We also incorporate a time-stepping scheme to enhance
training stability for nonlinear HJB equations, enabling the solution of 50D
economic models.

</details>


### [221] [Least Squares and Marginal Log-Likelihood Model Predictive Control using Normalizing Flows](https://arxiv.org/pdf/2409.17632)
*Eike Cramer*

Main category: cs.LG

TL;DR: The paper proposes using conditional normalizing flows for stochastic dynamics modeling in MPC, reducing setpoint error and constraint violations compared to nominal controllers.


<details>
  <summary>Details</summary>
Motivation: Real-world biochemical processes exhibit stochastic dynamics with correlations and state-dependent fluctuations, which traditional MPC models often oversimplify.

Method: Conditional normalizing flows are used to learn stochastic dynamics, with objectives including least squares and marginal log-likelihood based on explicit PDFs and Markov chain simulations.

Result: In a reactor study, the proposed method halves setpoint error and reduces constraint violations compared to nominal controllers, with MLL offering more stability for small scenario sets.

Conclusion: Conditional normalizing flows improve MPC performance by better capturing stochastic dynamics, with MLL objectives enhancing stability in limited scenarios.

Abstract: Real-world (bio)chemical processes often exhibit stochastic dynamics with
non-trivial correlations and state-dependent fluctuations. Model predictive
control (MPC) often must consider these fluctuations to achieve reliable
performance. However, most process models simply add stationary noise terms to
a deterministic prediction. This work proposes using conditional normalizing
flows as discrete-time models to learn stochastic dynamics. Normalizing flows
learn the probability density function (PDF) of the states explicitly, given
prior states and control inputs. In addition to standard least squares (LSQ)
objectives, this work derives a marginal log-likelihood (MLL) objective based
on the explicit PDF and Markov chain simulations. In a reactor study, the
normalizing flow MPC reduces the setpoint error in open and closed-loop cases
to half that of a nominal controller. Furthermore, the chance constraints lead
to fewer constraint violations than the nominal controller. The MLL objective
yields slightly more stable results than the LSQ, particularly for small
scenario sets.

</details>


### [222] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/pdf/2505.00887)
*Xi Chen, Yateng Tang, Jiarong Xu, Jiawei Zhang, Siwei Zhang, Sijia Peng, Xuehao Zheng, Yun Xiong*

Main category: cs.LG

TL;DR: The paper introduces LeTE, a learnable time encoding method using deep function learning to handle diverse and complex time patterns, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing time encoding methods are limited by narrow inductive biases, making them ineffective for real-world diverse and complex time patterns.

Method: Proposes Learnable Transformation-based Generalized Time Encoding (LeTE) using deep function learning to parameterize non-linear transformations.

Result: LeTE outperforms previous methods, demonstrating versatility and effectiveness across diverse domains.

Conclusion: LeTE generalizes previous methods and is adaptable for various tasks, addressing the limitations of existing time encoding approaches.

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>


### [223] [Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization](https://arxiv.org/pdf/2410.02247)
*Xinhao Yao, Hongjin Qian, Xiaolin Hu, Gengze Xu, Wei Liu, Jian Luan, Bin Wang, Yong Liu*

Main category: cs.LG

TL;DR: The paper explores efficient fine-tuning of LLMs by focusing on optimizing specific attention matrices (W_v, W_q) and using customized learning rates, achieving comparable or better performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is resource-intensive due to their large parameter size. The study aims to identify efficient fine-tuning strategies by analyzing the impact of optimizing specific attention matrices and learning rates.

Method: The study investigates two phenomena: (1) Unequal importance of attention matrices (W_v, W_q, W_k) and (2) Customized learning rates for these matrices. A new fine-tuning strategy is proposed based on these insights.

Result: Optimizing W_v and W_q is more effective than W_k, and using higher learning rates for W_v improves convergence and performance. The proposed strategy reduces storage and time costs while maintaining performance.

Conclusion: The findings provide a theoretical foundation for efficient LLM fine-tuning, demonstrating that selective optimization and tailored learning rates enhance performance and reduce resource usage.

Abstract: Large Language Models (LLMs), built on Transformer architectures, exhibit
remarkable generalization across a wide range of tasks. However, fine-tuning
these models for specific tasks remains resource-intensive due to their
extensive parameterization. In this paper, we explore two remarkable phenomena
related to the attention mechanism during the fine-tuning of LLMs (where
$\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$ denote the weights of the
query, key, and value layers, respectively). The first phenomenon, termed
"Unequal Importance of Attention Matrices", highlights the impact of
fine-tuning different weight matrices. It shows that optimizing the
$\mathbf{W}_v$ matrix yields significantly better performance than optimizing
the $\mathbf{W}_k$ matrix. Fine-tuning only the $\mathbf{W}_q$ and
$\mathbf{W}_v$ matrices is computationally efficient while delivering results
comparable to, or even better than fine-tuning all three matrices
($\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$). The second
phenomenon,"Attention Matrices with Customized Learning Rate Lead to Better
Convergence", emphasizes the importance of assigning distinct learning rates to
these matrices. Specifically, a higher learning rate for the $\mathbf{W}_v$
matrix compared to $\mathbf{W}_q$ and $\mathbf{W}_k$ accelerates convergence
and improves performance. Building on these insights, we propose a new strategy
that improves fine-tuning efficiency in terms of both storage and time.
Experimental results on benchmark datasets validate the effectiveness of this
approach, supporting our theoretical findings. Our analysis lays the
theoretical groundwork for configuring and improving algorithms in LLMs
fine-tuning.

</details>


### [224] [Don't be lazy: CompleteP enables compute-efficient deep transformers](https://arxiv.org/pdf/2505.01618)
*Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, Joel Hestness*

Main category: cs.LG

TL;DR: The paper introduces CompleteP, a parameterization method for LLM training that ensures hyperparameter transfer across model sizes and avoids lazy learning, improving compute efficiency by 12-34%.


<details>
  <summary>Details</summary>
Motivation: Current parameterizations often fail to transfer optimal hyperparameters across model sizes, leading to expensive re-tuning or sub-optimal training. Some also restrict learning to linearized features, limiting depth and nonlinearity.

Method: The study analyzes different parameterizations, develops theory on lazy learning, and identifies CompleteP, which ensures hyperparameter transfer and non-lazy learning.

Result: CompleteP allows flexible model shapes for hardware and operational needs, achieving 12-34% better compute efficiency than prior methods.

Conclusion: CompleteP is a superior parameterization for LLM training, enabling efficient scaling and better use of model depth and nonlinearity.

Abstract: We study compute efficiency of LLM training when using different
parameterizations, i.e., rules for adjusting model and optimizer
hyperparameters (HPs) as model size changes. Some parameterizations fail to
transfer optimal base HPs (such as learning rate) across changes in model
depth, requiring practitioners to either re-tune these HPs as they scale up
(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even
when they achieve HP transfer, we develop theory to show parameterizations may
still exist in the lazy learning regime where layers learn only features close
to their linearization, preventing effective use of depth and nonlinearity.
Finally, we identify and adopt the parameterization we call CompleteP that
achieves both depth-wise HP transfer and non-lazy learning in all layers.
CompleteP enables a wider range of model width/depth ratios to remain
compute-efficient, unlocking shapes better suited for different hardware
settings and operational contexts. Moreover, CompleteP enables 12-34% compute
efficiency improvements over the prior state-of-the-art.

</details>


### [225] [Time Can Invalidate Algorithmic Recourse](https://arxiv.org/pdf/2410.08007)
*Giovanni De Toni, Stefano Teso, Bruno Lepri, Andrea Passerini*

Main category: cs.LG

TL;DR: The paper explores the robustness of algorithmic recourse (AR) over time, highlighting its limitations in dynamic environments and proposing a temporal AR solution.


<details>
  <summary>Details</summary>
Motivation: AR provides actionable steps to reverse unfavorable ML decisions, but its validity diminishes over time due to environmental changes. The study aims to address this temporal fragility.

Method: The problem is framed causally, analyzing AR robustness theoretically and empirically. A temporal AR algorithm is proposed, assuming access to a stochastic process estimator.

Result: Causal AR methods fail over time unless the world is stationary. Counterfactual AR cannot be optimally solved unless the world is deterministic. The proposed temporal AR shows resilience in simulations.

Conclusion: Temporal AR, accounting for time and stochastic processes, offers more robust solutions in dynamic environments compared to traditional AR methods.

Abstract: Algorithmic Recourse (AR) aims to provide users with actionable steps to
overturn unfavourable decisions made by machine learning predictors. However,
these actions often take time to implement (e.g., getting a degree can take
years), and their effects may vary as the world evolves. Thus, it is natural to
ask for recourse that remains valid in a dynamic environment. In this paper, we
study the robustness of algorithmic recourse over time by casting the problem
through the lens of causality. We demonstrate theoretically and empirically
that (even robust) causal AR methods can fail over time, except in the --
unlikely -- case that the world is stationary. Even more critically, unless the
world is fully deterministic, counterfactual AR cannot be solved optimally. To
account for this, we propose a simple yet effective algorithm for temporal AR
that explicitly accounts for time under the assumption of having access to an
estimator approximating the stochastic process. Our simulations on synthetic
and realistic datasets show how considering time produces more resilient
solutions to potential trends in the data distribution.

</details>


### [226] [Combinatorial Logistic Bandits](https://arxiv.org/pdf/2410.17075)
*Xutong Liu, Xiangxiang Dai, Xuchuang Wang, Mohammad Hajiesmaili, John C. S. Lui*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a novel framework called combinatorial logistic bandits (CLogB),
where in each round, a subset of base arms (called the super arm) is selected,
with the outcome of each base arm being binary and its expectation following a
logistic parametric model. The feedback is governed by a general arm triggering
process. Our study covers CLogB with reward functions satisfying two smoothness
conditions, capturing application scenarios such as online content delivery,
online learning to rank, and dynamic channel allocation. We first propose a
simple yet efficient algorithm, CLogUCB, utilizing a variance-agnostic
exploration bonus. Under the 1-norm triggering probability modulated (TPM)
smoothness condition, CLogUCB achieves a regret bound of
$\tilde{O}(d\sqrt{\kappa KT})$, where $\tilde{O}$ ignores logarithmic factors,
$d$ is the dimension of the feature vector, $\kappa$ represents the
nonlinearity of the logistic model, and $K$ is the maximum number of base arms
a super arm can trigger. This result improves on prior work by a factor of
$\tilde{O}(\sqrt{\kappa})$. We then enhance CLogUCB with a variance-adaptive
version, VA-CLogUCB, which attains a regret bound of $\tilde{O}(d\sqrt{KT})$
under the same 1-norm TPM condition, improving another
$\tilde{O}(\sqrt{\kappa})$ factor. VA-CLogUCB shows even greater promise under
the stronger triggering probability and variance modulated (TPVM) condition,
achieving a leading $\tilde{O}(d\sqrt{T})$ regret, thus removing the additional
dependency on the action-size $K$. Furthermore, we enhance the computational
efficiency of VA-CLogUCB by eliminating the nonconvex optimization process when
the context feature map is time-invariant while maintaining the tight
$\tilde{O}(d\sqrt{T})$ regret. Finally, experiments on synthetic and real-world
datasets demonstrate the superior performance of our algorithms compared to
benchmark algorithms.

</details>


### [227] [A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems](https://arxiv.org/pdf/2412.09009)
*Sumanth Kumar Boya, Deepak Subramani*

Main category: cs.LG

TL;DR: PINTO, a physics-informed transformer neural operator, generalizes to unseen initial/boundary conditions without retraining, using only physics loss, outperforming existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neural approaches require retraining for new conditions and large simulation data. PINTO addresses these limitations by enabling efficient generalization and simulation-free training.

Method: Uses iterative kernel integral operator units with cross-attention to transform PDE solution domain points into condition-aware vectors, learning solutions for new scenarios.

Result: Achieves relative errors one-fifth to one-third of other methods for unseen conditions and solves equations accurately at untrained time steps.

Conclusion: PINTO offers a robust, efficient solution for nonlinear PDEs, advancing physics-informed operator learning.

Abstract: Initial boundary value problems arise commonly in applications with
engineering and natural systems governed by nonlinear partial differential
equations (PDEs). Operator learning is an emerging field for solving these
equations by using a neural network to learn a map between infinite dimensional
input and output function spaces. These neural operators are trained using a
combination of data (observations or simulations) and PDE-residuals
(physics-loss). A major drawback of existing neural approaches is the
requirement to retrain with new initial/boundary conditions, and the necessity
for a large amount of simulation data for training. We develop a
physics-informed transformer neural operator (named PINTO) that efficiently
generalizes to unseen initial and boundary conditions, trained in a
simulation-free setting using only physics loss. The main innovation lies in
our new iterative kernel integral operator units, implemented using
cross-attention, to transform the PDE solution's domain points into an
initial/boundary condition-aware representation vector, enabling efficient
learning of the solution function for new scenarios. The PINTO architecture is
applied to simulate the solutions of important equations used in engineering
applications: advection, Burgers, and steady and unsteady Navier-Stokes
equations (three flow scenarios). For these five test cases, we show that the
relative errors during testing under challenging conditions of unseen
initial/boundary conditions are only one-fifth to one-third of other leading
physics informed operator learning methods. Moreover, our PINTO model is able
to accurately solve the advection and Burgers equations at time steps that are
not included in the training collocation points. The code is available at
https://github.com/quest-lab-iisc/PINTO

</details>


### [228] [Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery](https://arxiv.org/pdf/2505.06795)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: The paper introduces a Regularized Sparse Autoencoder (RSAE) for multi-horizon commodity price forecasting, emphasizing interpretability of market drivers.


<details>
  <summary>Details</summary>
Motivation: Commodity price volatility and the lack of transparency in current models necessitate accurate and interpretable forecasting methods.

Method: The RSAE uses L1 regularization to enforce sparsity in latent vectors, enabling interpretable market driver discovery alongside multi-horizon price prediction.

Result: The RSAE achieves competitive forecasting accuracy for copper and crude oil prices while providing insights into underlying market dynamics.

Conclusion: The RSAE outperforms traditional black-box models by combining predictive accuracy with interpretability of latent market drivers.

Abstract: Commodity price volatility creates economic challenges, necessitating
accurate multi-horizon forecasting. Predicting prices for commodities like
copper and crude oil is complicated by diverse interacting factors
(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack
transparency, limiting strategic use. This paper presents a Regularized Sparse
Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon
commodity price prediction and discovery of interpretable latent market
drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,
1-month) using multivariate time series. Crucially, L1 regularization
($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity,
promoting parsimonious explanations of market dynamics through learned factors
representing underlying drivers (e.g., demand, supply shocks). Drawing from
energy-based models and sparse coding, the RSAE optimizes predictive accuracy
while learning sparse representations. Evaluated on historical Copper and Crude
Oil data with numerous indicators, our findings indicate the RSAE offers
competitive multi-horizon forecasting accuracy and data-driven insights into
price dynamics via its interpretable latent space, a key advantage over
traditional black-box approaches.

</details>


### [229] [BridgePure: Limited Protection Leakage Can Break Black-Box Data Protection](https://arxiv.org/pdf/2412.21061)
*Yihan Wang, Yiwei Lu, Xiao-Shan Gao, Gautam Kamath, Yaoliang Yu*

Main category: cs.LG

TL;DR: The paper reveals vulnerabilities in black-box data protection tools by showing how unprotected data can be used to bypass protections via a diffusion bridge model called BridgePure.


<details>
  <summary>Details</summary>
Motivation: To expose weaknesses in black-box data protection tools that claim to secure datasets against unauthorized machine learning.

Method: Proposes a threat model where an adversary uses unprotected data to train a diffusion bridge model (BridgePure) to map and remove protections from data.

Result: BridgePure effectively purifies protected data, demonstrating vulnerabilities in current protection methods.

Conclusion: Practitioners should implement multi-level countermeasures to address these risks.

Abstract: Availability attacks, or unlearnable examples, are defensive techniques that
allow data owners to modify their datasets in ways that prevent unauthorized
machine learning models from learning effectively while maintaining the data's
intended functionality. It has led to the release of popular black-box tools
(e.g., APIs) for users to upload personal data and receive protected
counterparts. In this work, we show that such black-box protections can be
substantially compromised if a small set of unprotected in-distribution data is
available. Specifically, we propose a novel threat model of protection leakage,
where an adversary can (1) easily acquire (unprotected, protected) pairs by
querying the black-box protections with a small unprotected dataset; and (2)
train a diffusion bridge model to build a mapping between unprotected and
protected data. This mapping, termed BridgePure, can effectively remove the
protection from any previously unseen data within the same distribution.
BridgePure demonstrates superior purification performance on classification and
style mimicry tasks, exposing critical vulnerabilities in black-box data
protection. We suggest that practitioners implement multi-level countermeasures
to mitigate such risks.

</details>


### [230] [A Comprehensive Social Bias Audit of Contrastive Vision Language Models](https://arxiv.org/pdf/2501.13223)
*Zahraa Al Sahili, Ioannis Patras, Matthew Purver*

Main category: cs.LG

TL;DR: FairCoT is a framework using Chain-of-Thought reasoning to reduce biases in text-to-image models, improving fairness and diversity without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Address ethical challenges from dataset biases in text-to-image models, especially in sensitive contexts.

Method: Uses iterative CoT refinement to adjust prompts dynamically, ensuring equitable representation.

Result: Significantly enhances fairness and diversity in models like DALL-E and Stable Diffusion, maintaining image quality.

Conclusion: FairCoT advances ethical AI content generation by balancing creativity and responsibility.

Abstract: In the domain of text-to-image generative models, biases inherent in training
datasets often propagate into generated content, posing significant ethical
challenges, particularly in socially sensitive contexts. We introduce FairCoT,
a novel framework that enhances fairness in text-to-image models through
Chain-of-Thought (CoT) reasoning within multimodal generative large language
models. FairCoT employs iterative CoT refinement to systematically mitigate
biases, and dynamically adjusts textual prompts in real time, ensuring diverse
and equitable representation in generated images. By integrating iterative
reasoning processes, FairCoT addresses the limitations of zero-shot CoT in
sensitive scenarios, balancing creativity with ethical responsibility.
Experimental evaluations across popular text-to-image systems--including DALL-E
and various Stable Diffusion variants--demonstrate that FairCoT significantly
enhances fairness and diversity without sacrificing image quality or semantic
fidelity. By combining robust reasoning, lightweight deployment, and
extensibility to multiple models, FairCoT represents a promising step toward
more socially responsible and transparent AI-driven content generation.

</details>


### [231] [Handling Missing Data in Downstream Tasks With Distribution-Preserving Guarantees](https://arxiv.org/pdf/2501.13786)
*Rahul Bordoloi, ClÃ©mence RÃ©da, Saptarshi Bej, Olaf Wolkenhauer*

Main category: cs.LG

TL;DR: F3I is a novel imputation method for missing data in classification tasks, offering theoretical guarantees and superior performance.


<details>
  <summary>Details</summary>
Motivation: Missing feature values hinder machine-learning tasks, and existing imputation methods lack efficiency and theoretical guarantees, especially for not-missing-at-random mechanisms.

Method: F3I improves K-nearest neighbor imputation by learning neighbor-specific weights via a concave, differentiable objective function, preserving data distribution. It can be jointly trained with classifiers.

Result: Theoretical analysis confirms F3I's imputation quality and data distribution preservation. It outperforms others in imputation and classification tasks, including drug repurposing and digit recognition.

Conclusion: F3I is an effective, theoretically grounded imputation method for high-dimensional data, enhancing downstream classification performance.

Abstract: Missing feature values are a significant hurdle for downstream
machine-learning tasks such as classification. However, imputation methods for
classification might be time-consuming for high-dimensional data, and offer few
theoretical guarantees on the preservation of the data distribution and
imputation quality, especially for not-missing-at-random mechanisms. First, we
propose an imputation approach named F3I based on the iterative improvement of
a K-nearest neighbor imputation, where neighbor-specific weights are learned
through the optimization of a novel concave, differentiable objective function
related to the preservation of the data distribution on non-missing values. F3I
can then be chained to and jointly trained with any classifier architecture.
Second, we provide a theoretical analysis of imputation quality and data
distribution preservation by F3I for several types of missing mechanisms.
Finally, we demonstrate the superior performance of F3I on several imputation
and classification tasks, with applications to drug repurposing and
handwritten-digit recognition data.

</details>


### [232] [Improving Network Threat Detection by Knowledge Graph, Large Language Model, and Imbalanced Learning](https://arxiv.org/pdf/2501.16393)
*Lili Zhang, Quanyan Zhu, Herman Ray, Ying Xie*

Main category: cs.LG

TL;DR: Proposes an integrated framework using Knowledge Graphs, Imbalanced Learning, and LLMs for network threat detection, improving threat capture rate by 3%-4%.


<details>
  <summary>Details</summary>
Motivation: Challenges in network threat detection due to complex attack activities and limited historical data.

Method: Combines Knowledge Graph for activity analysis, Imbalanced Learning for pruning, and LLM for interpretation.

Result: Improved threat capture rate by 3%-4% and better interpretability of risk predictions.

Conclusion: The framework enhances threat detection and interpretability, showing promise for practical applications.

Abstract: Network threat detection has been challenging due to the complexities of
attack activities and the limitation of historical threat data to learn from.
To help enhance the existing practices of using analytics, machine learning,
and artificial intelligence methods to detect the network threats, we propose
an integrated modelling framework, where Knowledge Graph is used to analyze the
users' activity patterns, Imbalanced Learning techniques are used to prune and
weigh Knowledge Graph, and LLM is used to retrieve and interpret the users'
activities from Knowledge Graph. The proposed framework is applied to Agile
Threat Detection through Online Sequential Learning. The preliminary results
show the improved threat capture rate by 3%-4% and the increased
interpretabilities of risk predictions based on the users' activities.

</details>


### [233] [Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency Approach for ECG Classification](https://arxiv.org/pdf/2502.00497)
*Sam Jeong, Hae Yong Kim*

Main category: cs.LG

TL;DR: The paper introduces CFAN, a novel CNN architecture integrating Fourier principles for ECG classification, outperforming benchmarks in accuracy.


<details>
  <summary>Details</summary>
Motivation: To resolve the unresolved optimal integration of time- and frequency-domain information in ECG classification using CNNs.

Method: Proposes CFAN, embedding Fourier principles into CNN layers with CONV-FAN blocks for joint time-frequency learning.

Result: CFAN achieved state-of-the-art accuracies: 98.95% (MIT-BIH), 96.83% (ECG-ID), and 95.01% (Apnea-ECG), with significant improvements over benchmarks.

Conclusion: CFAN demonstrates superior performance and potential for broader biomedical signal classification applications.

Abstract: Machine learning has revolutionized biomedical signal analysis, particularly
in electrocardiogram (ECG) classification. While convolutional neural networks
(CNNs) excel at automatic feature extraction, the optimal integration of time-
and frequency-domain information remains unresolved. This study introduces the
Convolutional Fourier Analysis Network (CFAN), a novel architecture that
unifies time-frequency analysis by embedding Fourier principles directly into
CNN layers. We evaluate CFAN against four benchmarks - spectrogram-based 2D CNN
(SPECT); 1D CNN (CNN1D); Fourier-based 1D CNN (FFT1D); and CNN1D with
integrated Fourier Analysis Network (CNN1D-FAN) - across three ECG tasks:
arrhythmia classification (MIT-BIH), identity recognition (ECG-ID), and apnea
detection (Apnea-ECG). CFAN achieved state-of-the-art performance, surpassing
all competing methods with accuracies of 98.95% (MIT-BIH), 96.83% (ECG-ID), and
95.01% (Apnea-ECG). Notably, on ECG-ID and Apnea-ECG, CFAN demonstrated
statistically significant improvements over the second-best method (CNN1D-FAN,
$p \leq 0.02$), further validating its superior performance. Key innovations
include CONV-FAN blocks that combine sine, cosine and GELU activations in
convolutional layers to capture periodic features and joint time-frequency
learning without spectrogram conversion. Our results highlight CFAN's potential
for broader biomedical and signal classification applications.

</details>


### [234] [Graph-structured Small Molecule Drug Discovery Through Deep Learning: Progress, Challenges, and Opportunities](https://arxiv.org/pdf/2502.08975)
*Kun Li, Yida Xiong, Hongzhi Zhang, Xiantao Cai, Jia Wu, Bo Du, Wenbin Hu*

Main category: cs.LG

TL;DR: The paper reviews DL-based methods for small molecule drug discovery, highlighting their advantages over traditional approaches, summarizing key tasks, methods, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance drug discovery efficiency and accuracy by leveraging DL techniques, addressing the need for systematic summaries of recent advancements in graph-structured small molecule drug discovery.

Method: Systematic review and generalization of key tasks, methods, datasets, and trends in DL-based small molecule drug discovery, focusing on graph-structured approaches.

Result: DL-based methods outperform traditional machine learning in accuracy, speed, and modeling complex molecular relationships, improving drug screening and optimization.

Conclusion: The paper identifies challenges like interpretability and generalization, suggesting future research directions to advance small molecule drug discovery.

Abstract: Due to their excellent drug-like and pharmacokinetic properties, small
molecule drugs are widely used to treat various diseases, making them a
critical component of drug discovery. In recent years, with the rapid
development of deep learning (DL) techniques, DL-based small molecule drug
discovery methods have achieved excellent performance in prediction accuracy,
speed, and complex molecular relationship modeling compared to traditional
machine learning approaches. These advancements enhance drug screening
efficiency and optimization and provide more precise and effective solutions
for various drug discovery tasks. Contributing to this field's development,
this paper aims to systematically summarize and generalize the recent key tasks
and representative techniques in graph-structured small molecule drug discovery
in recent years. Specifically, we provide an overview of the major tasks in
small molecule drug discovery and their interrelationships. Next, we analyze
the six core tasks, summarizing the related methods, commonly used datasets,
and technological development trends. Finally, we discuss key challenges, such
as interpretability and out-of-distribution generalization, and offer our
insights into future research directions for small molecule drug discovery.

</details>


### [235] [TuneNSearch: a hybrid transfer learning and local search approach for solving vehicle routing problems](https://arxiv.org/pdf/2503.12662)
*Arthur CorrÃªa, CristÃ³vÃ£o Silva, Liming Xu, Alexandra Brintrup, Samuel Moniz*

Main category: cs.LG

TL;DR: TuneNSearch combines transfer learning and local search to solve VRP variants efficiently, outperforming state-of-the-art models with minimal training epochs.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between adaptability and performance in multi-task learning for VRP variants by leveraging pre-training on complex problems.

Method: Pre-train a reinforcement learning model on multi-depot VRP, fine-tune for variants, and use a Transformer-based architecture with local search.

Result: Outperforms existing models, requires fewer training epochs, and generalizes well across tasks and problem sizes.

Conclusion: TuneNSearch bridges the gap in VRP literature by offering a high-performance, adaptable solution with strong generalization.

Abstract: This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing different variants of vehicle routing problems
(VRP). Recently, multi-task learning has gained much attention for solving VRP
variants. However, this adaptability often compromises the performance of the
models. To address this challenge, we first pre-train a reinforcement learning
model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it
to different variants. By leveraging the complexity of the multi-depot VRP, the
pre-trained model learns richer node representations and gains more
transferable knowledge compared to models trained on simpler routing problems,
such as the traveling salesman problem. TuneNSearch employs, in the first
stage, a Transformer-based architecture, augmented with a residual edge-graph
attention network to capture the impact of edge distances and residual
connections between layers. This architecture allows for a more precise capture
of graph-structured data, improving the encoding of VRP's features. After
inference, our model is also coupled with a second stage composed of a local
search algorithm, which yields substantial performance gains with minimal
computational overhead added. Results show that TuneNSearch outperforms many
existing state-of-the-art models trained for each VRP variant, requiring only
one-fifth of the training epochs. Our approach demonstrates strong
generalization, achieving high performance across different tasks,
distributions and problem sizes, thus addressing a long-standing gap in the
literature.

</details>


### [236] [GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration](https://arxiv.org/pdf/2504.02692)
*Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda*

Main category: cs.LG

TL;DR: GPTAQ is a finetuning-free quantization method for transformers, improving upon GPTQ by reducing quantization errors through asymmetric calibration and parallelization techniques.


<details>
  <summary>Details</summary>
Motivation: To compress large-scale transformer architectures without finetuning while minimizing quantization errors.

Method: Uses asymmetric calibration to match quantized layer outputs to full-precision models, employs optimal brain compression for analysis, and parallelizes calculations with techniques like channel parallelization and Cholesky reformulation.

Result: Achieves better performance under low-bit quantization, successfully quantizing a 405B language transformer and EVA-02 vision transformer on a single GPU.

Conclusion: GPTAQ is efficient, easy to implement, and outperforms GPTQ in reducing quantization errors.

Abstract: We introduce GPTAQ, a novel finetuning-free quantization method for
compressing large-scale transformer architectures. Unlike the previous GPTQ
method, which independently calibrates each layer, we always match the
quantized layer's output to the exact output in the full-precision model,
resulting in a scheme that we call asymmetric calibration. Such a scheme can
effectively reduce the quantization error accumulated in previous layers. We
analyze this problem using optimal brain compression to derive a close-formed
solution. The new solution explicitly minimizes the quantization error as well
as the accumulated asymmetry error. Furthermore, we utilize various techniques
to parallelize the solution calculation, including channel parallelization,
neuron decomposition, and Cholesky reformulation for matrix fusion. As a
result, GPTAQ is easy to implement, simply using 20 more lines of code than
GPTQ but improving its performance under low-bit quantization. Remarkably, on a
single GPU, we quantize a 405B language transformer as well as EVA-02, the rank
first vision transformer that achieves 90% pretraining Imagenet accuracy. Code
is available at Github.

</details>


### [237] [Towards More Efficient, Robust, Instance-adaptive, and Sequential Decision making](https://arxiv.org/pdf/2504.09192)
*Zhiyong Wang*

Main category: cs.LG

TL;DR: Developing efficient, robust, and adaptive algorithms for sequential decision-making in RL and bandits, addressing real-world challenges like model misspecifications and adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms often rely on idealized models, failing in real-world scenarios with uncertainty, adversarial conditions, or lack of prior knowledge. Robust, adaptive, and instance-dependent solutions are needed.

Method: Focuses on reinforcement learning (RL) and multi-armed bandits, aiming to improve efficiency, robustness, instance-adaptivity, and generalizability.

Result: Not explicitly stated, but the goal is to create algorithms that perform better in dynamic, uncertain, and adversarial environments.

Conclusion: The research aims to bridge the gap between theoretical guarantees and practical performance, enhancing the applicability of sequential decision-making methods in real-world settings.

Abstract: The primary goal of my Ph.D. study is to develop provably efficient and
practical algorithms for data-driven sequential decision-making under
uncertainty. My work focuses on reinforcement learning (RL), multi-armed
bandits, and their applications, including recommendation systems, computer
networks, video analytics, and large language models (LLMs). Sequential
decision-making methods, such as bandits and RL, have demonstrated remarkable
success - ranging from outperforming human players in complex games like Atari
and Go to advancing robotics, recommendation systems, and fine-tuning LLMs.
Despite these successes, many established algorithms rely on idealized models
that can fail under model misspecifications or adversarial perturbations,
particularly in settings where accurate prior knowledge of the underlying model
class is unavailable or where malicious users operate within dynamic systems.
These challenges are pervasive in real-world applications, where robust and
adaptive solutions are critical. Furthermore, while worst-case guarantees
provide theoretical reliability, they often fail to capture instance-dependent
performance, which can lead to more efficient and practical solutions. Another
key challenge lies in generalizing to new, unseen environments, a crucial
requirement for deploying these methods in dynamic and unpredictable settings.
To address these limitations, my research aims to develop more efficient,
robust, instance-adaptive, and generalizable sequential decision-making
algorithms for both reinforcement learning and bandits. Towards this end, I
focus on developing more efficient, robust, instance-adaptive, and
generalizable for both general reinforcement learning (RL) and bandits.

</details>


### [238] [Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition](https://arxiv.org/pdf/2504.11383)
*Wei Wang, Maryam Hakimzadeh, Haihui Ruan, Somdatta Goswami*

Main category: cs.LG

TL;DR: A hybrid framework combining PI-NO with finite element method (FE) via domain decomposition improves PDE solver efficiency, reducing computational costs and error accumulation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in balancing computational cost and accuracy for multiscale and dynamical PDE systems, leveraging neural operators (NOs) and FE methods.

Method: Integrates PI-NO with FE through domain decomposition, using Schwarz alternating method for coupling. NO handles complex regions, FE handles the rest, with embedded time-stepping and adaptive subdomain evolution.

Result: Validated across static, quasi-static, and dynamic regimes, showing 20% faster convergence and errors below 1%. Maintains solution continuity, reduces costs, and adapts to evolving phenomena.

Conclusion: The hybrid solver bridges numerical methods and AI, offering scalable, high-fidelity multiscale simulations.

Abstract: Numerical solvers for PDEs face challenges in balancing computational cost
and accuracy, particularly for multiscale and dynamical systems. Neural
operators (NOs) can significantly speed up simulations; however, they face
challenges such as error accumulation for dynamical systems and limited
generalization in multiphysics problems. This work introduces a novel hybrid
framework that integrates PI-NO with finite element method (FE) through domain
decomposition and leverages numerical analysis for time marching. The core
innovation lies in efficient coupling FE and NO subdomains via a Schwarz
alternating method: regions with complex, nonlinear, or high-gradient behavior
are resolved using a pretrained NO, while the remainder is handled by
conventional FE. To address the challenges of dynamic systems, we embed a
time-stepping scheme directly into the NO architecture, substantially reducing
long-term error propagation. Also, an adaptive subdomain evolution strategy
enables the ML resolved region to expand dynamically, capturing emerging fine
scale features without remeshing. The framework efficacy has been validated
across a range of problems, spanning static, quasi-static, and dynamic regimes
(e.g., linear elasticity, hyperelasticity, and elastodynamics), demonstrating
accelerated convergence (up to 20% improvement in convergence compared to
conventional FE coupling) while preserving solution fidelity with error margins
consistently below 1%. Our study shows that our hybrid solver: (1) maintains
solution continuity across subdomain interfaces, (2) reduces computational
costs by eliminating fine mesh requirements, (3) mitigates error accumulation
in time dependent simulations, and (4) enables automatic adaptation to evolving
physical phenomena. This work bridges the gap between numerical methods and
AI-driven surrogates, offering a scalable pathway for high-fidelity multiscale
simulations.

</details>


### [239] [Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design](https://arxiv.org/pdf/2505.07086)
*Tong Chen, Yinuo Zhang, Sophia Tang, Pranam Chatterjee*

Main category: cs.LG

TL;DR: MOG-DFM is a framework for multi-objective-guided biomolecule sequence design, improving upon existing discrete flow matching models by addressing multiple conflicting objectives.


<details>
  <summary>Details</summary>
Motivation: Existing methods for biomolecule engineering struggle with multiple conflicting objectives and often rely on continuous embeddings, which can distort discrete distributions.

Method: MOG-DFM combines hybrid rank-directional scoring and adaptive hypercone filtering to steer pretrained discrete flow matching models toward Pareto-efficient trade-offs.

Result: MOG-DFM successfully generates peptide binders and DNA sequences optimized for multiple properties, demonstrating its effectiveness in multi-property-guided design.

Conclusion: MOG-DFM is a powerful tool for designing biomolecules with balanced trade-offs across multiple functional and biophysical criteria.

Abstract: Designing biological sequences that satisfy multiple, often conflicting,
functional and biophysical criteria remains a central challenge in biomolecule
engineering. While discrete flow matching models have recently shown promise
for efficient sampling in high-dimensional sequence spaces, existing approaches
address only single objectives or require continuous embeddings that can
distort discrete distributions. We present Multi-Objective-Guided Discrete Flow
Matching (MOG-DFM), a general framework to steer any pretrained discrete flow
matching generator toward Pareto-efficient trade-offs across multiple scalar
objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional
score for candidate transitions and applies an adaptive hypercone filter to
enforce consistent multi-objective progression. We also trained two
unconditional discrete flow matching models, PepDFM for diverse peptide
generation and EnhancerDFM for functional enhancer DNA generation, as base
generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in
generating peptide binders optimized across five properties (hemolysis,
non-fouling, solubility, half-life, and binding affinity), and in designing DNA
sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM
proves to be a powerful tool for multi-property-guided biomolecule sequence
design.

</details>


### [240] [Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement](https://arxiv.org/pdf/2505.07961)
*Xuechen Zhang, Zijian Huang, Chenshun Ni, Ziyang Xiong, Jiasi Chen, Samet Oymak*

Main category: cs.LG

TL;DR: The paper proposes two methods, Temperature Scaling (TS) and TLDR, to improve token-efficient reasoning in small language models by controlling trace length and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Small language models often produce verbose and repetitive outputs due to inability to determine optimal stopping points in reasoning, leading to high computational costs.

Method: Two solutions are introduced: (1) Temperature Scaling (TS) to control trace length, and (2) TLDR, a length-regularized reinforcement learning method for multi-level trace length control.

Result: Experiments show TS outperforms budget forcing, and TLDR improves token efficiency by ~50% with minimal accuracy loss. TLDR also enables flexible response length control.

Conclusion: The work emphasizes the importance of stopping time control, identifies SFT limitations, and offers effective solutions for token-efficient reasoning in small models.

Abstract: Recent research enhances language model reasoning by scaling test-time
compute via longer chain-of-thought traces. This often improves accuracy but
also introduces redundancy and high computational cost, especially for small
language models distilled with supervised fine-tuning (SFT). In this work, we
propose new algorithms to improve token-efficient reasoning with small-scale
models by effectively trading off accuracy and computation. We first show that
the post-SFT model fails to determine the optimal stopping point of the
reasoning process, resulting in verbose and repetitive outputs. Verbosity also
significantly varies across wrong vs correct responses. To address these
issues, we propose two solutions: (1) Temperature scaling (TS) to control the
stopping point for the thinking phase and thereby trace length, and (2) TLDR: a
length-regularized reinforcement learning method based on GRPO that facilitates
multi-level trace length control (e.g. short, medium, long reasoning).
Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and
OlympiadBench, demonstrate that TS is highly effective compared to s1's budget
forcing approach and TLDR significantly improves token efficiency by about 50%
with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also
facilitates flexible control over the response length, offering a practical and
effective solution for token-efficient reasoning in small models. Ultimately,
our work reveals the importance of stopping time control, highlights
shortcomings of pure SFT, and provides effective algorithmic recipes.

</details>


### [241] [SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness](https://arxiv.org/pdf/2505.08320)
*Yoonhyuk Choi, Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SpecSphere is a dual-pass spectral-spatial GNN offering certified robustness, adaptability to homophily-heterophily, and expressive power beyond 1-WL, with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To create a GNN that combines high expressivity, adaptability, and provable robustness in a scalable architecture.

Method: Couples a Chebyshev-polynomial spectral branch with an attention-gated spatial branch, fused via a lightweight MLP trained in a min-max game.

Result: Achieves SOTA node-classification accuracy, tighter robustness guarantees, and theoretical advancements.

Conclusion: SpecSphere proves that expressivity, adaptability, and robustness can coexist in a scalable GNN.

Abstract: We introduce SpecSphere, the first dual-pass spectral-spatial GNN that
certifies every prediction against both $\ell\_{0}$ edge flips and
$\ell\_{\infty}$ feature perturbations, adapts to the full
homophily-heterophily spectrum, and surpasses the expressive power of
1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a
Chebyshev-polynomial spectral branch with an attention-gated spatial branch and
fuses their representations through a lightweight MLP trained in a
cooperative-adversarial min-max game. We further establish (i) a uniform
Chebyshev approximation theorem, (ii) minimax-optimal risk across the
homophily-heterophily spectrum, (iii) closed-form robustness certificates, and
(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves
state-of-the-art node-classification accuracy and delivers tighter certified
robustness guarantees on real-world benchmarks. These results demonstrate that
high expressivity, heterophily adaptation, and provable robustness can coexist
within a single, scalable architecture.

</details>


### [242] [OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain](https://arxiv.org/pdf/2505.08550)
*Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi*

Main category: cs.LG

TL;DR: OLinear is a linear-based multivariate time series forecasting model using an orthogonal transformation (OrthoTrans) for decorrelated feature encoding/decoding, outperforming traditional methods with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional temporal forecast (TF) models struggle with entangled step-wise dependencies in time series. Fixed transformations (e.g., Fourier) lack adaptability, prompting the need for a data-adaptive orthogonal transformation.

Method: OLinear uses OrthoTrans, a data-adaptive orthogonal matrix, to decorrelate features. It also introduces NormLin, a normalized linear layer, to capture multivariate dependencies efficiently.

Result: OLinear achieves state-of-the-art performance on 24 benchmarks and 140 tasks. NormLin outperforms multi-head self-attention with fewer FLOPs and enhances Transformer-based models.

Conclusion: OLinear offers an efficient, adaptable solution for multivariate time series forecasting, with NormLin proving superior to self-attention in performance and computational cost.

Abstract: This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-based
multivariate time series forecasting model that operates in an
$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically
adopt the temporal forecast (TF) paradigm, which directly encode and decode
time series in the time domain. However, the entangled step-wise dependencies
in series data can hinder the performance of TF. To address this, some
forecasters conduct encoding and decoding in the transformed domain using
fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier
transform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptive
transformation based on an orthogonal matrix that diagonalizes the series'
temporal Pearson correlation matrix. This approach enables more effective
encoding and decoding in the decorrelated feature domain and can serve as a
plug-in module to enhance existing forecasters. To enhance the representation
learning for multivariate time series, we introduce a customized linear layer,
$\mathbf{NormLin}$, which employs a normalized weight matrix to capture
multivariate dependencies. Empirically, the NormLin module shows a surprising
performance advantage over multi-head self-attention, while requiring nearly
half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting
tasks demonstrate that OLinear consistently achieves state-of-the-art
performance with high efficiency. Notably, as a plug-in replacement for
self-attention, the NormLin module consistently enhances Transformer-based
forecasters. The code and datasets are available at
https://anonymous.4open.science/r/OLinear

</details>


### [243] [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://arxiv.org/pdf/2505.08740)
*Abdolmehdi Behroozi, Chaopeng Shen and, Daniel Kifer*

Main category: cs.LG

TL;DR: SC-FNO improves FNO by adding sensitivity-based regularization, enhancing accuracy, scalability, and efficiency in solving parametric differential equations.


<details>
  <summary>Details</summary>
Motivation: Standard FNO struggles with inverse problems, sensitivity estimation, and concept drift in parametric differential equations.

Method: Introduces Sensitivity-Constrained Fourier Neural Operators (SC-FNO) with sensitivity-based regularization.

Result: SC-FNO outperforms standard FNO, improves parameter inversion, scales to high dimensions, and reduces data/training needs.

Conclusion: SC-FNO is a robust solution for parametric differential equations, generalizing well across various scenarios.

Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are
fundamental in science and engineering. While deep learning frameworks such as
the Fourier Neural Operator (FNO) can efficiently approximate solutions, they
struggle with inverse problems, sensitivity estimation (du/dp), and concept
drift. We address these limitations by introducing a sensitivity-based
regularization strategy, called Sensitivity-Constrained Fourier Neural
Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths
and consistently outperforms standard FNO and FNO with physics-informed
regularization. It improves performance in parameter inversion tasks, scales to
high-dimensional parameter spaces (tested with up to 82 parameters), and
reduces both data and training requirements. These gains are achieved with a
modest increase in training time (30% to 130% per epoch) and generalize across
various types of differential equations and neural operators. Code and selected
experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [244] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.08825)
*Pedro Antonio Alarcon Granadeno, Theodore Chambers, Jane Cleland-Huang*

Main category: cs.MA

TL;DR: A MARL algorithm using sUAS swarms for pollution source localization outperforms traditional methods by exploring only 1.29% of the environment.


<details>
  <summary>Details</summary>
Motivation: Industrial disasters highlight the need for reliable plume tracing. Traditional methods fail in turbulent conditions.

Method: Uses MARL with a POMG framework and LSTM-based ADDRQN, incorporating action histories and realistic simulations.

Result: The algorithm locates pollution sources by exploring just 1.29% of the environment, outperforming conventional methods.

Conclusion: The proposed MARL approach is effective for pollution source localization in complex, turbulent environments.

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [245] [Streaming Multi-agent Pathfinding](https://arxiv.org/pdf/2505.09472)
*Mingkai Tang, Lu Gan, Kaichen Zhang*

Main category: cs.MA

TL;DR: The paper introduces Streaming MAPF (S-MAPF) for periodic assembly line scenarios, proposing ASCBS, a solver that outperforms traditional MAPF methods in runtime for long-hour tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional MAPF is unsuitable for periodic, long-hour assembly line scenarios, necessitating a new approach.

Method: The study formalizes S-MAPF and develops ASCBS, incorporating cyclic constraints and disjoint splitting to handle conflicts.

Result: ASCBS outperforms traditional MAPF solvers in runtime for prolonged working hours.

Conclusion: ASCBS is effective for periodic, long-duration MAPF tasks, offering a viable solution for assembly line scenarios.

Abstract: The task of the multi-agent pathfinding (MAPF) problem is to navigate a team
of agents from their start point to the goal points. However, this setup is
unsuitable in the assembly line scenario, which is periodic with a long working
hour. To address this issue, the study formalizes the streaming MAPF (S-MAPF)
problem, which assumes that the agents in the same agent stream have a periodic
start time and share the same action sequence. The proposed solution, Agent
Stream Conflict-Based Search (ASCBS), is designed to tackle this problem by
incorporating a cyclic vertex/edge constraint to handle conflicts.
Additionally, this work explores the potential usage of the disjoint splitting
strategy within ASCBS. Experimental results indicate that ASCBS surpasses
traditional MAPF solvers in terms of runtime for scenarios with prolonged
working hours.

</details>


### [246] [Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation](https://arxiv.org/pdf/2405.18044)
*Jiaqi Shao, Tianjun Yuan, Tao Lin, Bing Luo*

Main category: cs.MA

TL;DR: Agents with higher Theory of Mind (ToM) don't always cooperate better. A new matching coalition mechanism aligns beliefs and abilities to improve cooperation.


<details>
  <summary>Details</summary>
Motivation: To enhance cooperation in multi-agent systems by leveraging ToM despite its limitations.

Method: Proposes a matching coalition mechanism considering belief alignment and specialized abilities.

Result: Stable coalitions maximize cooperation and system performance.

Conclusion: Incorporating ToM in multi-agent systems improves coordination and cooperation.

Abstract: Cognitive abilities, such as Theory of Mind (ToM), play a vital role in
facilitating cooperation in human social interactions. However, our study
reveals that agents with higher ToM abilities may not necessarily exhibit
better cooperative behavior compared to those with lower ToM abilities. To
address this challenge, we propose a novel matching coalition mechanism that
leverages the strengths of agents with different ToM levels by explicitly
considering belief alignment and specialized abilities when forming coalitions.
Our proposed matching algorithm seeks to find stable coalitions that maximize
the potential for cooperative behavior and ensure long-term viability. By
incorporating cognitive insights into the design of multi-agent systems, our
work demonstrates the potential of leveraging ToM to create more sophisticated
and human-like coordination strategies that foster cooperation and improve
overall system performance.

</details>


### [247] [Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis](https://arxiv.org/pdf/2504.12777)
*James Rudd-Jones, Mirco Musolesi, MarÃ­a PÃ©rez-Ortiz*

Main category: cs.MA

TL;DR: The paper proposes using Multi-Agent Reinforcement Learning (MARL) to enhance climate policy synthesis, addressing challenges like non-linear dynamics and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Climate policy development is hindered by uncertainty, complex dynamics, and stakeholder conflicts. Traditional methods like Earth System Models are limited to policy evaluation, not synthesis.

Method: The framework integrates MARL with climate simulations to optimize policy pathways, tackling issues like reward definition, scalability, and uncertainty propagation.

Result: The approach offers a foundation for advanced climate policy exploration but highlights limitations in interpretability and validation.

Conclusion: MARL-augmented climate simulations can improve policy synthesis, though further research is needed to address interpretability and scalability challenges.

Abstract: Climate policy development faces significant challenges due to deep
uncertainty, complex system dynamics, and competing stakeholder interests.
Climate simulation methods, such as Earth System Models, have become valuable
tools for policy exploration. However, their typical use is for evaluating
potential polices, rather than directly synthesizing them. The problem can be
inverted to optimize for policy pathways, but the traditional optimization
approaches often struggle with non-linear dynamics, heterogeneous agents, and
comprehensive uncertainty quantification. We propose a framework for augmenting
climate simulations with Multi-Agent Reinforcement Learning (MARL) to address
these limitations. We identify key challenges at the interface between climate
simulations and the application of MARL in the context of policy synthesis,
including reward definition, scalability with increasing agents and state
spaces, uncertainty propagation across linked systems, and solution validation.
Additionally, we discuss challenges in making MARL-derived solutions
interpretable and useful for policy-makers. Our framework provides a foundation
for more sophisticated climate policy exploration while acknowledging important
limitations and areas for future research.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [248] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/pdf/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

TL;DR: The paper proposes extensions to the Media Over QUIC Transport protocol for real-time content moderation in live video streams, removing only objectionable segments while minimizing latency.


<details>
  <summary>Details</summary>
Motivation: The rise of live video streaming on social media necessitates robust, low-latency content moderation to filter dangerous or illegal content.

Method: Extensions to the Media Over QUIC Transport protocol enable real-time moderation by distributing analysis tasks to client devices and removing only non-compliant video segments.

Result: The system adds minimal latency (one group-of-pictures duration) and successfully resumes playback once content complies with policies.

Conclusion: The proposed solution effectively balances real-time moderation with low-latency streaming, demonstrated in strobe removal for photosensitive viewers.

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


### [249] [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/pdf/2402.00045)
*Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu*

Main category: cs.MM

TL;DR: This paper presents the first comprehensive survey on detecting multimedia generated by Large AI Models (LAIMs), introducing a novel taxonomy for detection methods and addressing societal impacts and future research directions.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated multimedia poses risks like misuse and ethical concerns, yet lacks systematic surveys on detection methods, prompting this study.

Method: The survey categorizes detection methods by media modality and perspectives (pure detection and beyond detection), while also reviewing generation mechanisms, datasets, tools, and metrics.

Result: A novel taxonomy for detection methods is introduced, alongside insights into societal impacts and current challenges in detecting LAIM-generated content.

Conclusion: The survey fills an academic gap, aids global AI security, and proposes future research directions to address emerging issues in LAIM-generated multimedia detection.

Abstract: The rapid advancement of Large AI Models (LAIMs), particularly diffusion
models and large language models, has marked a new era where AI-generated
multimedia is increasingly integrated into various aspects of daily life.
Although beneficial in numerous fields, this content presents significant
risks, including potential misuse, societal disruptions, and ethical concerns.
Consequently, detecting multimedia generated by LAIMs has become crucial, with
a marked rise in related research. Despite this, there remains a notable gap in
systematic surveys that focus specifically on detecting LAIM-generated
multimedia. Addressing this, we provide the first survey to comprehensively
cover existing research on detecting multimedia (such as text, images, videos,
audio, and multimodal content) created by LAIMs. Specifically, we introduce a
novel taxonomy for detection methods, categorized by media modality, and
aligned with two perspectives: pure detection (aiming to enhance detection
performance) and beyond detection (adding attributes like generalizability,
robustness, and interpretability to detectors). Additionally, we have presented
a brief overview of generation mechanisms, public datasets, online detection
tools, and evaluation metrics to provide a valuable resource for researchers
and practitioners in this field. Most importantly, we offer a focused analysis
from a social media perspective to highlight their broader societal impact.
Furthermore, we identify current challenges in detection and propose directions
for future research that address unexplored, ongoing, and emerging issues in
detecting multimedia generated by LAIMs. Our aim for this survey is to fill an
academic gap and contribute to global AI security efforts, helping to ensure
the integrity of information in the digital realm. The project link is
https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.

</details>


### [250] [Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G](https://arxiv.org/pdf/2504.17938)
*Raza Ul Mustafa, Sesha Dassanayake, Noman Ashraf*

Main category: cs.MM

TL;DR: The paper explores how channel metrics (RSRP, RSRQ, SNR) correlate with YouTube video quality shifts, proposing ML classifiers to predict shifts and improve QoE.


<details>
  <summary>Details</summary>
Motivation: To enhance YouTube streaming QoE by reducing resolution shifts, moving beyond traditional QoS metrics.

Method: Analyzed relationship between quality shifts and channel metrics; used ML classifiers for prediction.

Result: Found positive correlation between channel metrics and shifts; achieved 77% accuracy in predicting shifts.

Conclusion: Proposed method can improve OTT services, especially in 5G networks, by leveraging channel metrics for better QoE.

Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [251] [Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](https://arxiv.org/pdf/2505.09439)
*Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass*

Main category: eess.AS

TL;DR: Omni-R1 fine-tunes Qwen2.5-Omni with GRPO on an audio QA dataset, achieving SOTA on MMAU. Performance gains stem from improved text reasoning, and text-only fine-tuning boosts audio performance.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-modal LLM performance on audio QA tasks and explore the impact of reinforcement learning (GRPO) and text-based reasoning.

Method: Fine-tuned Qwen2.5-Omni with GRPO on an audio QA dataset, tested with/without audio, and explored text-only fine-tuning.

Result: Achieved SOTA on MMAU, highest accuracies in sound, music, speech, and overall. Text reasoning and text-only fine-tuning improved audio performance.

Conclusion: GRPO and text-based reasoning significantly boost performance, and text-only fine-tuning unexpectedly enhances audio QA capabilities.

Abstract: We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,
on an audio question answering dataset with the reinforcement learning method
GRPO. This leads to new State-of-the-Art performance on the recent MMAU
benchmark. Omni-R1 achieves the highest accuracies on the sounds, music,
speech, and overall average categories, both on the Test-mini and Test-full
splits. To understand the performance improvement, we tested models both with
and without audio and found that much of the performance improvement from GRPO
could be attributed to better text-based reasoning. We also made a surprising
discovery that fine-tuning without audio on a text-only dataset was effective
at improving the audio-based performance.

</details>


### [252] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/pdf/2505.09558)
*Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang, Jinzheng He, Yifu Chen, Zhengqing Liu, Ziyue Jiang, Xize Cheng, Siqi Zheng, Jin Xu, Junyang Lin, Zhou Zhao*

Main category: eess.AS

TL;DR: The paper introduces WavReward, an audio-based reward feedback model for evaluating spoken dialogue systems, outperforming existing methods in accuracy and subjective testing.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for spoken dialogue models overlook non-textual information, prompting the need for an audio-based evaluation framework.

Method: WavReward uses audio language models with deep reasoning and nonlinear reward mechanisms, trained via reinforcement learning on the ChatReward-30K dataset.

Result: WavReward achieves 91.5% objective accuracy (vs. 55.1% for Qwen2.5-Omni) and 83% preference in subjective tests.

Conclusion: WavReward effectively evaluates spoken dialogue systems, with ablation studies validating its components; data and code will be publicly released.

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [253] [Granite-speech: open-source speech-aware LLMs with strong English ASR capabilities](https://arxiv.org/pdf/2505.08699)
*George Saon, Avihu Dekel, Alexander Brooks, Tohru Nagano, Abraham Daniels, Aharon Satt, Ashish Mittal, Brian Kingsbury, David Haws, Edmilson Morais, Gakuto Kurata, Hagai Aronowitz, Ibrahim Ibrahim, Jeff Kuo, Kate Soule, Luis Lastras, Masayuki Suzuki, Ron Hoory, Samuel Thomas, Sashi Novitasari, Takashi Fukuda, Vishal Sunder, Xiaodong Cui, Zvi Kons*

Main category: eess.AS

TL;DR: Granite-speech LLMs are efficient models for English ASR and AST, outperforming competitors with less data and supporting multiple languages.


<details>
  <summary>Details</summary>
Motivation: To create compact, efficient speech language models for ASR and AST tasks, leveraging publicly available data.

Method: Modality alignment of granite-3.3-instruct variants, using a conformer encoder, transformer adapter, and LoRA fine-tuning.

Result: Outperforms competitors in English ASR and matches performance in AST for major languages.

Conclusion: Granite-speech models are effective, freely available, and versatile for both speech and text tasks.

Abstract: Granite-speech LLMs are compact and efficient speech language models
specifically designed for English ASR and automatic speech translation (AST).
The models were trained by modality aligning the 2B and 8B parameter variants
of granite-3.3-instruct to speech on publicly available open-source corpora
containing audio inputs and text targets consisting of either human transcripts
for ASR or automatically generated translations for AST. Comprehensive
benchmarking shows that on English ASR, which was our primary focus, they
outperform several competitors' models that were trained on orders of magnitude
more proprietary data, and they keep pace on English-to-X AST for major
European languages, Japanese, and Chinese. The speech-specific components are:
a conformer acoustic encoder using block attention and self-conditioning
trained with connectionist temporal classification, a windowed
query-transformer speech modality adapter used to do temporal downsampling of
the acoustic embeddings and map them to the LLM text embedding space, and LoRA
adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two
modes: in speech mode, it performs ASR and AST by activating the encoder,
projector, and LoRA adapters; in text mode, it calls the underlying
granite-3.3-instruct model directly (without LoRA), essentially preserving all
the text LLM capabilities and safety. Both models are freely available on
HuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and
https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for
both research and commercial purposes under a permissive Apache 2.0 license.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [254] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/pdf/2505.08798)
*Mobina Shrestha, Bishwas Mandal, Vishal Mandal, Asis Shrestha*

Main category: eess.IV

TL;DR: The paper explores in-context learning (ICL) as an alternative to retraining AI models for oncology tasks, showing competitive performance with few-shot prompting across multiple vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of AI in oncology, such as reliance on large annotated datasets and the need for retraining, by leveraging ICL for adaptability with minimal labeled examples.

Method: Evaluated four vision-language models (VLMs) on three oncology datasets using few-shot prompting without parameter updates.

Result: GPT-4o achieved F1 scores of 0.81 (binary) and 0.60 (multi-class), while open-source models like Paligemma and CLIP showed competitive performance despite smaller size.

Conclusion: ICL is a practical solution for oncology, especially in resource-limited settings, as it approximates task-specific behavior with minimal data.

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [255] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/pdf/2505.08819)
*Asahi Miyazaki, Tsuyoshi Okita*

Main category: eess.IV

TL;DR: The paper introduces a new mask pattern, Mesh Mask, for the SparK model in self-supervised learning, analyzing its impact on performance.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of the SparK model by exploring the effect of different mask patterns in pre-training.

Method: Proposes the Mesh Mask-ed SparK model, a new mask pattern for image masking during pre-training.

Result: Reports the influence of the mask pattern on the model's performance.

Conclusion: The Mesh Mask pattern enhances the SparK model's effectiveness in self-supervised learning.

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [256] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/pdf/2505.08838)
*Peixuan Ge, Tongkun Su, Faqin Lv, Baoliang Zhao, Peng Zhang, Chi Hong Wong, Liang Yao, Yu Sun, Zenan Wang, Pak Kin Wong, Ying Hu*

Main category: eess.IV

TL;DR: A unified framework for multi-organ and multilingual ultrasound report generation is proposed, improving accuracy and reducing errors compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The variability of ultrasound images, operator dependence, and lack of standardized datasets make automated report generation challenging.

Method: The framework integrates fragment-based multilingual training, aligns modular text fragments with imaging data, and uses a bilingual English-Chinese dataset. Fine-tuning with selective unfreezing of ViT improves text-image alignment.

Result: Achieves relative gains of 2% in BLEU, 3% in ROUGE-L, and 15% in CIDEr, reducing errors like missing or incorrect content.

Conclusion: The framework shows strong potential for real-world clinical workflows by unifying multi-organ and multi-language report generation.

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [257] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/pdf/2505.08843)
*Marco Corrias, Giada Franceschi, Michele Riva, Alberto Tampieri, Karin FÃ¶ttinger, Ulrike Diebold, Thomas Pock, Cesare Franchini*

Main category: eess.IV

TL;DR: The paper proposes a TV-based workflow for denoising and decomposing microscopy images, evaluating TV-L1, Huber-ROF, and TGV-L1 methods. Huber-ROF is most flexible, while TGV-L1 excels in denoising.


<details>
  <summary>Details</summary>
Motivation: Microscopy images are degraded by noise, necessitating modern denoising solutions for high-speed acquisition.

Method: The approach involves decomposing images to extract unwanted signals or denoising them using TV-based methods (TV-L1, Huber-ROF, TGV-L1).

Result: Huber-ROF is the most flexible, and TGV-L1 is best for denoising. The method is applicable beyond STM, AFM, and SEM.

Conclusion: The workflow is widely applicable in microscopy, with publicly available Python code (AiSurf) for integration into experimental workflows.

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


### [258] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/pdf/2505.08845)
*Misgina Tsighe Hagos, Antti Suutala, Dmitrii Bychkov, Hakan KÃ¼cÃ¼kel, Joar von Bahr, Milda Poceviciute, Johan Lundin, Nina Linder, Claes LundstrÃ¶m*

Main category: eess.IV

TL;DR: The paper evaluates conformal prediction for cervical cancer classification, highlighting its limitations in alignment with human expectations and proposing expert-annotation-based validation.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for cervical cancer screening lack reliable uncertainty measures, and conformal prediction, while promising, often misaligns with human expectations.

Method: Three conformal prediction approaches are applied to three deep-learning models for cervical atypia classification, validated using expert annotations.

Result: Coverage-based evaluations overestimate performance, and current methods produce prediction sets misaligned with human labels.

Conclusion: Conformal prediction needs refinement to better align with human expectations and improve reliability in clinical settings.

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [259] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/pdf/2505.09193)
*Wei Jiang, Junru Li, Kai Zhang, Li Zhang*

Main category: eess.IV

TL;DR: BiECVC, a learned bidirectional video compression framework, outperforms VTM 13.2 by enhancing local and non-local context modeling and adaptive gating.


<details>
  <summary>Details</summary>
Motivation: Existing bidirectional video compression (BVC) methods lag behind forward-only ones due to limited context extraction and adaptability.

Method: BiECVC uses local feature reuse, linear attention for non-local dependencies, and bidirectional context gating for dynamic filtering.

Result: BiECVC reduces bit-rate by 13.4% and 15.7% compared to VTM 13.2 under RA configuration.

Conclusion: BiECVC is the first learned video codec to surpass VTM 13.2 RA, setting a new benchmark.

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead.To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [260] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/pdf/2505.09323)
*Pengli Zhu, Yingji Fu, Nanguang Chen, Anqi Qiu*

Main category: eess.IV

TL;DR: Q-CATN is a novel network for synthesizing MS-HARDI DWI from flexible q-space sampling, using structural MRI data and collaborative attention. It outperforms existing methods in accuracy and detail preservation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed q-space sampling schemes in DWI synthesis and leverage structural MRI data for improved multi-modal information extraction.

Method: Q-CATN uses a collaborative attention mechanism to dynamically adjust representations based on flexible q-space sampling and incorporates task-specific constraints for anatomical fidelity.

Result: Q-CATN surpasses existing methods (1D-qDL, 2D-qDL, MESC-SD, QGAN) in estimating parameter maps and fiber tracts, preserving fine details.

Conclusion: Q-CATN is a versatile tool for clinical and research applications, accommodating flexible q-space sampling effectively.

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [261] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/pdf/2505.09334)
*Sadman Sakib Alif, Nasim Anzum Promise, Fiaz Al Abid, Aniqua Nusrat Zereen*

Main category: eess.IV

TL;DR: The paper proposes a knowledge distillation-based approach for lung cancer detection, using lightweight models and explainable AI to improve efficiency and transparency.


<details>
  <summary>Details</summary>
Motivation: Early detection of lung cancer is critical, but deep learning models face challenges like high computational costs and lack of transparency, limiting their adoption in healthcare.

Method: The study evaluates eight CNN teacher models (e.g., ResNet50, EfficientNetB0) and develops a lightweight student model (DCSNet) using knowledge distillation and explainable AI techniques.

Result: The approach achieves high diagnostic performance while being suitable for resource-constrained environments and enhancing model transparency.

Conclusion: The proposed method facilitates the adoption of AI-driven diagnostic tools in healthcare by balancing efficiency, accuracy, and transparency.

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [262] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/pdf/2505.09521)
*Dongyi He, Shiyang Li, Bin Jiang, He Yan*

Main category: eess.IV

TL;DR: A lightweight EEG-to-fMRI generator, Spec2VolCAMU-Net, improves reconstruction quality and efficiency using a novel encoder-decoder architecture.


<details>
  <summary>Details</summary>
Motivation: High-resolution fMRI is costly and logistically challenging, while EEG is widely available. A reliable EEG-to-fMRI generator could democratize advanced neuroimaging.

Method: Proposes Spec2VolCAMU-Net with a Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net decoder, trained with a hybrid SSI-MSE loss.

Result: Achieves state-of-the-art fidelity on three benchmarks (NODDI, Oddball, CN-EPFL) with SSIM improvements of 14.5%, 14.9%, and 16.9%, and competitive PSNR scores.

Conclusion: The model is lightweight, efficient, and suitable for real-time applications, advancing EEG-to-fMRI reconstruction quality.

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [263] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/pdf/2505.09565)
*Maik Dannecker, Thomas Sanchez, Meritxell Bach Cuadra, ÃzgÃ¼n Turgut, Anthony N. Price, Lucilio Cordero-Grande, Vanessa Kyriakopoulou, Joseph V. Hajnal, Daniel Rueckert*

Main category: eess.IV

TL;DR: A novel SVR method using implicit neural representations improves MRI reconstruction quality and speed, especially for motion-corrupted data.


<details>
  <summary>Details</summary>
Motivation: Existing SVR methods struggle with artifacts and motion, requiring pre-alignment. The goal is to enable fast, accurate reconstruction despite severe corruption.

Method: Uses implicit neural representations for motion correction, outlier handling, and super-resolution. Initialized with task-specific priors via self-supervised meta-learning.

Result: Outperforms state-of-the-art in reconstruction quality and reduces time by up to 50%, validated on 480 simulated and clinical MRI datasets.

Conclusion: The proposed method effectively handles severe motion and artifacts, offering faster and more accurate MRI reconstruction.

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [264] [Error correcting 2D-3D cascaded network for myocardial infarct scar segmentation on late gadolinium enhancement cardiac magnetic resonance images](https://arxiv.org/pdf/2306.14725)
*Matthias Schwab, Mathias Pamminger, Christian Kremser, Daniel Obmann, Markus Haltmeier, Agnes Mayr*

Main category: eess.IV

TL;DR: A cascaded CNN framework automates LGE CMR image segmentation for infarct size and microvascular obstruction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient quantification of myocardial infarction markers in LGE CMR images is challenging due to complex patterns and manual delineation requirements.

Method: Proposes a cascaded 2D and 3D CNN framework, training with artificial segmentation errors to improve accuracy.

Result: Outperforms existing methods in segmentation accuracy, validated on public datasets.

Conclusion: The framework offers a robust, automated solution for myocardial infarction segmentation, with publicly available code.

Abstract: Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is
considered the in vivo reference standard for assessing infarct size (IS) and
microvascular obstruction (MVO) in ST-elevation myocardial infarction (STEMI)
patients. However, the exact quantification of those markers of myocardial
infarct severity remains challenging and very time-consuming. As LGE
distribution patterns can be quite complex and hard to delineate from the blood
pool or epicardial fat, automatic segmentation of LGE CMR images is
challenging. In this work, we propose a cascaded framework of two-dimensional
and three-dimensional convolutional neural networks (CNNs) which enables to
calculate the extent of myocardial infarction in a fully automated way. By
artificially generating segmentation errors which are characteristic for 2D
CNNs during training of the cascaded framework we are enforcing the detection
and correction of 2D segmentation errors and hence improve the segmentation
accuracy of the entire method. The proposed method was trained and evaluated on
two publicly available datasets. We perform comparative experiments where we
show that our framework outperforms state-of-the-art reference methods in
segmentation of myocardial infarction. Furthermore, in extensive ablation
studies we show the advantages that come with the proposed error correcting
cascaded method. The code of this project is publicly available at
https://github.com/matthi99/EcorC.git

</details>


### [265] [Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues](https://arxiv.org/pdf/2403.02043)
*Rui LourenÃ§o, Lucas Thomaz, Eduardo A. B. Silva, Sergio M. M. Faria*

Main category: eess.IV

TL;DR: A non-learning-based optimization method for light field depth estimation, leveraging 4D geometric cues, outperforms state-of-the-art in surface normal accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve depth estimation by explicitly using 4D geometric cues in light fields, addressing occlusion and surface normal accuracy.

Method: A non-learning-based optimization approach using a 4D geometric model to analyze intersections of 2D planes in 4D space.

Result: Achieves 26.3% lower Median Angle Error than state-of-the-art and competitive MSE and Badpix scores.

Conclusion: The method effectively exploits 4D geometry for accurate depth estimation, surpassing learning and non-learning benchmarks.

Abstract: Light field cameras and multi-camera arrays have emerged as promising
solutions for accurately estimating depth by passively capturing light
information. This is possible because the 3D information of a scene is embedded
in the 4D light field geometry. Commonly, depth estimation methods extract this
information relying on gradient information, heuristic-based optimisation
models, or learning-based approaches. This paper focuses mainly on explicitly
understanding and exploiting 4D geometrical cues for light field depth
estimation. Thus, a novel method is proposed, based on a non-learning-based
optimisation approach for depth estimation that explicitly considers surface
normal accuracy and occlusion regions by utilising a fully explainable 4D
geometric model of the light field. The 4D model performs depth/disparity
estimation by determining the orientations and analysing the intersections of
key 2D planes in 4D space, which are the images of 3D-space points in the 4D
light field. Experimental results show that the proposed method outperforms
both learning-based and non-learning-based state-of-the-art methods in terms of
surface normal angle accuracy, achieving a Median Angle Error on planar
surfaces, on average, 26.3$\%$ lower than the state-of-the-art, and still being
competitive with state-of-the-art methods in terms of MSE ${\times}$ 100 and
Badpix 0.07.

</details>


### [266] [Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures](https://arxiv.org/pdf/2404.06080)
*Ching-Kai Lin, Di-Chun Wei, Yun-Chien Cheng*

Main category: eess.IV

TL;DR: A few-shot learning model for lung metastasis detection in EBUS procedures, achieving 49.59% accuracy, improving to 55.48% with 20 samples.


<details>
  <summary>Details</summary>
Motivation: Early detection of lung metastases is challenging due to limited cytology images and cell similarities. Existing research lacks direct solutions.

Method: Proposes a few-shot learning model with hybrid pretrained backbone, fine-grained classification, and contrastive learning. Uses parameter-efficient fine-tuning on augmented support sets.

Result: Achieved 49.59% accuracy, rising to 55.48% with 20 samples, outperforming existing methods.

Conclusion: The model shows strong potential for rare cancer detection in low-data clinical settings.

Abstract: This study presents a computer-aided diagnosis (CAD) system to assist early
detection of lung metastases during endobronchial ultrasound (EBUS) procedures,
significantly reducing follow-up time and enabling timely treatment. Due to
limited cytology images and morphological similarities among cells, classifying
lung metastases is challenging, and existing research rarely targets this issue
directly.To overcome data scarcity and improve classification, the authors
propose a few-shot learning model using a hybrid pretrained backbone with
fine-grained classification and contrastive learning. Parameter-efficient
fine-tuning on augmented support sets enhances generalization and
transferability. The model achieved 49.59% accuracy, outperforming existing
methods. With 20 image samples, accuracy improved to 55.48%, showing strong
potential for identifying rare or novel cancer types in low-data clinical
environments.

</details>


### [267] [A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging](https://arxiv.org/pdf/2409.13498)
*Savvas Sifnaios, George Arvanitakis, Fotios K. Konstantinidis, Georgios Tsimiklis, Angelos Amditis, Panayiotis Frangos*

Main category: eess.IV

TL;DR: Combining hyperspectral imaging with deep learning achieves high accuracy in material classification, overcoming RGB limitations.


<details>
  <summary>Details</summary>
Motivation: RGB-based systems lack advanced object characterization needed in industries like waste sorting and pharmaceuticals. Hyperspectral imaging offers better spectral and spatial data.

Method: Designed an experimental setup with HS camera, generated a multi-object dataset, and developed a deep learning model for pixel-level material classification.

Result: Achieved 99.94% classification accuracy, robust to color, size, and shape variations, and handled material overlap well.

Conclusion: Hyperspectral imaging with deep learning is feasible and outperforms traditional methods, with strong potential for future applications.

Abstract: Recent advancements in computer vision, particularly in detection,
segmentation, and classification, have significantly impacted various domains.
However, these advancements are tied to RGB-based systems, which are
insufficient for applications in industries like waste sorting,
pharmaceuticals, and defense, where advanced object characterization beyond
shape or color is necessary. Hyperspectral (HS) imaging, capturing both
spectral and spatial information, addresses these limitations and offers
advantages over conventional technologies such as X-ray fluorescence and Raman
spectroscopy, particularly in terms of speed, cost, and safety.
  This study evaluates the potential of combining HS imaging with deep learning
for material characterization. The research involves: i) designing an
experimental setup with HS camera, conveyor, and controlled lighting; ii)
generating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with
semi-automated mask generation and Raman spectroscopy-based labeling; and iii)
developing a deep learning model trained on HS images for pixel-level material
classification. The model achieved 99.94\% classification accuracy,
demonstrating robustness in color, size, and shape invariance, and effectively
handling material overlap. Limitations, such as challenges with black objects,
are also discussed. Extending computer vision beyond RGB to HS imaging proves
feasible, overcoming major limitations of traditional methods and showing
strong potential for future applications.

</details>


### [268] [Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks](https://arxiv.org/pdf/2409.18872)
*Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho, Walter H. L. Pinaya, Daniel M. Lang, Julia A. Schnabel, Oliver Diaz, Karim Lekadir*

Main category: eess.IV

TL;DR: A method for virtual contrast enhancement in breast MRI using a generative adversarial network to predict DCE-MRI images from non-contrast MRIs, evaluated for tumor segmentation and temporal patterns.


<details>
  <summary>Details</summary>
Motivation: To provide a non-invasive alternative to traditional contrast agent-based DCE-MRI, reducing health risks for patients with contraindications.

Method: Uses a conditional generative adversarial network to predict DCE-MRI sequences from non-contrast MRIs, evaluated with a multi-metric Scaled Aggregate Measure (SAMe).

Result: Demonstrates promising results in generating realistic DCE-MRI sequences, useful for tumor localization and characterization.

Conclusion: Highlights the potential of virtual contrast enhancement for improving breast cancer diagnosis, especially for patients who cannot tolerate contrast agents.

Abstract: This paper presents a method for virtual contrast enhancement in breast MRI,
offering a promising non-invasive alternative to traditional contrast
agent-based DCE-MRI acquisition. Using a conditional generative adversarial
network, we predict DCE-MRI images, including jointly-generated sequences of
multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs,
enabling tumor localization and characterization without the associated health
risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic
DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe),
assessing their utility in a tumor segmentation downstream task, and conclude
with an analysis of the temporal patterns in multi-sequence DCE-MRI generation.
Our approach demonstrates promising results in generating realistic and useful
DCE-MRI sequences, highlighting the potential of virtual contrast enhancement
for improving breast cancer diagnosis and treatment, particularly for patients
where contrast agent administration is contraindicated.

</details>


### [269] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/pdf/2504.21227)
*Omid Halimi Milani, Amanda Nikho, Lauren Mills, Marouane Tliba, Ahmet Enis Cetin, Mohammed H. Elnagar*

Main category: eess.IV

TL;DR: A framework for verifying deep learning models in medical imaging ensures reliability by analyzing attention patterns, feature maps, and rejecting out-of-distribution inputs.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical imaging can produce unreliable predictions when applied to data different from their training sets, risking patient care.

Method: The framework uses Gradient Attention Maps (GAM) to analyze attention patterns, extends verification to early convolutional feature maps, and adds a garbage class to reject out-of-distribution inputs.

Result: The combined methods effectively identify unsuitable models and inputs, enhancing reliability.

Conclusion: The proposed framework promotes safer and more reliable deployment of deep learning in medical imaging.

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>


### [270] [Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs](https://arxiv.org/pdf/2505.01742)
*Yu Mao, Jingzong Li, Jun Wang, Hong Xu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue*

Main category: eess.IV

TL;DR: Easz is a transformer-based edge-compute-free image coding framework that shifts computational overhead to servers, improving adaptability, efficiency, and reconstruction quality for edge devices.


<details>
  <summary>Details</summary>
Motivation: Neural image compression faces challenges in edge devices due to heavy encode-decode structures and inflexibility in switching compression levels.

Method: Easz uses a patch-erase algorithm and conditional uniform-based sampler to remove image contents, with a transformer-based framework for reconstruction. A lightweight transformer reduces receiver-side load.

Result: Easz outperforms existing methods in adaptability, computational efficiency, and image reconstruction quality.

Conclusion: Easz effectively addresses edge device limitations by offloading computation to servers, offering a practical solution for neural image compression.

Abstract: Neural image compression, necessary in various machine-to-machine
communication scenarios, suffers from its heavy encode-decode structures and
inflexibility in switching between different compression levels. Consequently,
it raises significant challenges in applying the neural image compression to
edge devices that are developed for powerful servers with high computational
and storage capacities. We take a step to solve the challenges by proposing a
new transformer-based edge-compute-free image coding framework called Easz.
Easz shifts the computational overhead to the server, and hence avoids the
heavy encoding and model switching overhead on the edge. Easz utilizes a
patch-erase algorithm to selectively remove image contents using a conditional
uniform-based sampler. The erased pixels are reconstructed on the receiver side
through a transformer-based framework. To further reduce the computational
overhead on the receiver, we then introduce a lightweight transformer-based
reconstruction structure to reduce the reconstruction load on the receiver
side. Extensive evaluations conducted on a real-world testbed demonstrate
multiple advantages of Easz over existing compression approaches, in terms of
adaptability to different compression levels, computational efficiency, and
image reconstruction quality.

</details>
