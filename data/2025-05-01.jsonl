{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012", "abs": "https://arxiv.org/abs/2504.21012", "authors": ["Makoto Sato"], "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)--either fused together or presented separately--by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept--a form of conceptual fusion--current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds."}
{"id": "2504.21013", "pdf": "https://arxiv.org/pdf/2504.21013", "abs": "https://arxiv.org/abs/2504.21013", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Lionel Prevost", "Jérôme Da-Rugna"], "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5", "summary": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment."}
{"id": "2504.21016", "pdf": "https://arxiv.org/pdf/2504.21016", "abs": "https://arxiv.org/abs/2504.21016", "authors": ["Ngoc C. Lê", "Hai-Chung Nguyen-Phung", "Thu-Huong Pham Thi", "Hue Vu", "Phuong-Thao Nguyen Thi", "Thu-Thuy Tran", "Hong-Nhung Le Thi", "Thuy-Duong Nguyen-Thi", "Thanh-Huy Nguyen"], "title": "Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. AI4SG-21 The 3rd Workshop on Artificial Intelligence for\n  Social Good at IJCAI 2021", "summary": "The COVID-19 pandemic caused great losses worldwide, efforts are taken place\nto prevent but many countries have failed. In Vietnam, the traceability,\nlocalization, and quarantine of people who contact with patients contribute to\neffective disease prevention. However, this is done by hand, and take a lot of\nwork. In this research, we describe a named-entity recognition (NER) study that\nassists in the prevention of COVID-19 pandemic in Vietnam. We also present our\nmanually annotated COVID-19 dataset with nested named entity recognition task\nfor Vietnamese which be defined new entity types using for our system."}
{"id": "2504.21017", "pdf": "https://arxiv.org/pdf/2504.21017", "abs": "https://arxiv.org/abs/2504.21017", "authors": ["Hai-Chung Nguyen-Phung", "Ngoc C. Lê", "Van-Chien Nguyen", "Hang Thi Nguyen", "Thuy Phuong Thi Nguyen"], "title": "ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. Technical report", "summary": "After two years of appearance, COVID-19 has negatively affected people and\nnormal life around the world. As in May 2022, there are more than 522 million\ncases and six million deaths worldwide (including nearly ten million cases and\nover forty-three thousand deaths in Vietnam). Economy and society are both\nseverely affected. The variant of COVID-19, Omicron, has broken disease\nprevention measures of countries and rapidly increased number of infections.\nResources overloading in treatment and epidemics prevention is happening all\nover the world. It can be seen that, application of artificial intelligence\n(AI) to support people at this time is extremely necessary. There have been\nmany studies applying AI to prevent COVID-19 which are extremely useful, and\nstudies on machine reading comprehension (MRC) are also in it. Realizing that,\nwe created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and\ncan be used to build models and systems, contributing to disease prevention.\nBesides, ViQA-COVID is also the first multi-span extraction MRC dataset for\nVietnamese, we hope that it can contribute to promoting MRC studies in\nVietnamese and multilingual."}
{"id": "2504.21030", "pdf": "https://arxiv.org/pdf/2504.21030", "abs": "https://arxiv.org/abs/2504.21030", "authors": ["Naveen Krishnan"], "title": "Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Multi-agent systems represent a significant advancement in artificial\nintelligence, enabling complex problem-solving through coordinated specialized\nagents. However, these systems face fundamental challenges in context\nmanagement, coordination efficiency, and scalable operation. This paper\nintroduces a comprehensive framework for advancing multi-agent systems through\nModel Context Protocol (MCP), addressing these challenges through standardized\ncontext sharing and coordination mechanisms. We extend previous work on AI\nagent architectures by developing a unified theoretical foundation, advanced\ncontext management techniques, and scalable coordination patterns. Through\ndetailed implementation case studies across enterprise knowledge management,\ncollaborative research, and distributed problem-solving domains, we demonstrate\nsignificant performance improvements compared to traditional approaches. Our\nevaluation methodology provides a systematic assessment framework with\nbenchmark tasks and datasets specifically designed for multi-agent systems. We\nidentify current limitations, emerging research opportunities, and potential\ntransformative applications across industries. This work contributes to the\nevolution of more capable, collaborative, and context-aware artificial\nintelligence systems that can effectively address complex real-world\nchallenges."}
{"id": "2504.21528", "pdf": "https://arxiv.org/pdf/2504.21528", "abs": "https://arxiv.org/abs/2504.21528", "authors": ["Fredrik Cumlin", "Xinyu Liang", "Victor Ungureanu", "Chandan K. A. Reddy", "Christian Schüldt", "Saikat Chatterjee"], "title": "Impairments are Clustered in Latents of Deep Neural Network-based Speech Quality Models", "categories": ["eess.AS"], "comment": null, "summary": "In this article, we provide an experimental observation: Deep neural network\n(DNN) based speech quality assessment (SQA) models have inherent latent\nrepresentations where many types of impairments are clustered. While DNN-based\nSQA models are not trained for impairment classification, our experiments show\ngood impairment classification results in an appropriate SQA latent\nrepresentation. We investigate the clustering of impairments using various\nkinds of audio degradations that include different types of noises, waveform\nclipping, gain transition, pitch shift, compression, reverberation, etc. To\nvisualize the clusters we perform classification of impairments in the\nSQA-latent representation domain using a standard k-nearest neighbor (kNN)\nclassifier. We also develop a new DNN-based SQA model, named DNSMOS+, to\nexamine whether an improvement in SQA leads to an improvement in impairment\nclassification. The classification accuracy is 94% for LibriAugmented dataset\nwith 16 types of impairments and 54% for ESC-50 dataset with 50 types of real\nnoises."}
{"id": "2504.21171", "pdf": "https://arxiv.org/pdf/2504.21171", "abs": "https://arxiv.org/abs/2504.21171", "authors": ["Woongji Kim", "Beomseok Oh", "Chayeong Kim", "Wonkyu Moon"], "title": "Design, analysis, and experimental validation of a stepped plate parametric array loudspeaker", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "comment": "51 pages, 18 figures, arXiv:YYMM.NNNN(N) format preferred, submitted\n  to The Journal of the Acoustical Society of America (AIP)", "summary": "This study investigates the design and analysis of a stepped plate parametric\narray loudspeaker (SPPAL) as an alternative to conventional array-based\nparametric loudspeakers. The SPPAL utilizes a single Langevin-type ultrasonic\ntransducer coupled with a flexural stepped plate to generate narrow-beam\naudible sound via nonlinear acoustic interaction. To evaluate and optimize the\nperformance of the SPPAL, an integrated modeling framework is developed,\nconsisting of an approximate analytical 3D model for transducer dynamics, an\nequivalence ratio formulation to relate stepped plate and rigid piston\nbehavior, and a spherical wave expansion method for nonlinear sound field\nsimulation. The dual-resonance behavior of the transducer is optimized through\nmulti-objective analysis to enhance low-frequency audio performance.\nExperimental validation includes frequency response and modal analysis of the\ntransducer, as well as sound field measurements. The analytical methods are\nfurther verified through comparison with experimental data. Furthermore,\ncombination resonance--an unintended structural excitation resulting from\nintermodulation--is identified as an inherent phenomenon in SPPAL operation.\nThe findings offer practical guidance for the development of efficient,\ncompact, and manufacturable parametric array loudspeakers employing plate-based\nflexural vibration."}
{"id": "2504.21577", "pdf": "https://arxiv.org/pdf/2504.21577", "abs": "https://arxiv.org/abs/2504.21577", "authors": ["Zehao Chen", "Xinfeng Wei", "Haonan Tong", "Zhaohui Yang", "Changchuan Yin"], "title": "Latent Feature-Guided Conditional Diffusion for High-Fidelity Generative Image Semantic Communication", "categories": ["cs.MM"], "comment": "6 pages, 6 figures", "summary": "Semantic communication is proposed and expected to improve the efficiency and\neffectiveness of massive data transmission over sixth generation (6G) networks.\nHowever, existing deep learning-based joint source and channel coding\n(DeepJSCC) image semantic communication scheme predominantly focuses on\noptimizing pixel-level metrics, and neglects human perceptual requirements,\nwhich results in degraded perceptual quality. To address this issue, we propose\na latent representation-oriented image semantic communication (LRISC) system,\nwhich transmits latent semantic features for image generation with semantic\nconsistency, thereby ensuring the perceptual quality at the receiver. In\nparticular, we first map the source image to latent features in a\nhigh-dimensional semantic space via a neural network (NN)- based non-linear\ntransformation. Subsequently, these features are encoded using a joint source\nand channel coding (JSCC) scheme with adaptive coding length for efficient\ntransmission over a wireless channel. At the receiver, a conditional diffusion\nmodel is developed by using the received latent features as conditional\nguidance to steer the reverse diffusion process, progressively reconstructing\nhigh-fidelity images while preserving semantic consistency. Moreover, we\nintroduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing\none model to work across various channel states. Experiments show that the\nproposed method significantly outperforms existing methods, in terms of learned\nperceptual image patch similarity (LPIPS) and robustness against channel noise,\nwith an average LPIPS reduction of 43.3% compared to DeepJSCC, while\nguaranteeing the semantic consistency."}
{"id": "2504.21188", "pdf": "https://arxiv.org/pdf/2504.21188", "abs": "https://arxiv.org/abs/2504.21188", "authors": ["Natnael Alemayehu"], "title": "Light Weight CNN for classification of Brain Tumors from MRI Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages", "summary": "This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis."}
{"id": "2504.21047", "pdf": "https://arxiv.org/pdf/2504.21047", "abs": "https://arxiv.org/abs/2504.21047", "authors": ["Klemen Kotar", "Greta Tuckute"], "title": "Model Connectomes: A Generational Approach to Data-Efficient Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biological neural networks are shaped both by evolution across generations\nand by individual learning within an organism's lifetime, whereas standard\nartificial neural networks undergo a single, large training procedure without\ninherited constraints. In this preliminary work, we propose a framework that\nincorporates this crucial generational dimension - an \"outer loop\" of evolution\nthat shapes the \"inner loop\" of learning - so that artificial networks better\nmirror the effects of evolution and individual learning in biological\norganisms. Focusing on language, we train a model that inherits a \"model\nconnectome\" from the outer evolution loop before exposing it to a\ndevelopmental-scale corpus of 100M tokens. Compared with two closely matched\ncontrol models, we show that the connectome model performs better or on par on\nnatural language processing tasks as well as alignment to human behavior and\nbrain data. These findings suggest that a model connectome serves as an\nefficient prior for learning in low-data regimes - narrowing the gap between\nsingle-generation artificial models and biologically evolved neural networks."}
{"id": "2504.21131", "pdf": "https://arxiv.org/pdf/2504.21131", "abs": "https://arxiv.org/abs/2504.21131", "authors": ["Remo Christen", "Florian Pommerening", "Clemens Büchner", "Malte Helmert"], "title": "A Formalism for Optimal Search with Dynamic Heuristics", "categories": ["cs.AI"], "comment": null, "summary": "While most heuristics studied in heuristic search depend only on the state,\nsome accumulate information during search and thus also depend on the search\nhistory. Various existing approaches use such dynamic heuristics in\n$\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$\nto show optimality. However, doing so ignores the complexities of searching\nwith a mutable heuristic. In this paper we formalize the idea of dynamic\nheuristics and use them in a generic algorithm framework. We study a particular\ninstantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show\ngeneral optimality results. Finally we show how existing approaches from\nclassical planning can be viewed as special cases of this instantiation, making\nit possible to directly apply our optimality results."}
{"id": "2504.21040", "pdf": "https://arxiv.org/pdf/2504.21040", "abs": "https://arxiv.org/abs/2504.21040", "authors": ["Chenyi Cai", "Kosuke Kuriyama", "Youlong Gu", "Filip Biljecki", "Pieter Herthogs"], "title": "Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels", "categories": ["cs.CV"], "comment": null, "summary": "Urban street environments are vital to supporting human activity in public\nspaces. The emergence of big data, such as street view images (SVIs) combined\nwith multimodal large language models (MLLMs), is transforming how researchers\nand practitioners investigate, measure, and evaluate semantic and visual\nelements of urban environments. Considering the low threshold for creating\nautomated evaluative workflows using MLLMs, it is crucial to explore both the\nrisks and opportunities associated with these probabilistic models. In\nparticular, the extent to which the integration of expert knowledge can\ninfluence the performance of MLLMs in evaluating the quality of urban design\nhas not been fully explored. This study sets out an initial exploration of how\nintegrating more formal and structured representations of expert urban design\nknowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's\ncapability and reliability in evaluating the walkability of built environments\nusing SVIs. We collect walkability metrics from the existing literature and\ncategorize them using relevant ontologies. We then select a subset of these\nmetrics, focusing on the subthemes of pedestrian safety and attractiveness, and\ndevelop prompts for the MLLM accordingly. We analyze the MLLM's ability to\nevaluate SVI walkability subthemes through prompts with varying levels of\nclarity and specificity regarding evaluation criteria. Our experiments\ndemonstrate that MLLMs are capable of providing assessments and interpretations\nbased on general knowledge and can support the automation of multimodal\nimage-text evaluations. However, they generally provide more optimistic scores\nand can make mistakes when interpreting the provided metrics, resulting in\nincorrect evaluations. By integrating expert knowledge, the MLLM's evaluative\nperformance exhibits higher consistency and concentration."}
{"id": "2504.21018", "pdf": "https://arxiv.org/pdf/2504.21018", "abs": "https://arxiv.org/abs/2504.21018", "authors": ["Enes Özeren", "Yihong Liu", "Hinrich Schütze"], "title": "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, 15 tables", "summary": "Many pre-trained language models (PLMs) exhibit suboptimal performance on\nmid- and low-resource languages, largely due to limited exposure to these\nlanguages during pre-training. A common strategy to address this is to\nintroduce new tokens specific to the target languages, initialize their\nembeddings, and apply continual pre-training on target-language data. Among\nsuch methods, OFA (Liu et al., 2024a) proposes a similarity-based subword\nembedding initialization heuristic that is both effective and efficient.\nHowever, OFA restricts target-language token embeddings to be convex\ncombinations of a fixed number of source-language embeddings, which may limit\nexpressiveness. To overcome this limitation, we propose HYPEROFA, a\nhypernetwork-based approach for more adaptive token embedding initialization.\nThe hypernetwork is trained to map from an external multilingual word vector\nspace to the PLMs token embedding space using source-language tokens. Once\ntrained, it can generate flexible embeddings for target-language tokens,\nserving as a good starting point for continual pretraining. Experiments\ndemonstrate that HYPEROFA consistently outperforms random initialization\nbaseline and matches or exceeds the performance of OFA in both continual\npre-training convergence and downstream task performance. We make the code\npublicly available."}
{"id": "2504.21048", "pdf": "https://arxiv.org/pdf/2504.21048", "abs": "https://arxiv.org/abs/2504.21048", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for\nnumerous real-world applications, modeling distributed decision-making and\nlearning from interactions with complex environments. Resource Allocation\nOptimization (RAO) benefits significantly from MARL's ability to tackle dynamic\nand decentralized contexts. MARL-based approaches are increasingly applied to\nRAO challenges across sectors playing pivotal roles to Industry 4.0\ndevelopments. This survey provides a comprehensive review of recent MARL\nalgorithms for RAO, encompassing core concepts, classifications, and a\nstructured taxonomy. By outlining the current research landscape and\nidentifying primary challenges and future directions, this survey aims to\nsupport researchers and practitioners in leveraging MARL's potential to advance\nresource allocation solutions."}
{"id": "2504.21815", "pdf": "https://arxiv.org/pdf/2504.21815", "abs": "https://arxiv.org/abs/2504.21815", "authors": ["Huan Zhang", "Jinhua Liang", "Huy Phan", "Wenwu Wang", "Emmanouil Benetos"], "title": "From Aesthetics to Human Preferences: Comparative Perspectives of Evaluating Text-to-Music Systems", "categories": ["eess.AS"], "comment": null, "summary": "Evaluating generative models remains a fundamental challenge, particularly\nwhen the goal is to reflect human preferences. In this paper, we use music\ngeneration as a case study to investigate the gap between automatic evaluation\nmetrics and human preferences. We conduct comparative experiments across five\nstate-of-the-art music generation approaches, assessing both perceptual quality\nand distributional similarity to human-composed music. Specifically, we\nevaluate synthesis music from various perceptual dimensions and examine\nreference-based metrics such as Mauve Audio Divergence (MAD) and Kernel Audio\nDistance (KAD). Our findings reveal significant inconsistencies across the\ndifferent metrics, highlighting the limitation of the current evaluation\npractice. To support further research, we release a benchmark dataset\ncomprising samples from multiple models. This study provides a broader\nperspective on the alignment of human preference in generative modeling,\nadvocating for more human-centered evaluation strategies across domains."}
{"id": "2504.21366", "pdf": "https://arxiv.org/pdf/2504.21366", "abs": "https://arxiv.org/abs/2504.21366", "authors": ["Yinfeng Yu", "Shiyu Sun"], "title": "DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion", "categories": ["cs.SD", "cs.AI"], "comment": "Main paper (9 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025", "summary": "Current Audio-Visual Source Separation methods primarily adopt two design\nstrategies. The first strategy involves fusing audio and visual features at the\nbottleneck layer of the encoder, followed by processing the fused features\nthrough the decoder. However, when there is a significant disparity between the\ntwo modalities, this approach may lead to the loss of critical information. The\nsecond strategy avoids direct fusion and instead relies on the decoder to\nhandle the interaction between audio and visual features. Nonetheless, if the\nencoder fails to integrate information across modalities adequately, the\ndecoder may be unable to effectively capture the complex relationships between\nthem. To address these issues, this paper proposes a dynamic fusion method\nbased on a gating mechanism that dynamically adjusts the modality fusion\ndegree. This approach mitigates the limitations of solely relying on the\ndecoder and facilitates efficient collaboration between audio and visual\nfeatures. Additionally, an audio attention module is introduced to enhance the\nexpressive capacity of audio features, thereby further improving model\nperformance. Experimental results demonstrate that our method achieves\nsignificant performance improvements on two benchmark datasets, validating its\neffectiveness and advantages in Audio-Visual Source Separation tasks."}
{"id": "2504.21772", "pdf": "https://arxiv.org/pdf/2504.21772", "abs": "https://arxiv.org/abs/2504.21772", "authors": ["Minwoo Oh", "Minsu Park", "Eunil Park"], "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline", "categories": ["cs.MM", "cs.AI"], "comment": "will be presented in IJCAI 2025, 9 pages, 4 tables, 3 figures", "summary": "Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms."}
{"id": "2504.21227", "pdf": "https://arxiv.org/pdf/2504.21227", "abs": "https://arxiv.org/abs/2504.21227", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Lauren Mills", "Marouane Tliba", "Ahmet Enis Cetin", "Mohammed H. Elnagar"], "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging."}
{"id": "2504.21051", "pdf": "https://arxiv.org/pdf/2504.21051", "abs": "https://arxiv.org/abs/2504.21051", "authors": ["Jiarui Ye", "Hao Tang"], "title": "Multimodal Large Language Models for Medicine: A Comprehensive Survey", "categories": ["cs.LG", "cs.CL", "cs.MM"], "comment": null, "summary": "MLLMs have recently become a focal point in the field of artificial\nintelligence research. Building on the strong capabilities of LLMs, MLLMs are\nadept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs\nhave gained substantial attention from different domains. Researchers have\nbegun to explore the potential of MLLMs in the medical and healthcare domain.\nIn this paper, we first introduce the background and fundamental concepts\nrelated to LLMs and MLLMs, while emphasizing the working principles of MLLMs.\nSubsequently, we summarize three main directions of application within\nhealthcare: medical reporting, medical diagnosis, and medical treatment. Our\nfindings are based on a comprehensive review of 330 recent papers in this area.\nWe illustrate the remarkable capabilities of MLLMs in these domains by\nproviding specific examples. For data, we present six mainstream modes of data\nalong with their corresponding evaluation benchmarks. At the end of the survey,\nwe discuss the challenges faced by MLLMs in the medical and healthcare domain\nand propose feasible methods to mitigate or overcome these issues."}
{"id": "2504.21184", "pdf": "https://arxiv.org/pdf/2504.21184", "abs": "https://arxiv.org/abs/2504.21184", "authors": ["Emily Zhou", "Khushboo Khatri", "Yixue Zhao", "Bhaskar Krishnamachari"], "title": "AffectEval: A Modular and Customizable Framework for Affective Computing", "categories": ["cs.AI"], "comment": "The short version is published in ACM/IEEE CHASE 2025", "summary": "The field of affective computing focuses on recognizing, interpreting, and\nresponding to human emotions, and has broad applications across education,\nchild development, and human health and wellness. However, developing affective\ncomputing pipelines remains labor-intensive due to the lack of software\nframeworks that support multimodal, multi-domain emotion recognition\napplications. This often results in redundant effort when building pipelines\nfor different applications. While recent frameworks attempt to address these\nchallenges, they remain limited in reducing manual effort and ensuring\ncross-domain generalizability. We introduce AffectEval, a modular and\ncustomizable framework to facilitate the development of affective computing\npipelines while reducing the manual effort and duplicate work involved in\ndeveloping such pipelines. We validate AffectEval by replicating prior\naffective computing experiments, and we demonstrate that our framework reduces\nprogramming effort by up to 90%, as measured by the reduction in raw lines of\ncode."}
{"id": "2504.21136", "pdf": "https://arxiv.org/pdf/2504.21136", "abs": "https://arxiv.org/abs/2504.21136", "authors": ["Murali Ramanujam", "Yinwei Dai", "Kyle Jamieson", "Ravi Netravali"], "title": "Legilimens: Performant Video Analytics on the System-on-Chip Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continually retraining models has emerged as a primary technique to enable\nhigh-accuracy video analytics on edge devices. Yet, existing systems employ\nsuch adaptation by relying on the spare compute resources that traditional\n(memory-constrained) edge servers afford. In contrast, mobile edge devices such\nas drones and dashcams offer a fundamentally different resource profile:\nweak(er) compute with abundant unified memory pools. We present Legilimens, a\ncontinuous learning system for the mobile edge's System-on-Chip GPUs. Our\ndriving insight is that visually distinct scenes that require retraining\nexhibit substantial overlap in model embeddings; if captured into a base model\non device memory, specializing to each new scene can become lightweight,\nrequiring very few samples. To practically realize this approach, Legilimens\npresents new, compute-efficient techniques to (1) select high-utility data\nsamples for retraining specialized models, (2) update the base model without\ncomplete retraining, and (3) time-share compute resources between retraining\nand live inference for maximal accuracy. Across diverse workloads, Legilimens\nlowers retraining costs by 2.8-10x compared to existing systems, resulting in\n18-45% higher accuracies."}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019", "abs": "https://arxiv.org/abs/2504.21019", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net."}
{"id": "2504.21113", "pdf": "https://arxiv.org/pdf/2504.21113", "abs": "https://arxiv.org/abs/2504.21113", "authors": ["Donipolo Ghimire", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via Exemplar-Clustering", "categories": ["cs.MA", "cs.RO"], "comment": null, "summary": "This paper addresses multi-agent deployment in non-convex and uneven\nenvironments. To overcome the limitations of traditional approaches, we\nintroduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch\ncoverage framework that combines exemplar-clustering with obstacle-aware and\ntraversability-aware shortest distances, offering a deployment framework based\non submodular optimization. NavEX provides a unified approach to solve two\ncritical coverage tasks: (a) fair-access deployment, aiming to provide\nequitable service by minimizing agent-target distances, and (b) hotspot\ndeployment, prioritizing high-density target regions. A key feature of NavEX is\nthe use of exemplar-clustering for the coverage utility measure, which provides\nthe flexibility to employ non-Euclidean distance metrics that do not\nnecessarily conform to the triangle inequality. This allows NavEX to\nincorporate visibility graphs for shortest-path computation in environments\nwith planar obstacles, and traversability-aware RRT* for complex, rugged\nterrains. By leveraging submodular optimization, the NavEX framework enables\nefficient, near-optimal solutions with provable performance guarantees for\nmulti-agent deployment in realistic and complex settings, as demonstrated by\nour simulations."}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214", "abs": "https://arxiv.org/abs/2504.21214", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research."}
{"id": "2504.21847", "pdf": "https://arxiv.org/pdf/2504.21847", "abs": "https://arxiv.org/abs/2504.21847", "authors": ["Derong Jin", "Ruohan Gao"], "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "categories": ["cs.CV", "cs.SD"], "comment": "Project Page: https://humathe.github.io/avdar/", "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale."}
{"id": "2504.21263", "pdf": "https://arxiv.org/pdf/2504.21263", "abs": "https://arxiv.org/abs/2504.21263", "authors": ["Jinpeng Wang", "Tianci Luo", "Yaohua Zha", "Yan Feng", "Ruisheng Luo", "Bin Chen", "Tao Dai", "Long Chen", "Yaowei Wang", "Shu-Tao Xia"], "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by CVPR'25. 10 pages, 5 figures, 6 tables", "summary": "Visual In-Context Learning (VICL) enables adaptively solving vision tasks by\nleveraging pixel demonstrations, mimicking human-like task completion through\nanalogy. Prompt selection is critical in VICL, but current methods assume the\nexistence of a single \"ideal\" prompt in a pool of candidates, which in practice\nmay not hold true. Multiple suitable prompts may exist, but individually they\noften fall short, leading to difficulties in selection and the exclusion of\nuseful context. To address this, we propose a new perspective: prompt\ncondensation. Rather than relying on a single prompt, candidate prompts\ncollaborate to efficiently integrate informative contexts without sacrificing\nresolution. We devise Condenser, a lightweight external plugin that compresses\nrelevant fine-grained context across multiple prompts. Optimized end-to-end\nwith the backbone, Condenser ensures accurate integration of contextual cues.\nExperiments demonstrate Condenser outperforms state-of-the-arts across\nbenchmark tasks, showing superior context compression, scalability with more\nprompts, and enhanced computational efficiency compared to ensemble methods,\npositioning it as a highly competitive solution for VICL. Code is open-sourced\nat https://github.com/gimpong/CVPR25-Condenser."}
{"id": "2504.21445", "pdf": "https://arxiv.org/pdf/2504.21445", "abs": "https://arxiv.org/abs/2504.21445", "authors": ["Chuanmin Jia", "Feng Ye", "Siwei Ma", "Wen Gao", "Huifang Sun", "Leonardo Chiariglione"], "title": "Emerging Advances in Learned Video Compression: Models, Systems and Beyond", "categories": ["eess.IV"], "comment": null, "summary": "Video compression is a fundamental topic in the visual intelligence, bridging\nvisual signal sensing/capturing and high-level visual analytics. The broad\nsuccess of artificial intelligence (AI) technology has enriched the horizon of\nvideo compression into novel paradigms by leveraging end-to-end optimized\nneural models. In this survey, we first provide a comprehensive and systematic\noverview of recent literature on end-to-end optimized learned video coding,\ncovering the spectrum of pioneering efforts in both uni-directional and\nbi-directional prediction based compression model designation. We further delve\ninto the optimization techniques employed in learned video compression (LVC),\nemphasizing their technical innovations, advantages. Some standardization\nprogress is also reported. Furthermore, we investigate the system design and\nhardware implementation challenges of the LVC inclusively. Finally, we present\nthe extensive simulation results to demonstrate the superior compression\nperformance of LVC models, addressing the question that why learned codecs and\nAI-based video technology would have with broad impact on future visual\nintelligence research."}
{"id": "2504.21053", "pdf": "https://arxiv.org/pdf/2504.21053", "abs": "https://arxiv.org/abs/2504.21053", "authors": ["Yi Zhou", "Wenpeng Xing", "Dezhang Kong", "Changting Lin", "Meng Han"], "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Safety alignment in large language models (LLMs) is achieved through\nfine-tuning mechanisms that regulate neuron activations to suppress harmful\ncontent. In this work, we propose a novel approach to induce disalignment by\nidentifying and modifying the neurons responsible for safety constraints. Our\nmethod consists of three key steps: Neuron Activation Analysis, where we\nexamine activation patterns in response to harmful and harmless prompts to\ndetect neurons that are critical for distinguishing between harmful and\nharmless inputs; Similarity-Based Neuron Identification, which systematically\nlocates the neurons responsible for safe alignment; and Neuron Relearning for\nSafety Removal, where we fine-tune these selected neurons to restore the\nmodel's ability to generate previously restricted responses. Experimental\nresults demonstrate that our method effectively removes safety constraints with\nminimal fine-tuning, highlighting a critical vulnerability in current alignment\ntechniques. Our findings underscore the need for robust defenses against\nadversarial fine-tuning attacks on LLMs."}
{"id": "2504.21218", "pdf": "https://arxiv.org/pdf/2504.21218", "abs": "https://arxiv.org/abs/2504.21218", "authors": ["Sebastian Dumbrava"], "title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This monograph presents a modular cognitive architecture for artificial\nintelligence grounded in the formal modeling of belief as structured semantic\nstate. Belief states are defined as dynamic ensembles of linguistic expressions\nembedded within a navigable manifold, where operators enable assimilation,\nabstraction, nullification, memory, and introspection. Drawing from philosophy,\ncognitive science, and neuroscience, we develop a layered framework that\nenables self-regulating epistemic agents capable of reflective, goal-directed\nthought. At the core of this framework is the epistemic vacuum: a class of\nsemantically inert cognitive states that serves as the conceptual origin of\nbelief space. From this foundation, the Null Tower arises as a generative\nstructure recursively built through internal representational capacities. The\ntheoretical constructs are designed to be implementable in both symbolic and\nneural systems, including large language models, hybrid agents, and adaptive\nmemory architectures. This work offers a foundational substrate for\nconstructing agents that reason, remember, and regulate their beliefs in\nstructured, interpretable ways."}
{"id": "2504.21154", "pdf": "https://arxiv.org/pdf/2504.21154", "abs": "https://arxiv.org/abs/2504.21154", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel framework for emotion recognition in contemporary\ndance by improving existing Laban Movement Analysis (LMA) feature descriptors\nand introducing robust, novel descriptors that capture both quantitative and\nqualitative aspects of the movement. Our approach extracts expressive\ncharacteristics from 3D keypoints data of professional dancers performing\ncontemporary dance under various emotional states, and trains multiple\nclassifiers, including Random Forests and Support Vector Machines.\nAdditionally, we provide in-depth explanation of features and their impact on\nmodel predictions using explainable machine learning methods. Overall, our\nstudy improves emotion recognition in contemporary dance and offers promising\napplications in performance analysis, dance training, and human--computer\ninteraction, with a highest accuracy of 96.85\\%."}
{"id": "2504.21020", "pdf": "https://arxiv.org/pdf/2504.21020", "abs": "https://arxiv.org/abs/2504.21020", "authors": ["Jaydip Sen", "Rohit Pandey", "Hetvi Waghela"], "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the pre-review version of our paper, which has been accepted\n  for publication in the IEEE 6th International Conference on Emerging\n  Technologies (INCET). The conference will be organized at Belgaum, India,\n  from May 24 to 26, 2025. This is not the final camera-ready paper, which will\n  be available on IEEE Xplore. The paper is 9 pages long, and it contains 2\n  Figures and 4 Tables", "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\nadvancements in Natural Language Processing (NLP). However, generating\nhigh-quality text that balances coherence, diversity, and relevance remains\nchallenging. Traditional decoding methods, such as bean search and top-k\nsampling, often struggle with either repetitive or incoherent outputs,\nparticularly in tasks that require long-form text generation. To address these\nlimitations, the paper proposes a novel enhancement of the well-known\nContrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with\ncontextual calibration. The proposed scheme introduces several novelties\nincluding dynamic contextual importance weighting, multi-level Contrastive\nSearch, and adaptive temperature control, to optimize the balance between\nfluency, creativity, and precision. The performance of CECS is evaluated using\nseveral standard metrics such as BLEU, ROUGE, and semantic similarity.\nExperimental results demonstrate significant improvements in both coherence and\nrelevance of the generated texts by CECS outperforming the existing Contrastive\nSearch techniques. The proposed algorithm has several potential applications in\nthe real world including legal document drafting, customer service chatbots,\nand content marketing."}
{"id": "2504.21164", "pdf": "https://arxiv.org/pdf/2504.21164", "abs": "https://arxiv.org/abs/2504.21164", "authors": ["Bhavini Jeloka", "Yue Guan", "Panagiotis Tsiotras"], "title": "Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions", "categories": ["cs.MA"], "comment": "8 pages main + 7 pages supplementary, accepted to ALA workshop 2025\n  (under AAMAS)", "summary": "State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as\nMADDPG and MAAC fail to scale in situations where the number of agents becomes\nlarge. Mean-field theory has shown encouraging results in modeling macroscopic\nagent behavior for teams with a large number of agents through a continuum\napproximation of the agent population and its interaction with the environment.\nIn this work, we extend proximal policy optimization (PPO) to the mean-field\ndomain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization\n(MF-MAPPO), a novel algorithm that utilizes the effectiveness of the\nfinite-population mean-field approximation in the context of zero-sum\ncompetitive multi-agent games between two teams. The proposed algorithm can be\neasily scaled to hundreds and thousands of agents in each team as shown through\nnumerical experiments. In particular, the algorithm is applied to realistic\napplications such as large-scale offense-defense battlefield scenarios."}
{"id": "2407.13895", "pdf": "https://arxiv.org/pdf/2407.13895", "abs": "https://arxiv.org/abs/2407.13895", "authors": ["Jing-Tong Tzeng", "Jeng-Lin Li", "Huan-Yu Chen", "Chun-Hsiang Huang", "Chi-Hsin Chen", "Cheng-Yi Fan", "Edward Pei-Chuan Huang", "Chi-Chun Lee"], "title": "Improving the Robustness and Clinical Applicability of Automatic Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement: Algorithm Development and Validation", "categories": ["eess.AS"], "comment": "Published on JMIR AI https://ai.jmir.org/2025/1/e67239. Demo website:\n  https://rogertzeng.github.io/ReSC-AE/", "summary": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions remains challenging for clinical\ndeployment. In addition, predicting signals with only background noise may\nreduce user trust in the system. This study explores the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement step\ninto automatic respiratory sound classification systems to improve robustness\nand clinical applicability. We conducted extensive experiments using various\naudio enhancement model architectures, including time-domain and\ntime-frequency-domain approaches, combined with multiple classification models\nto evaluate the module's effectiveness. The classification performance was\ncompared against the noise injection data augmentation method. These\nexperiments were carried out on two datasets: the ICBHI respiratory sound\ndataset and the FABS dataset. Furthermore, a physician validation study\nassessed the system's clinical utility. Integrating the audio enhancement\nmodule resulted in a 21.9% increase in the ICBHI classification score and a\n4.1% improvement on the FABS dataset in multi-class noisy scenarios.\nQuantitative analysis revealed efficiency gains, higher diagnostic confidence,\nand increased trust, with workflows using enhanced audio improving diagnostic\nsensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an\naudio enhancement algorithm boosts the robustness and clinical utility of\nautomatic respiratory sound classification systems, enhancing performance in\nnoisy environments and fostering greater trust among medical professionals."}
{"id": "2407.08306", "pdf": "https://arxiv.org/pdf/2407.08306", "abs": "https://arxiv.org/abs/2407.08306", "authors": ["Zijian Zhao"], "title": "Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT ."}
{"id": "2504.21491", "pdf": "https://arxiv.org/pdf/2504.21491", "abs": "https://arxiv.org/abs/2504.21491", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF."}
{"id": "2504.21581", "pdf": "https://arxiv.org/pdf/2504.21581", "abs": "https://arxiv.org/abs/2504.21581", "authors": ["Yuxin Jing", "Yuchen Zheng", "Jufeng Zhao", "Guangmang Cui", "Yiming Zhu", "Tianpei Zhang"], "title": "Make Both Ends Meet: A Synergistic Optimization Infrared Small Target Detection with Streamlined Computational Overhead", "categories": ["eess.IV"], "comment": null, "summary": "Infrared small target detection(IRSTD) is widely recognized as a challenging\ntask due to the inherent limitations of infrared imaging, including low\nsignal-to-noise ratios, lack of texture details, and complex background\ninterference. While most existing methods model IRSTD as a semantic\nsegmentation task, but they suffer from two critical drawbacks: (1)blurred\ntarget boundaries caused by long-distance imaging dispersion; and (2) excessive\ncomputational overhead due to indiscriminate feature stackin. To address these\nissues, we propose the Lightweight Efficiency Infrared Small Target Detection\n(LE-IRSTD), a lightweight and efficient framework based on YOLOv8n, with\nfollowing key innovations. Firstly, we identify that the multiple bottleneck\nstructures within the C2f component of the YOLOv8-n backbone contribute to an\nincreased computational burden. Therefore, we implement the Mobile Inverted\nBottleneck Convolution block (MBConvblock) and Bottleneck Structure block\n(BSblock) in the backbone, effectively balancing the trade-off between\ncomputational efficiency and the extraction of deep semantic information.\nSecondly, we introduce the Attention-based Variable Convolution Stem (AVCStem)\nstructure, substituting the final convolution with Variable Kernel Convolution\n(VKConv), which allows for adaptive convolutional kernels that can transform\ninto various shapes, facilitating the receptive field for the extraction of\ntargets. Finally, we employ Global Shuffle Convolution (GSConv) to shuffle the\nchannel dimension features obtained from different convolutional approaches,\nthereby enhancing the robustness and generalization capabilities of our method.\nExperimental results demonstrate that our LE-IRSTD method achieves compelling\nresults in both accuracy and lightweight performance, outperforming several\nstate-of-the-art deep learning methods."}
{"id": "2504.21055", "pdf": "https://arxiv.org/pdf/2504.21055", "abs": "https://arxiv.org/abs/2504.21055", "authors": ["Shuai Ma", "Bin Shen", "Chuanhui Zhang", "Youlong Wu", "Hang Li", "Shiyin Li", "Guangming Shi", "Naofal Al-Dhahir"], "title": "Modeling and Performance Analysis for Semantic Communications Based on Empirical Results", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the black-box characteristics of deep learning based semantic encoders\nand decoders, finding a tractable method for the performance analysis of\nsemantic communications is a challenging problem. In this paper, we propose an\nAlpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end\nmeasurement and SNR, which can be applied for both image reconstruction tasks\nand inference tasks. Specifically, for image reconstruction tasks, the proposed\nABG formula can well fit the commonly used DL networks, such as SCUNet, and\nVision Transformer, for semantic encoding with the multi scale-structural\nsimilarity index measure (MS-SSIM) measurement. Furthermore, we find that the\nupper bound of the MS-SSIM depends on the number of quantized output bits of\nsemantic encoders, and we also propose a closed-form expression to fit the\nrelationship between the MS-SSIM and quantized output bits. To the best of our\nknowledge, this is the first theoretical expression between end-to-end\nperformance metrics and SNR for semantic communications. Based on the proposed\nABG formula, we investigate an adaptive power control scheme for semantic\ncommunications over random fading channels, which can effectively guarantee\nquality of service (QoS) for semantic communications, and then design the\noptimal power allocation scheme to maximize the energy efficiency of the\nsemantic communication system. Furthermore, by exploiting the bisection\nalgorithm, we develop the power allocation scheme to maximize the minimum QoS\nof multiple users for OFDMA downlink semantic communication Extensive\nsimulations verify the effectiveness and superiority of the proposed ABG\nformula and power allocation schemes."}
{"id": "2504.21277", "pdf": "https://arxiv.org/pdf/2504.21277", "abs": "https://arxiv.org/abs/2504.21277", "authors": ["Guanghao Zhou", "Panjia Qiu", "Cen Chen", "Jie Wang", "Zheming Yang", "Jian Xu", "Minghui Qiu"], "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era."}
{"id": "2504.21166", "pdf": "https://arxiv.org/pdf/2504.21166", "abs": "https://arxiv.org/abs/2504.21166", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Dance Style Recognition Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing interest in automated movement analysis has presented new\nchallenges in recognition of complex human activities including dance. This\nstudy focuses on dance style recognition using features extracted using Laban\nMovement Analysis. Previous studies for dance style recognition often focus on\ncross-frame movement analysis, which limits the ability to capture temporal\ncontext and dynamic transitions between movements. This gap highlights the need\nfor a method that can add temporal context to LMA features. For this, we\nintroduce a novel pipeline which combines 3D pose estimation, 3D human mesh\nreconstruction, and floor aware body modeling to effectively extract LMA\nfeatures. To address the temporal limitation, we propose a sliding window\napproach that captures movement evolution across time in features. These\nfeatures are then used to train various machine learning methods for\nclassification, and their explainability explainable AI methods to evaluate the\ncontribution of each feature to classification performance. Our proposed method\nachieves a highest classification accuracy of 99.18\\% which shows that the\naddition of temporal context significantly improves dance style recognition\nperformance."}
{"id": "2504.21022", "pdf": "https://arxiv.org/pdf/2504.21022", "abs": "https://arxiv.org/abs/2504.21022", "authors": ["Jun Wang", "David Smith Sundarsingh", "Jyotirmoy V. Deshmukh", "Yiannis Kantaros"], "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Linear Temporal Logic (LTL) has become a prevalent specification language for\nrobotic tasks. To mitigate the significant manual effort and expertise required\nto define LTL-encoded tasks, several methods have been proposed for translating\nNatural Language (NL) instructions into LTL formulas, which, however, lack\ncorrectness guarantees. To address this, we introduce a new NL-to-LTL\ntranslation method, called ConformalNL2LTL, that can achieve user-defined\ntranslation success rates over unseen NL commands. Our method constructs LTL\nformulas iteratively by addressing a sequence of open-vocabulary\nQuestion-Answering (QA) problems with LLMs. To enable uncertainty-aware\ntranslation, we leverage conformal prediction (CP), a distribution-free\nuncertainty quantification tool for black-box models. CP enables our method to\nassess the uncertainty in LLM-generated answers, allowing it to proceed with\ntranslation when sufficiently confident and request help otherwise. We provide\nboth theoretical and empirical results demonstrating that ConformalNL2LTL\nachieves user-specified translation accuracy while minimizing help rates."}
{"id": "2504.21278", "pdf": "https://arxiv.org/pdf/2504.21278", "abs": "https://arxiv.org/abs/2504.21278", "authors": ["Xuyan Ma", "Yawen Wang", "Junjie Wang", "Xiaofei Xie", "Boyu Wu", "Shoubin Li", "Fanjiang Xu", "Qing Wang"], "title": "Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training", "categories": ["cs.MA"], "comment": null, "summary": "In typical multi-agent reinforcement learning (MARL) problems, communication\nis important for agents to share information and make the right decisions.\nHowever, due to the complexity of training multi-agent communication, existing\nmethods often fall into the dilemma of local optimization, which leads to the\nconcentration of communication in a limited number of channels and presents an\nunbalanced structure. Such unbalanced communication policy are vulnerable to\nabnormal conditions, where the damage of critical communication channels can\ntrigger the crash of the entire system. Inspired by decentralization theory in\nsociology, we propose DMAC, which enhances the robustness of multi-agent\ncommunication policies by retraining them into decentralized patterns.\nSpecifically, we train an adversary DMAC\\_Adv which can dynamically identify\nand mask the critical communication channels, and then apply the adversarial\nsamples generated by DMAC\\_Adv to the adversarial learning of the communication\npolicy to force the policy in exploring other potential communication schemes\nand transition to a decentralized structure. As a training method to improve\nrobustness, DMAC can be fused with any learnable communication policy\nalgorithm. The experimental results in two communication policies and four\nmulti-agent tasks demonstrate that DMAC achieves higher improvement on\nrobustness and performance of communication policy compared with two\nstate-of-the-art and commonly-used baselines. Also, the results demonstrate\nthat DMAC can achieve decentralized communication structure with acceptable\ncommunication cost."}
{"id": "2409.15545", "pdf": "https://arxiv.org/pdf/2409.15545", "abs": "https://arxiv.org/abs/2409.15545", "authors": ["Yuanchao Li", "Azalea Gui", "Dimitra Emmanouilidou", "Hannes Gamper"], "title": "Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICME 2025", "summary": "The complex nature of musical emotion introduces inherent bias in both\nrecognition and generation, particularly when relying on a single audio\nencoder, emotion classifier, or evaluation metric. In this work, we conduct a\nstudy on Music Emotion Recognition (MER) and Emotional Music Generation (EMG),\nemploying diverse audio encoders alongside Frechet Audio Distance (FAD), a\nreference-free evaluation metric. Our study begins with a benchmark evaluation\nof MER, highlighting the limitations of using a single audio encoder and the\ndisparities observed across different measurements. We then propose assessing\nMER performance using FAD derived from multiple encoders to provide a more\nobjective measure of musical emotion. Furthermore, we introduce an enhanced EMG\napproach designed to improve both the variability and prominence of generated\nmusical emotion, thereby enhancing its realism. Additionally, we investigate\nthe differences in realism between the emotions conveyed in real and synthetic\nmusic, comparing our EMG model against two baseline models. Experimental\nresults underscore the issue of emotion bias in both MER and EMG and\ndemonstrate the potential of using FAD and diverse audio encoders to evaluate\nmusical emotion more objectively and effectively."}
{"id": "2504.20923", "pdf": "https://arxiv.org/pdf/2504.20923", "abs": "https://arxiv.org/abs/2504.20923", "authors": ["Andrea Di Pierno", "Luca Guarnera", "Dario Allegra", "Sebastiano Battiato"], "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Audio deepfakes represent a growing threat to digital security and trust,\nleveraging advanced generative models to produce synthetic speech that closely\nmimics real human voices. Detecting such manipulations is especially\nchallenging under open-world conditions, where spoofing methods encountered\nduring testing may differ from those seen during training. In this work, we\npropose an end-to-end deep learning framework for audio deepfake detection that\noperates directly on raw waveforms. Our model, RawNetLite, is a lightweight\nconvolutional-recurrent architecture designed to capture both spectral and\ntemporal features without handcrafted preprocessing. To enhance robustness, we\nintroduce a training strategy that combines data from multiple domains and\nadopts Focal Loss to emphasize difficult or ambiguous samples. We further\ndemonstrate that incorporating codec-based manipulations and applying\nwaveform-level audio augmentations (e.g., pitch shifting, noise, and time\nstretching) leads to significant generalization improvements under realistic\nacoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on\nin-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging\nout-of-distribution test set (AVSpoof2021 + CodecFake). These findings\nhighlight the importance of diverse training data, tailored objective functions\nand audio augmentations in building resilient and generalizable audio forgery\ndetectors. Code and pretrained models are available at\nhttps://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/."}
{"id": "2504.21495", "pdf": "https://arxiv.org/pdf/2504.21495", "abs": "https://arxiv.org/abs/2504.21495", "authors": ["Junxi Wang", "Jize liu", "Na Zhang", "Yaxiong Wang"], "title": "Consistency-aware Fake Videos Detection on Short Video Platforms", "categories": ["cs.CV", "cs.MM"], "comment": "2025 icic", "summary": "This paper focuses to detect the fake news on the short video platforms.\nWhile significant research efforts have been devoted to this task with notable\nprogress in recent years, current detection accuracy remains suboptimal due to\nthe rapid evolution of content manipulation and generation technologies.\nExisting approaches typically employ a cross-modal fusion strategy that\ndirectly combines raw video data with metadata inputs before applying a\nclassification layer. However, our empirical observations reveal a critical\noversight: manipulated content frequently exhibits inter-modal inconsistencies\nthat could serve as valuable discriminative features, yet remain underutilized\nin contemporary detection frameworks. Motivated by this insight, we propose a\nnovel detection paradigm that explicitly identifies and leverages cross-modal\ncontradictions as discriminative cues. Our approach consists of two core\nmodules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative\nDiagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal\nConsistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used\nto generate pseudo-labels for evaluating cross-modal semantic consistency.\nThen, CMCD extracts [CLS] tokens and computes cosine loss to quantify\ncross-modal inconsistencies. MMCD further integrates multimodal features\nthrough Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).\nMFF employs a co-attention mechanism to enhance semantic interactions across\ndifferent modalities, while a Transformer is utilized for comprehensive feature\nfusion. Meanwhile, PSF further integrates the fake news probability scores\nobtained in the previous step. Extensive experiments on established benchmarks\n(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in\nFake videos detection."}
{"id": "2504.21612", "pdf": "https://arxiv.org/pdf/2504.21612", "abs": "https://arxiv.org/abs/2504.21612", "authors": ["Yirui Chen", "Yiming Zhu", "Yuxin Jing", "Tianpei Zhang", "Yuchen Zheng"], "title": "Selective Variable Convolution Meets Dynamic Content Guided Attention for Infrared Small Target Detection", "categories": ["eess.IV"], "comment": null, "summary": "Infrared Small Target Detection (IRSTD) system aims to identify small targets\nin complex backgrounds. Due to the convolution operation in Convolutional\nNeural Networks (CNNs), applying traditional CNNs to IRSTD presents challenges,\nsince the feature extraction of small targets is often insufficient, resulting\nin the loss of critical features. To address these issues, we propose a dynamic\ncontent guided attention multiscale feature aggregation network (DCGANet),\nwhich adheres to the attention principle of 'coarse-to-fine' and achieves high\ndetection accuracy. First, we propose a selective variable convolution (SVC)\nmodule that integrates the benefits of standard convolution, irregular\ndeformable convolution, and multi-rate dilated convolution. This module is\ndesigned to expand the receptive field and enhance non-local features, thereby\neffectively improving the discrimination of targets from backgrounds. Second,\nthe core component of DCGANet is a two-stage content guided attention module.\nThis module employs two-stage attention mechanism to initially direct the\nnetwork's focus to salient regions within the feature maps and subsequently\ndetermine whether these regions correspond to targets or background\ninterference. By retaining the most significant responses, this mechanism\neffectively suppresses false alarms. Additionally, we propose adaptive dynamic\nfeature fusion (ADFF) module to substitute for static feature cascading. This\ndynamic feature fusion strategy enables DCGANet to adaptively integrate\ncontextual features, thereby enhancing its ability to discriminate true targets\nfrom false alarms. DCGANet has achieved new benchmarks across multiple\ndatasets."}
{"id": "2504.21062", "pdf": "https://arxiv.org/pdf/2504.21062", "abs": "https://arxiv.org/abs/2504.21062", "authors": ["Ngueuleweu Tiwang Gildas"], "title": "A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "comment": "19 pages, 7 figures", "summary": "Machine learning detects patterns, block chain guarantees trust and\nimmutability, and modern causal inference identifies directional linkages, yet\nnone alone exposes the full energetic anatomy of complex systems; the\nHamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these\ngaps. Grounded in classical mechanics but extended to Economics order\nelasticity terms, 2HOED represents economic, social, and physical systems as\nenergy-based Hamiltonians whose position, velocity, acceleration, and jerk of\nelasticity jointly determine systemic power, Inertia, policy sensitivity, and\nmarginal responses. Because the formalism is scaling free and coordinate\nagnostic, it transfers seamlessly from financial markets to climate science,\nfrom supply chain logistics to epidemiology, thus any discipline in which\nadaptation and shocks coexist. By embedding standard econometric variables\ninside a Hamiltonian, 2HOED enriches conventional economic analysis with\nrigorous diagnostics of resilience, tipping points, and feedback loops,\nrevealing failure modes invisible to linear models. Wavelet spectra, phase\nspace attractors, and topological persistence diagrams derived from 2HOED\nexpose multistage policy leverage that machine learning detects only\nempirically and block chain secures only after the fact. For economists,\nphysicians and other scientists, the method opens a new causal energetic\nchannel linking biological or mechanical elasticity to macro level outcomes.\nPortable, interpretable, and computationally light, 2HOED turns data streams\ninto dynamical energy maps, empowering decision makers to anticipate crises,\ndesign adaptive policies, and engineer robust systems delivering the predictive\npunch of AI with the explanatory clarity of physics."}
{"id": "2504.21318", "pdf": "https://arxiv.org/pdf/2504.21318", "abs": "https://arxiv.org/abs/2504.21318", "authors": ["Marah Abdin", "Sahaj Agarwal", "Ahmed Awadallah", "Vidhisha Balachandran", "Harkirat Behl", "Lingjiao Chen", "Gustavo de Rosa", "Suriya Gunasekar", "Mojan Javaheripi", "Neel Joshi", "Piero Kauffmann", "Yash Lara", "Caio César Teodoro Mendes", "Arindam Mitra", "Besmira Nushi", "Dimitris Papailiopoulos", "Olli Saarikivi", "Shital Shah", "Vaishnavi Shrivastava", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi", "Guoqing Zheng"], "title": "Phi-4-reasoning Technical Report", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models."}
{"id": "2504.21194", "pdf": "https://arxiv.org/pdf/2504.21194", "abs": "https://arxiv.org/abs/2504.21194", "authors": ["Vedika Srivastava", "Hemant Kumar Singh", "Jaisal Singh"], "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts."}
{"id": "2504.21023", "pdf": "https://arxiv.org/pdf/2504.21023", "abs": "https://arxiv.org/abs/2504.21023", "authors": ["Sheng Cao", "Mingrui Wu", "Karthik Prasad", "Yuandong Tian", "Zechun Liu"], "title": "Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost", "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The post-training phase of large language models is essential for enhancing\ncapabilities such as instruction-following, reasoning, and alignment with human\npreferences. However, it demands extensive high-quality data and poses risks\nlike overfitting, alongside significant computational costs due to repeated\npost-training and evaluation after each base model update. This paper\nintroduces $Param\\Delta$, a novel method that streamlines post-training by\ntransferring knowledge from an existing post-trained model to a newly updated\nbase model with ZERO additional training. By computing the difference between\npost-trained model weights ($\\Theta_\\text{post}$) and base model weights\n($\\Theta_\\text{base}$), and adding this to the updated base model\n($\\Theta'_\\text{base}$), we define $Param\\Delta$ Model as:\n$\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} +\n\\Theta'_\\text{base}$. This approach surprisingly equips the new base model with\npost-trained capabilities, achieving performance comparable to direct\npost-training. We did analysis on LLama3, Llama3.1, Qwen, and\nDeepSeek-distilled models. Results indicate $Param\\Delta$ Model effectively\nreplicates traditional post-training. For example, the $Param\\Delta$ Model\nobtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains\napproximately 95\\% of Llama3.1-inst model's performance on average.\n$Param\\Delta$ brings a new perspective on how to fully leverage models in the\nopen-weight community, where checkpoints for base and instruct models are\nreadily available and frequently updated, by providing a cost-free framework to\naccelerate the iterative cycle of model development."}
{"id": "2504.21579", "pdf": "https://arxiv.org/pdf/2504.21579", "abs": "https://arxiv.org/abs/2504.21579", "authors": ["Stavros Anagnou", "Christoph Salge", "Peter R. Lewis"], "title": "Uncertainty, bias and the institution bootstrapping problem", "categories": ["cs.MA", "cs.CY", "cs.GT"], "comment": "This paper was accepted for presentation at the International\n  Workshop on Coordination, Organizations, Institutions, Norms and Ethics for\n  Governance of Multi-Agent Systems (COINE), co-located with AAMAS 2025\n  https://coin-workshop.github.io/coine-2025-detroit/", "summary": "Institutions play a critical role in enabling communities to manage\ncommon-pool resources and avert tragedies of the commons. However, a\nfundamental issue arises: Individuals typically perceive participation as\nadvantageous only after an institution is established, creating a paradox: How\ncan institutions form if no one will join before a critical mass exists? We\nterm this conundrum the institution bootstrapping problem and propose that\nmisperception, specifically, agents' erroneous belief that an institution\nalready exists, could resolve this paradox. By integrating well-documented\npsychological phenomena, including cognitive biases, probability distortion,\nand perceptual noise, into a game-theoretic framework, we demonstrate how these\nfactors collectively mitigate the bootstrapping problem. Notably, unbiased\nperceptual noise (e.g., noise arising from agents' heterogeneous physical or\nsocial contexts) drastically reduces the critical mass of cooperators required\nfor institutional emergence. This effect intensifies with greater diversity of\nperceptions. We explain this counter-intuitive result through asymmetric\nboundary conditions: proportional underestimation of low-probability sanctions\nproduces distinct outcomes compared to equivalent overestimation. Furthermore,\nthe type of perceptual distortion, proportional versus absolute, yields\nqualitatively different evolutionary pathways. These findings challenge\nconventional assumptions about rationality in institutional design,\nhighlighting how \"noisy\" cognition can paradoxically enhance cooperation.\nFinally, we contextualize these insights within broader discussions of\nmulti-agent system design and collective action. Our analysis underscores the\nimportance of incorporating human-like cognitive constraints, not just\nidealized rationality, into models of institutional emergence and resilience."}
{"id": "2409.15551", "pdf": "https://arxiv.org/pdf/2409.15551", "abs": "https://arxiv.org/abs/2409.15551", "authors": ["Yuanchao Li", "Yuan Gong", "Chao-Han Huck Yang", "Peter Bell", "Catherine Lai"], "title": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains."}
{"id": "2409.16920", "pdf": "https://arxiv.org/pdf/2409.16920", "abs": "https://arxiv.org/abs/2409.16920", "authors": ["Zhichen Han", "Tianqi Geng", "Hui Feng", "Jiahong Yuan", "Korin Richmond", "Yuanchao Li"], "title": "Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception."}
{"id": "2410.16284", "pdf": "https://arxiv.org/pdf/2410.16284", "abs": "https://arxiv.org/abs/2410.16284", "authors": ["Aizierjiang Aiersilan", "Zhiqiang Wang"], "title": "A 3D Framework for Improving Low-Latency Multi-Channel Live Streaming", "categories": ["cs.MM"], "comment": "We are pleased to announce that this paper has been accepted for\n  presentation at the ICME 2025 Workshop on Surpassing Latency Limits in\n  Adaptive Live Video Streaming. We appreciate the valuable feedback from the\n  reviewers and look forward to sharing our findings with the community", "summary": "The advent of 5G has driven the demand for high-quality, low-latency live\nstreaming. However, challenges such as managing the increased data volume,\nensuring synchronization across multiple streams, and maintaining consistent\nquality under varying network conditions persist, particularly in real-time\nvideo streaming. To address these issues, we propose a novel framework that\nleverages 3D virtual environments within game engines (e.g., Unity 3D) to\noptimize multi-channel live streaming. Our approach consolidates multi-camera\nvideo data into a single stream using multiple virtual 3D canvases,\nsignificantly increasing channel amounts while reducing latency and enhancing\nuser flexibility. For demonstration of our approach, we utilize the Unity 3D\nengine to integrate multiple video inputs into a single-channel stream,\nsupporting one-to-many broadcasting, one-to-one video calling, and real-time\ncontrol of video channels. By mapping video data onto a world-space canvas and\ncapturing it via an in-world camera, we minimize redundant data transmission,\nachieving efficient, low-latency streaming. Our results demonstrate that this\nmethod outperforms some existing multi-channel live streaming solutions in both\nlatency reduction and user interaction responsiveness improvement. Our live\nvideo streaming system affiliated with this paper is also open-source at\nhttps://github.com/Aizierjiang/LiveStreaming."}
{"id": "2504.21670", "pdf": "https://arxiv.org/pdf/2504.21670", "abs": "https://arxiv.org/abs/2504.21670", "authors": ["Quentin Bonassies", "Thanh Huy Nguyen", "Ludovic Cassan", "Andrea Piacentini", "Sophie Ricci", "Charlotte Emery", "Christophe Fatras", "Santiago Peña Luque", "Raquel Rodriguez Suquet"], "title": "Assimilation of SWOT Altimetry Data for Riverine Flood Reanalysis: From Synthetic to Real Data", "categories": ["eess.IV"], "comment": "17 pages, 11 figures. Submitted to the IEEE Journal of Selected\n  Topics in Applied Earth Observations and Remote Sensing", "summary": "Floods are one of the most common and devastating natural disasters\nworldwide. The contribution of remote sensing is important for reducing the\nimpact of flooding both during the event itself and for improving hydrodynamic\nmodels by reducing their associated uncertainties. This article presents the\ninnovative capabilities of the Surface Water and Ocean Topography (SWOT)\nmission, especially its river node products, to enhance the accuracy of\nriverine flood reanalysis, performed on a 50-km stretch of the Garonne River.\nThe experiments incorporate various data assimilation strategies, based on the\nensemble Kalman filter (EnKF), which allows for sequential updates of model\nparameters based on available observations. The experimental results show that\nwhile SWOT data alone offers some improvements, combining it with in-situ water\nlevel measurements provides the most accurate representation of flood dynamics,\nboth at gauge stations and along the river. The study also investigates the\nimpact of different SWOT revisit frequencies on the models performance,\nrevealing that assimilating more frequent SWOT observations leads to more\nreliable flood reanalyses. In the real event, it was demonstrated that the\nassimilation of SWOT and in-situ data accurately reproduces the water level\ndynamics, offering promising prospects for future flood monitoring systems.\nOverall, this study emphasizes the complementary strengths of Earth Observation\ndata in improving the representation of the flood dynamics in the riverbed and\nthe floodplains."}
{"id": "2504.21063", "pdf": "https://arxiv.org/pdf/2504.21063", "abs": "https://arxiv.org/abs/2504.21063", "authors": ["Shuai Gong", "Chaoran Cui", "Xiaolin Dong", "Xiushan Nie", "Lei Zhu", "Xiaojun Chang"], "title": "Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "The manuscript has been submitted to IEEE Transactions on Knowledge\n  and Data Engineering", "summary": "Federated domain generalization (FedDG) aims to learn a globally\ngeneralizable model from decentralized clients with heterogeneous data while\npreserving privacy. Recent studies have introduced prompt learning to adapt\nvision-language models (VLMs) in FedDG by learning a single global prompt.\nHowever, such a one-prompt-fits-all learning paradigm typically leads to\nperformance degradation on personalized samples. Although the mixture of\nexperts (MoE) offers a promising solution for specialization, existing\nMoE-based methods suffer from coarse image-level expert assignment and high\ncommunication costs from parameterized routers. To address these limitations,\nwe propose TRIP, a Token-level prompt mixture with parameter-free routing\nframework for FedDG, which treats multiple prompts as distinct experts. Unlike\nexisting image-level routing designs, TRIP assigns different tokens within an\nimage to specific experts. To ensure communication efficiency, TRIP\nincorporates a parameter-free routing mechanism based on token clustering and\noptimal transport. The instance-specific prompt is then synthesized by\naggregating experts, weighted by the number of tokens assigned to each.\nAdditionally, TRIP develops an unbiased learning strategy for prompt experts,\nleveraging the VLM's zero-shot generalization capability. Extensive experiments\nacross four benchmarks demonstrate that TRIP achieves optimal generalization\nresults, with communication of only 1K parameters per round. Our code is\navailable at https://github.com/GongShuai8210/TRIP."}
{"id": "2504.21347", "pdf": "https://arxiv.org/pdf/2504.21347", "abs": "https://arxiv.org/abs/2504.21347", "authors": ["Seonghee Lee", "Denae Ford", "John Tang", "Sasa Junuzovic", "Asta Roseway", "Ed Cutrell", "Kori Inkpen"], "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces", "categories": ["cs.AI", "cs.HC", "H.5.2; I.2.9"], "comment": "8 pages, 3 figures", "summary": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent\ndesigned to represent remote colleagues in shared office spaces, creating\nopportunities for real-time exchanges even in their absence. IRL Ditto offers a\nunique hybrid experience by allowing in-person colleagues to encounter a\ndigital version of their remote teammates, initiating greetings, updates, or\nsmall talk as they might in person. Our research question examines: How can the\nIRL Ditto influence interactions and relationships among colleagues in a shared\noffice space? Through a four-day study, we assessed IRL Ditto's ability to\nstrengthen social ties by simulating presence and enabling meaningful\ninteractions across different levels of social familiarity. We find that\nenhancing social relationships depended deeply on the foundation of the\nrelationship participants had with the source of the IRL Ditto. This study\nprovides insights into the role of embodied agents in enriching workplace\ndynamics for distributed teams."}
{"id": "2504.21226", "pdf": "https://arxiv.org/pdf/2504.21226", "abs": "https://arxiv.org/abs/2504.21226", "authors": ["Jiaqi Liu", "Ran Tong", "Aowei Shen", "Shuzheng Li", "Changlin Yang", "Lisha Xu"], "title": "MemeBLIP2: A novel lightweight multimodal system to detect harmful memes", "categories": ["cs.CV", "cs.AI"], "comment": "11pages,2 figures, manucripts in preparation", "summary": "Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material."}
{"id": "2504.21024", "pdf": "https://arxiv.org/pdf/2504.21024", "abs": "https://arxiv.org/abs/2504.21024", "authors": ["Tianqing Fang", "Hongming Zhang", "Zhisong Zhang", "Kaixin Ma", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Agent self-improvement, where the backbone Large Language Model (LLM) of the\nagent are trained on trajectories sampled autonomously based on their own\npolicies, has emerged as a promising approach for enhancing performance. Recent\nadvancements, particularly in web environments, face a critical limitation:\ntheir performance will reach a stagnation point during autonomous learning\ncycles, hindering further improvement. We argue that this stems from limited\nexploration of the web environment and insufficient exploitation of pre-trained\nweb knowledge in LLMs. To improve the performance of self-improvement, we\npropose a novel framework that introduces a co-evolving World Model LLM. This\nworld model predicts the next observation based on the current observation and\naction within the web environment. Leveraging LLMs' pretrained knowledge of\nabundant web content, the World Model serves dual roles: (1) as a virtual web\nserver generating self-instructed training data to continuously refine the\nagent's policy, and (2) as an imagination engine during inference, enabling\nlook-ahead simulation to guide action selection for the agent LLM. Experiments\nin real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a\n10% performance gain over existing self-evolving agents, demonstrating the\nefficacy and generalizability of our approach, without using any distillation\nfrom more powerful close-sourced models. Our work establishes the necessity of\nintegrating world models into autonomous agent frameworks to unlock sustained\nadaptability."}
{"id": "2504.21582", "pdf": "https://arxiv.org/pdf/2504.21582", "abs": "https://arxiv.org/abs/2504.21582", "authors": ["Qirui Mi", "Mengyue Yang", "Xiangning Yu", "Zhiyu Zhao", "Cheng Deng", "Bo An", "Haifeng Zhang", "Xu Chen", "Jun Wang"], "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework", "categories": ["cs.MA", "cs.AI"], "comment": "27 pages, 8 figures, 4 tables", "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation."}
{"id": "2409.16937", "pdf": "https://arxiv.org/pdf/2409.16937", "abs": "https://arxiv.org/abs/2409.16937", "authors": ["Yuanchao Li", "Zixing Zhang", "Jing Han", "Peter Bell", "Catherine Lai"], "title": "Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines."}
{"id": "2409.17899", "pdf": "https://arxiv.org/pdf/2409.17899", "abs": "https://arxiv.org/abs/2409.17899", "authors": ["Yujia Sun", "Zeyu Zhao", "Korin Richmond", "Yuanchao Li"], "title": "Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems."}
{"id": "2504.16405", "pdf": "https://arxiv.org/pdf/2504.16405", "abs": "https://arxiv.org/abs/2504.16405", "authors": ["Lancheng Gao", "Ziheng Jia", "Yunhao Zeng", "Wei Sun", "Yiming Zhang", "Wei Zhou", "Guangtao Zhai", "Xiongkuo Min"], "title": "EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment", "categories": ["cs.MM"], "comment": null, "summary": "The furnishing of multi-modal large language models (MLLMs) has led to the\nemergence of numerous benchmark studies, particularly those evaluating their\nperception and understanding capabilities.\n  Among these, understanding image-evoked emotions aims to enhance MLLMs'\nempathy, with significant applications such as human-machine interaction and\nadvertising recommendations. However, current evaluations of this MLLM\ncapability remain coarse-grained, and a systematic and comprehensive assessment\nis still lacking.\n  To this end, we introduce EEmo-Bench, a novel benchmark dedicated to the\nanalysis of the evoked emotions in images across diverse content categories.\n  Our core contributions include:\n  1) Regarding the diversity of the evoked emotions, we adopt an emotion\nranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional\nattributes for emotional assessment. In line with this methodology, 1,960\nimages are collected and manually annotated.\n  2) We design four tasks to evaluate MLLMs' ability to capture the evoked\nemotions by single images and their associated attributes: Perception, Ranking,\nDescription, and Assessment. Additionally, image-pairwise analysis is\nintroduced to investigate the model's proficiency in performing joint and\ncomparative analysis.\n  In total, we collect 6,773 question-answer pairs and perform a thorough\nassessment on 19 commonly-used MLLMs.\n  The results indicate that while some proprietary and large-scale open-source\nMLLMs achieve promising overall performance, the analytical capabilities in\ncertain evaluation dimensions remain suboptimal.\n  Our EEmo-Bench paves the path for further research aimed at enhancing the\ncomprehensive perceiving and understanding capabilities of MLLMs concerning\nimage-evoked emotions, which is crucial for machine-centric emotion perception\nand understanding."}
{"id": "2504.21778", "pdf": "https://arxiv.org/pdf/2504.21778", "abs": "https://arxiv.org/abs/2504.21778", "authors": ["Ayman A. Ameen", "Thomas Richter", "André Kaup"], "title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology."}
{"id": "2504.21064", "pdf": "https://arxiv.org/pdf/2504.21064", "abs": "https://arxiv.org/abs/2504.21064", "authors": ["Chengkai Yang", "Xingping Dong", "Xiaofen Zong"], "title": "Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data-driven approaches for depression diagnosis have emerged as a significant\nresearch focus in neuromedicine, driven by the development of relevant\ndatasets. Recently, graph neural network (GNN)-based models have gained\nwidespread adoption due to their ability to capture brain channel functional\nconnectivity from both spatial and temporal perspectives. However, their\neffectiveness is hindered by the absence of a robust temporal biomarker. In\nthis paper, we introduce a novel and effective biomarker for depression\ndiagnosis by leveraging the discrete Fourier transform (DFT) and propose a\ncustomized graph network architecture based on Temporal Graph Convolutional\nNetwork (TGCN). Our model was trained on a dataset comprising 1,086 subjects,\nwhich is over 10 times larger than previous datasets in the field of depression\ndiagnosis. Furthermore, to align with medical requirements, we performed\npropensity score matching (PSM) to create a refined subset, referred to as the\nPSM dataset. Experimental results demonstrate that incorporating our newly\ndesigned biomarker enhances the representation of temporal characteristics in\nbrain channels, leading to improved F1 scores in both the real-world dataset\nand the PSM dataset. This advancement has the potential to contribute to the\ndevelopment of more effective depression diagnostic tools. In addition, we used\nSHapley Additive exPlaination (SHAP) to validate the interpretability of our\nmodel, ensuring its practical applicability in medical settings."}
{"id": "2504.21370", "pdf": "https://arxiv.org/pdf/2504.21370", "abs": "https://arxiv.org/abs/2504.21370", "authors": ["Jingyang Yi", "Jiazheng Wang"], "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning", "categories": ["cs.AI"], "comment": "An appendix will be uploaded soon", "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible."}
{"id": "2504.21231", "pdf": "https://arxiv.org/pdf/2504.21231", "abs": "https://arxiv.org/abs/2504.21231", "authors": ["Manikanta Varaganti", "Amulya Vankayalapati", "Nour Awad", "Gregory R. Dion", "Laura J. Brattain"], "title": "T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "submitted to IEEE EMBC 2025", "summary": "Neck ultrasound (US) plays a vital role in airway management by providing\nnon-invasive, real-time imaging that enables rapid and precise interventions.\nDeep learning-based anatomical landmark detection in neck US can further\nfacilitate procedural efficiency. However, class imbalance within datasets,\nwhere key structures like tracheal rings and vocal folds are underrepresented,\npresents significant challenges for object detection models. To address this,\nwe propose T2ID-CAS, a hybrid approach that combines a text-to-image latent\ndiffusion model with class-aware sampling to generate high-quality synthetic\nsamples for underrepresented classes. This approach, rarely explored in the\nultrasound domain, improves the representation of minority classes.\nExperimental results using YOLOv9 for anatomical landmark detection in neck US\ndemonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,\nsignificantly surpassing the baseline of 66. This highlights its potential as a\ncomputationally efficient and scalable solution for mitigating class imbalance\nin AI-assisted ultrasound-guided interventions."}
{"id": "2504.21025", "pdf": "https://arxiv.org/pdf/2504.21025", "abs": "https://arxiv.org/abs/2504.21025", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain", "Md. Ridwanul Islam"], "title": "Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh", "categories": ["cs.CL"], "comment": "It has been accepted in IEEE 27th International Conference on\n  Computer and Information Technology (ICCIT). Now, we are waiting for it to\n  get published in IEEE Xplore", "summary": "Road accidents pose significant concerns globally. They lead to large\nfinancial losses, injuries, disabilities, and societal challenges. Accurate and\ntimely accident data is essential for predicting and mitigating these events.\nThis paper presents a novel framework named 'Durghotona GPT' that integrates\nweb scraping and Large Language Models (LLMs) to automate the generation of\ncomprehensive accident datasets from prominent national dailies in Bangladesh.\nThe authors collected accident reports from three major newspapers: Prothom\nAlo, Dhaka Tribune, and The Daily Star. The collected news was then processed\nusing the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework\nefficiently extracts relevant information, categorizes reports, and compiles\ndetailed datasets. Thus, this framework overcomes limitations of manual data\ncollection methods such as delays, errors, and communication gaps. The authors'\nevaluation demonstrates that Llama-3, an open-source model, performs comparably\nto GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it\ncan be considered a cost-effective alternative for similar tasks. The results\nsuggest that the framework developed by the authors can drastically enhance the\nquality and availability of accident data. As a result, it can support critical\napplications in traffic safety analysis, urban planning, and public health. The\nauthors also developed an interface for 'Durghotona GPT' for ease of use as\npart of this paper. Future work will focus on expanding data collection methods\nand refining LLMs to further increase dataset accuracy and applicability."}
{"id": "2501.11219", "pdf": "https://arxiv.org/pdf/2501.11219", "abs": "https://arxiv.org/abs/2501.11219", "authors": ["Masahiko Ueda", "Ayaka Fujita"], "title": "Zero-determinant strategies in repeated continuously-relaxed games", "categories": ["physics.soc-ph", "cs.MA"], "comment": "22 pages, 3 figures", "summary": "Mixed extension has played an important role in game theory, especially in\nthe proof of the existence of Nash equilibria in strategic form games. Mixed\nextension can be regarded as continuous relaxation of a strategic form game.\nRecently, in repeated games, a class of behavior strategies, called\nzero-determinant strategies, was introduced. Zero-determinant strategies\ncontrol payoffs of players by unilaterally enforcing linear relations between\npayoffs. There are many attempts to extend zero-determinant strategies so as to\napply them to broader situations. Here, we extend zero-determinant strategies\nto repeated games where action sets of players in stage game are continuously\nrelaxed. We see that continuous relaxation broadens the range of possible\nzero-determinant strategies, compared to the original repeated games.\nFurthermore, we introduce a special type of zero-determinant strategies, called\none-point zero-determinant strategies, which repeat only one\ncontinuously-relaxed action in all rounds. By investigating several examples,\nwe show that some property of mixed-strategy Nash equilibria can be\nreinterpreted as a payoff-control property of one-point zero-determinant\nstrategies."}
{"id": "2501.04644", "pdf": "https://arxiv.org/pdf/2501.04644", "abs": "https://arxiv.org/abs/2501.04644", "authors": ["Hanzhao Li", "Yuke Li", "Xinsheng Wang", "Jingbin Hu", "Qicong Xie", "Shan Yang", "Lei Xie"], "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts", "categories": ["eess.AS", "cs.SD"], "comment": "14 pages, 3 figures", "summary": "Controllable speech generation methods typically rely on single or fixed\nprompts, hindering creativity and flexibility. These limitations make it\ndifficult to meet specific user needs in certain scenarios, such as adjusting\nthe style while preserving a selected speaker's timbre, or choosing a style and\ngenerating a voice that matches a character's visual appearance. To overcome\nthese challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech\ngeneration framework that allows for more flexible manipulation of speech\nattributes by integrating various forms of control. FleSpeech employs a\nmultimodal prompt encoder that processes and unifies different text, audio, and\nvisual prompts into a cohesive representation. This approach enhances the\nadaptability of speech synthesis and supports creative and precise control over\nthe generated speech. Additionally, we develop a data collection pipeline for\nmultimodal datasets to facilitate further research and applications in this\nfield. Comprehensive subjective and objective experiments demonstrate the\neffectiveness of FleSpeech. Audio samples are available at\nhttps://kkksuper.github.io/FleSpeech/"}
{"id": "2411.19509", "pdf": "https://arxiv.org/pdf/2411.19509", "abs": "https://arxiv.org/abs/2411.19509", "authors": ["Tianqi Li", "Ruobing Zheng", "Minghui Yang", "Jingdong Chen", "Ming Yang"], "title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "Project Page: https://digital-avatar.github.io/ai/Ditto/", "summary": "Recent advances in diffusion models have endowed talking head synthesis with\nsubtle expressions and vivid head movements, but have also led to slow\ninference speed and insufficient control over generated results. To address\nthese issues, we propose Ditto, a diffusion-based talking head framework that\nenables fine-grained controls and real-time inference. Specifically, we utilize\nan off-the-shelf motion extractor and devise a diffusion transformer to\ngenerate representations in a specific motion space. We optimize the model\narchitecture and training strategy to address the issues in generating motion\nrepresentations, including insufficient disentanglement between motion and\nidentity, and large internal discrepancies within the representation. Besides,\nwe employ diverse conditional signals while establishing a mapping between\nmotion representation and facial semantics, enabling control over the\ngeneration process and correction of the results. Moreover, we jointly optimize\nthe holistic framework to enable streaming processing, real-time inference, and\nlow first-frame delay, offering functionalities crucial for interactive\napplications such as AI assistants. Extensive experimental results demonstrate\nthat Ditto generates compelling talking head videos and exhibits superiority in\nboth controllability and real-time performance."}
{"id": "2408.05794", "pdf": "https://arxiv.org/pdf/2408.05794", "abs": "https://arxiv.org/abs/2408.05794", "authors": ["Xuanyu Su", "Yansong Li", "Diana Inkpen", "Nathalie Japkowicz"], "title": "HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes", "categories": ["cs.AI", "cs.CL", "cs.MM", "cs.SI"], "comment": "Accepted at NAACL 2025 Findings; camera-ready version", "summary": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread\napplication in generating and interpreting complex content, the risk of\npropagating biased and harmful memes remains significant. Current safety\nmeasures often fail to detect subtly integrated hateful content within\n``Confounder Memes''. To address this, we introduce \\textsc{HateSieve}, a new\nframework designed to enhance the detection and segmentation of hateful\nelements in memes. \\textsc{HateSieve} features a novel Contrastive Meme\nGenerator that creates semantically paired memes, a customized triplet dataset\nfor contrastive learning, and an Image-Text Alignment module that produces\ncontext-aware embeddings for accurate meme segmentation. Empirical experiments\non the Hateful Meme Dataset show that \\textsc{HateSieve} not only surpasses\nexisting LMMs in performance with fewer trainable parameters but also offers a\nrobust mechanism for precisely identifying and isolating hateful content.\n\\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer\ndiscretion advised.}"}
{"id": "2504.21533", "pdf": "https://arxiv.org/pdf/2504.21533", "abs": "https://arxiv.org/abs/2504.21533", "authors": ["Rémi Delogne", "Laurent Jacques"], "title": "Random Features for Grassmannian Kernels", "categories": ["eess.SP", "eess.IV"], "comment": "4 pages, 1 page reference, 2 figures", "summary": "The Grassmannian manifold G(k, n) serves as a fundamental tool in signal\nprocessing, computer vision, and machine learning, where problems often involve\nclassifying, clustering, or comparing subspaces. In this work, we propose a\nsketching-based approach to approximate Grassmannian kernels using random\nprojections. We introduce three variations of kernel approximation, including\ntwo that rely on binarised sketches, offering substantial memory gains. We\nestablish theoretical properties of our method in the special case of G(1, n)\nand extend it to general G(k, n). Experimental validation demonstrates that our\nsketched kernels closely match the performance of standard Grassmannian kernels\nwhile avoiding the need to compute or store the full kernel matrix. Our\napproach enables scalable Grassmannian-based methods for large-scale\napplications in machine learning and pattern recognition."}
{"id": "2504.21065", "pdf": "https://arxiv.org/pdf/2504.21065", "abs": "https://arxiv.org/abs/2504.21065", "authors": ["Anjie Qiao", "Junjie Xie", "Weifeng Huang", "Hao Zhang", "Jiahua Rao", "Shuangjia Zheng", "Yuedong Yang", "Zhen Wang", "Guo-Bo Li", "Jinping Lei"], "title": "A 3D pocket-aware and affinity-guided diffusion model for lead optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular optimization, aimed at improving binding affinity or other\nmolecular properties, is a crucial task in drug discovery that often relies on\nthe expertise of medicinal chemists. Recently, deep learning-based 3D\ngenerative models showed promise in enhancing the efficiency of molecular\noptimization. However, these models often struggle to adequately consider\nbinding affinities with protein targets during lead optimization. Herein, we\npropose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,\nto optimize molecules with enhanced binding affinity. The model explicitly\nincorporates the knowledge of protein-ligand binding affinity to guide the\ndenoising sampling for molecule generation with high affinity. The\ncomprehensive evaluations indicated that Diffleop outperforms baseline models\nacross multiple metrics, especially in terms of binding affinity."}
{"id": "2504.21433", "pdf": "https://arxiv.org/pdf/2504.21433", "abs": "https://arxiv.org/abs/2504.21433", "authors": ["Zhicong Li", "Hangyu Mao", "Jiangjin Yin", "Mingzhe Xing", "Zhiwei Xu", "Yuanxing Zhang", "Yang Xiao"], "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This paper argues that the next generation of AI agent (NGENT) should\nintegrate across-domain abilities to advance toward Artificial General\nIntelligence (AGI). Although current AI agents are effective in specialized\ntasks such as robotics, role-playing, and tool-using, they remain confined to\nnarrow domains. We propose that future AI agents should synthesize the\nstrengths of these specialized systems into a unified framework capable of\noperating across text, vision, robotics, reinforcement learning, emotional\nintelligence, and beyond. This integration is not only feasible but also\nessential for achieving the versatility and adaptability that characterize\nhuman intelligence. The convergence of technologies across AI domains, coupled\nwith increasing user demand for cross-domain capabilities, suggests that such\nintegration is within reach. Ultimately, the development of these versatile\nagents is a critical step toward realizing AGI. This paper explores the\nrationale for this shift, potential pathways for achieving it."}
{"id": "2504.21247", "pdf": "https://arxiv.org/pdf/2504.21247", "abs": "https://arxiv.org/abs/2504.21247", "authors": ["Yangyang Qu", "Dazhi Fu", "Jicong Fan"], "title": "Subject Information Extraction for Novelty Detection with Domain Shifts", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised novelty detection (UND), aimed at identifying novel samples, is\nessential in fields like medical diagnosis, cybersecurity, and industrial\nquality control. Most existing UND methods assume that the training data and\ntesting normal data originate from the same domain and only consider the\ndistribution variation between training data and testing data. However, in real\nscenarios, it is common for normal testing and training data to originate from\ndifferent domains, a challenge known as domain shift. The discrepancies between\ntraining and testing data often lead to incorrect classification of normal data\nas novel by existing methods. A typical situation is that testing normal data\nand training data describe the same subject, yet they differ in the background\nconditions. To address this problem, we introduce a novel method that separates\nsubject information from background variation encapsulating the domain\ninformation to enhance detection performance under domain shifts. The proposed\nmethod minimizes the mutual information between the representations of the\nsubject and background while modelling the background variation using a deep\nGaussian mixture model, where the novelty detection is conducted on the subject\nrepresentations solely and hence is not affected by the variation of domains.\nExtensive experiments demonstrate that our model generalizes effectively to\nunseen domains and significantly outperforms baseline methods, especially under\nsubstantial domain shifts between training and testing data."}
{"id": "2504.21026", "pdf": "https://arxiv.org/pdf/2504.21026", "abs": "https://arxiv.org/abs/2504.21026", "authors": ["Manish Pandey", "Nageshwar Prasad Yadav", "Mokshada Adduru", "Sawan Rai"], "title": "Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "With the growing presence of multilingual users on social media, detecting\nabusive language in code-mixed text has become increasingly challenging.\nCode-mixed communication, where users seamlessly switch between English and\ntheir native languages, poses difficulties for traditional abuse detection\nmodels, as offensive content may be context-dependent or obscured by linguistic\nblending. While abusive language detection has been extensively explored for\nhigh-resource languages like English and Hindi, low-resource languages such as\nTelugu and Nepali remain underrepresented, leaving gaps in effective\nmoderation. In this study, we introduce a novel, manually annotated dataset of\n2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized\nas abusive and non-abusive, collected from various social media platforms. The\ndataset undergoes rigorous preprocessing before being evaluated across multiple\nMachine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We\nexperimented with models including Logistic Regression, Random Forest, Support\nVector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing\ntheir performance through hyperparameter tuning, and evaluate it using 10-fold\ncross-validation and statistical significance testing (t-test). Our findings\nprovide key insights into the challenges of detecting abusive language in\ncode-mixed settings and offer a comparative analysis of computational\napproaches. This study contributes to advancing NLP for low-resource languages\nby establishing benchmarks for abusive language detection in Telugu-English and\nNepali-English code-mixed text. The dataset and insights can aid in the\ndevelopment of more robust moderation strategies for multilingual social media\nenvironments."}
{"id": "2502.11785", "pdf": "https://arxiv.org/pdf/2502.11785", "abs": "https://arxiv.org/abs/2502.11785", "authors": ["Rustam Galimullin", "Maksim Gladyshev", "Munyque Mittelmann", "Nima Motamed"], "title": "Changing the Rules of the Game: Reasoning about Dynamic Phenomena in Multi-Agent Systems", "categories": ["cs.LO", "cs.MA", "F.4.1; I.2.4; I.2.11"], "comment": "Extended version of the AAMAS 2025 paper of the same name. In this\n  version, proof of Theorem 4.1 is corrected, and all new text is in blue. We\n  want to thank St\\'ephane Demri for spotting the problem", "summary": "The design and application of multi-agent systems (MAS) require reasoning\nabout the effects of modifications on their underlying structure. In\nparticular, such changes may impact the satisfaction of system specifications\nand the strategic abilities of their autonomous components. In this paper, we\nare concerned with the problem of verifying and synthesising modifications (or\nupdates) of MAS. We propose an extension of the Alternating-Time Temporal Logic\n($\\mathsf{ATL}$) that enables reasoning about the dynamics of model change,\ncalled the Logic for $\\mathsf{ATL}$ Model Building ($\\mathsf{LAMB}$). We show\nhow $\\mathsf{LAMB}$ can express various intuitions and ideas about the dynamics\nof MAS, from normative updates to mechanism design. As the main technical\nresult, we prove that, while being strictly more expressive than\n$\\mathsf{ATL}$, $\\mathsf{LAMB}$ enjoys a P-complete model-checking procedure."}
{"id": "2504.18539", "pdf": "https://arxiv.org/pdf/2504.18539", "abs": "https://arxiv.org/abs/2504.18539", "authors": ["Sungnyun Kim", "Sungwoo Cho", "Sangmin Bae", "Kangwook Jang", "Se-Young Yun"], "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "comment": "ICLR 2025; 22 pages, 6 figures, 14 tables", "summary": "Audio-visual speech recognition (AVSR) incorporates auditory and visual\nmodalities to improve recognition accuracy, particularly in noisy environments\nwhere audio-only speech systems are insufficient. While previous research has\nlargely addressed audio disruptions, few studies have dealt with visual\ncorruptions, e.g., lip occlusions or blurred videos, which are also\ndetrimental. To address this real-world challenge, we propose CAV2vec, a novel\nself-supervised speech representation learning framework particularly designed\nto handle audio-visual joint corruption. CAV2vec employs a self-distillation\napproach with a corrupted prediction task, where the student model learns to\npredict clean targets, generated by the teacher model, with corrupted input\nframes. Specifically, we suggest a unimodal multi-task learning, which distills\ncross-modal knowledge and aligns the corrupted modalities, by predicting clean\naudio targets with corrupted videos, and clean video targets with corrupted\naudios. This strategy mitigates the dispersion in the representation space\ncaused by corrupted modalities, leading to more reliable and robust\naudio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that\nthe corrupted representation learning method significantly enhances recognition\naccuracy across generalized environments involving various types of corruption.\nOur code is available at https://github.com/sungnyun/cav2vec."}
{"id": "2504.09948", "pdf": "https://arxiv.org/pdf/2504.09948", "abs": "https://arxiv.org/abs/2504.09948", "authors": ["Huijie Liu", "Bingcan Wang", "Jie Hu", "Xiaoming Wei", "Guoliang Kang"], "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 10 figures, 3 tables", "summary": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods."}
{"id": "2504.21632", "pdf": "https://arxiv.org/pdf/2504.21632", "abs": "https://arxiv.org/abs/2504.21632", "authors": ["Fuma Ito", "Chihiro Tsutake", "Keita Takahashi", "Toshiaki Fujii"], "title": "Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification", "categories": ["cs.IT", "eess.IV", "math.IT"], "comment": null, "summary": "To efficiently compress the sign information of images, we address a sign\nretrieval problem for the block-wise discrete cosine transformation~(DCT):\nreconstruction of the signs of DCT coefficients from their amplitudes. To this\nend, we propose a fast sign retrieval method on the basis of binary\nclassification machine learning. We first introduce 3D representations of the\namplitudes and signs, where we pack amplitudes/signs belonging to the same\nfrequency band into a 2D slice, referred to as the sub-band block. We then\nretrieve the signs from the 3D amplitudes via binary classification, where each\nsign is regarded as a binary label. We implement a binary classification\nalgorithm using convolutional neural networks, which are advantageous for\nefficiently extracting features in the 3D amplitudes. Experimental results\ndemonstrate that our method achieves accurate sign retrieval with an\noverwhelmingly low computation cost."}
{"id": "2504.21066", "pdf": "https://arxiv.org/pdf/2504.21066", "abs": "https://arxiv.org/abs/2504.21066", "authors": ["Andreas Karathanasis", "John Violos", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training and deploying deepfake detection models on edge devices offers the\nadvantage of maintaining data privacy and confidentiality by processing it\nclose to its source. However, this approach is constrained by the limited\ncomputational and memory resources available at the edge. To address this\nchallenge, we explore compression techniques to reduce computational demands\nand inference time, alongside transfer learning methods to minimize training\noverhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate\nthe effectiveness of pruning, knowledge distillation (KD), quantization,\nfine-tuning, and adapter-based techniques. Our experimental results demonstrate\nthat both compression and transfer learning can be effectively achieved, even\nwith a high compression level of 90%, remaining at the same performance level\nwhen the training and validation data originate from the same DeepFake model.\nHowever, when the testing dataset is generated by DeepFake models not present\nin the training set, a domain generalization issue becomes evident."}
{"id": "2504.21568", "pdf": "https://arxiv.org/pdf/2504.21568", "abs": "https://arxiv.org/abs/2504.21568", "authors": ["Shui-jin Rong", "Wei Guo", "Da-qing Zhang"], "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks", "categories": ["cs.AI"], "comment": null, "summary": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts."}
{"id": "2504.21248", "pdf": "https://arxiv.org/pdf/2504.21248", "abs": "https://arxiv.org/abs/2504.21248", "authors": ["Ezra Engel", "Lishan Li", "Chris Hudy", "Robert Schleusner"], "title": "Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Facial expression recognition (FER) is a subset of computer vision with\nimportant applications for human-computer-interaction, healthcare, and customer\nservice. FER represents a challenging problem-space because accurate\nclassification requires a model to differentiate between subtle changes in\nfacial features. In this paper, we examine the use of multi-modal transfer\nlearning to improve performance on a challenging video-based FER dataset,\nDynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained\nResNets, OpenPose, and OmniVec networks, we explore the impact of\ncross-temporal, multi-modal features on classification accuracy. Ultimately, we\nfind that these finely-tuned multi-modal feature generators modestly improve\naccuracy of our transformer-based classification model."}
{"id": "2504.21027", "pdf": "https://arxiv.org/pdf/2504.21027", "abs": "https://arxiv.org/abs/2504.21027", "authors": ["Yu Zheng", "Longyi Liu", "Yuming Lin", "Jie Feng", "Guozhen Zhang", "Depeng Jin", "Yong Li"], "title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) holds promise for revolutionizing\nvarious fields traditionally dominated by human expertise. Urban planning, a\nprofessional discipline that fundamentally shapes our daily surroundings, is\none such field heavily relying on multifaceted domain knowledge and experience\nof human experts. The extent to which LLMs can assist human practitioners in\nurban planning remains largely unexplored. In this paper, we introduce a\ncomprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of\nLLMs in urban planning, which encompasses fundamental principles, professional\nknowledge, and management and regulations, aligning closely with the\nqualifications expected of human planners. Through extensive evaluation, we\nreveal a significant imbalance in the acquisition of planning knowledge among\nLLMs, with even the most proficient models falling short of meeting\nprofessional standards. For instance, we observe that 70% of LLMs achieve\nsubpar performance in understanding planning regulations compared to other\naspects. Besides the benchmark, we present the largest-ever supervised\nfine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction\npairs sourced from urban planning exams and textbooks. Our findings demonstrate\nthat fine-tuned models exhibit enhanced performance in memorization tests and\ncomprehension of urban planning knowledge, while there exists significant room\nfor improvement, particularly in tasks requiring domain-specific terminology\nand reasoning. By making our benchmark, dataset, and associated evaluation and\nfine-tuning toolsets publicly available at\nhttps://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the\nintegration of LLMs into practical urban planning, fostering a symbiotic\ncollaboration between human expertise and machine intelligence."}
{"id": "2504.20091", "pdf": "https://arxiv.org/pdf/2504.20091", "abs": "https://arxiv.org/abs/2504.20091", "authors": ["Noriyuki Kugo", "Xiang Li", "Zixin Li", "Ashish Gupta", "Arpandeep Khatua", "Nidhish Jain", "Chaitanya Patel", "Yuta Kyuragi", "Yasunori Ishii", "Masamoto Tanabiki", "Kazuki Kozuka", "Ehsan Adeli"], "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "Video Question Answering (VQA) inherently relies on multimodal reasoning,\nintegrating visual, temporal, and linguistic cues to achieve a deeper\nunderstanding of video content. However, many existing methods rely on feeding\nframe-level captions into a single model, making it difficult to adequately\ncapture temporal and interactive contexts. To address this limitation, we\nintroduce VideoMultiAgents, a framework that integrates specialized agents for\nvision, scene graph analysis, and text processing. It enhances video\nunderstanding leveraging complementary multimodal reasoning from independently\noperating agents. Our approach is also supplemented with a question-guided\ncaption generation, which produces captions that highlight objects, actions,\nand temporal transitions directly relevant to a given query, thus improving the\nanswer accuracy. Experimental results demonstrate that our method achieves\nstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),\nEgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%). The source code is\navailable at https://github.com/PanasonicConnect/VideoMultiAgents."}
{"id": "2412.06314", "pdf": "https://arxiv.org/pdf/2412.06314", "abs": "https://arxiv.org/abs/2412.06314", "authors": ["Yijie Dang", "Weijun Ma", "Xiaohu Luo", "Huaizhu Wang"], "title": "CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published in Medical Image Analysis, Volume 103, 2025, Pages 103583.\n  DOI: 10.1016/j.media.2025.103583 This is the author's pre-print version prior\n  to final journal edits. Final published version available at:\n  https://www.sciencedirect.com/science/article/pii/S1361841525001306", "summary": "Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has\nemerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical\nsettings, the segmentation of lung infections from computed tomography images\nenables rapid and accurate quantification and diagnosis of COVID-19.\nSegmentation of COVID-19 infections in the lungs poses a formidable challenge,\nprimarily due to the indistinct boundaries and limited contrast presented by\nground glass opacity manifestations. Moreover, the confounding similarity\nbetween infiltrates, lung tissues, and lung walls further complicates this\nsegmentation task. To address these challenges, this paper introduces a novel\ndeep network architecture, called CAD-Unet, for segmenting COVID-19 lung\ninfections. In this architecture, capsule networks are incorporated into the\nexisting Unet framework. Capsule networks represent a novel network\narchitecture that differs from traditional convolutional neural networks. They\nutilize vectors for information transfer among capsules, facilitating the\nextraction of intricate lesion spatial information. Additionally, we design a\ncapsule encoder path and establish a coupling path between the unet encoder and\nthe capsule encoder. This design maximizes the complementary advantages of both\nnetwork structures while achieving efficient information fusion. \\noindent\nFinally, extensive experiments are conducted on four publicly available\ndatasets, encompassing binary segmentation tasks and multi-class segmentation\ntasks. The experimental results demonstrate the superior segmentation\nperformance of the proposed model. The code has been released at:\nhttps://github.com/AmanoTooko-jie/CAD-Unet."}
{"id": "2504.21069", "pdf": "https://arxiv.org/pdf/2504.21069", "abs": "https://arxiv.org/abs/2504.21069", "authors": ["Anuradha Kumari", "Mushir Akhtar", "P. N. Suganthan", "M. Tanveer"], "title": "R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework", "categories": ["cs.LG"], "comment": null, "summary": "The random vector functional link (RVFL) neural network has shown significant\npotential in overcoming the constraints of traditional artificial neural\nnetworks, such as excessive computation time and suboptimal solutions. However,\nRVFL faces challenges when dealing with noise and outliers, as it assumes all\ndata samples contribute equally. To address this issue, we propose a novel\nrobust framework, R2VFL, RVFL with Huber weighting function and class\nprobability, which enhances the model's robustness and adaptability by\neffectively mitigating the impact of noise and outliers in the training data.\nThe Huber weighting function reduces the influence of outliers, while the class\nprobability mechanism assigns less weight to noisy data points, resulting in a\nmore resilient model. We explore two distinct approaches for calculating class\ncenters within the R2VFL framework: the simple average of all data points in\neach class and the median of each feature, the later providing a robust\nalternative by minimizing the effect of extreme values. These approaches give\nrise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively\nevaluate the proposed models on 47 UCI datasets, encompassing both binary and\nmulticlass datasets, and conduct rigorous statistical testing, which confirms\nthe superiority of the proposed models. Notably, the models also demonstrate\nexceptional performance in classifying EEG signals, highlighting their\npractical applicability in real-world biomedical domain."}
{"id": "2504.21643", "pdf": "https://arxiv.org/pdf/2504.21643", "abs": "https://arxiv.org/abs/2504.21643", "authors": ["Luca Marzari", "Francesco Trotti", "Enrico Marchesini", "Alessandro Farinelli"], "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios."}
{"id": "2504.21266", "pdf": "https://arxiv.org/pdf/2504.21266", "abs": "https://arxiv.org/abs/2504.21266", "authors": ["Zhifu Zhao", "Hanyang Hua", "Jianan Li", "Shaoxin Wu", "Fu Li", "Yangtao Zhou", "Yang Li"], "title": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton."}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117", "abs": "https://arxiv.org/abs/2504.21117", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Evaluating natural language generation (NLG) systems is challenging due to\nthe diversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\nbut is highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."}
{"id": "2503.03327", "pdf": "https://arxiv.org/pdf/2503.03327", "abs": "https://arxiv.org/abs/2503.03327", "authors": ["Saqib Qamar", "Syed Furqan Qadri", "Roobaea Alroobaea", "Goram Mufarah M Alshmrani", "Richard Jiang"], "title": "ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Melanoma is a malignant tumor originating from skin cell lesions. Accurate\nand efficient segmentation of skin lesions is essential for quantitative\nmedical analysis but remains challenging. To address this, we propose\nScaleFusionNet, a segmentation model that integrates Cross-Attention\nTransformer Module (CATM) and AdaptiveFusionBlock to enhance feature extraction\nand fusion. The model employs a hybrid architecture encoder that effectively\ncaptures both local and global features. We introduce CATM, which utilizes Swin\nTransformer Blocks and Cross Attention Fusion (CAF) to adaptively refine\nencoder-decoder feature fusion, reducing semantic gaps and improving\nsegmentation accuracy. Additionally, the AdaptiveFusionBlock is improved by\nintegrating adaptive multi-scale fusion, where Swin Transformer-based attention\ncomplements deformable convolution-based multi-scale feature extraction. This\nenhancement refines lesion boundaries and preserves fine-grained details.\nScaleFusionNet achieves Dice scores of 92.94% and 91.65% on ISIC-2016 and\nISIC-2018 datasets, respectively, demonstrating its effectiveness in skin\nlesion analysis. Our code implementation is publicly available at GitHub."}
{"id": "2504.21099", "pdf": "https://arxiv.org/pdf/2504.21099", "abs": "https://arxiv.org/abs/2504.21099", "authors": ["Jieming Bian", "Yuanzhe Peng", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "survey paper, under updating", "summary": "Foundation models have revolutionized artificial intelligence by providing\nrobust, versatile architectures pre-trained on large-scale datasets. However,\nadapting these massive models to specific downstream tasks requires\nfine-tuning, which can be prohibitively expensive in computational resources.\nParameter-Efficient Fine-Tuning (PEFT) methods address this challenge by\nselectively updating only a small subset of parameters. Meanwhile, Federated\nLearning (FL) enables collaborative model training across distributed clients\nwithout sharing raw data, making it ideal for privacy-sensitive applications.\nThis survey provides a comprehensive review of the integration of PEFT\ntechniques within federated learning environments. We systematically categorize\nexisting approaches into three main groups: Additive PEFT (which introduces new\ntrainable parameters), Selective PEFT (which fine-tunes only subsets of\nexisting parameters), and Reparameterized PEFT (which transforms model\narchitectures to enable efficient updates). For each category, we analyze how\nthese methods address the unique challenges of federated settings, including\ndata heterogeneity, communication efficiency, computational constraints, and\nprivacy concerns. We further organize the literature based on application\ndomains, covering both natural language processing and computer vision tasks.\nFinally, we discuss promising research directions, including scaling to larger\nfoundation models, theoretical analysis of federated PEFT methods, and\nsustainable approaches for resource-constrained environments."}
{"id": "2504.21659", "pdf": "https://arxiv.org/pdf/2504.21659", "abs": "https://arxiv.org/abs/2504.21659", "authors": ["Haotian Luo", "Haiying He", "Yibo Wang", "Jinluan Yang", "Rui Liu", "Naiqiang Tan", "Xiaochun Cao", "Dacheng Tao", "Li Shen"], "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1"}
{"id": "2504.21281", "pdf": "https://arxiv.org/pdf/2504.21281", "abs": "https://arxiv.org/abs/2504.21281", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Hua Li", "Pierre Vera", "Su Ruan"], "title": "Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal 3D medical image segmentation aims to accurately identify tumor\nregions across different modalities, facing challenges from variations in image\nintensity and tumor morphology. Traditional convolutional neural network\n(CNN)-based methods struggle with capturing global features, while\nTransformers-based methods, despite effectively capturing global context,\nencounter high computational costs in 3D medical image segmentation. The Mamba\nmodel combines linear scalability with long-distance modeling, making it a\npromising approach for visual representation learning. However, Mamba-based 3D\nmulti-modal segmentation still struggles to leverage modality-specific features\nand fuse complementary information effectively. In this paper, we propose a\nMamba based feature extraction and adaptive multilevel feature fusion for 3D\ntumor segmentation using multi-modal medical image. We first develop the\nspecific modality Mamba encoder to efficiently extract long-range relevant\nfeatures that represent anatomical and pathological structures present in each\nmodality. Moreover, we design an bi-level synergistic integration block that\ndynamically merges multi-modal and multi-level complementary features by the\nmodality attention and channel attention learning. Lastly, the decoder combines\ndeep semantic information with fine-grained details to generate the tumor\nsegmentation map. Experimental results on medical image datasets (PET/CT and\nMRI multi-sequence) show that our approach achieve competitive performance\ncompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches."}
{"id": "2504.21132", "pdf": "https://arxiv.org/pdf/2504.21132", "abs": "https://arxiv.org/abs/2504.21132", "authors": ["Naheed Rayhan", "Md. Ashrafuzzaman"], "title": "LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have demonstrated the\ncapability to generate human like, natural responses across a range of tasks,\nincluding task oriented dialogue and question answering. However, their\napplication in real world, critical scenarios is often hindered by a tendency\nto produce inaccurate information and a limited ability to leverage external\nknowledge sources. This paper introduces the LLM ENHANCER system, designed to\nintegrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to\nenhance data accuracy. The LLMs employed within this system are open source.\nThe data acquisition process for the LLM ENHANCER system operates in parallel,\nutilizing custom agent tools to manage the flow of information. Vector\nembeddings are used to identify the most pertinent information, which is\nsubsequently supplied to the LLM for user interaction. The LLM ENHANCER system\nmitigates hallucinations in chat based LLMs while preserving response\nnaturalness and accuracy."}
{"id": "2409.09779", "pdf": "https://arxiv.org/pdf/2409.09779", "abs": "https://arxiv.org/abs/2409.09779", "authors": ["Chengqin Wu", "Shuai Yu", "Tuyan Luo", "Qiuhua Rao", "Qingson Hu", "Jingxiang Xu", "Lijun Zhang"], "title": "Underwater Image Enhancement via Dehazing and Color Restoration", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Underwater visual imaging is crucial for marine engineering, but it suffers\nfrom low contrast, blurriness, and color degradation, which hinders downstream\nanalysis. Existing underwater image enhancement methods often treat the haze\nand color cast as a unified degradation process, neglecting their inherent\nindependence while overlooking their synergistic relationship. To overcome this\nlimitation, we propose a Vision Transformer (ViT)-based network (referred to as\nWaterFormer) to improve underwater image quality. WaterFormer contains three\nmajor components: a dehazing block (DehazeFormer Block) to capture the\nself-correlated haze features and extract deep-level features, a Color\nRestoration Block (CRB) to capture self-correlated color cast features, and a\nChannel Fusion Block (CFB) that dynamically integrates these decoupled features\nto achieve comprehensive enhancement. To ensure authenticity, a soft\nreconstruction layer based on the underwater imaging physics model is included.\nFurther, a Chromatic Consistency Loss and Sobel Color Loss are designed to\nrespectively preserve color fidelity and enhance structural details during\nnetwork training. Comprehensive experimental results demonstrate that\nWaterFormer outperforms other state-of-the-art methods in enhancing underwater\nimages."}
{"id": "2504.21152", "pdf": "https://arxiv.org/pdf/2504.21152", "abs": "https://arxiv.org/abs/2504.21152", "authors": ["Shayan Alahyari", "Mike Domaratzki"], "title": "SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Imbalanced regression refers to prediction tasks where the target variable is\nskewed. This skewness hinders machine learning models, especially neural\nnetworks, which concentrate on dense regions and therefore perform poorly on\nunderrepresented (minority) samples. Despite the importance of this problem,\nonly a few methods have been proposed for imbalanced regression. Many of the\navailable solutions for imbalanced regression adapt techniques from the class\nimbalance domain, such as linear interpolation and the addition of Gaussian\nnoise, to create synthetic data in sparse regions. However, in many cases, the\nunderlying distribution of the data is complex and non-linear. Consequently,\nthese approaches generate synthetic samples that do not accurately represent\nthe true feature-target relationship. To overcome these limitations, we propose\nSMOGAN, a two-step oversampling framework for imbalanced regression. In Stage\n1, an existing oversampler generates initial synthetic samples in sparse target\nregions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves\nas SMOGAN's filtering layer and refines these samples via adversarial loss\naugmented with a Maximum Mean Discrepancy objective, aligning them with the\ntrue joint feature-target distribution. Extensive experiments on 23 imbalanced\ndatasets show that SMOGAN consistently outperforms the default oversampling\nmethod without the DistGAN filtering layer."}
{"id": "2504.21683", "pdf": "https://arxiv.org/pdf/2504.21683", "abs": "https://arxiv.org/abs/2504.21683", "authors": ["Kenneth Skiba", "Tjitze Rienstra", "Matthias Thimm", "Jesse Heyninck", "Gabriele Kern-Isberner"], "title": "Extension-ranking Semantics for Abstract Argumentation Preprint", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we present a general framework for ranking sets of arguments\nin abstract argumentation based on their plausibility of acceptance. We present\na generalisation of Dung's extension semantics as extension-ranking semantics,\nwhich induce a preorder over the power set of all arguments, allowing us to\nstate that one set is \"closer\" to being acceptable than another. To evaluate\nthe extension-ranking semantics, we introduce a number of principles that a\nwell-behaved extension-ranking semantics should satisfy. We consider several\nsimple base relations, each of which models a single central aspect of\nargumentative reasoning. The combination of these base relations provides us\nwith a family of extension-ranking semantics. We also adapt a number of\napproaches from the literature for ranking extensions to be usable in the\ncontext of extension-ranking semantics, and evaluate their behaviour."}
{"id": "2504.21292", "pdf": "https://arxiv.org/pdf/2504.21292", "abs": "https://arxiv.org/abs/2504.21292", "authors": ["ZiYi Dong", "Chengxing Zhou", "Weijian Deng", "Pengxu Wei", "Xiangyang Ji", "Liang Lin"], "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions", "categories": ["cs.CV"], "comment": null, "summary": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\narchitectures have revolutionized image generation through transformer-based\nattention mechanisms. The prevailing paradigm has commonly employed\nself-attention with quadratic computational complexity to handle global spatial\nrelationships in complex images, thereby synthesizing high-fidelity images with\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\npre-trained diffusion models predominantly exhibits localized attention\npatterns, closely resembling convolutional inductive biases. This suggests that\nglobal interactions in self-attention may be less critical than commonly\nassumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional\nself-attention modules with Pyramid Convolution Blocks\n(\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized\nconvolutional operations while keeping other components frozen,\n\\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based\ncounterparts while reducing computational cost by 6929$\\times$ and surpassing\nLinFusion by 5.42$\\times$ in efficiency--all without compromising generative\nfidelity."}
{"id": "2504.21165", "pdf": "https://arxiv.org/pdf/2504.21165", "abs": "https://arxiv.org/abs/2504.21165", "authors": ["Mark Huasong Meng", "Ruizhe Wang", "Meng Xu", "Chuan Yan", "Guangdong Bai"], "title": "Detecting Manipulated Contents Using Knowledge-Grounded Inference", "categories": ["cs.CL", "cs.SI"], "comment": "16 pages", "summary": "The detection of manipulated content, a prevalent form of fake news, has been\nwidely studied in recent years. While existing solutions have been proven\neffective in fact-checking and analyzing fake news based on historical events,\nthe reliance on either intrinsic knowledge obtained during training or manually\ncurated context hinders them from tackling zero-day manipulated content, which\ncan only be recognized with real-time contextual information. In this work, we\npropose Manicod, a tool designed for detecting zero-day manipulated content.\nManicod first sources contextual information about the input claim from\nmainstream search engines, and subsequently vectorizes the context for the\nlarge language model (LLM) through retrieval-augmented generation (RAG). The\nLLM-based inference can produce a \"truthful\" or \"manipulated\" decision and\noffer a textual explanation for the decision. To validate the effectiveness of\nManicod, we also propose a dataset comprising 4270 pieces of manipulated fake\nnews derived from 2500 recent real-world news headlines. Manicod achieves an\noverall F1 score of 0.856 on this dataset and outperforms existing methods by\nup to 1.9x in F1 score on their benchmarks on fact-checking and claim\nverification."}
{"id": "2410.10005", "pdf": "https://arxiv.org/pdf/2410.10005", "abs": "https://arxiv.org/abs/2410.10005", "authors": ["Hairong Wang", "Lingchao Mao", "Zihan Zhang", "Jing Li"], "title": "SmoothSegNet: A Global-Local Framework for Liver Tumor Segmentation with Clinical KnowledgeInformed Label Smoothing", "categories": ["cs.LG", "eess.IV"], "comment": null, "summary": "Liver cancer is a leading cause of mortality worldwide, and accurate Computed\nTomography (CT)-based tumor segmentation is essential for diagnosis and\ntreatment. Manual delineation is time-intensive, prone to variability, and\nhighlights the need for reliable automation. While deep learning has shown\npromise for automated liver segmentation, precise liver tumor segmentation\nremains challenging due to the heterogeneous nature of tumors, imprecise tumor\nmargins, and limited labeled data. We present SmoothSegNet, a novel deep\nlearning framework that addresses these challenges with the three key designs:\n(1) A novel knowledge-informed label smoothing technique that distills\nknowledge from clinical data to generate smooth labels, which are used to\nregularize model training, reducing the overfitting risk and enhancing model\nperformance; (2) A global and local segmentation framework that breaks down the\nmain task into two simpler sub-tasks, allowing optimized preprocessing and\ntraining for each; and (3) Pre- and post-processing pipelines customized to the\nchallenges of each subtask aimed to enhance tumor visibility and refines tumor\nboundaries. We apply the proposed model on a challenging HCC-TACE-Seg dataset\nand show that SmoothSegNet outperformed various benchmarks in segmentation\nperformance, particularly at smaller tumors (<10cm). Our ablation studies show\nthat the three design components complementarily contribute to the model\nimproved performance. Code for the proposed method are available at\nhttps://github.com/lingchm/medassist-liver-cancer."}
{"id": "2504.21174", "pdf": "https://arxiv.org/pdf/2504.21174", "abs": "https://arxiv.org/abs/2504.21174", "authors": ["Leandro Giusti Mugnaini", "Bruno Lopes Yamamoto", "Lucas Lauton de Alcantara", "Victor Zacarias", "Edson Bollis", "Lucas Pellicer", "Anna Helena Reali Costa", "Artur Jordao"], "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning", "categories": ["cs.LG"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Deep learning drives a new wave in computing systems and triggers the\nautomation of increasingly complex problems. In particular, Large Language\nModels (LLMs) have significantly advanced cognitive tasks, often matching or\neven surpassing human-level performance. However, their extensive parameters\nresult in high computational costs and slow inference, posing challenges for\ndeployment in resource-limited settings. Among the strategies to overcome the\naforementioned challenges, pruning emerges as a successful mechanism since it\nreduces model size while maintaining predictive ability. In this paper, we\nintroduce AMP: Attention Heads and MLP Pruning, a novel structured pruning\nmethod that efficiently compresses LLMs by removing less critical structures\nwithin Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By\nprojecting the input data onto weights, AMP assesses structural importance and\novercomes the limitations of existing techniques, which often fall short in\nflexibility or efficiency. In particular, AMP surpasses the current\nstate-of-the-art on commonsense reasoning tasks by up to 1.49 percentage\npoints, achieving a 30% pruning ratio with minimal impact on zero-shot task\nperformance. Moreover, AMP also improves inference speeds, making it\nwell-suited for deployment in resource-constrained environments. We confirm the\nflexibility of AMP on different families of LLMs, including LLaMA and Phi."}
{"id": "2504.21694", "pdf": "https://arxiv.org/pdf/2504.21694", "abs": "https://arxiv.org/abs/2504.21694", "authors": ["Tom Westermann", "Malte Ramonat", "Johannes Hujer", "Felix Gehlhoff", "Alexander Fay"], "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation", "categories": ["cs.AI"], "comment": null, "summary": "AutomationML has seen widespread adoption as an open data exchange format in\nthe automation domain. It is an open and vendor neutral standard based on the\nextensible markup language XML. However, AutomationML extends XML with\nadditional semantics, that limit the applicability of common XML-tools for\napplications like querying or data validation. This article provides\npractitioners with 1) an up-to-date ontology of the concepts in the\nAutomationML-standard, as well as 2) a declarative mapping to automatically\ntransform any AutomationML model into RDF triples. Together, these artifacts\nallow practitioners an easy integration of AutomationML information into\nindustrial knowledge graphs. A study on examples from the automation domain\nconcludes that transforming AutomationML to OWL opens up new powerful ways for\nquerying and validation that are impossible without transformation."}
{"id": "2504.21294", "pdf": "https://arxiv.org/pdf/2504.21294", "abs": "https://arxiv.org/abs/2504.21294", "authors": ["Qianzi Yu", "Yang Cao", "Yu Kang"], "title": "Learning Multi-view Multi-class Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "The latest trend in anomaly detection is to train a unified model instead of\ntraining a separate model for each category. However, existing multi-class\nanomaly detection (MCAD) models perform poorly in multi-view scenarios because\nthey often fail to effectively model the relationships and complementary\ninformation among different views. In this paper, we introduce a Multi-View\nMulti-Class Anomaly Detection model (MVMCAD), which integrates information from\nmultiple views to accurately identify anomalies. Specifically, we propose a\nsemi-frozen encoder, where a pre-encoder prior enhancement mechanism is added\nbefore the frozen encoder, enabling stable cross-view feature modeling and\nefficient adaptation for improved anomaly detection. Furthermore, we propose an\nAnomaly Amplification Module (AAM) that models global token interactions and\nsuppresses normal regions to enhance anomaly signals, leading to improved\ndetection performance in multi-view settings. Finally, we propose a\nCross-Feature Loss that aligns shallow encoder features with deep decoder\nfeatures and vice versa, enhancing the model's sensitivity to anomalies at\ndifferent semantic levels under multi-view scenarios. Extensive experiments on\nthe Real-IAD dataset for multi-view multi-class anomaly detection validate the\neffectiveness of our approach, achieving state-of-the-art performance of\n91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,\nrespectively."}
{"id": "2504.21191", "pdf": "https://arxiv.org/pdf/2504.21191", "abs": "https://arxiv.org/abs/2504.21191", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Graham Sayle", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs."}
{"id": "2410.18794", "pdf": "https://arxiv.org/pdf/2410.18794", "abs": "https://arxiv.org/abs/2410.18794", "authors": ["Geoffrey Kasenbacher", "Felix Ehret", "Gerrit Ecke", "Sebastian Otte"], "title": "WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive Algorithm", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "The locally competitive algorithm (LCA) can solve sparse coding problems\nacross a wide range of use cases. Recently, convolution-based LCA approaches\nhave been shown to be highly effective for enhancing robustness for image\nrecognition tasks in vision pipelines. To additionally maximize\nrepresentational sparsity, LCA with hard-thresholding can be applied. While\nthis combination often yields very good solutions satisfying an $\\ell_0$\nsparsity criterion, it comes with significant drawbacks for practical\napplication: (i) LCA is very inefficient, typically requiring hundreds of\noptimization cycles for convergence; (ii) the use of hard-thresholding results\nin a non-convex loss function, which might lead to suboptimal minima. To\naddress these issues, we propose the Locally Competitive Algorithm with State\nWarm-up via Predictive Priming (WARP-LCA), which leverages a predictor network\nto provide a suitable initial guess of the LCA state based on the current\ninput. Our approach significantly improves both convergence speed and the\nquality of solutions, while maintaining and even enhancing the overall\nstrengths of LCA. We demonstrate that WARP-LCA converges faster by orders of\nmagnitude and reaches better minima compared to conventional LCA. Moreover, the\nlearned representations are more sparse and exhibit superior properties in\nterms of reconstruction and denoising quality as well as robustness when\napplied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image\ndenoising tasks, showcasing its robustness and practical effectiveness. Our\nfindings confirm that the naive use of LCA with hard-thresholding results in\nsuboptimal minima, whereas initializing LCA with a predictive guess results in\nbetter outcomes. This research advances the field of biologically inspired deep\nlearning by providing a novel approach to convolutional sparse coding."}
{"id": "2504.21186", "pdf": "https://arxiv.org/pdf/2504.21186", "abs": "https://arxiv.org/abs/2504.21186", "authors": ["Haoyan Xu", "Zhengtao Yao", "Xuzhi Zhang", "Ziyi Wang", "Langzhou He", "Yushun Dong", "Philip S. Yu", "Mengyuan Li", "Yue Zhao"], "title": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for ensuring the safety and\nreliability of machine learning systems, particularly in dynamic and open-world\nenvironments. In the vision and text domains, zero-shot OOD detection - which\nrequires no training on in-distribution (ID) data - has made significant\nprogress through the use of large-scale pretrained models such as\nvision-language models (VLMs) and large language models (LLMs). However,\nzero-shot OOD detection in graph-structured data remains largely unexplored,\nprimarily due to the challenges posed by complex relational structures and the\nabsence of powerful, large-scale pretrained models for graphs. In this work, we\ntake the first step toward enabling zero-shot graph OOD detection by leveraging\na graph foundation model (GFM). We show that, when provided only with class\nlabel names, the GFM can perform OOD detection without any node-level\nsupervision - outperforming existing supervised methods across multiple\ndatasets. To address the more practical setting where OOD label names are\nunavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to\ngenerate semantically informative pseudo-OOD labels from unlabeled data. These\nlabels enable the GFM to capture nuanced semantic boundaries between ID and OOD\nclasses and perform fine-grained OOD detection - without requiring any labeled\nnodes. Our approach is the first to enable node-level graph OOD detection in a\nfully zero-shot setting, and achieves state-of-the-art performance on four\nbenchmark text-attributed graph datasets."}
{"id": "2504.21774", "pdf": "https://arxiv.org/pdf/2504.21774", "abs": "https://arxiv.org/abs/2504.21774", "authors": ["Jiuwu Hao", "Liguo Sun", "Yuting Wan", "Yueyang Wu", "Ti Xiang", "Haolin Song", "Pin Lv"], "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?", "categories": ["cs.AI"], "comment": null, "summary": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF."}
{"id": "2504.21302", "pdf": "https://arxiv.org/pdf/2504.21302", "abs": "https://arxiv.org/abs/2504.21302", "authors": ["Zhelun Shen", "Zhuo Li", "Chenming Wu", "Zhibo Rao", "Lina Liu", "Yuchao Dai", "Liangjun Zhang"], "title": "CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 5 figures, accepted for publication in Pattern Recognition", "summary": "Recently, learning-based stereo matching methods have achieved great\nimprovement in public benchmarks, where soft argmin and smooth L1 loss play a\ncore contribution to their success. However, in unsupervised domain adaptation\nscenarios, we observe that these two operations often yield multimodal\ndisparity probability distributions in target domains, resulting in degraded\ngeneralization. In this paper, we propose a novel approach, Constrain\nMulti-modal Distribution (CMD), to address this issue. Specifically, we\nintroduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic\nsoft argmin} to encourage the network to produce predominantly unimodal\ndisparity distributions in the target domain, thereby improving prediction\naccuracy. Experimentally, we apply the proposed method to multiple\nrepresentative stereo-matching networks and conduct domain adaptation from\nsynthetic data to unlabeled real-world scenes. Results consistently demonstrate\nimproved generalization in both top-performing and domain-adaptable\nstereo-matching models. The code for CMD will be available at:\n\\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}."}
{"id": "2504.21202", "pdf": "https://arxiv.org/pdf/2504.21202", "abs": "https://arxiv.org/abs/2504.21202", "authors": ["Ramon Pires", "Roseval Malaquias Junior", "Rodrigo Nogueira"], "title": "Automatic Legal Writing Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available."}
{"id": "2411.06685", "pdf": "https://arxiv.org/pdf/2411.06685", "abs": "https://arxiv.org/abs/2411.06685", "authors": ["Li Yu", "Zhihui Li", "Jimin Xiao", "Moncef Gabbouj"], "title": "High-Frequency Enhanced Hybrid Neural Representation for Video Compression", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Neural Representations for Videos (NeRV) have simplified the video codec\nprocess and achieved swift decoding speeds by encoding video content into a\nneural network, presenting a promising solution for video compression. However,\nexisting work overlooks the crucial issue that videos reconstructed by these\nmethods lack high-frequency details. To address this problem, this paper\nintroduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our\nmethod focuses on leveraging high-frequency information to improve the\nsynthesis of fine details by the network. Specifically, we design a wavelet\nhigh-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD)\nblocks to generate high-frequency feature embeddings. Next, we design the\nHigh-Frequency Feature Modulation (HFM) block, which leverages the extracted\nhigh-frequency embeddings to enhance the fitting process of the decoder.\nFinally, with the refined Harmonic decoder block and a Dynamic Weighted\nFrequency Loss, we further reduce the potential loss of high-frequency\ninformation. Experiments on the Bunny and UVG datasets demonstrate that our\nmethod outperforms other methods, showing notable improvements in detail\npreservation and compression performance."}
{"id": "2504.21187", "pdf": "https://arxiv.org/pdf/2504.21187", "abs": "https://arxiv.org/abs/2504.21187", "authors": ["Neha Prakriya", "Zijian Ding", "Yizhou Sun", "Jason Cong"], "title": "LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "FPGAs are increasingly adopted in datacenter environments for their\nreconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have\neased FPGA programming by raising the abstraction level from RTL to untimed\nC/C++, yet attaining high performance still demands expert knowledge and\niterative manual insertion of optimization pragmas to modify the\nmicroarchitecture. To address this challenge, we propose LIFT, a large language\nmodel (LLM)-based coding assistant for HLS that automatically generates\nperformance-critical pragmas given a C/C++ design. We fine-tune the LLM by\ntightly integrating and supervising the training process with a graph neural\nnetwork (GNN), combining the sequential modeling capabilities of LLMs with the\nstructural and semantic understanding of GNNs necessary for reasoning over code\nand its control/data dependencies. On average, LIFT produces designs that\nimprove performance by 3.52x and 2.16x than prior state-of the art AutoDSE and\nHARP respectively, and 66x than GPT-4o."}
{"id": "2504.21008", "pdf": "https://arxiv.org/pdf/2504.21008", "abs": "https://arxiv.org/abs/2504.21008", "authors": ["Qiuyan Xiang", "Shuang Wu", "Dongze Wu", "Yuxin Liu", "Zhenkai Qin"], "title": "Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the widespread adoption of the Internet of Things (IoT) and Industrial\nIoT (IIoT) technologies, network architectures have become increasingly\ncomplex, and the volume of traffic has grown substantially. This evolution\nposes significant challenges to traditional security mechanisms, particularly\nin detecting high-frequency, diverse, and highly covert network attacks. To\naddress these challenges, this study proposes a novel network traffic anomaly\ndetection model that integrates a Convolutional Neural Network (CNN) with a\nBidirectional Long Short-Term Memory (BiLSTM) network, implemented on the\nMindSpore framework. Comprehensive experiments were conducted using the\nNF-BoT-IoT dataset. The results demonstrate that the proposed model achieves\n99% across accuracy, precision, recall, and F1-score, indicating its strong\nperformance and robustness in network intrusion detection tasks."}
{"id": "2504.21307", "pdf": "https://arxiv.org/pdf/2504.21307", "abs": "https://arxiv.org/abs/2504.21307", "authors": ["Siyi Chen", "Yimeng Zhang", "Sijia Liu", "Qing Qu"], "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies."}
{"id": "2504.21233", "pdf": "https://arxiv.org/pdf/2504.21233", "abs": "https://arxiv.org/abs/2504.21233", "authors": ["Haoran Xu", "Baolin Peng", "Hany Awadalla", "Dongdong Chen", "Yen-Chun Chen", "Mei Gao", "Young Jin Kim", "Yunsheng Li", "Liliang Ren", "Yelong Shen", "Shuohang Wang", "Weijian Xu", "Jianfeng Gao", "Weizhu Chen"], "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models."}
{"id": "2411.15388", "pdf": "https://arxiv.org/pdf/2411.15388", "abs": "https://arxiv.org/abs/2411.15388", "authors": ["Chiara Mauri", "Ryan Fritz", "Jocelyn Mora", "Benjamin Billot", "Juan Eugenio Iglesias", "Koen Van Leemput", "Jean Augustinack", "Douglas N Greve"], "title": "A Contrast-Agnostic Method for Ultra-High Resolution Claustrum Segmentation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "20 pages, 15 figures, 3 tables", "summary": "The claustrum is a band-like gray matter structure located between putamen\nand insula whose exact functions are still actively researched. Its sheet-like\nstructure makes it barely visible in in vivo Magnetic Resonance Imaging (MRI)\nscans at typical resolutions and neuroimaging tools for its study, including\nmethods for automatic segmentation, are currently very limited. In this paper,\nwe propose a contrast- and resolution-agnostic method for claustrum\nsegmentation at ultra-high resolution (0.35 mm isotropic); the method is based\non the SynthSeg segmentation framework (Billot et al., 2023), which leverages\nthe use of synthetic training intensity images to achieve excellent\ngeneralization. In particular, SynthSeg requires only label maps to be trained,\nsince corresponding intensity images are synthesized on the fly with random\ncontrast and resolution. We trained a deep learning network for automatic\nclaustrum segmentation, using claustrum manual labels obtained from 18\nultra-high resolution MRI scans (mostly ex vivo). We demonstrated the method to\nwork on these 18 high resolution cases (Dice score = 0.632, mean surface\ndistance = 0.458 mm, and volumetric similarity = 0.867 using 6-fold Cross\nValidation (CV)), and also on in vivo T1-weighted MRI scans at typical\nresolutions (~1 mm isotropic). We also demonstrated that the method is robust\nin a test-retest setting and when applied to multimodal imaging (T2-weighted,\nProton Density and quantitative T1 scans). To the best of our knowledge this is\nthe first accurate method for automatic ultra-high resolution claustrum\nsegmentation, which is robust against changes in contrast and resolution. The\nmethod is released at https://github.com/chiara-mauri/claustrum_segmentation\nand as part of the neuroimaging package Freesurfer (Fischl, 2012)."}
{"id": "2504.21189", "pdf": "https://arxiv.org/pdf/2504.21189", "abs": "https://arxiv.org/abs/2504.21189", "authors": ["Gulsah Hancerliogullari Koksalmis", "Bulent Soykan", "Laura J. Brattain", "Hsin-Hsiung Huang"], "title": "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "25 pages, 11 figures", "summary": "Alzheimer's Disease (AD) is marked by significant inter-individual\nvariability in its progression, complicating accurate prognosis and\npersonalized care planning. This heterogeneity underscores the critical need\nfor predictive models capable of forecasting patient-specific disease\ntrajectories. Artificial Intelligence (AI) offers powerful tools to address\nthis challenge by analyzing complex, multi-modal, and longitudinal patient\ndata. This paper provides a comprehensive survey of AI methodologies applied to\npersonalized AD progression prediction. We review key approaches including\nstate-space models for capturing temporal dynamics, deep learning techniques\nlike Recurrent Neural Networks for sequence modeling, Graph Neural Networks\n(GNNs) for leveraging network structures, and the emerging concept of AI-driven\ndigital twins for individualized simulation. Recognizing that data limitations\noften impede progress, we examine common challenges such as high\ndimensionality, missing data, and dataset imbalance. We further discuss\nAI-driven mitigation strategies, with a specific focus on synthetic data\ngeneration using Variational Autoencoders (VAEs) and Generative Adversarial\nNetworks (GANs) to augment and balance datasets. The survey synthesizes the\nstrengths and limitations of current approaches, emphasizing the trend towards\nmultimodal integration and the persistent need for model interpretability and\ngeneralizability. Finally, we identify critical open challenges, including\nrobust external validation, clinical integration, and ethical considerations,\nand outline promising future research directions such as hybrid models, causal\ninference, and federated learning. This review aims to consolidate current\nknowledge and guide future efforts in developing clinically relevant AI tools\nfor personalized AD prognostication."}
{"id": "2504.21308", "pdf": "https://arxiv.org/pdf/2504.21308", "abs": "https://arxiv.org/abs/2504.21308", "authors": ["Yunhao Li", "Sijing Wu", "Wei Sun", "Zhichao Zhang", "Yucheng Zhu", "Zicheng Zhang", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of text-to-image (T2I) generation approaches has\nattracted extensive interest in evaluating the quality of generated images,\nleading to the development of various quality assessment methods for\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\nmethods are limited to providing global quality scores, failing to deliver\nfine-grained perceptual evaluations for structurally complex subjects like\nhumans, which is a critical challenge considering the frequent anatomical and\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\nconduct a systematic subjective study to collect multidimensional annotations,\nincluding perceptual quality scores, text-image correspondence scores, visible\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\nweaknesses of current T2I methods in generating human images from multiple\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\nintegrates the large multimodal model (LMM) with domain-specific human features\nfor precise quality prediction and identification of visible and distorted body\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\nshowcases state-of-the-art performance, significantly outperforming existing\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\nin detecting structural distortions in AGHIs."}
{"id": "2504.21239", "pdf": "https://arxiv.org/pdf/2504.21239", "abs": "https://arxiv.org/abs/2504.21239", "authors": ["Xu Pan", "Ely Hahami", "Zechen Zhang", "Haim Sompolinsky"], "title": "Memorization and Knowledge Injection in Gated LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain."}
{"id": "2504.21190", "pdf": "https://arxiv.org/pdf/2504.21190", "abs": "https://arxiv.org/abs/2504.21190", "authors": ["Pradip Kunwar", "Minh N. Vu", "Maanak Gupta", "Mahmoud Abdelsalam", "Manish Bhattarai"], "title": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments."}
{"id": "2504.21028", "pdf": "https://arxiv.org/pdf/2504.21028", "abs": "https://arxiv.org/abs/2504.21028", "authors": ["Ivan Montoya Sanchez", "Shaswata Mitra", "Aritran Piplai", "Sudip Mittal"], "title": "Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "8 pages, 5 figures, 5 tables", "summary": "The rapid evolution of malware variants requires robust classification\nmethods to enhance cybersecurity. While Large Language Models (LLMs) offer\npotential for generating malware descriptions to aid family classification,\ntheir utility is limited by semantic embedding overlaps and misalignment with\nbinary behavioral features. We propose a contrastive fine-tuning (CFT) method\nthat refines LLM embeddings via targeted selection of hard negative samples\nbased on cosine similarity, enabling LLMs to distinguish between closely\nrelated malware families. Our approach combines high-similarity negatives to\nenhance discriminative power and mid-tier negatives to increase embedding\ndiversity, optimizing both precision and generalization. Evaluated on the\nCIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into\na multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework\non a few-shot setting. Experiments demonstrate significant improvements: our\nmethod achieves 63.15% classification accuracy with as few as 20 samples on\nCIC-AndMal-2020, outperforming baselines by 11--21 percentage points and\nsurpassing prior negative sampling strategies. Ablation studies confirm the\nsuperiority of similarity-based selection over random sampling, with gains of\n10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions\nthat generalize to unseen variants, bridging textual and binary feature gaps.\nThis work advances malware classification by enabling nuanced semantic\ndistinctions and provides a scalable framework for adapting LLMs to\ncybersecurity challenges."}
{"id": "2504.21309", "pdf": "https://arxiv.org/pdf/2504.21309", "abs": "https://arxiv.org/abs/2504.21309", "authors": ["Modesto Castrillón-Santana", "Oliverio J Santana", "David Freire-Obregón", "Daniel Hernández-Sosa", "Javier Lorenzo-Navarro"], "title": "An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Facial expression recognition (FER) is a key research area in computer vision\nand human-computer interaction. Despite recent advances in deep learning,\nchallenges persist, especially in generalizing to new scenarios. In fact,\nzero-shot FER significantly reduces the performance of state-of-the-art FER\nmodels. To address this problem, the community has recently started to explore\nthe integration of knowledge from Large Language Models for visual tasks. In\nthis work, we evaluate a broad collection of locally executed Visual Language\nModels (VLMs), avoiding the lack of task-specific knowledge by adopting a\nVisual Question Answering strategy. We compare the proposed pipeline with\nstate-of-the-art FER models, both integrating and excluding VLMs, evaluating\nwell-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show\nexcellent performance for some VLMs in zero-shot FER scenarios, indicating the\nneed for further exploration to improve FER generalization."}
{"id": "2504.21252", "pdf": "https://arxiv.org/pdf/2504.21252", "abs": "https://arxiv.org/abs/2504.21252", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Hao Wang", "Xiwen Chen", "Peijie Qiu", "Rui Yin", "Yi Su", "Yalin Wang"], "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA", "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG."}
{"id": "2504.21198", "pdf": "https://arxiv.org/pdf/2504.21198", "abs": "https://arxiv.org/abs/2504.21198", "authors": ["Haoyan Xu", "Zhengtao Yao", "Ziyi Wang", "Zhan Cheng", "Xiyang Hu", "Mengyuan Li", "Yue Zhao"], "title": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model\nrobustness in open-world and safety-sensitive applications. Existing approaches\nto graph OOD detection typically involve training an in-distribution (ID)\nclassifier using only ID data, followed by the application of post-hoc OOD\nscoring techniques. Although OOD exposure - introducing auxiliary OOD samples\nduring training - has proven to be an effective strategy for enhancing\ndetection performance, current methods in the graph domain generally assume\naccess to a set of real OOD nodes. This assumption, however, is often\nimpractical due to the difficulty and cost of acquiring representative OOD\nsamples. In this paper, we introduce GOE-LLM, a novel framework that leverages\nLarge Language Models (LLMs) for OOD exposure in graph OOD detection without\nrequiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying\npseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM\nannotations, and (2) generating semantically informative synthetic OOD nodes\nvia LLM-prompted text generation. These pseudo-OOD nodes are then used to\nregularize the training of the ID classifier for improved OOD awareness. We\nevaluate our approach across multiple benchmark datasets, showing that GOE-LLM\nsignificantly outperforms state-of-the-art graph OOD detection methods that do\nnot use OOD exposure and achieves comparable performance to those relying on\nreal OOD data."}
{"id": "2504.21029", "pdf": "https://arxiv.org/pdf/2504.21029", "abs": "https://arxiv.org/abs/2504.21029", "authors": ["Ben Goertzel", "Paulos Yibelo"], "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We propose a robust transformer architecture designed to prevent prompt\ninjection attacks and ensure secure, reliable response generation. Our PICO\n(Prompt Isolation and Cybersecurity Oversight) framework structurally separates\ntrusted system instructions from untrusted user inputs through dual channels\nthat are processed independently and merged only by a controlled, gated fusion\nmechanism. In addition, we integrate a specialized Security Expert Agent within\na Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge\nGraph (CKG) to supply domain-specific reasoning. Our training design further\nensures that the system prompt branch remains immutable while the rest of the\nnetwork learns to handle adversarial inputs safely. This PICO framework is\npresented via a general mathematical formulation, then elaborated in terms of\nthe specifics of transformer architecture, and fleshed out via hypothetical\ncase studies including Policy Puppetry attacks. While the most effective\nimplementation may involve training transformers in a PICO-based way from\nscratch, we also present a cost-effective fine-tuning approach."}
{"id": "2504.21325", "pdf": "https://arxiv.org/pdf/2504.21325", "abs": "https://arxiv.org/abs/2504.21325", "authors": ["Abdul Sami", "Avinash Kumar", "Irfanullah Memon", "Youngwon Jo", "Muhammad Rizwan", "Jaeyoung Choi"], "title": "Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, Accepted at ICOIN 2025", "summary": "Automatic font generation (AFG) is the process of creating a new font using\nonly a few examples of the style images. Generating fonts for complex languages\nlike Korean and Chinese, particularly in handwritten styles, presents\nsignificant challenges. Traditional AFGs, like Generative adversarial networks\n(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during\ntraining and often face mode collapse problems. They also struggle to capture\nfine details within font images. To address these problems, we present a\ndiffusion-based AFG method which generates high-quality, diverse Korean font\nimages using only a single reference image, focusing on handwritten and printed\nstyles. Our approach refines noisy images incrementally, ensuring stable\ntraining and visually appealing results. A key innovation is our text encoder,\nwhich processes phonetic representations to generate accurate and contextually\ncorrect characters, even for unseen characters. We used a pre-trained style\nencoder from DG FONT to effectively and accurately encode the style images. To\nfurther enhance the generation quality, we used perceptual loss that guides the\nmodel to focus on the global style of generated images. Experimental results on\nover 2000 Korean characters demonstrate that our model consistently generates\naccurate and detailed font images and outperforms benchmark methods, making it\na reliable tool for generating authentic Korean fonts across different styles."}
{"id": "2504.21299", "pdf": "https://arxiv.org/pdf/2504.21299", "abs": "https://arxiv.org/abs/2504.21299", "authors": ["Zhiting Fan", "Ruizhe Chen", "Zuozhu Liu"], "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline."}
{"id": "2504.21206", "pdf": "https://arxiv.org/pdf/2504.21206", "abs": "https://arxiv.org/abs/2504.21206", "authors": ["Zihan Chen", "Xingbo Fu", "Yushun Dong", "Jundong Li", "Cong Shen"], "title": "FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated Graph Learning (FGL) empowers clients to collaboratively train\nGraph neural networks (GNNs) in a distributed manner while preserving data\nprivacy. However, FGL methods usually require that the graph data owned by all\nclients is homophilic to ensure similar neighbor distribution patterns of\nnodes. Such an assumption ensures that the learned knowledge is consistent\nacross the local models from all clients. Therefore, these local models can be\nproperly aggregated as a global model without undermining the overall\nperformance. Nevertheless, when the neighbor distribution patterns of nodes\nvary across different clients (e.g., when clients hold graphs with different\nlevels of heterophily), their local models may gain different and even conflict\nknowledge from their node-level predictive tasks. Consequently, aggregating\nthese local models usually leads to catastrophic performance deterioration on\nthe global model. To address this challenge, we propose FedHERO, an FGL\nframework designed to harness and share insights from heterophilic graphs\neffectively. At the heart of FedHERO is a dual-channel GNN equipped with a\nstructure learner, engineered to discern the structural knowledge encoded in\nthe local graphs. With this specialized component, FedHERO enables the local\nmodel for each client to identify and learn patterns that are universally\napplicable across graphs with different patterns of node neighbor\ndistributions. FedHERO not only enhances the performance of individual client\nmodels by leveraging both local and shared structural insights but also sets a\nnew precedent in this field to effectively handle graph data with various node\nneighbor distribution patterns. We conduct extensive experiments to validate\nthe superior performance of FedHERO against existing alternatives."}
{"id": "2504.21032", "pdf": "https://arxiv.org/pdf/2504.21032", "abs": "https://arxiv.org/abs/2504.21032", "authors": ["Lior Limonad", "Fabiana Fournier", "Hadar Mulian", "George Manias", "Spiros Borotis", "Danai Kyrkou"], "title": "Selecting the Right LLM for eGov Explanations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "8 pages, 7 figures. ICEDEG 2025, Bern, Switzerland, June 2025", "summary": "The perceived quality of the explanations accompanying e-government services\nis key to gaining trust in these institutions, consequently amplifying further\nusage of these services. Recent advances in generative AI, and concretely in\nLarge Language Models (LLMs) allow the automation of such content\narticulations, eliciting explanations' interpretability and fidelity, and more\ngenerally, adapting content to various audiences. However, selecting the right\nLLM type for this has become a non-trivial task for e-government service\nproviders. In this work, we adapted a previously developed scale to assist with\nthis selection, providing a systematic approach for the comparative analysis of\nthe perceived quality of explanations generated by various LLMs. We further\ndemonstrated its applicability through the tax-return process, using it as an\nexemplar use case that could benefit from employing an LLM to generate\nexplanations about tax refund decisions. This was attained through a user study\nwith 128 survey respondents who were asked to rate different versions of\nLLM-generated explanations about tax refund decisions, providing a\nmethodological basis for selecting the most appropriate LLM. Recognizing the\npractical challenges of conducting such a survey, we also began exploring the\nautomation of this process by attempting to replicate human feedback using a\nselection of cutting-edge predictive techniques."}
{"id": "2504.21334", "pdf": "https://arxiv.org/pdf/2504.21334", "abs": "https://arxiv.org/abs/2504.21334", "authors": ["Misora Sugiyama", "Hirokatsu Kataoka"], "title": "Simple Visual Artifact Detection in Sora-Generated Videos", "categories": ["cs.CV"], "comment": null, "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety."}
{"id": "2504.21303", "pdf": "https://arxiv.org/pdf/2504.21303", "abs": "https://arxiv.org/abs/2504.21303", "authors": ["Xiao Xiao", "Yu Su", "Sijing Zhang", "Zhang Chen", "Yadong Chen", "Tian Liu"], "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios."}
{"id": "2504.21211", "pdf": "https://arxiv.org/pdf/2504.21211", "abs": "https://arxiv.org/abs/2504.21211", "authors": ["Juliana Barbosa", "Ulhas Gondhali", "Gohar Petrossian", "Kinshuk Sharma", "Sunandan Chakraborty", "Jennifer Jacquet", "Juliana Freire"], "title": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking."}
{"id": "2504.21033", "pdf": "https://arxiv.org/pdf/2504.21033", "abs": "https://arxiv.org/abs/2504.21033", "authors": ["Majid Behravan", "Maryam Haghani", "Denis Gracanin"], "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills."}
{"id": "2504.21336", "pdf": "https://arxiv.org/pdf/2504.21336", "abs": "https://arxiv.org/abs/2504.21336", "authors": ["Linshan Wu", "Yuxiang Nie", "Sunan He", "Jiaxin Zhuang", "Hao Chen"], "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "categories": ["cs.CV"], "comment": "The first universal foundation model for grounded biomedical image\n  interpretation", "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis."}
{"id": "2504.21330", "pdf": "https://arxiv.org/pdf/2504.21330", "abs": "https://arxiv.org/abs/2504.21330", "authors": ["Kaixun Yang", "Mladen Raković", "Dragan Gašević", "Guanliang Chen"], "title": "Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native."}
{"id": "2504.21254", "pdf": "https://arxiv.org/pdf/2504.21254", "abs": "https://arxiv.org/abs/2504.21254", "authors": ["Sixuan Wang", "Jiao Yin", "Jinli Cao", "MingJian Tang", "Hua Wang", "Yanchun Zhang"], "title": "ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Effective and efficient graph representation learning is essential for\nenabling critical downstream tasks, such as node classification, link\nprediction, and subgraph search. However, existing graph neural network (GNN)\narchitectures often struggle to adapt to diverse and complex graph structures,\nlimiting their ability to provide robust and generalizable representations. To\naddress this challenge, we propose ABG-NAS, a novel framework for automated\ngraph neural network architecture search tailored for efficient graph\nrepresentation learning. ABG-NAS encompasses three key components: a\nComprehensive Architecture Search Space (CASS), an Adaptive Genetic\nOptimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS\nsystematically explores diverse propagation (P) and transformation (T)\noperations, enabling the discovery of GNN architectures capable of capturing\nintricate graph characteristics. AGOS dynamically balances exploration and\nexploitation, ensuring search efficiency and preserving solution diversity.\nBGTM further optimizes hyperparameters periodically, enhancing the scalability\nand robustness of the resulting architectures. Empirical evaluations on\nbenchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that\nABG-NAS consistently outperforms both manually designed GNNs and\nstate-of-the-art neural architecture search (NAS) methods. These results\nhighlight the potential of ABG-NAS to advance graph representation learning by\nproviding scalable and adaptive solutions for diverse graph structures. Our\ncode is publicly available at https://github.com/sserranw/ABG-NAS."}
{"id": "2504.21034", "pdf": "https://arxiv.org/pdf/2504.21034", "abs": "https://arxiv.org/abs/2504.21034", "authors": ["Georgios Syros", "Anshuman Suri", "Cristina Nita-Rotaru", "Alina Oprea"], "title": "SAGA: A Security Architecture for Governing AI Agentic Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments."}
{"id": "2504.21340", "pdf": "https://arxiv.org/pdf/2504.21340", "abs": "https://arxiv.org/abs/2504.21340", "authors": ["Khoa Tuan Nguyen", "Ho-min Park", "Gaeun Oh", "Joris Vankerschaver", "Wesley De Neve"], "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ISBI 2025 \"Challenge 2: Pap Smear Cell Classification\n  Challenge\"", "summary": "We propose a novel approach to cervical cell image classification for\ncervical cancer screening using the EVA-02 transformer model. We developed a\nfour-step pipeline: fine-tuning EVA-02, feature extraction, selecting important\nfeatures through multiple machine learning models, and training a new\nartificial neural network with optional loss weighting for improved\ngeneralization. With this design, our best model achieved an F1-score of\n0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized\nKernel SHAP analysis and identified key features correlating with cell\nmorphology and staining characteristics, providing interpretable insights into\nthe decision-making process of the fine-tuned model. Our code is available at\nhttps://github.com/Khoa-NT/isbi2025_ps3c."}
{"id": "2504.21372", "pdf": "https://arxiv.org/pdf/2504.21372", "abs": "https://arxiv.org/abs/2504.21372", "authors": ["Máté Gedeon"], "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features."}
{"id": "2504.21261", "pdf": "https://arxiv.org/pdf/2504.21261", "abs": "https://arxiv.org/abs/2504.21261", "authors": ["Kasra Jalaldoust", "Saber Salehkaleybar", "Negar Kiyavash"], "title": "Multi-Domain Causal Discovery in Bijective Causal Models", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": "Proceedings of Causal Learning and Reasoning (CLeaR) 2025", "summary": "We consider the problem of causal discovery (a.k.a., causal structure\nlearning) in a multi-domain setting. We assume that the causal functions are\ninvariant across the domains, while the distribution of the exogenous noise may\nvary. Under causal sufficiency (i.e., no confounders exist), we show that the\ncausal diagram can be discovered under less restrictive functional assumptions\ncompared to previous work. What enables causal discovery in this setting is\nbijective generation mechanisms (BGM), which ensures that the functional\nrelation between the exogenous noise $E$ and the endogenous variable $Y$ is\nbijective and differentiable in both directions at every level of the cause\nvariable $X = x$. BGM generalizes a variety of models including additive noise\nmodel, LiNGAM, post-nonlinear model, and location-scale noise model. Further,\nwe derive a statistical test to find the parents set of the target variable.\nExperiments on various synthetic and real-world datasets validate our\ntheoretical findings."}
{"id": "2504.21036", "pdf": "https://arxiv.org/pdf/2504.21036", "abs": "https://arxiv.org/abs/2504.21036", "authors": ["Hao Du", "Shang Liu", "Yang Cao"], "title": "Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies."}
{"id": "2504.21344", "pdf": "https://arxiv.org/pdf/2504.21344", "abs": "https://arxiv.org/abs/2504.21344", "authors": ["Luoting Zhuang", "Seyed Mohammad Hossein Tabatabaei", "Ramin Salehi-Rad", "Linh M. Tran", "Denise R. Aberle", "Ashley E. Prosper", "William Hsu"], "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection", "categories": ["cs.CV", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings."}
{"id": "2504.21421", "pdf": "https://arxiv.org/pdf/2504.21421", "abs": "https://arxiv.org/abs/2504.21421", "authors": ["Linxuan Wang", "Shuiyuan Yu"], "title": "The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors", "categories": ["cs.CL"], "comment": "This paper has been accepted by the 13th International Quantitative\n  Linguistics Conference QUALICO 2025", "summary": "To explore the relationship between dependency distance (DD) and hierarchical\ndistance (HD) in Japanese, we compared the probability distributions of DD and\nHD with and without sentence length fixed, and analyzed the changes in mean\ndependency distance (MDD) and mean hierarchical distance (MHD) as sentence\nlength increases, along with their correlation coefficient based on the\nBalanced Corpus of Contemporary Written Japanese. It was found that the valency\nof the predicates is the underlying factor behind the trade-off relation\nbetween MDD and MHD in Japanese. Native speakers of Japanese regulate the\nlinear complexity and hierarchical complexity through the valency of the\npredicates, and the relative sizes of MDD and MHD depend on whether the\nthreshold of valency has been reached. Apart from the cognitive load, the\nvalency of the predicates also affects the probability distributions of DD and\nHD. The effect of the valency of the predicates on the distribution of HD is\ngreater than on that of DD, which leads to differences in their probability\ndistributions and causes the mean of MDD to be lower than that of MHD."}
{"id": "2504.21289", "pdf": "https://arxiv.org/pdf/2504.21289", "abs": "https://arxiv.org/abs/2504.21289", "authors": ["Yan Huang", "Da-Qing Zhang"], "title": "Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biclustering is an effective technique in data mining and pattern\nrecognition. Biclustering algorithms based on traditional clustering face two\nfundamental limitations when processing high-dimensional data: (1) The distance\nconcentration phenomenon in high-dimensional spaces leads to data sparsity,\nrendering similarity measures ineffective; (2) Mainstream linear dimensionality\nreduction methods disrupt critical local structural patterns. To apply\nbiclustering to high-dimensional datasets, we propose an orthogonal\nfactor-based biclustering algorithm (BCBOF). First, we constructed orthogonal\nfactors in the vector space of the high-dimensional dataset. Then, we performed\nclustering using the coordinates of the original data in the orthogonal\nsubspace as clustering targets. Finally, we obtained biclustering results of\nthe original dataset. Since dimensionality reduction was applied before\nclustering, the proposed algorithm effectively mitigated the data sparsity\nproblem caused by high dimensionality. Additionally, we applied this\nbiclustering algorithm to stock technical indicator combinations and stock\nprice trend prediction. Biclustering results were transformed into fuzzy rules,\nand we incorporated profit-preserving and stop-loss rules into the rule set,\nultimately forming a fuzzy inference system for stock price trend predictions\nand trading signals. To evaluate the performance of BCBOF, we compared it with\nexisting biclustering methods using multiple evaluation metrics. The results\nshowed that our algorithm outperformed other biclustering techniques. To\nvalidate the effectiveness of the fuzzy inference system, we conducted virtual\ntrading experiments using historical data from 10 A-share stocks. The\nexperimental results showed that the generated trading strategies yielded\nhigher returns for investors."}
{"id": "2504.21037", "pdf": "https://arxiv.org/pdf/2504.21037", "abs": "https://arxiv.org/abs/2504.21037", "authors": ["Farnaz Soltaniani", "Mohammad Ghafari", "Mohammed Sayagh"], "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF."}
{"id": "2504.21356", "pdf": "https://arxiv.org/pdf/2504.21356", "abs": "https://arxiv.org/abs/2504.21356", "authors": ["Hong Zhang", "Zhongjie Duan", "Xingjun Wang", "Yingda Chen", "Yuze Zhao", "Yu Zhang"], "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."}
{"id": "2504.21463", "pdf": "https://arxiv.org/pdf/2504.21463", "abs": "https://arxiv.org/abs/2504.21463", "authors": ["Haowen Hou", "Zhiyi Huang", "Kaifeng Tan", "Rongchang Lu", "Fei Richard Yu"], "title": "RWKV-X: A Linear Complexity Hybrid Language Model", "categories": ["cs.CL"], "comment": "12 pages", "summary": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X."}
{"id": "2504.21296", "pdf": "https://arxiv.org/pdf/2504.21296", "abs": "https://arxiv.org/abs/2504.21296", "authors": ["Renqiang Luo", "Ziqi Xu", "Xikun Zhang", "Qing Qing", "Huafei Huang", "Enyan Dai", "Zhe Wang", "Bo Yang"], "title": "Fairness in Graph Learning Augmented with Machine Learning: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Augmenting specialised machine learning techniques into traditional graph\nlearning models has achieved notable success across various domains, including\nfederated graph learning, dynamic graph learning, and graph transformers.\nHowever, the intricate mechanisms of these specialised techniques introduce\nsignificant challenges in maintaining model fairness, potentially resulting in\ndiscriminatory outcomes in high-stakes applications such as recommendation\nsystems, disaster response, criminal justice, and loan approval. This paper\nsystematically examines the unique fairness challenges posed by Graph Learning\naugmented with Machine Learning (GL-ML). It highlights the complex interplay\nbetween graph learning mechanisms and machine learning techniques, emphasising\nhow the augmentation of machine learning both enhances and complicates\nfairness. Additionally, we explore four critical techniques frequently employed\nto improve fairness in GL-ML methods. By thoroughly investigating the root\ncauses and broader implications of fairness challenges in this rapidly evolving\nfield, this work establishes a robust foundation for future research and\ninnovation in GL-ML fairness."}
{"id": "2504.21038", "pdf": "https://arxiv.org/pdf/2504.21038", "abs": "https://arxiv.org/abs/2504.21038", "authors": ["Yakai Li", "Jiekang Hu", "Weiduan Sang", "Luping Ma", "Jing Xie", "Weijuan Zhang", "Aimin Yu", "Shijie Zhao", "Qingjia Huang", "Qihang Zhou"], "title": "Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are designed to generate helpful and safe\ncontent. However, adversarial attacks, commonly referred to as jailbreak, can\nbypass their safety protocols, prompting LLMs to generate harmful content or\nreveal sensitive data. Consequently, investigating jailbreak methodologies is\ncrucial for exposing systemic vulnerabilities within LLMs, ultimately guiding\nthe continuous implementation of security enhancements by developers. In this\npaper, we introduce a novel jailbreak attack method that leverages the\nprefilling feature of LLMs, a feature designed to enhance model output\nconstraints. Unlike traditional jailbreak methods, the proposed attack\ncircumvents LLMs' safety mechanisms by directly manipulating the probability\ndistribution of subsequent tokens, thereby exerting control over the model's\noutput. We propose two attack variants: Static Prefilling (SP), which employs a\nuniversal prefill text, and Optimized Prefilling (OP), which iteratively\noptimizes the prefill text to maximize the attack success rate. Experiments on\nsix state-of-the-art LLMs using the AdvBench benchmark validate the\neffectiveness of our method and demonstrate its capability to substantially\nenhance attack success rates when combined with existing jailbreak approaches.\nThe OP method achieved attack success rates of up to 99.82% on certain models,\nsignificantly outperforming baseline methods. This work introduces a new\njailbreak attack method in LLMs, emphasizing the need for robust content\nvalidation mechanisms to mitigate the adversarial exploitation of prefilling\nfeatures. All code and data used in this paper are publicly available."}
{"id": "2504.21368", "pdf": "https://arxiv.org/pdf/2504.21368", "abs": "https://arxiv.org/abs/2504.21368", "authors": ["Pramook Khungurn", "Sukit Seripanitkarn", "Phonphrm Thawatdamrongkit", "Supasorn Suwajanakorn"], "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality", "categories": ["cs.CV", "cs.AI"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes."}
{"id": "2504.21474", "pdf": "https://arxiv.org/pdf/2504.21474", "abs": "https://arxiv.org/abs/2504.21474", "authors": ["Hadi Bayrami Asl Tekanlou", "Jafar Razmara", "Mahsa Sanaei", "Mostafa Rahgouy", "Hamed Babaei Giglou"], "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025", "summary": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries."}
{"id": "2504.21304", "pdf": "https://arxiv.org/pdf/2504.21304", "abs": "https://arxiv.org/abs/2504.21304", "authors": ["Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haoyue Bai", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "title": "Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 2025", "summary": "Feature transformation involves generating a new set of features from the\noriginal dataset to enhance the data's utility. In certain domains like\nmaterial performance screening, dimensionality is large and collecting labels\nis expensive and lengthy. It highly necessitates transforming feature spaces\nefficiently and without supervision to enhance data readiness and AI utility.\nHowever, existing methods fall short in efficient navigation of a vast space of\nfeature combinations, and are mostly designed for supervised settings. To fill\nthis gap, our unique perspective is to leverage a generator-critic duet-play\nteaming framework using LLM agents and in-context learning to derive\npseudo-supervision from unsupervised data. The framework consists of three\ninterconnected steps: (1) Critic agent diagnoses data to generate actionable\nadvice, (2) Generator agent produces tokenized feature transformations guided\nby the critic's advice, and (3) Iterative refinement ensures continuous\nimprovement through feedback between agents. The generator-critic framework can\nbe generalized to human-agent collaborative generation, by replacing the critic\nagent with human experts. Extensive experiments demonstrate that the proposed\nframework outperforms even supervised baselines in feature transformation\nefficiency, robustness, and practical applicability across diverse datasets."}
{"id": "2504.21039", "pdf": "https://arxiv.org/pdf/2504.21039", "abs": "https://arxiv.org/abs/2504.21039", "authors": ["Paul Kassianik", "Baturay Saglam", "Alexander Chen", "Blaine Nelson", "Anu Vellore", "Massimo Aufiero", "Fraser Burch", "Dhruv Kedia", "Avi Zohary", "Sajana Weerawardhena", "Aman Priyanshu", "Adam Swanda", "Amy Chang", "Hyrum Anderson", "Kojin Oshiba", "Omar Santos", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts."}
{"id": "2504.21385", "pdf": "https://arxiv.org/pdf/2504.21385", "abs": "https://arxiv.org/abs/2504.21385", "authors": ["Shijun Zhou", "Yajing Liu", "Chunhui Hao", "Zhiyuan Liu", "Jiandong Tian"], "title": "IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Due to the domain gap between real-world and synthetic hazy images, current\ndata-driven dehazing algorithms trained on synthetic datasets perform well on\nsynthetic data but struggle to generalize to real-world scenarios. To address\nthis challenge, we propose \\textbf{I}mage \\textbf{D}ehazing \\textbf{D}iffusion\n\\textbf{M}odels (IDDM), a novel diffusion process that incorporates the\natmospheric scattering model into noise diffusion. IDDM aims to use the gradual\nhaze formation process to help the denoising Unet robustly learn the\ndistribution of clear images from the conditional input hazy images. We design\na specialized training strategy centered around IDDM. Diffusion models are\nleveraged to bridge the domain gap from synthetic to real-world, while the\natmospheric scattering model provides physical guidance for haze formation.\nDuring the forward process, IDDM simultaneously introduces haze and noise into\nclear images, and then robustly separates them during the sampling process. By\ntraining with physics-guided information, IDDM shows the ability of domain\ngeneralization, and effectively restores the real-world hazy images despite\nbeing trained on synthetic datasets. Extensive experiments demonstrate the\neffectiveness of our method through both quantitative and qualitative\ncomparisons with state-of-the-art approaches."}
{"id": "2504.21475", "pdf": "https://arxiv.org/pdf/2504.21475", "abs": "https://arxiv.org/abs/2504.21475", "authors": ["Serry Sibaee", "Samar Ahmed", "Abdullah Al Harbi", "Omer Nacar", "Adel Ammar", "Yasser Habashi", "Wadii Boulila"], "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic."}
{"id": "2504.21314", "pdf": "https://arxiv.org/pdf/2504.21314", "abs": "https://arxiv.org/abs/2504.21314", "authors": ["Xunpeng Huang", "Yujin Han", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Capturing Conditional Dependence via Auto-regressive Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion models have demonstrated appealing performance in both image and\nvideo generation. However, many works discover that they struggle to capture\nimportant, high-level relationships that are present in the real world. For\nexample, they fail to learn physical laws from data, and even fail to\nunderstand that the objects in the world exist in a stable fashion. This is due\nto the fact that important conditional dependence structures are not adequately\ncaptured in the vanilla diffusion models. In this work, we initiate an in-depth\nstudy on strengthening the diffusion model to capture the conditional\ndependence structures in the data. In particular, we examine the efficacy of\nthe auto-regressive (AR) diffusion models for such purpose and develop the\nfirst theoretical results on the sampling error of AR diffusion models under\n(possibly) the mildest data assumption. Our theoretical findings indicate that,\ncompared with typical diffusion models, the AR variant produces samples with a\nreduced gap in approximating the data conditional distribution. On the other\nhand, the overall inference time of the AR-diffusion models is only moderately\nlarger than that for the vanilla diffusion models, making them still practical\nfor large scale applications. We also provide empirical results showing that\nwhen there is clear conditional dependence structure in the data, the AR\ndiffusion models captures such structure, whereas vanilla DDPM fails to do so.\nOn the other hand, when there is no obvious conditional dependence across\npatches of the data, AR diffusion does not outperform DDPM."}
{"id": "2504.21042", "pdf": "https://arxiv.org/pdf/2504.21042", "abs": "https://arxiv.org/abs/2504.21042", "authors": ["Jiamin Chang", "Haoyang Li", "Hammond Pearce", "Ruoxi Sun", "Bo Li", "Minhui Xue"], "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accept By The ACM Conference on Computer and Communications Security\n  (CCS) 2025", "summary": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation."}
{"id": "2504.21387", "pdf": "https://arxiv.org/pdf/2504.21387", "abs": "https://arxiv.org/abs/2504.21387", "authors": ["Teodor Boyadzhiev", "Gabriele Lagani", "Luca Ciampi", "Giuseppe Amato", "Krassimira Ivanova"], "title": "Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain", "categories": ["cs.CV"], "comment": "Accepted at the 10th International Euro-Mediterranean Conference\n  (EuroMed 2024)", "summary": "The integration of computer vision and deep learning is an essential part of\ndocumenting and preserving cultural heritage, as well as improving visitor\nexperiences. In recent years, two deep learning paradigms have been established\nin the field of computer vision: convolutional neural networks and transformer\narchitectures. The present study aims to make a comparative analysis of some\nrepresentatives of these two techniques of their ability to transfer knowledge\nfrom generic dataset, such as ImageNet, to cultural heritage specific tasks.\nThe results of testing examples of the architectures VGG, ResNet, DenseNet,\nVisual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is\nthe best in terms of efficiency-computability ratio."}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540", "abs": "https://arxiv.org/abs/2504.21540", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "title": "Improving Informally Romanized Language Identification", "categories": ["cs.CL"], "comment": "16 pages, 14 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), there is no\nconventional spelling of words in the Latin script, hence there will be high\nspelling variability in written text. Such romanization renders languages that\nare normally easily distinguished based on script highly confusable, such as\nHindi and Urdu. In this work, we increase language identification (LID)\naccuracy for romanized text by improving the methods used to synthesize\ntraining sets. We find that training on synthetic samples which incorporate\nnatural spelling variation yields higher LID system accuracy than including\navailable naturally occurring examples in the training set, or even training\nhigher capacity models. We demonstrate new state-of-the-art LID performance on\nromanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set\n(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a\npretrained neural model) to 85.4% using a linear classifier trained solely on\nsynthetic data and 88.2% when also training on available harvested text."}
{"id": "2504.21326", "pdf": "https://arxiv.org/pdf/2504.21326", "abs": "https://arxiv.org/abs/2504.21326", "authors": ["Junkyu Lee", "Tian Gao", "Elliot Nelson", "Miao Liu", "Debarun Bhattacharjya", "Songtao Lu"], "title": "Q-function Decomposition with Intervention Semantics with Factored Action Spaces", "categories": ["cs.LG", "cs.AI"], "comment": "AISTATS 2025", "summary": "Many practical reinforcement learning environments have a discrete factored\naction space that induces a large combinatorial set of actions, thereby posing\nsignificant challenges. Existing approaches leverage the regular structure of\nthe action space and resort to a linear decomposition of Q-functions, which\navoids enumerating all combinations of factored actions. In this paper, we\nconsider Q-functions defined over a lower dimensional projected subspace of the\noriginal action space, and study the condition for the unbiasedness of\ndecomposed Q-functions using causal effect estimation from the no unobserved\nconfounder setting in causal statistics. This leads to a general scheme which\nwe call action decomposed reinforcement learning that uses the projected\nQ-functions to approximate the Q-function in standard model-free reinforcement\nlearning algorithms. The proposed approach is shown to improve sample\ncomplexity in a model-based reinforcement learning setting. We demonstrate\nimprovements in sample efficiency compared to state-of-the-art baselines in\nonline continuous control environments and a real-world offline sepsis\ntreatment environment."}
{"id": "2504.21043", "pdf": "https://arxiv.org/pdf/2504.21043", "abs": "https://arxiv.org/abs/2504.21043", "authors": ["Lingxiang wang", "Hainan Zhang", "Qinnan Zhang", "Ziwei Wang", "Hongwei Zheng", "Jin Dong", "Zhiming Zheng"], "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code."}
{"id": "2504.21403", "pdf": "https://arxiv.org/pdf/2504.21403", "abs": "https://arxiv.org/abs/2504.21403", "authors": ["Yumeng Shi", "Quanyu Long", "Wenya Wang"], "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Video question answering benefits from the rich information available in\nvideos, enabling a wide range of applications. However, the large volume of\ntokens generated from longer videos presents significant challenges to memory\nefficiency and model performance. To alleviate this issue, existing works\npropose to compress video inputs, but usually overlooking the varying\nimportance of static and dynamic information across different queries, leading\nto inefficient token usage within limited budgets. To tackle this, we propose a\nnovel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust\nstatic and dynamic information needed based on question requirements. Our\nframework first explores different token allocations between static frames,\nwhich preserve spatial details, and dynamic frames, which capture temporal\nchanges. Next, it employs a query-aware attention-based metric to select the\noptimal token combination without model updates. Our proposed framework is\nplug-and-play that can be seamlessly integrated within diverse video-language\nmodels. Extensive experiments show that our method achieves significant\nperformance improvements (up to 5.8%) among various video question answering\nbenchmarks."}
{"id": "2504.21547", "pdf": "https://arxiv.org/pdf/2504.21547", "abs": "https://arxiv.org/abs/2504.21547", "authors": ["Aleksei Dorkin", "Kairit Sirts"], "title": "TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval", "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025)", "summary": "We present our submission to the Task 5 of SemEval-2025 that aims to aid\nlibrarians in assigning subject tags to the library records by producing a list\nof likely relevant tags for a given document. We frame the task as an\ninformation retrieval problem, where the document content is used to retrieve\nsubject tags from a large subject taxonomy. We leverage two types of encoder\nmodels to build a two-stage information retrieval system -- a bi-encoder for\ncoarse-grained candidate extraction at the first stage, and a cross-encoder for\nfine-grained re-ranking at the second stage. This approach proved effective,\ndemonstrating significant improvements in recall compared to single-stage\nmethods and showing competitive results according to qualitative evaluation."}
{"id": "2504.21327", "pdf": "https://arxiv.org/pdf/2504.21327", "abs": "https://arxiv.org/abs/2504.21327", "authors": ["Mohammad Vahid Jamali", "Hamid Saber", "Jung Hyun Bae"], "title": "A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees", "categories": ["cs.LG"], "comment": null, "summary": "Meta federated learning (FL) is a personalized variant of FL, where multiple\nagents collaborate on training an initial shared model without exchanging raw\ndata samples. The initial model should be trained in a way that current or new\nagents can easily adapt it to their local datasets after one or a few\nfine-tuning steps, thus improving the model personalization. Conventional meta\nFL approaches minimize the average loss of agents on the local models obtained\nafter one step of fine-tuning. In practice, agents may need to apply several\nfine-tuning steps to adapt the global model to their local data, especially\nunder highly heterogeneous data distributions across agents. To this end, we\npresent a generalized framework for the meta FL by minimizing the average loss\nof agents on their local model after any arbitrary number $\\nu$ of fine-tuning\nsteps. For this generalized framework, we present a variant of the well-known\nfederated averaging (FedAvg) algorithm and conduct a comprehensive theoretical\nconvergence analysis to characterize the convergence speed as well as behavior\nof the meta loss functions in both the exact and approximated cases. Our\nexperiments on real-world datasets demonstrate superior accuracy and faster\nconvergence for the proposed scheme compared to conventional approaches."}
{"id": "2504.21044", "pdf": "https://arxiv.org/pdf/2504.21044", "abs": "https://arxiv.org/abs/2504.21044", "authors": ["Jianbo Gao", "Keke Gai", "Jing Yu", "Liehuang Zhu", "Qi Wu"], "title": "AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advancement in large-scale Artificial Intelligence (AI) models\noffering multimodal services have become foundational in AI systems, making\nthem prime targets for model theft. Existing methods select Out-of-Distribution\n(OoD) data as backdoor watermarks and retrain the original model for copyright\nprotection. However, existing methods are susceptible to malicious detection\nand forgery by adversaries, resulting in watermark evasion. In this work, we\npropose Model-\\underline{ag}nostic Black-box Backdoor W\\underline{ate}rmarking\nFramework (AGATE) to address stealthiness and robustness challenges in\nmultimodal model copyright protection. Specifically, we propose an adversarial\ntrigger generation method to generate stealthy adversarial triggers from\nordinary dataset, providing visual fidelity while inducing semantic shifts. To\nalleviate the issue of anomaly detection among model outputs, we propose a\npost-transform module to correct the model output by narrowing the distance\nbetween adversarial trigger image embedding and text embedding. Subsequently, a\ntwo-phase watermark verification is proposed to judge whether the current model\ninfringes by comparing the two results with and without the transform module.\nConsequently, we consistently outperform state-of-the-art methods across five\ndatasets in the downstream tasks of multimodal image-text retrieval and image\nclassification. Additionally, we validated the robustness of AGATE under two\nadversarial attack scenarios."}
{"id": "2504.21414", "pdf": "https://arxiv.org/pdf/2504.21414", "abs": "https://arxiv.org/abs/2504.21414", "authors": ["Qi Fan", "Kaiqi Liu", "Nian Liu", "Hisham Cholakkal", "Rao Muhammad Anwer", "Wenbin Li", "Yang Gao"], "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining", "categories": ["cs.CV"], "comment": null, "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks."}
{"id": "2504.21553", "pdf": "https://arxiv.org/pdf/2504.21553", "abs": "https://arxiv.org/abs/2504.21553", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes."}
{"id": "2504.21328", "pdf": "https://arxiv.org/pdf/2504.21328", "abs": "https://arxiv.org/abs/2504.21328", "authors": ["Yao-Hsuan Tsai", "Hsiao-Tung Juan", "Pao-Hsiung Chiu", "Chao-An Lin"], "title": "Multi-level datasets training method in Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.CE", "physics.flu-dyn"], "comment": "33 pages, 12figures", "summary": "Physics-Informed Neural Networks have emerged as a promising methodology for\nsolving PDEs, gaining significant attention in computer science and various\nphysics-related fields. Despite being demonstrated the ability to incorporate\nthe physics of laws for versatile applications, PINNs still struggle with the\nchallenging problems which are stiff to be solved and/or have high-frequency\ncomponents in the solutions, resulting in accuracy and convergence issues. It\nmay not only increase computational costs, but also lead to accuracy loss or\nsolution divergence. In this study, an alternative approach is proposed to\nmitigate the above-mentioned problems. Inspired by the multi-grid method in CFD\ncommunity, the underlying idea of the current approach is to efficiently remove\ndifferent frequency errors via training with different levels of training\nsamples, resulting in a simpler way to improve the training accuracy without\nspending time in fine-tuning of neural network structures, loss weights as well\nas hyperparameters. To demonstrate the efficacy of current approach, we first\ninvestigate canonical 1D ODE with high-frequency component and 2D\nconvection-diffusion equation with V-cycle training strategy. Finally, the\ncurrent method is employed for the classical benchmark problem of steady\nLid-driven cavity flows at different Reynolds numbers, to investigate the\napplicability and efficacy for the problem involved multiple modes of high and\nlow frequency. By virtue of various training sequence modes, improvement\nthrough predictions lead to 30% to 60% accuracy improvement. We also\ninvestigate the synergies between current method and transfer learning\ntechniques for more challenging problems (i.e., higher Re). From the present\nresults, it also revealed that the current framework can produce good\npredictions even for the case of Re=5000, demonstrating the ability to solve\ncomplex high-frequency PDEs."}
{"id": "2504.21045", "pdf": "https://arxiv.org/pdf/2504.21045", "abs": "https://arxiv.org/abs/2504.21045", "authors": ["Dennis Miczek", "Divyesh Gabbireddy", "Suman Saha"], "title": "Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection", "categories": ["cs.CR", "cs.AI"], "comment": "This work has been accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)", "summary": "According to the Open Web Application Security Project (OWASP), Cross-Site\nScripting (XSS) is a critical security vulnerability. Despite decades of\nresearch, XSS remains among the top 10 security vulnerabilities. Researchers\nhave proposed various techniques to protect systems from XSS attacks, with\nmachine learning (ML) being one of the most widely used methods. An ML model is\ntrained on a dataset to identify potential XSS threats, making its\neffectiveness highly dependent on the size and diversity of the training data.\nA variation of XSS is obfuscated XSS, where attackers apply obfuscation\ntechniques to alter the code's structure, making it challenging for security\nsystems to detect its malicious intent. Our study's random forest model was\ntrained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.\nHowever, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,\nunderscoring the importance of training ML models with obfuscated data to\nimprove their effectiveness in detecting XSS attacks. A significant challenge\nis to generate highly complex obfuscated code despite the availability of\nseveral public tools. These tools can only produce obfuscation up to certain\nlevels of complexity.\n  In our proposed system, we fine-tune a Large Language Model (LLM) to generate\ncomplex obfuscated XSS payloads automatically. By transforming original XSS\nsamples into diverse obfuscated variants, we create challenging training data\nfor ML model evaluation. Our approach achieved a 99.5% accuracy rate with the\nobfuscated dataset. We also found that the obfuscated samples generated by the\nLLMs were 28.1% more complex than those created by other tools, significantly\nimproving the model's ability to handle advanced XSS attacks and making it more\neffective for real-world application security."}
{"id": "2504.21423", "pdf": "https://arxiv.org/pdf/2504.21423", "abs": "https://arxiv.org/abs/2504.21423", "authors": ["Weicai Yan", "Wang Lin", "Zirun Guo", "Ye Wang", "Fangming Feng", "Xiaoda Yang", "Zehan Wang", "Tao Jin"], "title": "Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision", "categories": ["cs.CV"], "comment": "Accepted at ICLR 2025", "summary": "Prompt learning has demonstrated promising results in fine-tuning pre-trained\nmultimodal models. However, the performance improvement is limited when applied\nto more complex and fine-grained tasks. The reason is that most existing\nmethods directly optimize the parameters involved in the prompt generation\nprocess through loss backpropagation, which constrains the richness and\nspecificity of the prompt representations. In this paper, we propose\nDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion\nmodel to generate rich and fine-grained prompt information for complex\ndownstream tasks. Specifically, our approach consists of three stages. In the\nfirst stage, we train a Mask-VAE to compress the masks into latent space. In\nthe second stage, we leverage an improved Diffusion Transformer (DiT) to train\na prompt generator in the latent space, using the masks for supervision. In the\nthird stage, we align the denoising process of the prompt generator with the\npre-trained model in the semantic space, and use the generated prompts to\nfine-tune the model. We conduct experiments on a complex pixel-level downstream\ntask, referring expression comprehension, and compare our method with various\nparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum\nimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model\nand also outperforms other state-of-the-art methods across multiple metrics.\nThe experimental results validate the effectiveness of our approach and\nhighlight the potential of using generative models for prompt generation. Code\nis available at https://github.com/Kelvin-ywc/diff-prompt."}
{"id": "2504.21589", "pdf": "https://arxiv.org/pdf/2504.21589", "abs": "https://arxiv.org/abs/2504.21589", "authors": ["Lisa Kluge", "Maximilian Kähler"], "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts."}
{"id": "2504.21353", "pdf": "https://arxiv.org/pdf/2504.21353", "abs": "https://arxiv.org/abs/2504.21353", "authors": ["Vinti Nayar", "Kanica Sachdev", "Brejesh Lall"], "title": "Generative QoE Modeling: A Lightweight Approach for Telecom Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Quality of Experience (QoE) prediction plays a crucial role in optimizing\nresource management and enhancing user satisfaction across both\ntelecommunication and OTT services. While recent advances predominantly rely on\ndeep learning models, this study introduces a lightweight generative modeling\nframework that balances computational efficiency, interpretability, and\npredictive accuracy. By validating the use of Vector Quantization (VQ) as a\npreprocessing technique, continuous network features are effectively\ntransformed into discrete categorical symbols, enabling integration with a\nHidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline\nenhances the model's capacity to capture dynamic QoE patterns while supporting\nprobabilistic inference on new and unseen data. Experimental results on\npublicly available time-series datasets incorporating both objective indicators\nand subjective QoE scores demonstrate the viability of this approach in\nreal-time and resource-constrained environments, where inference latency is\nalso critical. The framework offers a scalable alternative to complex deep\nlearning methods, particularly in scenarios with limited computational\nresources or where latency constraints are critical."}
{"id": "2504.21049", "pdf": "https://arxiv.org/pdf/2504.21049", "abs": "https://arxiv.org/abs/2504.21049", "authors": ["Sneha Baskota"], "title": "Phishing URL Detection using Bi-LSTM", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Phishing attacks threaten online users, often leading to data breaches,\nfinancial losses, and identity theft. Traditional phishing detection systems\nstruggle with high false positive rates and are usually limited by the types of\nattacks they can identify. This paper proposes a deep learning-based approach\nusing a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs\ninto four categories: benign, phishing, defacement, and malware. The model\nleverages sequential URL data and captures contextual information, improving\nthe accuracy of phishing detection. Experimental results on a dataset\ncomprising over 650,000 URLs demonstrate the model's effectiveness, achieving\n97% accuracy and significant improvements over traditional techniques."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2504.21604", "pdf": "https://arxiv.org/pdf/2504.21604", "abs": "https://arxiv.org/abs/2504.21604", "authors": ["Bing Wang", "Ximing Li", "Changchun Li", "Bingrui Zhao", "Bo Fu", "Renchu Guan", "Shengsheng Wang"], "title": "Robust Misinformation Detection by Visiting Potential Commonsense Conflict", "categories": ["cs.CL", "cs.CY"], "comment": "11 pages, 2 figures. Accepted by IJCAI 2025. Code:\n  https://github.com/wangbing1416/MD-PCC", "summary": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines."}
{"id": "2504.21358", "pdf": "https://arxiv.org/pdf/2504.21358", "abs": "https://arxiv.org/abs/2504.21358", "authors": ["Xiao Zheng", "Saeed Asadi Bagloee", "Majid Sarvi"], "title": "A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "32 pages, 16 figures", "summary": "Traffic forecasting is vital for Intelligent Transportation Systems, for\nwhich Machine Learning (ML) methods have been extensively explored to develop\ndata-driven Artificial Intelligence (AI) solutions. Recent research focuses on\nmodelling spatial-temporal correlations for short-term traffic prediction,\nleaving the favourable long-term forecasting a challenging and open issue. This\npaper presents a comparative study on large-scale real-world signalized\narterials and freeway traffic flow datasets, aiming to evaluate promising ML\nmethods in the context of large forecasting horizons up to 30 days. Focusing on\nmodelling capacity for temporal dynamics, we develop one ensemble ML method,\neXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,\nincluding Recurrent Neural Network (RNN)-based methods and the state-of-the-art\nTransformer-based method. Time embedding is leveraged to enhance their\nunderstanding of seasonality and event factors. Experimental results highlight\nthat while the attention mechanism/Transformer framework is effective for\ncapturing long-range dependencies in sequential data, as the forecasting\nhorizon extends, the key to effective traffic forecasting gradually shifts from\ntemporal dependency capturing to periodicity modelling. Time embedding is\nparticularly effective in this context, helping naive RNN outperform Informer\nby 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust\nmodel, XGBoost, while learning solely from time features, performs\ncompetitively with DL methods. Moreover, we investigate the impacts of various\nfactors like input sequence length, holiday traffic, data granularity, and\ntraining data size. The findings offer valuable insights and serve as a\nreference for future long-term traffic forecasting research and the improvement\nof AI's corresponding learning capabilities."}
{"id": "2504.21052", "pdf": "https://arxiv.org/pdf/2504.21052", "abs": "https://arxiv.org/abs/2504.21052", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Zhishuai Li", "Weifeng Liu"], "title": "SFIBA: Spatial-based Full-target Invisible Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Multi-target backdoor attacks pose significant security threats to deep\nneural networks, as they can preset multiple target classes through a single\nbackdoor injection. This allows attackers to control the model to misclassify\npoisoned samples with triggers into any desired target class during inference,\nexhibiting superior attack performance compared with conventional backdoor\nattacks. However, existing multi-target backdoor attacks fail to guarantee\ntrigger specificity and stealthiness in black-box settings, resulting in two\nmain issues. First, they are unable to simultaneously target all classes when\nonly training data can be manipulated, limiting their effectiveness in\nrealistic attack scenarios. Second, the triggers often lack visual\nimperceptibility, making poisoned samples easy to detect. To address these\nproblems, we propose a Spatial-based Full-target Invisible Backdoor Attack,\ncalled SFIBA. It restricts triggers for different classes to specific local\nspatial regions and morphologies in the pixel space to ensure specificity,\nwhile employing a frequency-domain-based trigger injection method to guarantee\nstealthiness. Specifically, for injection of each trigger, we first apply fast\nfourier transform to obtain the amplitude spectrum of clean samples in local\nspatial regions. Then, we employ discrete wavelet transform to extract the\nfeatures from the amplitude spectrum and use singular value decomposition to\nintegrate the trigger. Subsequently, we selectively filter parts of the trigger\nin pixel space to implement trigger morphology constraints and adjust injection\ncoefficients based on visual effects. We conduct experiments on multiple\ndatasets and models. The results demonstrate that SFIBA can achieve excellent\nattack performance and stealthiness, while preserving the model's performance\non benign samples, and can also bypass existing backdoor defenses."}
{"id": "2504.21447", "pdf": "https://arxiv.org/pdf/2504.21447", "abs": "https://arxiv.org/abs/2504.21447", "authors": ["Haoran Chen", "Junyan Lin", "Xinhao Chen", "Yue Fan", "Xin Jin", "Hui Su", "Jianfeng Dong", "Jinlan Fu", "Xiaoyu Shen"], "title": "Rethinking Visual Layer Selection in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures, submitted to ICCV 2025", "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs."}
{"id": "2504.21605", "pdf": "https://arxiv.org/pdf/2504.21605", "abs": "https://arxiv.org/abs/2504.21605", "authors": ["Jonas Gwozdz", "Andreas Both"], "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy."}
{"id": "2504.21375", "pdf": "https://arxiv.org/pdf/2504.21375", "abs": "https://arxiv.org/abs/2504.21375", "authors": ["Sangyeon Cho", "Jangyeong Jeon", "Mingi Kim", "Junyeong Kim"], "title": "Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning", "categories": ["cs.LG"], "comment": "Multi-modal, Multi-modal Representation Learning, Missing Modality,\n  Missing Modality Reconstruction, Speech and Multi-modality, Vision and\n  Language", "summary": "Multi-modal representation learning has become a pivotal area in artificial\nintelligence, enabling the integration of diverse modalities such as vision,\ntext, and audio to solve complex problems. However, existing approaches\npredominantly focus on bimodal interactions, such as image-text pairs, which\nlimits their ability to fully exploit the richness of multi-modal data.\nFurthermore, the integration of modalities in equal-scale environments remains\nunderexplored due to the challenges of constructing large-scale, balanced\ndatasets. In this study, we propose Synergy-CLIP, a novel framework that\nextends the contrastive language-image pre-training (CLIP) architecture to\nenhance multi-modal representation learning by integrating visual, textual, and\naudio modalities. Unlike existing methods that focus on adapting individual\nmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information\nacross three modalities equally. To address the high cost of constructing\nlarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal\ndataset designed to provide equal-scale representation of visual, textual, and\naudio data. Synergy-CLIP is validated on various downstream tasks, including\nzero-shot classification, where it outperforms existing baselines.\nAdditionally, we introduce a missing modality reconstruction task,\ndemonstrating Synergy-CLIP's ability to extract synergy among modalities in\nrealistic application scenarios. These contributions provide a robust\nfoundation for advancing multi-modal representation learning and exploring new\nresearch directions."}
{"id": "2504.21054", "pdf": "https://arxiv.org/pdf/2504.21054", "abs": "https://arxiv.org/abs/2504.21054", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Liantao Wu", "Zhe Li", "Weifeng Liu"], "title": "FFCBA: Feature-based Full-target Clean-label Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Backdoor attacks pose a significant threat to deep neural networks, as\nbackdoored models would misclassify poisoned samples with specific triggers\ninto target classes while maintaining normal performance on clean samples.\nAmong these, multi-target backdoor attacks can simultaneously target multiple\nclasses. However, existing multi-target backdoor attacks all follow the\ndirty-label paradigm, where poisoned samples are mislabeled, and most of them\nrequire an extremely high poisoning rate. This makes them easily detectable by\nmanual inspection. In contrast, clean-label attacks are more stealthy, as they\navoid modifying the labels of poisoned samples. However, they generally\nstruggle to achieve stable and satisfactory attack performance and often fail\nto scale effectively to multi-target attacks. To address this issue, we propose\nthe Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which\nconsists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and\nFeature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional\nautoencoders to generate noise triggers that align perturbed in-class samples\nwith the original category's features, ensuring the effectiveness, intra-class\nconsistency, inter-class specificity and natural-feature correlation of\ntriggers. While FSBA supports swift and efficient attacks, its cross-model\nattack capability is relatively weak. FMBA employs a two-stage\nclass-conditional autoencoder training process that alternates between using\nout-of-class samples and in-class samples. This allows FMBA to generate\ntriggers with strong target-class features, making it highly effective for\ncross-model attacks. We conduct experiments on multiple datasets and models,\nthe results show that FFCBA achieves outstanding attack performance and\nmaintains desirable robustness against the state-of-the-art backdoor defenses."}
{"id": "2504.21464", "pdf": "https://arxiv.org/pdf/2504.21464", "abs": "https://arxiv.org/abs/2504.21464", "authors": ["Shamim Rahim Refat", "Ziyan Shirin Raha", "Shuvashis Sarker", "Faika Fairuj Preotee", "MD. Musfikur Rahman", "Tashreef Muhammad", "Mohammad Shafiul Islam"], "title": "VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification", "categories": ["cs.CV"], "comment": "33 pages, 49 figures", "summary": "Diabetic retinopathy is a severe eye condition caused by diabetes where the\nretinal blood vessels get damaged and can lead to vision loss and blindness if\nnot treated. Early and accurate detection is key to intervention and stopping\nthe disease progressing. For addressing this disease properly, this paper\npresents a comprehensive approach for automated diabetic retinopathy detection\nby proposing a new hybrid deep learning model called VR-FuseNet. Diabetic\nretinopathy is a major eye disease and leading cause of blindness especially\namong diabetic patients so accurate and efficient automated detection methods\nare required. To address the limitations of existing methods including dataset\nimbalance, diversity and generalization issues this paper presents a hybrid\ndataset created from five publicly available diabetic retinopathy datasets.\nEssential preprocessing techniques such as SMOTE for class balancing and CLAHE\nfor image enhancement are applied systematically to the dataset to improve the\nrobustness and generalizability of the dataset. The proposed VR-FuseNet model\ncombines the strengths of two state-of-the-art convolutional neural networks,\nVGG19 which captures fine-grained spatial features and ResNet50V2 which is\nknown for its deep hierarchical feature extraction. This fusion improves the\ndiagnostic performance and achieves an accuracy of 91.824%. The model\noutperforms individual architectures on all performance metrics demonstrating\nthe effectiveness of hybrid feature extraction in Diabetic Retinopathy\nclassification tasks. To make the proposed model more clinically useful and\ninterpretable this paper incorporates multiple XAI techniques. These techniques\ngenerate visual explanations that clearly indicate the retinal features\naffecting the model's prediction such as microaneurysms, hemorrhages and\nexudates so that clinicians can interpret and validate."}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625", "abs": "https://arxiv.org/abs/2504.21625", "authors": ["Jiaming Wang"], "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability", "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications."}
{"id": "2504.21380", "pdf": "https://arxiv.org/pdf/2504.21380", "abs": "https://arxiv.org/abs/2504.21380", "authors": ["Inês Cardoso Oliveira", "Decebal Constantin Mocanu", "Luis A. Leiva"], "title": "Sparse-to-Sparse Training of Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs."}
{"id": "2504.21072", "pdf": "https://arxiv.org/pdf/2504.21072", "abs": "https://arxiv.org/abs/2504.21072", "authors": ["Jonas Henry Grebe", "Tobias Braun", "Marcus Rohrbach", "Anna Rohrbach"], "title": "Erased but Not Forgotten: How Backdoors Compromise Concept Erasure", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns about their potential to generate undesirable or harmful\ncontent, ranging from fabricated depictions of public figures to sexually\nexplicit images. To mitigate these risks, prior work has devised machine\nunlearning techniques that attempt to erase unwanted concepts through\nfine-tuning. However, in this paper, we introduce a new threat model, Toxic\nErasure (ToxE), and demonstrate how recent unlearning algorithms, including\nthose explicitly designed for robustness, can be circumvented through targeted\nbackdoor attacks. The threat is realized by establishing a link between a\ntrigger and the undesired content. Subsequent unlearning attempts fail to erase\nthis link, allowing adversaries to produce harmful content. We instantiate ToxE\nvia two established backdoor attacks: one targeting the text encoder and\nanother manipulating the cross-attention layers. Further, we introduce Deep\nIntervention Score-based Attack (DISA), a novel, deeper backdoor attack that\noptimizes the entire U-Net using a score-based objective, improving the\nattack's persistence across different erasure methods. We evaluate five recent\nconcept erasure methods against our threat model. For celebrity identity\nerasure, our deep attack circumvents erasure with up to 82% success, averaging\n57% across all erasure methods. For explicit content erasure, ToxE attacks can\nelicit up to 9 times more exposed body parts, with DISA yielding an average\nincrease by a factor of 2.9. These results highlight a critical security gap in\ncurrent unlearning strategies."}
{"id": "2504.21467", "pdf": "https://arxiv.org/pdf/2504.21467", "abs": "https://arxiv.org/abs/2504.21467", "authors": ["Luc Vedrenne", "Sylvain Faisan", "Denis Fortun"], "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space", "categories": ["cs.CV"], "comment": "14 pages, 19 figures, IEEE Transactions on Image Processing", "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration."}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635", "abs": "https://arxiv.org/abs/2504.21635", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools."}
{"id": "2504.21383", "pdf": "https://arxiv.org/pdf/2504.21383", "abs": "https://arxiv.org/abs/2504.21383", "authors": ["Pulkit Agrawal", "Rukma Talwadker", "Aditya Pareek", "Tridib Mukherjee"], "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform."}
{"id": "2504.21074", "pdf": "https://arxiv.org/pdf/2504.21074", "abs": "https://arxiv.org/abs/2504.21074", "authors": ["Adrian Rebmann", "Fabian David Schmidt", "Goran Glavaš", "Han van der Aa"], "title": "On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks", "categories": ["cs.DB", "cs.AI"], "comment": "31 pages, submitted to PS", "summary": "Large language models (LLMs) have shown to be valuable tools for tackling\nprocess mining tasks. Existing studies report on their capability to support\nvarious data-driven process analyses and even, to some extent, that they are\nable to reason about how processes work. This reasoning ability suggests that\nthere is potential for LLMs to tackle semantics-aware process mining tasks,\nwhich are tasks that rely on an understanding of the meaning of activities and\ntheir relationships. Examples of these include process discovery, where the\nmeaning of activities can indicate their dependency, whereas in anomaly\ndetection the meaning can be used to recognize process behavior that is\nabnormal. In this paper, we systematically explore the capabilities of LLMs for\nsuch tasks. Unlike prior work, which largely evaluates LLMs in their default\nstate, we investigate their utility through both in-context learning and\nsupervised fine-tuning. Concretely, we define five process mining tasks\nrequiring semantic understanding and provide extensive benchmarking datasets\nfor evaluation. Our experiments reveal that while LLMs struggle with\nchallenging process mining tasks when used out of the box or with minimal\nin-context examples, they achieve strong performance when fine-tuned for these\ntasks across a broad range of process types and industries."}
{"id": "2504.21468", "pdf": "https://arxiv.org/pdf/2504.21468", "abs": "https://arxiv.org/abs/2504.21468", "authors": ["Yu Guo", "Guoqing Chen", "Tieyong Zeng", "Qiyu Jin", "Michael Kwok-Po Ng"], "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion", "categories": ["cs.CV", "65F35, 90C30, 94A08, 68U10"], "comment": null, "summary": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods."}
{"id": "2504.21677", "pdf": "https://arxiv.org/pdf/2504.21677", "abs": "https://arxiv.org/abs/2504.21677", "authors": ["Michelle Wastl", "Jannis Vamvas", "Selena Calleri", "Rico Sennrich"], "title": "20min-XD: A Comparable Corpus of Swiss News Articles", "categories": ["cs.CL"], "comment": "10 pages; accepted at SwissText 2025", "summary": "We present 20min-XD (20 Minuten cross-lingual document-level), a\nFrench-German, document-level comparable corpus of news articles, sourced from\nthe Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises\naround 15,000 article pairs spanning 2015 to 2024, automatically aligned based\non semantic similarity. We detail the data collection process and alignment\nmethodology. Furthermore, we provide a qualitative and quantitative analysis of\nthe corpus. The resulting dataset exhibits a broad spectrum of cross-lingual\nsimilarity, ranging from near-translations to loosely related articles, making\nit valuable for various NLP applications and broad linguistically motivated\nstudies. We publicly release the dataset in document- and sentence-aligned\nversions and code for the described experiments."}
{"id": "2504.21389", "pdf": "https://arxiv.org/pdf/2504.21389", "abs": "https://arxiv.org/abs/2504.21389", "authors": ["Jianyu Zhang", "Jianshe Feng", "Yizhang Zhu", "Fanyu Qi"], "title": "Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction", "categories": ["cs.LG"], "comment": "19 pages, 14 figures", "summary": "In tackling frequent anomalies in stamping processes, this study introduces a\nnovel semi-supervised in-process anomaly monitoring framework, utilizing\naccelerometer signals and physics information, to capture the process anomaly\neffectively. The proposed framework facilitates the construction of a\nmonitoring model with imbalanced sample distribution, which enables in-process\ncondition monitoring in real-time to prevent batch anomalies, which helps to\nreduce batch defects risk and enhance production yield. Firstly, to effectively\ncapture key features from raw data containing redundant information, a hybrid\nfeature extraction algorithm is proposed to utilize data-driven methods and\nphysical mechanisms simultaneously. Secondly, to address the challenge brought\nby imbalanced sample distribution, a semi-supervised anomaly detection model is\nestablished, which merely employs normal samples to build a golden baseline\nmodel, and a novel deviation score is proposed to quantify the anomaly level of\neach online stamping stroke. The effectiveness of the proposed feature\nextraction method is validated with various classification algorithms. A\nreal-world in-process dataset from stamping manufacturing workshop is employed\nto illustrate the superiority of proposed semi-supervised framework with\nenhance performance for process anomaly monitoring."}
{"id": "2504.21155", "pdf": "https://arxiv.org/pdf/2504.21155", "abs": "https://arxiv.org/abs/2504.21155", "authors": ["Fauzan Nazranda Rizqa", "Matthew Hole", "Charles Gretton"], "title": "Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation", "categories": ["physics.plasm-ph", "cs.AI", "cs.NE"], "comment": "9 pages, 4 figures", "summary": "Our contributions are motivated by fusion reactors that rely on maintaining\nmagnetohydrodynamic (MHD) equilibrium, where the balance between plasma\npressure and confining magnetic fields is required for stable operation. In\naxisymmetric tokamak reactors in particular, and under the assumption of\ntoroidal symmetry, this equilibrium can be mathematically modelled using the\nGrad-Shafranov Equation (GSE). Recent works have demonstrated the potential of\nusing Physics-Informed Neural Networks (PINNs) to model the GSE. Existing\nstudies did not examine realistic scenarios in which a single network\ngeneralizes to a variety of boundary conditions. Addressing that limitation, we\nevaluate a PINN architecture that incorporates boundary points as network\ninputs. Additionally, we compare PINN model accuracy and inference speeds with\na Fourier Neural Operator (FNO) model. Finding the PINN model to be the most\nperformant, and accurate in our setting, we use the network verification tool\nMarabou to perform a range of verification tasks. Although we find some\ndiscrepancies between evaluations of the networks natively in PyTorch, compared\nto via Marabou, we are able to demonstrate useful and practical verification\nworkflows. Our study is the first investigation of verification of such\nnetworks."}
{"id": "2504.21472", "pdf": "https://arxiv.org/pdf/2504.21472", "abs": "https://arxiv.org/abs/2504.21472", "authors": ["Jingjing Liu", "Nian Wu", "Xianchao Xiu", "Jianhua Zhang"], "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu."}
{"id": "2504.21681", "pdf": "https://arxiv.org/pdf/2504.21681", "abs": "https://arxiv.org/abs/2504.21681", "authors": ["Andrei-Alexandru Manea", "Jindřich Libovický"], "title": "Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Most pre-trained Vision-Language (VL) models and training data for the\ndownstream tasks are only available in English. Therefore, multilingual VL\ntasks are solved using cross-lingual transfer: fine-tune a multilingual\npre-trained model or transfer the text encoder using parallel data. We study\nthe alternative approach: transferring an already trained encoder using\nparallel data. We investigate the effect of parallel data: domain and the\nnumber of languages, which were out of focus in previous work. Our results show\nthat even machine-translated task data are the best on average, caption-like\nauthentic parallel data outperformed it in some languages. Further, we show\nthat most languages benefit from multilingual training."}
{"id": "2504.21427", "pdf": "https://arxiv.org/pdf/2504.21427", "abs": "https://arxiv.org/abs/2504.21427", "authors": ["Shermin Shahbazi", "Mohammad-Reza Nasiri", "Majid Ramezani"], "title": "MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages ,3 figures", "summary": "Accurate classification of EEG signals is crucial for brain-computer\ninterfaces (BCIs) and neuroprosthetic applications, yet many existing methods\nfail to account for the non-Euclidean, manifold structure of EEG data,\nresulting in suboptimal performance. Preserving this manifold information is\nessential to capture the true geometry of EEG signals, but traditional\nclassification techniques largely overlook this need. To this end, we propose\nMPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based\nClassifiers), that introduces two key innovations: (1) a feature engineering\nphase that combines covariance matrices and Radial Basis Function (RBF) kernels\nto capture both linear and non-linear relationships among EEG channels, and (2)\na clustering phase that employs a modified K-means algorithm tailored for the\nRiemannian manifold space, ensuring local geometric sensitivity. Ensembling\nmultiple clustering-based classifiers, MPEC achieves superior results,\nvalidated by significant improvements on the BCI Competition IV dataset 2a."}
{"id": "2504.21195", "pdf": "https://arxiv.org/pdf/2504.21195", "abs": "https://arxiv.org/abs/2504.21195", "authors": ["Kelsey E. Ennis", "Elizabeth A. Barnes", "Marybeth C. Arcodia", "Martin A. Fernandez", "Eric D. Maloney"], "title": "Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Extreme heat is the deadliest weather-related hazard in the United States.\nFurthermore, it is increasing in intensity, frequency, and duration, making\nskillful forecasts vital to protecting life and property. Traditional numerical\nweather prediction (NWP) models struggle with extreme heat for medium-range and\nsubseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial\nintelligence-based weather prediction (AIWP) models are progressing rapidly.\nHowever, it is largely unknown how well AIWP models forecast extremes,\nespecially for medium-range and S2S timescales. This study investigates 2-m\ntemperature forecasts for 60 heat waves across the four boreal seasons and over\nfour CONUS regions at lead times up to 20 days, using two AIWP models (Google\nGraphCast and Pangu-Weather) and one traditional NWP model (NOAA United\nForecast System Global Ensemble Forecast System (UFS GEFS)). First, case study\nanalyses show that both AIWP models and the UFS GEFS exhibit consistent cold\nbiases on regional scales in the 5-10 days of lead time before heat wave onset.\nGraphCast is the more skillful AIWP model, outperforming UFS GEFS and\nPangu-Weather in most locations. Next, the two AIWP models are isolated and\nanalyzed across all heat waves and seasons, with events split among the model's\ntesting (2018-2023) and training (1979-2017) periods. There are cold biases\nbefore and during the heat waves in both models and all seasons, except\nPangu-Weather in winter, which exhibits a mean warm bias before heat wave\nonset. Overall, results offer encouragement that AIWP models may be useful for\nmedium-range and S2S predictability of extreme heat."}
{"id": "2504.21476", "pdf": "https://arxiv.org/pdf/2504.21476", "abs": "https://arxiv.org/abs/2504.21476", "authors": ["Xinyu Li", "Qi Yao", "Yuanda Wang"], "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)", "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/."}
{"id": "2504.21685", "pdf": "https://arxiv.org/pdf/2504.21685", "abs": "https://arxiv.org/abs/2504.21685", "authors": ["Reem Abdel-Salam", "Mary Adewunmi"], "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency."}
{"id": "2504.21436", "pdf": "https://arxiv.org/pdf/2504.21436", "abs": "https://arxiv.org/abs/2504.21436", "authors": ["Zhixuan Ma", "Haichang Gao", "Junxiang Huang", "Ping Wang"], "title": "Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Federated Learning enables collaborative training of a global model across\nmultiple geographically dispersed clients without the need for data sharing.\nHowever, it is susceptible to inference attacks, particularly label inference\nattacks.\n  Existing studies on label distribution inference exhibits sensitive to the\nspecific settings of the victim client and typically underperforms under\ndefensive strategies. In this study, we propose a novel label distribution\ninference attack that is stable and adaptable to various scenarios.\nSpecifically, we estimate the size of the victim client's dataset and construct\nseveral virtual clients tailored to the victim client. We then quantify the\ntemporal generalization of each class label for the virtual clients and utilize\nthe variation in temporal generalization to train an inference model that\npredicts the label distribution proportions of the victim client.\n  We validate our approach on multiple datasets, including MNIST,\nFashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of\nour method compared to state-of-the-art techniques. Furthermore, our attack\nremains effective even under differential privacy defense mechanisms,\nunderscoring its potential for real-world applications."}
{"id": "2504.21205", "pdf": "https://arxiv.org/pdf/2504.21205", "abs": "https://arxiv.org/abs/2504.21205", "authors": ["Connor Dilgren", "Purva Chiniya", "Luke Griffith", "Yu Ding", "Yizheng Chen"], "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories."}
{"id": "2504.21478", "pdf": "https://arxiv.org/pdf/2504.21478", "abs": "https://arxiv.org/abs/2504.21478", "authors": ["Zherui Zhang", "Changwei Wang", "Rongtao Xu", "Wenhao Xu", "Shibiao Xu", "Yu Zhang", "Li Guo"], "title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\nthe given pre-trained teacher network to the target student model without\naccess to the real training data. Existing DFKD methods focus primarily on\nimproving image recognition performance on associated datasets, often\nneglecting the crucial aspect of the transferability of learned\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\nlimitations of previous rely on image-level methods to improve model\ngeneralization but fail when directly applied to DFKD. The superiority and\nflexibility of CAE-DFKD are extensively evaluated, including:\n\\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering\nthe generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance\nwith existing DFKD state-of-the-art methods on image recognition tasks;\n\\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned\nrepresentations demonstrated in downstream tasks."}
{"id": "2504.21742", "pdf": "https://arxiv.org/pdf/2504.21742", "abs": "https://arxiv.org/abs/2504.21742", "authors": ["Emelie Hallenberg"], "title": "Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The Greek fictional narratives often termed love novels or romances, ranging\nfrom the first century CE to the middle of the 15th century, have long been\nconsidered as similar in many ways, not least in the use of particular literary\nmotifs. By applying the use of fine-tuned large language models, this study\naims to investigate which motifs exactly that the texts in this corpus have in\ncommon, and in which ways they differ from each other. The results show that\nwhile some motifs persist throughout the corpus, others fluctuate in frequency,\nindicating certain trends or external influences. Conclusively, the method\nproves to adequately extract literary motifs according to a set definition,\nproviding data for both quantitative and qualitative analyses."}
{"id": "2504.21457", "pdf": "https://arxiv.org/pdf/2504.21457", "abs": "https://arxiv.org/abs/2504.21457", "authors": ["Andrea Zanola", "Louis Fabrice Tshimanga", "Federico Del Pup", "Marco Baiesi", "Manfredo Atzori"], "title": "xEEGNet: Towards Explainable AI in EEG Dementia Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work presents xEEGNet, a novel, compact, and explainable neural network\nfor EEG data analysis. It is fully interpretable and reduces overfitting\nthrough major parameter reduction. As an applicative use case, we focused on\nclassifying common dementia conditions, Alzheimer's and frontotemporal\ndementia, versus controls. xEEGNet is broadly applicable to other neurological\nconditions involving spectral alterations. We initially used ShallowNet, a\nsimple and popular model from the EEGNet-family. Its structure was analyzed and\ngradually modified to move from a \"black box\" to a more transparent model,\nwithout compromising performance. The learned kernels and weights were examined\nfrom a clinical standpoint to assess medical relevance. Model variants,\nincluding ShallowNet and the final xEEGNet, were evaluated using robust\nNested-Leave-N-Subjects-Out cross-validation for unbiased performance\nestimates. Variability across data splits was explained using embedded EEG\nrepresentations, grouped by class and set, with pairwise separability to\nquantify group distinction. Overfitting was assessed through\ntraining-validation loss correlation and training speed. xEEGNet uses only 168\nparameters, 200 times fewer than ShallowNet, yet retains interpretability,\nresists overfitting, achieves comparable median performance (-1.5%), and\nreduces variability across splits. This variability is explained by embedded\nEEG representations: higher accuracy correlates with greater separation between\ntest set controls and Alzheimer's cases, without significant influence from\ntraining data. xEEGNet's ability to filter specific EEG bands, learn\nband-specific topographies, and use relevant spectral features demonstrates its\ninterpretability. While large deep learning models are often prioritized for\nperformance, this study shows smaller architectures like xEEGNet can be equally\neffective in EEG pathology classification."}
{"id": "2504.21228", "pdf": "https://arxiv.org/pdf/2504.21228", "abs": "https://arxiv.org/abs/2504.21228", "authors": ["Rui Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ruiyi Zhang", "Ryan Rossi", "Lina Yao", "Julian McAuley"], "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."}
{"id": "2504.21487", "pdf": "https://arxiv.org/pdf/2504.21487", "abs": "https://arxiv.org/abs/2504.21487", "authors": ["Hebaixu Wang", "Jing Zhang", "Haonan Guo", "Di Wang", "Jiayi Ma", "Bo Du"], "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver."}
{"id": "2504.21747", "pdf": "https://arxiv.org/pdf/2504.21747", "abs": "https://arxiv.org/abs/2504.21747", "authors": ["Maxime Bouthors", "Josep Crego", "François Yvon"], "title": "Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data", "categories": ["cs.CL", "I.2.7"], "comment": "13 pages", "summary": "Conventional retrieval-augmented neural machine translation (RANMT) systems\nleverage bilingual corpora, e.g., translation memories (TMs). Yet, in many\nsettings, in-domain monolingual target-side corpora are often available. This\nwork explores ways to take advantage of such resources by retrieving relevant\nsegments directly in the target language, based on a source-side query. For\nthis, we design improved cross-lingual retrieval systems, trained with both\nsentence level and word-level matching objectives. In our experiments with two\nRANMT architectures, we first demonstrate the benefits of such cross-lingual\nobjectives in a controlled setting, obtaining translation performances that\nsurpass standard TM-based models. We then showcase our method on a real-world\nset-up, where the target monolingual resources far exceed the amount of\nparallel data and observe large improvements of our new techniques, which\noutperform both the baseline setting, and general-purpose cross-lingual\nretrievers."}
{"id": "2504.21501", "pdf": "https://arxiv.org/pdf/2504.21501", "abs": "https://arxiv.org/abs/2504.21501", "authors": ["Yaru Liu", "Yiqi Gu", "Michael K. Ng"], "title": "Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables", "categories": ["cs.LG"], "comment": "32 pages, 11 figures", "summary": "In this paper, we develop a new optimization framework for the least squares\nlearning problem via fully connected neural networks or physics-informed neural\nnetworks. The gradient descent sometimes behaves inefficiently in deep learning\nbecause of the high non-convexity of loss functions and the vanishing gradient\nissue. Our idea is to introduce auxiliary variables to separate the layers of\nthe deep neural networks and reformulate the loss functions for ease of\noptimization. We design the self-adaptive weights to preserve the consistency\nbetween the reformulated loss and the original mean squared loss, which\nguarantees that optimizing the new loss helps optimize the original problem.\nNumerical experiments are presented to verify the consistency and show the\neffectiveness and robustness of our models over gradient descent."}
{"id": "2504.21235", "pdf": "https://arxiv.org/pdf/2504.21235", "abs": "https://arxiv.org/abs/2504.21235", "authors": ["Ben Goertzel"], "title": "Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "We present a lattice-based scheme for homomorphic evaluation of quantum\nprograms and proofs that remains secure against quantum adversaries. Classical\nhomomorphic encryption is lifted to the quantum setting by replacing\ncomposite-order groups with Module Learning-With-Errors (MLWE) lattices and by\ngeneralizing polynomial functors to bounded natural super functors (BNSFs). A\nsecret depolarizing BNSF mask hides amplitudes, while each quantum state is\nstored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game\nthat allows coherent access to the encryption oracle and give a four-hybrid\nreduction to decisional MLWE.\n  The design also covers practical issues usually left open. A typed QC-bridge\nkeeps classical bits produced by measurements encrypted yet still usable as\ncontrols, with weak-measurement semantics for expectation-value workloads.\nEncrypted Pauli twirls add circuit privacy. If a fixed knowledge base is\nneeded, its axioms are shipped as MLWE \"capsules\"; the evaluator can use them\nbut cannot read them. A rho-calculus driver schedules encrypted tasks across\nseveral QPUs and records an auditable trace on an RChain-style ledger.\n  Performance analysis shows that the extra lattice arithmetic fits inside\ntoday's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof\nruns in about 10 ms, the public key (seed only) is 32 bytes, and even a\nCCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes\nhomomorphic teleportation plus knowledge-base-relative amplitude checks appears\nfeasible with current hardware. These results indicate that fully homomorphic,\nknowledge-base-aware quantum reasoning is compatible with near-term quantum\nclouds and standard post-quantum security assumptions."}
{"id": "2504.21497", "pdf": "https://arxiv.org/pdf/2504.21497", "abs": "https://arxiv.org/abs/2504.21497", "authors": ["Mengting Wei", "Yante Li", "Tuomas Varanka", "Yan Jiang", "Licai Sun", "Guoying Zhao"], "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait."}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773", "abs": "https://arxiv.org/abs/2504.21773", "authors": ["Junsheng Huang", "Zhitao He", "Sandeep Polisetty", "Qingyun Wang", "May Fung"], "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."}
{"id": "2504.21565", "pdf": "https://arxiv.org/pdf/2504.21565", "abs": "https://arxiv.org/abs/2504.21565", "authors": ["David Fernández Narro", "Pablo Ferri", "Juan M. García-Gómez", "Carlos Sáez"], "title": "Towards proactive self-adaptive AI for non-stationary environments with dataset shifts", "categories": ["cs.LG", "cs.AI", "I.2.8"], "comment": "6 pages, 4 figures, conference paper", "summary": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health."}
{"id": "2504.21276", "pdf": "https://arxiv.org/pdf/2504.21276", "abs": "https://arxiv.org/abs/2504.21276", "authors": ["Wanyi Chen", "Meng-Wen Su", "Mary L. Cummings"], "title": "Assessing LLM code generation quality through path planning tasks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing."}
{"id": "2504.21544", "pdf": "https://arxiv.org/pdf/2504.21544", "abs": "https://arxiv.org/abs/2504.21544", "authors": ["Uzair Shah", "Marco Agus", "Daniya Boges", "Vanessa Chiappini", "Mahmood Alzubaidi", "Jens Schneider", "Markus Hadwiger", "Pierre J. Magistretti", "Mowafa Househ", "Corrado Calı"], "title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks", "categories": ["cs.CV"], "comment": "Accepted at (CVPRW) 10th IEEE Workshop on Computer Vision for\n  Microscopy Image Analysis (CVMI)", "summary": "We present SAM4EM, a novel approach for 3D segmentation of complex neural\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\ninclude the development of a prompt-free adapter for SAM using two stage mask\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\nlimited annotated data, and a 3D memory attention mechanism to ensure\nsegmentation consistency across 3D stacks. We further release a unique\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\nevaluated our method on challenging neuroscience segmentation benchmarks,\nspecifically targeting mitochondria, glia, and synapses, with significant\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\nSAM-based adapters developed for the medical domain and other vision\ntransformer-based approaches. Experimental results indicate that our approach\noutperforms existing solutions in the segmentation of complex processes like\nglia and post-synaptic densities. Our code and models are available at\nhttps://github.com/Uzshah/SAM4EM."}
{"id": "2504.21776", "pdf": "https://arxiv.org/pdf/2504.21776", "abs": "https://arxiv.org/abs/2504.21776", "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker."}
{"id": "2504.21662", "pdf": "https://arxiv.org/pdf/2504.21662", "abs": "https://arxiv.org/abs/2504.21662", "authors": ["Mauricio Ortiz Torres", "Markus Lange", "Arne P. Raulf"], "title": "On Advancements of the Forward-Forward Algorithm", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The Forward-Forward algorithm has evolved in machine learning research,\ntackling more complex tasks that mimic real-life applications. In the last\nyears, it has been improved by several techniques to perform better than its\noriginal version, handling a challenging dataset like CIFAR10 without losing\nits flexibility and low memory usage. We have shown in our results that\nimprovements are achieved through a combination of convolutional channel\ngrouping, learning rate schedules, and independent block structures during\ntraining that lead to a 20\\% decrease in test error percentage. Additionally,\nto approach further implementations on low-capacity hardware projects we have\npresented a series of lighter models that achieve low test error percentages\nwithin (21$\\pm$6)\\% and number of trainable parameters between 164,706 and\n754,386. This serving also as a basis for our future study on complete\nverification and validation of these kinds of neural networks."}
{"id": "2504.21297", "pdf": "https://arxiv.org/pdf/2504.21297", "abs": "https://arxiv.org/abs/2504.21297", "authors": ["Wenjun Yang", "Eyhab Al-Masri"], "title": "Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI", "categories": ["cs.IT", "cs.AI", "cs.CY", "cs.ET", "math.IT"], "comment": null, "summary": "This paper introduces a conversational interface system that enables\nparticipatory design of differentially private AI systems in public sector\napplications. Addressing the challenge of balancing mathematical privacy\nguarantees with democratic accountability, we propose three key contributions:\n(1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria\ndecision analysis to align citizen preferences with differential privacy (DP)\nparameters, (2) an explainable noise-injection framework featuring real-time\nMean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and\n(3) an integrated legal-compliance mechanism that dynamically modulates privacy\nbudgets based on evolving regulatory constraints. Our results advance\nparticipatory AI practices by demonstrating how conversational interfaces can\nenhance public engagement in algorithmic privacy mechanisms, ensuring that\nprivacy-preserving AI in public sector governance remains both mathematically\nrobust and democratically accountable."}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559", "abs": "https://arxiv.org/abs/2504.21559", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800", "abs": "https://arxiv.org/abs/2504.21800", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain."}
{"id": "2504.21707", "pdf": "https://arxiv.org/pdf/2504.21707", "abs": "https://arxiv.org/abs/2504.21707", "authors": ["Anthony D Martin"], "title": "Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications."}
{"id": "2504.21323", "pdf": "https://arxiv.org/pdf/2504.21323", "abs": "https://arxiv.org/abs/2504.21323", "authors": ["Chen Wu", "Qian Ma", "Prasenjit Mitra", "Sencun Zhu"], "title": "How to Backdoor the Knowledge Distillation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge distillation has become a cornerstone in modern machine learning\nsystems, celebrated for its ability to transfer knowledge from a large, complex\nteacher model to a more efficient student model. Traditionally, this process is\nregarded as secure, assuming the teacher model is clean. This belief stems from\nconventional backdoor attacks relying on poisoned training data with backdoor\ntriggers and attacker-chosen labels, which are not involved in the distillation\nprocess. Instead, knowledge distillation uses the outputs of a clean teacher\nmodel to guide the student model, inherently preventing recognition or response\nto backdoor triggers as intended by an attacker. In this paper, we challenge\nthis assumption by introducing a novel attack methodology that strategically\npoisons the distillation dataset with adversarial examples embedded with\nbackdoor triggers. This technique allows for the stealthy compromise of the\nstudent model while maintaining the integrity of the teacher model. Our\ninnovative approach represents the first successful exploitation of\nvulnerabilities within the knowledge distillation process using clean teacher\nmodels. Through extensive experiments conducted across various datasets and\nattack settings, we demonstrate the robustness, stealthiness, and effectiveness\nof our method. Our findings reveal previously unrecognized vulnerabilities and\npave the way for future research aimed at securing knowledge distillation\nprocesses against backdoor attacks."}
{"id": "2504.21561", "pdf": "https://arxiv.org/pdf/2504.21561", "abs": "https://arxiv.org/abs/2504.21561", "authors": ["Pengxiang Li", "Zhi Gao", "Bofei Zhang", "Yapeng Mi", "Xiaojian Ma", "Chenrui Shi", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Iterative Trajectory Exploration for Multimodal Agents", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io."}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801", "abs": "https://arxiv.org/abs/2504.21801", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing."}
{"id": "2504.21775", "pdf": "https://arxiv.org/pdf/2504.21775", "abs": "https://arxiv.org/abs/2504.21775", "authors": ["Rongguang Ye", "Ming Tang"], "title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts."}
{"id": "2504.21411", "pdf": "https://arxiv.org/pdf/2504.21411", "abs": "https://arxiv.org/abs/2504.21411", "authors": ["Xinyi Liu", "Yujie Wang", "Shenhan Zhu", "Fangcheng Fu", "Qingshuo Liu", "Guangming Lin", "Bin Cui"], "title": "Galvatron: An Automatic Distributed System for Efficient Foundation Model Training", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Galvatron is a distributed system for efficiently training large-scale\nFoundation Models. It overcomes the complexities of selecting optimal\nparallelism strategies by automatically identifying the most efficient hybrid\nstrategy, incorporating data, tensor, pipeline, sharded data, and sequence\nparallelism, along with recomputation. The system's architecture includes a\nprofiler for hardware and model analysis, a search engine for strategy\noptimization using decision trees and dynamic programming, and a runtime for\nexecuting these strategies efficiently. Benchmarking on various clusters\ndemonstrates Galvatron's superior throughput compared to existing frameworks.\nThis open-source system offers user-friendly interfaces and comprehensive\ndocumentation, making complex distributed training accessible and efficient.\nThe source code of Galvatron is available at\nhttps://github.com/PKU-DAIR/Hetu-Galvatron."}
{"id": "2504.21562", "pdf": "https://arxiv.org/pdf/2504.21562", "abs": "https://arxiv.org/abs/2504.21562", "authors": ["Henry John Krumb", "Anirban Mukhopadhyay"], "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule."}
{"id": "2504.21851", "pdf": "https://arxiv.org/pdf/2504.21851", "abs": "https://arxiv.org/abs/2504.21851", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 4 tables", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."}
{"id": "2504.21808", "pdf": "https://arxiv.org/pdf/2504.21808", "abs": "https://arxiv.org/abs/2504.21808", "authors": ["Atieh Rahmani", "Mansoor Davoodi", "Justin M. Calabrese"], "title": "Stable Trajectory Clustering: An Efficient Split and Merge Algorithm", "categories": ["cs.LG", "cs.CG"], "comment": null, "summary": "Clustering algorithms group data points by characteristics to identify\npatterns. Over the past two decades, researchers have extended these methods to\nanalyze trajectories of humans, animals, and vehicles, studying their behavior\nand movement across applications. This paper presents whole-trajectory\nclustering and sub-trajectory clustering algorithms based on DBSCAN line\nsegment clustering, which encompasses two key events: split and merge of line\nsegments. The events are employed by object movement history and the average\nEuclidean distance between line segments. In this framework, whole-trajectory\nclustering considers entire entities' trajectories, whereas sub-trajectory\nclustering employs a sliding window model to identify similar sub-trajectories.\nMany existing trajectory clustering algorithms respond to temporary anomalies\nin data by splitting trajectories, which often obscures otherwise consistent\nclustering patterns and leads to less reliable insights. We introduce the\nstable trajectory clustering algorithm, which leverages the mean absolute\ndeviation concept to demonstrate that selective omission of transient\ndeviations not only preserves the integrity of clusters but also improves their\nstability and interpretability. We run all proposed algorithms on real\ntrajectory datasets to illustrate their effectiveness and sensitivity to\nparameter variations."}
{"id": "2504.21415", "pdf": "https://arxiv.org/pdf/2504.21415", "abs": "https://arxiv.org/abs/2504.21415", "authors": ["Yi Wang", "Chengyv Wu", "Yang Liao", "Maowei You"], "title": "Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges", "categories": ["cs.CR", "cs.AI"], "comment": "14pages, 10 figures", "summary": "User authentication is essential to ensure secure access to computer systems,\nyet traditional methods face limitations in usability, cost, and security.\nMouse dynamics authentication, based on the analysis of users' natural\ninteraction behaviors with mouse devices, offers a cost-effective,\nnon-intrusive, and adaptable solution. However, challenges remain in\ndetermining the optimal data volume, balancing accuracy and practicality, and\neffectively capturing temporal behavioral patterns. In this study, we propose a\nstatistical method using Gaussian kernel density estimate (KDE) and\nKullback-Leibler (KL) divergence to estimate the sufficient data volume for\ntraining authentication models. We introduce the Mouse Authentication Unit\n(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for\nefficient and accurate behavioral representation. Furthermore, we design the\nLocal-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet\nfor local feature extraction and GRU for modeling long-term temporal\ndependencies. Taking the Balabit and DFL datasets as examples, we significantly\nreduced the data scale, particularly by a factor of 10 for the DFL dataset,\ngreatly alleviating the training burden. Additionally, we determined the\noptimal input recognition unit length for the user authentication system on\ndifferent datasets based on the slope of Approximate Entropy. Training with\nimbalanced samples, our model achieved a successful defense AUC 98.52% for\nblind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing\nthe current sota performance."}
{"id": "2504.21598", "pdf": "https://arxiv.org/pdf/2504.21598", "abs": "https://arxiv.org/abs/2504.21598", "authors": ["Thomas L. Athey", "Shashata Sawmya", "Nir Shavit"], "title": "Cascade Detector Analysis and Application to Biomedical Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains."}
{"id": "2504.21015", "pdf": "https://arxiv.org/pdf/2504.21015", "abs": "https://arxiv.org/abs/2504.21015", "authors": ["Aarush Sinha"], "title": "Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Training effective dense retrieval models often relies on hard negative (HN)\nexamples mined from the document corpus via methods like BM25 or cross-encoders\n(CE), processes that can be computationally demanding and require full corpus\naccess. This paper introduces a different approach, an end-to-end pipeline\nwhere a Large Language Model (LLM) first generates a query from a passage, and\nthen generates a hard negative example using \\emph{only} that query text. This\ncorpus-free negative generation contrasts with standard mining techniques. We\nevaluated this \\textsc{LLM Query $\\rightarrow$ LLM HN} approach against\ntraditional \\textsc{LLM Query $\\rightarrow$ BM25 HN} and \\textsc{LLM Query\n$\\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several\nBEIR benchmark datasets. Our results show the proposed all-LLM pipeline\nachieves performance identical to both the BM25 and the computationally\nintensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.\nThis demonstrates that our corpus-free negative generation method matches the\neffectiveness of complex, corpus-dependent mining techniques, offering a\npotentially simpler and more efficient pathway for training high-performance\nretrievers without sacrificing results. We make the dataset including the\nqueries and the hard-negatives for all three methods publicly available\nhttps://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449."}
{"id": "2504.19835", "pdf": "https://arxiv.org/pdf/2504.19835", "abs": "https://arxiv.org/abs/2504.19835", "authors": ["Cornelius Hake", "Christian Friedrich"], "title": "Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This study examines the digital value chain in automotive manufacturing,\nfocusing on the identification, software flashing, customization, and\ncommissioning of electronic control units in vehicle networks. A novel\nprecedence graph design is proposed to optimize this process chain using an\nautomated scheduling algorithm that employs mixed integer linear programming\ntechniques. The results show significant improvements in key metrics. The\nalgorithm reduces the number of production stations equipped with expensive\nhardware and software to execute digital value chain processes, while\nincreasing capacity utilization through efficient scheduling and reduced idle\ntime. Task parallelization is optimized, resulting in streamlined workflows and\nincreased throughput. Compared to the traditional method, the automated\napproach has reduced preparation time by 50% and reduced scheduling activities,\nas it now takes two minutes to create the precedence graph. The flexibility of\nthe algorithm's constraints allows for vehicle-specific configurations while\nmaintaining high responsiveness, eliminating backup stations and facilitating\nthe integration of new topologies. Automated scheduling significantly\noutperforms manual methods in efficiency, functionality, and adaptability."}
{"id": "2504.21428", "pdf": "https://arxiv.org/pdf/2504.21428", "abs": "https://arxiv.org/abs/2504.21428", "authors": ["Kıvanç Şerefoğlu", "Önder Gürcan", "Reyhan Aydoğan"], "title": "UAV Marketplace Simulation Tool for BVLOS Operations", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": "3 pages, 2 figures, the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2025)", "summary": "We present a simulation tool for evaluating team formation in autonomous\nmulti-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of\nSight (BVLOS). The tool models UAV collaboration and mission execution in\ndynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt\noperations. Our tool allows researchers to integrate and compare various team\nformation strategies in a controlled environment with configurable mission\nparameters and adversarial behaviors. The log of each simulation run is stored\nin a structured way along with performance metrics so that statistical analysis\ncould be done straightforwardly. The tool is versatile for testing and\nimproving UAV coordination strategies in real-world applications."}
{"id": "2504.21614", "pdf": "https://arxiv.org/pdf/2504.21614", "abs": "https://arxiv.org/abs/2504.21614", "authors": ["Daniel Bogdoll", "Rajanikant Patnaik Ananta", "Abeyankar Giridharan", "Isabel Moore", "Gregory Stevens", "Henry X. Liu"], "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection", "categories": ["cs.CV"], "comment": null, "summary": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine"}
{"id": "2504.21035", "pdf": "https://arxiv.org/pdf/2504.21035", "abs": "https://arxiv.org/abs/2504.21035", "authors": ["Rui Xin", "Niloofar Mireshghallah", "Shuyue Stella Li", "Michael Duan", "Hyunwoo Kim", "Yejin Choi", "Yulia Tsvetkov", "Sewoong Oh", "Pang Wei Koh"], "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage."}
{"id": "2504.20185", "pdf": "https://arxiv.org/pdf/2504.20185", "abs": "https://arxiv.org/abs/2504.20185", "authors": ["Aspen Hopkins", "Sarah H. Cen", "Andrew Ilyas", "Isabella Struckman", "Luis Videgaray", "Aleksander Mądry"], "title": "AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services", "categories": ["cs.CY", "cs.LG"], "comment": "27 pages, 8 figures", "summary": "The widespread adoption of AI in recent years has led to the emergence of AI\nsupply chains: complex networks of AI actors contributing models, datasets, and\nmore to the development of AI products and services. AI supply chains have many\nimplications yet are poorly understood. In this work, we take a first step\ntoward a formal study of AI supply chains and their implications, providing two\nillustrative case studies indicating that both AI development and regulation\nare complicated in the presence of supply chains. We begin by presenting a\nbrief historical perspective on AI supply chains, discussing how their rise\nreflects a longstanding shift towards specialization and outsourcing that\nsignals the healthy growth of the AI industry. We then model AI supply chains\nas directed graphs and demonstrate the power of this abstraction by connecting\nexamples of AI issues to graph properties. Finally, we examine two case studies\nin detail, providing theoretical and empirical results in both. In the first,\nwe show that information passing (specifically, of explanations) along the AI\nsupply chains is imperfect, which can result in misunderstandings that have\nreal-world implications. In the second, we show that upstream design choices\n(e.g., by base model providers) have downstream consequences (e.g., on AI\nproducts fine-tuned on the base model). Together, our findings motivate further\nstudy of AI supply chains and their increasingly salient social, economic,\nregulatory, and technical implications."}
{"id": "2504.21646", "pdf": "https://arxiv.org/pdf/2504.21646", "abs": "https://arxiv.org/abs/2504.21646", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection", "categories": ["cs.CV"], "comment": null, "summary": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun."}
{"id": "2504.21400", "pdf": "https://arxiv.org/pdf/2504.21400", "abs": "https://arxiv.org/abs/2504.21400", "authors": ["Sugat Chaturvedi", "Rochana Chaturvedi"], "title": "Who Gets the Callback? Generative AI and Gender Bias", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms."}
{"id": "2504.21454", "pdf": "https://arxiv.org/pdf/2504.21454", "abs": "https://arxiv.org/abs/2504.21454", "authors": ["Federico Nesti", "Gianluca D'Amico", "Mauro Marinoni", "Giorgio Buttazzo"], "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to IEEE ITSC 2025", "summary": "The use of machine learning in cyber-physical systems has attracted the\ninterest of both industry and academia. However, no general solution has yet\nbeen found against the unpredictable behavior of neural networks and\nreinforcement learning agents. Nevertheless, the improvements of\nphoto-realistic simulators have paved the way towards extensive testing of\ncomplex algorithms in different virtual scenarios, which would be expensive and\ndangerous to implement in the real world.\n  This paper presents SimPRIVE, a simulation framework for physical robot\ninteraction with virtual environments, which operates as a vehicle-in-the-loop\nplatform, rendering a virtual world while operating the vehicle in the real\nworld.\n  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be\nconfigured to move its digital twin in a virtual world built with the Unreal\nEngine 5 graphic engine, which can be populated with objects, people, or other\nvehicles with programmable behavior.\n  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds\nwhile being light-weight to contain execution times and allow fast rendering.\nIts main advantage lies in the possibility of testing complex algorithms on the\nfull software and hardware stack while minimizing the risks and costs of a test\ncampaign. The framework has been validated by testing a reinforcement learning\nagent trained for obstacle avoidance on an AgileX Scout Mini rover that\nnavigates a virtual office environment where everyday objects and people are\nplaced as obstacles. The physical rover moves with no collision in an indoor\nlimited space, thanks to a LiDAR-based heuristic."}
{"id": "2504.21650", "pdf": "https://arxiv.org/pdf/2504.21650", "abs": "https://arxiv.org/abs/2504.21650", "authors": ["Haiyang Zhou", "Wangbo Yu", "Jiawen Guan", "Xinhua Cheng", "Yonghong Tian", "Li Yuan"], "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation", "categories": ["cs.CV"], "comment": "Project homepage: https://zhouhyocean.github.io/holotime/", "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications."}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578", "abs": "https://arxiv.org/abs/2504.21578", "authors": ["Kamila Barylska", "Frank Delaplace", "Anna Gogolińska", "Ewa Pańkowska"], "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules."}
{"id": "2504.21480", "pdf": "https://arxiv.org/pdf/2504.21480", "abs": "https://arxiv.org/abs/2504.21480", "authors": ["Yuchen Ding", "Hongli Peng", "Xiaoqi Li"], "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures."}
{"id": "2504.21682", "pdf": "https://arxiv.org/pdf/2504.21682", "abs": "https://arxiv.org/abs/2504.21682", "authors": ["Yan Shu", "Weichao Zeng", "Fangmin Zhao", "Zeyu Chen", "Zhenhang Li", "Xiaomeng Yang", "Yu Zhou", "Paolo Rota", "Xiang Bai", "Lianwen Jin", "Xu-Cheng Yin", "Nicu Sebe"], "title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Visual text is a crucial component in both document and scene images,\nconveying rich semantic information and attracting significant attention in the\ncomputer vision community. Beyond traditional tasks such as text detection and\nrecognition, visual text processing has witnessed rapid advancements driven by\nthe emergence of foundation models, including text image reconstruction and\ntext image manipulation. Despite significant progress, challenges remain due to\nthe unique properties that differentiate text from general objects. Effectively\ncapturing and leveraging these distinct textual characteristics is essential\nfor developing robust visual text processing models. In this survey, we present\na comprehensive, multi-perspective analysis of recent advancements in visual\ntext processing, focusing on two key questions: (1) What textual features are\nmost suitable for different visual text processing tasks? (2) How can these\ndistinctive text features be effectively incorporated into processing\nframeworks? Furthermore, we introduce VTPBench, a new benchmark that\nencompasses a broad range of visual text processing datasets. Leveraging the\nadvanced visual quality assessment capabilities of multimodal large language\nmodels (MLLMs), we propose VTPScore, a novel evaluation metric designed to\nensure fair and reliable evaluation. Our empirical study with more than 20\nspecific models reveals substantial room for improvement in the current\ntechniques. Our aim is to establish this work as a fundamental resource that\nfosters future exploration and innovation in the dynamic field of visual text\nprocessing. The relevant repository is available at\nhttps://github.com/shuyansy/Visual-Text-Processing-survey."}
{"id": "2504.21716", "pdf": "https://arxiv.org/pdf/2504.21716", "abs": "https://arxiv.org/abs/2504.21716", "authors": ["Marc Glocker", "Peter Hönig", "Matthias Hirschmanner", "Markus Vincze"], "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr."}
{"id": "2504.21135", "pdf": "https://arxiv.org/pdf/2504.21135", "abs": "https://arxiv.org/abs/2504.21135", "authors": ["Hanjing Xu", "Xiaoyuan Liu", "Alex Pothen", "Ilya Safro"], "title": "QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "The quantum approximate optimization algorithm (QAOA) is one of the promising\nvariational approaches of quantum computing to solve combinatorial optimization\nproblems. In QAOA, variational parameters need to be optimized by solving a\nseries of nonlinear, nonconvex optimization programs. In this work, we propose\na QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve\nMaximum Independent Set (MIS) problems. We prepare optimized parameters for\ngraphs of 12 and 14 vertices and use GATs to transfer their parameters to\nlarger graphs. Additionally, we design a hybrid distributed resource-aware\nalgorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller\nones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We\nintegrate our GAT-based parameter transfer approach to HyDRA-MIS and\ndemonstrate competitive results compared to KaMIS, a state-of-the-art classical\nMIS solver, on graphs with several thousands vertices."}
{"id": "2504.21489", "pdf": "https://arxiv.org/pdf/2504.21489", "abs": "https://arxiv.org/abs/2504.21489", "authors": ["Shirin Anlen", "Zuzanna Wojciak"], "title": "TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS", "categories": ["cs.CY", "cs.AI"], "comment": "33 pages", "summary": "The rise of generative AI and deceptive synthetic media threatens the global\ninformation ecosystem, especially across the Global Majority. This report from\nWITNESS highlights the limitations of current AI detection tools, which often\nunderperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policymakers, and standards bodies to design\naccountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility."}
{"id": "2504.21692", "pdf": "https://arxiv.org/pdf/2504.21692", "abs": "https://arxiv.org/abs/2504.21692", "authors": ["Zihan Zhou", "Changrui Dai", "Aibo Song", "Xiaolin Fang"], "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking."}
{"id": "2504.21751", "pdf": "https://arxiv.org/pdf/2504.21751", "abs": "https://arxiv.org/abs/2504.21751", "authors": ["Sizhe Wang", "Zhengren Wang", "Dongsheng Ma", "Yongan Yu", "Rui Ling", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks."}
{"id": "2504.21182", "pdf": "https://arxiv.org/pdf/2504.21182", "abs": "https://arxiv.org/abs/2504.21182", "authors": ["Maximilian Egger", "Rüdiger Urbanke", "Rawad Bitar"], "title": "Federated One-Shot Learning with Data Privacy and Objective-Hiding", "categories": ["cs.CR", "cs.DC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "comment": null, "summary": "Privacy in federated learning is crucial, encompassing two key aspects:\nsafeguarding the privacy of clients' data and maintaining the privacy of the\nfederator's objective from the clients. While the first aspect has been\nextensively studied, the second has received much less attention.\n  We present a novel approach that addresses both concerns simultaneously,\ndrawing inspiration from techniques in knowledge distillation and private\ninformation retrieval to provide strong information-theoretic privacy\nguarantees.\n  Traditional private function computation methods could be used here; however,\nthey are typically limited to linear or polynomial functions. To overcome these\nconstraints, our approach unfolds in three stages. In stage 0, clients perform\nthe necessary computations locally. In stage 1, these results are shared among\nthe clients, and in stage 2, the federator retrieves its desired objective\nwithout compromising the privacy of the clients' data. The crux of the method\nis a carefully designed protocol that combines secret-sharing-based multi-party\ncomputation and a graph-based private information retrieval scheme. We show\nthat our method outperforms existing tools from the literature when properly\nadapted to this setting."}
{"id": "2504.21545", "pdf": "https://arxiv.org/pdf/2504.21545", "abs": "https://arxiv.org/abs/2504.21545", "authors": ["Yangyang Li", "Guanlong Liu", "Ronghua Shang", "Licheng Jiao"], "title": "Meta knowledge assisted Evolutionary Neural Architecture Search", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness."}
{"id": "2504.21699", "pdf": "https://arxiv.org/pdf/2504.21699", "abs": "https://arxiv.org/abs/2504.21699", "authors": ["Abu Mohammed Raisuddin", "Jesper Holmblad", "Hamed Haghighi", "Yuri Poledna", "Maikol Funk Drechsler", "Valentina Donzella", "Eren Erdal Aksoy"], "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D."}
{"id": "2504.21798", "pdf": "https://arxiv.org/pdf/2504.21798", "abs": "https://arxiv.org/abs/2504.21798", "authors": ["John Yang", "Kilian Leret", "Carlos E. Jimenez", "Alexander Wettig", "Kabir Khandpur", "Yanzhe Zhang", "Binyuan Hui", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "SWE-smith: Scaling Data for Software Engineering Agents", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com."}
{"id": "2504.21199", "pdf": "https://arxiv.org/pdf/2504.21199", "abs": "https://arxiv.org/abs/2504.21199", "authors": ["Terrance Liu", "Eileen Xiao", "Pratiksha Thaker", "Adam Smith", "Zhiwei Steven Wu"], "title": "Generate-then-Verify: Reconstructing Data from Limited Published Statistics", "categories": ["stat.ML", "cs.CR", "cs.LG"], "comment": null, "summary": "We study the problem of reconstructing tabular data from aggregate\nstatistics, in which the attacker aims to identify interesting claims about the\nsensitive data that can be verified with 100% certainty given the aggregates.\nSuccessful attempts in prior work have conducted studies in settings where the\nset of published statistics is rich enough that entire datasets can be\nreconstructed with certainty. In our work, we instead focus on the regime where\nmany possible datasets match the published statistics, making it impossible to\nreconstruct the entire private dataset perfectly (i.e., when approaches in\nprior work fail). We propose the problem of partial data reconstruction, in\nwhich the goal of the adversary is to instead output a $\\textit{subset}$ of\nrows and/or columns that are $\\textit{guaranteed to be correct}$. We introduce\na novel integer programming approach that first $\\textbf{generates}$ a set of\nclaims and then $\\textbf{verifies}$ whether each claim holds for all possible\ndatasets consistent with the published aggregates. We evaluate our approach on\nthe housing-level microdata from the U.S. Decennial Census release,\ndemonstrating that privacy violations can still persist even when information\npublished about such data is relatively sparse."}
{"id": "2504.21585", "pdf": "https://arxiv.org/pdf/2504.21585", "abs": "https://arxiv.org/abs/2504.21585", "authors": ["Yingzhuo Jiang", "Wenjun Huang", "Rongdun Lin", "Chenyang Miao", "Tianfu Sun", "Yunduan Cui"], "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper tackles the challenge of learning multi-goal dexterous hand\nmanipulation tasks using model-based Reinforcement Learning. We propose\nGoal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing\nprobabilistic neural network ensembles to describe the high-dimensional\ndexterous hand dynamics and introducing an asynchronous MPC policy to meet the\ncontrol frequency requirements in real-world dexterous hand systems. Extensive\nevaluations on four simulated Shadow Hand manipulation scenarios with randomly\ngenerated goals demonstrate GC-PMPC's superior performance over\nstate-of-the-art baselines. It successfully drives a cable-driven Dexterous\nhand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn\nmanipulating a cubic die to three goal poses within approximately 80 minutes of\ninteractions, demonstrating exceptional learning efficiency and control\nperformance on a cost-effective dexterous hand platform."}
{"id": "2504.21706", "pdf": "https://arxiv.org/pdf/2504.21706", "abs": "https://arxiv.org/abs/2504.21706", "authors": ["Saber Mehdipour", "Seyed Abolghasem Mirroshandel", "Seyed Amirhossein Tabatabaei"], "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture."}
{"id": "2310.18964", "pdf": "https://arxiv.org/pdf/2310.18964", "abs": "https://arxiv.org/abs/2310.18964", "authors": ["Ahmad Nasir", "Aadish Sharma", "Kokil Jaidka", "Saifuddin Ahmed"], "title": "LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection", "categories": ["cs.CL"], "comment": "18 pages, 3 figures, 5 tables", "summary": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind."}
{"id": "2504.21209", "pdf": "https://arxiv.org/pdf/2504.21209", "abs": "https://arxiv.org/abs/2504.21209", "authors": ["Xuhang Chen", "Ihsane Olakorede", "Stefan Yu Bögli", "Wenhao Xu", "Erta Beqiri", "Xuemeng Li", "Chenyu Tang", "Zeyu Gao", "Shuo Gao", "Ari Ercole", "Peter Smielewski"], "title": "Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Artefacts compromise clinical decision-making in the use of medical time\nseries. Pulsatile waveforms offer probabilities for accurate artefact\ndetection, yet most approaches rely on supervised manners and overlook\npatient-level distribution shifts. To address these issues, we introduce a\ngeneralised label-free framework, GenClean, for real-time artefact cleaning and\nleverage an in-house dataset of 180,000 ten-second arterial blood pressure\n(ABP) samples for training. We first investigate patient-level generalisation,\ndemonstrating robust performances under both intra- and inter-patient\ndistribution shifts. We further validate its effectiveness through challenging\ncross-disease cohort experiments on the MIMIC-III database. Additionally, we\nextend our method to photoplethysmography (PPG), highlighting its applicability\nto diverse medical pulsatile signals. Finally, its integration into ICM+, a\nclinical research monitoring software, confirms the real-time feasibility of\nour framework, emphasising its practical utility in continuous physiological\nmonitoring. This work provides a foundational step toward precision medicine in\nimproving the reliability of high-resolution medical time series analysis"}
{"id": "2504.21586", "pdf": "https://arxiv.org/pdf/2504.21586", "abs": "https://arxiv.org/abs/2504.21586", "authors": ["Robin Ferede", "Till Blaha", "Erin Lucassen", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform."}
{"id": "2504.21718", "pdf": "https://arxiv.org/pdf/2504.21718", "abs": "https://arxiv.org/abs/2504.21718", "authors": ["Shiying Li", "Xingqun Qi", "Bingkun Yang", "Chen Weile", "Zezhao Tian", "Muyi Sun", "Qifeng Liu", "Man Zhang", "Zhenan Sun"], "title": "VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics."}
{"id": "2402.13517", "pdf": "https://arxiv.org/pdf/2402.13517", "abs": "https://arxiv.org/abs/2402.13517", "authors": ["Canaan Yung", "Hadi Mohaghegh Dolatabadi", "Sarah Erfani", "Christopher Leckie"], "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence\n  This version of the article has been accepted for publication, after peer\nreview (when applicable) but is not the Version of Record and does not reflect\npost-acceptance improvements, or any corrections. The Version of Record is\navailable online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this\nAccepted Version is subject to the publisher's Accepted Manuscript terms of use\nhttps://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms"}
{"id": "2504.21242", "pdf": "https://arxiv.org/pdf/2504.21242", "abs": "https://arxiv.org/abs/2504.21242", "authors": ["Samy Abdel-Ghaffar", "Isaac Galatzer-Levy", "Conor Heneghan", "Xin Liu", "Sarah Kernasovskiy", "Brennan Garrett", "Andrew Barakat", "Daniel McDuff"], "title": "Passive Measurement of Autonomic Arousal in Real-World Settings", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "The autonomic nervous system (ANS) is activated during stress, which can have\nnegative effects on cardiovascular health, sleep, the immune system, and mental\nhealth. While there are ways to quantify ANS activity in laboratories, there is\na paucity of methods that have been validated in real-world contexts. We\npresent the Fitbit Body Response Algorithm, an approach to continuous remote\nmeasurement of ANS activation through widely available remote wrist-based\nsensors. The design was validated via two experiments, a Trier Social Stress\nTest (n = 45) and ecological momentary assessments (EMA) of perceived stress\n(n=87), providing both controlled and ecologically valid test data. Model\nperformance predicting perceived stress when using all available sensor\nmodalities was consistent with expectations (accuracy=0.85) and outperformed\nmodels with access to only a subset of the signals. We discuss and address\nchallenges to sensing that arise in real world settings that do not present in\nconventional lab environments."}
{"id": "2504.21596", "pdf": "https://arxiv.org/pdf/2504.21596", "abs": "https://arxiv.org/abs/2504.21596", "authors": ["Huihui Guo", "Huilong Pi", "Yunchuan Qin", "Zhuo Tang", "Kenli Li"], "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution."}
{"id": "2504.21749", "pdf": "https://arxiv.org/pdf/2504.21749", "abs": "https://arxiv.org/abs/2504.21749", "authors": ["Leonhard Sommer", "Olaf Dünkel", "Christian Theobalt", "Adam Kortylewski"], "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space", "categories": ["cs.CV"], "comment": null, "summary": "3D morphable models (3DMMs) are a powerful tool to represent the possible\nshapes and appearances of an object category. Given a single test image, 3DMMs\ncan be used to solve various tasks, such as predicting the 3D shape, pose,\nsemantic correspondence, and instance segmentation of an object. Unfortunately,\n3DMMs are only available for very few object categories that are of particular\ninterest, like faces or human bodies, as they require a demanding 3D data\nacquisition and category-specific training process. In contrast, we introduce a\nnew method, Common3D, that learns 3DMMs of common objects in a fully\nself-supervised manner from a collection of object-centric videos. For this\npurpose, our model represents objects as a learned 3D template mesh and a\ndeformation field that is parameterized as an image-conditioned neural network.\nDifferent from prior works, Common3D represents the object appearance with\nneural features instead of RGB colors, which enables the learning of more\ngeneralizable representations through an abstraction from pixel intensities.\nImportantly, we train the appearance features using a contrastive objective by\nexploiting the correspondences defined through the deformable template mesh.\nThis leads to higher quality correspondence features compared to related works\nand a significantly improved model performance at estimating 3D object pose and\nsemantic correspondence. Common3D is the first completely self-supervised\nmethod that can solve various vision tasks in a zero-shot manner."}
{"id": "2404.19442", "pdf": "https://arxiv.org/pdf/2404.19442", "abs": "https://arxiv.org/abs/2404.19442", "authors": ["David Ifeoluwa Adelani", "A. Seza Doğruöz", "Iyanuoluwa Shode", "Anuoluwapo Aremu"], "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (findings), please cite ACL anthology\n  reference on https://aclanthology.org/2025.findings-naacl.85/", "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija."}
{"id": "2504.21243", "pdf": "https://arxiv.org/pdf/2504.21243", "abs": "https://arxiv.org/abs/2504.21243", "authors": ["Yuexin Bian", "Yuanyuan Shi"], "title": "Data-driven operator learning for energy-efficient building control", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Energy-efficient ventilation control plays a vital role in reducing building\nenergy consumption while ensuring occupant health and comfort. While\nComputational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of\nairflow for building HVAC design, their high computational cost makes them\nimpractical for practical adoption in real-time building management system. In\nthis work, we present a data-driven framework that combines the physical\naccuracy of CFD with the computational efficiency of machine learning to enable\nenergy-efficient building ventilation control. Our method jointly optimizes\nairflow supply rates and vent angles to reduce energy use and adhere to air\nquality constraints. We train a neural operator transformer to learn the\nmapping from building control actions to airflow field distributions using\nhigh-resolution CFD data. This learned operator enables a gradient-based\ncontrol framework capable of optimal decision-making. Experimental results\ndemonstrate that our approach achieves substantial energy savings compared to\nmaximum airflow rate control, rule-based control, and data-driven control based\non regional average CO2 predictions, while consistently maintaining safe indoor\nair quality. These results highlight the practicality and scalability of our\nmethod for enabling safe and energy-efficient building management."}
{"id": "2504.21634", "pdf": "https://arxiv.org/pdf/2504.21634", "abs": "https://arxiv.org/abs/2504.21634", "authors": ["Chih-Cheng Rex Yuan", "Bow-Yaw Wang"], "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Fairness auditing of AI systems can identify and quantify biases. However,\ntraditional auditing using real-world data raises security and privacy\nconcerns. It exposes auditors to security risks as they become custodians of\nsensitive information and targets for cyberattacks. Privacy risks arise even\nwithout direct breaches, as data analyses can inadvertently expose confidential\ninformation. To address these, we propose a framework that leverages\ndifferentially private synthetic data to audit the fairness of AI systems. By\napplying privacy-preserving mechanisms, it generates synthetic data that\nmirrors the statistical properties of the original dataset while ensuring\nprivacy. This method balances the goal of rigorous fairness auditing and the\nneed for strong privacy protections. Through experiments on real datasets like\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\ndata. By analyzing the alignment and discrepancies between these metrics, we\nassess the capacity of synthetic data to preserve the fairness properties of\nreal data. Our results demonstrate the framework's ability to enable meaningful\nfairness evaluations while safeguarding sensitive information, proving its\napplicability across critical and sensitive domains."}
{"id": "2504.21771", "pdf": "https://arxiv.org/pdf/2504.21771", "abs": "https://arxiv.org/abs/2504.21771", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri."}
{"id": "2405.15471", "pdf": "https://arxiv.org/pdf/2405.15471", "abs": "https://arxiv.org/abs/2405.15471", "authors": ["Emily Cheng", "Diego Doimo", "Corentin Kervadec", "Iuri Macocco", "Jade Yu", "Alessandro Laio", "Marco Baroni"], "title": "Emergence of a High-Dimensional Abstraction Phase in Language Transformers", "categories": ["cs.CL"], "comment": "Published as conference paper at ICLR 2025", "summary": "A language model (LM) is a mapping from a linguistic context to an output\ntoken. However, much remains to be known about this mapping, including how its\ngeometric properties relate to its function. We take a high-level geometric\napproach to its analysis, observing, across five pre-trained transformer-based\nLMs and three input datasets, a distinct phase characterized by high intrinsic\ndimensionality. During this phase, representations (1) correspond to the first\nfull linguistic abstraction of the input; (2) are the first to viably transfer\nto downstream tasks; (3) predict each other across different LMs. Moreover, we\nfind that an earlier onset of the phase strongly predicts better language\nmodelling performance. In short, our results suggest that a central\nhigh-dimensionality phase underlies core linguistic processing in many common\nLM architectures."}
{"id": "2504.21259", "pdf": "https://arxiv.org/pdf/2504.21259", "abs": "https://arxiv.org/abs/2504.21259", "authors": ["S. Chalavadi", "A. Pastor", "T. Leitch"], "title": "LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias", "categories": ["cs.CY", "cs.LG"], "comment": null, "summary": "Accurate imputation of race and ethnicity (R&E) is crucial for analyzing\ndisparities and informing policy. Methods like Bayesian Improved Surname\nGeocoding (BISG) are widely used but exhibit limitations, including systematic\nmisclassification biases linked to socioeconomic status. This paper introduces\nLSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks\nwith census tract geolocation information. Using a large voter dataset, we\ndemonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone\nLSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in\naccuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate\nat which non-White individuals are misclassified as White (White FPR 19.3%)\ncompared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble\nmethods incorporating XGBoost achieve the highest overall accuracy (up to\n89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone\nperformance with improved bias characteristics compared to baseline models.\nIntegrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,\nhighlighting its utility as both a standalone model and a component for\nadvanced systems. We give a caution at the end regarding the appropriate use of\nthese methods."}
{"id": "2504.21695", "pdf": "https://arxiv.org/pdf/2504.21695", "abs": "https://arxiv.org/abs/2504.21695", "authors": ["Stavrow A. Bahnam", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment."}
{"id": "2504.21789", "pdf": "https://arxiv.org/pdf/2504.21789", "abs": "https://arxiv.org/abs/2504.21789", "authors": ["Alessia Hu", "Regina Beets-Tan", "Lishan Cai", "Eduardo Pooch"], "title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Paper accepted for publication at 2025 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  Copyright 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future media", "summary": "Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification."}
{"id": "2410.07825", "pdf": "https://arxiv.org/pdf/2410.07825", "abs": "https://arxiv.org/abs/2410.07825", "authors": ["Zhipeng Chen", "Kun Zhou", "Liang Song", "Wayne Xin Zhao", "Bingning Wang", "Weipeng Chen", "Ji-Rong Wen"], "title": "Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models", "categories": ["cs.CL"], "comment": "17 Pages. Working in progress", "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET."}
{"id": "2504.21260", "pdf": "https://arxiv.org/pdf/2504.21260", "abs": "https://arxiv.org/abs/2504.21260", "authors": ["Daniel Glover", "Parikshit Pareek", "Deepjyoti Deka", "Anamika Dubey"], "title": "Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "5 pages, 7 figures, Accepted at 2025 IEEE PES General Meeting", "summary": "Learning-based approaches are increasingly leveraged to manage and coordinate\nthe operation of grid-edge resources in active power distribution networks.\nAmong these, model-based techniques stand out for their superior data\nefficiency and robustness compared to model-free methods. However, effective\nmodel learning requires a learning-based approximator for the underlying power\nflow model. This study extends existing work by introducing a data-driven power\nflow method based on Gaussian Processes (GPs) to approximate the multiphase\npower flow model, by mapping net load injections to nodal voltages. Simulation\nresults using the IEEE 123-bus and 8500-node distribution test feeders\ndemonstrate that the trained GP model can reliably predict the nonlinear power\nflow solutions with minimal training data. We also conduct a comparative\nanalysis of the training efficiency and testing performance of the proposed\nGP-based power flow approximator against a deep neural network-based\napproximator, highlighting the advantages of our data-efficient approach.\nResults over realistic operating conditions show that despite an 85% reduction\nin the training sample size (corresponding to a 92.8% improvement in training\ntime), GP models produce a 99.9% relative reduction in mean absolute error\ncompared to the baselines of deep neural networks."}
{"id": "2504.21700", "pdf": "https://arxiv.org/pdf/2504.21700", "abs": "https://arxiv.org/abs/2504.21700", "authors": ["Marco Arazzi", "Vignesh Kumar Kembu", "Antonino Nocera", "Vinod P"], "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack."}
{"id": "2504.21810", "pdf": "https://arxiv.org/pdf/2504.21810", "abs": "https://arxiv.org/abs/2504.21810", "authors": ["Franko Hrzic", "Mohammadreza Movahhedi", "Ophelie Lavoie-Gagne", "Ata Kiapour"], "title": "A simple and effective approach for body part recognition on CT scans based on projection estimation", "categories": ["cs.CV", "68T01, 65D19", "I.4.0; I.4.10; I.2.1"], "comment": "19 pages, 6 figures", "summary": "It is well known that machine learning models require a high amount of\nannotated data to obtain optimal performance. Labelling Computed Tomography\n(CT) data can be a particularly challenging task due to its volumetric nature\nand often missing and$/$or incomplete associated meta-data. Even inspecting one\nCT scan requires additional computer software, or in the case of programming\nlanguages $-$ additional programming libraries. This study proposes a simple,\nyet effective approach based on 2D X-ray-like estimation of 3D CT scans for\nbody region identification. Although body region is commonly associated with\nthe CT scan, it often describes only the focused major body region neglecting\nother anatomical regions present in the observed CT. In the proposed approach,\nestimated 2D images were utilized to identify 14 distinct body regions,\nproviding valuable information for constructing a high-quality medical dataset.\nTo evaluate the effectiveness of the proposed method, it was compared against\n2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed\nthe others, where it came on top with statistical significance and F1-Score for\nthe best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the\n0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852\n$\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three\ndifferent clinical centers and counted 15,622 CT scans (44,135 labels)."}
{"id": "2410.16658", "pdf": "https://arxiv.org/pdf/2410.16658", "abs": "https://arxiv.org/abs/2410.16658", "authors": ["Janghoon Ock", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": null, "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."}
{"id": "2504.21317", "pdf": "https://arxiv.org/pdf/2504.21317", "abs": "https://arxiv.org/abs/2504.21317", "authors": ["Jiarui Xie", "Yaoyao Fiona Zhao"], "title": "Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing", "categories": ["cs.CE", "cs.LG", "eess.SP"], "comment": "13 pages, 5 figures, 2 tables. Accepted by IDETC-CIE 2025", "summary": "The deployment of machine learning (ML)-based process monitoring systems has\nsignificantly advanced additive manufacturing (AM) by enabling real-time defect\ndetection, quality assessment, and process optimization. However, redundancy is\na critical yet often overlooked challenge in the deployment and operation of\nML-based AM process monitoring systems. Excessive redundancy leads to increased\nequipment costs, compromised model performance, and high computational\nrequirements, posing barriers to industrial adoption. However, existing\nresearch lacks a unified definition of redundancy and a systematic framework\nfor its evaluation and mitigation. This paper defines redundancy in ML-based AM\nprocess monitoring and categorizes it into sample-level, feature-level, and\nmodel-level redundancy. A comprehensive multi-level redundancy mitigation\n(MLRM) framework is proposed, incorporating advanced methods such as data\nregistration, downscaling, cross-modality knowledge transfer, and model pruning\nto systematically reduce redundancy while improving model performance. The\nframework is validated through an ML-based in-situ defect detection case study\nfor directed energy deposition (DED), demonstrating a 91% reduction in latency,\na 47% decrease in error rate, and a 99.4% reduction in storage requirements.\nAdditionally, the proposed approach lowers sensor costs and energy consumption,\nenabling a lightweight, cost-effective, and scalable monitoring system. By\ndefining redundancy and introducing a structured mitigation framework, this\nstudy establishes redundancy analysis and mitigation as a key enabler of\nefficient ML-based process monitoring in production environments."}
{"id": "2504.21719", "pdf": "https://arxiv.org/pdf/2504.21719", "abs": "https://arxiv.org/abs/2504.21719", "authors": ["Fayçal Aït Aoudia", "Jakob Hoydis", "Merlin Nimier-David", "Sebastian Cammerer", "Alexander Keller"], "title": "Sionna RT: Technical Report", "categories": ["cs.IT", "cs.AI", "eess.SP", "math.IT"], "comment": null, "summary": "Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach."}
{"id": "2504.21814", "pdf": "https://arxiv.org/pdf/2504.21814", "abs": "https://arxiv.org/abs/2504.21814", "authors": ["Yixin Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen"], "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields."}
{"id": "2501.00571", "pdf": "https://arxiv.org/pdf/2501.00571", "abs": "https://arxiv.org/abs/2501.00571", "authors": ["Chengcheng Mai", "Yuxiang Wang", "Ziyu Gong", "Hanxiang Wang", "Yihua Huang"], "title": "KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities", "categories": ["cs.CL"], "comment": "This work has been accepted by IJCAI 2025", "summary": "Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA."}
{"id": "2504.21338", "pdf": "https://arxiv.org/pdf/2504.21338", "abs": "https://arxiv.org/abs/2504.21338", "authors": ["Aoi Kato", "Kenta Kojima", "Masahiro Nomura", "Isao Ono"], "title": "A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters", "categories": ["cs.NE", "cs.LG"], "comment": "IEEE CEC 2025 (Poster)", "summary": "Black-box discrete optimization (BB-DO) problems arise in many real-world\napplications, such as neural architecture search and mathematical model\nestimation. A key challenge in BB-DO is epistasis among parameters where\nmultiple variables must be modified simultaneously to effectively improve the\nobjective function. Estimation of Distribution Algorithms (EDAs) provide a\npowerful framework for tackling BB-DO problems. In particular, an EDA\nleveraging a Variational Autoencoder (VAE) has demonstrated strong performance\non relatively low-dimensional problems with epistasis while reducing\ncomputational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,\nwhich integrate bit-flip-based local search with linkage learning, have shown\nexcellent performance on high-dimensional problems. In this study, we propose a\nnew memetic algorithm that combines VAE-based sampling with local search. The\nproposed method inherits the strengths of both VAE-based EDAs and local\nsearch-based approaches: it effectively handles high-dimensional problems with\nepistasis among parameters without incurring excessive computational overhead.\nExperiments on NK landscapes -- a challenging benchmark for BB-DO involving\nepistasis among parameters -- demonstrate that our method outperforms\nstate-of-the-art VAE-based EDA methods, as well as leading approaches such as\nP3 and DSMGA-II."}
{"id": "2504.21730", "pdf": "https://arxiv.org/pdf/2504.21730", "abs": "https://arxiv.org/abs/2504.21730", "authors": ["Ting Qiao", "Yingjia Wang", "Xing Liu", "Sixing Wu", "Jianbing Li", "Yiming Li"], "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB."}
{"id": "2504.21831", "pdf": "https://arxiv.org/pdf/2504.21831", "abs": "https://arxiv.org/abs/2504.21831", "authors": ["Anas Anwarul Haq Khan", "Utkarsh Verma", "Prateek Chanda", "Ganesh Ramakrishnan"], "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research."}
{"id": "2502.11258", "pdf": "https://arxiv.org/pdf/2502.11258", "abs": "https://arxiv.org/abs/2502.11258", "authors": ["Thanushon Sivakaran", "En-Hui Yang"], "title": "Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification", "categories": ["cs.CL"], "comment": "6 pages, 2 figures, Published to IEEE ISIT 2025", "summary": "Although large language models (LLMs) have demonstrated remarkable\ncapabilities in recent years, the potential of information theory (IT) to\nenhance LLM development remains underexplored. This paper introduces the\ninformation theoretic principle of Conditional Mutual Information (CMI) to LLM\nfine-tuning for classification tasks, exploring its promise in two main ways:\nminimizing CMI to improve a model's standalone performance and maximizing CMI\nto enhance knowledge distillation (KD) for more capable student models. To\napply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained\ndeep learning framework, which was initially developed for image\nclassification, with some modification. By minimizing CMI during LLM\nfine-tuning, we achieve superior performance gains on 6 of 8 GLUE\nclassification tasks compared to BERT. Additionally, maximizing CMI during the\nKD process results in significant performance improvements in 6 of 8 GLUE\nclassification tasks compared to DistilBERT. These findings demonstrate CMI's\nadaptability for optimizing both standalone LLMs and student models, showcasing\nits potential as a robust framework for advancing LLM fine-tuning. Our work\nbridges the gap between information theory and LLM development, offering new\ninsights for building high-performing language models."}
{"id": "2504.21419", "pdf": "https://arxiv.org/pdf/2504.21419", "abs": "https://arxiv.org/abs/2504.21419", "authors": ["Damir Filipovic", "Paul Schneider"], "title": "Kernel Density Machines", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G07, 65D05, 65D15, 65C60, 62G10, 62G20"], "comment": null, "summary": "We introduce kernel density machines (KDM), a novel density ratio estimator\nin a reproducing kernel Hilbert space setting. KDM applies to general\nprobability measures on countably generated measurable spaces without\nrestrictive assumptions on continuity, or the existence of a Lebesgue density.\nFor computational efficiency, we incorporate a low-rank approximation with\nprecisely controlled error that grants scalability to large-sample settings. We\nprovide rigorous theoretical guarantees, including asymptotic consistency, a\nfunctional central limit theorem, and finite-sample error bounds, establishing\na strong foundation for practical use. Empirical results based on simulated and\nreal data demonstrate the efficacy and precision of KDM."}
{"id": "2504.21731", "pdf": "https://arxiv.org/pdf/2504.21731", "abs": "https://arxiv.org/abs/2504.21731", "authors": ["Feiyu Lu", "Mengyu Chen", "Hsiang Hsu", "Pranav Deshpande", "Cheng Yao Wang", "Blair MacIntyre"], "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)", "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR."}
{"id": "2504.21836", "pdf": "https://arxiv.org/pdf/2504.21836", "abs": "https://arxiv.org/abs/2504.21836", "authors": ["Ipek Oztas", "Duygu Ceylan", "Aysegul Dundar"], "title": "3D Stylization via Large Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to SIGGRAPH 2025", "summary": "With the growing success of text or image guided 3D generators, users demand\nmore control over the generation process, appearance stylization being one of\nthem. Given a reference image, this requires adapting the appearance of a\ngenerated 3D asset to reflect the visual style of the reference while\nmaintaining visual consistency from multiple viewpoints. To tackle this\nproblem, we draw inspiration from the success of 2D stylization methods that\nleverage the attention mechanisms in large image generation models to capture\nand transfer visual style. In particular, we probe if large reconstruction\nmodels, commonly used in the context of 3D generation, has a similar\ncapability. We discover that the certain attention blocks in these models\ncapture the appearance specific features. By injecting features from a visual\nstyle image to such blocks, we develop a simple yet effective 3D appearance\nstylization method. Our method does not require training or test time\noptimization. Through both quantitative and qualitative evaluations, we\ndemonstrate that our approach achieves superior results in terms of 3D\nappearance stylization, significantly improving efficiency while maintaining\nhigh-quality visual outcomes."}
{"id": "2503.04785", "pdf": "https://arxiv.org/pdf/2503.04785", "abs": "https://arxiv.org/abs/2503.04785", "authors": ["José Siqueira de Cerqueira", "Kai-Kristian Kemell", "Muhammad Waseem", "Rebekah Rousi", "Nannan Xi", "Juho Hamari", "Pekka Abrahamsson"], "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications."}
{"id": "2504.21438", "pdf": "https://arxiv.org/pdf/2504.21438", "abs": "https://arxiv.org/abs/2504.21438", "authors": ["Stéphane Lhaut", "Holger Rootzén", "Johan Segers"], "title": "Wasserstein-Aitchison GAN for angular measures of multivariate extremes", "categories": ["stat.ML", "cs.LG"], "comment": "38 pages, 11 figures", "summary": "Economically responsible mitigation of multivariate extreme risks -- extreme\nrainfall in a large area, huge variations of many stock prices, widespread\nbreakdowns in transportation systems -- requires estimates of the probabilities\nthat such risks will materialize in the future. This paper develops a new\nmethod, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which\nprovides simulated values of future $d$-dimensional multivariate extreme events\nand which hence can be used to give estimates of such probabilities. The main\nhypothesis is that, after transforming the observations to the unit-Pareto\nscale, their distribution is regularly varying in the sense that the\ndistributions of their radial and angular components (with respect to the\n$L_1$-norm) converge and become asymptotically independent as the radius gets\nlarge. The method is a combination of standard extreme value analysis modeling\nof the tails of the marginal distributions with nonparametric GAN modeling of\nthe angular distribution. For the latter, the angular values are transformed to\nAitchison coordinates in a full $(d-1)$-dimensional linear space, and a\nWasserstein GAN is trained on these coordinates and used to generate new\nvalues. A reverse transformation is then applied to these values and gives\nsimulated values on the original data scale. The method shows good performance\ncompared to other existing methods in the literature, both in terms of\ncapturing the dependence structure of the extremes in the data, as well as in\ngenerating accurate new extremes of the data distribution. The comparison is\nperformed on simulated multivariate extremes from a logistic model in\ndimensions up to 50 and on a 30-dimensional financial data set."}
{"id": "2504.21846", "pdf": "https://arxiv.org/pdf/2504.21846", "abs": "https://arxiv.org/abs/2504.21846", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies."}
{"id": "2503.14258", "pdf": "https://arxiv.org/pdf/2503.14258", "abs": "https://arxiv.org/abs/2503.14258", "authors": ["Weihang Su", "Baoqing Yue", "Qingyao Ai", "Yiran Hu", "Jiaqi Li", "Changyue Wang", "Kaiyuan Zhang", "Yueyue Wu", "Yiqun Liu"], "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."}
{"id": "2504.21505", "pdf": "https://arxiv.org/pdf/2504.21505", "abs": "https://arxiv.org/abs/2504.21505", "authors": ["Jakob Benjamin Wessel", "Callum J. R. Murphy-Barltrop", "Emma S. Simpson"], "title": "A comparison of generative deep learning methods for multivariate angular simulation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "With the recent development of new geometric and angular-radial frameworks\nfor multivariate extremes, reliably simulating from angular variables in\nmoderate-to-high dimensions is of increasing importance. Empirical approaches\nhave the benefit of simplicity, and work reasonably well in low dimensions, but\nas the number of variables increases, they can lack the required flexibility\nand scalability. Classical parametric models for angular variables, such as the\nvon Mises-Fisher (vMF) distribution, provide an alternative. Exploiting\nmixtures of vMF distributions increases their flexibility, but there are cases\nwhere even this is not sufficient to capture the intricate features that can\narise in data. Owing to their flexibility, generative deep learning methods are\nable to capture complex data structures; they therefore have the potential to\nbe useful in the simulation of angular variables. In this paper, we explore a\nrange of deep learning approaches for this task, including generative\nadversarial networks, normalizing flows and flow matching. We assess their\nperformance via a range of metrics and make comparisons to the more classical\napproach of using a mixture of vMF distributions. The methods are also applied\nto a metocean data set, demonstrating their applicability to real-world,\ncomplex data structures."}
{"id": "2504.21848", "pdf": "https://arxiv.org/pdf/2504.21848", "abs": "https://arxiv.org/abs/2504.21848", "authors": ["Atoosa Kasirzadeh", "Iason Gabriel"], "title": "Characterizing AI Agents for Alignment and Governance", "categories": ["cs.CY", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals."}
{"id": "2504.21850", "pdf": "https://arxiv.org/pdf/2504.21850", "abs": "https://arxiv.org/abs/2504.21850", "authors": ["Xindi Wu", "Hee Seung Hwang", "Polina Kirichenko", "Olga Russakovsky"], "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks."}
{"id": "2503.21934", "pdf": "https://arxiv.org/pdf/2503.21934", "abs": "https://arxiv.org/abs/2503.21934", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Lyuben Baltadzhiev", "Maria Drencheva", "Kristian Minchev", "Mislav Balunović", "Nikola Jovanović", "Martin Vechev"], "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad", "categories": ["cs.CL"], "comment": null, "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly: only Gemini-2.5-Pro\nachieves a non-trivial score of 25%, while all other models achieve less than\n5%. Through detailed analysis of reasoning traces, we identify the most common\nfailure modes and find several unwanted artifacts arising from the optimization\nstrategies employed during model training. Overall, our results suggest that\ncurrent LLMs are inadequate for rigorous mathematical reasoning tasks,\nhighlighting the need for substantial improvements in reasoning and proof\ngeneration capabilities."}
{"id": "2504.21527", "pdf": "https://arxiv.org/pdf/2504.21527", "abs": "https://arxiv.org/abs/2504.21527", "authors": ["Sebastian Esche", "Martin Stoll"], "title": "Low-rank computation of the posterior mean in Multi-Output Gaussian Processes", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Gaussian processes (GP) are a versatile tool in machine learning and\ncomputational science. We here consider the case of multi-output Gaussian\nprocesses (MOGP) and present low-rank approaches for efficiently computing the\nposterior mean of a MOGP. Starting from low-rank spatio-temporal data we\nconsider a structured covariance function, assuming separability across space\nand time. This separability, in turn, gives a decomposition of the covariance\nmatrix into a Kronecker product of individual covariance matrices.\nIncorporating the typical noise term to the model then requires the solution of\na large-scale Stein equation for computing the posterior mean. For this, we\npropose efficient low-rank methods based on a combination of a LRPCG method\nwith the Sylvester equation solver KPIK adjusted for solving Stein equations.\nWe test the developed method on real world street network graphs by using graph\nfilters as covariance matrices. Moreover, we propose a degree-weighted average\ncovariance matrix, which can be employed under specific assumptions to achieve\nmore efficient convergence."}
{"id": "2504.21849", "pdf": "https://arxiv.org/pdf/2504.21849", "abs": "https://arxiv.org/abs/2504.21849", "authors": ["Justin B. Bullock", "Janet V. T. Pauketat", "Hsini Huang", "Yi-Fan Wang", "Jacy Reese Anthis"], "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure, 5 tables, accepted to Public Performance and\n  Management Review", "summary": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape."}
{"id": "2504.21853", "pdf": "https://arxiv.org/pdf/2504.21853", "abs": "https://arxiv.org/abs/2504.21853", "authors": ["Jiwen Yu", "Yiran Qin", "Haoxuan Che", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Hao Chen", "Xihui Liu"], "title": "A Survey of Interactive Generative Video", "categories": ["cs.CV"], "comment": null, "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications."}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342", "abs": "https://arxiv.org/abs/2504.10342", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."}
{"id": "2504.21602", "pdf": "https://arxiv.org/pdf/2504.21602", "abs": "https://arxiv.org/abs/2504.21602", "authors": ["Hannes Reichert", "Benjamin Serfling", "Elijah Schüssler", "Kerim Turacan", "Konrad Doll", "Bernhard Sick"], "title": "Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "In recent studies, numerous previous works emphasize the importance of\nsemantic segmentation of LiDAR data as a critical component to the development\nof driver-assistance systems and autonomous vehicles. However, many\nstate-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors\nand struggle with real-time constraints. This study introduces a novel semantic\nsegmentation framework tailored for modern high-resolution LiDAR sensors that\naddresses both accuracy and real-time processing demands. We propose a novel\nLiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban\ntraffic scenes. Furthermore, we propose a semantic segmentation method\nutilizing surface normals as strong input features. Our approach is bridging\nthe gap between cutting-edge research and practical automotive applications.\nAdditionaly, we provide a Robot Operating System (ROS2) implementation that we\noperate on our research vehicle. Our dataset and code are publicly available:\nhttps://github.com/kav-institute/SemanticLiDAR."}
{"id": "2309.16960", "pdf": "https://arxiv.org/pdf/2309.16960", "abs": "https://arxiv.org/abs/2309.16960", "authors": ["Mikihisa Yuasa", "Huy T. Tran", "Ramavarapu S. Sreenivas"], "title": "On Generating Explanations for Reinforcement Learning Policies: An Empirical Study", "categories": ["cs.AI"], "comment": null, "summary": "Understanding a \\textit{reinforcement learning} policy, which guides\nstate-to-action mappings to maximize rewards, necessitates an accompanying\nexplanation for human comprehension. In this paper, we introduce a set of\n\\textit{linear temporal logic} formulae designed to provide explanations for\npolicies, and an algorithm for searching through those formulae for the one\nthat best explains a given policy. Our focus is on explanations that elucidate\nboth the ultimate objectives accomplished by the policy and the prerequisite\nconditions it upholds throughout its execution. The effectiveness of our\nproposed approach is illustrated through a simulated game of capture-the-flag\nand a car-parking environment,"}
{"id": "2504.21855", "pdf": "https://arxiv.org/pdf/2504.21855", "abs": "https://arxiv.org/abs/2504.21855", "authors": ["Qihao Liu", "Ju He", "Qihang Yu", "Liang-Chieh Chen", "Alan Yuille"], "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction", "categories": ["cs.CV"], "comment": "Project Page: https://revision-video.github.io/", "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration."}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311", "abs": "https://arxiv.org/abs/2504.12311", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer."}
{"id": "2504.21668", "pdf": "https://arxiv.org/pdf/2504.21668", "abs": "https://arxiv.org/abs/2504.21668", "authors": ["Baolei Zhang", "Haoran Xin", "Minghong Fang", "Zhuqing Liu", "Biao Yi", "Tong Li", "Zheli Liu"], "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.IR", "cs.LG"], "comment": "Accepted by The Web Conference 2025", "summary": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."}
{"id": "2411.08165", "pdf": "https://arxiv.org/pdf/2411.08165", "abs": "https://arxiv.org/abs/2411.08165", "authors": ["Muzhi Li", "Cehao Yang", "Chengjin Xu", "Xuhui Jiang", "Yiyan Qi", "Jian Guo", "Ho-fung Leung", "Irwin King"], "title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by NAACL2025 main", "summary": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets."}
{"id": "2504.21067", "pdf": "https://arxiv.org/pdf/2504.21067", "abs": "https://arxiv.org/abs/2504.21067", "authors": ["Yuhan Xie", "Yixi Cai", "Yinqiang Zhang", "Lei Yang", "Jia Pan"], "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "This research tackles the challenge of real-time active view selection and\nuncertainty quantification on visual quality for active 3D reconstruction.\nVisual quality is a critical aspect of 3D reconstruction. Recent advancements\nsuch as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have\nnotably enhanced the image rendering quality of reconstruction models.\nNonetheless, the efficient and effective acquisition of input images for\nreconstruction-specifically, the selection of the most informative\nviewpoint-remains an open challenge, which is crucial for active\nreconstruction. Existing studies have primarily focused on evaluating geometric\ncompleteness and exploring unobserved or unknown regions, without direct\nevaluation of the visual uncertainty within the reconstruction model. To\naddress this gap, this paper introduces a probabilistic model that quantifies\nvisual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we\nformulate a criterion, Gaussian Splatting Shannon Mutual Information\n(GauSS-MI), for real-time assessment of visual mutual information from novel\nviewpoints, facilitating the selection of next best view. GauSS-MI is\nimplemented within an active reconstruction system integrated with a view and\nmotion planner. Extensive experiments across various simulated and real-world\nscenes showcase the superior visual quality and reconstruction efficiency\nperformance of the proposed system."}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428", "abs": "https://arxiv.org/abs/2504.18428", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."}
{"id": "2504.21787", "pdf": "https://arxiv.org/pdf/2504.21787", "abs": "https://arxiv.org/abs/2504.21787", "authors": ["Jaouad Mourtada"], "title": "Estimation of discrete distributions in relative entropy, and the deviations of the missing mass", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "stat.ML", "stat.TH"], "comment": "54 pages", "summary": "We study the problem of estimating a distribution over a finite alphabet from\nan i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler\ndivergence). While optimal expected risk bounds are known, high-probability\nguarantees remain less well-understood. First, we analyze the classical Laplace\n(add-$1$) estimator, obtaining matching upper and lower bounds on its\nperformance and showing its optimality among confidence-independent estimators.\nWe then characterize the minimax-optimal high-probability risk achievable by\nany estimator, which is attained via a simple confidence-dependent smoothing\ntechnique. Interestingly, the optimal non-asymptotic risk contains an\nadditional logarithmic factor over the ideal asymptotic risk. Next, motivated\nby scenarios where the alphabet exceeds the sample size, we investigate methods\nthat adapt to the sparsity of the distribution at hand. We introduce an\nestimator using data-dependent smoothing, for which we establish a\nhigh-probability risk bound depending on two effective sparsity parameters. As\npart of the analysis, we also derive a sharp high-probability upper bound on\nthe missing mass."}
{"id": "2411.12256", "pdf": "https://arxiv.org/pdf/2411.12256", "abs": "https://arxiv.org/abs/2411.12256", "authors": ["Honghua Zhang", "Benjie Wang", "Marcelo Arenas", "Guy Van den Broeck"], "title": "Restructuring Tractable Probabilistic Circuits", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Probabilistic circuits (PCs) are a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time."}
{"id": "2504.21331", "pdf": "https://arxiv.org/pdf/2504.21331", "abs": "https://arxiv.org/abs/2504.21331", "authors": ["Alfred Yan", "Muhammad Nur Talha Kilic", "Gert Nolze", "Ankit Agrawal", "Alok Choudhary", "Roberto dos Reis", "Vinayak Dravid"], "title": "Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": "33 pages, preliminary version", "summary": "The design of novel materials hinges on the understanding of\nstructure-property relationships. However, our capability to synthesize a large\nnumber of materials has outpaced the ability and speed needed to characterize\nthem. While the overall chemical constituents can be readily known during\nsynthesis, the structural evolution and characterization of newly synthesized\nsamples remains a bottleneck for the ultimate goal of high throughput\nnanomaterials discovery. Thus, scalable methods for crystal symmetry\ndetermination that can analyze a large volume of material samples within a\nshort time-frame are especially needed. Kikuchi diffraction in the SEM is a\npromising technique for this due to its sensitivity to dynamical scattering,\nwhich may provide information beyond just the seven crystal systems and\nfourteen Bravais lattices. After diffraction patterns are collected from\nmaterial samples, deep learning methods may be able to classify the space group\nsymmetries using the patterns as input, which paired with the elemental\ncomposition, would help enable the determination of the crystal structure. To\ninvestigate the feasibility of this solution, neural networks were trained to\npredict the space group type of background corrected EBSD patterns. Our\nnetworks were first trained and tested on an artificial dataset of EBSD\npatterns of 5,148 different cubic phases, created through physics-based\ndynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised\ndeep learning-based domain adaptation method, was utilized to train neural\nnetworks to make predictions for experimental EBSD patterns. We introduce a\nrelabeling scheme, which enables our models to achieve accuracy scores higher\nthan 90% on simulated and experimental data, suggesting that neural networks\nare capable of making predictions of crystal symmetry from an EBSD pattern."}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravanakumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.21795", "pdf": "https://arxiv.org/pdf/2504.21795", "abs": "https://arxiv.org/abs/2504.21795", "authors": ["Yuankang Zhao", "Matthew Engelhard"], "title": "Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "The Hawkes process (HP) is commonly used to model event sequences with\nself-reinforcing dynamics, including electronic health records (EHRs).\nTraditional HPs capture self-reinforcement via parametric impact functions that\ncan be inspected to understand how each event modulates the intensity of\nothers. Neural network-based HPs offer greater flexibility, resulting in\nimproved fit and prediction performance, but at the cost of interpretability,\nwhich is often critical in healthcare. In this work, we aim to understand and\nimprove upon this tradeoff. We propose a novel HP formulation in which impact\nfunctions are modeled by defining a flexible impact kernel, instantiated as a\nneural network, in event embedding space, which allows us to model large-scale\nevent sequences with many event types. This approach is more flexible than\ntraditional HPs yet more interpretable than other neural network approaches,\nand allows us to explicitly trade flexibility for interpretability by adding\ntransformer encoder layers to further contextualize the event embeddings.\nResults show that our method accurately recovers impact functions in\nsimulations, achieves competitive performance on MIMIC-IV procedure dataset,\nand gains clinically meaningful interpretation on XX-EHR with children\ndiagnosis dataset even without transformer layers. This suggests that our\nflexible impact kernel is often sufficient to capture self-reinforcing dynamics\nin EHRs and other data effectively, implying that interpretability can be\nmaintained without loss of performance."}
{"id": "2412.12119", "pdf": "https://arxiv.org/pdf/2412.12119", "abs": "https://arxiv.org/abs/2412.12119", "authors": ["John Schultz", "Jakub Adamek", "Matej Jusup", "Marc Lanctot", "Michael Kaisers", "Sarah Perrin", "Daniel Hennes", "Jeremy Shar", "Cannada Lewis", "Anian Ruoss", "Tom Zahavy", "Petar Veličković", "Laurel Prince", "Satinder Singh", "Eric Malmi", "Nenad Tomašev"], "title": "Mastering Board Games by External and Internal Planning with Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "70 pages, 10 figures", "summary": "Advancing planning and reasoning capabilities of Large Language Models (LLMs)\nis one of the key prerequisites towards unlocking their potential for\nperforming reliably in complex and impactful domains. In this paper, we aim to\ndemonstrate this across board games (Chess, Fischer Random / Chess960, Connect\nFour, and Hex), and we show that search-based planning can yield significant\nimprovements in LLM game-playing strength. We introduce, compare and contrast\ntwo major approaches: In external search, the model guides Monte Carlo Tree\nSearch (MCTS) rollouts and evaluations without calls to an external game\nengine, and in internal search, the model is trained to generate in-context a\nlinearized tree of search and a resulting final choice. Both build on a\nlanguage model pre-trained on relevant domain knowledge, reliably capturing the\ntransition and value functions in the respective environments, with minimal\nhallucinations. We evaluate our LLM search implementations against\ngame-specific state-of-the-art engines, showcasing substantial improvements in\nstrength over the base model, and reaching Grandmaster-level performance in\nchess while operating closer to the human search budget. Our proposed approach,\ncombining search with domain knowledge, is not specific to board games, hinting\nat more general future applications."}
{"id": "2504.21432", "pdf": "https://arxiv.org/pdf/2504.21432", "abs": "https://arxiv.org/abs/2504.21432", "authors": ["Pranav Saxena", "Nishant Raghuvanshi", "Neena Goveas"], "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.21844", "pdf": "https://arxiv.org/pdf/2504.21844", "abs": "https://arxiv.org/abs/2504.21844", "authors": ["William Sutcliffe", "Marta Calvi", "Simone Capelli", "Jonas Eschle", "Julián García Pardiñas", "Abhijit Mathad", "Azusa Uzuki", "Nicola Serra"], "title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks", "categories": ["physics.data-an", "cs.LG", "hep-ex"], "comment": "21 pages, 10 figures, 4 tables", "summary": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme."}
{"id": "2412.19723", "pdf": "https://arxiv.org/pdf/2412.19723", "abs": "https://arxiv.org/abs/2412.19723", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Chengyou Jia", "Liheng Chen", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Work in progress", "summary": "Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\nhttps://qiushisun.github.io/OS-Genesis-Home/."}
{"id": "2504.21530", "pdf": "https://arxiv.org/pdf/2504.21530", "abs": "https://arxiv.org/abs/2504.21530", "authors": ["Haifeng Huang", "Xinyi Chen", "Yilun Chen", "Hao Li", "Xiaoshen Han", "Zehan Wang", "Tai Wang", "Jiangmiao Pang", "Zhou Zhao"], "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in robotic manipulation have highlighted the potential of\nintermediate representations for improving policy generalization. In this work,\nwe explore grounding masks as an effective intermediate representation,\nbalancing two key advantages: (1) effective spatial guidance that specifies\ntarget objects and placement areas while also conveying information about\nobject shape and size, and (2) broad generalization potential driven by\nlarge-scale vision-language models pretrained on diverse grounding datasets. We\nintroduce RoboGround, a grounding-aware robotic manipulation system that\nleverages grounding masks as an intermediate representation to guide policy\nnetworks in object manipulation tasks. To further explore and enhance\ngeneralization, we propose an automated pipeline for generating large-scale,\nsimulated data with a diverse set of objects and instructions. Extensive\nexperiments show the value of our dataset and the effectiveness of grounding\nmasks as intermediate guidance, significantly enhancing the generalization\nabilities of robot policies."}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5\npoints of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost\n4 times less computing time, providing a cost-effective solution for industries\nwith limited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "1901.08125", "pdf": "https://arxiv.org/pdf/1901.08125", "abs": "https://arxiv.org/abs/1901.08125", "authors": ["Alvaro E. Ulloa Cerna", "Marios Pattichis", "David P. vanMaanen", "Linyuan Jing", "Aalpen A. Patel", "Joshua V. Stough", "Christopher M. Haggerty", "Brandon K. Fornwalt"], "title": "A Large-scale Multimodal Study for Predicting Mortality Risk Using Minimal and Low Parameter Models and Separable Risk Assessment", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The majority of biomedical studies use limited datasets that may not\ngeneralize over large heterogeneous datasets that have been collected over\nseveral decades. The current paper develops and validates several multimodal\nmodels that can predict 1-year mortality based on a massive clinical dataset.\nOur focus on predicting 1-year mortality can provide a sense of urgency to the\npatients. Using the largest dataset of its kind, the paper considers the\ndevelopment and validation of multimodal models based on 25,137,015 videos\nassociated with 699,822 echocardiography studies from 316,125 patients, and\n2,922,990 8-lead electrocardiogram (ECG) traces from 631,353 patients. Our\nmodels allow us to assess the contribution of individual factors and modalities\nto the overall risk. Our approach allows us to develop extremely low-parameter\nmodels that use optimized feature selection based on feature importance. Based\non available clinical information, we construct a family of models that are\nmade available in the DISIML package. Overall, performance ranges from an AUC\nof 0.72 with just ten parameters to an AUC of 0.89 with under 105k for the full\nmultimodal model. The proposed approach represents a modular neural network\nframework that can provide insights into global risk trends and guide therapies\nfor reducing mortality risk."}
{"id": "2501.15857", "pdf": "https://arxiv.org/pdf/2501.15857", "abs": "https://arxiv.org/abs/2501.15857", "authors": ["Yutong Yin", "Zhaoran Wang"], "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing."}
{"id": "2302.06308", "pdf": "https://arxiv.org/pdf/2302.06308", "abs": "https://arxiv.org/abs/2302.06308", "authors": ["Jan Kohút", "Michal Hradiš"], "title": "Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In many machine learning tasks, a large general dataset and a small\nspecialized dataset are available. In such situations, various domain\nadaptation methods can be used to adapt a general model to the target dataset.\nWe show that in the case of neural networks trained for handwriting recognition\nusing CTC, simple fine-tuning with data augmentation works surprisingly well in\nsuch scenarios and that it is resistant to overfitting even for very small\ntarget domain datasets. We evaluated the behavior of fine-tuning with respect\nto augmentation, training data size, and quality of the pre-trained network,\nboth in writer-dependent and writer-independent settings. On a large real-world\ndataset, fine-tuning on new writers provided an average relative CER\nimprovement of 25 % for 16 text lines and 50 % for 256 text lines."}
{"id": "2301.11564", "pdf": "https://arxiv.org/pdf/2301.11564", "abs": "https://arxiv.org/abs/2301.11564", "authors": ["Yaoxian Song", "Penglei Sun", "Piaopiao Jin", "Yi Ren", "Yu Zheng", "Zhixu Li", "Xiaowen Chu", "Yue Zhang", "Tiefeng Li", "Jason Gu"], "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.HC"], "comment": "15 pages, 8 figures, 7 tables", "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape"}
{"id": "2208.04360", "pdf": "https://arxiv.org/pdf/2208.04360", "abs": "https://arxiv.org/abs/2208.04360", "authors": ["Jingbo Zhou", "Xinjiang Lu", "Yixiong Xiao", "Jiantao Su", "Junfu Lyu", "Yanjun Ma", "Dejing Dou"], "title": "SDWPF: A Dataset for Spatial Dynamic Wind Power Forecasting Challenge at KDD Cup 2022", "categories": ["cs.LG", "eess.SP"], "comment": "This is a journal paper published in Scientific Data, titled SDWPF: A\n  Dataset for Spatial Dynamic Wind Power Forecasting over a Large Turbine\n  Array. The released sdwpf_full dataset provides more comprehensive\n  information than the dataset made available for the KDD Cup 2022", "summary": "The variability of wind power supply can present substantial challenges to\nincorporating wind power into a grid system. Thus, Wind Power Forecasting (WPF)\nhas been widely recognized as one of the most critical issues in wind power\nintegration and operation. There has been an explosion of studies on wind power\nforecasting problems in the past decades. Nevertheless, how to well handle the\nWPF problem is still challenging, since high prediction accuracy is always\ndemanded to ensure grid stability and security of supply. We present a unique\nSpatial Dynamic Wind Power Forecasting dataset: SDWPF, which includes the\nspatial distribution of wind turbines, as well as the dynamic context factors.\nWhereas, most of the existing datasets have only a small number of wind\nturbines without knowing the locations and context information of wind turbines\nat a fine-grained time scale. By contrast, SDWPF provides the wind power data\nof 134 wind turbines from a wind farm over half a year with their relative\npositions and internal statuses. We use this dataset to launch the Baidu KDD\nCup 2022 to examine the limit of current WPF solutions. The dataset is released\nat https://aistudio.baidu.com/aistudio/competition/detail/152/0/datasets."}
{"id": "2502.05439", "pdf": "https://arxiv.org/pdf/2502.05439", "abs": "https://arxiv.org/abs/2502.05439", "authors": ["Izunna Okpala", "Ashkan Golgoon", "Arjun Ravi Kannan"], "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.LG", "68T01 (Primary) 68T05, 68N99, 68T05, 68T20, 68T50, 62H30, 65C20,\n  68P20 (Secondary)", "I.2.0; I.2.1; I.2.2; I.2.6; I.2.7; I.5.1; I.6.0; I.7.1"], "comment": null, "summary": "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews with human-in-the-loop module that can effectively\ncollaborate to perform complex modeling and model risk management (MRM) tasks.\nThe modeling crew consists of a judge agent and multiple agents who perform\nspecific tasks such as exploratory data analysis, feature engineering, model\nselection/hyperparameter tuning, model training, model evaluation, and writing\ndocumentation. The MRM crew consists of a judge agent along with specialized\nagents who perform tasks such as checking compliance of modeling documentation,\nmodel replication, conceptual soundness, analysis of outcomes, and writing\ndocumentation. We demonstrate the effectiveness and robustness of modeling and\nMRM crews by presenting a series of numerical examples applied to credit card\nfraud detection, credit card approval, and portfolio credit risk modeling\ndatasets."}
{"id": "2302.06318", "pdf": "https://arxiv.org/pdf/2302.06318", "abs": "https://arxiv.org/abs/2302.06318", "authors": ["Jan Kohút", "Michal Hradiš", "Martin Kišš"], "title": "Towards Writing Style Adaptation in Handwriting Recognition", "categories": ["cs.CV"], "comment": null, "summary": "One of the challenges of handwriting recognition is to transcribe a large\nnumber of vastly different writing styles. State-of-the-art approaches do not\nexplicitly use information about the writer's style, which may be limiting\noverall accuracy due to various ambiguities. We explore models with\nwriter-dependent parameters which take the writer's identity as an additional\ninput. The proposed models can be trained on datasets with partitions likely\nwritten by a single author (e.g. single letter, diary, or chronicle). We\npropose a Writer Style Block (WSB), an adaptive instance normalization layer\nconditioned on learned embeddings of the partitions. We experimented with\nvarious placements and settings of WSB and contrastively pre-trained\nembeddings. We show that our approach outperforms a baseline with no WSB in a\nwriter-dependent scenario and that it is possible to estimate embeddings for\nnew writers. However, domain adaptation using simple fine-tuning in a\nwriter-independent setting provides superior accuracy at a similar\ncomputational cost. The proposed approach should be further investigated in\nterms of training stability and embedding regularization to overcome such a\nbaseline."}
{"id": "2405.10718", "pdf": "https://arxiv.org/pdf/2405.10718", "abs": "https://arxiv.org/abs/2405.10718", "authors": ["Sen Fang", "Chen Chen", "Lei Wang", "Ce Zheng", "Chunyu Sui", "Yapeng Tian"], "title": "SignLLM: Sign Language Production Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "website at https://signllm.github.io/", "summary": "In this paper, we propose SignLLM, a multilingual Sign Language Production\n(SLP) large language model, which includes two novel multilingual SLP modes\nMLSF and Prompt2LangGloss that allow sign language gestures generation from\nquery texts input and question-style prompts input respectively. Both modes can\nuse a new RL loss based on reinforcement learning and a new RL module named\nPriority Learning Channel. These RL components can accelerate the training by\nenhancing the model's capability to sample high-quality data. To train SignLLM,\nwe introduce Prompt2Sign, a comprehensive multilingual sign language dataset,\nwhich builds from public data, including American Sign Language (ASL) and seven\nothers. This dataset standardizes information by extracting pose information\nfrom sign language videos into a unified compressed format. We extensively\nevaluate SignLLM, demonstrating that our model achieves state-of-the-art\nperformance on SLP tasks across eight sign languages."}
{"id": "2309.01115", "pdf": "https://arxiv.org/pdf/2309.01115", "abs": "https://arxiv.org/abs/2309.01115", "authors": ["Xuanming Zhang"], "title": "Quantitative Energy Prediction based on Carbon Emission Analysis by DPR Framework", "categories": ["cs.LG"], "comment": "13 pages,14 figures", "summary": "This study proposes a novel analytical framework that integrates DBSCAN\nclustering with the Elastic Net regression model to address multifactorial\nproblems characterized by structural complexity and multicollinearity,\nexemplified by carbon emissions analysis. DBSCAN is employed for unsupervised\nlearning to objectively cluster features, while the Elastic Net is utilized for\nhigh-dimensional feature selection and complexity control. The Elastic Net is\nspecifically chosen for its ability to balance feature selection and\nregularization by combining L1 (lasso) and L2 (ridge) penalties, making it\nparticularly suited for datasets with correlated predictors. Applying this\nframework to energy consumption data from 46 industries in China (2000-2019)\nresulted in the identification of 16 categories. Emission characteristics and\ndrivers were quantitatively assessed for each category, demonstrating the\nframework's capacity to identify primary emission sources and provide\nactionable insights. This research underscores the global applicability of the\nframework for analyzing complex regional challenges, such as carbon emissions,\nand highlights its potential to identify opportunities for emission reduction."}
{"id": "2502.19915", "pdf": "https://arxiv.org/pdf/2502.19915", "abs": "https://arxiv.org/abs/2502.19915", "authors": ["Jiahui Cen", "Jianghao Lin", "Weixuan Zhong", "Dong Zhou", "Jin Chen", "Aimin Yang", "Yongmei Zhou"], "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty", "categories": ["cs.AI"], "comment": "During a careful review of our base-experiment results, we discovered\n  a possible error in the way some data were recorded. To ensure the integrity\n  and accuracy of our work, we must correct these results and revise the\n  corresponding analysis before making the manuscript publicly available", "summary": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability."}
{"id": "2303.12675", "pdf": "https://arxiv.org/pdf/2303.12675", "abs": "https://arxiv.org/abs/2303.12675", "authors": ["Zeqing Xia", "Bojun Xiong", "Zhouhui Lian"], "title": "VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2023", "summary": "Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art."}
{"id": "2410.10116", "pdf": "https://arxiv.org/pdf/2410.10116", "abs": "https://arxiv.org/abs/2410.10116", "authors": ["Fermi Ma", "Hsin-Yuan Huang"], "title": "How to Construct Random Unitaries", "categories": ["quant-ph", "cs.CC", "cs.CL", "math-ph", "math.MP"], "comment": "76 pages; added grant acknowledgments", "summary": "The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits\nthat are computationally indistinguishable from Haar-random unitaries -- has\nbeen a central open question, with significant implications for cryptography,\ncomplexity theory, and fundamental physics. In this work, we close this\nquestion by proving that PRUs exist, assuming that any quantum-secure one-way\nfunction exists. We establish this result for both (1) the standard notion of\nPRUs, which are secure against any efficient adversary that makes queries to\nthe unitary $U$, and (2) a stronger notion of PRUs, which are secure even\nagainst adversaries that can query both the unitary $U$ and its inverse\n$U^\\dagger$. In the process, we prove that any algorithm that makes queries to\na Haar-random unitary can be efficiently simulated on a quantum computer, up to\ninverse-exponential trace distance."}
{"id": "2312.02312", "pdf": "https://arxiv.org/pdf/2312.02312", "abs": "https://arxiv.org/abs/2312.02312", "authors": ["Lukas Schäfer", "Logan Jones", "Anssi Kanervisto", "Yuhan Cao", "Tabish Rashid", "Raluca Georgescu", "Dave Bignell", "Siddhartha Sen", "Andrea Treviño Gavito", "Sam Devlin"], "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Presented at the Adaptive and Learning Agents Workshop at the AAMAS\n  2025 conference", "summary": "Video games have served as useful benchmarks for the decision-making\ncommunity, but going beyond Atari games towards modern games has been\nprohibitively expensive for the vast majority of the research community. Prior\nwork in modern video games typically relied on game-specific integration to\nobtain game features and enable online training, or on existing large datasets.\nAn alternative approach is to train agents using imitation learning to play\nvideo games purely from images. However, this setting poses a fundamental\nquestion: which visual encoders obtain representations that retain information\ncritical for decision making? To answer this question, we conduct a systematic\nstudy of imitation learning with publicly available pre-trained visual encoders\ncompared to the typical task-specific end-to-end training approach in\nMinecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our\nresults show that end-to-end training can be effective with comparably\nlow-resolution images and only minutes of demonstrations, but significant\nimprovements can be gained by utilising pre-trained encoders such as DINOv2\ndepending on the game. In addition to enabling effective decision making, we\nshow that pre-trained encoders can make decision-making research in video games\nmore accessible by significantly reducing the cost of training."}
{"id": "2504.09689", "pdf": "https://arxiv.org/pdf/2504.09689", "abs": "https://arxiv.org/abs/2504.09689", "authors": ["Jiahao Qiu", "Yinghui He", "Xinzhe Juan", "Yimin Wang", "Yuhan Liu", "Zixin Yao", "Yue Wu", "Xun Jiang", "Ling Yang", "Mengdi Wang"], "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "18 pages, 8 figures", "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"}
{"id": "2305.13800", "pdf": "https://arxiv.org/pdf/2305.13800", "abs": "https://arxiv.org/abs/2305.13800", "authors": ["Haiwei Wu", "Jiantao Zhou", "Shile Zhang"], "title": "Generalizable Synthetic Image Detection via Language-guided Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "The heightened realism of AI-generated images can be attributed to the rapid\ndevelopment of synthetic models, including generative adversarial networks\n(GANs) and diffusion models (DMs). The malevolent use of synthetic images, such\nas the dissemination of fake news or the creation of fake profiles, however,\nraises significant concerns regarding the authenticity of images. Though many\nforensic algorithms have been developed for detecting synthetic images, their\nperformance, especially the generalization capability, is still far from being\nadequate to cope with the increasing number of synthetic models. In this work,\nwe propose a simple yet very effective synthetic image detection method via a\nlanguage-guided contrastive learning. Specifically, we augment the training\nimages with carefully-designed textual labels, enabling us to use a joint\nvisual-language contrastive supervision for learning a forensic feature space\nwith better generalization. It is shown that our proposed LanguAge-guided\nSynThEsis Detection (LASTED) model achieves much improved generalizability to\nunseen image generation models and delivers promising performance that far\nexceeds state-of-the-art competitors over four datasets. The code is available\nat https://github.com/HighwayWu/LASTED."}
{"id": "2411.16508", "pdf": "https://arxiv.org/pdf/2411.16508", "abs": "https://arxiv.org/abs/2411.16508", "authors": ["Ashmal Vayani", "Dinura Dissanayake", "Hasindri Watawana", "Noor Ahsan", "Nevasini Sasikumar", "Omkar Thawakar", "Henok Biadglign Ademtew", "Yahya Hmaiti", "Amandeep Kumar", "Kartik Kuckreja", "Mykola Maslych", "Wafa Al Ghallabi", "Mihail Mihaylov", "Chao Qin", "Abdelrahman M Shaker", "Mike Zhang", "Mahardika Krisna Ihsani", "Amiel Esplana", "Monil Gokani", "Shachar Mirkin", "Harsh Singh", "Ashay Srivastava", "Endre Hamerlik", "Fathinah Asma Izzati", "Fadillah Adamsyah Maani", "Sebastian Cavada", "Jenny Chim", "Rohit Gupta", "Sanjay Manjunath", "Kamila Zhumakhanova", "Feno Heriniaina Rabevohitra", "Azril Amirudin", "Muhammad Ridzuan", "Daniya Kareem", "Ketan More", "Kunyang Li", "Pramesh Shakya", "Muhammad Saad", "Amirpouya Ghasemaghaei", "Amirbek Djanibekov", "Dilshod Azizov", "Branislava Jankovic", "Naman Bhatia", "Alvaro Cabrera", "Johan Obando-Ceron", "Olympiah Otieno", "Fabian Farestam", "Muztoba Rabbani", "Sanoojan Baliah", "Santosh Sanjeev", "Abduragim Shtanchaev", "Maheen Fatima", "Thao Nguyen", "Amrin Kareem", "Toluwani Aremu", "Nathan Xavier", "Amit Bhatkal", "Hawau Toyin", "Aman Chadha", "Hisham Cholakkal", "Rao Muhammad Anwer", "Michael Felsberg", "Jorma Laaksonen", "Thamar Solorio", "Monojit Choudhury", "Ivan Laptev", "Mubarak Shah", "Salman Khan", "Fahad Khan"], "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages", "categories": ["cs.CV", "cs.CL"], "comment": "A Multilingual Multimodal cultural benchmark for 100 languages", "summary": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available."}
{"id": "2401.06898", "pdf": "https://arxiv.org/pdf/2401.06898", "abs": "https://arxiv.org/abs/2401.06898", "authors": ["Mike Heddes", "Narayan Srinivasa", "Tony Givargis", "Alexandru Nicolau"], "title": "Always-Sparse Training by Growing Connections with Guided Stochastic Exploration", "categories": ["cs.LG"], "comment": "Published at the 2025 International Joint Conference on Neural\n  Networks (IJCNN)", "summary": "The excessive computational requirements of modern artificial neural networks\n(ANNs) are posing limitations on the machines that can run them. Sparsification\nof ANNs is often motivated by time, memory and energy savings only during model\ninference, yielding no benefits during training. A growing body of work is now\nfocusing on providing the benefits of model sparsification also during\ntraining. While these methods greatly improve the training efficiency, the\ntraining algorithms yielding the most accurate models still materialize the\ndense weights, or compute dense gradients during training. We propose an\nefficient, always-sparse training algorithm with excellent scaling to larger\nand sparser models, supported by its linear time complexity with respect to the\nmodel width during training and inference. Moreover, our guided stochastic\nexploration algorithm improves over the accuracy of previous sparse training\nmethods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG,\nand ViT models, and compare it against a range of sparsification methods."}
{"id": "2504.15457", "pdf": "https://arxiv.org/pdf/2504.15457", "abs": "https://arxiv.org/abs/2504.15457", "authors": ["Paresh Chaudhary", "Yancheng Liang", "Daphne Chen", "Simon S. Du", "Natasha Jaques"], "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models", "categories": ["cs.AI"], "comment": null, "summary": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."}
{"id": "2306.16122", "pdf": "https://arxiv.org/pdf/2306.16122", "abs": "https://arxiv.org/abs/2306.16122", "authors": ["Mohammad Alkhalefi", "Georgios Leontidis", "Mingjun Zhong"], "title": "Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination Methods", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 6 figures, 12 tables, V2: fixed typos in the references", "summary": "Self-supervised learning algorithms (SSL) based on instance discrimination\nhave shown promising results, performing competitively or even outperforming\nsupervised learning counterparts in some downstream tasks. Such approaches\nemploy data augmentation to create two views of the same instance (i.e.,\npositive pairs) and encourage the model to learn good representations by\nattracting these views closer in the embedding space without collapsing to the\ntrivial solution. However, data augmentation is limited in representing\npositive pairs, and the repulsion process between the instances during\ncontrastive learning may discard important features for instances that have\nsimilar categories. To address this issue, we propose an approach to identify\nthose images with similar semantic content and treat them as positive\ninstances, thereby reducing the chance of discarding important features during\nrepresentation learning and increasing the richness of the latent\nrepresentation. Our approach is generic and could work with any self-supervised\ninstance discrimination frameworks such as MoCo and SimSiam. To evaluate our\nmethod, we run experiments on three benchmark datasets: ImageNet, STL-10 and\nCIFAR-10 with different instance discrimination SSL approaches. The\nexperimental results show that our approach consistently outperforms the\nbaseline methods across all three datasets; for instance, we improve upon the\nvanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800\nepochs. We also report results on semi-supervised learning, transfer learning\non downstream tasks, and object detection."}
{"id": "2502.12734", "pdf": "https://arxiv.org/pdf/2502.12734", "abs": "https://arxiv.org/abs/2502.12734", "authors": ["Yuanfan Li", "Zhaohan Zhang", "Chengzhengxu Li", "Chao Shen", "Xiaoming Liu"], "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 10 text perturbation strategies and 6\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches.\nCodes and dataset are available in https://github.com/Liyuuuu111/GREATER."}
{"id": "2401.13185", "pdf": "https://arxiv.org/pdf/2401.13185", "abs": "https://arxiv.org/abs/2401.13185", "authors": ["Ole-Christian Galbo Engstrøm", "Martin Holm Jensen"], "title": "Fast Partition-Based Cross-Validation With Centering and Scaling for $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$", "categories": ["cs.LG", "cs.DS"], "comment": "This version matches the published article in Journal of\n  Chemometrics, DOI: https://doi.org/10.1002/cem.70008", "summary": "We present algorithms that substantially accelerate partition-based\ncross-validation for machine learning models that require matrix products\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our\nalgorithms have applications in model selection for, for example, principal\ncomponent analysis (PCA), principal component regression (PCR), ridge\nregression (RR), ordinary least squares (OLS), and partial least squares (PLS).\nOur algorithms support all combinations of column-wise centering and scaling of\n$\\mathbf{X}$ and $\\mathbf{Y}$, and we demonstrate in our accompanying\nimplementation that this adds only a manageable, practical constant over\nefficient variants without preprocessing. We prove the correctness of our\nalgorithms under a fold-based partitioning scheme and show that the running\ntime is independent of the number of folds; that is, they have the same time\ncomplexity as that of computing $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and space complexity equivalent to storing\n$\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Importantly, unlike alternatives found in\nthe literature, we avoid data leakage due to preprocessing. We achieve these\nresults by eliminating redundant computations in the overlap between training\npartitions. Concretely, we show how to manipulate\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using\nonly samples from the validation partition to obtain the preprocessed training\npartition-wise $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our knowledge, we are the first to derive\ncorrect and efficient cross-validation algorithms for any of the $16$\ncombinations of column-wise centering and scaling, for which we also prove only\n$12$ give distinct matrix products."}
{"id": "2504.20462", "pdf": "https://arxiv.org/pdf/2504.20462", "abs": "https://arxiv.org/abs/2504.20462", "authors": ["Qi Wang", "Xiao Zhang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data", "categories": ["cs.AI"], "comment": null, "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness."}
{"id": "2308.13505", "pdf": "https://arxiv.org/pdf/2308.13505", "abs": "https://arxiv.org/abs/2308.13505", "authors": ["Jiaming Zhang", "Yutao Cui", "Gangshan Wu", "Limin Wang"], "title": "Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation", "categories": ["cs.CV"], "comment": "Accepted by TPAMI. Code and trained models are available at\n  \\url{https://github.com/MCG-NJU/JointFormer}", "summary": "Current prevailing Video Object Segmentation methods follow the pipeline of\nextraction-then-matching, which first extracts features on current and\nreference frames independently, and then performs dense matching between them.\nThis decoupled pipeline limits information propagation between frames to\nhigh-level features, hindering fine-grained details for matching. Furthermore,\nthe pixel-wise matching lacks holistic target understanding, making it prone to\ndisturbance by similar distractors. To address these issues, we propose a\nunified VOS framework, coined as JointFormer, for jointly modeling feature\nextraction, correspondence matching, and a compressed memory. The core Joint\nModeling Block leverages attention to simultaneously extract and propagate the\ntarget information from the reference frame to the current frame and a\ncompressed memory token. This joint scheme enables extensive multi-layer\npropagation beyond high-level feature space and facilitates robust\ninstance-distinctive feature learning. To incorporate the long-term and\nholistic target information, we introduce a compressed memory token with a\ncustomized online updating mechanism, which aggregates target features and\nfacilitates temporal information propagation in a frame-wise manner, enhancing\nglobal modeling consistency. Our JointFormer achieves a new state-of-the-art\nperformance on the DAVIS 2017 val/test-dev (89.7\\% and 87.6\\%) benchmarks and\nthe YouTube-VOS 2018/2019 val (87.0\\% and 87.0\\%) benchmarks, outperforming the\nexisting works. To demonstrate the generalizability of our model, it is further\nevaluated on four new benchmarks with various difficulties, including MOSE for\ncomplex scenes, VISOR for egocentric videos, VOST for complex transformations,\nand LVOS for long-term videos."}
{"id": "2502.16949", "pdf": "https://arxiv.org/pdf/2502.16949", "abs": "https://arxiv.org/abs/2502.16949", "authors": ["Md Saidul Hoque Anik", "Ariful Azad"], "title": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations", "categories": ["cs.LG", "cs.CL"], "comment": "15 pages, 9 figures, 9 tables. MLSys 2025", "summary": "Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well. An implementation of the SpTransX framework is publicly\navailable as a Python package in https://github.com/HipGraph/SpTransX."}
{"id": "2403.02241", "pdf": "https://arxiv.org/pdf/2403.02241", "abs": "https://arxiv.org/abs/2403.02241", "authors": ["Damien Teney", "Armand Nicolicioiu", "Valentin Hartmann", "Ehsan Abbasnejad"], "title": "Neural Redshift: Random Networks are not Random Functions", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Our understanding of the generalization capabilities of neural networks (NNs)\nis still incomplete. Prevailing explanations are based on implicit biases of\ngradient descent (GD) but they cannot account for the capabilities of models\nfrom gradient-free methods nor the simplicity bias recently observed in\nuntrained networks. This paper seeks other sources of generalization in NNs.\n  Findings. To understand the inductive biases provided by architectures\nindependently from GD, we examine untrained, random-weight networks. Even\nsimple MLPs show strong inductive biases: uniform sampling in weight space\nyields a very biased distribution of functions in terms of complexity. But\nunlike common wisdom, NNs do not have an inherent \"simplicity bias\". This\nproperty depends on components such as ReLUs, residual connections, and layer\nnormalizations. Alternative architectures can be built with a bias for any\nlevel of complexity. Transformers also inherit all these properties from their\nbuilding blocks.\n  Implications. We provide a fresh explanation for the success of deep learning\nindependent from gradient-based training. It points at promising avenues for\ncontrolling the solutions implemented by trained models."}
{"id": "2504.20828", "pdf": "https://arxiv.org/pdf/2504.20828", "abs": "https://arxiv.org/abs/2504.20828", "authors": ["Azam Ikram", "Xiang Li", "Sameh Elnikety", "Saurabh Bagchi"], "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."}
{"id": "2308.16082", "pdf": "https://arxiv.org/pdf/2308.16082", "abs": "https://arxiv.org/abs/2308.16082", "authors": ["Sen Fang", "Chunyu Sui", "Yanghao Zhou", "Xuedong Zhang", "Hongbin Zhong", "Yapeng Tian", "Chen Chen"], "title": "SignDiff: Diffusion Model for American Sign Language Production", "categories": ["cs.CV"], "comment": "Camera-Ready Version; Project Page at https://signdiff.github.io", "summary": "In this paper, we propose a dual-condition diffusion pre-training model named\nSignDiff that can generate human sign language speakers from a skeleton pose.\nSignDiff has a novel Frame Reinforcement Network called FR-Net, similar to\ndense human pose estimation work, which enhances the correspondence between\ntext lexical symbols and sign language dense pose frames, reduces the\noccurrence of multiple fingers in the diffusion model. In addition, we propose\na new method for American Sign Language Production (ASLP), which can generate\nASL skeletal pose videos from text input, integrating two new improved modules\nand a new loss function to improve the accuracy and quality of sign language\nskeletal posture and enhance the ability of the model to train on large-scale\ndata. We propose the first baseline for ASL production and report the scores of\n17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We evaluated our model\non the previous mainstream dataset PHOENIX14T, and the experiments achieved the\nSOTA results. In addition, our image quality far exceeds all previous results\nby 10 percentage points in terms of SSIM."}
{"id": "2502.19407", "pdf": "https://arxiv.org/pdf/2502.19407", "abs": "https://arxiv.org/abs/2502.19407", "authors": ["Hasnain Heickal", "Andrew Lan"], "title": "Learning Code-Edit Embedding to Model Student Debugging Behavior", "categories": ["cs.SE", "cs.CL"], "comment": "Published on the 26th International Conference on Artificial\n  Intelligence in Education (AIED 2025)", "summary": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior."}
{"id": "2405.10621", "pdf": "https://arxiv.org/pdf/2405.10621", "abs": "https://arxiv.org/abs/2405.10621", "authors": ["Jinchuan Zhang", "Ming Sun", "Chong Mu", "Jinhao Zhang", "Quanjiang Guo", "Ling Tian"], "title": "Historically Relevant Event Structuring for Temporal Knowledge Graph Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICDE 2025, 12 pages", "summary": "Temporal Knowledge Graph (TKG) reasoning focuses on predicting events through\nhistorical information within snapshots distributed on a timeline. Existing\nstudies mainly concentrate on two perspectives of leveraging the history of\nTKGs, including capturing evolution of each recent snapshot or correlations\namong global historical facts. Despite the achieved significant\naccomplishments, these models still fall short of I) investigating the impact\nof multi-granular interactions across recent snapshots, and II) harnessing the\nexpressive semantics of significant links accorded with queries throughout the\nentire history, particularly events exerting a profound impact on the future.\nThese inadequacies restrict representation ability to reflect historical\ndependencies and future trends thoroughly. To overcome these drawbacks, we\npropose an innovative TKG reasoning approach towards \\textbf{His}torically\n\\textbf{R}elevant \\textbf{E}vents \\textbf{S}tructuring (HisRES). Concretely,\nHisRES comprises two distinctive modules excelling in structuring historically\nrelevant events within TKGs, including a multi-granularity evolutionary encoder\nthat captures structural and temporal dependencies of the most recent\nsnapshots, and a global relevance encoder that concentrates on crucial\ncorrelations among events relevant to queries from the entire history.\nFurthermore, HisRES incorporates a self-gating mechanism for adaptively merging\nmulti-granularity recent and historically relevant structuring representations.\nExtensive experiments on four event-based benchmarks demonstrate the\nstate-of-the-art performance of HisRES and indicate the superiority and\neffectiveness of structuring historical relevance for TKG reasoning."}
{"id": "2206.14175", "pdf": "https://arxiv.org/pdf/2206.14175", "abs": "https://arxiv.org/abs/2206.14175", "authors": ["Pedro Orvalho", "Mikoláš Janota", "Vasco Manquinho"], "title": "InvAASTCluster: On Applying Invariant-Based Program Clustering to Introductory Programming Assignments", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.PL"], "comment": "31 pages, 21 Figures, 5 Tables. Accepted for publication at the\n  Journal of Systems and Software. GitHub repo:\n  https://github.com/pmorvalho/InvAASTCluster", "summary": "Due to the vast number of students enrolled in programming courses, there has\nbeen an increasing number of automated program repair techniques focused on\nintroductory programming assignments (IPAs). Typically, such techniques use\nprogram clustering to take advantage of previous correct student\nimplementations to repair a new incorrect submission. These repair techniques\nuse clustering methods since analyzing all available correct submissions to\nrepair a program is not feasible. However, conventional clustering methods rely\non program representations based on features such as abstract syntax trees\n(ASTs), syntax, control flow, and data flow.\n  This paper proposes InvAASTCluster, a novel approach for program clustering\nthat uses dynamically generated program invariants to cluster semantically\nequivalent IPAs. InvAASTCluster's program representation uses a combination of\nthe program's semantics, through its invariants, and its structure through its\nanonymized abstract syntax tree (AASTs). Invariants denote conditions that must\nremain true during program execution, while AASTs are ASTs devoid of variable\nand function names, retaining only their types. Our experiments show that the\nproposed program representation outperforms syntax-based representations when\nclustering a set of correct IPAs. Furthermore, we integrate InvAASTCluster into\na state-of-the-art clustering-based program repair tool. Our results show that\nInvAASTCluster advances the current state-of-the-art when used by\nclustering-based repair tools by repairing around 13% more students' programs,\nin a shorter amount of time."}
{"id": "2309.06129", "pdf": "https://arxiv.org/pdf/2309.06129", "abs": "https://arxiv.org/abs/2309.06129", "authors": ["Sean Anthony Byrne", "Virmarie Maquiling", "Marcus Nyström", "Enkelejda Kasneci", "Diederick C. Niehorster"], "title": "LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "32 pages, 8 figures", "summary": "Deep learning has bolstered gaze estimation techniques, but real-world\ndeployment has been impeded by inadequate training datasets. This problem is\nexacerbated by both hardware-induced variations in eye images and inherent\nbiological differences across the recorded participants, leading to both\nfeature and pixel-level variance that hinders the generalizability of models\ntrained on specific datasets. While synthetic datasets can be a solution, their\ncreation is both time and resource-intensive. To address this problem, we\npresent a framework called Light Eyes or \"LEyes\" which, unlike conventional\nphotorealistic methods, only models key image features required for video-based\neye tracking using simple light distributions. LEyes facilitates easy\nconfiguration for training neural networks across diverse gaze-estimation\ntasks. We demonstrate that models trained using LEyes are consistently on-par\nor outperform other state-of-the-art algorithms in terms of pupil and CR\nlocalization across well-known datasets. In addition, a LEyes trained model\noutperforms the industry standard eye tracker using significantly more\ncost-effective hardware. Going forward, we are confident that LEyes will\nrevolutionize synthetic data generation for gaze estimation models, and lead to\nsignificant improvements of the next generation video-based eye trackers."}
{"id": "2503.01453", "pdf": "https://arxiv.org/pdf/2503.01453", "abs": "https://arxiv.org/abs/2503.01453", "authors": ["Pankaj Choudhury", "Yogesh Aggarwal", "Prabhanjan Jadhav", "Prithwijit Guha", "Sukumar Nandi"], "title": "AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Most existing works in image caption synthesis use computation heavy deep\nneural networks and generates image descriptions in English language. This\noften restricts this important assistive tool for widespread use across\nlanguage and accessibility barriers. This work presents AC-Lite, a\ncomputationally efficient model for image captioning in low-resource Assamese\nlanguage. AC-Lite reduces computational requirements by replacing\ncomputation-heavy deep network components with lightweight alternatives. The\nAC-Lite model is designed through extensive ablation experiments with different\nimage feature extractor networks and language decoders. A combination of\nShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention\nis found to provide the best performance with minimum compute. AC-Lite was\nobserved to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs\nand 22.87M parameters."}
{"id": "2405.16386", "pdf": "https://arxiv.org/pdf/2405.16386", "abs": "https://arxiv.org/abs/2405.16386", "authors": ["Jiayu Chen", "Tian Lan", "Vaneet Aggarwal"], "title": "Variational Offline Multi-agent Skill Discovery", "categories": ["cs.LG", "cs.AI"], "comment": "This work will appear in the proceedings of IJCAI 2025", "summary": "Skills are effective temporal abstractions established for sequential\ndecision making, which enable efficient hierarchical learning for long-horizon\ntasks and facilitate multi-task learning through their transferability. Despite\nextensive research, research gaps remain in multi-agent scenarios, particularly\nfor automatically extracting subgroup coordination patterns in a multi-agent\ntask. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and\nVO-MASD-Hier, to simultaneously capture subgroup- and temporal-level\nabstractions and form multi-agent skills, which firstly solves the\naforementioned challenge. An essential algorithm component of these schemes is\na dynamic grouping function that can automatically detect latent subgroups\nbased on agent interactions in a task. Further, our method can be applied to\noffline multi-task data, and the discovered subgroup skills can be transferred\nacross relevant tasks without retraining. Empirical evaluations on StarCraft\ntasks indicate that our approach significantly outperforms existing\nhierarchical multi-agent reinforcement learning (MARL) methods. Moreover,\nskills discovered using our method can effectively reduce the learning\ndifficulty in MARL scenarios with delayed and sparse reward signals. The\ncodebase is available at https://github.com/LucasCJYSDL/VOMASD."}
{"id": "2403.09326", "pdf": "https://arxiv.org/pdf/2403.09326", "abs": "https://arxiv.org/abs/2403.09326", "authors": ["Duotun Wang", "Hengyu Meng", "Zeyu Cai", "Zhijing Shao", "Qianxi Liu", "Lin Wang", "Mingming Fan", "Xiaohang Zhan", "Zeyu Wang"], "title": "HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation", "categories": ["cs.GR", "cs.AI", "I.2.6; I.3.8"], "comment": "13 pages, 20 figures", "summary": "Current text-to-avatar methods often rely on implicit representations (e.g.,\nNeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit\nand animate in graphics software. This paper introduces a novel framework for\ngenerating stylized head avatars from text guidance, which leverages locally\nlearnable mesh deformation and 2D diffusion priors to achieve high-quality\ndigital assets for attribute-preserving manipulation. Given a template mesh,\nour method represents mesh deformation with per-face Jacobians and adaptively\nmodulates local deformation using a learnable vector field. This vector field\nenables anisotropic scaling while preserving the rotation of vertices, which\ncan better express identity and geometric details. We employ landmark- and\ncontour-based regularization terms to balance the expressiveness and\nplausibility of generated avatars from multiple views without relying on any\nspecific shape prior. Our framework can generate realistic shapes and textures\nthat can be further edited via text, while supporting seamless editing using\nthe preserved attributes from the template mesh, such as 3DMM parameters,\nblendshapes, and UV coordinates. Extensive experiments demonstrate that our\nframework can generate diverse and expressive head avatars with high-quality\nmeshes that artists can easily manipulate in graphics software, facilitating\ndownstream applications such as efficient asset creation and animation with\npreserved attributes."}
{"id": "2312.12028", "pdf": "https://arxiv.org/pdf/2312.12028", "abs": "https://arxiv.org/abs/2312.12028", "authors": ["Siamul Karim Khan", "Patrick Tinsley", "Mahsa Mitcheff", "Patrick Flynn", "Kevin W. Bowyer", "Adam Czajka"], "title": "EyePreserve: Identity-Preserving Iris Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthesis of same-identity biometric iris images, both for existing and\nnon-existing identities while preserving the identity across a wide range of\npupil sizes, is complex due to the intricate iris muscle constriction\nmechanism, requiring a precise model of iris non-linear texture deformations to\nbe embedded into the synthesis pipeline. This paper presents the first method\nof fully data-driven, identity-preserving, pupil size-varying synthesis of iris\nimages. This approach is capable of synthesizing images of irises with\ndifferent pupil sizes representing non-existing identities, as well as\nnon-linearly deforming the texture of iris images of existing subjects given\nthe segmentation mask of the target iris image. Iris recognition experiments\nsuggest that the proposed deformation model both preserves the identity when\nchanging the pupil size, and offers better similarity between same-identity\niris samples with significant differences in pupil size, compared to\nstate-of-the-art linear and non-linear (bio-mechanical-based) iris deformation\nmodels. Two immediate applications of the proposed approach are: (a) synthesis\nof, or enhancement of the existing biometric datasets for iris recognition,\nmimicking those acquired with iris sensors, and (b) helping forensic human\nexperts examine iris image pairs with significant differences in pupil\ndilation. Images considered in this work conform to selected ISO/IEC 29794-6\nquality metrics to make them applicable in biometric systems. The source codes\nand model weights are offered with this paper."}
{"id": "2504.02009", "pdf": "https://arxiv.org/pdf/2504.02009", "abs": "https://arxiv.org/abs/2504.02009", "authors": ["Zhonghang Li", "Lianghao Xia", "Xubin Ren", "Jiabin Tang", "Tianyi Chen", "Yong Xu", "Chao Huang"], "title": "Urban Computing in the Era of Large Language Models", "categories": ["cs.CY", "cs.CL"], "comment": "https://github.com/HKUDS/Awesome-LLM4Urban-Papers", "summary": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing."}
{"id": "2406.09495", "pdf": "https://arxiv.org/pdf/2406.09495", "abs": "https://arxiv.org/abs/2406.09495", "authors": ["Yujie Lin", "Dong Li", "Minglai Shao", "Guihong Wan", "Chen Zhao"], "title": "FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fairness-aware domain generalization (FairDG) has emerged as a critical\nchallenge for deploying trustworthy AI systems, particularly in scenarios\ninvolving distribution shifts. Traditional methods for addressing fairness have\nfailed in domain generalization due to their lack of consideration for\ndistribution shifts. Although disentanglement has been used to tackle FairDG,\nit is limited by its strong assumptions. To overcome these limitations, we\npropose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as\na novel approach to effectively address the FairDG issue. Specifically, we\nfirst pre-train a score-based diffusion model (SDM) and two classifiers to\nequip the model with strong generalization capabilities across different\ndomains. Then, we guide the SDM using these pre-trained classifiers to\neffectively eliminate sensitive information from the generated data. Finally,\nthe generated fair data is used to train downstream classifiers, ensuring\nrobust performance under new data distributions. Extensive experiments on three\nreal-world datasets demonstrate that FADE not only enhances fairness but also\nimproves accuracy in the presence of distribution shifts. Additionally, FADE\noutperforms existing methods in achieving the best accuracy-fairness\ntrade-offs."}
{"id": "2405.20774", "pdf": "https://arxiv.org/pdf/2405.20774", "abs": "https://arxiv.org/abs/2405.20774", "authors": ["Ruochen Jiao", "Shaoyuan Xie", "Justin Yue", "Takami Sato", "Lixu Wang", "Yixuan Wang", "Qi Alfred Chen", "Qi Zhu"], "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted paper at ICLR 2025, 31 pages, including main paper,\n  references, and appendix", "summary": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks."}
{"id": "2401.09736", "pdf": "https://arxiv.org/pdf/2401.09736", "abs": "https://arxiv.org/abs/2401.09736", "authors": ["Siyu Ren", "Junhui Hou", "Xiaodong Chen", "Hongkai Xiong", "Wenping Wang"], "title": "DDM: A Metric for Comparing 3D Shapes Using Directional Distance Fields", "categories": ["cs.CV"], "comment": "Accepted by T-PAMI", "summary": "Qualifying the discrepancy between 3D geometric models, which could be\nrepresented with either point clouds or triangle meshes, is a pivotal issue\nwith board applications. Existing methods mainly focus on directly establishing\nthe correspondence between two models and then aggregating point-wise distance\nbetween corresponding points, resulting in them being either inefficient or\nineffective. In this paper, we propose DDM, an efficient, effective, robust,\nand differentiable distance metric for 3D geometry data. Specifically, we\nconstruct DDM based on the proposed implicit representation of 3D models,\nnamely directional distance field (DDF), which defines the directional\ndistances of 3D points to a model to capture its local surface geometry. We\nthen transfer the discrepancy between two 3D geometric models as the\ndiscrepancy between their DDFs defined on an identical domain, naturally\nestablishing model correspondence. To demonstrate the advantage of our DDM, we\nexplore various distance metric-driven 3D geometric modeling tasks, including\ntemplate surface fitting, rigid registration, non-rigid registration, scene\nflow estimation and human pose optimization. Extensive experiments show that\nour DDM achieves significantly higher accuracy under all tasks. As a generic\ndistance metric, DDM has the potential to advance the field of 3D geometric\nmodeling. The source code is available at https://github.com/rsy6318/DDM."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures Updated Appendix with example model responses", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2406.16193", "pdf": "https://arxiv.org/pdf/2406.16193", "abs": "https://arxiv.org/abs/2406.16193", "authors": ["Saber Malekmohammadi", "Yaoliang Yu"], "title": "Semi-Variance Reduction for Fair Federated Learning", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Ensuring fairness in a Federated Learning (FL) system, i.e., a satisfactory\nperformance for all of the participating diverse clients, is an important and\nchallenging problem. There are multiple fair FL algorithms in the literature,\nwhich have been relatively successful in providing fairness. However, these\nalgorithms mostly emphasize on the loss functions of worst-off clients to\nimprove their performance, which often results in the suppression of\nwell-performing ones. As a consequence, they usually sacrifice the system's\noverall average performance for achieving fairness. Motivated by this and\ninspired by two well-known risk modeling methods in Finance, Mean-Variance and\nMean-Semi-Variance, we propose and study two new fair FL algorithms, Variance\nReduction (VRed) and Semi-Variance Reduction (SemiVRed). VRed encourages\nequality between clients' loss functions by penalizing their variance. In\ncontrast, SemiVRed penalizes the discrepancy of only the worst-off clients'\nloss functions from the average loss. Through extensive experiments on multiple\nvision and language datasets, we show that, SemiVRed achieves SoTA performance\nin scenarios with heterogeneous data distributions and improves both fairness\nand system overall average performance."}
{"id": "2406.01698", "pdf": "https://arxiv.org/pdf/2406.01698", "abs": "https://arxiv.org/abs/2406.01698", "authors": ["Abhimanyu Bambhaniya", "Ritik Raj", "Geonhwa Jeong", "Souvik Kundu", "Sudarshan Srinivasan", "Suvinay Subramanian", "Midhilesh Elavazhagan", "Madhu Kumar", "Tushar Krishna"], "title": "Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "comment": "19 Pages, https://github.com/abhibambhaniya/GenZ-LLM-Analyzer,\n  https://genz-llm-analyzer.streamlit.app/", "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of applications, often outperforming human experts. However, deploying\nthese gigantic models efficiently for diverse inference use cases requires\ncarefully designed hardware platforms with ample computing, memory, and network\nresources. With constant innovation in LLM serving optimizations and model\narchitecture evolving at breakneck speed, the hardware requirements to meet\nService Level Objectives (SLOs) remain an open research question.\n  To answer the question, we present an analytical tool, GenZ, to efficiently\nnavigate the relationship between diverse LLM model architectures(Dense, GQA,\nMoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding,\nquanitization), and AI platform design parameters. Our tool estimates LLM\ninference performance metrics for the given scenario. We have validated against\nreal hardware platforms running various different LLM models, achieving a max\ngeomean error of 5.82.We use GenZ to identify compute, memory capacity, memory\nbandwidth, network latency, and network bandwidth requirements across diverse\nLLM inference use cases. We also study diverse architectural choices in use\ntoday (inspired by LLM serving platforms from several vendors) to help inform\ncomputer architects designing next-generation AI hardware accelerators and\nplatforms. The trends and insights derived from GenZ can guide AI engineers\ndeploying LLMs as well as computer architects designing next-generation\nhardware accelerators and platforms. Ultimately, this work sheds light on the\nplatform design considerations for unlocking the full potential of large\nlanguage models across a spectrum of applications. The source code is available\nat https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be\ntried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on\nyour web browser."}
{"id": "2403.10635", "pdf": "https://arxiv.org/pdf/2403.10635", "abs": "https://arxiv.org/abs/2403.10635", "authors": ["Wenrui Fan", "Mohammod N. I. Suvon", "Shuo Zhou", "Xianyuan Liu", "Samer Alabed", "Venet Osmani", "Andrew J. Swift", "Chen Chen", "Haiping Lu"], "title": "MeDSLIP: Medical Dual-Stream Language-Image Pre-training with Pathology-Anatomy Semantic Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Pathology and anatomy are two essential groups of semantics in medical data.\nPathology describes what the diseases are, while anatomy explains where the\ndiseases occur. They describe diseases from different perspectives, providing\ncomplementary insights into diseases. Thus, properly understanding these\nsemantics and their relationships can enhance medical vision-language models\n(VLMs). However, pathology and anatomy semantics are usually entangled in\nmedical data, hindering VLMs from explicitly modeling these semantics and their\nrelationships. To address this challenge, we propose MeDSLIP, a novel Medical\nDual-Stream Language-Image Pre-training pipeline, to disentangle pathology and\nanatomy semantics and model the relationships between them. We introduce a\ndual-stream mechanism in MeDSLIP to explicitly disentangle medical semantics\ninto pathology-relevant and anatomy-relevant streams and align visual and\ntextual information within each stream. Furthermore, we propose an interaction\nmodeling module with prototypical contrastive learning loss and intra-image\ncontrastive learning loss to regularize the relationships between pathology and\nanatomy semantics. We apply MeDSLIP to chest X-ray analysis and conduct\ncomprehensive evaluations with four benchmark datasets: NIH CXR14, RSNA\nPneumonia, SIIM-ACR Pneumothorax, and COVIDx CXR-4. The results demonstrate\nMeDSLIP's superior generalizability and transferability across different\nscenarios. The code is available at https://github.com/Shef-AIRE/MeDSLIP, and\nthe pre-trained model is released at https://huggingface.co/pykale/MeDSLIP."}
{"id": "2408.07753", "pdf": "https://arxiv.org/pdf/2408.07753", "abs": "https://arxiv.org/abs/2408.07753", "authors": ["Ying Fan", "Jingling Li", "Adith Swaminathan", "Aditya Modi", "Ching-An Cheng"], "title": "How to Solve Contextual Goal-Oriented Problems with Offline Datasets?", "categories": ["cs.LG"], "comment": "NeurIPS 2024", "summary": "We present a novel method, Contextual goal-Oriented Data Augmentation (CODA),\nwhich uses commonly available unlabeled trajectories and context-goal pairs to\nsolve Contextual Goal-Oriented (CGO) problems. By carefully constructing an\naction-augmented MDP that is equivalent to the original MDP, CODA creates a\nfully labeled transition dataset under training contexts without additional\napproximation error. We conduct a novel theoretical analysis to demonstrate\nCODA's capability to solve CGO problems in the offline data setup. Empirical\nresults also showcase the effectiveness of CODA, which outperforms other\nbaseline methods across various context-goal relationships of CGO problem. This\napproach offers a promising direction to solving CGO problems using offline\ndatasets."}
{"id": "2407.02994", "pdf": "https://arxiv.org/pdf/2407.02994", "abs": "https://arxiv.org/abs/2407.02994", "authors": ["Irene Siragusa", "Salvatore Contino", "Massimo La Ciura", "Rosario Alicata", "Roberto Pirrone"], "title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. Moreover, the recent rising of Large Multimodal\nModels (LMM) leads to a need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding CT or MR scans. This\npaper illustrates the entire workflow for building the data set MedPix 2.0.\nStarting from the well-known multimodal data set MedPix, mainly used by\nphysicians, nurses and healthcare students for Continuing Medical Education\npurposes, a semi-automatic pipeline was developed to extract visual and textual\ndata followed by a manual curing procedure where noisy samples were removed,\nthus creating a MongoDB database. Along with the data set, we developed a GUI\naimed at navigating efficiently the MongoDB instance, and obtaining the raw\ndata that can be easily used for training and/or fine-tuning LMMs. To enforce\nthis point, we also propose a CLIP-based model trained on MedPix 2.0 for\nscanning modality and location classification tasks. MedPix 2.0 is available on\nGitHub"}
{"id": "2403.18816", "pdf": "https://arxiv.org/pdf/2403.18816", "abs": "https://arxiv.org/abs/2403.18816", "authors": ["Nikolaos Sarafianos", "Tuur Stuyck", "Xiaoyu Xiang", "Yilei Li", "Jovan Popovic", "Rakesh Ranjan"], "title": "Garment3DGen: 3D Garment Stylization and Texture Generation", "categories": ["cs.CV"], "comment": "3DV 2025. Project Page and Code:\n  https://nsarafianos.github.io/garment3dgen", "summary": "We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. We leverage the recent progress of\nimage-to-3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nCarefully designed losses allow the base mesh to freely deform towards the\ndesired target, yet preserve mesh quality and topology such that they can be\nsimulated. Finally, we generate high-fidelity texture maps that are globally\nand locally consistent and faithfully capture the input guidance, allowing us\nto render the generated 3D assets. With Garment3DGen users can generate the\nsimulation-ready 3D garment of their choice without the need of artist\nintervention. We present a plethora of quantitative and qualitative comparisons\non various assets and demonstrate that Garment3DGen unlocks key applications\nranging from sketch-to-simulated garments or interacting with the garments in\nVR. Code is publicly available."}
{"id": "2408.07841", "pdf": "https://arxiv.org/pdf/2408.07841", "abs": "https://arxiv.org/abs/2408.07841", "authors": ["Avisek Naug", "Antonio Guillen", "Ricardo Luna", "Vineet Gundecha", "Desik Rengarajan", "Sahand Ghorbanpour", "Sajad Mousavi", "Ashwin Ramesh Babu", "Dejan Markovikj", "Lekhapriya D Kashyap", "Soumyendu Sarkar"], "title": "SustainDC: Benchmarking for Sustainable Data Center Control", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted at Advances in Neural Information Processing Systems 2024\n  (NeurIPS 2024)", "summary": "Machine learning has driven an exponential increase in computational demand,\nleading to massive data centers that consume significant amounts of energy and\ncontribute to climate change. This makes sustainable data center control a\npriority. In this paper, we introduce SustainDC, a set of Python environments\nfor benchmarking multi-agent reinforcement learning (MARL) algorithms for data\ncenters (DC). SustainDC supports custom DC configurations and tasks such as\nworkload scheduling, cooling optimization, and auxiliary battery management,\nwith multiple agents managing these operations while accounting for the effects\nof each other. We evaluate various MARL algorithms on SustainDC, showing their\nperformance across diverse DC designs, locations, weather conditions, grid\ncarbon intensity, and workload requirements. Our results highlight significant\nopportunities for improvement of data center operations using MARL algorithms.\nGiven the increasing use of DC due to AI, SustainDC provides a crucial platform\nfor the development and benchmarking of advanced algorithms essential for\nachieving sustainable computing and addressing other heterogeneous real-world\nchallenges."}
{"id": "2407.05679", "pdf": "https://arxiv.org/pdf/2407.05679", "abs": "https://arxiv.org/abs/2407.05679", "authors": ["Yumeng Zhang", "Shi Gong", "Kaixin Xiong", "Xiaoqing Ye", "Xiaofan Li", "Xiao Tan", "Fan Wang", "Jizhou Huang", "Hua Wu", "Haifeng Wang"], "title": "BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "World models have attracted increasing attention in autonomous driving for\ntheir ability to forecast potential future scenarios. In this paper, we propose\nBEVWorld, a novel framework that transforms multimodal sensor inputs into a\nunified and compact Bird's Eye View (BEV) latent space for holistic environment\nmodeling. The proposed world model consists of two main components: a\nmulti-modal tokenizer and a latent BEV sequence diffusion model. The\nmulti-modal tokenizer first encodes heterogeneous sensory data, and its decoder\nreconstructs the latent BEV tokens into LiDAR and surround-view image\nobservations via ray-casting rendering in a self-supervised manner. This\nenables joint modeling and bidirectional encoding-decoding of panoramic imagery\nand point cloud data within a shared spatial representation. On top of this,\nthe latent BEV sequence diffusion model performs temporally consistent\nforecasting of future scenes, conditioned on high-level action tokens, enabling\nscene-level reasoning over time. Extensive experiments demonstrate the\neffectiveness of BEVWorld on autonomous driving benchmarks, showcasing its\ncapability in realistic future scene generation and its benefits for downstream\ntasks such as perception and motion prediction."}
{"id": "2405.20152", "pdf": "https://arxiv.org/pdf/2405.20152", "abs": "https://arxiv.org/abs/2405.20152", "authors": ["Phillip Howard", "Kathleen C. Fraser", "Anahita Bhiwandiwalla", "Svetlana Kiritchenko"], "title": "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals", "categories": ["cs.CV"], "comment": "Accepted to NAACL 2025 main track (oral)", "summary": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals."}
{"id": "2409.09894", "pdf": "https://arxiv.org/pdf/2409.09894", "abs": "https://arxiv.org/abs/2409.09894", "authors": ["Keyon Vafa", "Susan Athey", "David M. Blei"], "title": "Estimating Wage Disparities Using Foundation Models", "categories": ["cs.LG", "econ.EM", "stat.ME", "stat.ML"], "comment": null, "summary": "The rise of foundation models marks a paradigm shift in machine learning:\ninstead of training specialized models from scratch, foundation models are\nfirst trained on massive datasets before being adapted or fine-tuned to make\npredictions on smaller datasets. Initially developed for text, foundation\nmodels have also excelled at making predictions about social science data.\nHowever, while many estimation problems in the social sciences use prediction\nas an intermediate step, they ultimately require different criteria for\nsuccess. In this paper, we develop methods for fine-tuning foundation models to\nperform these estimation problems. We first characterize an omitted variable\nbias that can arise when a foundation model is only fine-tuned to maximize\npredictive accuracy. We then provide a novel set of conditions for fine-tuning\nunder which estimates derived from a foundation model are root-n-consistent.\nBased on this theory, we develop new fine-tuning algorithms that empirically\nmitigate this omitted variable bias. To demonstrate our ideas, we study gender\nwage decomposition. This is a statistical estimation problem from econometrics\nwhere the goal is to decompose the gender wage gap into components that can and\ncannot be explained by career histories of workers. Classical methods for\ndecomposing the wage gap employ simple predictive models of wages which\ncondition on coarse summaries of career history that may omit factors that are\nimportant for explaining the gap. Instead, we use a custom-built foundation\nmodel to decompose the gender wage gap, which captures a richer representation\nof career history. Using data from the Panel Study of Income Dynamics, we find\nthat career history explains more of the gender wage gap than standard\neconometric models can measure, and we identify elements of career history that\nare omitted by standard models but are important for explaining the wage gap."}
{"id": "2407.07723", "pdf": "https://arxiv.org/pdf/2407.07723", "abs": "https://arxiv.org/abs/2407.07723", "authors": ["Ziguang Li", "Chao Huang", "Xuliang Wang", "Haibo Hu", "Cole Wyeth", "Dongbo Bu", "Quan Yu", "Wen Gao", "Xingwu Liu", "Ming Li"], "title": "Lossless data compression by large models", "categories": ["cs.IT", "cs.AI", "math.IT"], "comment": "Published by Nature Machine Intelligence at\n  https://www.nature.com/articles/s42256-025-01033-7", "summary": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses."}
{"id": "2406.17774", "pdf": "https://arxiv.org/pdf/2406.17774", "abs": "https://arxiv.org/abs/2406.17774", "authors": ["Ruben Wiersma", "Julien Philip", "Miloš Hašan", "Krishna Mullia", "Fujun Luan", "Elmar Eisemann", "Valentin Deschaintre"], "title": "Uncertainty for SVBRDF Acquisition using Frequency Analysis", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://svbrdf-uncertainty.github.io", "summary": "This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view\ncaptures. Under uncontrolled illumination and unstructured viewpoints, there is\nno guarantee that the observations contain enough information to reconstruct\nthe appearance properties of a captured object. We study this ambiguity, or\nuncertainty, using entropy and accelerate the analysis by using the frequency\ndomain, rather than the domain of incoming and outgoing viewing angles. The\nresult is a method that computes a map of uncertainty over an entire object\nwithin a millisecond. We find that the frequency model allows us to recover\nSVBRDF parameters with competitive performance, that the accelerated entropy\ncomputation matches results with a physically-based path tracer, and that there\nis a positive correlation between error and uncertainty. We then show that the\nuncertainty map can be applied to improve SVBRDF acquisition using capture\nguidance, sharing information on the surface, and using a diffusion model to\ninpaint uncertain regions. Our code is available at\nhttps://github.com/rubenwiersma/svbrdf_uncertainty."}
{"id": "2409.15647", "pdf": "https://arxiv.org/pdf/2409.15647", "abs": "https://arxiv.org/abs/2409.15647", "authors": ["Ying Fan", "Yilun Du", "Kannan Ramchandran", "Kangwook Lee"], "title": "Looped Transformers for Length Generalization", "categories": ["cs.LG"], "comment": "ICLR 2025", "summary": "Recent work has shown that Transformers trained from scratch can successfully\nsolve various arithmetic and algorithmic tasks, such as adding numbers and\ncomputing parity. While these Transformers generalize well on unseen inputs of\nthe same length, they struggle with length generalization, i.e., handling\ninputs of unseen lengths. In this work, we demonstrate that looped Transformers\nwith an adaptive number of steps significantly improve length generalization.\nWe focus on tasks with a known iterative solution, involving multiple\niterations of a RASP-L operation - a length-generalizable operation that can be\nexpressed by a finite-sized Transformer. We train looped Transformers using our\nproposed learning algorithm and observe that they learn highly\nlength-generalizable solutions for various tasks."}
{"id": "2407.16557", "pdf": "https://arxiv.org/pdf/2407.16557", "abs": "https://arxiv.org/abs/2407.16557", "authors": ["Asankhaya Sharma"], "title": "Patched RTC: evaluating LLMs for diverse software development tasks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows."}
{"id": "2408.06502", "pdf": "https://arxiv.org/pdf/2408.06502", "abs": "https://arxiv.org/abs/2408.06502", "authors": ["Joshua Nathaniel Williams", "Avi Schwarzschild", "Yutong He", "J. Zico Kolter"], "title": "Prompt Recovery for Image Generation Models: A Comparative Study of Discrete Optimizers", "categories": ["cs.CV", "cs.LG"], "comment": "11 Pages, 3 Figures", "summary": "Recovering natural language prompts for image generation models, solely based\non the generated images is a difficult discrete optimization problem. In this\nwork, we present the first head-to-head comparison of recent discrete\noptimization techniques for the problem of prompt inversion. We evaluate Greedy\nCoordinate Gradients (GCG), PEZ , Random Search, AutoDAN and BLIP2's image\ncaptioner across various evaluation metrics related to the quality of inverted\nprompts and the quality of the images generated by the inverted prompts. We\nfind that focusing on the CLIP similarity between the inverted prompts and the\nground truth image acts as a poor proxy for the similarity between ground truth\nimage and the image generated by the inverted prompts. While the discrete\noptimizers effectively minimize their objectives, simply using responses from a\nwell-trained captioner often leads to generated images that more closely\nresemble those produced by the original prompts."}
{"id": "2410.00381", "pdf": "https://arxiv.org/pdf/2410.00381", "abs": "https://arxiv.org/abs/2410.00381", "authors": ["Yuhao Liu", "James Doss-Gollin", "Qiushi Dai", "Guha Balakrishnan", "Ashok Veeraraghavan"], "title": "Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 10 figures, 4 tables", "summary": "Understanding the risks posed by extreme rainfall events necessitates both\nhigh-resolution products (to assess localized hazards) and extensive historical\nrecords (to capture rare occurrences). Radar and mesonet networks provide\nkilometer-scale precipitation fields, but with limited historical records and\ngeographical coverage. Conversely, global gauge and blended products span\ndecades, yet their coarse 30-50 km grids obscure local extremes. This work\nintroduces Wasserstein Regularized Diffusion (WassDiff), a generative\ndownscaling framework that integrates diffusion modeling with a\ndistribution-matching (Wasserstein) regularizer, suppressing bias throughout\nthe entire generative denoising process. Conditioned on 55 km CPC gauge-based\nprecipitation and the 31 km ERA5 reanalysis, WassDiff generates 1 km\nprecipitation estimates that remain well-calibrated to targets across the full\nintensity range, including the extremes. Comprehensive evaluations demonstrate\nthat WassDiff outperforms existing state-of-the-art downscaling methods,\ndelivering lower reconstruction error and reduced bias. Case studies further\ndemonstrate its ability to reproduce realistic fine-scale structures and\naccurate peak intensities from extreme weather phenomena, such as tropical\nstorms and cold fronts. By unlocking decades of high-resolution rainfall\ninformation from globally available coarse records, WassDiff offers a practical\npathway toward more accurate flood-risk assessments and climate-adaptation\nplanning."}
{"id": "2407.18521", "pdf": "https://arxiv.org/pdf/2407.18521", "abs": "https://arxiv.org/abs/2407.18521", "authors": ["Asankhaya Sharma"], "title": "Patched MOA: optimizing inference for diverse software development tasks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm."}
{"id": "2409.03766", "pdf": "https://arxiv.org/pdf/2409.03766", "abs": "https://arxiv.org/abs/2409.03766", "authors": ["Reza Kakavand", "Reza Ahmadi", "Atousa Parsaei", "W. Brent Edwards", "Amin Komeili"], "title": "Comparison of Kinematics and Kinetics Between OpenCap and a Marker-Based Motion Capture System in Cycling", "categories": ["cs.CV"], "comment": null, "summary": "This study evaluates the agreement of marker-based and markerless (OpenCap)\nmotion capture systems in assessing joint kinematics and kinetics during\ncycling. Markerless systems, such as OpenCap, offer the advantage of capturing\nnatural movements without physical markers, making them more practical for\nreal-world applications. However, the agreement of OpenCap with a marker-based\nsystem, particularly in cycling, remains underexplored. Ten participants cycled\nat varying speeds and resistances while motion data were recorded using both\nsystems. Key metrics, including joint angles, moments, and joint reaction\nloads, were computed using OpenSim and compared using root mean squared error\n(RMSE) per trial across participants, Pearson correlation coefficients (r) per\ntrial across participants and repeated measures Bland-Altman to control trials\ndependency within subject. Results revealed very strong agreement (r GT 0.9)\nfor hip (flexion/extension), knee (flexion/extension), and ankle\n(dorsiflexion/plantarflexion) joint angles."}
{"id": "2410.02140", "pdf": "https://arxiv.org/pdf/2410.02140", "abs": "https://arxiv.org/abs/2410.02140", "authors": ["Xinting Huang", "Andy Yang", "Satwik Bhattamishra", "Yash Sarrof", "Andreas Krebs", "Hattie Zhou", "Preetum Nakkiran", "Michael Hahn"], "title": "A Formal Framework for Understanding Length Generalization in Transformers", "categories": ["cs.LG"], "comment": "85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025", "summary": "A major challenge for transformers is generalizing to sequences longer than\nthose observed during training. While previous works have empirically shown\nthat transformers can either succeed or fail at length generalization depending\non the task, theoretical understanding of this phenomenon remains limited. In\nthis work, we introduce a rigorous theoretical framework to analyze length\ngeneralization in causal transformers with learnable absolute positional\nencodings. In particular, we characterize those functions that are identifiable\nin the limit from sufficiently long inputs with absolute positional encodings\nunder an idealized inference scheme using a norm-based regularizer. This\nenables us to prove the possibility of length generalization for a rich family\nof problems. We experimentally validate the theory as a predictor of success\nand failure of length generalization across a range of algorithmic and formal\nlanguage tasks. Our theory not only explains a broad set of empirical\nobservations but also opens the way to provably predicting length\ngeneralization capabilities in transformers."}
{"id": "2410.07836", "pdf": "https://arxiv.org/pdf/2410.07836", "abs": "https://arxiv.org/abs/2410.07836", "authors": ["Cristian Meo", "Mircea Lica", "Zarif Ikram", "Akihiro Nakano", "Vedant Shah", "Aniket Rajiv Didolkar", "Dianbo Liu", "Anirudh Goyal", "Justin Dauwels"], "title": "Masked Generative Priors Improve World Models Sequence Modelling Capabilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."}
{"id": "2411.01624", "pdf": "https://arxiv.org/pdf/2411.01624", "abs": "https://arxiv.org/abs/2411.01624", "authors": ["Xinyu Xu", "Huazhen Liu", "Tao Zhang", "Huilin Xiong", "Wenxian Yu"], "title": "PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation", "categories": ["cs.CV"], "comment": "14 pages, 14 figures, submitted to TIP", "summary": "Semantic segmentation is an important branch of image processing and computer\nvision. With the popularity of deep learning, various convolutional neural\nnetworks have been proposed for pixel-level classification and segmentation\ntasks. In practical scenarios, however, imaging angles are often arbitrary,\nencompassing instances such as water body images from remote sensing and\ncapillary and polyp images in the medical domain, where prior orientation\ninformation is typically unavailable to guide these networks to extract more\neffective features. In this case, learning features from objects with diverse\norientation information poses a significant challenge, as the majority of\nCNN-based semantic segmentation networks lack rotation equivariance to resist\nthe disturbance from orientation information. To address this challenge, this\npaper first constructs a universal convolution-group framework aimed at more\nfully utilizing orientation information and equipping the network with rotation\nequivariance. Subsequently, we mathematically design a padding-based rotation\nequivariant convolution mode (PreCM), which is not only applicable to\nmulti-scale images and convolutional kernels but can also serve as a\nreplacement component for various types of convolutions, such as dilated\nconvolutions, transposed convolutions, and asymmetric convolution. To\nquantitatively assess the impact of image rotation in semantic segmentation\ntasks, we also propose a new evaluation metric, Rotation Difference (RD). The\nreplacement experiments related to six existing semantic segmentation networks\non three datasets show that, the average Intersection Over Union (IOU) of their\nPreCM-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%,\n8.33% compared to their original versions in terms of random angle rotation.\nAnd the average RD values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%,\n3.43% respectively."}
{"id": "2410.05807", "pdf": "https://arxiv.org/pdf/2410.05807", "abs": "https://arxiv.org/abs/2410.05807", "authors": ["Binchuan Qi", "Wei Gong", "Li Li"], "title": "Extended convexity and smoothness and their applications in deep learning", "categories": ["cs.LG", "cs.DS", "math.OC"], "comment": null, "summary": "Classical assumptions like strong convexity and Lipschitz smoothness often\nfail to capture the nature of deep learning optimization problems, which are\ntypically non-convex and non-smooth, making traditional analyses less\napplicable. This study aims to elucidate the mechanisms of non-convex\noptimization in deep learning by extending the conventional notions of strong\nconvexity and Lipschitz smoothness. By leveraging these concepts, we prove\nthat, under the established constraints, the empirical risk minimization\nproblem is equivalent to optimizing the local gradient norm and structural\nerror, which together constitute the upper and lower bounds of the empirical\nrisk. Furthermore, our analysis demonstrates that the stochastic gradient\ndescent (SGD) algorithm can effectively minimize the local gradient norm.\nAdditionally, techniques like skip connections, over-parameterization, and\nrandom parameter initialization are shown to help control the structural error.\nUltimately, we validate the core conclusions of this paper through extensive\nexperiments. Theoretical analysis and experimental results indicate that our\nfindings provide new insights into the mechanisms of non-convex optimization in\ndeep learning."}
{"id": "2410.08852", "pdf": "https://arxiv.org/pdf/2410.08852", "abs": "https://arxiv.org/abs/2410.08852", "authors": ["Michelle Zhao", "Reid Simmons", "Henny Admoni", "Aaditya Ramdas", "Andrea Bajcsy"], "title": "Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "In interactive imitation learning (IL), uncertainty quantification offers a\nway for the learner (i.e. robot) to contend with distribution shifts\nencountered during deployment by actively seeking additional feedback from an\nexpert (i.e. human) online. Prior works use mechanisms like ensemble\ndisagreement or Monte Carlo dropout to quantify when black-box IL policies are\nuncertain; however, these approaches can lead to overconfident estimates when\nfaced with deployment-time distribution shifts. Instead, we contend that we\nneed uncertainty quantification algorithms that can leverage the expert human\nfeedback received during deployment time to adapt the robot's uncertainty\nonline. To tackle this, we draw upon online conformal prediction, a\ndistribution-free method for constructing prediction intervals online given a\nstream of ground-truth labels. Human labels, however, are intermittent in the\ninteractive IL setting. Thus, from the conformal prediction side, we introduce\na novel uncertainty quantification algorithm called intermittent quantile\ntracking (IQT) that leverages a probabilistic model of intermittent labels,\nmaintains asymptotic coverage guarantees, and empirically achieves desired\ncoverage levels. From the interactive IL side, we develop ConformalDAgger, a\nnew approach wherein the robot uses prediction intervals calibrated by IQT as a\nreliable measure of deployment-time uncertainty to actively query for more\nexpert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger\nmethods in scenarios where the distribution shift is (and isn't) present\nbecause of changes in the expert's policy. We find that in simulated and\nhardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects\nhigh uncertainty when the expert shifts and increases the number of\ninterventions compared to baselines, allowing the robot to more quickly learn\nthe new behavior."}
{"id": "2411.10232", "pdf": "https://arxiv.org/pdf/2411.10232", "abs": "https://arxiv.org/abs/2411.10232", "authors": ["Xingxi Yin", "Zhi Li", "Jingfeng Zhang", "Chenglin Li", "Yin Zhang"], "title": "ColorEdit: Training-free Image-Guided Color editing with diffusion model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models, with their impressive generative\ncapabilities, have been adopted for image editing tasks, demonstrating\nremarkable efficacy. However, due to attention leakage and collision between\nthe cross-attention map of the object and the new color attribute from the text\nprompt, text-guided image editing methods may fail to change the color of an\nobject, resulting in a misalignment between the resulting image and the text\nprompt. In this paper, we conduct an in-depth analysis on the process of\ntext-guided image synthesizing and what semantic information different\ncross-attention blocks have learned. We observe that the visual representation\nof an object is determined in the up-block of the diffusion model in the early\nstage of the denoising process, and color adjustment can be achieved through\nvalue matrices alignment in the cross-attention layer. Based on our findings,\nwe propose a straightforward, yet stable, and effective image-guided method to\nmodify the color of an object without requiring any additional fine-tuning or\ntraining. Lastly, we present a benchmark dataset called COLORBENCH, the first\nbenchmark to evaluate the performance of color change methods. Extensive\nexperiments validate the effectiveness of our method in object-level color\nediting and surpass the performance of popular text-guided image editing\napproaches in both synthesized and real images."}
{"id": "2410.06399", "pdf": "https://arxiv.org/pdf/2410.06399", "abs": "https://arxiv.org/abs/2410.06399", "authors": ["Aku Kammonen", "Anamika Pandey", "Erik von Schwerin", "Raúl Tempone"], "title": "Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression", "categories": ["cs.LG"], "comment": "41 pages", "summary": "This paper presents an enhanced adaptive random Fourier features (ARFF)\ntraining algorithm for shallow neural networks, building upon the work\nintroduced in \"Adaptive Random Fourier Features with Metropolis Sampling\",\nKammonen et al., \\emph{Foundations of Data Science}, 2(3):309--332, 2020. This\nimproved method uses a particle filter-type resampling technique to stabilize\nthe training process and reduce the sensitivity to parameter choices. The\nMetropolis test can also be omitted when resampling is used, reducing the\nnumber of hyperparameters by one and reducing the computational cost per\niteration compared to the ARFF method. We present comprehensive numerical\nexperiments demonstrating the efficacy of the proposed algorithm in function\nregression tasks as a stand-alone method and as a pretraining step before\ngradient-based optimization, using the Adam optimizer. Furthermore, we apply\nthe proposed algorithm to a simple image regression problem, illustrating its\nutility in sampling frequencies for the random Fourier features (RFF) layer of\ncoordinate-based multilayer perceptrons. In this context, we use the proposed\nalgorithm to sample the parameters of the RFF layer in an automated manner."}
{"id": "2411.02788", "pdf": "https://arxiv.org/pdf/2411.02788", "abs": "https://arxiv.org/abs/2411.02788", "authors": ["Chak Lam Shek", "Kasra Torshizi", "Troi Williams", "Pratap Tokekar"], "title": "When to Localize? A Risk-Constrained Reinforcement Learning Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In a standard navigation pipeline, a robot localizes at every time step to\nlower navigational errors. However, in some scenarios, a robot needs to\nselectively localize when it is expensive to obtain observations. For example,\nan underwater robot surfacing to localize too often hinders it from searching\nfor critical items underwater, such as black boxes from crashed aircraft. On\nthe other hand, if the robot never localizes, poor state estimates cause\nfailure to find the items due to inadvertently leaving the search area or\nentering hazardous, restricted areas. Motivated by these scenarios, we\ninvestigate approaches to help a robot determine \"when to localize?\" We\nformulate this as a bi-criteria optimization problem: minimize the number of\nlocalization actions while ensuring the probability of failure (due to\ncollision or not reaching a desired goal) remains bounded. In recent work, we\nshowed how to formulate this active localization problem as a constrained\nPartially Observable Markov Decision Process (POMDP), which was solved using an\nonline POMDP solver. However, this approach is too slow and requires full\nknowledge of the robot transition and observation models. In this paper, we\npresent RiskRL, a constrained Reinforcement Learning (RL) framework that\novercomes these limitations. RiskRL uses particle filtering and recurrent Soft\nActor-Critic network to learn a policy that minimizes the number of\nlocalizations while ensuring the probability of failure constraint is met. Our\nnumerical experiments show that RiskRL learns a robust policy that leads to at\nleast a 26% increase in success rates when traversing unseen test environments."}
{"id": "2411.18159", "pdf": "https://arxiv.org/pdf/2411.18159", "abs": "https://arxiv.org/abs/2411.18159", "authors": ["Wataru Shimoda", "Naoto Inoue", "Daichi Haraguchi", "Hayato Mitani", "Seiichi Uchida", "Kota Yamaguchi"], "title": "Type-R: Automatically Retouching Typos for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Codes:\n  https://github.com/CyberAgentAILab/Type-R", "summary": "While recent text-to-image models can generate photorealistic images from\ntext prompts that reflect detailed instructions, they still face significant\nchallenges in accurately rendering words in the image. In this paper, we\npropose to retouch erroneous text renderings in the post-processing pipeline.\nOur approach, called Type-R, identifies typographical errors in the generated\nimage, erases the erroneous text, regenerates text boxes for missing words, and\nfinally corrects typos in the rendered words. Through extensive experiments, we\nshow that Type-R, in combination with the latest text-to-image models such as\nStable Diffusion or Flux, achieves the highest text rendering accuracy while\nmaintaining image quality and also outperforms text-focused generation\nbaselines in terms of balancing text accuracy and image quality."}
{"id": "2411.15403", "pdf": "https://arxiv.org/pdf/2411.15403", "abs": "https://arxiv.org/abs/2411.15403", "authors": ["Xiaoyu Gan", "Jingbo Jiang", "Jingyang Zhu", "Xiaomeng Wang", "Xizi Chen", "Chi-Ying Tsui"], "title": "Partial Knowledge Distillation for Alleviating the Inherent Inter-Class Discrepancy in Federated Learning", "categories": ["cs.LG"], "comment": "10 pages, under review", "summary": "Substantial efforts have been devoted to alleviating the impact of the\nlong-tailed class distribution in federated learning. In this work, we observe\nan interesting phenomenon that certain weak classes consistently exist even for\nclass-balanced learning. These weak classes, different from the minority\nclasses in the previous works, are inherent to data and remain fairly\nconsistent for various network structures, learning paradigms, and data\npartitioning methods. The inherent inter-class accuracy discrepancy can reach\nover 36.9% for federated learning on the FashionMNIST and CIFAR-10 datasets,\neven when the class distribution is balanced both globally and locally. In this\nstudy, we empirically analyze the potential reason for this phenomenon.\nFurthermore, a partial knowledge distillation (PKD) method is proposed to\nimprove the model's classification accuracy for weak classes. In this approach,\nknowledge transfer is initiated upon the occurrence of specific\nmisclassifications within certain weak classes. Experimental results show that\nthe accuracy of weak classes can be improved by 10.7%, reducing the inherent\ninter-class discrepancy effectively."}
{"id": "2411.05282", "pdf": "https://arxiv.org/pdf/2411.05282", "abs": "https://arxiv.org/abs/2411.05282", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Tushar Krishna"], "title": "MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": "ISCA 2025", "summary": "Quantization of foundational models (FMs) is significantly more challenging\nthan traditional DNNs due to the emergence of large magnitude values called\noutliers. Existing outlier-aware algorithm-architecture co-design techniques\neither use mixed-precision, retaining outliers at high precision but compromise\nhardware efficiency, or quantize inliers and outliers at the same precision,\nimproving hardware efficiency at the cost of accuracy. To address this mutual\nexclusivity, we propose MicroScopiQ, a novel co-design technique that leverages\npruning to complement outlier-aware quantization. MicroScopiQ retains outliers\nat higher precision while pruning a certain fraction of least important weights\nto distribute the additional outlier bits; ensuring high accuracy, aligned\nmemory and hardware efficiency. We design a high-throughput, low overhead\naccelerator architecture composed of multi-precision INT processing elements\nand a network-on-chip called ReCoN that efficiently abstracts the complexity of\nsupporting high-precision outliers. Additionally, unlike prior techniques,\nMicroScopiQ does not assume any locality of outlier weights, enabling\napplicability to a broad range of FMs. Extensive experiments across diverse\nquantization settings demonstrate that MicroScopiQ achieves state-of-the-art\nquantization accuracy, while delivering up to 3x faster inference and 2x lower\nenergy consumption compared to existing alternatives. Code is available at:\nhttps://github.com/georgia-tech-synergy-lab/MicroScopiQ-LLM-Quantization"}
{"id": "2411.19167", "pdf": "https://arxiv.org/pdf/2411.19167", "abs": "https://arxiv.org/abs/2411.19167", "authors": ["Prithviraj Banerjee", "Sindi Shkodrani", "Pierre Moulon", "Shreyas Hampali", "Shangchen Han", "Fan Zhang", "Linguang Zhang", "Jade Fountain", "Edward Miller", "Selen Basol", "Richard Newcombe", "Robert Wang", "Jakob Julian Engel", "Tomas Hodan"], "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025", "summary": "We introduce HOT3D, a publicly available dataset for egocentric hand and\nobject tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of\nrecordings that feature 19 subjects interacting with 33 diverse rigid objects.\nIn addition to simple pick-up, observe, and put-down actions, the subjects\nperform actions typical for a kitchen, office, and living room environment. The\nrecordings include multiple synchronized data streams containing egocentric\nmulti-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D\nposes of cameras, hands, and objects. The dataset is recorded with two headsets\nfrom Meta: Project Aria, which is a research prototype of AI glasses, and Quest\n3, a virtual-reality headset that has shipped millions of units. Ground-truth\nposes were obtained by a motion-capture system using small optical markers\nattached to hands and objects. Hand annotations are provided in the UmeTrack\nand MANO formats, and objects are represented by 3D meshes with PBR materials\nobtained by an in-house scanner. In our experiments, we demonstrate the\neffectiveness of multi-view egocentric data for three popular tasks: 3D hand\ntracking, model-based 6DoF object pose estimation, and 3D lifting of unknown\nin-hand objects. The evaluated multi-view methods, whose benchmarking is\nuniquely enabled by HOT3D, significantly outperform their single-view\ncounterparts."}
{"id": "2412.10354", "pdf": "https://arxiv.org/pdf/2412.10354", "abs": "https://arxiv.org/abs/2412.10354", "authors": ["Jean Kossaifi", "Nikola Kovachki", "Zongyi Li", "David Pitt", "Miguel Liu-Schiaffini", "Robert Joseph George", "Boris Bonev", "Kamyar Azizzadenesheli", "Julius Berner", "Valentin Duruisseaux", "Anima Anandkumar"], "title": "A Library for Learning Neural Operators", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers."}
{"id": "2412.08378", "pdf": "https://arxiv.org/pdf/2412.08378", "abs": "https://arxiv.org/abs/2412.08378", "authors": ["Shiding Zhu", "Wenhui Dong", "Jun Song", "Yingbo Wang", "Yanan Guo", "Bo Zheng"], "title": "FILA: Fine-Grained Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 4 figures, accepted to ICLR 2025 workshop", "summary": "Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task."}
{"id": "2412.04204", "pdf": "https://arxiv.org/pdf/2412.04204", "abs": "https://arxiv.org/abs/2412.04204", "authors": ["Valerio Marsocci", "Yuru Jia", "Georges Le Bellier", "David Kerekes", "Liang Zeng", "Sebastian Hafner", "Sebastian Gerard", "Eric Brune", "Ritu Yadav", "Ali Shibli", "Heng Fang", "Yifang Ban", "Maarten Vergauwen", "Nicolas Audebert", "Andrea Nascetti"], "title": "PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Geospatial Foundation Models (GFMs) have emerged as powerful tools for\nextracting representations from Earth observation data, but their evaluation\nremains inconsistent and narrow. Existing works often evaluate on suboptimal\ndownstream datasets and tasks, that are often too easy or too narrow, limiting\nthe usefulness of the evaluations to assess the real-world applicability of\nGFMs. Additionally, there is a distinct lack of diversity in current evaluation\nprotocols, which fail to account for the multiplicity of image resolutions,\nsensor types, and temporalities, which further complicates the assessment of\nGFM performance. In particular, most existing benchmarks are geographically\nbiased towards North America and Europe, questioning the global applicability\nof GFMs. To overcome these challenges, we introduce PANGAEA, a standardized\nevaluation protocol that covers a diverse set of datasets, tasks, resolutions,\nsensor modalities, and temporalities. It establishes a robust and widely\napplicable benchmark for GFMs. We evaluate the most popular GFMs openly\navailable on this benchmark and analyze their performance across several\ndomains. In particular, we compare these models to supervised baselines (e.g.\nUNet and vanilla ViT), and assess their effectiveness when faced with limited\nlabeled data. Our findings highlight the limitations of GFMs, under different\nscenarios, showing that they do not consistently outperform supervised models.\nPANGAEA is designed to be highly extensible, allowing for the seamless\ninclusion of new datasets, models, and tasks in future research. By releasing\nthe evaluation code and benchmark, we aim to enable other researchers to\nreplicate our experiments and build upon our work, fostering a more principled\nevaluation protocol for large pre-trained geospatial models. The code is\navailable at https://github.com/VMarsocci/pangaea-bench."}
{"id": "2412.14810", "pdf": "https://arxiv.org/pdf/2412.14810", "abs": "https://arxiv.org/abs/2412.14810", "authors": ["Camillo Maria Caruso", "Paolo Soda", "Valerio Guarrasi"], "title": "MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In healthcare, the integration of multimodal data is pivotal for developing\ncomprehensive diagnostic and predictive models. However, managing missing data\nremains a significant challenge in real-world applications. We introduce MARIA\n(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based\ndeep learning model designed to address these challenges through an\nintermediate fusion strategy. Unlike conventional approaches that depend on\nimputation, MARIA utilizes a masked self-attention mechanism, which processes\nonly the available data without generating synthetic values. This approach\nenables it to effectively handle incomplete datasets, enhancing robustness and\nminimizing biases introduced by imputation methods. We evaluated MARIA against\n10 state-of-the-art machine learning and deep learning models across 8\ndiagnostic and prognostic tasks. The results demonstrate that MARIA outperforms\nexisting methods in terms of performance and resilience to varying levels of\ndata incompleteness, underscoring its potential for critical healthcare\napplications."}
{"id": "2412.10133", "pdf": "https://arxiv.org/pdf/2412.10133", "abs": "https://arxiv.org/abs/2412.10133", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects", "categories": ["cs.SE", "cs.AI"], "comment": "PUBLISHED AT ISSTA 2025", "summary": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects."}
{"id": "2412.05538", "pdf": "https://arxiv.org/pdf/2412.05538", "abs": "https://arxiv.org/abs/2412.05538", "authors": ["Hao Cheng", "Erjia Xiao", "Jiayan Yang", "Jiahang Cao", "Qiang Zhang", "Jize Zhang", "Kaidi Xu", "Jindong Gu", "Renjing Xu"], "title": "Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models", "categories": ["cs.CV", "cs.PF"], "comment": "This paper is accept by CVPR2025\n  (https://cvpr.thecvf.com/virtual/2025/poster/34964)", "summary": "Current image generation models can effortlessly produce high-quality, highly\nrealistic images, but this also increases the risk of misuse. In various\nText-to-Image or Image-to-Image tasks, attackers can generate a series of\nimages containing inappropriate content by simply editing the language modality\ninput. To mitigate this security concern, numerous guarding or defensive\nstrategies have been proposed, with a particular emphasis on safeguarding\nlanguage modality. However, in practical applications, threats in the vision\nmodality, particularly in tasks involving the editing of real-world images,\npresent heightened security risks as they can easily infringe upon the rights\nof the image owner. Therefore, this paper employs a method named typographic\nattack to reveal that various image generation models are also susceptible to\nthreats within the vision modality. Furthermore, we also evaluate the defense\nperformance of various existing methods when facing threats in the vision\nmodality and uncover their ineffectiveness. Finally, we propose the Vision\nModal Threats in Image Generation Models (VMT-IGMs) dataset, which would serve\nas a baseline for evaluating the vision modality vulnerability of various image\ngeneration models."}
{"id": "2412.16406", "pdf": "https://arxiv.org/pdf/2412.16406", "abs": "https://arxiv.org/abs/2412.16406", "authors": ["Erica Chiang", "Divya Shanmugam", "Ashley N. Beecy", "Gabriel Sayer", "Deborah Estrin", "Nikhil Garg", "Emma Pierson"], "title": "Learning Disease Progression Models That Capture Health Disparities", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.AP", "stat.ML"], "comment": null, "summary": "Disease progression models are widely used to inform the diagnosis and\ntreatment of many progressive diseases. However, a significant limitation of\nexisting models is that they do not account for health disparities that can\nbias the observed data. To address this, we develop an interpretable Bayesian\ndisease progression model that captures three key health disparities: certain\npatient populations may (1) start receiving care only when their disease is\nmore severe, (2) experience faster disease progression even while receiving\ncare, or (3) receive follow-up care less frequently conditional on disease\nseverity. We show theoretically and empirically that failing to account for any\nof these disparities can result in biased estimates of severity (e.g.,\nunderestimating severity for disadvantaged groups). On a dataset of heart\nfailure patients, we show that our model can identify groups that face each\ntype of health disparity, and that accounting for these disparities while\ninferring disease severity meaningfully shifts which patients are considered\nhigh-risk."}
{"id": "2501.01991", "pdf": "https://arxiv.org/pdf/2501.01991", "abs": "https://arxiv.org/abs/2501.01991", "authors": ["Elhoucine Elfatimi", "Lahcen El Fatimi", "Hanifa Bouchaneb"], "title": "A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation", "categories": ["cs.CV", "cs.AI", "I.2.6; I.4.6"], "comment": "22 pages, 8 figures", "summary": "Model checking, a formal verification technique, ensures systems meet\npredefined requirements, playing a crucial role in minimizing errors and\nenhancing quality during development. This paper introduces a novel hybrid\nframework integrating model checking with deep learning for brain tumor\ndetection and validation in medical imaging. By combining model-checking\nprinciples with CNN-based feature extraction and K-FCM clustering for\nsegmentation, the proposed approach enhances the reliability of tumor detection\nand segmentation. Experimental results highlight the framework's effectiveness,\nachieving 98\\% accuracy, 96.15\\% precision, and 100\\% recall, demonstrating its\npotential as a robust tool for advanced medical image analysis."}
{"id": "2412.09621", "pdf": "https://arxiv.org/pdf/2412.09621", "abs": "https://arxiv.org/abs/2412.09621", "authors": ["Linyi Jin", "Richard Tucker", "Zhengqi Li", "David Fouhey", "Noah Snavely", "Aleksander Holynski"], "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos", "categories": ["cs.CV"], "comment": "CVPR 2025 Camera Ready; Data released", "summary": "Learning to understand dynamic 3D scenes from imagery is crucial for\napplications ranging from robotics to scene reconstruction. Yet, unlike other\nproblems where large-scale supervised training has enabled rapid progress,\ndirectly supervising methods for recovering 3D motion remains challenging due\nto the fundamental difficulty of obtaining ground truth annotations. We present\na system for mining high-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the outputs of camera pose\nestimation, stereo depth estimation, and temporal tracking methods into\nhigh-quality dynamic 3D reconstructions. We use this method to generate\nlarge-scale data in the form of world-consistent, pseudo-metric 3D point clouds\nwith long-term motion trajectories. We demonstrate the utility of this data by\ntraining a variant of DUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed data enables\ngeneralization to diverse real-world scenes. Project page and data at:\nhttps://stereo4d.github.io"}
{"id": "2501.13734", "pdf": "https://arxiv.org/pdf/2501.13734", "abs": "https://arxiv.org/abs/2501.13734", "authors": ["Maria-Florina Balcan", "Anh Tuan Nguyen", "Dravyansh Sharma"], "title": "Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function", "categories": ["cs.LG"], "comment": "57 pages, 4 figures", "summary": "Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter; our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks."}
{"id": "2501.17690", "pdf": "https://arxiv.org/pdf/2501.17690", "abs": "https://arxiv.org/abs/2501.17690", "authors": ["Zixue Zeng", "Xiaoyan Zhao", "Matthew Cartier", "Tong Yu", "Jing Wang", "Xin Meng", "Zhiyu Sheng", "Maryam Satarpour", "John M Cormack", "Allison Bean", "Ryan Nussbaum", "Maya Maurer", "Emily Landis-Walkenhorst", "Dinesh Kumbhare", "Kang Kim", "Ajay Wasan", "Jiantao Pu"], "title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce a novel segmentation-aware joint training framework called\ngenerative reinforcement network (GRN) that integrates segmentation loss\nfeedback to optimize both image generation and segmentation performance in a\nsingle stage. An image enhancement technique called segmentation-guided\nenhancement (SGE) is also developed, where the generator produces images\ntailored specifically for the segmentation model. Two variants of GRN were also\ndeveloped, including GRN for sample-efficient learning (GRN-SEL) and GRN for\nsemi-supervised learning (GRN-SSL). GRN's performance was evaluated using a\ndataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The\nannotations included six anatomical structures: dermis, superficial fat,\nsuperficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and\nmuscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up\nto 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient\n(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone\nreduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling\nrequirements by 70%, and GRN-SSL alone by 60%, all while maintaining\nperformance comparable to fully supervised models. These findings suggest the\neffectiveness of the GRN framework in optimizing segmentation performance with\nsignificantly less labeled data, offering a scalable and efficient solution for\nultrasound image analysis and reducing the burdens associated with data\nannotation."}
{"id": "2412.13695", "pdf": "https://arxiv.org/pdf/2412.13695", "abs": "https://arxiv.org/abs/2412.13695", "authors": ["Dominik Werner Wolf", "Alexander Braun", "Markus Ulrich"], "title": "Optical aberrations in autonomous driving: Physics-informed parameterized temperature scaling for neural network uncertainty calibration", "categories": ["cs.CV"], "comment": "Under review at IEEE Transactions on Intelligent Transportation\n  Systems (T-ITS)", "summary": "'A trustworthy representation of uncertainty is desirable and should be\nconsidered as a key feature of any machine learning method' (Huellermeier and\nWaegeman, 2021). This conclusion of Huellermeier et al. underpins the\nimportance of calibrated uncertainties. Since AI-based algorithms are heavily\nimpacted by dataset shifts, the automotive industry needs to safeguard its\nsystem against all possible contingencies. One important but often neglected\ndataset shift is caused by optical aberrations induced by the windshield. For\nthe verification of the perception system performance, requirements on the AI\nperformance need to be translated into optical metrics by a bijective mapping.\nGiven this bijective mapping it is evident that the optical system\ncharacteristics add additional information about the magnitude of the dataset\nshift. As a consequence, we propose to incorporate a physical inductive bias\ninto the neural network calibration architecture to enhance the robustness and\nthe trustworthiness of the AI target application, which we demonstrate by using\na semantic segmentation task as an example. By utilizing the Zernike\ncoefficient vector of the optical system as a physical prior we can\nsignificantly reduce the mean expected calibration error in case of optical\naberrations. As a result, we pave the way for a trustworthy uncertainty\nrepresentation and for a holistic verification strategy of the perception\nchain."}
{"id": "2501.18972", "pdf": "https://arxiv.org/pdf/2501.18972", "abs": "https://arxiv.org/abs/2501.18972", "authors": ["Yuxuan Liu", "Jingmin Sun", "Hayden Schaeffer"], "title": "BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid Dynamics", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We introduce BCAT, a PDE foundation model designed for autoregressive\nprediction of solutions to two dimensional fluid dynamics problems. Our\napproach uses a block causal transformer architecture to model next frame\npredictions, leveraging previous frames as contextual priors rather than\nrelying solely on sub-frames or pixel-based inputs commonly used in image\ngeneration methods. This block causal framework more effectively captures the\nspatial dependencies inherent in nonlinear spatiotemporal dynamics and physical\nphenomena. In an ablation study, next frame prediction demonstrated a 3.5x\naccuracy improvement over next token prediction. BCAT is trained on a diverse\nrange of fluid dynamics datasets, including incompressible and compressible\nNavier-Stokes equations across various geometries and parameter regimes, as\nwell as the shallow-water equations. The model's performance was evaluated on 6\ndistinct downstream prediction tasks and tested on about 8K trajectories to\nmeasure robustness on a variety of fluid dynamics simulations. BCAT achieved an\naverage relative error of 1.18% across all evaluation tasks, outperforming\nprior approaches on standard benchmarks. With fine-tuning on a turbulence\ndataset, we show that the method adapts to new settings with more than 40%\nbetter accuracy over prior methods."}
{"id": "2502.06485", "pdf": "https://arxiv.org/pdf/2502.06485", "abs": "https://arxiv.org/abs/2502.06485", "authors": ["Filip Ekström Kelvinius", "Oskar B. Andersson", "Abhijith S. Parackal", "Dong Qian", "Rickard Armiento", "Fredrik Lindsten"], "title": "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": "Code is available online at https://github.com/httk/wyckoffdiff", "summary": "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation. Code is\navailable online at https://github.com/httk/wyckoffdiff"}
{"id": "2501.08659", "pdf": "https://arxiv.org/pdf/2501.08659", "abs": "https://arxiv.org/abs/2501.08659", "authors": ["Dongzhihan Wang", "Yang Yang", "Liang Xu"], "title": "BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module", "categories": ["cs.CV"], "comment": null, "summary": "Visual odometry (VO) plays a crucial role in autonomous driving, robotic\nnavigation, and other related tasks by estimating the position and orientation\nof a camera based on visual input. Significant progress has been made in\ndata-driven VO methods, particularly those leveraging deep learning techniques\nto extract image features and estimate camera poses. However, these methods\noften struggle in low-light conditions because of the reduced visibility of\nfeatures and the increased difficulty of matching keypoints. To address this\nlimitation, we introduce BrightVO, a novel VO model based on Transformer\narchitecture, which not only performs front-end visual feature extraction, but\nalso incorporates a multi-modality refinement module in the back-end that\nintegrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,\nthis module iteratively refines pose estimates to reduce errors and improve\nboth accuracy and robustness. Furthermore, we create a synthetic low-light\ndataset, KiC4R, which includes a variety of lighting conditions to facilitate\nthe training and evaluation of VO frameworks in challenging environments.\nExperimental results demonstrate that BrightVO achieves state-of-the-art\nperformance on both the KiC4R dataset and the KITTI benchmarks. Specifically,\nit provides an average improvement of 20% in pose estimation accuracy in normal\noutdoor environments and 259% in low-light conditions, outperforming existing\nmethods. For widespread use and further development, the research work is fully\nopen-source at https://github.com/Anastasiawd/BrightVO."}
{"id": "2502.00456", "pdf": "https://arxiv.org/pdf/2502.00456", "abs": "https://arxiv.org/abs/2502.00456", "authors": ["Daniel Sikar", "Artur d'Avila Garcez", "Tillman Weyde"], "title": "Explorations of the Softmax Space: Knowing When the Neural Network Doesn't Know", "categories": ["cs.LG", "cs.CV"], "comment": "15 pages, 6 figures, 2 table", "summary": "Ensuring the reliability of automated decision-making based on neural\nnetworks will be crucial as Artificial Intelligence systems are deployed more\nwidely in critical situations. This paper proposes a new approach for measuring\nconfidence in the predictions of any neural network that relies on the\npredictions of a softmax layer. We identify that a high-accuracy trained\nnetwork may have certain outputs for which there should be low confidence. In\nsuch cases, decisions should be deferred and it is more appropriate for the\nnetwork to provide a \\textit{not known} answer to a corresponding\nclassification task. Our approach clusters the vectors in the softmax layer to\nmeasure distances between cluster centroids and network outputs. We show that a\ncluster with centroid calculated simply as the mean softmax output for all\ncorrect predictions can serve as a suitable proxy in the evaluation of\nconfidence. Defining a distance threshold for a class as the smallest distance\nfrom an incorrect prediction to the given class centroid offers a simple\napproach to adding \\textit{not known} answers to any network classification\nfalling outside of the threshold. We evaluate the approach on the MNIST and\nCIFAR-10 datasets using a Convolutional Neural Network and a Vision\nTransformer, respectively. The results show that our approach is consistent\nacross datasets and network models, and indicate that the proposed distance\nmetric can offer an efficient way of determining when automated predictions are\nacceptable and when they should be deferred to human operators."}
{"id": "2502.21138", "pdf": "https://arxiv.org/pdf/2502.21138", "abs": "https://arxiv.org/abs/2502.21138", "authors": ["Jong Ho Jhee", "Alberto Megina", "Pacôme Constant Dit Beaufils", "Matilde Karakachoff", "Richard Redon", "Alban Gaignard", "Adrien Coulet"], "title": "Predicting clinical outcomes from patient care pathways represented with temporal knowledge graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background: With the increasing availability of healthcare data, predictive\nmodeling finds many applications in the biomedical domain, such as the\nevaluation of the level of risk for various conditions, which in turn can guide\nclinical decision making. However, it is unclear how knowledge graph data\nrepresentations and their embedding, which are competitive in some settings,\ncould be of interest in biomedical predictive modeling. Method: We simulated\nsynthetic but realistic data of patients with intracranial aneurysm and\nexperimented on the task of predicting their clinical outcome. We compared the\nperformance of various classification approaches on tabular data versus a\ngraph-based representation of the same data. Next, we investigated how the\nadopted schema for representing first individual data and second temporal data\nimpacts predictive performances. Results: Our study illustrates that in our\ncase, a graph representation and Graph Convolutional Network (GCN) embeddings\nreach the best performance for a predictive task from observational data. We\nemphasize the importance of the adopted schema and of the consideration of\nliteral values in the representation of individual data. Our study also\nmoderates the relative impact of various time encoding on GCN performance."}
{"id": "2501.11124", "pdf": "https://arxiv.org/pdf/2501.11124", "abs": "https://arxiv.org/abs/2501.11124", "authors": ["Quan Zhang", "Yuxin Qi", "Xi Tang", "Rui Yuan", "Xi Lin", "Ke Zhang", "Chun Yuan"], "title": "Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction", "categories": ["cs.CV"], "comment": null, "summary": "Pseudo-label learning methods have been widely applied in weakly-supervised\ntemporal action localization. Existing works directly utilize weakly-supervised\nbase model to generate instance-level pseudo-labels for training the\nfully-supervised detection head. We argue that the noise in pseudo-labels would\ninterfere with the learning of fully-supervised detection head, leading to\nsignificant performance leakage. Issues with noisy labels include:(1)\ninaccurate boundary localization; (2) undetected short action clips; (3)\nmultiple adjacent segments incorrectly detected as one segment. To target these\nissues, we introduce a two-stage noisy label learning strategy to harness every\npotential useful signal in noisy labels. First, we propose a frame-level\npseudo-label generation model with a context-aware denoising algorithm to\nrefine the boundaries. Second, we introduce an online-revised teacher-student\nframework with a missing instance compensation module and an ambiguous instance\ncorrection module to solve the short-action-missing and many-to-one problems.\nBesides, we apply a high-quality pseudo-label mining loss in our online-revised\nteacher-student framework to add different weights to the noisy labels to train\nmore effectively. Our model outperforms the previous state-of-the-art method in\ndetection accuracy and inference speed greatly upon the THUMOS14 and\nActivityNet v1.2 benchmarks."}
{"id": "2502.02922", "pdf": "https://arxiv.org/pdf/2502.02922", "abs": "https://arxiv.org/abs/2502.02922", "authors": ["Kaiwen Zheng", "Guande He", "Jianfei Chen", "Fan Bao", "Jun Zhu"], "title": "Elucidating the Preconditioning in Consistency Distillation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ICLR 2025", "summary": "Consistency distillation is a prevalent way for accelerating diffusion models\nadopted in consistency (trajectory) models, in which a student model is trained\nto traverse backward on the probability flow (PF) ordinary differential\nequation (ODE) trajectory determined by the teacher model. Preconditioning is a\nvital technique for stabilizing consistency distillation, by linear combining\nthe input data and the network output with pre-defined coefficients as the\nconsistency function. It imposes the boundary condition of consistency\nfunctions without restricting the form and expressiveness of the neural\nnetwork. However, previous preconditionings are hand-crafted and may be\nsuboptimal choices. In this work, we offer the first theoretical insights into\nthe preconditioning in consistency distillation, by elucidating its design\ncriteria and the connection to the teacher ODE trajectory. Based on these\nanalyses, we further propose a principled way dubbed \\textit{Analytic-Precond}\nto analytically optimize the preconditioning according to the consistency gap\n(defined as the gap between the teacher denoiser and the optimal student\ndenoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond\ncan facilitate the learning of trajectory jumpers, enhance the alignment of the\nstudent trajectory with the teacher's, and achieve $2\\times$ to $3\\times$\ntraining acceleration of consistency trajectory models in multi-step generation\nacross various datasets."}
{"id": "2503.01713", "pdf": "https://arxiv.org/pdf/2503.01713", "abs": "https://arxiv.org/abs/2503.01713", "authors": ["Jintao Zhang", "Guoliang Li", "Jinyang Su"], "title": "SAGE: A Framework of Precise Retrieval for RAG", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG."}
{"id": "2501.16289", "pdf": "https://arxiv.org/pdf/2501.16289", "abs": "https://arxiv.org/abs/2501.16289", "authors": ["Younggun Kim", "Beomsik Cho", "Seonghoon Ryoo", "Soomok Lee"], "title": "Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles", "categories": ["cs.CV"], "comment": "36 pages, 6 figures", "summary": "Point cloud representation has recently become a research hotspot in the\nfield of computer vision and has been utilized for autonomous vehicles.\nHowever, adapting deep learning networks for point cloud data recognition is\nchallenging due to the variability in datasets and sensor technologies. This\nvariability underscores the necessity for adaptive techniques to maintain\naccuracy under different conditions. In this paper, we present the Multi-View\nStructural Convolution Network (MSCN) designed for domain-invariant point cloud\nrecognition. MSCN comprises Structural Convolution Layers (SCL) that extract\nlocal context geometric features from point clouds and Structural Aggregation\nLayers (SAL) that extract and aggregate both local and overall context features\nfrom point clouds. Additionally, our MSCN enhances feature representation\nrobustness by training with unseen domain point clouds derived from source\ndomain point clouds. This method acquires domain-invariant features and\nexhibits robust, consistent performance across various point cloud datasets,\nensuring compatibility with diverse sensor configurations without the need for\nparameter adjustments. This highlights MSCN's potential to significantly\nimprove the reliability and domain invariant features in different\nenvironments. Our code is available at https://github.com/MLMLab/MSCN."}
{"id": "2502.06805", "pdf": "https://arxiv.org/pdf/2502.06805", "abs": "https://arxiv.org/abs/2502.06805", "authors": ["Hui Shen", "Jingxuan Zhang", "Boning Xiong", "Rui Hu", "Shoufa Chen", "Zhongwei Wan", "Xin Wang", "Yu Zhang", "Zixuan Gong", "Guangyin Bao", "Chaofan Tao", "Yongfeng Huang", "Ye Yuan", "Mi Zhang"], "title": "Efficient Diffusion Models: A Survey", "categories": ["cs.LG", "cs.GR"], "comment": "Published in Transactions on Machine Learning Research (TMLR-2025)", "summary": "Diffusion models have emerged as powerful generative models capable of\nproducing high-quality contents such as images, videos, and audio,\ndemonstrating their potential to revolutionize digital content creation.\nHowever, these capabilities come at the cost of their significant computational\nresources and lengthy generation time, underscoring the critical need to\ndevelop efficient techniques for practical deployment. In this survey, we\nprovide a systematic and comprehensive review of research on efficient\ndiffusion models. We organize the literature in a taxonomy consisting of three\nmain categories, covering distinct yet interconnected efficient diffusion model\ntopics from algorithm-level, system-level, and framework perspective,\nrespectively. We have also created a GitHub repository where we organize the\npapers featured in this survey at\nhttps://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey. We hope our\nsurvey can serve as a valuable resource to help researchers and practitioners\ngain a systematic understanding of efficient diffusion model research and\ninspire them to contribute to this important and exciting field."}
{"id": "2503.18578", "pdf": "https://arxiv.org/pdf/2503.18578", "abs": "https://arxiv.org/abs/2503.18578", "authors": ["Tianyu Chen", "Xingcheng Fu", "Yisen Gao", "Haodong Qian", "Yuecen Wei", "Kun Yan", "Haoyi Zhou", "Jianxin Li"], "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs."}
{"id": "2501.18635", "pdf": "https://arxiv.org/pdf/2501.18635", "abs": "https://arxiv.org/abs/2501.18635", "authors": ["Sophie Kergaßner", "Taimoor Tariq", "Piotr Didyk"], "title": "Towards Understanding Depth Perception in Foveated Rendering", "categories": ["cs.CV", "cs.GR", "I.3.m"], "comment": "9 pages including references", "summary": "The true vision for real-time virtual and augmented reality is reproducing\nour visual reality in its entirety on immersive displays. To this end, foveated\nrendering leverages the limitations of spatial acuity in human peripheral\nvision to allocate computational resources to the fovea while reducing quality\nin the periphery. Such methods are often derived from studies on the spatial\nresolution of the human visual system and its ability to perceive blur in the\nperiphery, enabling the potential for high spatial quality in real-time.\nHowever, the effects of blur on other visual cues that depend on luminance\ncontrast, such as depth, remain largely unexplored. It is critical to\nunderstand this interplay, as accurate depth representation is a fundamental\naspect of visual realism. In this paper, we present the first evaluation\nexploring the effects of foveated rendering on stereoscopic depth perception.\nWe design a psychovisual experiment to quantitatively study the effects of\nperipheral blur on depth perception. Our analysis demonstrates that\nstereoscopic acuity remains unaffected (or even improves) by high levels of\nperipheral blur. Based on our studies, we derive a simple perceptual model that\ndetermines the amount of foveation that does not affect stereoacuity.\nFurthermore, we analyze the model in the context of common foveation practices\nreported in literature. The findings indicate that foveated rendering does not\nimpact stereoscopic depth perception, and stereoacuity remains unaffected with\nup to 2x stronger foveation than commonly used. Finally, we conduct a\nvalidation experiment and show that our findings hold for complex natural\nstimuli."}
{"id": "2502.14259", "pdf": "https://arxiv.org/pdf/2502.14259", "abs": "https://arxiv.org/abs/2502.14259", "authors": ["Sujeong Im", "Jungwoo Oh", "Edward Choi"], "title": "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records", "categories": ["cs.LG"], "comment": "11 pages for main text, 13 pages for appendix", "summary": "Lab tests are fundamental for diagnosing diseases and monitoring patient\nconditions. However, frequent testing can be burdensome for patients, and test\nresults may not always be immediately available. To address these challenges,\nwe propose LabTOP, a unified model that predicts lab test outcomes by\nleveraging a language modeling approach on EHR data. Unlike conventional\nmethods that estimate only a subset of lab tests or classify discrete value\nranges, LabTOP performs continuous numerical predictions for a diverse range of\nlab items. We evaluate LabTOP on three publicly available EHR datasets and\ndemonstrate that it outperforms existing methods, including traditional machine\nlearning models and state-of-the-art large language models. We also conduct\nextensive ablation studies to confirm the effectiveness of our design choices.\nWe believe that LabTOP will serve as an accurate and generalizable framework\nfor lab test outcome prediction, with potential applications in clinical\ndecision support and early detection of critical conditions."}
{"id": "2504.05255", "pdf": "https://arxiv.org/pdf/2504.05255", "abs": "https://arxiv.org/abs/2504.05255", "authors": ["Sviatoslav Dzhenzher", "Michael H. Freedman"], "title": "Adversarial KA", "categories": ["cs.LG", "cs.AI", "math.FA"], "comment": "8 pages, 3 figures; minor revision, question 4.1 added", "summary": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs."}
{"id": "2502.02454", "pdf": "https://arxiv.org/pdf/2502.02454", "abs": "https://arxiv.org/abs/2502.02454", "authors": ["Quan Zhang", "Yuxin Qi", "Xi Tang", "Jinwei Fang", "Xi Lin", "Ke Zhang", "Chun Yuan"], "title": "IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning", "categories": ["cs.CV"], "comment": null, "summary": "Using extensive training data from SA-1B, the Segment Anything Model (SAM)\nhas demonstrated exceptional generalization and zero-shot capabilities,\nattracting widespread attention in areas such as medical image segmentation and\nremote sensing image segmentation. However, its performance in the field of\nimage manipulation detection remains largely unexplored and unconfirmed. There\nare two main challenges in applying SAM to image manipulation detection: a)\nreliance on manual prompts, and b) the difficulty of single-view information in\nsupporting cross-dataset generalization. To address these challenges, we\ndevelops a cross-view prompt learning paradigm called IMDPrompter based on SAM.\nBenefiting from the design of automated prompts, IMDPrompter no longer relies\non manual guidance, enabling automated detection and localization.\nAdditionally, we propose components such as Cross-view Feature Perception,\nOptimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate\ncross-view perceptual learning and guide SAM to generate accurate masks.\nExtensive experimental results from five datasets (CASIA, Columbia, Coverage,\nIMD2020, and NIST16) validate the effectiveness of our proposed method."}
{"id": "2503.00094", "pdf": "https://arxiv.org/pdf/2503.00094", "abs": "https://arxiv.org/abs/2503.00094", "authors": ["Pierre Houdouin", "Lucas Saludjian"], "title": "Gaussian process surrogate model to approximate power grid simulators -- An application to the certification of a congestion management controller", "categories": ["cs.LG"], "comment": "6 pages, 7 figures, conference", "summary": "With the digitalization of power grids, physical equations become\ninsufficient to describe the network's behavior, and realistic but\ntime-consuming simulators must be used. Numerical experiments, such as safety\nvalidation, that involve simulating a large number of scenarios become\ncomputationally intractable. A popular solution to reduce the computational\nburden is to learn a surrogate model of the simulator with Machine Learning\n(ML) and then conduct the experiment directly on the fast-to-evaluate surrogate\nmodel. Among the various ML possibilities for building surrogate models,\nGaussian processes (GPs) emerged as a popular solution due to their\nflexibility, data efficiency, and interpretability. Their probabilistic nature\nenables them to provide both predictions and uncertainty quantification (UQ).\nThis paper starts with a discussion on the interest of using GPs to approximate\npower grid simulators and fasten numerical experiments. Such simulators,\nhowever, often violate the GP's underlying Gaussian assumption, leading to poor\napproximations. To address this limitation, an approach that consists in adding\nan adaptive residual uncertainty term to the UQ is proposed. It enables the GP\nto remain accurate and reliable despite the simulator's non-Gaussian behaviors.\nThis approach is successfully applied to the certification of the proper\nfunctioning of a congestion management controller, with over 98% of simulations\navoided."}
{"id": "2504.10478", "pdf": "https://arxiv.org/pdf/2504.10478", "abs": "https://arxiv.org/abs/2504.10478", "authors": ["Xingyu Dang", "Christina Baek", "Kaiyue Wen", "Zico Kolter", "Aditi Raghunathan"], "title": "Weight Ensembling Improves Reasoning in Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We investigate a failure mode that arises during the training of reasoning\nmodels, where the diversity of generations begins to collapse, leading to\nsuboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during\nsupervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a\nsimple intervention of interpolating the weights of the latest SFT checkpoint\nwith an early checkpoint, otherwise known as WiSE-FT, almost completely\nrecovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves\nbetter test-time scaling (Best@k, majority vote) and achieves superior results\nwith less data when tuned further by reinforcement learning. Finally, we find\nthat WiSE-FT provides complementary performance gains that cannot be achieved\nonly through diversity-inducing decoding strategies, like temperature scaling.\nWe formalize a bias-variance tradeoff of Pass@k with respect to the expectation\nand variance of Pass@1 over the test distribution. We find that WiSE-FT can\nreduce bias and variance simultaneously, while temperature scaling inherently\ntrades off between bias and variance."}
{"id": "2502.20292", "pdf": "https://arxiv.org/pdf/2502.20292", "abs": "https://arxiv.org/abs/2502.20292", "authors": ["Kyle Stein", "Arash Mahyari", "Guillermo Francia", "Eman El-Sheikh"], "title": "Visual Adaptive Prompting for Compositional Zero-Shot Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nlearning joint representations of visual and textual data, making them powerful\ntools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires\nmodels to generalize to novel combinations of visual primitives-such as\nattributes and objects-that were not explicitly encountered during training.\nRecent works in prompting for CZSL have focused on modifying inputs for the\ntext encoder, often using static prompts that do not change across varying\nvisual contexts. However, these approaches struggle to fully capture varying\nvisual contexts, as they focus on text adaptation rather than leveraging visual\nfeatures for compositional reasoning. To address this, we propose Visual\nAdaptive Prompting System (VAPS) that leverages a learnable visual prompt\nrepository and similarity-based retrieval mechanism within the framework of\nVLMs to bridge the gap between semantic and visual features. Our method\nintroduces a dynamic visual prompt repository mechanism that selects the most\nrelevant attribute and object prompts based on the visual features of the\nimage. Our proposed system includes a visual prompt adapter that encourages the\nmodel to learn a more generalizable embedding space. Experiments on three CZSL\nbenchmarks, across both closed and open-world scenarios, demonstrate\nstate-of-the-art results."}
{"id": "2503.03276", "pdf": "https://arxiv.org/pdf/2503.03276", "abs": "https://arxiv.org/abs/2503.03276", "authors": ["Jiayi Zhang", "Yiming Zhang", "Yuan Zheng", "Yuchen Wang", "Jinjiang You", "Yuchen Xu", "Wenxing Jiang", "Soumyabrata Dev"], "title": "TrafficKAN-GCN: Graph Convolutional-based Kolmogorov-Arnold Network for Traffic Flow Optimization", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Urban traffic optimization is critical for improving transportation\nefficiency and alleviating congestion, particularly in large-scale dynamic\nnetworks. Traditional methods, such as Dijkstra's and Floyd's algorithms,\nprovide effective solutions in static settings, but they struggle with the\nspatial-temporal complexity of real-world traffic flows. In this work, we\npropose TrafficKAN-GCN, a hybrid deep learning framework combining\nKolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN),\ndesigned to enhance urban traffic flow optimization. By integrating KAN's\nadaptive nonlinear function approximation with GCN's spatial graph learning\ncapabilities, TrafficKAN-GCN captures both complex traffic patterns and\ntopological dependencies. We evaluate the proposed framework using real-world\ntraffic data from the Baltimore Metropolitan area. Compared with baseline\nmodels such as MLP-GCN, standard GCN, and Transformer-based approaches,\nTrafficKAN-GCN achieves competitive prediction accuracy while demonstrating\nimproved robustness in handling noisy and irregular traffic data. Our\nexperiments further highlight the framework's ability to redistribute traffic\nflow, mitigate congestion, and adapt to disruptive events, such as the Francis\nScott Key Bridge collapse. This study contributes to the growing body of work\non hybrid graph learning for intelligent transportation systems, highlighting\nthe potential of combining KAN and GCN for real-time traffic optimization.\nFuture work will focus on reducing computational overhead and integrating\nTransformer-based temporal modeling for enhanced long-term traffic prediction.\nThe proposed TrafficKAN-GCN framework offers a promising direction for\ndata-driven urban mobility management, balancing predictive accuracy,\nrobustness, and computational efficiency."}
{"id": "2504.11014", "pdf": "https://arxiv.org/pdf/2504.11014", "abs": "https://arxiv.org/abs/2504.11014", "authors": ["Eunsoo Im", "Changhyun Jee", "Jung Kwon Lee"], "title": "GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted (Poster) to the 3rd CV4MR Workshop at CVPR 2025:\n  https://openreview.net/forum?id=00RQ8Cv3ia", "summary": "The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/"}
{"id": "2503.02891", "pdf": "https://arxiv.org/pdf/2503.02891", "abs": "https://arxiv.org/abs/2503.02891", "authors": ["Shaibal Saha", "Lanyu Xu"], "title": "Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies", "categories": ["cs.CV", "cs.AR"], "comment": "Accepted in Neurocomputing, Elsevier", "summary": "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices."}
{"id": "2504.06048", "pdf": "https://arxiv.org/pdf/2504.06048", "abs": "https://arxiv.org/abs/2504.06048", "authors": ["Joery A. de Vries", "Jinke He", "Yaniv Oren", "Matthijs T. J. Spaan"], "title": "Trust-Region Twisted Policy Improvement", "categories": ["cs.LG"], "comment": null, "summary": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains."}
{"id": "2504.14070", "pdf": "https://arxiv.org/pdf/2504.14070", "abs": "https://arxiv.org/abs/2504.14070", "authors": ["Jinesh Jhonsa", "William Whitehead", "David McCarthy", "Shuvro Chowdhury", "Kerem Camsari", "Luke Theogarajan"], "title": "A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning", "categories": ["cs.AR", "cs.AI"], "comment": "3 pages 12 figuewa", "summary": "This paper demonstrates a probabilistic bit physics inspired solver with 440\nspins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area\nefficiency is maximized through a current-mode implementation of the neuron\nupdate circuit, standard cell design for analog blocks pitch-matched to digital\nblocks, and a shared power supply for both digital and analog components.\nProcess variation related mismatches introduced by this approach are\neffectively mitigated using a hardware aware contrastive divergence algorithm\nduring training. We validate the chip's ability to perform probabilistic\ncomputing tasks such as modeling logic gates and full adders, as well as\noptimization tasks such as MaxCut, demonstrating its potential for AI and\nmachine learning applications."}
{"id": "2503.12026", "pdf": "https://arxiv.org/pdf/2503.12026", "abs": "https://arxiv.org/abs/2503.12026", "authors": ["Zihan Zhou", "Changrui Dai", "Aibo Song", "Xiaolin Fang"], "title": "Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised video correspondence learning depends on the ability to\naccurately associate pixels between video frames that correspond to the same\nvisual object. However, achieving reliable pixel matching without supervision\nremains a major challenge. To address this issue, recent research has focused\non feature learning techniques that aim to encode unique pixel representations\nfor matching. Despite these advances, existing methods still struggle to\nachieve exact pixel correspondences and often suffer from false matches,\nlimiting their effectiveness in self-supervised settings.\n  To this end, we explore an efficient self-supervised Video Correspondence\nLearning framework (MER) that aims to accurately extract object details from\nunlabeled videos. First, we design a dedicated Motion Enhancement Engine that\nemphasizes capturing the dynamic motion of objects in videos. In addition, we\nintroduce a flexible sampling strategy for inter-pixel correspondence\ninformation (Multi-Cluster Sampler) that enables the model to pay more\nattention to the pixel changes of important objects in motion. Through\nexperiments, our algorithm outperforms the state-of-the-art competitors on\nvideo correspondence learning tasks such as video object segmentation and video\nobject keypoint tracking."}
{"id": "2504.14697", "pdf": "https://arxiv.org/pdf/2504.14697", "abs": "https://arxiv.org/abs/2504.14697", "authors": ["Shi Chen", "Zhengjiang Lin", "Yury Polyanskiy", "Philippe Rigollet"], "title": "Quantitative Clustering in Mean-Field Transformer Models", "categories": ["cs.LG", "math.AP", "math.DS", "stat.ML"], "comment": "48 pages, 4 figures", "summary": "The evolution of tokens through a deep transformer models can be modeled as\nan interacting particle system that has been shown to exhibit an asymptotic\nclustering behavior akin to the synchronization phenomenon in Kuramoto models.\nIn this work, we investigate the long-time clustering of mean-field transformer\nmodels. More precisely, we establish exponential rates of contraction to a\nDirac point mass for any suitably regular initialization under some assumptions\non the parameters of transformer models, any suitably regular mean-field\ninitialization synchronizes exponentially fast with some quantitative rates."}
{"id": "2504.14493", "pdf": "https://arxiv.org/pdf/2504.14493", "abs": "https://arxiv.org/abs/2504.14493", "authors": ["Xinyu Wang", "Jijun Chi", "Zhenghan Tai", "Tung Sum Thomas Kwok", "Muzhi Li", "Zhuhong Li", "Hailin He", "Yuchen Hua", "Peng Lu", "Suyuchen Wang", "Yihong Wu", "Jerry Huang", "Jingrui Tian", "Ling Zhou"], "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people."}
{"id": "2503.13435", "pdf": "https://arxiv.org/pdf/2503.13435", "abs": "https://arxiv.org/abs/2503.13435", "authors": ["Ling Yang", "Kaixin Zhu", "Juanxi Tian", "Bohan Zeng", "Mingbao Lin", "Hongjuan Pei", "Wentao Zhang", "Shuicheng Yan"], "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/WideRange4D", "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D"}
{"id": "2504.16214", "pdf": "https://arxiv.org/pdf/2504.16214", "abs": "https://arxiv.org/abs/2504.16214", "authors": ["Xiao Zhang", "Yaoyao Ding", "Yang Hu", "Gennady Pekhimenko"], "title": "Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "17 pages, 24 figures", "summary": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation."}
{"id": "2504.17751", "pdf": "https://arxiv.org/pdf/2504.17751", "abs": "https://arxiv.org/abs/2504.17751", "authors": ["Enqi Zhang"], "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks. From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps. To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling."}
{"id": "2503.19769", "pdf": "https://arxiv.org/pdf/2503.19769", "abs": "https://arxiv.org/abs/2503.19769", "authors": ["Suzhe Xu", "Jialin Peng", "Chengyuan Zhang"], "title": "BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Segmentation is a fundamental task in computer vision, with prompt-driven\nmethods gaining prominence due to their flexibility. The Segment Anything Model\n(SAM) excels at point-prompted segmentation, while text-based models, often\nleveraging powerful multimodal encoders like BEIT-3, provide rich semantic\nunderstanding. However, effectively combining these complementary modalities\nremains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal\nprompt segmentation framework employing an explicit selection mechanism. We\nleverage SAM's ability to generate multiple mask candidates from a single point\nprompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select\nthe point-generated mask that best aligns spatially, measured by Intersection\nover Union (IoU). This approach, interpretable as a simplified Mixture of\nExperts (MoE), effectively fuses spatial precision and semantic context without\ncomplex model modifications. Notably, our method achieves strong zero-shot\nperformance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using\nonly a single point prompt per instance. This significantly reduces annotation\nburden compared to bounding boxes and aligns better with practical clinical\nworkflows, demonstrating the method's effectiveness without domain-specific\ntraining. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8%\nIoU, significantly outperforming existing approaches. Experiments show\nBiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic\ndisambiguation, offering a simple, effective, and interpretable perspective on\nmulti-modal prompt fusion."}
{"id": "2504.17074", "pdf": "https://arxiv.org/pdf/2504.17074", "abs": "https://arxiv.org/abs/2504.17074", "authors": ["William R. Keely", "Otto Lamminpää", "Steffen Mauceri", "Sean M. R. Crowell", "Christopher W. O'Dell", "Gregory R. McGarragh"], "title": "Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Published as a workshop paper in \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025 Workshop on Tackling Climate Change with Machine\n  Learning. https://www.climatechange.ai/papers/iclr2025/12", "summary": "Satellite-based estimates of greenhouse gas (GHG) properties from\nobservations of reflected solar spectra are integral for understanding and\nmonitoring complex terrestrial systems and their impact on the carbon cycle due\nto their near global coverage. Known as retrieval, making GHG concentration\nestimations from these observations is a non-linear Bayesian inverse problem,\nwhich is operationally solved using a computationally expensive algorithm\ncalled Optimal Estimation (OE), providing a Gaussian approximation to a\nnon-Gaussian posterior. This leads to issues in solver algorithm convergence,\nand to unrealistically confident uncertainty estimates for the retrieved\nquantities. Upcoming satellite missions will provide orders of magnitude more\ndata than the current constellation of GHG observers. Development of fast and\naccurate retrieval algorithms with robust uncertainty quantification is\ncritical. Doing so stands to provide substantial climate impact of moving\ntowards the goal of near continuous real-time global monitoring of carbon\nsources and sinks which is essential for policy making. To achieve this goal,\nwe propose a diffusion-based approach to flexibly retrieve a Gaussian or\nnon-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,\nwhile providing a substantial computational speed-up over the current\noperational state-of-the-art."}
{"id": "2504.17827", "pdf": "https://arxiv.org/pdf/2504.17827", "abs": "https://arxiv.org/abs/2504.17827", "authors": ["Bingye Zhou", "Caiyang Yu"], "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness."}
{"id": "2504.00879", "pdf": "https://arxiv.org/pdf/2504.00879", "abs": "https://arxiv.org/abs/2504.00879", "authors": ["Fenglei Hao", "Yuliang Yang", "Ruiyuan Su", "Zhengran Zhao", "Yukun Qiao", "Mengyu Zhu"], "title": "GISE-TTT:A Framework for Global InformationSegmentation and Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of capturing global temporaldependencies\nin long video sequences for Video Object Segmentation (VOS). Existing\narchitectures often fail to effectively model these dependencies acrossextended\ntemporal horizons. To overcome this limitation, we introduce GISE-TTT, anovel\narchitecture that integrates Temporal Transformer (TTT) layers\nintotransformer-based frameworks through a co-designed hierarchical\napproach.The TTTlayer systematically condenses historical temporal information\ninto hidden states thatencode globally coherent contextual representations. By\nleveraging multi-stagecontextual aggregation through hierarchical\nconcatenation, our frameworkprogressively refines spatiotemporal dependencies\nacross network layers. This designrepresents the first systematic empirical\nevidence that distributing global informationacross multiple network layers is\ncritical for optimal dependency utilization in videosegmentation tasks.Ablation\nstudies demonstrate that incorporating TTT modules athigh-level feature stages\nsignificantly enhances global modeling capabilities, therebyimproving the\nnetwork's ability to capture long-range temporal relationships. Extensive\nexperiments on DAVIS 2017 show that GISE-TTT achieves a 3.2%improvement in\nsegmentation accuracy over the baseline model, providingcomprehensive evidence\nthat global information should be strategically leveragedthroughout the network\narchitecture.The code will be made available\nat:https://github.com/uuool/GISE-TTT."}
{"id": "2504.17079", "pdf": "https://arxiv.org/pdf/2504.17079", "abs": "https://arxiv.org/abs/2504.17079", "authors": ["Esam Mahdi", "C. Martin-Barreiro", "X. Cabezas"], "title": "A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In this article, we introduce a novel deep learning hybrid model that\nintegrates attention Transformer and Gated Recurrent Unit (GRU) architectures\nto improve the accuracy of cryptocurrency price predictions. By combining the\nTransformer's strength in capturing long-range patterns with the GRU's ability\nto model short-term and sequential trends, the hybrid model provides a\nwell-rounded approach to time series forecasting. We apply the model to predict\nthe daily closing prices of Bitcoin and Ethereum based on historical data that\ninclude past prices, trading volumes, and the Fear and Greed index. We evaluate\nthe performance of our proposed model by comparing it with four other machine\nlearning models: two are non-sequential feedforward models: Radial Basis\nFunction Network (RBFN) and General Regression Neural Network (GRNN), and two\nare bidirectional sequential memory-based models: Bidirectional Long-Short-Term\nMemory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance\nof the model is assessed using several metrics, including Mean Squared Error\n(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean\nAbsolute Percentage Error (MAPE), along with statistical validation through the\nnonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.\nThe results demonstrate that our hybrid model consistently achieves superior\naccuracy, highlighting its effectiveness for financial prediction tasks. These\nfindings provide valuable insights for improving real-time decision making in\ncryptocurrency markets and support the growing use of hybrid deep learning\nmodels in financial analytics."}
{"id": "2504.19394", "pdf": "https://arxiv.org/pdf/2504.19394", "abs": "https://arxiv.org/abs/2504.19394", "authors": ["Toby Simonds"], "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment."}
{"id": "2504.15032", "pdf": "https://arxiv.org/pdf/2504.15032", "abs": "https://arxiv.org/abs/2504.15032", "authors": ["Weijie He", "Mushui Liu", "Yunlong Yu", "Zhao Wang", "Chao Wu"], "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving precise control over\nindividual entities; and (3) An Entity-Consistency Constraint strategy that\npropagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis. The code is released in\nhttps://github.com/XiaoBuL/DyST-XL."}
{"id": "2504.20869", "pdf": "https://arxiv.org/pdf/2504.20869", "abs": "https://arxiv.org/abs/2504.20869", "authors": ["Junyuan Fang", "Han Yang", "Haixian Wen", "Jiajing Wu", "Zibin Zheng", "Chi K. Tse"], "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under Review", "summary": "Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes."}
{"id": "2504.20114", "pdf": "https://arxiv.org/pdf/2504.20114", "abs": "https://arxiv.org/abs/2504.20114", "authors": ["Zhonghao Li", "Kunpeng Zhang", "Jinghuai Ou", "Shuliang Liu", "Xuming Hu"], "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG."}
{"id": "2504.15122", "pdf": "https://arxiv.org/pdf/2504.15122", "abs": "https://arxiv.org/abs/2504.15122", "authors": ["Minh-Quan Viet Bui", "Jongmin Park", "Juan Luis Gonzalez Bello", "Jaeho Moon", "Jihyong Oh", "Munchurl Kim"], "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video", "categories": ["cs.CV"], "comment": "The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at https://kaist-viclab.github.io/mobgs-site/", "summary": "We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur."}
{"id": "2004.12571", "pdf": "https://arxiv.org/pdf/2004.12571", "abs": "https://arxiv.org/abs/2004.12571", "authors": ["Xinjian Luo", "Xianglong Zhang"], "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning", "categories": ["cs.CR", "cs.LG"], "comment": "Published in ACM Transactions on Knowledge Discovery from Data\n  (TKDD), 2025", "summary": "Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model."}
{"id": "2504.16290", "pdf": "https://arxiv.org/pdf/2504.16290", "abs": "https://arxiv.org/abs/2504.16290", "authors": ["André Longon"], "title": "Naturally Computed Scale Invariance in the Residual Stream of ResNet18", "categories": ["cs.CV", "cs.LG"], "comment": "Fixed broken reference", "summary": "An important capacity in visual object recognition is invariance to\nimage-altering variables which leave the identity of objects unchanged, such as\nlighting, rotation, and scale. How do neural networks achieve this? Prior\nmechanistic interpretability research has illuminated some invariance-building\ncircuitry in InceptionV1, but the results are limited and networks with\ndifferent architectures have remained largely unexplored. This work\ninvestigates ResNet18 with a particular focus on its residual stream, an\narchitectural component which InceptionV1 lacks. We observe that many\nconvolutional channels in intermediate blocks exhibit scale invariant\nproperties, computed by the element-wise residual summation of scale\nequivariant representations: the block input's smaller-scale copy with the\nblock pre-sum output's larger-scale copy. Through subsequent ablation\nexperiments, we attempt to causally link these neural properties with\nscale-robust object recognition behavior. Our tentative findings suggest how\nthe residual stream computes scale invariance and its possible role in\nbehavior. Code is available at:\nhttps://github.com/cest-andre/residual-stream-interp"}
{"id": "2209.10346", "pdf": "https://arxiv.org/pdf/2209.10346", "abs": "https://arxiv.org/abs/2209.10346", "authors": ["Guy Kornowski", "Ohad Shamir"], "title": "On the Complexity of Finding Small Subgradients in Nonsmooth Optimization", "categories": ["math.OC", "cs.LG"], "comment": "Fixed bug in proof of Lemma 1 (result remains unaffected)", "summary": "We study the oracle complexity of producing $(\\delta,\\epsilon)$-stationary\npoints of Lipschitz functions, in the sense proposed by Zhang et al. [2020].\nWhile there exist dimension-free randomized algorithms for producing such\npoints within $\\widetilde{O}(1/\\delta\\epsilon^3)$ first-order oracle calls, we\nshow that no dimension-free rate can be achieved by a deterministic algorithm.\nOn the other hand, we point out that this rate can be derandomized for smooth\nfunctions with merely a logarithmic dependence on the smoothness parameter.\nMoreover, we establish several lower bounds for this task which hold for any\nrandomized algorithm, with or without convexity. Finally, we show how the\nconvergence rate of finding $(\\delta,\\epsilon)$-stationary points can be\nimproved in case the function is convex, a setting which we motivate by proving\nthat in general no finite time algorithm can produce points with small\nsubgradients even for convex functions."}
{"id": "2504.16570", "pdf": "https://arxiv.org/pdf/2504.16570", "abs": "https://arxiv.org/abs/2504.16570", "authors": ["Giacomo Pacini", "Lorenzo Bianchi", "Luca Ciampi", "Nicola Messina", "Giuseppe Amato", "Fabrizio Falchi"], "title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones", "categories": ["cs.CV"], "comment": "13 pages, 2 figures, 2 tables. Project website:\n  https://lorebianchi98.github.io/CountingDINO/", "summary": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we consistently outperform a baseline based on an SOTA\nunsupervised object detector under the same label- and training-free setting.\nAdditionally, we achieve competitive results -- and in some cases surpass --\ntraining-free methods that rely on supervised backbones, non-training-free\nunsupervised methods, as well as several fully supervised SOTA approaches. This\ndemonstrates that label- and training-free CAC can be both scalable and\neffective. Code: https://lorebianchi98.github.io/CountingDINO/."}
{"id": "2312.08298", "pdf": "https://arxiv.org/pdf/2312.08298", "abs": "https://arxiv.org/abs/2312.08298", "authors": ["Jiachen Liu", "Fan Lai", "Ding Ding", "Yiwen Zhang", "Mosharaf Chowdhury"], "title": "Venn: Resource Management for Collaborative Learning Jobs", "categories": ["cs.DC", "cs.LG"], "comment": "14 pages, 15 figrues", "summary": "In recent years, collaborative learning (CL) has emerged as a promising\napproach for machine learning (ML) and data science across distributed edge\ndevices. As the deployment of CL jobs increases, they inevitably contend for\nlimited resources. However, efficient resource scheduling in this context is\nchallenging because of the ephemeral nature and resource heterogeneity of\ndevices, coupled with the overlapping resource requirements of diverse CL jobs.\nExisting resource managers often assign devices to CL jobs randomly for\nsimplicity and scalability, but this approach compromises job efficiency.\n  In this paper, we present Venn, a CL resource manager that efficiently\nschedules ephemeral, heterogeneous devices among multiple CL jobs to reduce the\naverage job completion time (JCT). Venn formulates the Intersection Resource\nScheduling (IRS) problem to identify complex resource contention among multiple\nCL jobs. It then proposes a contention-aware scheduling heuristic to minimize\nthe average scheduling delay. Furthermore, it proposes a resource-aware\ndevice-to-job matching heuristic to optimize response collection time by\nmitigating stragglers. Our evaluation shows that, compared to the\nstate-of-the-art CL resource managers, Venn improves the average JCT by up to\n1.88x. The code is available at https://github.com/SymbioticLab/Venn."}
{"id": "2504.19258", "pdf": "https://arxiv.org/pdf/2504.19258", "abs": "https://arxiv.org/abs/2504.19258", "authors": ["Shuhao Kang", "Martin Y. Liao", "Yan Xia", "Olaf Wysocki", "Boris Jutzi", "Daniel Cremers"], "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion", "categories": ["cs.CV", "cs.RO"], "comment": "Technical report. 15 pages, 9 figures", "summary": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components. First, a\ncross-modal visibility mask that identifies maximal observable regions from\nboth modalities to guide feature learning. Second, an adaptive radial fusion\nmodule that dynamically consolidates radial features into discriminative global\ndescriptors. Extensive experiments on the KITTI and KITTI-360 datasets\ndemonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold\nfor top-1 retrieved matches, along with 12x faster inference speed compared to\nthe state-of-the-art approach. Code and datasets will be publicly available."}
{"id": "2402.01338", "pdf": "https://arxiv.org/pdf/2402.01338", "abs": "https://arxiv.org/abs/2402.01338", "authors": ["Youngkyoung Bae", "Seungwoong Ha", "Hawoong Jeong"], "title": "Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks", "categories": ["cond-mat.stat-mech", "cond-mat.soft", "cs.LG", "physics.bio-ph"], "comment": "34 pages, 17 figures", "summary": "Pervasive across diverse domains, stochastic systems exhibit fluctuations in\nprocesses ranging from molecular dynamics to climate phenomena. The Langevin\nequation has served as a common mathematical model for studying such systems,\nenabling predictions of their temporal evolution and analyses of thermodynamic\nquantities, including absorbed heat, work done on the system, and entropy\nproduction. However, inferring the Langevin equation from observed trajectories\nis a challenging problem, and assessing the uncertainty associated with the\ninferred equation has yet to be accomplished. In this study, we present a\ncomprehensive framework that employs Bayesian neural networks for inferring\nLangevin equations in both overdamped and underdamped regimes. Our framework\nfirst provides the drift force and diffusion matrix separately and then\ncombines them to construct the Langevin equation. By providing a distribution\nof predictions instead of a single value, our approach allows us to assess\nprediction uncertainties, which can help prevent potential misunderstandings\nand erroneous decisions about the system. We demonstrate the effectiveness of\nour framework in inferring Langevin equations for various scenarios including a\nneuron model and microscopic engine, highlighting its versatility and potential\nimpact."}
{"id": "2504.19735", "pdf": "https://arxiv.org/pdf/2504.19735", "abs": "https://arxiv.org/abs/2504.19735", "authors": ["Rustam Tagiew", "Prasannavenkatesh Balaji"], "title": "Measuring Train Driver Performance as Key to Approval of Driverless Trains", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, abstract accepted by IAVVC 2025, full paper to be\n  submitted to IAVVC 2025", "summary": "Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation\n(EU) No. 402/2013 allow a simplified approach for the safety approval of\ncomputer vision systems for driverless trains, if they have 'similar' functions\nand interfaces as the replaced human driver. The human driver is not replaced\none-to-one by a technical system - only a limited set of cognitive functions\nare replaced. However, performance in the most challenging function, obstacle\ndetection, is difficult to quantify due to the deficiency of published\nmeasurement results. This article summarizes the data published so far. This\narticle also goes a long way to remedy this situation by providing a new public\nand anonymized dataset of 711 train driver performance measurements from\ncontrolled experiments. The measurements are made for different speeds,\nobstacle sizes, train protection systems and obstacle color contrasts\nrespectively. The measured values are reaction time and distance to the\nobstacle. The goal of this paper is an unbiased and exhaustive description of\nthe presented dataset for research, standardization and regulation. The dataset\nwith supplementing information and literature is published on\nhttps://data.fid-move.de/de/dataset/atosensedata"}
{"id": "2407.17280", "pdf": "https://arxiv.org/pdf/2407.17280", "abs": "https://arxiv.org/abs/2407.17280", "authors": ["Bertille Follain", "Francis Bach"], "title": "Enhanced Feature Learning via Regularisation: Integrating Neural Networks and Kernel Methods", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose a new method for feature learning and function estimation in\nsupervised learning via regularised empirical risk minimisation. Our approach\nconsiders functions as expectations of Sobolev functions over all possible\none-dimensional projections of the data. This framework is similar to kernel\nridge regression, where the kernel is $\\mathbb{E}_w ( k^{(B)}(w^\\top x,w^\\top\nx^\\prime))$, with $k^{(B)}(a,b) := \\min(|a|, |b|)\\mathds{1}_{ab>0}$ the\nBrownian kernel, and the distribution of the projections $w$ is learnt. This\ncan also be viewed as an infinite-width one-hidden layer neural network,\noptimising the first layer's weights through gradient descent and explicitly\nadjusting the non-linearity and weights of the second layer. We introduce a\ngradient-based computational method for the estimator, called Brownian Kernel\nNeural Network (BKerNN), using particles to approximate the expectation, where\nthe positive homogeneity of the Brownian kernel \\red{leads to improved\nrobustness to local minima}. Using Rademacher complexity, we show that BKerNN's\nexpected risk converges to the minimal risk with explicit high-probability\nrates of $O( \\min((d/n)^{1/2}, n^{-1/6}))$ (up to logarithmic factors).\nNumerical experiments confirm our optimisation intuitions, and BKerNN\noutperforms kernel ridge regression, and favourably compares to a one-hidden\nlayer neural network with ReLU activations in various settings and real data\nsets."}
{"id": "2504.20438", "pdf": "https://arxiv.org/pdf/2504.20438", "abs": "https://arxiv.org/abs/2504.20438", "authors": ["Ziyang Xu", "Kangsheng Duan", "Xiaolei Shen", "Zhifeng Ding", "Wenyu Liu", "Xiaohu Ruan", "Xiaoxin Chen", "Xinggang Wang"], "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency", "categories": ["cs.CV"], "comment": "https://hustvl.github.io/PixelHacker", "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/PixelHacker."}
{"id": "2410.02833", "pdf": "https://arxiv.org/pdf/2410.02833", "abs": "https://arxiv.org/abs/2410.02833", "authors": ["Francisco Daunas", "Iñaki Esnaola", "Samir M. Perlaza", "H. Vincent Poor"], "title": "Asymmetry of the Relative Entropy in the Regularization of Empirical Risk Minimization", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "The effect of relative entropy asymmetry is analyzed in the context of\nempirical risk minimization (ERM) with relative entropy regularization\n(ERM-RER). Two regularizations are considered: $(a)$ the relative entropy of\nthe measure to be optimized with respect to a reference measure (Type-I\nERM-RER); and $(b)$ the relative entropy of the reference measure with respect\nto the measure to be optimized (Type-II ERM-RER). The main result is the\ncharacterization of the solution to the Type-II ERM-RER problem and its key\nproperties. By comparing the well-understood Type-I ERM-RER with Type-II\nERM-RER, the effects of entropy asymmetry are highlighted. The analysis shows\nthat in both cases, regularization by relative entropy forces the solution's\nsupport to collapse into the support of the reference measure, introducing a\nstrong inductive bias that negates the evidence provided by the training data.\nFinally, it is shown that Type-II regularization is equivalent to Type-I\nregularization with an appropriate transformation of the empirical risk\nfunction."}
{"id": "2504.20948", "pdf": "https://arxiv.org/pdf/2504.20948", "abs": "https://arxiv.org/abs/2504.20948", "authors": ["Yanghui Song", "Chengfu Yang"], "title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition", "categories": ["cs.CV"], "comment": "9 pages, 14 figures, 2025 3rd International Conference on Algorithms,\n  Mathematical Modeling and Machinery Processing (AMMMP 2025)", "summary": "Given the severe challenges confronting the global growth security of\neconomic crops, precise identification and prevention of plant diseases has\nemerged as a critical issue in artificial intelligence-enabled agricultural\ntechnology. To address the technical challenges in plant disease recognition,\nincluding small-sample learning, leaf occlusion, illumination variations, and\nhigh inter-class similarity, this study innovatively proposes a Dynamic\nDual-Stream Fusion Network (DS_FusionNet). The network integrates a\ndual-backbone architecture, deformable dynamic fusion modules, and\nbidirectional knowledge distillation strategy, significantly enhancing\nrecognition accuracy. Experimental results demonstrate that DS_FusionNet\nachieves classification accuracies exceeding 90% using only 10% of the\nPlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the\ncomplex PlantWild dataset, exhibiting exceptional generalization capabilities.\nThis research not only provides novel technical insights for fine-grained image\nclassification but also establishes a robust foundation for precise\nidentification and management of agricultural diseases."}
{"id": "2411.00625", "pdf": "https://arxiv.org/pdf/2411.00625", "abs": "https://arxiv.org/abs/2411.00625", "authors": ["Zeyuan Ma", "Hongshu Guo", "Yue-Jiao Gong", "Jun Zhang", "Kay Chen Tan"], "title": "Toward Automated Algorithm Design: A Survey and Practical Guide to Meta-Black-Box-Optimization", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "In this survey, we introduce Meta-Black-Box-Optimization~(MetaBBO) as an\nemerging avenue within the Evolutionary Computation~(EC) community, which\nincorporates Meta-learning approaches to assist automated algorithm design.\nDespite the success of MetaBBO, the current literature provides insufficient\nsummaries of its key aspects and lacks practical guidance for implementation.\nTo bridge this gap, we offer a comprehensive review of recent advances in\nMetaBBO, providing an in-depth examination of its key developments. We begin\nwith a unified definition of the MetaBBO paradigm, followed by a systematic\ntaxonomy of various algorithm design tasks, including algorithm selection,\nalgorithm configuration, solution manipulation, and algorithm generation.\nFurther, we conceptually summarize different learning methodologies behind\ncurrent MetaBBO works, including reinforcement learning, supervised learning,\nneuroevolution, and in-context learning with Large Language Models. A\ncomprehensive evaluation of the latest representative MetaBBO methods is then\ncarried out, alongside an experimental analysis of their optimization\nperformance, computational efficiency, and generalization ability. Based on the\nevaluation results, we meticulously identify a set of core designs that enhance\nthe generalization and learning effectiveness of MetaBBO. Finally, we outline\nthe vision for the field by providing insight into the latest trends and\npotential future directions. Relevant literature will be continuously collected\nand updated at https://github.com/MetaEvo/Awesome-MetaBBO."}
{"id": "2412.03118", "pdf": "https://arxiv.org/pdf/2412.03118", "abs": "https://arxiv.org/abs/2412.03118", "authors": ["Ruiping Liu", "Jiaming Zhang", "Angela Schön", "Karin Müller", "Junwei Zheng", "Kailun Yang", "Anhong Guo", "Kathrin Gerling", "Rainer Stiefelhagen"], "title": "ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object Search by Blind People", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Searching for objects in unfamiliar scenarios is a challenging task for blind\npeople. It involves specifying the target object, detecting it, and then\ngathering detailed information according to the user's intent. However,\nexisting description- and detection-based assistive technologies do not\nsufficiently support the multifaceted nature of interactive object search\ntasks. We present ObjectFinder, an open-vocabulary wearable assistive system\nfor interactive object search by blind people. ObjectFinder allows users to\nquery target objects using flexible wording. Once the target object is\ndetected, it provides egocentric localization information in real-time,\nincluding distance and direction. Users can then initiate different branches to\ngather detailed information based on their intent towards the target object,\nsuch as navigating to it or perceiving its surroundings. ObjectFinder is\npowered by a seamless combination of open-vocabulary models, namely an\nopen-vocabulary object detector and a multimodal large language model. The\nObjectFinder design concept and its development were carried out in\ncollaboration with a blind co-designer. To evaluate ObjectFinder, we conducted\nan exploratory user study with eight blind participants. We compared\nObjectFinder to BeMyAI and Google Lookout, popular description- and\ndetection-based assistive applications. Our findings indicate that most\nparticipants felt more independent with ObjectFinder and preferred it for\nobject search, as it enhanced scene context gathering and navigation, and\nallowed for active target identification. Finally, we discuss the implications\nfor future assistive systems to support interactive object search."}
{"id": "2412.13722", "pdf": "https://arxiv.org/pdf/2412.13722", "abs": "https://arxiv.org/abs/2412.13722", "authors": ["Andrew G. T. Pyo", "Yuta Nagano", "Martina Milighetti", "James Henderson", "Curtis G. Callan Jr.", "Benny Chain", "Ned S. Wingreen", "Andreas Tiffeau-Mayer"], "title": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity Rules", "categories": ["q-bio.BM", "cs.LG", "physics.bio-ph"], "comment": "17 pages, 12 figures", "summary": "The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we systematically characterize how co-specificity depends on the type\nand position of amino-acid differences between receptors. We also demonstrate\nthat the inferred rules generalize to ligands highly dissimilar to any seen\nduring training. Our analysis reveals that matching of steric properties\nbetween substituted amino acids is more important for receptor co-specificity\nred than the hydrophobic properties that prominently determine evolutionary\nsubstitutability. Our analysis also quantifies the substantial importance of\npositions not in direct contact with the peptide for specificity. These\nfindings highlight the potential for data-driven approaches to uncover the\nmolecular mechanisms underpinning the specificity of adaptive immune responses."}
{"id": "2502.08528", "pdf": "https://arxiv.org/pdf/2502.08528", "abs": "https://arxiv.org/abs/2502.08528", "authors": ["Ao liu", "Zelin Zhang", "Songbai Chen", "Cuihong Wen", "Jieci Wang"], "title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation", "categories": ["astro-ph.GA", "cs.CV"], "comment": "19 pages, 11 figures", "summary": "The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting."}
{"id": "2502.02304", "pdf": "https://arxiv.org/pdf/2502.02304", "abs": "https://arxiv.org/abs/2502.02304", "authors": ["Fotis I. Giasemis", "Vladimir Lončar", "Bertrand Granado", "Vladimir Vava Gligorov"], "title": "Comparative Analysis of FPGA and GPU Performance for Machine Learning-Based Track Reconstruction at LHCb", "categories": ["hep-ex", "cs.DC", "cs.LG", "physics.ins-det"], "comment": null, "summary": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power."}
{"id": "2503.06669", "pdf": "https://arxiv.org/pdf/2503.06669", "abs": "https://arxiv.org/abs/2503.06669", "authors": ["AgiBot-World-Contributors", "Qingwen Bu", "Jisong Cai", "Li Chen", "Xiuqi Cui", "Yan Ding", "Siyuan Feng", "Shenyuan Gao", "Xindong He", "Xuan Hu", "Xu Huang", "Shu Jiang", "Yuxin Jiang", "Cheng Jing", "Hongyang Li", "Jialu Li", "Chiming Liu", "Yi Liu", "Yuxiang Lu", "Jianlan Luo", "Ping Luo", "Yao Mu", "Yuehan Niu", "Yixuan Pan", "Jiangmiao Pang", "Yu Qiao", "Guanghui Ren", "Cheng Ruan", "Jiaqi Shan", "Yongjian Shen", "Chengshi Shi", "Mingkang Shi", "Modi Shi", "Chonghao Sima", "Jianheng Song", "Huijie Wang", "Wenhao Wang", "Dafeng Wei", "Chengen Xie", "Guo Xu", "Junchi Yan", "Cunbiao Yang", "Lei Yang", "Shukai Yang", "Maoqing Yao", "Jia Zeng", "Chi Zhang", "Qinglin Zhang", "Bin Zhao", "Chengyue Zhao", "Jiaqi Zhao", "Jianchao Zhu"], "title": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project website: https://agibot-world.com/. Github repo:\n  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered\n  alphabetically by surname, with detailed contributions provided in the\n  appendix", "summary": "We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence."}
{"id": "2502.07836", "pdf": "https://arxiv.org/pdf/2502.07836", "abs": "https://arxiv.org/abs/2502.07836", "authors": ["Luoting Zhuang", "Stephen H. Park", "Steven J. Skates", "Ashley E. Prosper", "Denise R. Aberle", "William Hsu"], "title": "Advancing Precision Oncology Through Modeling of Longitudinal and Multimodal Data", "categories": ["q-bio.QM", "cs.LG"], "comment": "This work has been submitted to the IEEE RBME for potential\n  publication", "summary": "Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology."}
{"id": "2503.08061", "pdf": "https://arxiv.org/pdf/2503.08061", "abs": "https://arxiv.org/abs/2503.08061", "authors": ["DongHeun Han", "Byungmin Kim", "RoUn Lee", "KyeongMin Kim", "Hyoseok Hwang", "HyeongYeop Kang"], "title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation", "categories": ["cs.RO", "cs.GR", "cs.HC", "cs.LG"], "comment": "11 pages, 11 figures. Accepted to SIGGRAPH Conference Papers '25.\n  Project page: https://han-dongheun.github.io/ForceGrip", "summary": "Realistic Hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. Demo videos are available as supplementary material and the code is\nprovided at https://han-dongheun.github.io/ForceGrip."}
{"id": "2503.20803", "pdf": "https://arxiv.org/pdf/2503.20803", "abs": "https://arxiv.org/abs/2503.20803", "authors": ["Bamidele Ajayi", "Basel Barakat", "Ken McGarry"], "title": "Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with Machine Learning Classifiers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "This paper assesses the performance of five machine learning classifiers:\nDecision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest\nusing latent representations learned by a Variational Autoencoder from malware\ndatasets. Results from the experiments conducted on different training-test\nsplits with different random seeds reveal that all the models perform well in\ndetecting malware with ensemble methods (LightGBM and Random Forest) performing\nslightly better than the rest. In addition, the use of latent features reduces\nthe computational cost of the model and the need for extensive hyperparameter\ntuning for improved efficiency of the model for deployment. Statistical tests\nshow that these improvements are significant, and thus, the practical relevance\nof integrating latent space representation with traditional classifiers for\neffective malware detection in cybersecurity is established."}
{"id": "2504.10507", "pdf": "https://arxiv.org/pdf/2504.10507", "abs": "https://arxiv.org/abs/2504.10507", "authors": ["Anirudhan Badrinath", "Prabhat Agarwal", "Laksh Bhasin", "Jaewon Yang", "Jiajing Xu", "Charles Rosenberg"], "title": "PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems", "categories": ["cs.IR", "cs.LG"], "comment": "Submitted to KDD ADS 2025", "summary": "Generative retrieval methods utilize generative sequential modeling\ntechniques, such as transformers, to generate candidate items for recommender\nsystems. These methods have demonstrated promising results in academic\nbenchmarks, surpassing traditional retrieval models like two-tower\narchitectures. However, current generative retrieval methods lack the\nscalability required for industrial recommender systems, and they are\ninsufficiently flexible to satisfy the multiple metric requirements of modern\nsystems. This paper introduces PinRec, a novel generative retrieval model\ndeveloped for applications at Pinterest. PinRec utilizes outcome-conditioned\ngeneration, enabling modelers to specify how to balance various outcome\nmetrics, such as the number of saves and clicks, to effectively align with\nbusiness goals and user exploration. Additionally, PinRec incorporates\nmulti-token generation to enhance output diversity while optimizing generation.\nOur experiments demonstrate that PinRec can successfully balance performance,\ndiversity, and efficiency, delivering a significant positive impact to users\nusing generative models. This paper marks a significant milestone in generative\nretrieval, as it presents, to our knowledge, the first rigorous study on\nimplementing generative retrieval at the scale of Pinterest."}
{"id": "2504.19342", "pdf": "https://arxiv.org/pdf/2504.19342", "abs": "https://arxiv.org/abs/2504.19342", "authors": ["Nan Lu", "Ethan X. Fang", "Junwei Lu"], "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a pivotal\nparadigm in artificial intelligence to align large models with human\npreferences. In this paper, we propose a novel statistical framework to\nsimultaneously conduct the online decision-making and statistical inference on\nthe optimal model using human preference data based on dynamic contextual\ninformation. Our approach introduces an efficient decision strategy that\nachieves both the optimal regret bound and the asymptotic distribution of the\nestimators. A key challenge in RLHF is handling the dependent online human\npreference outcomes with dynamic contexts. To address this, in the\nmethodological aspect, we propose a two-stage algorithm starting with\n$\\epsilon$-greedy followed by exploitations; in the theoretical aspect, we\ntailor anti-concentration inequalities and matrix martingale concentration\ntechniques to derive the uniform estimation rate and asymptotic normality of\nthe estimators using dependent samples from both stages. Extensive simulation\nresults demonstrate that our method outperforms state-of-the-art strategies. We\napply the proposed framework to analyze the human preference data for ranking\nlarge language models on the Massive Multitask Language Understanding dataset,\nyielding insightful results on the performance of different large language\nmodels for medical anatomy knowledge."}
{"id": "2504.20127", "pdf": "https://arxiv.org/pdf/2504.20127", "abs": "https://arxiv.org/abs/2504.20127", "authors": ["Huiyang Hong", "Xinkai Wu", "Hongyu Sun", "Chaoyang Xie", "Qi Wang", "Yuquan Li"], "title": "Learning Hierarchical Interaction for Accurate Molecular Property Prediction", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Discovering molecules with desirable molecular properties, including ADMET\n(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of\ngreat importance in drug discovery. Existing approaches typically employ deep\nlearning models, such as Graph Neural Networks (GNNs) and Transformers, to\npredict these molecular properties by learning from diverse chemical\ninformation. However, these models often fail to efficiently capture and\nutilize the hierarchical nature of molecular structures, and lack mechanisms\nfor effective interaction among multi-level features. To address these\nlimitations, we propose a Hierarchical Interaction Message Passing Mechanism,\nwhich serves as the foundation of our novel model, HimNet. Our method enables\ninteraction-aware representation learning across atomic, motif, and molecular\nlevels via hierarchical attention-guided message passing. This design allows\nHimNet to effectively balance global and local information, ensuring rich and\ntask-relevant feature extraction for downstream property prediction tasks, such\nas Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple\nbenchmark datasets demonstrate that HimNet achieves the best or near-best\nperformance in most molecular property prediction tasks. Furthermore, our\nmethod exhibits promising hierarchical interpretability, aligning well with\nchemical intuition on representative molecules. We believe that HimNet offers\nan accurate and efficient solution for molecular activity and ADMET property\nprediction, contributing significantly to advanced decision-making in the early\nstages of drug discovery."}
{"id": "2504.20877", "pdf": "https://arxiv.org/pdf/2504.20877", "abs": "https://arxiv.org/abs/2504.20877", "authors": ["Meltem Tatlı", "Arpan Mukherjee", "Prashanth L. A.", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms", "categories": ["stat.ML", "cs.LG"], "comment": "An earlier version of this manuscript, which focused on\n  risk-sensitive bandits, has appeared in the Proceedings of the 2025\n  International Conference on Artificial Intelligence and Statistics (AISTATS)", "summary": "The objective of canonical multi-armed bandits is to identify and repeatedly\nselect an arm with the largest reward, often in the form of the expected value\nof the arm's probability distribution. Such a utilitarian perspective and focus\non the probability models' first moments, however, is agnostic to the\ndistributions' tail behavior and their implications for variability and risks\nin decision-making. This paper introduces a principled framework for shifting\nfrom expectation-based evaluation to an alternative reward formulation, termed\na preference metric (PM). The PMs can place the desired emphasis on different\nreward realization and can encode a richer modeling of preferences that\nincorporate risk aversion, robustness, or other desired attitudes toward\nuncertainty. A fundamentally distinct observation in such a PM-centric\nperspective is that designing bandit algorithms will have a significantly\ndifferent principle: as opposed to the reward-based models in which the optimal\nsampling policy converges to repeatedly sampling from the single best arm, in\nthe PM-centric framework the optimal policy converges to selecting a mix of\narms based on specific mixing weights. Designing such mixture policies departs\nfrom the principles for designing bandit algorithms in significant ways,\nprimarily because of uncountable mixture possibilities. The paper formalizes\nthe PM-centric framework and presents two algorithm classes (horizon-dependent\nand anytime) that learn and track mixtures in a regret-efficient fashion. These\nalgorithms have two distinctions from their canonical counterparts: (i) they\ninvolve an estimation routine to form reliable estimates of optimal mixtures,\nand (ii) they are equipped with tracking mechanisms to navigate arm selection\nfractions to track the optimal mixtures. These algorithms' regret guarantees\nare investigated under various algebraic forms of the PMs."}
