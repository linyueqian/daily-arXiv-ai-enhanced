{"id": "2504.11468", "pdf": "https://arxiv.org/pdf/2504.11468", "abs": "https://arxiv.org/abs/2504.11468", "authors": ["Hardy Chen", "Haoqin Tu", "Fali Wang", "Hui Liu", "Xianfeng Tang", "Xinya Du", "Yuyin Zhou", "Cihang Xie"], "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This work revisits the dominant supervised fine-tuning (SFT) then\nreinforcement learning (RL) paradigm for training Large Vision-Language Models\n(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent\nRL by inducing ``pseudo reasoning paths'' imitated from expert models. While\nthese paths may resemble the native reasoning paths of RL models, they often\ninvolve prolonged, hesitant, less informative steps, and incorrect reasoning.\nTo systematically study this effect, we introduce VLAA-Thinking, a new\nmultimodal dataset designed to support reasoning in LVLMs. Constructed via a\nsix-step pipeline involving captioning, reasoning distillation, answer rewrite\nand verification, VLAA-Thinking comprises high-quality, step-by-step visual\nreasoning traces for SFT, along with a more challenging RL split from the same\ndata source. Using this dataset, we conduct extensive experiments comparing\nSFT, RL and their combinations. Results show that while SFT helps models learn\nreasoning formats, it often locks aligned models into imitative, rigid\nreasoning modes that impede further learning. In contrast, building on the\nGroup Relative Policy Optimization (GRPO) with a novel mixed reward module\nintegrating both perception and cognition signals, our RL approach fosters more\ngenuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on\nQwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard\n(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)\namong 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope\nour findings provide valuable insights in developing reasoning-capable LVLMs\nand can inform future research in this area.", "AI": {"tldr": "SFT in LVLMs can hinder RL by inducing pseudo reasoning paths. VLAA-Thinking dataset and GRPO-based RL improve reasoning, achieving top performance on Open LMM Reasoning Leaderboard.", "motivation": "To address how SFT undermines RL in LVLMs by creating pseudo reasoning paths and to improve reasoning capabilities.", "method": "Introduce VLAA-Thinking dataset, compare SFT and RL, and use GRPO with a mixed reward module for adaptive reasoning.", "result": "VLAA-Thinker (Qwen2.5VL 3B) achieves top-1 performance on Open LMM Reasoning Leaderboard, surpassing SOTA by 1.8%.", "conclusion": "SFT locks models into rigid reasoning, while RL with GRPO fosters genuine reasoning. Insights can guide future LVLM research."}}
{"id": "2504.11536", "pdf": "https://arxiv.org/pdf/2504.11536", "abs": "https://arxiv.org/abs/2504.11536", "authors": ["Jiazhan Feng", "Shijue Huang", "Xingwei Qu", "Ge Zhang", "Yujia Qin", "Baoquan Zhong", "Chengquan Jiang", "Jinxin Chi", "Wanjun Zhong"], "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.", "AI": {"tldr": "ReTool enhances reasoning models by integrating real-time code execution and automated RL, outperforming text-based baselines in complex tasks like mathematical reasoning.", "motivation": "Existing reasoning models struggle with structured problem-solving (e.g., geometric reasoning), while computational tools like code interpreters excel. ReTool aims to bridge this gap.", "method": "ReTool combines dynamic interleaving of code execution with natural language reasoning and an automated RL paradigm for tool invocation. Training involves synthetic data generation and RL refinement.", "result": "ReTool-32B achieves 67% accuracy on AIME (vs. 40% for baseline) and 72.5% in extended settings, surpassing OpenAI's o1-preview by 27.9%. Emergent behaviors like code self-correction are observed.", "conclusion": "ReTool demonstrates the potential of outcome-driven tool integration for advancing complex reasoning and hybrid neuro-symbolic systems."}}
{"id": "2504.11582", "pdf": "https://arxiv.org/pdf/2504.11582", "abs": "https://arxiv.org/abs/2504.11582", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "categories": ["cs.CL"], "comment": "38 pages, 7 figures", "summary": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics.", "AI": {"tldr": "AskQE is a framework for detecting critical MT errors via question generation and answering, aiding monolingual users in evaluating translations without target language knowledge.", "motivation": "Existing MT error detection and QE techniques fail to address the practical scenario of monolingual users assessing translation quality.", "method": "AskQE uses LLaMA-3 70B and entailed facts for question generation, tested on synthetic (ContraTICO) and real (BioMQM) MT error datasets.", "result": "AskQE outperforms other QE metrics in correlation (Kendall's Tau) and decision accuracy with human ratings.", "conclusion": "AskQE effectively helps monolingual users evaluate MT outputs, offering actionable feedback for decision-making."}}
{"id": "2504.11626", "pdf": "https://arxiv.org/pdf/2504.11626", "abs": "https://arxiv.org/abs/2504.11626", "authors": ["Ozan \u0130rsoy", "Pengxiang Cheng", "Jennifer L. Chen", "Daniel Preo\u0163iuc-Pietro", "Shiyue Zhang", "Duccio Pappadopulo"], "title": "Improving Instruct Models for Free: A Study on Partial Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Author ordering chosen at random", "summary": "Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model being overly conversational or verbose. This, in turn, can\nlead to degradation of in-context few-shot learning performance. In this work,\nwe study the performance trajectory between base and instruct models by scaling\ndown the strength of instruction-tuning via the partial adaption method. We\nshow that, across several model families and model sizes, reducing the strength\nof instruction-tuning results in material improvement on a few-shot in-context\nlearning benchmark covering a variety of classic natural language tasks. This\ncomes at the cost of losing some degree of instruction following ability as\nmeasured by AlpacaEval. Our study shines light on the potential trade-off\nbetween in-context learning and instruction following abilities that is worth\nconsidering in practice.", "AI": {"tldr": "Reducing instruction-tuning strength improves few-shot learning but weakens instruction-following ability, revealing a trade-off.", "motivation": "To investigate the trade-off between instruction-following ability and in-context few-shot learning performance in instruct models.", "method": "Partial adaptation method to scale down instruction-tuning strength across various model families and sizes.", "result": "Reduced instruction-tuning improves few-shot learning performance but diminishes instruction-following ability.", "conclusion": "A trade-off exists between in-context learning and instruction-following, requiring practical consideration."}}
{"id": "2504.12005", "pdf": "https://arxiv.org/pdf/2504.12005", "abs": "https://arxiv.org/abs/2504.12005", "authors": ["Soobin Suh", "Dabi Ahn", "Heewoong Park", "Jonghun Park"], "title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "2 pages, Machine Learning in Speech and Language Processing Workshop\n  (MLSLP) 2018", "summary": "Voice conversion is a task of synthesizing an utterance with target speaker's\nvoice while maintaining linguistic information of the source utterance. While a\nspeaker can produce varying utterances from a single script with different\nintonations, conventional voice conversion models were limited to producing\nonly one result per source input. To overcome this limitation, we propose a\nnovel approach for voice conversion with diverse intonations using conditional\nvariational autoencoder (CVAE). Experiments have shown that the speaker's style\nfeature can be mapped into a latent space with Gaussian distribution. We have\nalso been able to convert voices with more diverse intonation by making the\nposterior of the latent space more complex with inverse autoregressive flow\n(IAF). As a result, the converted voice not only has a diversity of\nintonations, but also has better sound quality than the model without CVAE.", "AI": {"tldr": "Proposes a CVAE-based voice conversion method to generate diverse intonations, improving sound quality and variety.", "motivation": "Conventional voice conversion models produce only one result per input, lacking intonation diversity.", "method": "Uses conditional variational autoencoder (CVAE) and inverse autoregressive flow (IAF) to model complex latent spaces.", "result": "Achieves diverse intonations and better sound quality compared to non-CVAE models.", "conclusion": "The CVAE-based approach successfully enhances voice conversion with varied intonations and improved quality."}}
{"id": "2504.11622", "pdf": "https://arxiv.org/pdf/2504.11622", "abs": "https://arxiv.org/abs/2504.11622", "authors": ["Seyyed Ali Ayati", "Jin Hyun Park", "Yichen Cai", "Marcus Botacin"], "title": "Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction", "categories": ["cs.CR", "cs.SD", "eess.AS"], "comment": "Length: 13 pages Figures: 5 figures Tables: 7 tables Keywords:\n  Acoustic side-channel attacks, machine learning, Visual Transformers, Large\n  Language Models (LLMs), security Conference: Accepted at the 19th USENIX WOOT\n  Conference on Offensive Technologies (WOOT '25). Licensing: This paper is\n  submitted under the CC BY Creative Commons Attribution license. arXiv admin\n  note: text overlap with arXiv:2502.09782", "summary": "The large integration of microphones into devices increases the opportunities\nfor Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture\nkeystrokes' audio signals that might reveal sensitive information. However, the\ncurrent State-Of-The-Art (SOTA) models for ASCAs, including Convolutional\nNeural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit\nlimited robustness under realistic noisy conditions. Solving this problem\nrequires either: (i) an increased model's capacity to infer contextual\ninformation from longer sequences, allowing the model to learn that an\ninitially noisily typed word is the same as a futurely collected non-noisy\nword, or (ii) an approach to fix misidentified information from the contexts,\nas one does not type random words, but the ones that best fit the conversation\ncontext. In this paper, we demonstrate that both strategies are viable and\ncomplementary solutions for making ASCAs practical. We observed that no\nexisting solution leverages advanced transformer architectures' power for these\ntasks and propose that: (i) Visual Transformers (VTs) are the candidate\nsolutions for capturing long-term contextual information and (ii)\ntransformer-powered Large Language Models (LLMs) are the candidate solutions to\nfix the ``typos'' (mispredictions) the model might make. Thus, we here present\nthe first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes\nwhen compared to the previous CNN benchmark. Second, we demonstrate that LLMs\ncan mitigate the impact of real-world noise. Evaluations on the natural\nsentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA\npipeline boosts the performance of error-correction tasks; and (ii) the\ncomparable performance can be attained by a lightweight, fine-tuned smaller LLM\n(67 times smaller than GPT-4o), using...", "AI": {"tldr": "The paper proposes integrating Visual Transformers (VTs) and Large Language Models (LLMs) to enhance Acoustic Side-Channel Attacks (ASCAs) by improving robustness under noisy conditions and correcting mispredictions.", "motivation": "Current ASCA models lack robustness in noisy environments, necessitating better contextual understanding and error correction.", "method": "Uses VTs for long-term contextual learning and LLMs (like GPT-4o or smaller fine-tuned models) for error correction.", "result": "VTs outperform CNNs in keystroke classification, and LLMs significantly improve error correction, even with smaller models.", "conclusion": "Combining VTs and LLMs provides a practical solution for robust ASCAs, with lightweight LLMs offering comparable performance to larger models."}}
{"id": "2504.11491", "pdf": "https://arxiv.org/pdf/2504.11491", "abs": "https://arxiv.org/abs/2504.11491", "authors": ["Mansoor Hayat", "Supavadee Aramvith", "Subrata Bhattacharjee", "Nouman Ahmad"], "title": "Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted for presentation in the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "Accurate segmentation of abdominal adipose tissue, including subcutaneous\n(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is\nessential for understanding body composition and associated health risks such\nas type 2 diabetes and cardiovascular disease. This study proposes Attention\nGhostUNet++, a novel deep learning model incorporating Channel, Spatial, and\nDepth Attention mechanisms into the Ghost UNet++ bottleneck for automated,\nprecise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model\nachieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for\nliver segmentation, surpassing baseline models. Despite minor limitations in\nboundary detail segmentation, the proposed model significantly enhances feature\nrefinement, contextual understanding, and computational efficiency, offering a\nrobust solution for body composition analysis. The implementation of the\nproposed Attention GhostUNet++ model is available\nat:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus.", "AI": {"tldr": "The paper introduces Attention GhostUNet++, a deep learning model for precise segmentation of abdominal adipose tissue and liver, outperforming baseline models with high Dice coefficients.", "motivation": "Accurate segmentation of SAT, VAT, and liver is crucial for assessing body composition and health risks like diabetes and cardiovascular disease.", "method": "The model integrates Channel, Spatial, and Depth Attention mechanisms into Ghost UNet++ for automated segmentation, evaluated on AATTCT-IDS and LiTS datasets.", "result": "Achieved Dice coefficients of 0.9430 (VAT), 0.9639 (SAT), and 0.9652 (liver), surpassing baselines.", "conclusion": "Attention GhostUNet++ improves feature refinement, contextual understanding, and efficiency, offering a robust solution despite minor boundary detail limitations."}}
{"id": "2504.11469", "pdf": "https://arxiv.org/pdf/2504.11469", "abs": "https://arxiv.org/abs/2504.11469", "authors": ["Guillaume Garret", "Antoine Vacavant", "Carole Frindel"], "title": "Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Open access version of an article submitted to Medical Image\n  Understanding and Analysis (MIUA) 2025", "summary": "Deep learning models have achieved impressive performance in medical image\nsegmentation, yet their black-box nature limits clinical adoption. In vascular\napplications, trustworthy segmentation should rely on both local image cues and\nglobal anatomical structures, such as vessel connectivity or branching.\nHowever, the extent to which models leverage such global context remains\nunclear. We present a novel explainability pipeline for 3D vessel segmentation,\ncombining gradient-based attribution with graph-guided point selection and a\nblob-based analysis of Saliency maps. Using vascular graphs extracted from\nground truth, we define anatomically meaningful points of interest (POIs) and\nassess the contribution of input voxels via Saliency maps. These are analyzed\nat both global and local scales using a custom blob detector. Applied to IRCAD\nand Bullitt datasets, our analysis shows that model decisions are dominated by\nhighly localized attribution blobs centered near POIs. Attribution features\nshow little correlation with vessel-level properties such as thickness,\ntubularity, or connectivity -- suggesting limited use of global anatomical\nreasoning. Our results underline the importance of structured explainability\ntools and highlight the current limitations of segmentation models in capturing\nglobal vascular context.", "AI": {"tldr": "The paper introduces an explainability pipeline for 3D vessel segmentation to assess how models use global anatomical context, revealing a reliance on localized cues over global structures.", "motivation": "To address the lack of transparency in deep learning models for medical image segmentation, especially in vascular applications where global anatomical context is crucial.", "method": "A novel pipeline combining gradient-based attribution, graph-guided point selection, and blob-based analysis of saliency maps, applied to IRCAD and Bullitt datasets.", "result": "Models rely heavily on localized attribution blobs near points of interest, with little correlation to global vessel properties like connectivity or thickness.", "conclusion": "The study highlights the need for structured explainability tools and reveals current models' limitations in leveraging global anatomical context."}}
{"id": "2504.11460", "pdf": "https://arxiv.org/pdf/2504.11460", "abs": "https://arxiv.org/abs/2504.11460", "authors": ["Tobias Hallmen", "Robin-Nico Kampa", "Fabian Deuser", "Norbert Oswald", "Elisabeth Andr\u00e9"], "title": "Semantic Matters: Multimodal Features for Affective Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this study, we present our methodology for two tasks: the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry\nIntensity (EMI) Estimation Challenge, both conducted as part of the 8th\nWorkshop and Competition on Affective & Behavior Analysis in-the-wild. Building\non previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast\ndataset to extract various audio features, capturing both linguistic and\nparalinguistic information. Our approach incorporates a\nvalence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like\nencoder, and a vision transformer (ViT) with predictions subsequently processed\nthrough a long short-term memory (LSTM) architecture for temporal modeling. In\nthis iteration, we integrate the textual and visual modality into our analysis,\nrecognizing that semantic content provides valuable contextual cues and\nunderscoring that the meaning of speech often conveys more critical insights\nthan its acoustic counterpart alone. Fusing in the vision modality helps in\nsome cases to interpret the textual modality more precisely. This combined\napproach yields significant performance improvements over baseline methods.", "AI": {"tldr": "A multimodal approach using Wav2Vec 2.0, BERT-like encoder, ViT, and LSTM improves performance in BAH and EMI challenges by integrating audio, text, and visual modalities.", "motivation": "To enhance recognition of behavioral ambivalence/hesitancy and emotional mimicry intensity by leveraging multimodal data (audio, text, vision) for richer contextual understanding.", "method": "Combines Wav2Vec 2.0 for audio features, a BERT-like encoder for text, ViT for vision, and LSTM for temporal modeling, integrating all modalities.", "result": "Significant performance improvements over baseline methods in both tasks.", "conclusion": "Multimodal integration (audio, text, vision) is effective for behavioral and emotional analysis, outperforming unimodal approaches."}}
{"id": "2504.11476", "pdf": "https://arxiv.org/pdf/2504.11476", "abs": "https://arxiv.org/abs/2504.11476", "authors": ["Ritik Mishra", "Mushir Akhtar", "M. Tanveer"], "title": "CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines", "categories": ["cs.LG"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Restricted kernel machines (RKMs) represent a versatile and powerful\nframework within the kernel machine family, leveraging conjugate feature\nduality to address a wide range of machine learning tasks, including\nclassification, regression, and feature learning. However, their performance\ncan degrade significantly in the presence of noise and outliers, which\ncompromises robustness and predictive accuracy. In this paper, we propose a\nnovel enhancement to the RKM framework by integrating a class-informed weighted\nfunction. This weighting mechanism dynamically adjusts the contribution of\nindividual training points based on their proximity to class centers and\nclass-specific characteristics, thereby mitigating the adverse effects of noisy\nand outlier data. By incorporating weighted conjugate feature duality and\nleveraging the Schur complement theorem, we introduce the class-informed\nrestricted kernel machine (CI-RKM), a robust extension of the RKM designed to\nimprove generalization and resilience to data imperfections. Experimental\nevaluations on benchmark datasets demonstrate that the proposed CI-RKM\nconsistently outperforms existing baselines, achieving superior classification\naccuracy and enhanced robustness against noise and outliers. Our proposed\nmethod establishes a significant advancement in the development of kernel-based\nlearning models, addressing a core challenge in the field.", "AI": {"tldr": "Proposes a class-informed weighted function to enhance Restricted Kernel Machines (RKMs), improving robustness against noise and outliers.", "motivation": "RKMs degrade in noisy or outlier-rich environments, limiting their effectiveness.", "method": "Integrates a class-informed weighting mechanism and Schur complement theorem to create CI-RKM.", "result": "CI-RKM outperforms baselines in classification accuracy and robustness.", "conclusion": "CI-RKM advances kernel-based learning by addressing noise and outlier challenges."}}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673", "abs": "https://arxiv.org/abs/2504.11673", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan"], "title": "Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses during the\nearly phases of survey design. While previous studies have examined whether\nmodels can reflect individual opinions or attitudes, we argue that a\n\\emph{higher-order} binding of virtual personas requires successfully\napproximating not only the opinions of a user as an identified member of a\ngroup, but also the nuanced ways in which that user perceives and evaluates\nthose outside the group. In particular, faithfully simulating how humans\nperceive different social groups is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies. Altogether, our work extends the\napplicability of LLMs beyond estimating individual self-opinions, enabling\ntheir use in a broader range of human studies.", "AI": {"tldr": "The paper introduces a method to enhance LLMs' ability to simulate human behavior by creating detailed virtual personas with synthetic backstories, improving their use in political science studies.", "motivation": "To address the need for LLMs to better approximate higher-order human behaviors, like inter-group perceptions, for applications in political science.", "method": "Proposes a novel methodology using multi-turn interview transcripts to generate rich, consistent backstories for virtual personas.", "result": "Virtual personas with these backstories closely replicate human responses (87% improvement in Wasserstein Distance) and match original study effect sizes.", "conclusion": "Extends LLMs' applicability beyond individual opinions, enabling broader use in human studies."}}
{"id": "2504.12272", "pdf": "https://arxiv.org/pdf/2504.12272", "abs": "https://arxiv.org/abs/2504.12272", "authors": ["Kong Ka Hing", "Mehran Behjati"], "title": "Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "This is a preprint version of a paper accepted and published in\n  Springer Lecture Notes in Networks and Systems. The final version is\n  available at https://doi.org/10.1007/978-981-96-3949-6_40", "summary": "Hornbills, an iconic species of Malaysia's biodiversity, face threats from\nhabi-tat loss, poaching, and environmental changes, necessitating accurate and\nreal-time population monitoring that is traditionally challenging and re-source\nintensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to\ntransform wildlife monitoring by enabling efficient, real-time da-ta analysis\ndirectly on edge devices. Addressing the challenge of wildlife conservation,\nthis research paper explores the pivotal role of machine learn-ing,\nspecifically TinyML, in the classification and monitoring of hornbill calls in\nMalaysia. Leveraging audio data from the Xeno-canto database, the study aims to\ndevelop a speech recognition system capable of identifying and classifying\nhornbill vocalizations. The proposed methodology involves pre-processing the\naudio data, extracting features using Mel-Frequency Energy (MFE), and deploying\nthe model on an Arduino Nano 33 BLE, which is adept at edge computing. The\nresearch encompasses foundational work, in-cluding a comprehensive\nintroduction, literature review, and methodology. The model is trained using\nEdge Impulse and validated through real-world tests, achieving high accuracy in\nhornbill species identification. The project underscores the potential of\nTinyML for environmental monitoring and its broader application in ecological\nconservation efforts, contributing to both the field of TinyML and wildlife\nconservation.", "AI": {"tldr": "The paper explores TinyML for real-time hornbill call classification in Malaysia, using audio data and edge devices to aid conservation.", "motivation": "Hornbills face threats like habitat loss and poaching, requiring efficient monitoring. Traditional methods are resource-intensive, prompting the use of TinyML for real-time, edge-based solutions.", "method": "Audio data from Xeno-canto is pre-processed, features extracted using MFE, and a model is deployed on Arduino Nano 33 BLE. Training and validation are done via Edge Impulse.", "result": "The model achieves high accuracy in identifying hornbill species, demonstrating TinyML's effectiveness for wildlife monitoring.", "conclusion": "TinyML shows promise for ecological conservation, offering scalable, efficient solutions for real-time wildlife monitoring."}}
{"id": "2504.12279", "pdf": "https://arxiv.org/pdf/2504.12279", "abs": "https://arxiv.org/abs/2504.12279", "authors": ["Mikhail Osipov"], "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Preprint. 11 pages, 3 figures, 2 tables, 8 appendices. Code and data\n  available upon request", "summary": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders", "AI": {"tldr": "A geometry-driven method normalizes dysarthric speech using Lie group transformations on spectrograms, improving ASR performance without pathological data.", "motivation": "To address distortions in dysarthric speech for better ASR performance without relying on pathological data.", "method": "Uses local Lie group transformations (time, frequency, amplitude) parameterized by scalar fields, inferred by a neural network trained on synthetic distortions.", "result": "Achieves up to 16% WER reduction on TORGO samples, with no degradation on clean speech.", "conclusion": "Introduces a principled, interpretable approach for robust ASR under motor speech disorders."}}
{"id": "2504.11515", "pdf": "https://arxiv.org/pdf/2504.11515", "abs": "https://arxiv.org/abs/2504.11515", "authors": ["Kangsheng Wang", "Chengwei Ye", "Huanzhen Zhang", "Linuo Xu", "Shuyan Liu"], "title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance.", "AI": {"tldr": "A multimodal framework combining visual, audio, and text features for personality prediction in short videos, outperforming existing methods.", "motivation": "Automating personality trait prediction is challenging; this paper addresses it by leveraging multimodal data for richer analysis.", "method": "Uses facial graphs, Geo-based two-stream networks (GCN, CNN), ResNet18, VGGFace, BiGRU with temporal attention, VGGish for audio, XLM-Roberta for text, and MLP for regression.", "result": "The framework outperforms state-of-the-art methods in personality trait prediction.", "conclusion": "The proposed multimodal approach effectively integrates diverse features for accurate personality analysis."}}
{"id": "2504.11474", "pdf": "https://arxiv.org/pdf/2504.11474", "abs": "https://arxiv.org/abs/2504.11474", "authors": ["Byunggun Kim", "Younghun Kwon"], "title": "Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of\nthe common mental diseases discovered not only in children but also in adults.\nIn this context, we propose a ADHD diagnosis transformer model that can\neffectively simultaneously find important brain spatiotemporal biomarkers from\nresting-state functional magnetic resonance (rs-fMRI). This model not only\nlearns spatiotemporal individual features but also learns the correlation with\nfull attention structures specialized in ADHD diagnosis. In particular, it\nfocuses on learning local blood oxygenation level dependent (BOLD) signals and\ndistinguishing important regions of interest (ROI) in the brain. Specifically,\nthe three proposed methods for ADHD diagnosis transformer are as follows.\nFirst, we design a CNN-based embedding block to obtain more expressive\nembedding features in brain region attention. It is reconstructed based on the\npreviously CNN-based ADHD diagnosis models for the transformer. Next, for\nindividual spatiotemporal feature attention, we change the attention method to\nlocal temporal attention and ROI-rank based masking. For the temporal features\nof fMRI, the local temporal attention enables to learn local BOLD signal\nfeatures with only simple window masking. For the spatial feature of fMRI,\nROI-rank based masking can distinguish ROIs with high correlation in ROI\nrelationships based on attention scores, thereby providing a more specific\nbiomarker for ADHD diagnosis. The experiment was conducted with various types\nof transformer models. To evaluate these models, we collected the data from 939\nindividuals from all sites provided by the ADHD-200 competition. Through this,\nthe spatiotemporal enhanced transformer for ADHD diagnosis outperforms the\nperformance of other different types of transformer variants. (77.78ACC\n76.60SPE 79.22SEN 79.30AUC)", "AI": {"tldr": "A transformer model for ADHD diagnosis using rs-fMRI learns spatiotemporal features and correlations, outperforming other variants with 77.78% accuracy.", "motivation": "ADHD is a common mental disorder in both children and adults, necessitating effective diagnostic tools.", "method": "Proposes a CNN-based embedding block, local temporal attention, and ROI-rank based masking to learn BOLD signals and distinguish ROIs.", "result": "Achieved 77.78% accuracy, 76.60% specificity, 79.22% sensitivity, and 79.30% AUC, outperforming other transformer models.", "conclusion": "The spatiotemporal enhanced transformer is effective for ADHD diagnosis, leveraging fMRI data for improved biomarker identification."}}
{"id": "2504.11467", "pdf": "https://arxiv.org/pdf/2504.11467", "abs": "https://arxiv.org/abs/2504.11467", "authors": ["Qianxue Zhang", "Eiman Kanjo"], "title": "MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 10 figures", "summary": "The advancement of technology has revolutionised the agricultural industry,\ntransitioning it from labour-intensive farming practices to automated,\nAI-powered management systems. In recent years, more intelligent livestock\nmonitoring solutions have been proposed to enhance farming efficiency and\nproductivity. This work presents a novel approach to animal activity\nrecognition and movement tracking, leveraging tiny machine learning (TinyML)\ntechniques, wireless communication framework, and microcontroller platforms to\ndevelop an efficient, cost-effective livestock sensing system. It collects and\nfuses accelerometer data and vision inputs to build a multi-modal network for\nthree tasks: image classification, object detection, and behaviour recognition.\nThe system is deployed and evaluated on commercial microcontrollers for\nreal-time inference using embedded applications, demonstrating up to\n270$\\times$ model size reduction, less than 80ms response latency, and on-par\nperformance comparable to existing methods. The incorporation of the TinyML\ntechnique allows for seamless data transmission between devices, benefiting use\ncases in remote locations with poor Internet connectivity. This work delivers a\nrobust, scalable IoT-edge livestock monitoring solution adaptable to diverse\nfarming needs, offering flexibility for future extensions.", "AI": {"tldr": "A novel TinyML-based livestock monitoring system combines accelerometer and vision data for efficient, real-time animal activity recognition, achieving significant model size reduction and low latency.", "motivation": "To enhance farming efficiency and productivity by transitioning from labor-intensive practices to automated, AI-powered livestock monitoring, especially in remote areas with poor connectivity.", "method": "Leverages TinyML, wireless communication, and microcontrollers to fuse accelerometer and vision data for multi-modal tasks (image classification, object detection, behavior recognition).", "result": "Demonstrates 270\u00d7 model size reduction, <80ms latency, and performance comparable to existing methods, enabling real-time inference on microcontrollers.", "conclusion": "The system provides a robust, scalable IoT-edge solution adaptable to diverse farming needs, with potential for future extensions."}}
{"id": "2504.11497", "pdf": "https://arxiv.org/pdf/2504.11497", "abs": "https://arxiv.org/abs/2504.11497", "authors": ["Chang Liu", "Emmanuel A. Olowe", "Danial Chitnis"], "title": "LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit", "categories": ["cs.LG", "cs.AR"], "comment": "to be presented in IEEE NEWCAS 2025", "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often\ninvolves significant manual effort, especially during the transistor sizing\nprocess. While Machine Learning techniques in Electronic Design Automation\n(EDA) have shown promise in reducing complexity and minimizing human\nintervention, they still face challenges such as numerous iterations and a lack\nof knowledge about AMS circuit design. Recently, Large Language Models (LLMs)\nhave demonstrated significant potential across various fields, showing a\ncertain level of knowledge in circuit design and indicating their potential to\nautomate the transistor sizing process. In this work, we propose an LLM-based\nAI agent for AMS circuit design to assist in the sizing process. By integrating\nLLMs with external circuit simulation tools and data analysis functions and\nemploying prompt engineering strategies, the agent successfully optimized\nmultiple circuits to achieve target performance metrics. We evaluated the\nperformance of different LLMs to assess their applicability and optimization\neffectiveness across seven basic circuits, and selected the best-performing\nmodel Claude 3.5 Sonnet for further exploration on an operational amplifier,\nwith complementary input stage and class AB output stage. This circuit was\nevaluated against nine performance metrics, and we conducted experiments under\nthree distinct performance requirement groups. A success rate of up to 60% was\nachieved for reaching the target requirements. Overall, this work demonstrates\nthe potential of LLMs to improve AMS circuit design.", "AI": {"tldr": "An LLM-based AI agent is proposed to automate transistor sizing in AMS circuit design, achieving up to 60% success in meeting performance targets.", "motivation": "Manual transistor sizing in AMS ICs is labor-intensive, and existing ML techniques in EDA face challenges like iterations and lack of design knowledge. LLMs show promise in automating this process.", "method": "The AI agent integrates LLMs with circuit simulation tools and data analysis, using prompt engineering to optimize circuits. Different LLMs were evaluated, with Claude 3.5 Sonnet selected for further testing on an operational amplifier.", "result": "The agent achieved a 60% success rate in meeting target performance metrics across three requirement groups for the operational amplifier.", "conclusion": "LLMs have significant potential to enhance AMS circuit design by automating and optimizing the transistor sizing process."}}
{"id": "2504.11770", "pdf": "https://arxiv.org/pdf/2504.11770", "abs": "https://arxiv.org/abs/2504.11770", "authors": ["Takashi Morita", "Timothy J. O'Donnell"], "title": "Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters", "categories": ["cs.CL"], "comment": null, "summary": "Cross-linguistically, native words and loanwords follow different\nphonological rules. In English, for example, words of Germanic and Latinate\norigin exhibit different stress patterns, and a certain syntactic structure is\nexclusive to Germanic verbs. When seeing them as a cognitive model, however,\nsuch etymology-based generalizations face challenges in terms of learnability,\nsince the historical origins of words are presumably inaccessible information\nfor general language learners. In this study, we present computational evidence\nindicating that the Germanic-Latinate distinction in the English lexicon is\nlearnable from the phonotactic information of individual words. Specifically,\nwe performed an unsupervised clustering on corpus-extracted words, and the\nresulting word clusters largely aligned with the etymological distinction. The\nmodel-discovered clusters also recovered various linguistic generalizations\ndocumented in the previous literature regarding the corresponding etymological\nclasses. Moreover, our findings also uncovered previously unrecognized features\nof the quasi-etymological clusters, offering novel hypotheses for future\nexperimental studies.", "AI": {"tldr": "The study shows that the Germanic-Latinate distinction in English words is learnable from phonotactic information, using unsupervised clustering to align with etymological classes and uncover new linguistic features.", "motivation": "To address the challenge of learnability in etymology-based generalizations, as historical origins are inaccessible to learners.", "method": "Unsupervised clustering on corpus-extracted words to analyze phonotactic information.", "result": "Clusters aligned with etymological distinctions and recovered known linguistic generalizations, while uncovering new features.", "conclusion": "The findings suggest phonotactic information can reveal etymological distinctions, offering new hypotheses for future research."}}
{"id": "2504.11695", "pdf": "https://arxiv.org/pdf/2504.11695", "abs": "https://arxiv.org/abs/2504.11695", "authors": ["Isabel Papadimitriou", "Huangyuan Su", "Thomas Fel", "Naomi Saphra", "Sham Kakade", "Stephanie Gil"], "title": "Interpreting the Linear Structure of Vision-language Model Embedding Spaces", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Vision-language models encode images and text in a joint space, minimizing\nthe distance between corresponding image and text pairs. How are language and\nimages organized in this joint space, and how do the models encode meaning and\nmodality? To investigate this, we train and release sparse autoencoders (SAEs)\non the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,\nand AIMv2). SAEs approximate model embeddings as sparse linear combinations of\nlearned directions, or \"concepts\". We find that, compared to other methods of\nlinear feature learning, SAEs are better at reconstructing the real embeddings,\nwhile also able to retain the most sparsity. Retraining SAEs with different\nseeds or different data diet leads to two findings: the rare, specific concepts\ncaptured by the SAEs are liable to change drastically, but we also show that\nthe key commonly-activating concepts extracted by SAEs are remarkably stable\nacross runs. Interestingly, while most concepts are strongly unimodal in\nactivation, we find they are not merely encoding modality per se. Many lie\nclose to - but not entirely within - the subspace defining modality, suggesting\nthat they encode cross-modal semantics despite their unimodal usage. To\nquantify this bridging behavior, we introduce the Bridge Score, a metric that\nidentifies concept pairs which are both co-activated across aligned image-text\ninputs and geometrically aligned in the shared space. This reveals that even\nunimodal concepts can collaborate to support cross-modal integration. We\nrelease interactive demos of the SAEs for all models, allowing researchers to\nexplore the organization of the concept spaces. Overall, our findings uncover a\nsparse linear structure within VLM embedding spaces that is shaped by modality,\nyet stitched together through latent bridges-offering new insight into how\nmultimodal meaning is constructed.", "AI": {"tldr": "The paper investigates the organization of language and images in vision-language models' joint space using sparse autoencoders (SAEs). It finds SAEs reconstruct embeddings better and retain sparsity, with stable key concepts across runs. Unimodal concepts often encode cross-modal semantics, and the Bridge Score metric identifies concept pairs supporting cross-modal integration.", "motivation": "To understand how vision-language models encode meaning and modality in their joint space and explore the structure of their embeddings.", "method": "Train and analyze sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, AIMv2). Introduce the Bridge Score to quantify cross-modal concept alignment.", "result": "SAEs outperform other linear feature learning methods in reconstruction and sparsity. Key concepts are stable across runs, while rare ones vary. Unimodal concepts often encode cross-modal semantics, and the Bridge Score reveals latent bridges for cross-modal integration.", "conclusion": "The study reveals a sparse linear structure in VLM embedding spaces shaped by modality but connected through latent bridges, offering insights into multimodal meaning construction."}}
{"id": "2504.11485", "pdf": "https://arxiv.org/pdf/2504.11485", "abs": "https://arxiv.org/abs/2504.11485", "authors": ["Sonia Foschiatti", "Axel Kittenberger", "Otmar Scherzer"], "title": "Deciphering scrolls with tomography: A training experiment", "categories": ["eess.IV", "cs.CV", "97M10, 44A12"], "comment": null, "summary": "The recovery of severely damaged ancient written documents has proven to be a\nmajor challenge for many scientists, mainly due to the impracticality of\nphysical unwrapping them. Non-destructive techniques, such as X-ray computed\ntomography (CT), combined with computer vision algorithms, have emerged as a\nmeans of facilitating the virtual reading of the hidden contents of the damaged\ndocuments. This paper proposes an educational laboratory aimed at simulating\nthe entire process of acquisition and virtual recovery of the ancient works. We\nhave developed an experimental setup that uses visible light to replace the\ndetrimental X-rays, and a didactic software pipeline that allows students to\nvirtually reconstruct a transparent rolled sheet with printed text on it, the\nwrapped scroll.", "AI": {"tldr": "A non-destructive educational lab setup using visible light and software to simulate virtual recovery of ancient damaged documents.", "motivation": "To address the challenge of recovering severely damaged ancient documents without physical unwrapping, using safer alternatives to X-rays.", "method": "Developed an experimental setup with visible light and a didactic software pipeline for virtual reconstruction of wrapped scrolls.", "result": "A practical and educational method for students to virtually reconstruct ancient texts without harmful X-rays.", "conclusion": "The proposed lab setup provides a safe, effective, and educational approach to simulating the recovery of ancient documents."}}
{"id": "2504.11470", "pdf": "https://arxiv.org/pdf/2504.11470", "abs": "https://arxiv.org/abs/2504.11470", "authors": ["Huaxiang Zhang", "Hao Zhang", "Aoran Mei", "Zhongxue Gan", "Guo-Niu Zhu"], "title": "SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detection Transformer-based methods have achieved significant advancements in\ngeneral object detection. However, challenges remain in effectively detecting\nsmall objects. One key difficulty is that existing encoders struggle to\nefficiently fuse low-level features. Additionally, the query selection\nstrategies are not effectively tailored for small objects. To address these\nchallenges, this paper proposes an efficient model, Small Object Detection\nTransformer (SO-DETR). The model comprises three key components: a dual-domain\nhybrid encoder, an enhanced query selection mechanism, and a knowledge\ndistillation strategy. The dual-domain hybrid encoder integrates spatial and\nfrequency domains to fuse multi-scale features effectively. This approach\nenhances the representation of high-resolution features while maintaining\nrelatively low computational overhead. The enhanced query selection mechanism\noptimizes query initialization by dynamically selecting high-scoring anchor\nboxes using expanded IoU, thereby improving the allocation of query resources.\nFurthermore, by incorporating a lightweight backbone network and implementing a\nknowledge distillation strategy, we develop an efficient detector for small\nobjects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets\ndemonstrate that SO-DETR outperforms existing methods with similar\ncomputational demands. The project page is available at\nhttps://github.com/ValiantDiligent/SO_DETR.", "AI": {"tldr": "SO-DETR improves small object detection by integrating a dual-domain hybrid encoder, enhanced query selection, and knowledge distillation, outperforming existing methods on VisDrone-2019-DET and UAVVaste datasets.", "motivation": "Existing encoders and query selection strategies struggle with small object detection, prompting the need for an efficient solution.", "method": "SO-DETR uses a dual-domain hybrid encoder, enhanced query selection, and knowledge distillation to improve small object detection.", "result": "SO-DETR outperforms existing methods on VisDrone-2019-DET and UAVVaste datasets with similar computational demands.", "conclusion": "SO-DETR effectively addresses small object detection challenges, offering a computationally efficient solution."}}
{"id": "2504.11506", "pdf": "https://arxiv.org/pdf/2504.11506", "abs": "https://arxiv.org/abs/2504.11506", "authors": ["Hongliang Lu", "Shuqi Shen", "Junjie Yang", "Chao Lu", "Xinhu Zheng", "Hai Yang"], "title": "Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "More than the adherence to specific traffic regulations, driving culture\ntouches upon a more implicit part - an informal, conventional, collective\nbehavioral pattern followed by drivers - that varies across countries, regions,\nand even cities. Such cultural divergence has become one of the biggest\nchallenges in deploying autonomous vehicles (AVs) across diverse regions today.\nThe current emergence of data-driven methods has shown a potential solution to\nenable culture-compatible driving through learning from data, but what if some\nunderdeveloped regions cannot provide sufficient local data to inform driving\nculture? This issue is particularly significant for a broader global AV market.\nHere, we propose a cross-cultural deployment scheme for AVs, called data-light\ninverse reinforcement learning, designed to re-calibrate culture-specific AVs\nand assimilate them into other cultures. First, we report the divergence in\ndriving cultures through a comprehensive comparative analysis of naturalistic\ndriving datasets on highways from three countries: Germany, China, and the USA.\nThen, we demonstrate the effectiveness of our scheme by testing the expeditious\ncross-cultural deployment across these three countries, with cumulative testing\nmileage of over 56084 km. The performance is particularly advantageous when\ncross-cultural deployment is carried out without affluent local data. Results\nshow that we can reduce the dependence on local data by a margin of 98.67% at\nbest. This study is expected to bring a broader, fairer AV global market,\nparticularly in those regions that lack enough local data to develop\nculture-compatible AVs.", "AI": {"tldr": "The paper proposes a data-light inverse reinforcement learning scheme for cross-cultural deployment of autonomous vehicles (AVs), reducing local data dependence by up to 98.67%.", "motivation": "Driving culture varies globally, posing challenges for AV deployment in data-scarce regions. The study aims to enable culture-compatible AVs without requiring extensive local data.", "method": "The scheme involves comparative analysis of driving datasets from Germany, China, and the USA, followed by testing cross-cultural deployment with minimal local data.", "result": "The method reduces local data dependence by 98.67%, facilitating AV deployment in underdeveloped regions.", "conclusion": "The study promotes a fairer global AV market by addressing data scarcity in culturally diverse regions."}}
{"id": "2504.11788", "pdf": "https://arxiv.org/pdf/2504.11788", "abs": "https://arxiv.org/abs/2504.11788", "authors": ["Zhisong Zhang", "Tianqing Fang", "Kaixin Ma", "Wenhao Yu", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Enhancing Web Agents with Explicit Rollback Mechanisms", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With recent advancements in large language models, web agents have been\ngreatly improved. However, dealing with complex and dynamic web environments\nrequires more advanced planning and search abilities. Previous studies usually\nadopt a greedy one-way search strategy, which may struggle to recover from\nerroneous states. In this work, we enhance web agents with an explicit rollback\nmechanism, enabling the agent to revert back to a previous state in its\nnavigation trajectory. This mechanism gives the model the flexibility to\ndirectly control the search process, leading to an effective and efficient web\nnavigation method. We conduct experiments on two live web navigation benchmarks\nwith zero-shot and fine-tuning settings. The results demonstrate the\neffectiveness of our proposed approach.", "AI": {"tldr": "The paper introduces a rollback mechanism for web agents to improve navigation in dynamic web environments, showing effectiveness in benchmarks.", "motivation": "Current web agents struggle with complex, dynamic environments due to limited planning and search abilities, especially in recovering from errors.", "method": "The proposed method enhances web agents with an explicit rollback mechanism, allowing them to revert to previous states for better control over navigation.", "result": "Experiments on live web navigation benchmarks demonstrate the approach's effectiveness in both zero-shot and fine-tuning settings.", "conclusion": "The rollback mechanism significantly improves web agent performance by enabling flexible and efficient navigation."}}
{"id": "2504.12204", "pdf": "https://arxiv.org/pdf/2504.12204", "abs": "https://arxiv.org/abs/2504.12204", "authors": ["Zhihua Wang", "Yu Long", "Qinghua Lin", "Kai Zhang", "Yazhu Zhang", "Yuming Fang", "Li Liu", "Xiaochun Cao"], "title": "Towards Realistic Low-Light Image Enhancement via ISP Driven Data Modeling", "categories": ["cs.CV", "cs.MM"], "comment": "17 pages, 11 tables, 10 figures", "summary": "Deep neural networks (DNNs) have recently become the leading method for\nlow-light image enhancement (LLIE). However, despite significant progress,\ntheir outputs may still exhibit issues such as amplified noise, incorrect white\nbalance, or unnatural enhancements when deployed in real world applications. A\nkey challenge is the lack of diverse, large scale training data that captures\nthe complexities of low-light conditions and imaging pipelines. In this paper,\nwe propose a novel image signal processing (ISP) driven data synthesis pipeline\nthat addresses these challenges by generating unlimited paired training data.\nSpecifically, our pipeline begins with easily collected high-quality\nnormal-light images, which are first unprocessed into the RAW format using a\nreverse ISP. We then synthesize low-light degradations directly in the RAW\ndomain. The resulting data is subsequently processed through a series of ISP\nstages, including white balance adjustment, color space conversion, tone\nmapping, and gamma correction, with controlled variations introduced at each\nstage. This broadens the degradation space and enhances the diversity of the\ntraining data, enabling the generated data to capture a wide range of\ndegradations and the complexities inherent in the ISP pipeline. To demonstrate\nthe effectiveness of our synthetic pipeline, we conduct extensive experiments\nusing a vanilla UNet model consisting solely of convolutional layers, group\nnormalization, GeLU activation, and convolutional block attention modules\n(CBAMs). Extensive testing across multiple datasets reveals that the vanilla\nUNet model trained with our data synthesis pipeline delivers high fidelity,\nvisually appealing enhancement results, surpassing state-of-the-art (SOTA)\nmethods both quantitatively and qualitatively.", "AI": {"tldr": "A novel ISP-driven data synthesis pipeline generates diverse low-light training data, improving DNN performance for low-light image enhancement.", "motivation": "Current DNNs for low-light image enhancement suffer from issues like noise amplification and unnatural enhancements due to limited training data diversity.", "method": "Proposes a data synthesis pipeline: reverse ISP to RAW, RAW-domain degradations, and varied ISP processing stages. Trains a vanilla UNet with this data.", "result": "The UNet trained with synthetic data outperforms SOTA methods in fidelity and visual quality.", "conclusion": "The synthetic pipeline effectively addresses data diversity challenges, enhancing DNN performance for low-light image enhancement."}}
{"id": "2504.11512", "pdf": "https://arxiv.org/pdf/2504.11512", "abs": "https://arxiv.org/abs/2504.11512", "authors": ["Sara Sippola", "Siiri Rautio", "Andreas Hauptmann", "Takanori Ide", "Samuli Siltanen"], "title": "Learned enclosure method for experimental EIT data", "categories": ["eess.IV", "cs.LG", "math.AP"], "comment": null, "summary": "Electrical impedance tomography (EIT) is a non-invasive imaging method with\ndiverse applications, including medical imaging and non-destructive testing.\nThe inverse problem of reconstructing internal electrical conductivity from\nboundary measurements is nonlinear and highly ill-posed, making it difficult to\nsolve accurately. In recent years, there has been growing interest in combining\nanalytical methods with machine learning to solve inverse problems. In this\npaper, we propose a method for estimating the convex hull of inclusions from\nboundary measurements by combining the enclosure method proposed by Ikehata\nwith neural networks. We demonstrate its performance using experimental data.\nCompared to the classical enclosure method with least squares fitting, the\nlearned convex hull achieves superior performance on both simulated and\nexperimental data.", "AI": {"tldr": "The paper proposes a hybrid method combining the enclosure method with neural networks to estimate the convex hull of inclusions in EIT, outperforming traditional least squares fitting.", "motivation": "EIT's inverse problem is nonlinear and ill-posed, prompting the need for innovative solutions combining analytical and machine learning approaches.", "method": "The method integrates Ikehata's enclosure method with neural networks to estimate the convex hull of inclusions from boundary measurements.", "result": "The proposed approach outperforms the classical enclosure method with least squares fitting on both simulated and experimental data.", "conclusion": "Combining analytical methods with machine learning improves accuracy in solving EIT's inverse problem, particularly for estimating inclusion convex hulls."}}
{"id": "2504.11472", "pdf": "https://arxiv.org/pdf/2504.11472", "abs": "https://arxiv.org/abs/2504.11472", "authors": ["Kebin Contreras", "Brayan Monroy", "Jorge Bacca"], "title": "High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Object detection precision is crucial for ensuring the safety and efficacy of\nautonomous driving systems. The quality of acquired images directly influences\nthe ability of autonomous driving systems to correctly recognize and respond to\nother vehicles, pedestrians, and obstacles in real-time. However, real\nenvironments present extreme variations in lighting, causing saturation\nproblems and resulting in the loss of crucial details for detection.\nTraditionally, High Dynamic Range (HDR) images have been preferred for their\nability to capture a broad spectrum of light intensities, but the need for\nmultiple captures to construct HDR images is inefficient for real-time\napplications in autonomous vehicles. To address these issues, this work\nintroduces the use of modulo sensors for robust object detection. The modulo\nsensor allows pixels to `reset/wrap' upon reaching saturation level by\nacquiring an irradiance encoding image which can then be recovered using\nunwrapping algorithms. The applied reconstruction techniques enable HDR\nrecovery of color intensity and image details, ensuring better visual quality\neven under extreme lighting conditions at the cost of extra time. Experiments\nwith the YOLOv10 model demonstrate that images processed using modulo images\nachieve performance comparable to HDR images and significantly surpass\nsaturated images in terms of object detection accuracy. Moreover, the proposed\nmodulo imaging step combined with HDR image reconstruction is shorter than the\ntime required for conventional HDR image acquisition.", "AI": {"tldr": "The paper proposes using modulo sensors for robust object detection in autonomous driving, addressing lighting variations and saturation issues. It achieves HDR-like quality with faster processing than traditional HDR methods.", "motivation": "Real-world lighting variations cause image saturation, losing crucial details for object detection in autonomous driving. Traditional HDR methods are inefficient for real-time use.", "method": "Modulo sensors reset upon saturation, capturing irradiance encoding images. Unwrapping algorithms reconstruct HDR-like images, preserving details under extreme lighting.", "result": "Modulo-processed images match HDR performance in object detection (tested with YOLOv10) and outperform saturated images. Processing time is shorter than conventional HDR.", "conclusion": "Modulo sensors offer a practical solution for real-time HDR-like imaging in autonomous driving, improving detection accuracy under challenging lighting."}}
{"id": "2504.11508", "pdf": "https://arxiv.org/pdf/2504.11508", "abs": "https://arxiv.org/abs/2504.11508", "authors": ["Clement Nyanhongo", "Bruno Miranda Henrique", "Eugene Santos"], "title": "Reward Distance Comparisons Under Transition Sparsity", "categories": ["cs.LG"], "comment": "Published in the TMLR, https://openreview.net/forum?id=haP586YomL", "summary": "Reward comparisons are vital for evaluating differences in agent behaviors\ninduced by a set of reward functions. Most conventional techniques utilize the\ninput reward functions to learn optimized policies, which are then used to\ncompare agent behaviors. However, learning these policies can be\ncomputationally expensive and can also raise safety concerns. Direct reward\ncomparison techniques obviate policy learning but suffer from transition\nsparsity, where only a small subset of transitions are sampled due to data\ncollection challenges and feasibility constraints. Existing state-of-the-art\ndirect reward comparison methods are ill-suited for these sparse conditions\nsince they require high transition coverage, where the majority of transitions\nfrom a given coverage distribution are sampled. When this requirement is not\nsatisfied, a distribution mismatch between sampled and expected transitions can\noccur, leading to significant errors. This paper introduces the Sparsity\nResilient Reward Distance (SRRD) pseudometric, designed to eliminate the need\nfor high transition coverage by accommodating diverse sample distributions,\nwhich are common under transition sparsity. We provide theoretical\njustification for SRRD's robustness and conduct experiments to demonstrate its\npractical efficacy across multiple domains.", "AI": {"tldr": "The paper introduces SRRD, a pseudometric for comparing rewards without requiring high transition coverage, addressing sparsity issues in conventional methods.", "motivation": "Existing direct reward comparison methods fail under transition sparsity due to high coverage requirements, leading to errors.", "method": "Proposes the Sparsity Resilient Reward Distance (SRRD) pseudometric, which accommodates diverse sample distributions without needing high transition coverage.", "result": "Theoretical and experimental validation shows SRRD's robustness and efficacy across domains.", "conclusion": "SRRD provides a reliable solution for reward comparison under sparse transition conditions, outperforming existing methods."}}
{"id": "2504.11793", "pdf": "https://arxiv.org/pdf/2504.11793", "abs": "https://arxiv.org/abs/2504.11793", "authors": ["Yue Li", "Lihong Zhang"], "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.", "AI": {"tldr": "SAFL reduces FL communication overhead and enhances privacy in LLMs by dynamically fine-tuning attention-critical transformer layers.", "motivation": "Address challenges of communication overhead and model privacy in FL for LLMs, especially in healthcare.", "method": "Introduces Selective Attention Federated Learning (SAFL), which dynamically fine-tunes attention-critical transformer layers using attention patterns.", "result": "SAFL achieves competitive performance with centralized models on clinical NLP benchmarks while improving communication efficiency and privacy.", "conclusion": "SAFL is effective for FL in LLMs, balancing performance, efficiency, and privacy."}}
{"id": "2504.11782", "pdf": "https://arxiv.org/pdf/2504.11782", "abs": "https://arxiv.org/abs/2504.11782", "authors": ["Chia-Hsiang Lin", "Si-Sheng Young"], "title": "HyperKING: Quantum-Classical Generative Adversarial Networks for Hyperspectral Image Restoration", "categories": ["eess.IV"], "comment": "19 pages, 15 figures, accepted by IEEE Transactions on Geoscience and\n  Remote Sensing", "summary": "Quantum machine intelligence starts showing its impact on satellite remote\nsensing (SRS). Also, recent literature exhibits that quantum generative\nintelligences encompass superior potential than their classical counterpart,\nmotivating us to develop quantum generative adversarial networks (GANs) for\nSRS. However, existing quantum GANs are restricted by the limited quantum bit\n(qubit) resources of current quantum computers and process merely a small 2x2\ngrayscale image, far from being applicable to SRS. Recently, the novel concept\nof hybrid quantum-classical GAN, a quantum generator with a classical\ndiscriminator, has upgraded the order to 28x28 (still grayscale), whereas it is\nstill insufficient for SRS. This motivates us to design a radically new hybrid\nframework, where both generator and discriminator are hybrid architectures. We\ndemonstrate this feasibility, leading to a breakthrough of processing 128x128\nhyperspectral images for SRS. Specifically, we design the quantum part with\nmathematically provable quantum full expressibility (FE) to address core signal\nprocessing tasks, wherein the FE property allows the quantum network to realize\nany valid quantum operator with appropriate training. The classical part,\ncomposed of convolutional layers, treats the read-in (compressing the optical\ninformation into limited qubits) and read-out (addressing the quantum collapse\neffect) procedures. The proposed innovative hybrid quantum GAN, named\nHyperspectral Knot-like IntelligeNt dIscrimiNator and Generator (HyperKING),\nwhere knot partly symbolizes the quantum entanglement and partly the compressed\nquantum domain in the central part of the network architecture. HyperKING\nsignificantly surpasses the classical approaches in hyperspectral tensor\ncompletion, mixed noise removal (about 3dB improvement), and blind source\nseparation results.", "AI": {"tldr": "A hybrid quantum-classical GAN framework (HyperKING) is proposed for hyperspectral SRS, outperforming classical methods in tasks like tensor completion and noise removal.", "motivation": "Quantum GANs show superior potential but are limited by qubit resources, prompting the development of a hybrid framework for larger hyperspectral images.", "method": "Combines quantum (provably fully expressive) and classical (convolutional layers) architectures to process 128x128 hyperspectral images.", "result": "Achieves significant improvements (3dB) in hyperspectral tensor completion, noise removal, and blind source separation.", "conclusion": "HyperKING demonstrates the feasibility and superiority of hybrid quantum-classical GANs for advanced SRS applications."}}
{"id": "2504.11473", "pdf": "https://arxiv.org/pdf/2504.11473", "abs": "https://arxiv.org/abs/2504.11473", "authors": ["Warren Zhu", "Aida Ramezani", "Yang Xu"], "title": "Visual moral inference and communication", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Humans can make moral inferences from multiple sources of input. In contrast,\nautomated moral inference in artificial intelligence typically relies on\nlanguage models with textual input. However, morality is conveyed through\nmodalities beyond language. We present a computational framework that supports\nmoral inference from natural images, demonstrated in two related tasks: 1)\ninferring human moral judgment toward visual images and 2) analyzing patterns\nin moral content communicated via images from public news. We find that models\nbased on text alone cannot capture the fine-grained human moral judgment toward\nvisual stimuli, but language-vision fusion models offer better precision in\nvisual moral inference. Furthermore, applications of our framework to news data\nreveal implicit biases in news categories and geopolitical discussions. Our\nwork creates avenues for automating visual moral inference and discovering\npatterns of visual moral communication in public media.", "AI": {"tldr": "A framework for moral inference from images, outperforming text-only models, reveals biases in news media.", "motivation": "Morality is conveyed beyond text, but AI lacks multimodal moral inference.", "method": "A computational framework for moral inference from images, tested on human judgment and news data.", "result": "Language-vision fusion models outperform text-only models in visual moral inference; biases found in news.", "conclusion": "Enables automated visual moral inference and reveals patterns in public media."}}
{"id": "2504.11511", "pdf": "https://arxiv.org/pdf/2504.11511", "abs": "https://arxiv.org/abs/2504.11511", "authors": ["Flint Xiaofeng Fan", "Cheston Tan", "Roger Wattenhofer", "Yew-Soon Ong"], "title": "Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IJCNN 2025 Position Paper Track", "summary": "The rise of reinforcement learning (RL) in critical real-world applications\ndemands a fundamental rethinking of privacy in AI systems. Traditional privacy\nframeworks, designed to protect isolated data points, fall short for sequential\ndecision-making systems where sensitive information emerges from temporal\npatterns, behavioral strategies, and collaborative dynamics. Modern RL\nparadigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in\nlarge language models (LLMs), exacerbate these challenges by introducing\ncomplex, interactive, and context-dependent learning environments that\ntraditional methods do not address. In this position paper, we argue for a new\nprivacy paradigm built on four core principles: multi-scale protection,\nbehavioral pattern protection, collaborative privacy preservation, and\ncontext-aware adaptation. These principles expose inherent tensions between\nprivacy, utility, and interpretability that must be navigated as RL systems\nbecome more pervasive in high-stakes domains like healthcare, autonomous\nvehicles, and decision support systems powered by LLMs. To tackle these\nchallenges, we call for the development of new theoretical frameworks,\npractical mechanisms, and rigorous evaluation methodologies that collectively\nenable effective privacy protection in sequential decision-making systems.", "AI": {"tldr": "The paper advocates for a new privacy paradigm in reinforcement learning (RL) to address shortcomings of traditional frameworks, proposing four core principles and calling for new theoretical and practical solutions.", "motivation": "Traditional privacy frameworks are inadequate for RL systems due to their sequential, interactive, and context-dependent nature, especially in modern paradigms like federated RL and RL with human feedback.", "method": "The paper proposes a new privacy paradigm based on four principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation.", "result": "The principles highlight tensions between privacy, utility, and interpretability in RL systems, emphasizing the need for new frameworks and evaluation methods.", "conclusion": "The paper calls for developing new theoretical and practical approaches to ensure effective privacy protection in RL systems, particularly in high-stakes domains."}}
{"id": "2504.11809", "pdf": "https://arxiv.org/pdf/2504.11809", "abs": "https://arxiv.org/abs/2504.11809", "authors": ["Biao Fu", "Donglei Yu", "Minpeng Liao", "Chengxi Li", "Yidong Chen", "Kai Fan", "Xiaodong Shi"], "title": "Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture", "categories": ["cs.CL"], "comment": null, "summary": "Simultaneous speech translation (SimulST) produces translations incrementally\nwhile processing partial speech input. Although large language models (LLMs)\nhave showcased strong capabilities in offline translation tasks, applying them\nto SimulST poses notable challenges. Existing LLM-based SimulST approaches\neither incur significant computational overhead due to repeated encoding of\nbidirectional speech encoder, or they depend on a fixed read/write policy,\nlimiting the efficiency and performance. In this work, we introduce Efficient\nand Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional\narchitecture, including both speech encoder and LLM. EASiST includes a\nmulti-latency data curation strategy to generate semantically aligned SimulST\ntraining samples and redefines SimulST as an interleaved generation task with\nexplicit read/write tokens. To facilitate adaptive inference, we incorporate a\nlightweight policy head that dynamically predicts read/write actions.\nAdditionally, we employ a multi-stage training strategy to align speech-text\nmodalities and optimize both translation and policy behavior. Experiments on\nthe MuST-C En$\\rightarrow$De and En$\\rightarrow$Es datasets demonstrate that\nEASiST offers superior latency-quality trade-offs compared to several strong\nbaselines.", "AI": {"tldr": "EASiST introduces an efficient and adaptive approach for simultaneous speech translation (SimulST) using a fully unidirectional architecture, dynamic read/write policy, and multi-stage training, outperforming baselines in latency-quality trade-offs.", "motivation": "Existing LLM-based SimulST methods face computational inefficiency or rigid policies, limiting performance. EASiST aims to address these challenges.", "method": "EASiST employs a unidirectional architecture, multi-latency data curation, explicit read/write tokens, a lightweight policy head, and multi-stage training.", "result": "EASiST achieves superior latency-quality trade-offs on MuST-C En\u2192De and En\u2192Es datasets.", "conclusion": "EASiST provides an efficient and adaptive solution for SimulST, improving performance and flexibility over existing methods."}}
{"id": "2504.11825", "pdf": "https://arxiv.org/pdf/2504.11825", "abs": "https://arxiv.org/abs/2504.11825", "authors": ["Kangbo Ma"], "title": "TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have demonstrated significant potential\nin 3D medical image segmentation tasks. However, their high computational cost\nand inability to fully capture global 3D contextual information limit their\npractical applications. To address these challenges, we propose a novel\ntext-guided diffusion model framework, TextDiffSeg. This method leverages a\nconditional diffusion framework that integrates 3D volumetric data with natural\nlanguage descriptions, enabling cross-modal embedding and establishing a shared\nsemantic space between visual and textual modalities. By enhancing the model's\nability to recognize complex anatomical structures, TextDiffSeg incorporates\ninnovative label embedding techniques and cross-modal attention mechanisms,\neffectively reducing computational complexity while preserving global 3D\ncontextual integrity. Experimental results demonstrate that TextDiffSeg\nconsistently outperforms existing methods in segmentation tasks involving\nkidney and pancreas tumors, as well as multi-organ segmentation scenarios.\nAblation studies further validate the effectiveness of key components,\nhighlighting the synergistic interaction between text fusion, image feature\nextractor, and label encoder. TextDiffSeg provides an efficient and accurate\nsolution for 3D medical image segmentation, showcasing its broad applicability\nin clinical diagnosis and treatment planning.", "AI": {"tldr": "TextDiffSeg is a text-guided diffusion model for 3D medical image segmentation, improving accuracy and reducing computational costs by integrating text and image data.", "motivation": "Addressing the limitations of Diffusion Probabilistic Models (DPMs) in 3D medical image segmentation, such as high computational costs and poor global context capture.", "method": "Proposes TextDiffSeg, a framework combining 3D volumetric data with natural language descriptions using cross-modal embedding and attention mechanisms.", "result": "Outperforms existing methods in segmentation tasks for kidney and pancreas tumors, and multi-organ scenarios, with validated key components.", "conclusion": "TextDiffSeg offers an efficient, accurate solution for 3D medical image segmentation, with broad clinical applicability."}}
{"id": "2504.11477", "pdf": "https://arxiv.org/pdf/2504.11477", "abs": "https://arxiv.org/abs/2504.11477", "authors": ["Yunkai Zhang", "Shiyin Wei", "Yong Huang", "Yawu Su", "Shanshan Lu", "Hui Li"], "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing computer vision(CV)-based structural damage identification models\ndemonstrate notable accuracy in categorizing and localizing damage. However,\nthese models present several critical limitations that hinder their practical\napplication in civil engineering(CE). Primarily, their ability to recognize\ndamage types remains constrained, preventing comprehensive analysis of the\nhighly varied and complex conditions encountered in real-world CE structures.\nSecond, these models lack linguistic capabilities, rendering them unable to\narticulate structural damage characteristics through natural language\ndescriptions. With the continuous advancement of artificial intelligence(AI),\nlarge multi-modal models(LMMs) have emerged as a transformative solution,\nenabling the unified encoding and alignment of textual and visual data. These\nmodels can autonomously generate detailed descriptive narratives of structural\ndamage while demonstrating robust generalization across diverse scenarios and\ntasks. This study introduces SDIGLM, an innovative LMM for structural damage\nidentification, developed based on the open-source VisualGLM-6B architecture.\nTo address the challenge of adapting LMMs to the intricate and varied operating\nconditions in CE, this work integrates a U-Net-based semantic segmentation\nmodule to generate defect segmentation maps as visual Chain of Thought(CoT).\nAdditionally, a multi-round dialogue fine-tuning dataset is constructed to\nenhance logical reasoning, complemented by a language CoT formed through prompt\nengineering. By leveraging this multi-modal CoT, SDIGLM surpasses\ngeneral-purpose LMMs in structural damage identification, achieving an accuracy\nof 95.24% across various infrastructure types. Moreover, the model effectively\ndescribes damage characteristics such as hole size, crack direction, and\ncorrosion severity.", "AI": {"tldr": "SDIGLM, a large multi-modal model (LMM), improves structural damage identification by combining visual and textual data, achieving 95.24% accuracy and detailed damage descriptions.", "motivation": "Existing CV-based models lack comprehensive damage recognition and linguistic capabilities, limiting their practical use in civil engineering.", "method": "SDIGLM integrates a U-Net-based semantic segmentation module for visual Chain of Thought (CoT) and uses a multi-round dialogue dataset for fine-tuning.", "result": "The model achieves 95.24% accuracy in damage identification and effectively describes damage characteristics.", "conclusion": "SDIGLM demonstrates superior performance in structural damage analysis, offering a practical solution for civil engineering applications."}}
{"id": "2504.11513", "pdf": "https://arxiv.org/pdf/2504.11513", "abs": "https://arxiv.org/abs/2504.11513", "authors": ["Wonjun Yi", "Yong-Hwa Park"], "title": "Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor", "categories": ["cs.LG"], "comment": "Extended version of \"Multi-output Classification for Compound Fault\n  Diagnosis in Motor under Partially Labeled Target Domain\" Will not be\n  published in any conferences or journels", "summary": "This work introduces a multi-output classification (MOC) framework designed\nfor domain adaptation in fault diagnosis, particularly under partially labeled\n(PL) target domain scenarios and compound fault conditions in rotating\nmachinery. Unlike traditional multi-class classification (MCC) methods that\ntreat each fault combination as a distinct class, the proposed approach\nindependently estimates the severity of each fault type, improving both\ninterpretability and diagnostic accuracy. The model incorporates multi-kernel\nmaximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to\nfacilitate feature transfer from the source to the target domain. In addition,\nfrequency layer normalization (FLN) is applied to preserve structural\nproperties in the frequency domain, which are strongly influenced by system\ndynamics and are often stationary with respect to changes in rpm. Evaluations\nacross six domain adaptation cases with PL data demonstrate that MOC\noutperforms baseline models in macro F1 score. Moreover, MOC consistently\nachieves better classification performance for individual fault types, and FLN\nshows superior adaptability compared to other normalization techniques.", "AI": {"tldr": "A multi-output classification (MOC) framework for fault diagnosis under partially labeled target domains and compound faults, using MK-MMD and EM losses, with FLN for frequency domain preservation, outperforming baselines in accuracy and adaptability.", "motivation": "Addressing challenges in fault diagnosis under partially labeled target domains and compound fault conditions, where traditional methods treat fault combinations as single classes, limiting interpretability and accuracy.", "method": "Proposes MOC to independently estimate fault severity, incorporating MK-MMD and EM losses for domain adaptation and FLN to preserve frequency domain properties.", "result": "MOC outperforms baselines in macro F1 score and achieves better classification for individual faults; FLN shows superior adaptability.", "conclusion": "The MOC framework enhances interpretability and accuracy in fault diagnosis, with FLN proving effective for domain adaptation in rotating machinery."}}
{"id": "2504.11814", "pdf": "https://arxiv.org/pdf/2504.11814", "abs": "https://arxiv.org/abs/2504.11814", "authors": ["Kirill Chirkunov", "Bashar Alhafni", "Chatrine Qwaider", "Nizar Habash", "Ted Briscoe"], "title": "ARWI: Arabic Write and Improve", "categories": ["cs.CL"], "comment": null, "summary": "Although Arabic is spoken by over 400 million people, advanced Arabic writing\nassistance tools remain limited. To address this gap, we present ARWI, a new\nwriting assistant that helps learners improve essay writing in Modern Standard\nArabic. ARWI is the first publicly available Arabic writing assistant to\ninclude a prompt database for different proficiency levels, an Arabic text\neditor, state-of-the-art grammatical error detection and correction, and\nautomated essay scoring aligned with the Common European Framework of Reference\nstandards for language attainment. Moreover, ARWI can be used to gather a\ngrowing auto-annotated corpus, facilitating further research on Arabic grammar\ncorrection and essay scoring, as well as profiling patterns of errors made by\nnative speakers and non-native learners. A preliminary user study shows that\nARWI provides actionable feedback, helping learners identify grammatical gaps,\nassess language proficiency, and guide improvement.", "AI": {"tldr": "ARWI is a new Arabic writing assistant with grammar correction, essay scoring, and a prompt database for learners at different proficiency levels.", "motivation": "Advanced Arabic writing tools are limited despite the language's widespread use.", "method": "ARWI includes a prompt database, text editor, grammatical error detection/correction, and automated essay scoring.", "result": "A preliminary study shows ARWI helps learners identify gaps and improve proficiency.", "conclusion": "ARWI fills a gap in Arabic writing assistance and supports further research."}}
{"id": "2504.11953", "pdf": "https://arxiv.org/pdf/2504.11953", "abs": "https://arxiv.org/abs/2504.11953", "authors": ["Daiqi Liu", "Fuxin Fan", "Andreas Maier"], "title": "Novel-view X-ray Projection Synthesis through Geometry-Integrated Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 3 figures, 1 table", "summary": "X-ray imaging plays a crucial role in the medical field, providing essential\ninsights into the internal anatomy of patients for diagnostics, image-guided\nprocedures, and clinical decision-making. Traditional techniques often require\nmultiple X-ray projections from various angles to obtain a comprehensive view,\nleading to increased radiation exposure and more complex clinical processes.\nThis paper explores an innovative approach using the DL-GIPS model, which\nsynthesizes X-ray projections from new viewpoints by leveraging a single\nexisting projection. The model strategically manipulates geometry and texture\nfeatures extracted from an initial projection to match new viewing angles. It\nthen synthesizes the final projection by merging these modified geometry\nfeatures with consistent texture information through an advanced image\ngeneration process. We demonstrate the effectiveness and broad applicability of\nthe DL-GIPS framework through lung imaging examples, highlighting its potential\nto revolutionize stereoscopic and volumetric imaging by minimizing the need for\nextensive data acquisition.", "AI": {"tldr": "The paper introduces DL-GIPS, a model that synthesizes new X-ray projections from a single existing one, reducing radiation exposure and simplifying clinical processes.", "motivation": "Traditional X-ray imaging requires multiple projections, increasing radiation exposure and complexity. DL-GIPS aims to minimize these drawbacks by generating new views from a single projection.", "method": "DL-GIPS manipulates geometry and texture features from an initial X-ray projection to match new angles, then synthesizes the final projection using an advanced image generation process.", "result": "The model effectively synthesizes X-ray projections, demonstrated through lung imaging, showing potential to revolutionize stereoscopic and volumetric imaging.", "conclusion": "DL-GIPS offers a promising solution to reduce radiation exposure and streamline X-ray imaging, with broad applicability in medical diagnostics."}}
{"id": "2504.11478", "pdf": "https://arxiv.org/pdf/2504.11478", "abs": "https://arxiv.org/abs/2504.11478", "authors": ["Hao Kang", "Stathi Fotiadis", "Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Min Jin Chong", "Xin Lu"], "title": "Flux Already Knows - Activating Subject-Driven Image Generation without Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a simple yet effective zero-shot framework for subject-driven\nimage generation using a vanilla Flux model. By framing the task as grid-based\nimage completion and simply replicating the subject image(s) in a mosaic\nlayout, we activate strong identity-preserving capabilities without any\nadditional data, training, or inference-time fine-tuning. This \"free lunch\"\napproach is further strengthened by a novel cascade attention design and meta\nprompting technique, boosting fidelity and versatility. Experimental results\nshow that our method outperforms baselines across multiple key metrics in\nbenchmarks and human preference studies, with trade-offs in certain aspects.\nAdditionally, it supports diverse edits, including logo insertion, virtual\ntry-on, and subject replacement or insertion. These results demonstrate that a\npre-trained foundational text-to-image model can enable high-quality,\nresource-efficient subject-driven generation, opening new possibilities for\nlightweight customization in downstream applications.", "AI": {"tldr": "A zero-shot framework using a vanilla Flux model for subject-driven image generation, achieving high fidelity without extra training or data.", "motivation": "To enable resource-efficient, high-quality subject-driven image generation without additional training or fine-tuning.", "method": "Grid-based image completion with mosaic layout, cascade attention design, and meta prompting.", "result": "Outperforms baselines in benchmarks and human studies, supports diverse edits like logo insertion and virtual try-on.", "conclusion": "Pre-trained models can enable lightweight, high-quality customization for downstream applications."}}
{"id": "2504.11521", "pdf": "https://arxiv.org/pdf/2504.11521", "abs": "https://arxiv.org/abs/2504.11521", "authors": ["Wei-Jer Chang", "Wei Zhan", "Masayoshi Tomizuka", "Manmohan Chandraker", "Francesco Pittaluga"], "title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation", "categories": ["cs.LG", "cs.RO", "I.2.9; I.2.6"], "comment": "Dataset and project website in preparation", "summary": "Evaluating autonomous vehicles with controllability enables scalable testing\nin counterfactual or structured settings, enhancing both efficiency and safety.\nWe introduce LangTraj, a language-conditioned scene-diffusion model that\nsimulates the joint behavior of all agents in traffic scenarios. By\nconditioning on natural language inputs, LangTraj provides flexible and\nintuitive control over interactive behaviors, generating nuanced and realistic\nscenarios. Unlike prior approaches that depend on domain-specific guidance\nfunctions, LangTraj incorporates language conditioning during training,\nfacilitating more intuitive traffic simulation control. We propose a novel\nclosed-loop training strategy for diffusion models, explicitly tailored to\nenhance stability and realism during closed-loop simulation. To support\nlanguage-conditioned simulation, we develop Inter-Drive, a large-scale dataset\nwith diverse and interactive labels for training language-conditioned diffusion\nmodels. Our dataset is built upon a scalable pipeline for annotating\nagent-agent interactions and single-agent behaviors, ensuring rich and varied\nsupervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates\nstrong performance in realism, language controllability, and\nlanguage-conditioned safety-critical simulation, establishing a new paradigm\nfor flexible and scalable autonomous vehicle testing.", "AI": {"tldr": "LangTraj, a language-conditioned scene-diffusion model, enhances autonomous vehicle testing by simulating realistic traffic scenarios with flexible language control.", "motivation": "To improve the efficiency and safety of autonomous vehicle testing by enabling scalable, intuitive, and nuanced control over traffic simulations.", "method": "Introduces LangTraj, a diffusion model conditioned on natural language, and a closed-loop training strategy for stability. Uses the Inter-Drive dataset for diverse interaction labels.", "result": "LangTraj achieves strong performance in realism, language controllability, and safety-critical simulation on the Waymo Motion Dataset.", "conclusion": "LangTraj sets a new standard for flexible and scalable autonomous vehicle testing through language-conditioned simulation."}}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829", "abs": "https://arxiv.org/abs/2504.11829", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "title": "D\u00e9j\u00e0 Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.", "AI": {"tldr": "The paper highlights gaps in evaluating multilingual large language models (mLLMs) and proposes adopting best practices from machine translation (MT) evaluation to improve rigor and transparency.", "motivation": "Current evaluation practices for mLLMs lack comprehensiveness and consistency, hindering their development.", "method": "The study draws parallels with MT evaluation, conducts experiments, and identifies components for robust meta-evaluation.", "result": "Demonstrates how MT evaluation practices can enhance understanding of mLLM quality differences and provides a checklist for better evaluation.", "conclusion": "Adopting MT-inspired evaluation practices can improve mLLM development and ensure rigorous assessment of evaluation methods."}}
{"id": "2504.12203", "pdf": "https://arxiv.org/pdf/2504.12203", "abs": "https://arxiv.org/abs/2504.12203", "authors": ["Levente Lippenszky", "Istv\u00e1n Megyeri", "Krisztian Koos", "Zs\u00f3fia Karancsi", "Borb\u00e1la De\u00e1k-Karancsi", "Andr\u00e1s Front\u00f3", "\u00c1rp\u00e1d Makk", "Attila R\u00e1dics", "Erhan Bas", "L\u00e1szl\u00f3 Rusk\u00f3"], "title": "Modality-Independent Explainable Detection of Inaccurate Organ Segmentations Using Denoising Autoencoders", "categories": ["eess.IV", "cs.CV"], "comment": "Short version of this paper was accepted for poster presentation at\n  IEEE ISBI 2025", "summary": "In radiation therapy planning, inaccurate segmentations of organs at risk can\nresult in suboptimal treatment delivery, if left undetected by the clinician.\nTo address this challenge, we developed a denoising autoencoder-based method to\ndetect inaccurate organ segmentations. We applied noise to ground truth organ\nsegmentations, and the autoencoders were tasked to denoise them. Through the\napplication of our method to organ segmentations generated on both MR and CT\nscans, we demonstrated that the method is independent of imaging modality. By\nproviding reconstructions, our method offers visual information about\ninaccurate regions of the organ segmentations, leading to more explainable\ndetection of suboptimal segmentations. We compared our method to existing\napproaches in the literature and demonstrated that it achieved superior\nperformance for the majority of organs.", "AI": {"tldr": "A denoising autoencoder-based method detects inaccurate organ segmentations in radiation therapy planning, outperforming existing approaches and working across MR and CT scans.", "motivation": "Inaccurate organ segmentations in radiation therapy can lead to suboptimal treatment, necessitating a reliable detection method.", "method": "Noise was added to ground truth organ segmentations, and autoencoders were trained to denoise them, providing visual reconstructions of inaccuracies.", "result": "The method is imaging-modality independent (works on MR and CT) and outperforms existing approaches for most organs.", "conclusion": "The denoising autoencoder method offers explainable and superior detection of suboptimal organ segmentations."}}
{"id": "2504.11482", "pdf": "https://arxiv.org/pdf/2504.11482", "abs": "https://arxiv.org/abs/2504.11482", "authors": ["Vidya Sudevan", "Fakhreddine Zayer", "Rizwana Kausar", "Sajid Javed", "Hamad Karki", "Giulia De Masi", "Jorge Dias"], "title": "snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing", "categories": ["cs.CV", "cs.AI", "cs.PF", "cs.RO", "eess.IV"], "comment": null, "summary": "Underwater image dehazing is critical for vision-based marine operations\nbecause light scattering and absorption can severely reduce visibility. This\npaper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)\nspecifically designed for underwater dehazing. By leveraging the temporal\ndynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image\nsequences while maintaining low power consumption. Static underwater images are\nfirst converted into time-dependent sequences by repeatedly inputting the same\nimage over user-defined timesteps. These RGB sequences are then transformed\ninto LAB color space representations and processed concurrently. The\narchitecture features three key modules: (i) a K estimator that extracts\nfeatures from multiple color space representations; (ii) a Background Light\nEstimator that jointly infers the background light component from the RGB-LAB\nimages; and (iii) a soft image reconstruction module that produces haze-free,\nvisibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a\nsurrogate gradient-based backpropagation through time (BPTT) strategy alongside\na novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ\nachieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it\nyields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million\nnetwork parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the\nalgorithm significantly outperforms existing state-of-the-art methods in terms\nof efficiency. These features make snnTrans-DHZ highly suitable for deployment\nin underwater robotics, marine exploration, and environmental monitoring.", "AI": {"tldr": "snnTrans-DHZ is a lightweight SNN for underwater image dehazing, achieving high efficiency and performance with low power consumption.", "motivation": "Underwater image dehazing is essential for marine operations due to visibility issues caused by light scattering and absorption.", "method": "Uses a Spiking Neural Network (SNN) to process time-dependent image sequences in LAB color space, with modules for feature extraction, background light estimation, and image reconstruction. Trained via surrogate gradient-based BPTT and a novel loss function.", "result": "Achieves PSNR of 21.68 dB (UIEB) and 23.46 dB (EUVP), with SSIM of 0.8795 and 0.8439, respectively. Low energy (0.0151 J) and computational cost (7.42 GSOPs).", "conclusion": "snnTrans-DHZ is efficient and effective, making it ideal for underwater robotics and marine applications."}}
{"id": "2504.11558", "pdf": "https://arxiv.org/pdf/2504.11558", "abs": "https://arxiv.org/abs/2504.11558", "authors": ["Mete Erdogan", "Cengiz Pehlevan", "Alper T. Erdogan"], "title": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel\nlearning framework that addresses the credit assignment problem in neural\nnetworks by directly broadcasting output error to individual layers. Leveraging\nthe stochastic orthogonality property of the optimal minimum mean square error\n(MMSE) estimator, EBD defines layerwise loss functions to penalize correlations\nbetween layer activations and output errors, offering a principled approach to\nerror broadcasting without the need for weight transport. The optimization\nframework naturally leads to the experimentally observed three-factor learning\nrule and integrates with biologically plausible frameworks to enhance\nperformance and plausibility. Numerical experiments demonstrate that EBD\nachieves performance comparable to or better than known error-broadcast methods\non benchmark datasets. While the scalability of EBD to very large or complex\ndatasets remains to be further explored, our findings suggest it provides a\nbiologically plausible, efficient, and adaptable alternative for neural network\ntraining. This approach could inform future advancements in artificial and\nnatural learning paradigms.", "AI": {"tldr": "EBD is a new algorithm for neural networks that broadcasts output errors to layers, using stochastic orthogonality to avoid weight transport, achieving competitive performance on benchmarks.", "motivation": "Addresses the credit assignment problem in neural networks by providing a biologically plausible method for error broadcasting.", "method": "Uses stochastic orthogonality of MMSE estimators to define layerwise loss functions, penalizing correlations between activations and errors.", "result": "Achieves performance comparable or better than existing error-broadcast methods on benchmark datasets.", "conclusion": "EBD offers a biologically plausible, efficient alternative for neural network training, with potential for future advancements in learning paradigms."}}
{"id": "2504.11833", "pdf": "https://arxiv.org/pdf/2504.11833", "abs": "https://arxiv.org/abs/2504.11833", "authors": ["Changjiang Gao", "Xu Huang", "Wenhao Zhu", "Shujian Huang", "Lei Li", "Fei Yuan"], "title": "Could Thinking Multilingually Empower LLM Reasoning?", "categories": ["cs.CL"], "comment": null, "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.", "AI": {"tldr": "Multilingual reasoning in LLMs can outperform English-only tasks by nearly 10 Acc@$k$ points, offering robustness and higher upper bounds, though current methods fall short due to biases and limitations.", "motivation": "To explore the under-examined phenomenon where non-English languages sometimes outperform English in reasoning tasks and to determine the upper bounds of multilingual reasoning.", "method": "Analyze the performance of multilingual reasoning tasks, compare it to English-only reasoning, and investigate the reasons for the upper bound and challenges in achieving it.", "result": "Multilingual reasoning shows a nearly 10 Acc@$k$ points improvement over English-only tasks, with robustness to translation and language variations, but current answer selection methods fail to reach this potential.", "conclusion": "Multilingual reasoning in LLMs holds significant promise, but new methods are needed to overcome biases and limitations to fully harness its potential."}}
{"id": "2504.12249", "pdf": "https://arxiv.org/pdf/2504.12249", "abs": "https://arxiv.org/abs/2504.12249", "authors": ["Zhijin He", "Alan B. McMillan"], "title": "Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The application of artificial intelligence (AI) in medical imaging has\nrevolutionized diagnostic practices, enabling advanced analysis and\ninterpretation of radiological data. This study presents a comprehensive\nevaluation of radiomics-based and deep learning-based approaches for disease\ndetection in chest radiography, focusing on COVID-19, lung opacity, and viral\npneumonia. While deep learning models, particularly convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), learn directly from image data,\nradiomics-based models extract and analyze quantitative features, potentially\nproviding advantages in data-limited scenarios. This study systematically\ncompares the diagnostic accuracy and robustness of various AI models, including\nDecision Trees, Gradient Boosting, Random Forests, Support Vector Machines\n(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against\nstate-of-the-art computer vision deep learning architectures. Performance\nmetrics across varying sample sizes reveal insights into each model's efficacy,\nhighlighting the contexts in which specific AI approaches may offer enhanced\ndiagnostic capabilities. The results aim to inform the integration of AI-driven\ndiagnostic tools in clinical practice, particularly in automated and\nhigh-throughput environments where timely, reliable diagnosis is critical. This\ncomparative study addresses an essential gap, establishing guidance for the\nselection of AI models based on clinical and operational needs.", "AI": {"tldr": "The study evaluates radiomics and deep learning models for disease detection in chest radiography, comparing their accuracy and robustness to guide clinical AI integration.", "motivation": "To address the gap in understanding which AI models (radiomics or deep learning) perform better for disease detection in medical imaging, especially in data-limited scenarios.", "method": "Systematic comparison of radiomics-based models (Decision Trees, Gradient Boosting, Random Forests, SVM, MLP) and deep learning models (CNNs, ViTs) using performance metrics across varying sample sizes.", "result": "Insights into the efficacy of each model type, highlighting contexts where specific AI approaches may enhance diagnostic capabilities.", "conclusion": "Provides guidance for selecting AI models in clinical practice, emphasizing timely and reliable diagnosis in automated environments."}}
{"id": "2504.11489", "pdf": "https://arxiv.org/pdf/2504.11489", "abs": "https://arxiv.org/abs/2504.11489", "authors": ["Matthew Bozoukov"], "title": "Uncovering Branch specialization in InceptionV1 using k sparse autoencoders", "categories": ["cs.CV"], "comment": "Accepted to CVPR MIV workshop. 9 pages with an appendix", "summary": "Sparse Autoencoders (SAEs) have shown to find interpretable features in\nneural networks from polysemantic neurons caused by superposition. Previous\nwork has shown SAEs are an effective tool to extract interpretable features\nfrom the early layers of InceptionV1. Since then, there have been many\nimprovements to SAEs but branch specialization is still an enigma in the later\nlayers of InceptionV1. We show various examples of branch specialization\noccuring in each layer of the mixed4a-4e branch, in the 5x5 branch and in one\n1x1 branch. We also provide evidence to claim that branch specialization seems\nto be consistent across layers, similar features across the model will be\nlocalized in the same convolution size branches in their respective layer.", "AI": {"tldr": "SAEs reveal interpretable features in neural networks, with branch specialization in InceptionV1's later layers showing consistent patterns across convolution sizes.", "motivation": "To understand and demonstrate branch specialization in later layers of InceptionV1, which remains poorly understood despite SAE improvements.", "method": "Analyzed branch specialization in mixed4a-4e, 5x5, and 1x1 branches of InceptionV1 using SAEs.", "result": "Found consistent branch specialization patterns across layers, with similar features localized in same-sized convolution branches.", "conclusion": "Branch specialization in InceptionV1 is layer-consistent, suggesting a structured feature organization in later layers."}}
{"id": "2504.11581", "pdf": "https://arxiv.org/pdf/2504.11581", "abs": "https://arxiv.org/abs/2504.11581", "authors": ["Mert Sehri", "Igor Varej\u00e3o", "Zehui Hua", "Vitor Bonella", "Adriano Santos", "Francisco de Assis Boldt", "Patrick Dumond", "Flavio Miguel Varej\u00e3o"], "title": "Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "ImageNet has become a reputable resource for transfer learning, allowing the\ndevelopment of efficient ML models with reduced training time and data\nrequirements. However, vibration analysis in predictive maintenance, structural\nhealth monitoring, and fault diagnosis, lacks a comparable large-scale,\nannotated dataset to facilitate similar advancements. To address this, a\ndataset framework is proposed that begins with bearing vibration data as an\ninitial step towards creating a universal dataset for vibration-based\nspectrogram analysis for all machinery. The initial framework includes a\ncollection of bearing vibration signals from various publicly available\ndatasets. To demonstrate the advantages of this framework, experiments were\nconducted using a deep learning architecture, showing improvements in model\nperformance when pre-trained on bearing vibration data and fine-tuned on a\nsmaller, domain-specific dataset. These findings highlight the potential to\nparallel the success of ImageNet in visual computing but for vibration\nanalysis. For future work, this research will include a broader range of\nvibration signals from multiple types of machinery, emphasizing\nspectrogram-based representations of the data. Each sample will be labeled\naccording to machinery type, operational status, and the presence or type of\nfaults, ensuring its utility for supervised and unsupervised learning tasks.\nAdditionally, a framework for data preprocessing, feature extraction, and model\ntraining specific to vibration data will be developed. This framework will\nstandardize methodologies across the research community, allowing for\ncollaboration and accelerating progress in predictive maintenance, structural\nhealth monitoring, and related fields. By mirroring the success of ImageNet in\nvisual computing, this dataset has the potential to improve the development of\nintelligent systems in industrial applications.", "AI": {"tldr": "A framework for a large-scale, annotated vibration dataset is proposed to advance vibration analysis, inspired by ImageNet's success in transfer learning. Initial experiments show improved model performance when pre-trained on bearing vibration data.", "motivation": "The lack of a large-scale, annotated vibration dataset hinders progress in predictive maintenance and fault diagnosis, unlike the success seen with ImageNet in visual computing.", "method": "The framework collects bearing vibration signals from public datasets, uses deep learning for pre-training, and fine-tunes on domain-specific data. Future work includes expanding to more machinery types and spectrogram-based representations.", "result": "Experiments demonstrate improved model performance when pre-trained on bearing vibration data, validating the framework's potential.", "conclusion": "The proposed dataset framework can standardize vibration analysis methodologies, accelerate research, and mirror ImageNet's impact in industrial applications."}}
{"id": "2504.11837", "pdf": "https://arxiv.org/pdf/2504.11837", "abs": "https://arxiv.org/abs/2504.11837", "authors": ["Yue Zhao", "Qingqing Gu", "Xiaoyu Wang", "Teng Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "accepted by CMCL", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.", "AI": {"tldr": "FiSMiness, a framework using Finite State Machine (FSM) on LLMs, improves emotional support conversations by enabling self-reasoning of emotions and strategies, outperforming existing methods.", "motivation": "Current LLM-based ESC studies lack state model perspective, leading to suboptimal long-term satisfaction.", "method": "Leverages FSM on LLMs for self-reasoning of seeker's emotion, support strategy, and response per turn.", "result": "FiSMiness outperforms baselines like direct inference, self-refine, and chain of thought, even with fewer parameters.", "conclusion": "FiSMiness provides a superior solution for ESC by integrating state model reasoning into LLMs."}}
{"id": "2504.12265", "pdf": "https://arxiv.org/pdf/2504.12265", "abs": "https://arxiv.org/abs/2504.12265", "authors": ["Xiaojian Chen", "Yihao Liu", "Shuwen Wei", "Aaron Carass", "Yong Du", "Junyu Chen"], "title": "Correlation Ratio for Unsupervised Learning of Multi-modal Deformable Registration", "categories": ["eess.IV"], "comment": "Accepted by SPIE MI'25 ((c) SPIE). Code available at\n  https://github.com/junyuchen245/Correlation_Ratio", "summary": "In recent years, unsupervised learning for deformable image registration has\nbeen a major research focus. This approach involves training a registration\nnetwork using pairs of moving and fixed images, along with a loss function that\ncombines an image similarity measure and deformation regularization. For\nmulti-modal image registration tasks, the correlation ratio has been a\nwidely-used image similarity measure historically, yet it has been\nunderexplored in current deep learning methods. Here, we propose a\ndifferentiable correlation ratio to use as a loss function for learning-based\nmulti-modal deformable image registration. This approach extends the\ntraditionally non-differentiable implementation of the correlation ratio by\nusing the Parzen windowing approximation, enabling backpropagation with deep\nneural networks. We validated the proposed correlation ratio on a multi-modal\nneuroimaging dataset. In addition, we established a Bayesian training framework\nto study how the trade-off between the deformation regularizer and similarity\nmeasures, including mutual information and our proposed correlation ratio,\naffects the registration performance.", "AI": {"tldr": "Proposes a differentiable correlation ratio for deep learning-based multi-modal deformable image registration, validated on neuroimaging data, and explores trade-offs in a Bayesian framework.", "motivation": "Current deep learning methods underexplore the correlation ratio, a historically useful similarity measure for multi-modal image registration.", "method": "Extends the non-differentiable correlation ratio using Parzen windowing approximation for backpropagation and employs a Bayesian framework to study trade-offs.", "result": "Validated on a multi-modal neuroimaging dataset, showing effectiveness of the proposed differentiable correlation ratio.", "conclusion": "The differentiable correlation ratio and Bayesian framework enhance multi-modal deformable image registration performance."}}
{"id": "2504.11500", "pdf": "https://arxiv.org/pdf/2504.11500", "abs": "https://arxiv.org/abs/2504.11500", "authors": ["Kaicong Huang", "Talha Azfar", "Jack Reilly", "Ruimin Ke"], "title": "TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Transit Origin-Destination (OD) data are essential for transit planning,\nparticularly in route optimization and demand-responsive paratransit systems.\nTraditional methods, such as manual surveys, are costly and inefficient, while\nBluetooth and WiFi-based approaches require passengers to carry specific\ndevices, limiting data coverage. On the other hand, most transit vehicles are\nequipped with onboard cameras for surveillance, offering an opportunity to\nrepurpose them for edge-based OD data collection through visual person\nre-identification (ReID). However, such approaches face significant challenges,\nincluding severe occlusion and viewpoint variations in transit environments,\nwhich greatly reduce matching accuracy and hinder their adoption. Moreover,\ndesigning effective algorithms that can operate efficiently on edge devices\nremains an open challenge. To address these challenges, we propose TransitReID,\na novel framework for individual-level transit OD data collection. TransitReID\nconsists of two key components: (1) An occlusion-robust ReID algorithm\nfeaturing a variational autoencoder guided region-attention mechanism that\nadaptively focuses on visible body regions through reconstruction\nloss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic\nMatching (HSDM) mechanism specifically designed for efficient and robust\ntransit OD matching which balances storage, speed, and accuracy. Additionally,\na multi-threaded design supports near real-time operation on edge devices,\nwhich also ensuring privacy protection. We also introduce a ReID dataset\ntailored for complex bus environments to address the lack of relevant training\ndata. Experimental results demonstrate that TransitReID achieves\nstate-of-the-art performance in ReID tasks, with an accuracy of approximately\n90\\% in bus route simulations.", "AI": {"tldr": "TransitReID is a novel framework for collecting transit OD data using onboard cameras and visual person re-identification, addressing occlusion and efficiency challenges with innovative algorithms and edge-device optimization.", "motivation": "Traditional transit OD data collection methods are costly, inefficient, or limited in coverage. Onboard cameras offer an opportunity, but occlusion and viewpoint variations hinder accuracy.", "method": "TransitReID combines an occlusion-robust ReID algorithm with a Hierarchical Storage and Dynamic Matching mechanism, optimized for edge devices and privacy protection.", "result": "The framework achieves ~90% accuracy in bus route simulations, outperforming existing methods.", "conclusion": "TransitReID provides an efficient, accurate, and privacy-preserving solution for transit OD data collection, overcoming key challenges in the field."}}
{"id": "2504.11601", "pdf": "https://arxiv.org/pdf/2504.11601", "abs": "https://arxiv.org/abs/2504.11601", "authors": ["Bruno Giorgio"], "title": "Dueling Deep Reinforcement Learning for Financial Time Series", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for solving\ndecision-making problems in dynamic environments. In this research, we explore\nthe application of Double DQN (DDQN) and Dueling Network Architectures, to\nfinancial trading tasks using historical SP500 index data. Our focus is\ntraining agents capable of optimizing trading strategies while accounting for\npractical constraints such as transaction costs. The study evaluates the model\nperformance across scenarios with and without commissions, highlighting the\nimpact of cost-sensitive environments on reward dynamics. Despite computational\nlimitations and the inherent complexity of financial time series data, the\nagent successfully learned meaningful trading policies. The findings confirm\nthat RL agents, even when trained on limited datasets, can outperform random\nstrategies by leveraging advanced architectures such as DDQN and Dueling\nNetworks. However, significant challenges persist, particularly with a\nsub-optimal policy due to the complexity of data source.", "AI": {"tldr": "The paper explores using Double DQN and Dueling Networks for financial trading with SP500 data, focusing on cost-sensitive environments. RL agents outperformed random strategies despite data complexity.", "motivation": "To apply RL in financial trading, optimizing strategies under practical constraints like transaction costs.", "method": "Used Double DQN and Dueling Network Architectures on historical SP500 data, evaluating performance with and without commissions.", "result": "RL agents learned meaningful policies and outperformed random strategies, though data complexity caused sub-optimal policies.", "conclusion": "RL shows promise in trading but faces challenges due to data complexity and computational limits."}}
{"id": "2504.11900", "pdf": "https://arxiv.org/pdf/2504.11900", "abs": "https://arxiv.org/abs/2504.11900", "authors": ["Kabir Ahuja", "Melanie Sclar", "Yulia Tsvetkov"], "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.", "AI": {"tldr": "The paper introduces FlawedFictions, a benchmark for evaluating LLMs' ability to detect plot holes in stories, revealing their limitations in narrative consistency and reasoning.", "motivation": "To rigorously assess LLMs' deeper language understanding and reasoning skills, particularly in narrative consistency, which existing benchmarks overlook.", "method": "Developed FlawedFictionsMaker, an algorithm to synthesize plot holes in stories, and constructed the FlawedFictions benchmark with human filtering for quality.", "result": "State-of-the-art LLMs perform poorly in detecting plot holes, especially in longer stories, and LLM-based story tasks introduce more plot holes than human-written originals.", "conclusion": "LLMs struggle with nuanced narrative reasoning, highlighting the need for improved benchmarks and models for deeper language understanding."}}
{"id": "2504.11517", "pdf": "https://arxiv.org/pdf/2504.11517", "abs": "https://arxiv.org/abs/2504.11517", "authors": ["Riad Ibadulla", "Thomas M. Chen", "Constantino Carlos Reyes-Aldasoro"], "title": "ConvShareViT: Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper introduces ConvShareViT, a novel deep learning architecture that\nadapts Vision Transformers (ViTs) to the 4f free-space optical system.\nConvShareViT replaces linear layers in multi-head self-attention (MHSA) and\nMultilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared\nweights across input channels. Through the development of ConvShareViT, the\nbehaviour of convolutions within MHSA and their effectiveness in learning the\nattention mechanism were analysed systematically. Experimental results\ndemonstrate that certain configurations, particularly those using valid-padded\nshared convolutions, can successfully learn attention, achieving comparable\nattention scores to those obtained with standard ViTs. However, other\nconfigurations, such as those using same-padded convolutions, show limitations\nin attention learning and operate like regular CNNs rather than transformer\nmodels. ConvShareViT architectures are specifically optimised for the 4f\noptical system, which takes advantage of the parallelism and high-resolution\ncapabilities of optical systems. Results demonstrate that ConvShareViT can\ntheoretically achieve up to 3.04 times faster inference than GPU-based systems.\nThis potential acceleration makes ConvShareViT an attractive candidate for\nfuture optical deep learning applications and proves that our ViT\n(ConvShareViT) can be employed using only the convolution operation, via the\nnecessary optimisation of the ViT to balance performance and complexity.", "AI": {"tldr": "ConvShareViT adapts Vision Transformers for optical systems by replacing linear layers with shared depthwise convolutions, achieving comparable attention scores and faster inference.", "motivation": "To adapt Vision Transformers for 4f optical systems, leveraging their parallelism and high-resolution capabilities while simplifying the architecture.", "method": "Replaces MHSA and MLP linear layers with shared depthwise convolutions, analyzing their effectiveness in learning attention.", "result": "Valid-padded shared convolutions achieve comparable attention scores to standard ViTs, while same-padded ones behave like CNNs. Theoretical inference is 3.04x faster than GPUs.", "conclusion": "ConvShareViT is promising for optical deep learning, balancing performance and complexity by using only convolutions."}}
{"id": "2504.11588", "pdf": "https://arxiv.org/pdf/2504.11588", "abs": "https://arxiv.org/abs/2504.11588", "authors": ["Siteng Ma", "Honghui Du", "Yu An", "Jing Wang", "Qinqin Wang", "Haochang Wu", "Aonghus Lawlor", "Ruihai Dong"], "title": "Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 92C50, 92C55", "I.2.10; I.4.5; I.4.6; I.4.9; J.3"], "comment": "33 pages, 10 figures, 8 tables. Will be submit to Medical Image\n  Analysis", "summary": "Deep learning has achieved significant breakthroughs in medical imaging, but\nthese advancements are often dependent on large, well-annotated datasets.\nHowever, obtaining such datasets poses a significant challenge, as it requires\ntime-consuming and labor-intensive annotations from medical experts.\nConsequently, there is growing interest in learning paradigms such as\nincomplete, inexact, and absent supervision, which are designed to operate\nunder limited, inexact, or missing labels. This survey categorizes and reviews\nthe evolving research in these areas, analyzing around 600 notable\ncontributions since 2018. It covers tasks such as image classification,\nsegmentation, and detection across various medical application areas, including\nbut not limited to brain, chest, and cardiac imaging. We attempt to establish\nthe relationships among existing research studies in related areas. We provide\nformal definitions of different learning paradigms and offer a comprehensive\nsummary and interpretation of various learning mechanisms and strategies,\naiding readers in better understanding the current research landscape and\nideas. We also discuss potential future research challenges.", "AI": {"tldr": "A survey on deep learning in medical imaging under limited, inexact, or missing labels, reviewing 600+ studies since 2018, covering tasks like classification, segmentation, and detection.", "motivation": "Challenges in obtaining large, well-annotated medical datasets drive interest in learning paradigms like incomplete, inexact, and absent supervision.", "method": "Categorizes and reviews research, establishes relationships among studies, and provides formal definitions and summaries of learning mechanisms.", "result": "Comprehensive analysis of learning strategies and their applications in medical imaging tasks.", "conclusion": "Identifies future research challenges and aids understanding of the current landscape in limited-label learning for medical imaging."}}
{"id": "2504.11623", "pdf": "https://arxiv.org/pdf/2504.11623", "abs": "https://arxiv.org/abs/2504.11623", "authors": ["Jinsung Jeon", "Jaehyeon Park", "Sewon Park", "Jeongwhan Choi", "Minjung Kim", "Noseong Park"], "title": "Possibility for Proactive Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICLR 2025 I Can't Believe It's Not Better: Challenges in\n  Applied Deep Learning Workshop (ICBINB)", "summary": "Time-series anomaly detection, which detects errors and failures in a\nworkflow, is one of the most important topics in real-world applications. The\npurpose of time-series anomaly detection is to reduce potential damages or\nlosses. However, existing anomaly detection models detect anomalies through the\nerror between the model output and the ground truth (observed) value, which\nmakes them impractical. In this work, we present a \\textit{proactive} approach\nfor time-series anomaly detection based on a time-series forecasting model\nspecialized for anomaly detection and a data-driven anomaly detection model.\nOur proactive approach establishes an anomaly threshold from training data with\na data-driven anomaly detection model, and anomalies are subsequently detected\nby identifying predicted values that exceed the anomaly threshold. In addition,\nwe extensively evaluated the model using four anomaly detection benchmarks and\nanalyzed both predictable and unpredictable anomalies. We attached the source\ncode as supplementary material.", "AI": {"tldr": "A proactive approach for time-series anomaly detection using forecasting and data-driven models, evaluated on benchmarks.", "motivation": "Existing models rely on error between output and ground truth, making them impractical. This work aims to reduce potential damages by detecting anomalies proactively.", "method": "Combines a time-series forecasting model with a data-driven anomaly detection model to establish thresholds from training data.", "result": "Evaluated on four benchmarks, analyzing predictable and unpredictable anomalies. Source code provided.", "conclusion": "Proactive approach improves practicality and effectiveness in detecting anomalies."}}
{"id": "2504.11934", "pdf": "https://arxiv.org/pdf/2504.11934", "abs": "https://arxiv.org/abs/2504.11934", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025", "summary": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.", "AI": {"tldr": "The paper explores using large language models (LLMs) to evaluate gender-neutral translation (GNT), addressing limitations of current monolingual classifiers. Two prompting methods are tested, with phrase-level annotations improving accuracy.", "motivation": "Current GNT evaluation methods are limited to monolingual classifiers, which ignore the source text and require extensive data for new languages. LLMs offer a scalable alternative.", "method": "Two prompting approaches are investigated: one for sentence-level assessments and another with phrase-level annotations (chain-of-thought) before sentence-level judgments. Experiments involve five LLMs across multiple languages.", "result": "LLMs effectively evaluate GNT, with phrase-level prompting consistently improving accuracy across all models.", "conclusion": "LLMs provide a scalable and accurate solution for GNT evaluation, outperforming current monolingual methods."}}
{"id": "2504.11966", "pdf": "https://arxiv.org/pdf/2504.11966", "abs": "https://arxiv.org/abs/2504.11966", "authors": ["Linjuan Fan", "Di Wen", "Kunyu Peng", "Kailun Yang", "Jiaming Zhang", "Ruiping Liu", "Yufan Chen", "Junwei Zheng", "Jiamin Wu", "Xudong Han", "Rainer Stiefelhagen"], "title": "Exploring Video-Based Driver Activity Recognition under Noisy Labels", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "comment": "The source code is available at\n  https://github.com/ilonafan/DAR-noisy-labels", "summary": "As an open research topic in the field of deep learning, learning with noisy\nlabels has attracted much attention and grown rapidly over the past ten years.\nLearning with label noise is crucial for driver distraction behavior\nrecognition, as real-world video data often contains mislabeled samples,\nimpacting model reliability and performance. However, label noise learning is\nbarely explored in the driver activity recognition field. In this paper, we\npropose the first label noise learning approach for the driver activity\nrecognition task. Based on the cluster assumption, we initially enable the\nmodel to learn clustering-friendly low-dimensional representations from given\nvideos and assign the resultant embeddings into clusters. We subsequently\nperform co-refinement within each cluster to smooth the classifier outputs.\nFurthermore, we propose a flexible sample selection strategy that combines two\nselection criteria without relying on any hyperparameters to filter clean\nsamples from the training dataset. We also incorporate a self-adaptive\nparameter into the sample selection process to enforce balancing across\nclasses. A comprehensive variety of experiments on the public Drive&Act dataset\nfor all granularity levels demonstrates the superior performance of our method\nin comparison with other label-denoising methods derived from the image\nclassification field. The source code is available at\nhttps://github.com/ilonafan/DAR-noisy-labels.", "AI": {"tldr": "The paper introduces a label noise learning approach for driver activity recognition, combining clustering, co-refinement, and a flexible sample selection strategy to improve model reliability.", "motivation": "Real-world video data often contains mislabeled samples, which degrade model performance, but label noise learning is underexplored in driver activity recognition.", "method": "The method involves learning clustering-friendly representations, co-refining classifier outputs within clusters, and using a hyperparameter-free sample selection strategy with a self-adaptive parameter for class balancing.", "result": "The approach outperforms other label-denoising methods on the Drive&Act dataset across all granularity levels.", "conclusion": "The proposed method effectively addresses label noise in driver activity recognition, offering superior performance and flexibility."}}
{"id": "2504.11637", "pdf": "https://arxiv.org/pdf/2504.11637", "abs": "https://arxiv.org/abs/2504.11637", "authors": ["Yiming Xiao", "Ali Mostafavi"], "title": "DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization", "categories": ["cs.CV"], "comment": "23 pages, 6 figures", "summary": "Natural disasters increasingly threaten communities worldwide, creating an\nurgent need for rapid, reliable building damage assessment to guide emergency\nresponse and recovery efforts. Current methods typically classify damage in\nbinary (damaged/undamaged) or ordinal severity terms, limiting their practical\nutility. In fact, the determination of damage typology is crucial for response\nand recovery efforts. To address this important gap, this paper introduces\nDamageCAT, a novel framework that provides typology-based categorical damage\ndescriptions rather than simple severity ratings. Accordingly, this study\npresents two key contributions: (1) the BD-TypoSAT dataset containing satellite\nimage triplets (pre-disaster, post-disaster, and damage masks) from Hurricane\nIda with four damage categories (partial roof damage, total roof damage,\npartial structural collapse, and total structural collapse), and (2) a\nhierarchical U-Net-based transformer architecture that effectively processes\npre-post disaster image pairs to identify and categorize building damage.\nDespite significant class imbalances in the training data, our model achieved\nrobust performance with overall metrics of 0.7921 Intersection over Union (IoU)\nand 0.8835 F1 scores across all categories. The model's capability to recognize\nintricate damage typology in less common categories is especially remarkable.\nThe DamageCAT framework advances automated damage assessment by providing\nactionable, typological information that better supports disaster response\ndecision-making and resource allocation compared to traditional severity-based\napproaches.", "AI": {"tldr": "DamageCAT introduces a typology-based framework for building damage assessment, outperforming traditional severity-based methods with a hierarchical U-Net-based transformer model and a new dataset (BD-TypoSAT).", "motivation": "Current damage assessment methods use binary or ordinal severity ratings, which lack practical utility for disaster response. Typology-based descriptions are needed for better decision-making.", "method": "The paper presents DamageCAT, using a hierarchical U-Net-based transformer to analyze pre-post disaster satellite images (BD-TypoSAT dataset) and categorize damage into four types.", "result": "The model achieved 0.7921 IoU and 0.8835 F1 scores, excelling in recognizing rare damage types despite class imbalances.", "conclusion": "DamageCAT provides actionable, typological damage information, improving disaster response and resource allocation over traditional methods."}}
{"id": "2504.11645", "pdf": "https://arxiv.org/pdf/2504.11645", "abs": "https://arxiv.org/abs/2504.11645", "authors": ["Feng Zhu", "Aritra Mitra", "Robert W. Heath"], "title": "Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Motivated by collaborative reinforcement learning (RL) and optimization with\ntime-correlated data, we study a generic federated stochastic approximation\nproblem involving $M$ agents, where each agent is characterized by an\nagent-specific (potentially nonlinear) local operator. The goal is for the\nagents to communicate intermittently via a server to find the root of the\naverage of the agents' local operators. The generality of our setting stems\nfrom allowing for (i) Markovian data at each agent and (ii) heterogeneity in\nthe roots of the agents' local operators. The limited recent work that has\naccounted for both these features in a federated setting fails to guarantee\nconvergence to the desired point or to show any benefit of collaboration;\nfurthermore, they rely on projection steps in their algorithms to guarantee\nbounded iterates. Our work overcomes each of these limitations. We develop a\nnovel algorithm titled \\texttt{FedHSA}, and prove that it guarantees\nconvergence to the correct point, while enjoying an $M$-fold linear speedup in\nsample-complexity due to collaboration. To our knowledge, \\emph{this is the\nfirst finite-time result of its kind}, and establishing it (without relying on\na projection step) entails a fairly intricate argument that accounts for the\ninterplay between complex temporal correlations due to Markovian sampling,\nmultiple local steps to save communication, and the drift-effects induced by\nheterogeneous local operators. Our results have implications for a broad class\nof heterogeneous federated RL problems (e.g., policy evaluation and control)\nwith function approximation, where the agents' Markov decision processes can\ndiffer in their probability transition kernels and reward functions.", "AI": {"tldr": "The paper introduces FedHSA, a novel federated stochastic approximation algorithm for collaborative RL with heterogeneous agents, ensuring convergence and sample-complexity benefits.", "motivation": "The study addresses gaps in federated RL by accounting for Markovian data and heterogeneous agent-specific operators, overcoming limitations of prior work.", "method": "Develops FedHSA, a projection-free algorithm, handling Markovian sampling and local operator heterogeneity, with intermittent agent-server communication.", "result": "Proves FedHSA converges to the correct point with an M-fold linear speedup in sample-complexity, a first in finite-time results for such settings.", "conclusion": "FedHSA advances federated RL by enabling efficient collaboration among heterogeneous agents, with implications for policy evaluation and control."}}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952", "abs": "https://arxiv.org/abs/2504.11952", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Hamza Farooq"], "title": "Robust and Fine-Grained Detection of AI Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Feb ARR Submission", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "AI": {"tldr": "The paper introduces token classification models for detecting AI-generated content, especially in human-LLM co-authored texts, and presents a new dataset of 2.4M texts across 23 languages.", "motivation": "Existing systems struggle with short and co-authored texts, prompting the need for a robust detection method adaptable to various generators and domains.", "method": "Token classification models trained on a large dataset of human-LLM co-authored texts, tested on unseen domains, generators, and adversarial inputs.", "result": "Models performed well across diverse scenarios, including non-native speakers and adversarial inputs, with detailed performance analysis by domain and generator.", "conclusion": "The proposed models and dataset advance detection of AI-generated content, particularly in co-authored and adversarial settings."}}
{"id": "2504.12112", "pdf": "https://arxiv.org/pdf/2504.12112", "abs": "https://arxiv.org/abs/2504.12112", "authors": ["Zhenyu Yu", "Mohd Yamani Inda Idris", "Pei Wang"], "title": "A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Remote sensing imagery is essential for environmental monitoring,\nagricultural management, and disaster response. However, data loss due to cloud\ncover, sensor failures, or incomplete acquisition-especially in high-resolution\nand high-frequency tasks-severely limits satellite imagery's effectiveness.\nTraditional interpolation methods struggle with large missing areas and complex\nstructures. Remote sensing imagery consists of multiple bands, each with\ndistinct meanings, and ensuring consistency across bands is critical to avoid\nanomalies in the combined images. This paper proposes SatelliteMaker, a\ndiffusion-based method that reconstructs missing data across varying levels of\ndata loss while maintaining spatial, spectral, and temporal consistency. We\nalso propose Digital Elevation Model (DEM) as a conditioning input and use\ntailored prompts to generate realistic images, making diffusion models\napplicable to quantitative remote sensing tasks. Additionally, we propose a\nVGG-Adapter module based on Distribution Loss, which reduces distribution\ndiscrepancy and ensures style consistency. Extensive experiments show that\nSatelliteMaker achieves state-of-the-art performance across multiple tasks.", "AI": {"tldr": "SatelliteMaker is a diffusion-based method for reconstructing missing remote sensing data, ensuring consistency across bands and outperforming traditional methods.", "motivation": "Data loss in remote sensing imagery due to cloud cover, sensor failures, or incomplete acquisition limits its effectiveness, especially in high-resolution tasks.", "method": "Proposes SatelliteMaker, a diffusion-based method with DEM conditioning and tailored prompts, and introduces a VGG-Adapter module for style consistency.", "result": "Achieves state-of-the-art performance in reconstructing missing data while maintaining spatial, spectral, and temporal consistency.", "conclusion": "SatelliteMaker effectively addresses data loss challenges in remote sensing, offering a robust solution for quantitative tasks."}}
{"id": "2504.11662", "pdf": "https://arxiv.org/pdf/2504.11662", "abs": "https://arxiv.org/abs/2504.11662", "authors": ["Marcos Mendes", "Gon\u00e7alo Perna", "Pedro Rito", "Duarte Raposo", "Susana Sargento"], "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing", "categories": ["cs.CV", "68T45"], "comment": "30th ITS World Congress, Dubai, UAE, 16-20 September 2024", "summary": "The World Health Organization suggests that road traffic crashes cost\napproximately 518 billion dollars globally each year, which accounts for 3% of\nthe gross domestic product for most countries. Most fatal road accidents in\nurban areas involve Vulnerable Road Users (VRUs). Smart cities environments\npresent innovative approaches to combat accidents involving cutting-edge\ntechnologies, that include advanced sensors, extensive datasets, Machine\nLearning (ML) models, communication systems, and edge computing. This paper\nproposes a strategy and an implementation of a system for road monitoring and\nsafety for smart cities, based on Computer Vision (CV) and edge computing.\nPromising results were obtained by implementing vision algorithms and tracking\nusing surveillance cameras, that are part of a Smart City testbed, the Aveiro\nTech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars,\npedestrians, and bicycles, while predicting the road state, the distance\nbetween moving objects, and inferring on collision events to prevent\ncollisions, in near real-time.", "AI": {"tldr": "The paper proposes a computer vision and edge computing-based system for road monitoring in smart cities to reduce accidents involving vulnerable road users, achieving real-time detection and collision prediction.", "motivation": "Road traffic crashes cost 3% of global GDP annually, with urban fatal accidents often involving vulnerable road users. Smart cities can leverage technology to address this issue.", "method": "The system uses computer vision and edge computing, implemented with vision algorithms and tracking via surveillance cameras in the Aveiro Tech City Living Lab.", "result": "The algorithm accurately detects and tracks cars, pedestrians, and bicycles, predicts road state and distances, and infers collision events in near real-time.", "conclusion": "The proposed system shows promise for enhancing road safety in smart cities by preventing collisions through real-time monitoring."}}
{"id": "2504.11651", "pdf": "https://arxiv.org/pdf/2504.11651", "abs": "https://arxiv.org/abs/2504.11651", "authors": ["Tianyi Zhang", "Yang Sui", "Shaochen Zhong", "Vipin Chaudhary", "Xia Hu", "Anshumali Shrivastava"], "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) have grown rapidly in size, creating significant\nchallenges for efficient deployment on resource-constrained hardware. In this\npaper, we introduce Dynamic-Length Float (DFloat11), a lossless compression\nframework that reduces LLM size by 30% while preserving outputs that are\nbit-for-bit identical to the original model. DFloat11 is motivated by the low\nentropy in the BFloat16 weight representation of LLMs, which reveals\nsignificant inefficiency in existing storage format. By applying entropy\ncoding, DFloat11 assigns dynamic-length encodings to weights based on\nfrequency, achieving near information-optimal compression without any loss of\nprecision. To facilitate efficient inference with dynamic-length encodings, we\ndevelop a custom GPU kernel for fast online decompression. Our design\nincorporates the following: (i) decomposition of memory-intensive lookup tables\n(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for\ncoordinating thread read/write positions using lightweight auxiliary variables,\nand (iii) transformer-block-level decompression to minimize latency.\nExperiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,\nvalidates our hypothesis that DFloat11 achieves around 30% model size reduction\nwhile preserving bit-for-bit exact outputs. Compared to a potential alternative\nof offloading parts of an uncompressed model to the CPU to meet memory\nconstraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.\nWith a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context\nlengths than uncompressed models. Notably, our method enables lossless\ninference of Llama-3.1-405B, an 810GB model, on a single node equipped with\n8x80GB GPUs. Our code and models are available at\nhttps://github.com/LeanModels/DFloat11.", "AI": {"tldr": "DFloat11 is a lossless compression framework for LLMs, reducing model size by 30% while maintaining bit-for-bit identical outputs. It uses entropy coding and custom GPU kernels for efficient decompression, achieving higher throughput and longer context lengths.", "motivation": "The rapid growth of LLM sizes poses deployment challenges on resource-constrained hardware. Existing storage formats are inefficient due to low entropy in BFloat16 weight representations.", "method": "DFloat11 applies entropy coding for dynamic-length encodings, decomposes LUTs for GPU SRAM, and uses a two-phase kernel for efficient decompression.", "result": "Experiments show 30% model size reduction, 1.9-38.8x higher throughput, and 5.3-13.17x longer context lengths compared to uncompressed models.", "conclusion": "DFloat11 enables efficient, lossless deployment of large LLMs on constrained hardware, with significant performance improvements."}}
{"id": "2504.11972", "pdf": "https://arxiv.org/pdf/2504.11972", "abs": "https://arxiv.org/abs/2504.11972", "authors": ["Xanh Ho", "Jiahao Huang", "Florian Boudin", "Akiko Aizawa"], "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA", "categories": ["cs.CL"], "comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa", "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.", "AI": {"tldr": "LLM-as-a-judge outperforms traditional EM/F1 metrics in evaluating QA models, showing higher correlation with human judgments (0.85 vs. 0.17/0.36).", "motivation": "Traditional metrics (EM/F1) inadequately capture QA model performance, prompting exploration of LLM-as-a-judge for better evaluation.", "method": "Reassessed QA model performance using LLM-as-a-judge across four datasets, comparing LLM families and answer types.", "result": "LLM-as-a-judge correlates strongly with human judgments (0.85), outperforming EM/F1, though struggles with complex answer types like 'job'.", "conclusion": "LLM-as-a-judge is a superior alternative to EM/F1, despite minor limitations, and avoids bias issues like self-preference."}}
{"id": "2504.12169", "pdf": "https://arxiv.org/pdf/2504.12169", "abs": "https://arxiv.org/abs/2504.12169", "authors": ["Joanne Lin", "Crispian Morris", "Ruirui Lin", "Fan Zhang", "David Bull", "Nantheera Anantrasirichai"], "title": "Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video Pipeline", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Low-light conditions pose significant challenges for both human and machine\nannotation. This in turn has led to a lack of research into machine\nunderstanding for low-light images and (in particular) videos. A common\napproach is to apply annotations obtained from high quality datasets to\nsynthetically created low light versions. In addition, these approaches are\noften limited through the use of unrealistic noise models. In this paper, we\npropose a new Degradation Estimation Network (DEN), which synthetically\ngenerates realistic standard RGB (sRGB) noise without the requirement for\ncamera metadata. This is achieved by estimating the parameters of\nphysics-informed noise distributions, trained in a self-supervised manner. This\nzero-shot approach allows our method to generate synthetic noisy content with a\ndiverse range of realistic noise characteristics, unlike other methods which\nfocus on recreating the noise characteristics of the training data. We evaluate\nour proposed synthetic pipeline using various methods trained on its synthetic\ndata for typical low-light tasks including synthetic noise replication, video\nenhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\%\nLPIPS, and 62\\% AP$_{50-95}$, respectively.", "AI": {"tldr": "A new Degradation Estimation Network (DEN) generates realistic sRGB noise for low-light images/videos without camera metadata, improving synthetic data quality for tasks like noise replication, video enhancement, and object detection.", "motivation": "Low-light conditions hinder annotation and research, with existing methods relying on unrealistic noise models or limited synthetic data.", "method": "DEN estimates physics-informed noise parameters in a self-supervised, zero-shot manner to create diverse, realistic synthetic noise.", "result": "Improvements of up to 24% KLD, 21% LPIPS, and 62% AP$_{50-95}$ in noise replication, video enhancement, and object detection tasks.", "conclusion": "DEN advances low-light machine understanding by generating realistic synthetic noise without metadata, outperforming existing methods."}}
{"id": "2504.11669", "pdf": "https://arxiv.org/pdf/2504.11669", "abs": "https://arxiv.org/abs/2504.11669", "authors": ["Amirhossein Dadashzadeh", "Parsa Esmati", "Majid Mirmehdi"], "title": "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Source-Free Unsupervised Video Domain Adaptation (SFUVDA)\nleverage vision-language models to enhance pseudo-label generation. However,\nchallenges such as noisy pseudo-labels and over-confident predictions limit\ntheir effectiveness in adapting well across domains. We propose Co-STAR, a\nnovel framework that integrates curriculum learning with collaborative\nself-training between a source-trained teacher and a contrastive\nvision-language model (CLIP). Our curriculum learning approach employs a\nreliability-based weight function that measures bidirectional prediction\nalignment between the teacher and CLIP, balancing between confident and\nuncertain predictions. This function preserves uncertainty for difficult\nsamples, while prioritizing reliable pseudo-labels when the predictions from\nboth models closely align. To further improve adaptation, we propose Adaptive\nCurriculum Regularization, which modifies the learning priority of samples in a\nprobabilistic, adaptive manner based on their confidence scores and prediction\nstability, mitigating overfitting to noisy and over-confident samples.\nExtensive experiments across multiple video domain adaptation benchmarks\ndemonstrate that Co-STAR consistently outperforms state-of-the-art SFUVDA\nmethods. Code is available at: https://github.com/Plrbear/Co-Star", "AI": {"tldr": "Co-STAR integrates curriculum learning and collaborative self-training between a teacher model and CLIP to improve pseudo-label quality in SFUVDA, outperforming state-of-the-art methods.", "motivation": "Addressing noisy pseudo-labels and over-confident predictions in SFUVDA by leveraging vision-language models.", "method": "Combines curriculum learning with a reliability-based weight function and Adaptive Curriculum Regularization to balance confident and uncertain predictions.", "result": "Co-STAR consistently outperforms existing SFUVDA methods in benchmarks.", "conclusion": "The proposed framework effectively mitigates noise and overconfidence, enhancing domain adaptation performance."}}
{"id": "2504.11699", "pdf": "https://arxiv.org/pdf/2504.11699", "abs": "https://arxiv.org/abs/2504.11699", "authors": ["Rui Xue", "Tianfu Wu"], "title": "H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in\nrepresentation learning, a challenge further amplified in self-supervised\nsettings. We propose H$^3$GNNs, an end-to-end self-supervised learning\nframework that harmonizes both structural properties through two key\ninnovations: (i) Joint Structural Node Encoding. We embed nodes into a unified\nspace combining linear and non-linear feature projections with K-hop structural\nrepresentations via a Weighted Graph Convolution Network(WGCN). A\ncross-attention mechanism enhances awareness and adaptability to heterophily\nand homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive\nArchitectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a\nteacher-student model, the student sees the masked input graph and predicts\nnode features inferred by the teacher that sees the full input graph in the\njoint encoding space. To enhance learning difficulty, we introduce two novel\nnode-predictive-difficulty-based masking strategies. Experiments on seven\nbenchmarks (four heterophily datasets and three homophily datasets) confirm the\neffectiveness and efficiency of H$^3$GNNs across diverse graph types. Our\nH$^3$GNNs achieves overall state-of-the-art performance on the four heterophily\ndatasets, while retaining on-par performance to previous state-of-the-art\nmethods on the three homophily datasets.", "AI": {"tldr": "H$^3$GNNs is a self-supervised framework balancing heterophily and homophily in GNNs using joint structural encoding and dynamic masking strategies, achieving state-of-the-art results.", "motivation": "Address the challenge of balancing heterophily and homophily in GNNs, especially in self-supervised settings.", "method": "Proposes H$^3$GNNs with joint structural node encoding (linear/non-linear projections + WGCN) and teacher-student predictive architectures with dynamic masking.", "result": "Achieves state-of-the-art on heterophily datasets and matches performance on homophily datasets.", "conclusion": "H$^3$GNNs effectively harmonizes heterophily and homophily, demonstrating superior performance across diverse graph types."}}
{"id": "2504.11975", "pdf": "https://arxiv.org/pdf/2504.11975", "abs": "https://arxiv.org/abs/2504.11975", "authors": ["Ra\u00fal V\u00e1zquez", "Timothee Mickus", "Elaine Zosa", "Teemu Vahtola", "J\u00f6rg Tiedemann", "Aman Sinha", "Vincent Segonne", "Fernando S\u00e1nchez-Vega", "Alessandro Raganato", "Jind\u0159ich Libovick\u00fd", "Jussi Karlgren", "Shaoxiong Ji", "Jind\u0159ich Helcl", "Liane Guillou", "Ona de Gibert", "Jaione Bengoetxea", "Joseph Attieh", "Marianna Apidianaki"], "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes", "categories": ["cs.CL"], "comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)", "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.", "AI": {"tldr": "The Mu-SHROOM shared task focuses on detecting hallucinations in LLM outputs across 14 languages, using span-labeling. It attracted 2,618 submissions from 43 teams, highlighting community interest. Results and key performance factors are analyzed, with challenges like language variability and annotator disagreement noted.", "motivation": "To address the issue of hallucinations and overgeneration in instruction-tuned LLMs, and to foster community engagement in developing solutions.", "method": "Framed as a span-labeling task, the task involved detecting hallucinations in LLM outputs across 14 languages, with submissions from diverse methodologies.", "result": "2,618 submissions from 43 teams were received, with empirical analysis identifying key performance factors. Challenges included language variability and annotator disagreement.", "conclusion": "The task successfully engaged the community in hallucination detection, revealing performance insights and ongoing challenges like language differences and labeling consistency."}}
{"id": "2504.12245", "pdf": "https://arxiv.org/pdf/2504.12245", "abs": "https://arxiv.org/abs/2504.12245", "authors": ["Xia Wang", "Haiyang Sun", "Tiantian Cao", "Yueying Sun", "Min Feng"], "title": "SIDME: Self-supervised Image Demoir\u00e9ing via Masked Encoder-Decoder Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "21 pages, 13 figures", "summary": "Moir\\'e patterns, resulting from aliasing between object light signals and\ncamera sampling frequencies, often degrade image quality during capture.\nTraditional demoir\\'eing methods have generally treated images as a whole for\nprocessing and training, neglecting the unique signal characteristics of\ndifferent color channels. Moreover, the randomness and variability of moir\\'e\npattern generation pose challenges to the robustness of existing methods when\napplied to real-world data. To address these issues, this paper presents SIDME\n(Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction),\na novel model designed to generate high-quality visual images by effectively\nprocessing moir\\'e patterns. SIDME combines a masked encoder-decoder\narchitecture with self-supervised learning, allowing the model to reconstruct\nimages using the inherent properties of camera sampling frequencies. A key\ninnovation is the random masked image reconstructor, which utilizes an\nencoder-decoder structure to handle the reconstruction task. Furthermore, since\nthe green channel in camera sampling has a higher sampling frequency compared\nto red and blue channels, a specialized self-supervised loss function is\ndesigned to improve the training efficiency and effectiveness. To ensure the\ngeneralization ability of the model, a self-supervised moir\\'e image generation\nmethod has been developed to produce a dataset that closely mimics real-world\nconditions. Extensive experiments demonstrate that SIDME outperforms existing\nmethods in processing real moir\\'e pattern data, showing its superior\ngeneralization performance and robustness.", "AI": {"tldr": "SIDME introduces a self-supervised model for demoir\u00e9ing by leveraging masked encoder-decoder reconstruction and specialized loss functions for color channels, outperforming traditional methods.", "motivation": "Traditional demoir\u00e9ing methods treat images uniformly, ignoring color channel differences and struggling with real-world moir\u00e9 variability.", "method": "SIDME uses a masked encoder-decoder with self-supervised learning, a random masked reconstructor, and a specialized loss for the green channel.", "result": "SIDME excels in real-world moir\u00e9 pattern processing, showing superior generalization and robustness.", "conclusion": "SIDME's innovative approach effectively addresses moir\u00e9 pattern challenges, offering high-quality image reconstruction."}}
{"id": "2504.11686", "pdf": "https://arxiv.org/pdf/2504.11686", "abs": "https://arxiv.org/abs/2504.11686", "authors": ["Yiran He", "Yun Cao", "Bowen Yang", "Zeyu Zhang"], "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 11 figures, 13IHMMSec2025", "summary": "The rapid development of generative AI facilitates content creation and makes\nimage manipulation easier and more difficult to detect. While multimodal Large\nLanguage Models (LLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating AI-generated Content (AIGC) and struggle to\ncomprehend local forgery details. In this work, we investigate the application\nof multimodal LLMs in forgery detection. We propose a framework capable of\nevaluating image authenticity, localizing tampered regions, providing evidence,\nand tracing generation methods based on semantic tampering clues. Our method\ndemonstrates that the potential of LLMs in forgery analysis can be effectively\nunlocked through meticulous prompt engineering and the application of few-shot\nlearning techniques. We conduct qualitative and quantitative experiments and\nshow that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in\nLaMa, which is competitive with state-of-the-art AIGC detection methods. We\nfurther discuss the limitations of multimodal LLMs in such tasks and propose\npotential improvements.", "AI": {"tldr": "The paper explores using multimodal LLMs for detecting AI-generated content, proposing a framework for image authenticity evaluation, localization, and tracing. It achieves competitive accuracy with state-of-the-art methods.", "motivation": "Generative AI makes content manipulation harder to detect, and existing LLMs lack tailored capabilities for forgery analysis.", "method": "A framework leveraging prompt engineering and few-shot learning to unlock LLMs' potential in forgery detection.", "result": "GPT4V achieves 92.1% accuracy in Autosplice and 86.3% in LaMa, competitive with top AIGC detection methods.", "conclusion": "Multimodal LLMs show promise in forgery detection but have limitations; improvements are suggested."}}
{"id": "2504.11702", "pdf": "https://arxiv.org/pdf/2504.11702", "abs": "https://arxiv.org/abs/2504.11702", "authors": ["Dorottya Zelenyanszki", "Zhe Hou", "Kamanashis Biswas", "Vallipuram Muthukkumarasamy"], "title": "Clustering and analysis of user behaviour in blockchain: A case study of Planet IX", "categories": ["cs.LG", "cs.CR"], "comment": "15 pages, 8 figures, submitted to Blockchain: Research and\n  Applications", "summary": "Decentralised applications (dApps) that run on public blockchains have the\nbenefit of trustworthiness and transparency as every activity that happens on\nthe blockchain can be publicly traced through the transaction data. However,\nthis introduces a potential privacy problem as this data can be tracked and\nanalysed, which can reveal user-behaviour information. A user behaviour\nanalysis pipeline was proposed to present how this type of information can be\nextracted and analysed to identify separate behavioural clusters that can\ndescribe how users behave in the game. The pipeline starts with the collection\nof transaction data, involving smart contracts, that is collected from a\nblockchain-based game called Planet IX. Both the raw transaction information\nand the transaction events are considered in the data collection. From this\ndata, separate game actions can be formed and those are leveraged to present\nhow and when the users conducted their in-game activities in the form of user\nflows. An extended version of these user flows also presents how the\nNon-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter\nis given as input for a Graph Neural Network (GNN) model to provide graph\nembeddings for these flows which then can be leveraged by clustering algorithms\nto cluster user behaviours into separate behavioural clusters. We benchmark and\ncompare well-known clustering algorithms as a part of the proposed method. The\nuser behaviour clusters were analysed and visualised in a graph format. It was\nfound that behavioural information can be extracted regarding the users that\nbelong to these clusters. Such information can be exploited by malicious users\nto their advantage. To demonstrate this, a privacy threat model was also\npresented based on the results that correspond to multiple potentially affected\nareas.", "AI": {"tldr": "The paper proposes a pipeline to analyze user behavior in blockchain-based games, revealing privacy risks through transaction data and clustering algorithms.", "motivation": "To address privacy concerns in decentralized applications (dApps) by demonstrating how user behavior can be extracted and analyzed from public blockchain data.", "method": "A pipeline involving data collection from a blockchain game (Planet IX), user flow analysis, GNN-based graph embeddings, and clustering algorithms to identify behavioral clusters.", "result": "Behavioral clusters were identified and visualized, revealing potential privacy threats from malicious exploitation of user data.", "conclusion": "The study highlights privacy risks in dApps and proposes a threat model to address potential vulnerabilities."}}
{"id": "2504.11986", "pdf": "https://arxiv.org/pdf/2504.11986", "abs": "https://arxiv.org/abs/2504.11986", "authors": ["Jose Manuel Guevara-Vela"], "title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.", "AI": {"tldr": "The paper draws an analogy between LLMs and quasicrystals, emphasizing their ability to produce coherent linguistic patterns without strict repetition, suggesting new evaluation and design approaches.", "motivation": "To reframe the understanding of LLMs by comparing them to quasicrystals, highlighting their unique structural coherence rather than just predictive accuracy or factuality.", "method": "Proposes viewing LLMs as generators of quasi-structured language, focusing on constraint propagation and coherence over token-level accuracy.", "result": "LLMs exhibit emergent patterning, characterized by constraint, resonance, and structural depth, rather than randomness or strict rules.", "conclusion": "Evaluating LLMs through this structural lens opens new paths for design and analysis, emphasizing patterns of coherence and constraint."}}
{"id": "2504.12255", "pdf": "https://arxiv.org/pdf/2504.12255", "abs": "https://arxiv.org/abs/2504.12255", "authors": ["Samuel R\u00e4ber", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Human Aligned Compression for Robust Models", "categories": ["cs.CV", "eess.IV"], "comment": "Presented at the Workshop AdvML at CVPR 2025", "summary": "Adversarial attacks on image models threaten system robustness by introducing\nimperceptible perturbations that cause incorrect predictions. We investigate\nhuman-aligned learned lossy compression as a defense mechanism, comparing two\nlearned models (HiFiC and ELIC) against traditional JPEG across various quality\nlevels. Our experiments on ImageNet subsets demonstrate that learned\ncompression methods outperform JPEG, particularly for Vision Transformer\narchitectures, by preserving semantically meaningful content while removing\nadversarial noise. Even in white-box settings where attackers can access the\ndefense, these methods maintain substantial effectiveness. We also show that\nsequential compression--applying rounds of\ncompression/decompression--significantly enhances defense efficacy while\nmaintaining classification performance. Our findings reveal that human-aligned\ncompression provides an effective, computationally efficient defense that\nprotects the image features most relevant to human and machine understanding.\nIt offers a practical approach to improving model robustness against\nadversarial threats.", "AI": {"tldr": "Learned compression methods (HiFiC, ELIC) outperform JPEG in defending against adversarial attacks on image models, especially for Vision Transformers, by preserving semantic content and removing noise. Sequential compression further enhances defense efficacy.", "motivation": "Adversarial attacks introduce imperceptible perturbations to images, leading to incorrect predictions. The study aims to evaluate human-aligned learned compression as a defense mechanism.", "method": "Compare learned compression models (HiFiC, ELIC) with JPEG across quality levels. Test on ImageNet subsets, including white-box settings, and evaluate sequential compression.", "result": "Learned methods outperform JPEG, preserving semantic content and removing adversarial noise. Sequential compression boosts defense efficacy without harming classification.", "conclusion": "Human-aligned compression is an effective, efficient defense against adversarial attacks, protecting features relevant to human and machine understanding."}}
{"id": "2504.11701", "pdf": "https://arxiv.org/pdf/2504.11701", "abs": "https://arxiv.org/abs/2504.11701", "authors": ["Yaohui Fang", "Xingce Wang"], "title": "Non-uniform Point Cloud Upsampling via Local Manifold Distribution", "categories": ["cs.CV", "math.DG"], "comment": null, "summary": "Existing learning-based point cloud upsampling methods often overlook the\nintrinsic data distribution charac?teristics of point clouds, leading to\nsuboptimal results when handling sparse and non-uniform point clouds. We\npropose a novel approach to point cloud upsampling by imposing constraints from\nthe perspective of manifold distributions. Leveraging the strong fitting\ncapability of Gaussian functions, our method employs a network to iteratively\noptimize Gaussian components and their weights, accurately representing local\nmanifolds. By utilizing the probabilistic distribution properties of Gaussian\nfunctions, we construct a unified statistical manifold to impose distribution\nconstraints on the point cloud. Experimental results on multiple datasets\ndemonstrate that our method generates higher-quality and more uniformly\ndistributed dense point clouds when processing sparse and non-uniform inputs,\noutperforming state-of-the-art point cloud upsampling techniques.", "AI": {"tldr": "A novel point cloud upsampling method using Gaussian functions to model manifold distributions, improving quality and uniformity in sparse, non-uniform inputs.", "motivation": "Existing methods ignore intrinsic data distribution, leading to suboptimal results for sparse and non-uniform point clouds.", "method": "Uses a network to iteratively optimize Gaussian components and weights, leveraging their probabilistic properties to impose distribution constraints.", "result": "Produces higher-quality, more uniformly distributed dense point clouds, outperforming state-of-the-art techniques.", "conclusion": "The approach effectively addresses limitations of current methods by incorporating manifold distribution constraints."}}
{"id": "2504.11713", "pdf": "https://arxiv.org/pdf/2504.11713", "abs": "https://arxiv.org/abs/2504.11713", "authors": ["Aaron Havens", "Benjamin Kurt Miller", "Bing Yan", "Carles Domingo-Enrich", "Anuroop Sriram", "Brandon Wood", "Daniel Levine", "Bin Hu", "Brandon Amos", "Brian Karrer", "Xiang Fu", "Guan-Horng Liu", "Ricky T. Q. Chen"], "title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.", "AI": {"tldr": "Adjoint Sampling is a scalable, efficient algorithm for learning diffusion processes to sample from unnormalized densities, outperforming previous methods in gradient updates and scalability.", "motivation": "To address limitations in scalability and efficiency of existing methods for sampling from unnormalized densities, particularly in large problem settings.", "method": "The algorithm leverages stochastic optimal control, allowing more gradient updates than energy evaluations, and incorporates symmetries and periodic boundary conditions for molecular modeling.", "result": "Demonstrated effectiveness on classical energy functions and neural network-based models, enabling amortized conformer generation across molecular systems.", "conclusion": "The approach is theoretically grounded, scalable, and practical, with plans to open-source benchmarks to advance computational chemistry research."}}
{"id": "2504.12052", "pdf": "https://arxiv.org/pdf/2504.12052", "abs": "https://arxiv.org/abs/2504.12052", "authors": ["Fran\u00e7ois Haguinet", "Jeffery L Painter", "Gregory E Powell", "Andrea Callegaro", "Andrew Bate"], "title": "Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": "30 pages, 7 figures, 5 supplementary figures", "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.", "AI": {"tldr": "The paper introduces a Bayesian dynamic borrowing (BDB) approach, IC SSM, to improve adverse event identification in spontaneous reporting systems by leveraging semantic similarity measures and Bayesian hierarchical modeling.", "motivation": "Current disproportionality analysis (DPA) methods rely on rigid hierarchical grouping, limiting their effectiveness. The study aims to enhance AE identification by incorporating continuous similarity-based borrowing.", "method": "The IC SSM approach integrates meta-analytic predictive priors and semantic similarity measures within a Bayesian hierarchical model, evaluated against traditional IC and HLGT-based methods using FAERS data (2015-2019).", "result": "IC SSM outperformed traditional methods in sensitivity and early signal detection, identifying more true positives and detecting signals sooner, despite minor trade-offs in F1 scores and Youden's index.", "conclusion": "The study advocates for SSM-informed Bayesian borrowing as a scalable improvement to DPA, suggesting future validation across datasets and exploration of additional similarity metrics."}}
{"id": "2504.11705", "pdf": "https://arxiv.org/pdf/2504.11705", "abs": "https://arxiv.org/abs/2504.11705", "authors": ["Adriano D'Alessandro", "Ali Mahdavi-Amiri", "Ghassan Hamarneh"], "title": "Learning What NOT to Count", "categories": ["cs.CV"], "comment": null, "summary": "Few/zero-shot object counting methods reduce the need for extensive\nannotations but often struggle to distinguish between fine-grained categories,\nespecially when multiple similar objects appear in the same scene. To address\nthis limitation, we propose an annotation-free approach that enables the\nseamless integration of new fine-grained categories into existing few/zero-shot\ncounting models. By leveraging latent generative models, we synthesize\nhigh-quality, category-specific crowded scenes, providing a rich training\nsource for adapting to new categories without manual labeling. Our approach\nintroduces an attention prediction network that identifies fine-grained\ncategory boundaries trained using only synthetic pseudo-annotated data. At\ninference, these fine-grained attention estimates refine the output of existing\nfew/zero-shot counting networks. To benchmark our method, we further introduce\nthe FGTC dataset, a taxonomy-specific fine-grained object counting dataset for\nnatural images. Our method substantially enhances pre-trained state-of-the-art\nmodels on fine-grained taxon counting tasks, while using only synthetic data.\nCode and data to be released upon acceptance.", "AI": {"tldr": "Proposes an annotation-free method for fine-grained object counting using synthetic data and attention prediction, improving few/zero-shot models.", "motivation": "Addresses the challenge of distinguishing fine-grained categories in few/zero-shot object counting, reducing reliance on manual annotations.", "method": "Leverages latent generative models to synthesize category-specific crowded scenes and introduces an attention prediction network trained on synthetic data.", "result": "Enhances pre-trained models on fine-grained counting tasks using only synthetic data, validated with the FGTC dataset.", "conclusion": "The method effectively integrates new fine-grained categories into counting models without manual labeling, improving performance."}}
{"id": "2504.11726", "pdf": "https://arxiv.org/pdf/2504.11726", "abs": "https://arxiv.org/abs/2504.11726", "authors": ["Yunzhe Li", "Facheng Hu", "Hongzi Zhu", "Shifan Zhang", "Liang Zhang", "Shan Chang", "Minyi Guo"], "title": "Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception", "categories": ["cs.LG", "cs.AI"], "comment": "2025 IEEE 45th International Conference on Distributed Computing\n  Systems (ICDCS)", "summary": "Inertial measurement units (IMUs), have been prevalently used in a wide range\nof mobile perception applications such as activity recognition and user\nauthentication, where a large amount of labelled data are normally required to\ntrain a satisfactory model. However, it is difficult to label micro-activities\nin massive IMU data due to the hardness of understanding raw IMU data and the\nlack of ground truth. In this paper, we propose a novel fine-grained user\nperception approach, called Saga, which only needs a small amount of labelled\nIMU data to achieve stunning user perception accuracy. The core idea of Saga is\nto first pre-train a backbone feature extraction model, utilizing the rich\nsemantic information of different levels embedded in the massive unlabelled IMU\ndata. Meanwhile, for a specific downstream user perception application,\nBayesian Optimization is employed to determine the optimal weights for\npre-training tasks involving different semantic levels. We implement Saga on\nfive typical mobile phones and evaluate Saga on three typical tasks on three\nIMU datasets. Results show that when only using about 100 training samples per\nclass, Saga can achieve over 90% accuracy of the full-fledged model trained on\nover ten thousands training samples with no additional system overhead.", "AI": {"tldr": "Saga is a fine-grained user perception approach using minimal labeled IMU data, leveraging pre-training and Bayesian Optimization for high accuracy.", "motivation": "Labeling IMU data is challenging due to raw data complexity and lack of ground truth, requiring a solution for efficient model training with minimal labels.", "method": "Pre-train a feature extraction model using unlabeled IMU data, then use Bayesian Optimization to optimize weights for downstream tasks.", "result": "Achieves over 90% accuracy with only 100 labeled samples per class, matching performance of models trained on thousands of samples.", "conclusion": "Saga demonstrates efficient and accurate user perception with minimal labeled data, reducing reliance on large labeled datasets."}}
{"id": "2504.12082", "pdf": "https://arxiv.org/pdf/2504.12082", "abs": "https://arxiv.org/abs/2504.12082", "authors": ["Yumin Kim", "Hwanhee Lee"], "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.", "AI": {"tldr": "Proposes a novel method for detecting implicit hate speech using in-context learning, improving precision and reducing biases without fine-tuning.", "motivation": "Implicit hate speech detection is challenging due to context, cultural subtleties, and model biases, leading to inconsistent results and misclassifications.", "method": "Utilizes in-context learning with adaptive retrieval of demonstrations focusing on similar groups or high similarity scores.", "result": "Outperforms current state-of-the-art techniques in experimental evaluations.", "conclusion": "The method enhances contextual comprehension and robustness in hate speech detection, addressing key challenges in the field."}}
{"id": "2504.11707", "pdf": "https://arxiv.org/pdf/2504.11707", "abs": "https://arxiv.org/abs/2504.11707", "authors": ["Muhammad Shahid Muneer", "Simon S. Woo"], "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Short Paper The Web Conference", "summary": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense.", "AI": {"tldr": "The paper addresses misuse of Text-to-Image (T2I) models, focusing on adversarial attacks bypassing NSFW filters. It introduces a million-scale dataset and a robust multimodal defense method.", "motivation": "To combat the growing concern of adversarial attacks on T2I models and the lack of a robust multimodal NSFW dataset.", "method": "Proposes a dataset of prompt-image pairs and adversarial examples, then develops a multimodal defense system.", "result": "The model outperforms existing NSFW detection methods, reducing Attack Success Rate (ASR) significantly.", "conclusion": "The work provides a scalable solution to enhance safety in T2I models against adversarial threats."}}
{"id": "2504.11757", "pdf": "https://arxiv.org/pdf/2504.11757", "abs": "https://arxiv.org/abs/2504.11757", "authors": ["Pradeep Singh", "Ashutosh Kumar", "Sutirtha Ghosh", "Hrishit B P", "Balasubramanian Raman"], "title": "Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective", "categories": ["cs.LG", "cs.NE", "37N35, 37D45, 93C10, 93C35, 93C40, 93C55", "I.2.8; I.5.2"], "comment": "100 pages, 17 tables, 41 figures", "summary": "Reservoir computing (RC) represents a class of state-space models (SSMs)\ncharacterized by a fixed state transition mechanism (the reservoir) and a\nflexible readout layer that maps from the state space. It is a paradigm of\ncomputational dynamical systems that harnesses the transient dynamics of\nhigh-dimensional state spaces for efficient processing of temporal data. Rooted\nin concepts from recurrent neural networks, RC achieves exceptional\ncomputational power by decoupling the training of the dynamic reservoir from\nthe linear readout layer, thereby circumventing the complexities of\ngradient-based optimization. This work presents a systematic exploration of RC,\naddressing its foundational properties such as the echo state property, fading\nmemory, and reservoir capacity through the lens of dynamical systems theory. We\nformalize the interplay between input signals and reservoir states,\ndemonstrating the conditions under which reservoirs exhibit stability and\nexpressive power. Further, we delve into the computational trade-offs and\nrobustness characteristics of RC architectures, extending the discussion to\ntheir applications in signal processing, time-series prediction, and control\nsystems. The analysis is complemented by theoretical insights into\noptimization, training methodologies, and scalability, highlighting open\nchallenges and potential directions for advancing the theoretical underpinnings\nof RC.", "AI": {"tldr": "Reservoir computing (RC) leverages high-dimensional state spaces for efficient temporal data processing, decoupling reservoir training from readout layer optimization. This paper explores RC's foundational properties, stability, and applications, while addressing theoretical challenges.", "motivation": "To systematically analyze RC's foundational properties, stability, and expressive power, and to explore its computational trade-offs and applications in signal processing, prediction, and control systems.", "method": "The study formalizes RC's properties (echo state, fading memory, capacity) using dynamical systems theory, examines input-signal interplay, and evaluates robustness and scalability.", "result": "Demonstrates conditions for reservoir stability and expressive power, provides insights into optimization and training, and identifies open challenges for RC's theoretical advancement.", "conclusion": "RC offers powerful computational capabilities for temporal data, but further theoretical work is needed to address scalability and optimization challenges."}}
{"id": "2504.12098", "pdf": "https://arxiv.org/pdf/2504.12098", "abs": "https://arxiv.org/abs/2504.12098", "authors": ["Adil Bahaj", "Hamed Rahimi", "Mohamed Chetouani", "Mounir Ghogho"], "title": "Gauging Overprecision in LLMs: An Empirical Study", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.", "AI": {"tldr": "The paper investigates overconfidence in LLMs, focusing on overprecision, and introduces a three-phase framework (generation, refinement, evaluation) to study it. Findings include LLMs' poor calibration for numerical tasks and lack of correlation between interval length and confidence level.", "motivation": "To address biases in existing methods for measuring LLM confidence by studying overprecision, inspired by cognitive science, to better understand and quantify LLM trustworthiness.", "method": "A three-phase framework: 1) Generate answers to numerical questions as intervals with imposed confidence levels, 2) Refine answers, and 3) Evaluate and analyze LLM behavior.", "result": "LLMs are uncalibrated for numerical tasks, show no correlation between interval length and confidence level, and refinement rarely improves precision.", "conclusion": "The study provides new insights into LLM overconfidence and establishes a baseline for future research on overprecision in LLMs."}}
{"id": "2504.11732", "pdf": "https://arxiv.org/pdf/2504.11732", "abs": "https://arxiv.org/abs/2504.11732", "authors": ["Jilan Xu", "Yifei Huang", "Baoqi Pei", "Junlin Hou", "Qingqiu Li", "Guo Chen", "Yuejie Zhang", "Rui Feng", "Weidi Xie"], "title": "EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Generating videos in the first-person perspective has broad application\nprospects in the field of augmented reality and embodied intelligence. In this\nwork, we explore the cross-view video prediction task, where given an\nexo-centric video, the first frame of the corresponding ego-centric video, and\ntextual instructions, the goal is to generate futur frames of the ego-centric\nvideo. Inspired by the notion that hand-object interactions (HOI) in\nego-centric videos represent the primary intentions and actions of the current\nactor, we present EgoExo-Gen that explicitly models the hand-object dynamics\nfor cross-view video prediction. EgoExo-Gen consists of two stages. First, we\ndesign a cross-view HOI mask prediction model that anticipates the HOI masks in\nfuture ego-frames by modeling the spatio-temporal ego-exo correspondence. Next,\nwe employ a video diffusion model to predict future ego-frames using the first\nego-frame and textual instructions, while incorporating the HOI masks as\nstructural guidance to enhance prediction quality. To facilitate training, we\ndevelop an automated pipeline to generate pseudo HOI masks for both ego- and\nexo-videos by exploiting vision foundation models. Extensive experiments\ndemonstrate that our proposed EgoExo-Gen achieves better prediction performance\ncompared to previous video prediction models on the Ego-Exo4D and H2O benchmark\ndatasets, with the HOI masks significantly improving the generation of hands\nand interactive objects in the ego-centric videos.", "AI": {"tldr": "EgoExo-Gen is a two-stage model for cross-view video prediction, using hand-object interaction (HOI) masks to enhance ego-centric video generation from exo-centric inputs and textual instructions.", "motivation": "First-person video generation has applications in augmented reality and embodied intelligence. The paper focuses on leveraging HOI dynamics to improve cross-view video prediction.", "method": "EgoExo-Gen first predicts HOI masks for future ego-frames using spatio-temporal correspondence, then uses a video diffusion model with HOI masks as structural guidance. An automated pipeline generates pseudo HOI masks for training.", "result": "EgoExo-Gen outperforms previous models on Ego-Exo4D and H2O datasets, with HOI masks improving hand and object generation in ego-centric videos.", "conclusion": "Explicit modeling of HOI dynamics enhances cross-view video prediction, demonstrating the effectiveness of EgoExo-Gen."}}
{"id": "2504.11808", "pdf": "https://arxiv.org/pdf/2504.11808", "abs": "https://arxiv.org/abs/2504.11808", "authors": ["Kishan Gurumurthy", "Himanshu Pal", "Charu Sharma"], "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs", "categories": ["cs.LG"], "comment": "The first two listed authors contributed equally to this work", "summary": "Graph Neural Network (GNN) research is rapidly advancing due to GNNs'\ncapacity to learn distributed representations from graph-structured data.\nHowever, centralizing large volumes of real-world graph data for GNN training\nis often impractical due to privacy concerns, regulatory restrictions, and\ncommercial competition. Federated learning (FL), a distributed learning\nparadigm, offers a solution by preserving data privacy with collaborative model\ntraining. Despite progress in training huge vision and language models,\nfederated learning for GNNs remains underexplored. To address this challenge,\nwe present a novel method for federated learning on GNNs based on spectral GNNs\nequipped with neural ordinary differential equations (ODE) for better\ninformation capture, showing promising results across both homophilic and\nheterophilic graphs. Our approach effectively handles non-Independent and\nIdentically Distributed (non-IID) data, while also achieving performance\ncomparable to existing methods that only operate on IID data. It is designed to\nbe privacy-preserving and bandwidth-optimized, making it suitable for\nreal-world applications such as social network analysis, recommendation\nsystems, and fraud detection, which often involve complex, non-IID, and\nheterophilic graph structures. Our results in the area of federated learning on\nnon-IID heterophilic graphs demonstrate significant improvements, while also\nachieving better performance on homophilic graphs. This work highlights the\npotential of federated learning in diverse and challenging graph settings.\nOpen-source code available on GitHub\n(https://github.com/SpringWiz11/Fed-GNODEFormer).", "AI": {"tldr": "A novel federated learning method for GNNs using spectral GNNs and neural ODEs, addressing privacy and non-IID data challenges, with promising results on homophilic and heterophilic graphs.", "motivation": "Centralizing graph data for GNN training is impractical due to privacy and regulatory issues. Federated learning offers a solution but remains underexplored for GNNs.", "method": "Spectral GNNs equipped with neural ODEs for better information capture, designed for non-IID data and optimized for privacy and bandwidth.", "result": "Effective handling of non-IID data, performance comparable to IID methods, and improvements on heterophilic and homophilic graphs.", "conclusion": "The method demonstrates federated learning's potential in diverse graph settings, with applications in social networks, recommendations, and fraud detection."}}
{"id": "2504.12108", "pdf": "https://arxiv.org/pdf/2504.12108", "abs": "https://arxiv.org/abs/2504.12108", "authors": ["Shizhan Cai", "Liang Ding", "Dacheng Tao"], "title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.", "AI": {"tldr": "A novel watermarking scheme for LLMs improves detectability and text quality using a cumulative watermark entropy threshold, outperforming existing methods by over 80% on datasets like MATH and GSM8K.", "motivation": "Addressing concerns about content traceability and misuse in LLMs, while balancing text quality and robust detection against attacks.", "method": "Introduces a cumulative watermark entropy threshold, compatible with existing sampling functions, to enhance adaptability.", "result": "Significant improvements (over 80%) on datasets like MATH and GSM8K, with high detection accuracy.", "conclusion": "The proposed scheme effectively balances detectability and text quality, outperforming current methods."}}
{"id": "2504.11733", "pdf": "https://arxiv.org/pdf/2504.11733", "abs": "https://arxiv.org/abs/2504.11733", "authors": ["Li Yu", "Situo Wang", "Wei Zhou", "Moncef Gabbouj"], "title": "DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Inspired by the dual-stream theory of the human visual system (HVS) - where\nthe ventral stream is responsible for object recognition and detail analysis,\nwhile the dorsal stream focuses on spatial relationships and motion perception\n- an increasing number of video quality assessment (VQA) works built upon this\nframework are proposed. Recent advancements in large multi-modal models,\nnotably Contrastive Language-Image Pretraining (CLIP), have motivated\nresearchers to incorporate CLIP into dual-stream-based VQA methods. This\nintegration aims to harness the model's superior semantic understanding\ncapabilities to replicate the object recognition and detail analysis in ventral\nstream, as well as spatial relationship analysis in dorsal stream. However,\nCLIP is originally designed for images and lacks the ability to capture\ntemporal and motion information inherent in videos. %Furthermore, existing\nfeature fusion strategies in no-reference video quality assessment (NR-VQA)\noften rely on fixed weighting schemes, which fail to adaptively adjust feature\nimportance. To address the limitation, this paper propose a Decoupled\nVision-Language Modeling with Text-Guided Adaptation for Blind Video Quality\nAssessment (DVLTA-VQA), which decouples CLIP's visual and textual components,\nand integrates them into different stages of the NR-VQA pipeline.", "AI": {"tldr": "The paper proposes DVLTA-VQA, a method integrating decoupled CLIP components into NR-VQA to address limitations in capturing temporal/motion info and adaptive feature fusion.", "motivation": "Inspired by the dual-stream HVS theory and CLIP's semantic capabilities, the paper aims to enhance VQA by addressing CLIP's lack of temporal/motion understanding and rigid feature fusion in NR-VQA.", "method": "Decouples CLIP's visual and textual components, integrating them into NR-VQA stages for adaptive feature fusion and improved semantic understanding.", "result": "DVLTA-VQA enhances video quality assessment by leveraging CLIP's strengths while addressing its limitations for temporal and motion analysis.", "conclusion": "The proposed method effectively bridges gaps in dual-stream VQA by adapting CLIP for video-specific needs, improving NR-VQA performance."}}
{"id": "2504.11811", "pdf": "https://arxiv.org/pdf/2504.11811", "abs": "https://arxiv.org/abs/2504.11811", "authors": ["Marco Forgione", "Ankush Chakrabarty", "Dario Piga", "Matteo Rufolo", "Alberto Bemporad"], "title": "Manifold meta-learning for reduced-complexity neural system identification", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "System identification has greatly benefited from deep learning techniques,\nparticularly for modeling complex, nonlinear dynamical systems with partially\nunknown physics where traditional approaches may not be feasible. However, deep\nlearning models often require large datasets and significant computational\nresources at training and inference due to their high-dimensional\nparameterizations. To address this challenge, we propose a meta-learning\nframework that discovers a low-dimensional manifold within the parameter space\nof an over-parameterized neural network architecture. This manifold is learned\nfrom a meta-dataset of input-output sequences generated by a class of related\ndynamical systems, enabling efficient model training while preserving the\nnetwork's expressive power for the considered system class. Unlike bilevel\nmeta-learning approaches, our method employs an auxiliary neural network to map\ndatasets directly onto the learned manifold, eliminating the need for costly\nsecond-order gradient computations during meta-training and reducing the number\nof first-order updates required in inference, which could be expensive for\nlarge models. We validate our approach on a family of Bouc-Wen oscillators,\nwhich is a well-studied nonlinear system identification benchmark. We\ndemonstrate that we are able to learn accurate models even in small-data\nscenarios.", "AI": {"tldr": "A meta-learning framework is proposed to reduce the computational demands of deep learning for system identification by learning a low-dimensional manifold in the parameter space, validated on Bouc-Wen oscillators.", "motivation": "Deep learning for system identification is resource-intensive; the goal is to enable efficient training and inference with small datasets.", "method": "Uses an auxiliary neural network to map datasets to a learned low-dimensional manifold, avoiding costly second-order gradients and reducing first-order updates.", "result": "Accurate models are learned even with small datasets, as demonstrated on Bouc-Wen oscillators.", "conclusion": "The framework efficiently balances computational cost and model accuracy for nonlinear system identification."}}
{"id": "2504.12140", "pdf": "https://arxiv.org/pdf/2504.12140", "abs": "https://arxiv.org/abs/2504.12140", "authors": ["Miguel Moura Ramos", "Patrick Fernandes", "Sweta Agrawal", "Andr\u00e9 F. T. Martins"], "title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation", "categories": ["cs.CL"], "comment": "9 pages, work-in-progress", "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.", "AI": {"tldr": "The paper proposes a method to enhance LLM-based document-level translation by fine-tuning on curated DocBlocks, improving cross-sentence dependencies and translation quality.", "motivation": "Addressing the challenge of scaling LLMs to document-level translation, particularly in handling long-range dependencies and discourse phenomena.", "method": "Targeted fine-tuning on high-quality document-level data (DocBlocks), supporting multiple translation paradigms (direct document-to-document and chunk-level) with contextual instructions.", "result": "Improved document-level translation quality and inference speed compared to prompting and agent-based methods.", "conclusion": "The proposed method effectively enhances LLM performance in document-level translation by leveraging multiple paradigms and curated data."}}
{"id": "2504.11739", "pdf": "https://arxiv.org/pdf/2504.11739", "abs": "https://arxiv.org/abs/2504.11739", "authors": ["Bingjie Gao", "Xinyu Gao", "Xiaoxue Wu", "Yujie Zhou", "Yu Qiao", "Li Niu", "Xinyuan Chen", "Yaohui Wang"], "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "categories": ["cs.CV", "cs.CL"], "comment": "accepted by CVPR2025", "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.", "AI": {"tldr": "RAPO is a retrieval-augmented prompt optimization framework designed to improve Text-to-Video (T2V) generation by refining user prompts through dual optimization branches.", "motivation": "Current T2V models are sensitive to input prompts, and prior methods lack tailored guidance for prompt vocabulary and structure. RAPO addresses this gap.", "method": "RAPO uses two branches: one augments prompts with modifiers from a relational graph and aligns them via a fine-tuned LLM, while the other rewrites prompts using a pre-trained LLM.", "result": "RAPO enhances both static and dynamic aspects of generated videos, proving the importance of prompt optimization.", "conclusion": "RAPO effectively improves T2V generation by optimizing user prompts, demonstrating its value in the field."}}
{"id": "2504.11816", "pdf": "https://arxiv.org/pdf/2504.11816", "abs": "https://arxiv.org/abs/2504.11816", "authors": ["Kihyun Kim", "Jinwoo Kim", "Hyunsun Chung", "Myung-Hoon Cha", "Hong-Yeon Kim", "Youngjae Kim"], "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading", "categories": ["cs.LG", "cs.DC"], "comment": "10 pages, 6 figures", "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.", "AI": {"tldr": "InferSave is a cost-efficient VM selection framework for cloud-based LLM inference, optimizing KV cache offloading and GPU memory needs, improving cost efficiency by up to 73.7%.", "motivation": "High GPU instance costs from CSPs like AWS burden LLM inference applications.", "method": "Proposes InferSave, which optimizes KV cache offloading, estimates GPU memory needs, and uses CTCF for accurate VM selection.", "result": "Lower-cost instances improve efficiency by 73.7% for online workloads; KV cache offloading saves 20.19% for offline workloads.", "conclusion": "InferSave effectively reduces costs for LLM inference in cloud environments."}}
{"id": "2504.12172", "pdf": "https://arxiv.org/pdf/2504.12172", "abs": "https://arxiv.org/abs/2504.12172", "authors": ["Maged S. Al-Shaibani", "Zaid Alyafeai", "Irfan Ahmad"], "title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.", "AI": {"tldr": "A framework for automatic identification of Arabic poetry meters in recited poems, integrating high-resource systems to address low-resource challenges, with a published benchmark for future research.", "motivation": "Arabic poetry's rhythmic structure (meter) is complex and requires technical knowledge. Automating meter identification for recited poetry is challenging due to limited labeled data.", "method": "Proposes a state-of-the-art framework integrating two high-resource systems to tackle the low-resource task of meter identification in recited Arabic poetry.", "result": "Developed a framework for meter identification and published a benchmark to support future research.", "conclusion": "The study advances automatic meter identification in Arabic poetry and provides a foundation for further research with its benchmark."}}
{"id": "2504.11749", "pdf": "https://arxiv.org/pdf/2504.11749", "abs": "https://arxiv.org/abs/2504.11749", "authors": ["Zongye Zhang", "Wenrui Cai", "Qingjie Liu", "Yunhong Wang"], "title": "SkeletonX: Data-Efficient Skeleton-based Action Recognition via Cross-sample Feature Aggregation", "categories": ["cs.CV", "I.4.9"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM). 13 pages, 7\n  figures, 11 tables", "summary": "While current skeleton action recognition models demonstrate impressive\nperformance on large-scale datasets, their adaptation to new application\nscenarios remains challenging. These challenges are particularly pronounced\nwhen facing new action categories, diverse performers, and varied skeleton\nlayouts, leading to significant performance degeneration. Additionally, the\nhigh cost and difficulty of collecting skeleton data make large-scale data\ncollection impractical. This paper studies one-shot and limited-scale learning\nsettings to enable efficient adaptation with minimal data. Existing approaches\noften overlook the rich mutual information between labeled samples, resulting\nin sub-optimal performance in low-data scenarios. To boost the utility of\nlabeled data, we identify the variability among performers and the commonality\nwithin each action as two key attributes. We present SkeletonX, a lightweight\ntraining pipeline that integrates seamlessly with existing GCN-based skeleton\naction recognizers, promoting effective training under limited labeled data.\nFirst, we propose a tailored sample pair construction strategy on two key\nattributes to form and aggregate sample pairs. Next, we develop a concise and\neffective feature aggregation module to process these pairs. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various\nGCN backbones, demonstrating that the pipeline effectively improves performance\nwhen trained from scratch with limited data. Moreover, it surpasses previous\nstate-of-the-art methods in the one-shot setting, with only 1/10 of the\nparameters and much fewer FLOPs. The code and data are available at:\nhttps://github.com/zzysteve/SkeletonX", "AI": {"tldr": "SkeletonX is a lightweight training pipeline for skeleton action recognition, improving performance in low-data and one-shot settings by leveraging performer variability and action commonality.", "motivation": "Current skeleton action recognition models struggle with new action categories, diverse performers, and varied skeleton layouts, especially with limited data. Existing methods underutilize mutual information between labeled samples.", "method": "SkeletonX introduces a sample pair construction strategy and a feature aggregation module to enhance labeled data utility. It integrates with GCN-based recognizers.", "result": "Experiments on NTU RGB+D, NTU RGB+D 120, and PKU-MMD show improved performance with limited data and surpasses state-of-the-art in one-shot settings with fewer parameters and FLOPs.", "conclusion": "SkeletonX effectively addresses low-data challenges in skeleton action recognition, offering a lightweight and efficient solution."}}
{"id": "2504.11830", "pdf": "https://arxiv.org/pdf/2504.11830", "abs": "https://arxiv.org/abs/2504.11830", "authors": ["Rohan Hitchcock", "Gary W. Delaney", "Jonathan H. Manton", "Richard Scalzo", "Jingge Zhu"], "title": "Emergence of Computational Structure in a Neural Network Physics Simulator", "categories": ["cs.LG"], "comment": "35 pages", "summary": "Neural networks often have identifiable computational structures - components\nof the network which perform an interpretable algorithm or task - but the\nmechanisms by which these emerge and the best methods for detecting these\nstructures are not well understood. In this paper we investigate the emergence\nof computational structure in a transformer-like model trained to simulate the\nphysics of a particle system, where the transformer's attention mechanism is\nused to transfer information between particles. We show that (a) structures\nemerge in the attention heads of the transformer which learn to detect particle\ncollisions, (b) the emergence of these structures is associated to degenerate\ngeometry in the loss landscape, and (c) the dynamics of this emergence follows\na power law. This suggests that these components are governed by a degenerate\n\"effective potential\". These results have implications for the convergence time\nof computational structure within neural networks and suggest that the\nemergence of computational structure can be detected by studying the dynamics\nof network components.", "AI": {"tldr": "The paper explores how computational structures emerge in neural networks, focusing on a transformer-like model simulating particle physics. It identifies collision-detecting structures in attention heads, links their emergence to degenerate loss landscape geometry, and observes power-law dynamics.", "motivation": "To understand the mechanisms behind the emergence of interpretable computational structures in neural networks and how to detect them.", "method": "A transformer-like model is trained to simulate particle physics, using attention mechanisms to transfer information between particles. The study analyzes attention heads for emergent structures.", "result": "Structures detecting particle collisions emerge in attention heads, associated with degenerate loss landscape geometry and following power-law dynamics.", "conclusion": "Computational structures in networks are governed by a degenerate \"effective potential,\" detectable by studying component dynamics, with implications for convergence time."}}
{"id": "2504.12177", "pdf": "https://arxiv.org/pdf/2504.12177", "abs": "https://arxiv.org/abs/2504.12177", "authors": ["Victor Manuel Hernandez Lopez", "Jaime E. Cuellar"], "title": "Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube", "categories": ["cs.CL", "cs.AI"], "comment": "in Spanish language", "summary": "This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives.", "AI": {"tldr": "The paper analyzes Spanish-language YouTube comments on the Hamas-Israel conflict using NLP and BERT, revealing a shift in public opinion influenced by media coverage.", "motivation": "To understand public opinion dynamics and media influence on the Hamas-Israel conflict through computational and social science methods.", "method": "Combines STS and NLP (BERT) to classify 253,925 YouTube comments into seven categories, applying agenda-setting theory.", "result": "Pro-Palestinian comments were most common, but pro-Israeli and anti-Palestinian ones got more likes. Media coverage shifted public opinion.", "conclusion": "Highlights the value of integrating computational tools with social theories to analyze public opinion and media narratives."}}
{"id": "2504.11754", "pdf": "https://arxiv.org/pdf/2504.11754", "abs": "https://arxiv.org/abs/2504.11754", "authors": ["Zihui Zhang", "Yafei Yang", "Hongtao Wen", "Bo Yang"], "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS", "summary": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.", "AI": {"tldr": "GrabS, a two-stage pipeline, learns object-centric priors and uses an embodied agent to discover objects, outperforming unsupervised methods in 3D segmentation.", "motivation": "Existing unsupervised 3D object segmentation methods rely on 2D features or external signals, limiting performance on complex objects.", "method": "GrabS first learns generative and discriminative priors from object datasets, then uses an embodied agent to query these priors for object discovery.", "result": "Evaluated on real-world and synthetic datasets, GrabS surpasses existing unsupervised methods in segmentation performance.", "conclusion": "GrabS advances unsupervised 3D segmentation by leveraging object-centric priors and active discovery."}}
{"id": "2504.11831", "pdf": "https://arxiv.org/pdf/2504.11831", "abs": "https://arxiv.org/abs/2504.11831", "authors": ["Changming Xu", "Debangshu Banerjee", "Deepak Vasisht", "Gagandeep Singh"], "title": "Support is All You Need for Certified VAE Training", "categories": ["cs.LG", "stat.ML"], "comment": "21 pages, 3 figures, ICLR '25", "summary": "Variational Autoencoders (VAEs) have become increasingly popular and deployed\nin safety-critical applications. In such applications, we want to give\ncertified probabilistic guarantees on performance under adversarial attacks. We\npropose a novel method, CIVET, for certified training of VAEs. CIVET depends on\nthe key insight that we can bound worst-case VAE error by bounding the error on\ncarefully chosen support sets at the latent layer. We show this point\nmathematically and present a novel training algorithm utilizing this insight.\nWe show in an extensive evaluation across different datasets (in both the\nwireless and vision application areas), architectures, and perturbation\nmagnitudes that our method outperforms SOTA methods achieving good standard\nperformance with strong robustness guarantees.", "AI": {"tldr": "CIVET is a novel method for certified training of VAEs, providing robustness guarantees against adversarial attacks by bounding errors on latent support sets.", "motivation": "VAEs are used in safety-critical applications, necessitating certified performance guarantees under adversarial attacks.", "method": "CIVET bounds worst-case VAE error by focusing on latent layer support sets, with a novel training algorithm.", "result": "Outperforms SOTA methods in standard performance and robustness across datasets, architectures, and perturbations.", "conclusion": "CIVET offers a reliable approach for certified VAE training with strong adversarial robustness."}}
{"id": "2504.12180", "pdf": "https://arxiv.org/pdf/2504.12180", "abs": "https://arxiv.org/abs/2504.12180", "authors": ["Jaime E. Cuellar", "Oscar Moreno-Martinez", "Paula Sofia Torres-Rodriguez", "Jaime Andres Pavlich-Mariscal", "Andres Felipe Mican-Castiblanco", "Juan Guillermo Torres-Hurtado"], "title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification", "categories": ["cs.CL", "cs.AI"], "comment": "in Spanish language", "summary": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.", "AI": {"tldr": "The study examines how minor prompt variations affect GPT-4o mini's sentiment classification, revealing inconsistencies and vulnerabilities in the model's robustness.", "motivation": "To assess trust in complex predictive models like ChatGPT by testing their sensitivity to prompt changes in sentiment analysis.", "method": "Used 100,000 Spanish comments on Latin American presidents, classified 10 times with varied prompts, followed by exploratory and confirmatory analyses.", "result": "Minor prompt changes led to inconsistent classifications, including mixed categories and hallucinations, with statistical significance in most cases.", "conclusion": "Large Language Models' trustworthiness is questioned due to sensitivity to prompts, emphasizing the need for structured grammar and social-institutional trust."}}
{"id": "2504.11763", "pdf": "https://arxiv.org/pdf/2504.11763", "abs": "https://arxiv.org/abs/2504.11763", "authors": ["Aoran Liu", "Kun Hu", "Clinton Mo", "Changyang Li", "Zhiyong Wang"], "title": "Extended Short- and Long-Range Mesh Learning for Fast and Generalized Garment Simulation", "categories": ["cs.CV"], "comment": null, "summary": "3D garment simulation is a critical component for producing cloth-based\ngraphics. Recent advancements in graph neural networks (GNNs) offer a promising\napproach for efficient garment simulation. However, GNNs require extensive\nmessage-passing to propagate information such as physical forces and maintain\ncontact awareness across the entire garment mesh, which becomes computationally\ninefficient at higher resolutions. To address this, we devise a novel GNN-based\nmesh learning framework with two key components to extend the message-passing\nrange with minimal overhead, namely the Laplacian-Smoothed Dual Message-Passing\n(LSDMP) and the Geodesic Self-Attention (GSA) modules. LSDMP enhances\nmessage-passing with a Laplacian features smoothing process, which efficiently\npropagates the impact of each vertex to nearby vertices. Concurrently, GSA\nintroduces geodesic distance embeddings to represent the spatial relationship\nbetween vertices and utilises attention mechanisms to capture global mesh\ninformation. The two modules operate in parallel to ensure both short- and\nlong-range mesh modelling. Extensive experiments demonstrate the\nstate-of-the-art performance of our method, requiring fewer layers and lower\ninference latency.", "AI": {"tldr": "A novel GNN-based framework with LSDMP and GSA modules improves 3D garment simulation by extending message-passing range efficiently.", "motivation": "Overcoming computational inefficiency in GNNs for high-resolution garment simulation.", "method": "Introduces Laplacian-Smoothed Dual Message-Passing (LSDMP) and Geodesic Self-Attention (GSA) modules for efficient short- and long-range mesh modeling.", "result": "Achieves state-of-the-art performance with fewer layers and lower inference latency.", "conclusion": "The proposed framework effectively enhances garment simulation efficiency and accuracy."}}
{"id": "2504.11866", "pdf": "https://arxiv.org/pdf/2504.11866", "abs": "https://arxiv.org/abs/2504.11866", "authors": ["Houshuang Chen", "Yuchen He", "Chihao Zhang"], "title": "On the Problem of Best Arm Retention", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "This paper presents a comprehensive study on the problem of Best Arm\nRetention (BAR), which has recently found applications in streaming algorithms\nfor multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms\nwith the best arm included from $n$ after some trials, in stochastic\nmulti-armed bandit settings. We first investigate pure exploration for the BAR\nproblem under different criteria, and then minimize the regret with specific\nconstraints, in the context of further exploration in streaming algorithms.\n  - We begin by revisiting the lower bound for the $(\\varepsilon,\\delta)$-PAC\nalgorithm for Best Arm Identification (BAI) and adapt the classical\nKL-divergence argument to derive optimal bounds for $(\\varepsilon,\\delta)$-PAC\nalgorithms for BAR.\n  - We further study another variant of the problem, called $r$-BAR, which\nrequires the expected gap between the best arm and the optimal arm retained is\nless than $r$. We prove tight sample complexity for the problem.\n  - We explore the regret minimization problem for $r$-BAR and develop\nalgorithm beyond pure exploration. We conclude with a conjecture on the optimal\nregret in this setting.", "AI": {"tldr": "The paper studies Best Arm Retention (BAR) in multi-armed bandits, focusing on pure exploration and regret minimization, with tight bounds and sample complexity for variants like $r$-BAR.", "motivation": "To address the BAR problem in streaming algorithms for multi-armed bandits, ensuring retention of top arms efficiently.", "method": "Revisits lower bounds for PAC algorithms, adapts KL-divergence arguments, and explores variants like $r$-BAR with tight sample complexity proofs.", "result": "Derives optimal bounds for PAC algorithms, proves tight sample complexity for $r$-BAR, and develops regret-minimizing algorithms.", "conclusion": "The work advances BAR understanding with theoretical guarantees and leaves a conjecture on optimal regret for future exploration."}}
{"id": "2504.12185", "pdf": "https://arxiv.org/pdf/2504.12185", "abs": "https://arxiv.org/abs/2504.12185", "authors": ["Suyoung Bae", "Hyojun Kim", "YunSeok Choi", "Jee-Hyong Lee"], "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures", "summary": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.", "AI": {"tldr": "SALAD improves NLP model robustness by generating structure-aware and counterfactual data for contrastive learning, reducing spurious correlations.", "motivation": "Fine-tuning PLMs often leads to spurious correlations, harming performance on out-of-distribution data.", "method": "SALAD uses tagging for structure-aware positives and LLMs for counterfactual negatives, applying contrastive learning.", "result": "SALAD boosts robustness and performance in Sentiment Classification, Sexism Detection, and NLI, including cross-domain scenarios.", "conclusion": "SALAD effectively mitigates spurious correlations, enhancing generalization and robustness in NLP tasks."}}
{"id": "2504.11773", "pdf": "https://arxiv.org/pdf/2504.11773", "abs": "https://arxiv.org/abs/2504.11773", "authors": ["Yiran Wang", "Jiaqi Li", "Chaoyi Hong", "Ruibo Li", "Liusheng Sun", "Xiao Song", "Zhe Wang", "Zhiguo Cao", "Guosheng Lin"], "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Oral Presentation)", "summary": "Radar-Camera depth estimation aims to predict dense and accurate metric depth\nby fusing input images and Radar data. Model efficiency is crucial for this\ntask in pursuit of real-time processing on autonomous vehicles and robotic\nplatforms. However, due to the sparsity of Radar returns, the prevailing\nmethods adopt multi-stage frameworks with intermediate quasi-dense depth, which\nare time-consuming and not robust. To address these challenges, we propose\nTacoDepth, an efficient and accurate Radar-Camera depth estimation model with\none-stage fusion. Specifically, the graph-based Radar structure extractor and\nthe pyramid-based Radar fusion module are designed to capture and integrate the\ngraph structures of Radar point clouds, delivering superior model efficiency\nand robustness without relying on the intermediate depth results. Moreover,\nTacoDepth can be flexible for different inference modes, providing a better\nbalance of speed and accuracy. Extensive experiments are conducted to\ndemonstrate the efficacy of our method. Compared with the previous\nstate-of-the-art approach, TacoDepth improves depth accuracy and processing\nspeed by 12.8% and 91.8%. Our work provides a new perspective on efficient\nRadar-Camera depth estimation.", "AI": {"tldr": "TacoDepth is a one-stage Radar-Camera fusion model for efficient and accurate depth estimation, improving accuracy by 12.8% and speed by 91.8%.", "motivation": "Existing methods are slow and not robust due to multi-stage frameworks and sparse Radar data.", "method": "Uses a graph-based Radar structure extractor and pyramid-based fusion module for efficient one-stage fusion.", "result": "Outperforms state-of-the-art with 12.8% better accuracy and 91.8% faster processing.", "conclusion": "TacoDepth offers a robust, efficient solution for Radar-Camera depth estimation, balancing speed and accuracy."}}
{"id": "2504.11873", "pdf": "https://arxiv.org/pdf/2504.11873", "abs": "https://arxiv.org/abs/2504.11873", "authors": ["Weiqiang Jiao", "Suzhi Bi", "Xian Li", "Cheng Guo", "Hao Chen", "Zhi Quan"], "title": "Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "14 pages, 14 figures, the paper is submitted for potential journal\n  publication", "summary": "This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications.", "AI": {"tldr": "Proposes DASEIN, a domain adaptation method for semantic edge inference systems, enabling high accuracy in new environments without labeled data.", "motivation": "High costs in annotating and re-training models for new environments due to varying data and channel distributions.", "method": "Uses unsupervised domain adaptation and knowledge distillation to align data distributions and adapt to channel variations.", "result": "Outperforms benchmarks by 7.09% and 21.33% in accuracy under varying conditions.", "conclusion": "DASEIN effectively adapts to new environments without labeled data, proving practical for transfer deployments."}}
{"id": "2504.12187", "pdf": "https://arxiv.org/pdf/2504.12187", "abs": "https://arxiv.org/abs/2504.12187", "authors": ["C\u00e9line Budding"], "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for publication in Philosophy of Science", "summary": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.", "AI": {"tldr": "The paper argues that LLMs can acquire tacit knowledge, contrary to Davies' view, by meeting semantic, syntactic, and causal constraints.", "motivation": "To challenge the assumption that LLMs inherently 'know' language or facts, and to explore whether they can possess tacit knowledge.", "method": "Analyzes LLM architectures against Davies' criteria for tacit knowledge (semantic description, syntactic structure, causal systematicity).", "result": "Demonstrates that LLMs satisfy the constraints for tacit knowledge, suggesting they can acquire it.", "conclusion": "Tacit knowledge provides a useful framework for understanding and intervening in LLM behavior."}}
{"id": "2504.11777", "pdf": "https://arxiv.org/pdf/2504.11777", "abs": "https://arxiv.org/abs/2504.11777", "authors": ["Yongpei Ma", "Pengyu Wang", "Adam Dunn", "Usman Naseem", "Jinman Kim"], "title": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets", "categories": ["cs.CV", "cs.LG"], "comment": "The first two listed authors contributed equally to this work", "summary": "Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency.", "AI": {"tldr": "The paper proposes SEQA, a framework using LLMs to generate diverse but semantically equivalent questions for MVQA, improving model consistency and accuracy.", "motivation": "Linguistic variability in question phrasing undermines MVQA system consistency.", "method": "SEQA framework leverages LLMs for question augmentation and introduces TAR-SC and other diversity metrics.", "result": "Enhanced datasets show significant improvements (e.g., ANQI +86.1). Fine-tuned models achieve 19.35% higher accuracy.", "conclusion": "SEQA improves MVQA model consistency and accuracy, validated by new metrics."}}
{"id": "2504.11874", "pdf": "https://arxiv.org/pdf/2504.11874", "abs": "https://arxiv.org/abs/2504.11874", "authors": ["Ruoyu Sun", "Angelos Stefanidis", "Zhengyong Jiang", "Jionglong Su"], "title": "Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization", "categories": ["cs.LG"], "comment": null, "summary": "Typical deep reinforcement learning (DRL) agents for dynamic portfolio\noptimization learn the factors influencing portfolio return and risk by\nanalyzing the output values of the reward function while adjusting portfolio\nweights within the training environment. However, it faces a major limitation\nwhere it is difficult for investors to intervene in the training based on\ndifferent levels of risk aversion towards each portfolio asset. This difficulty\narises from another limitation: existing DRL agents may not develop a thorough\nunderstanding of the factors responsible for the portfolio return and risk by\nonly learning from the output of the reward function. As a result, the strategy\nfor determining the target portfolio weights is entirely dependent on the DRL\nagents themselves. To address these limitations, we propose a reward factor\nmatrix for elucidating the return and risk of each asset in the portfolio.\nAdditionally, we propose a novel learning system named Factor-MCLS using a\nmulti-critic framework that facilitates learning of the reward factor matrix.\nIn this way, our DRL-based learning system can effectively learn the factors\ninfluencing portfolio return and risk. Moreover, based on the critic networks\nwithin the multi-critic framework, we develop a risk constraint term in the\ntraining objective function of the policy function. This risk constraint term\nallows investors to intervene in the training of the DRL agent according to\ntheir individual levels of risk aversion towards the portfolio assets.", "AI": {"tldr": "The paper proposes a reward factor matrix and a multi-critic learning system (Factor-MCLS) to address limitations in DRL for portfolio optimization, enabling better understanding of return/risk factors and investor intervention based on risk aversion.", "motivation": "Existing DRL agents struggle with investor intervention and lack thorough understanding of portfolio return/risk factors due to reliance on reward function outputs.", "method": "Introduces a reward factor matrix and Factor-MCLS, a multi-critic framework, to learn these factors and incorporates a risk constraint term for investor customization.", "result": "The system effectively learns return/risk factors and allows investor intervention via risk constraints.", "conclusion": "The proposed approach enhances DRL-based portfolio optimization by improving factor understanding and enabling investor-specific risk adjustments."}}
{"id": "2504.12216", "pdf": "https://arxiv.org/pdf/2504.12216", "abs": "https://arxiv.org/abs/2504.12216", "authors": ["Siyan Zhao", "Devaansh Gupta", "Qinqing Zheng", "Aditya Grover"], "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages, project page at https://dllm-reasoning.github.io/", "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.", "AI": {"tldr": "The paper introduces d1, a framework to adapt pre-trained masked diffusion-based LLMs (dLLMs) for reasoning tasks using supervised finetuning (SFT) and RL, achieving competitive performance.", "motivation": "To explore whether diffusion-based LLMs (dLLMs) can leverage advances in reasoning like autoregressive LLMs, given their competitive language modeling performance.", "method": "Proposes d1, combining masked SFT for knowledge distillation and a novel critic-free RL algorithm (diffu-GRPO) to enhance reasoning in dLLMs.", "result": "d1 significantly improves reasoning performance of a state-of-the-art dLLM on mathematical and logical benchmarks.", "conclusion": "The framework successfully adapts dLLMs for reasoning, demonstrating their potential beyond traditional autoregressive paradigms."}}
{"id": "2504.11779", "pdf": "https://arxiv.org/pdf/2504.11779", "abs": "https://arxiv.org/abs/2504.11779", "authors": ["Qishun Wang", "Zhengzheng Tu", "Chenglong Li", "Bo Jiang"], "title": "Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of\ntraditional RGB-based VOD in challenging lighting conditions, making it more\npractical and effective in many applications.\n  However, similar to most RGBT fusion tasks, it still mainly relies on\nmanually aligned multimodal image pairs.\n  In this paper, we propose a novel Multimodal Spatio-temporal Graph learning\nNetwork (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust\ngraph representation learning model.\n  Specifically, we first design an Adaptive Partitioning Layer (APL) to\nestimate the corresponding regions of the Thermal image within the RGB image\n(high-resolution), achieving a preliminary inexact alignment.\n  Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which\nemploys a sparse information passing mechanism on the estimated inexact\nalignment to achieve reliable information interaction between different\nmodalities.\n  Moreover, to fully exploit the temporal cues for RGBT VOD problem, we\nintroduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal\nSparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM\naims to filter out some redundant information between adjacent frames by\nemploying the sparse aggregation mechanism on the temporal graph. Meanwhile,\nTSB is dedicated to achieving the complementary learning of local spatial\nrelationships.\n  Extensive comparative experiments conducted on both the aligned dataset\nVT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness\nand superiority of our proposed method. Our project will be made available on\nour website for free public access.", "AI": {"tldr": "The paper proposes MSGNet, a graph-based method for alignment-free RGB-Thermal Video Object Detection (RGBT VOD), addressing limitations of manual alignment in multimodal tasks. It introduces adaptive partitioning and spatio-temporal graph learning for robust performance.", "motivation": "Traditional RGBT VOD relies on manually aligned multimodal pairs, limiting practicality. The paper aims to overcome this by developing an alignment-free solution using graph representation learning.", "method": "MSGNet includes an Adaptive Partitioning Layer (APL) for inexact alignment, Spatial Sparse Graph Learning Module (S-SGLM) for cross-modal interaction, and Hybrid Structured Temporal Modeling (HSTM) with T-SGLM and TSB for temporal cues.", "result": "Experiments on VT-VOD50 and UVT-VOD2024 datasets show the method's effectiveness and superiority over existing approaches.", "conclusion": "MSGNet offers a robust, alignment-free solution for RGBT VOD, leveraging graph learning and temporal modeling for improved performance in challenging conditions."}}
{"id": "2504.11877", "pdf": "https://arxiv.org/pdf/2504.11877", "abs": "https://arxiv.org/abs/2504.11877", "authors": ["Sarang S", "Harsh D. Chothani", "Qilei Li", "Ahmed M. Abdelmoniem", "Arnab K. Paul"], "title": "Benchmarking Mutual Information-based Loss Functions in Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "6 pages, 4 figures", "summary": "Federated Learning (FL) has attracted considerable interest due to growing\nprivacy concerns and regulations like the General Data Protection Regulation\n(GDPR), which stresses the importance of privacy-preserving and fair machine\nlearning approaches. In FL, model training takes place on decentralized data,\nso as to allow clients to upload a locally trained model and receive a globally\naggregated model without exposing sensitive information. However, challenges\nrelated to fairness-such as biases, uneven performance among clients, and the\n\"free rider\" issue complicates its adoption. In this paper, we examine the use\nof Mutual Information (MI)-based loss functions to address these concerns. MI\nhas proven to be a powerful method for measuring dependencies between variables\nand optimizing deep learning models. By leveraging MI to extract essential\nfeatures and minimize biases, we aim to improve both the fairness and\neffectiveness of FL systems. Through extensive benchmarking, we assess the\nimpact of MI-based losses in reducing disparities among clients while enhancing\nthe overall performance of FL.", "AI": {"tldr": "The paper explores using Mutual Information (MI)-based loss functions to address fairness and performance issues in Federated Learning (FL), aiming to reduce biases and disparities among clients.", "motivation": "Growing privacy concerns and regulations like GDPR highlight the need for privacy-preserving and fair machine learning, but FL faces challenges like biases and uneven performance among clients.", "method": "The study leverages MI-based loss functions to measure dependencies between variables, extract essential features, and minimize biases in FL.", "result": "Benchmarking shows MI-based losses reduce disparities among clients and enhance FL performance.", "conclusion": "MI-based loss functions improve fairness and effectiveness in FL, addressing key challenges in decentralized model training."}}
{"id": "2504.12285", "pdf": "https://arxiv.org/pdf/2504.12285", "abs": "https://arxiv.org/abs/2504.12285", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "title": "BitNet b1.58 2B4T Technical Report", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.", "AI": {"tldr": "BitNet b1.58 2B4T is the first open-source 1-bit LLM with 2B parameters, trained on 4T tokens, matching full-precision models in performance while being more efficient.", "motivation": "To create a highly efficient LLM with reduced computational costs without sacrificing performance.", "method": "Training a 1-bit LLM (BitNet b1.58 2B4T) on 4 trillion tokens and evaluating it across multiple benchmarks.", "result": "Achieves performance comparable to full-precision LLMs with significant efficiency gains in memory, energy, and latency.", "conclusion": "BitNet b1.58 2B4T is a viable, efficient alternative to traditional LLMs, with open-source availability for broader adoption."}}
{"id": "2504.11781", "pdf": "https://arxiv.org/pdf/2504.11781", "abs": "https://arxiv.org/abs/2504.11781", "authors": ["Guanchun Wang", "Xiangrong Zhang", "Yifei Zhang", "Zelin Peng", "Tianyang Zhang", "Xu Tang", "Licheng Jiao"], "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figures", "summary": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.", "AI": {"tldr": "ACMamba reduces computational costs for unsupervised anomaly detection in hyperspectral images (HSI) by using region-level instances and a Mamba-based module, achieving faster and more accurate results.", "motivation": "Current HSI anomaly detection methods are computationally expensive due to high-dimensional data and dense sampling. The paper aims to reduce costs without sacrificing accuracy.", "method": "Proposes ACMamba, an asymmetrical anomaly detection paradigm using region-level instances and a Mamba-based module for global context. Includes consensus learning for background reconstruction and anomaly compression.", "result": "ACMamba outperforms state-of-the-art methods in speed and performance across eight benchmarks.", "conclusion": "ACMamba offers a cost-effective and efficient solution for HSI anomaly detection, balancing accuracy and computational efficiency."}}
{"id": "2504.11885", "pdf": "https://arxiv.org/pdf/2504.11885", "abs": "https://arxiv.org/abs/2504.11885", "authors": ["Qiyue Chen", "Shaolin Tan", "Suixiang Gao", "Jinhu L\u00fc"], "title": "HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems", "categories": ["cs.LG"], "comment": null, "summary": "Graph neural networks (GNNs) have shown promising performance in solving both\nBoolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due\nto their ability to efficiently model and capture the structural dependencies\nbetween literals and clauses. However, GNN methods for solving Weighted MaxSAT\nproblems remain underdeveloped. The challenges arise from the non-linear\ndependency and sensitive objective function, which are caused by the\nnon-uniform distribution of weights across clauses. In this paper, we present\nHyperSAT, a novel neural approach that employs an unsupervised hypergraph\nneural network model to solve Weighted MaxSAT problems. We propose a hypergraph\nrepresentation for Weighted MaxSAT instances and design a cross-attention\nmechanism along with a shared representation constraint loss function to\ncapture the logical interactions between positive and negative literal nodes in\nthe hypergraph. Extensive experiments on various Weighted MaxSAT datasets\ndemonstrate that HyperSAT achieves better performance than state-of-the-art\ncompetitors.", "AI": {"tldr": "HyperSAT, a novel GNN-based method, improves Weighted MaxSAT problem-solving by using hypergraph representation and cross-attention mechanisms.", "motivation": "Existing GNN methods for Weighted MaxSAT are underdeveloped due to non-linear dependencies and sensitive objectives from non-uniform clause weights.", "method": "HyperSAT employs an unsupervised hypergraph neural network with cross-attention and shared representation constraint loss to model literal-clause interactions.", "result": "HyperSAT outperforms state-of-the-art methods on various Weighted MaxSAT datasets.", "conclusion": "HyperSAT effectively addresses Weighted MaxSAT challenges, demonstrating superior performance through innovative hypergraph modeling."}}
{"id": "2504.11459", "pdf": "https://arxiv.org/pdf/2504.11459", "abs": "https://arxiv.org/abs/2504.11459", "authors": ["Peter Stockinger"], "title": "From Conceptual Data Models to Multimodal Representation", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "in French language", "summary": "1) Introduction and Conceptual Framework: This document explores the concept\nof information design by dividing it into two major practices: defining the\nmeaning of a corpus of textual data and its visual or multimodal\nrepresentation. It draws on expertise in enriching textual corpora,\nparticularly audiovisual ones, and transforming them into multiple narrative\nformats. The text highlights a crucial distinction between the semantic content\nof a domain and the modalities of its graphic expression, illustrating this\napproach with concepts rooted in structural semiotics and linguistics\ntraditions.\n  2) Modeling and Conceptual Design: The article emphasizes the importance of\nsemantic modeling, often achieved through conceptual networks or graphs. These\ntools enable the structuring of knowledge within a domain by accounting for\nrelationships between concepts, contexts of use, and specific objectives.\nStockinger also highlights the constraints and challenges involved in creating\ndynamic and adaptable models, integrating elements such as thesauri or\ninteroperable ontologies to facilitate the analysis and publication of complex\ncorpora.\n  3) Applications and Multimodal Visualization: The text concludes by examining\nthe practical application of these models in work environments like OKAPI,\ndeveloped to analyze, publish, and reuse audiovisual data. It also discusses\ninnovative approaches such as visual storytelling and document reengineering,\nwhich involve transforming existing content into new resources tailored to\nvarious contexts. These methods emphasize interoperability, flexibility, and\nthe intelligence of communication systems, paving the way for richer and more\ncollaborative use of digital data. The content of this document was presented\nduring the \"Semiotics of Information Design\" Day organized by Anne\nBeyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on\nJune 21, 2018, in Bordeaux.", "AI": {"tldr": "The paper explores information design by separating semantic content from visual representation, using semantic modeling and multimodal visualization for practical applications like audiovisual data analysis.", "motivation": "To bridge the gap between semantic content and its visual/multimodal expression, leveraging structural semiotics and linguistics.", "method": "Semantic modeling via conceptual networks/graphs, integrating thesauri or ontologies, and applying these in tools like OKAPI for data analysis and visualization.", "result": "Enhanced interoperability and flexibility in communication systems, enabling richer use of digital data through visual storytelling and document reengineering.", "conclusion": "The approach fosters collaborative and intelligent use of digital data, demonstrated in practical applications like audiovisual analysis and multimodal visualization."}}
{"id": "2504.11786", "pdf": "https://arxiv.org/pdf/2504.11786", "abs": "https://arxiv.org/abs/2504.11786", "authors": ["Sang-Jun Park", "Keun-Soo Heo", "Dong-Hee Shin", "Young-Han Son", "Ji-Hye Oh", "Tae-Eui Kam"], "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation", "categories": ["cs.CV"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "The automatic generation of radiology reports has emerged as a promising\nsolution to reduce a time-consuming task and accurately capture critical\ndisease-relevant findings in X-ray images. Previous approaches for radiology\nreport generation have shown impressive performance. However, there remains\nsignificant potential to improve accuracy by ensuring that retrieved reports\ncontain disease-relevant findings similar to those in the X-ray images and by\nrefining generated reports. In this study, we propose a Disease-aware\nimage-text Alignment and self-correcting Re-alignment for Trustworthy radiology\nreport generation (DART) framework. In the first stage, we generate initial\nreports based on image-to-text retrieval with disease-matching, embedding both\nimages and texts in a shared embedding space through contrastive learning. This\napproach ensures the retrieval of reports with similar disease-relevant\nfindings that closely align with the input X-ray images. In the second stage,\nwe further enhance the initial reports by introducing a self-correction module\nthat re-aligns them with the X-ray images. Our proposed framework achieves\nstate-of-the-art results on two widely used benchmarks, surpassing previous\napproaches in both report generation and clinical efficacy metrics, thereby\nenhancing the trustworthiness of radiology reports.", "AI": {"tldr": "The paper introduces DART, a framework for radiology report generation that improves accuracy by aligning reports with disease-relevant findings in X-ray images and refining them through self-correction.", "motivation": "To enhance the accuracy and trustworthiness of radiology reports by ensuring they closely match disease-relevant findings in X-ray images.", "method": "Uses a two-stage approach: (1) image-to-text retrieval with disease-matching via contrastive learning, and (2) a self-correction module to refine reports.", "result": "Achieves state-of-the-art performance on benchmarks, outperforming previous methods in report generation and clinical efficacy.", "conclusion": "DART significantly improves radiology report accuracy and trustworthiness, demonstrating its potential for clinical applications."}}
{"id": "2504.11903", "pdf": "https://arxiv.org/pdf/2504.11903", "abs": "https://arxiv.org/abs/2504.11903", "authors": ["Yuan Zhou", "Jiachen Zhong", "Xinli Shi", "Guanghui Wen", "Xinghuo Yu"], "title": "FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "Composite federated learning offers a general framework for solving machine\nlearning problems with additional regularization terms. However, many existing\nmethods require clients to perform multiple proximal operations to handle\nnon-smooth terms and their performance are often susceptible to data\nheterogeneity. To overcome these limitations, we propose a novel composite\nfederated learning algorithm called \\textbf{FedCanon}, designed to solve the\noptimization problems comprising a possibly non-convex loss function and a\nweakly convex, potentially non-smooth regularization term. By decoupling\nproximal mappings from local updates, FedCanon requires only a single proximal\nevaluation on the server per iteration, thereby reducing the overall proximal\ncomputation cost. It also introduces control variables that incorporate global\ngradient information into client updates, which helps mitigate the effects of\ndata heterogeneity. Theoretical analysis demonstrates that FedCanon achieves\nsublinear convergence rates under general non-convex settings and linear\nconvergence under the Polyak-{\\L}ojasiewicz condition, without relying on\nbounded heterogeneity assumptions. Experiments demonstrate that FedCanon\noutperforms the state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, particularly under heterogeneous data distributions.", "AI": {"tldr": "FedCanon is a novel composite federated learning algorithm that reduces proximal computation costs and mitigates data heterogeneity effects, achieving strong convergence rates and outperforming state-of-the-art methods.", "motivation": "Existing composite federated learning methods often require multiple proximal operations and struggle with data heterogeneity, limiting their efficiency and performance.", "method": "FedCanon decouples proximal mappings from local updates, requiring only one server-side proximal evaluation per iteration, and uses control variables to incorporate global gradient information.", "result": "Theoretical analysis shows sublinear and linear convergence rates under non-convex and Polyak-\u0141ojasiewicz conditions, respectively. Experiments confirm superior accuracy and efficiency, especially with heterogeneous data.", "conclusion": "FedCanon addresses key limitations of existing methods, offering improved computational efficiency and robustness to data heterogeneity."}}
{"id": "2504.11492", "pdf": "https://arxiv.org/pdf/2504.11492", "abs": "https://arxiv.org/abs/2504.11492", "authors": ["Mayukh Bagchi"], "title": "Language and Knowledge Representation: A Stratified Approach", "categories": ["cs.DB", "cs.CL", "cs.DL"], "comment": "Doctor of Philosophy (Ph.D) in Information Engineering and Computer\n  Science, DISI, University of Trento, Italy", "summary": "The thesis proposes the problem of representation heterogeneity to emphasize\nthe fact that heterogeneity is an intrinsic property of any representation,\nwherein, different observers encode different representations of the same\ntarget reality in a stratified manner using different concepts, language and\nknowledge (as well as data). The thesis then advances a top-down solution\napproach to the above stratified problem of representation heterogeneity in\nterms of several solution components, namely: (i) a representation formalism\nstratified into concept level, language level, knowledge level and data level\nto accommodate representation heterogeneity, (ii) a top-down language\nrepresentation using Universal Knowledge Core (UKC), UKC namespaces and domain\nlanguages to tackle the conceptual and language level heterogeneity, (iii) a\ntop-down knowledge representation using the notions of language teleontology\nand knowledge teleontology to tackle the knowledge level heterogeneity, (iv)\nthe usage and further development of the existing LiveKnowledge catalog for\nenforcing iterative reuse and sharing of language and knowledge\nrepresentations, and, (v) the kTelos methodology integrating the solution\ncomponents above to iteratively generate the language and knowledge\nrepresentations absolving representation heterogeneity. The thesis also\nincludes proof-of-concepts of the language and knowledge representations\ndeveloped for two international research projects - DataScientia (data\ncatalogs) and JIDEP (materials modelling). Finally, the thesis concludes with\nfuture lines of research.", "AI": {"tldr": "The thesis addresses representation heterogeneity by proposing a stratified top-down approach using UKC, teleontology, and the kTelos methodology, with proof-of-concepts in DataScientia and JIDEP.", "motivation": "Heterogeneity is intrinsic to representations, as different observers encode reality differently using varied concepts, language, and knowledge.", "method": "A top-down approach with stratified representation formalism, UKC for language, teleontology for knowledge, and the LiveKnowledge catalog for reuse. The kTelos methodology integrates these components.", "result": "Proof-of-concepts in DataScientia (data catalogs) and JIDEP (materials modelling) demonstrate the approach's effectiveness.", "conclusion": "The thesis presents a solution to representation heterogeneity and suggests future research directions."}}
{"id": "2504.11798", "pdf": "https://arxiv.org/pdf/2504.11798", "abs": "https://arxiv.org/abs/2504.11798", "authors": ["Chao Yuan", "Tianyi Zhang", "Guanglin Niu"], "title": "Neighbor-Based Feature and Index Enhancement for Person Re-Identification", "categories": ["cs.CV"], "comment": "Comment: This paper has been accepted for publication in the 2025\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW)", "summary": "Person re-identification (Re-ID) aims to match the same pedestrian in a large\ngallery with different cameras and views. Enhancing the robustness of the\nextracted feature representations is a main challenge in Re-ID. Existing\nmethods usually improve feature representation by improving model architecture,\nbut most methods ignore the potential contextual information, which limits the\neffectiveness of feature representation and retrieval performance. Neighborhood\ninformation, especially the potential information of multi-order neighborhoods,\ncan effectively enrich feature expression and improve retrieval accuracy, but\nthis has not been fully explored in existing research. Therefore, we propose a\nnovel model DMON-ARO that leverages latent neighborhood information to enhance\nboth feature representation and index performance. Our approach is built on two\ncomplementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and\nAsymmetric Relationship Optimization (ARO). The DMON module dynamically\naggregates multi-order neighbor relationships, allowing it to capture richer\ncontextual information and enhance feature representation through adaptive\nneighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing\nquery-to-gallery relationships, improving the index accuracy. Extensive\nexperiments on three benchmark datasets demonstrate that our approach achieves\nperformance improvements against baseline models, which illustrate the\neffectiveness of our model. Specifically, our model demonstrates improvements\nin Rank-1 accuracy and mAP. Moreover, this method can also be directly extended\nto other re-identification tasks.", "AI": {"tldr": "The paper proposes DMON-ARO, a model for person re-identification that enhances feature representation and retrieval performance by leveraging multi-order neighborhood information and optimizing query-to-gallery relationships.", "motivation": "Existing Re-ID methods often overlook contextual information, limiting feature representation effectiveness. Multi-order neighborhood information can enrich features but is underexplored.", "method": "DMON-ARO combines Dynamic Multi-Order Neighbor Modeling (DMON) for adaptive neighborhood aggregation and Asymmetric Relationship Optimization (ARO) for refining distance matrices.", "result": "Experiments on benchmark datasets show improved Rank-1 accuracy and mAP compared to baselines.", "conclusion": "DMON-ARO effectively enhances Re-ID performance and can be extended to other re-identification tasks."}}
{"id": "2504.11923", "pdf": "https://arxiv.org/pdf/2504.11923", "abs": "https://arxiv.org/abs/2504.11923", "authors": ["Zeyu Dai", "Shengcai Liu", "Rui He", "Jiahao Wu", "Ning Lu", "Wenqi Fan", "Qing Li", "Ke Tang"], "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening.", "AI": {"tldr": "SemDiff is a novel unrestricted adversarial attack using diffusion models' semantic latent space to create natural and imperceptible adversarial examples, outperforming state-of-the-art methods.", "motivation": "Existing unrestricted adversarial examples (UAEs) lack naturalness and imperceptibility due to suboptimal optimization in latent noise.", "method": "SemDiff explores semantic latent space for meaningful attributes and uses multi-attribute optimization to balance attack success and naturalness.", "result": "SemDiff achieves higher attack success rates and better imperceptibility on high-resolution datasets (CelebA-HQ, AFHQ, ImageNet) and evades defenses effectively.", "conclusion": "SemDiff is a highly effective and threatening adversarial attack method, producing natural and semantically meaningful UAEs."}}
{"id": "2504.11524", "pdf": "https://arxiv.org/pdf/2504.11524", "abs": "https://arxiv.org/abs/2504.11524", "authors": ["Haokun Liu", "Sicong Huang", "Jingyu Hu", "Yangqiaoyu Zhou", "Chenhao Tan"], "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, website link:\n  https://chicagohai.github.io/HypoBench/", "summary": "There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery.", "AI": {"tldr": "HypoBench is a benchmark for evaluating LLMs in hypothesis generation, testing practical utility, generalizability, and discovery rate across real-world and synthetic tasks. Results show current methods find valid patterns but struggle with synthetic data, recovering only 38.8% of ground-truth hypotheses.", "motivation": "To address the lack of systematic evaluation for hypothesis generation methods in LLMs and define what makes a good hypothesis.", "method": "Introduces HypoBench, a benchmark with 7 real-world and 5 synthetic tasks (194 datasets), evaluating 4 LLMs and 6 hypothesis-generation methods.", "result": "Existing methods discover valid patterns but perform poorly on synthetic data, recovering only 38.8% of ground-truth hypotheses as task difficulty increases.", "conclusion": "HypoBench highlights challenges in hypothesis generation and serves as a resource for improving AI systems in scientific discovery."}}
{"id": "2504.11820", "pdf": "https://arxiv.org/pdf/2504.11820", "abs": "https://arxiv.org/abs/2504.11820", "authors": ["Delong Suzhang", "Meng Yang"], "title": "Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps.", "AI": {"tldr": "The paper addresses the challenge of recovering high-quality depth from low-quality raw depth maps in real-world RGB-D datasets, proposing a method to improve generalization by tackling structure misalignment in both input and output stages.", "motivation": "Real-world depth recovery is hindered by the lack of paired raw-GT data and the diversity of structure misalignment in raw depth maps, leading to poor generalization. Existing methods fail to address these issues adequately.", "method": "The proposed method includes a new raw depth generation pipeline to diversify structure misalignment in input, a structure uncertainty module to identify misaligned structures, and a robust feature alignment module for output to align with accurate RGB structures.", "result": "Extensive experiments show the method achieves competitive accuracy and generalization across challenging raw depth maps.", "conclusion": "The approach effectively improves generalization in real-world depth recovery by addressing structure misalignment in both input and output stages."}}
{"id": "2504.11944", "pdf": "https://arxiv.org/pdf/2504.11944", "abs": "https://arxiv.org/abs/2504.11944", "authors": ["Xuyang Chen", "Guojian Wang", "Keyu Yan", "Lin Zhao"], "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline reinforcement learning (RL) learns effective policies from\npre-collected datasets, offering a practical solution for applications where\nonline interactions are risky or costly. Model-based approaches are\nparticularly advantageous for offline RL, owing to their data efficiency and\ngeneralizability. However, due to inherent model errors, model-based methods\noften artificially introduce conservatism guided by heuristic uncertainty\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\nmodel-based offline RL algorithm that incorporates self-supervised feedback\nfrom value estimation to enhance model training. Specifically, the model is\nlearned by additionally minimizing the inconsistency between the value learned\ndirectly from the offline data and the one estimated from the model. We perform\ncomprehensive evaluations from multiple perspectives to show that VIPO can\nlearn a highly accurate model efficiently and consistently outperform existing\nmethods. It offers a general framework that can be readily integrated into\nexisting model-based offline RL algorithms to systematically enhance model\naccuracy. As a result, VIPO achieves state-of-the-art performance on almost all\ntasks in both D4RL and NeoRL benchmarks.", "AI": {"tldr": "VIPO is a novel model-based offline RL algorithm that uses self-supervised feedback from value estimation to improve model training, outperforming existing methods on benchmarks.", "motivation": "Offline RL is practical but suffers from unreliable conservatism due to model errors. VIPO aims to enhance model accuracy systematically.", "method": "VIPO minimizes inconsistency between value estimates from offline data and the model, improving training via self-supervised feedback.", "result": "VIPO achieves state-of-the-art performance on D4RL and NeoRL benchmarks, demonstrating superior model accuracy and efficiency.", "conclusion": "VIPO provides a general, effective framework for enhancing model-based offline RL, setting new benchmarks in performance."}}
{"id": "2504.11571", "pdf": "https://arxiv.org/pdf/2504.11571", "abs": "https://arxiv.org/abs/2504.11571", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "categories": ["cs.AI", "cs.CL"], "comment": "41 pages, 11 figures", "summary": "Large Language Model (LLM)-powered agents have unlocked new possibilities for\nautomating human tasks. While prior work has focused on well-defined tasks with\nspecified goals, the capabilities of agents in creative design tasks with\nopen-ended goals remain underexplored. We introduce GraphicBench, a new\nplanning benchmark for graphic design that covers 1,079 user queries and input\nimages across four design types. We further present GraphicTown, an LLM agent\nframework with three design experts and 46 actions (tools) to choose from for\nexecuting each step of the planned workflows in web environments. Experiments\nwith six LLMs demonstrate their ability to generate workflows that integrate\nboth explicit design constraints from user queries and implicit commonsense\nconstraints. However, these workflows often do not lead to successful execution\noutcomes, primarily due to challenges in: (1) reasoning about spatial\nrelationships, (2) coordinating global dependencies across experts, and (3)\nretrieving the most appropriate action per step. We envision GraphicBench as a\nchallenging yet valuable testbed for advancing LLM-agent planning and execution\nin creative design tasks.", "AI": {"tldr": "The paper introduces GraphicBench, a benchmark for evaluating LLM-powered agents in creative graphic design tasks, and GraphicTown, an agent framework. While LLMs can generate workflows, execution challenges remain.", "motivation": "To explore LLM agents' capabilities in open-ended creative design tasks, which are underexplored compared to well-defined tasks.", "method": "Developed GraphicBench (1,079 queries across four design types) and GraphicTown (a framework with three experts and 46 tools). Tested six LLMs for workflow generation and execution.", "result": "LLMs can generate workflows integrating explicit and implicit constraints, but execution often fails due to spatial reasoning, global coordination, and action retrieval issues.", "conclusion": "GraphicBench serves as a valuable testbed for advancing LLM-agent planning and execution in creative design, despite current challenges."}}
{"id": "2504.11838", "pdf": "https://arxiv.org/pdf/2504.11838", "abs": "https://arxiv.org/abs/2504.11838", "authors": ["Bianca Lamm", "Janis Keuper"], "title": "A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification", "categories": ["cs.CV"], "comment": null, "summary": "Despite the rapid evolution of learning and computer vision algorithms,\nFine-Grained Classification (FGC) still poses an open problem in many\npractically relevant applications. In the retail domain, for example, the\nidentification of fast changing and visually highly similar products and their\nproperties are key to automated price-monitoring and product recommendation.\nThis paper presents a novel Visual RAG pipeline that combines the Retrieval\nAugmented Generation (RAG) approach and Vision Language Models (VLMs) for\nfew-shot FGC. This Visual RAG pipeline extracts product and promotion data in\nadvertisement leaflets from various retailers and simultaneously predicts\nfine-grained product ids along with price and discount information. Compared to\nprevious approaches, the key characteristic of the Visual RAG pipeline is that\nit allows the prediction of novel products without re-training, simply by\nadding a few class samples to the RAG database. Comparing several VLM back-ends\nlike GPT-4o [23], GPT-4o-mini [24], and Gemini 2.0 Flash [10], our approach\nachieves 86.8% accuracy on a diverse dataset.", "AI": {"tldr": "The paper introduces a Visual RAG pipeline combining Retrieval Augmented Generation (RAG) and Vision Language Models (VLMs) for few-shot Fine-Grained Classification (FGC), achieving 86.8% accuracy.", "motivation": "Fine-Grained Classification remains challenging in retail for tasks like automated price-monitoring and product recommendation due to fast-changing, visually similar products.", "method": "A novel Visual RAG pipeline integrates RAG and VLMs to extract and predict product details from advertisement leaflets without re-training for new products.", "result": "The approach achieves 86.8% accuracy on a diverse dataset, outperforming previous methods by enabling prediction of novel products with minimal updates.", "conclusion": "The Visual RAG pipeline is effective for FGC in retail, offering flexibility and high accuracy without requiring frequent re-training."}}
{"id": "2504.11981", "pdf": "https://arxiv.org/pdf/2504.11981", "abs": "https://arxiv.org/abs/2504.11981", "authors": ["Sosei Ikeda", "Hiromitsu Awano", "Takashi Sato"], "title": "Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Reservoir computing (RC) is attracting attention as a machine-learning\ntechnique for edge computing. In time-series classification tasks, the number\nof features obtained using a reservoir depends on the length of the input\nseries. Therefore, the features must be converted to a constant-length\nintermediate representation (IR), such that they can be processed by an output\nlayer. Existing conversion methods involve computationally expensive matrix\ninversion that significantly increases the circuit size and requires processing\npower when implemented in hardware. In this article, we propose a simple but\neffective IR, namely, dot-product-based reservoir representation (DPRR), for RC\nbased on the dot product of data features. Additionally, we propose a\nhardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear\nelement and delayed feedback loop with DPRR. The proposed DFR successfully\nclassified multivariate time series data that has been considered particularly\ndifficult to implement efficiently in hardware. In contrast to conventional DFR\nmodels that require analog circuits, the proposed model can be implemented in a\nfully digital manner suitable for high-level syntheses. A comparison with\nexisting machine-learning methods via field-programmable gate array\nimplementation using 12 multivariate time-series classification tasks confirmed\nthe superior accuracy and small circuit size of the proposed method.", "AI": {"tldr": "Proposes a dot-product-based reservoir representation (DPRR) and a hardware-friendly delayed-feedback reservoir (DFR) for efficient time-series classification in edge computing.", "motivation": "Existing methods for converting features in reservoir computing (RC) involve costly matrix inversion, increasing hardware complexity. A simpler, hardware-efficient solution is needed.", "method": "Introduces DPRR for constant-length intermediate representation and a fully digital DFR model, avoiding analog circuits.", "result": "The proposed DFR successfully classified difficult multivariate time-series data with superior accuracy and smaller circuit size compared to existing methods.", "conclusion": "The DPRR and digital DFR offer a practical, efficient solution for hardware implementation in RC, outperforming conventional methods."}}
{"id": "2504.11741", "pdf": "https://arxiv.org/pdf/2504.11741", "abs": "https://arxiv.org/abs/2504.11741", "authors": ["Yiyou Sun", "Georgia Zhou", "Hao Wang", "Dacheng Li", "Nouha Dziri", "Dawn Song"], "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.", "AI": {"tldr": "The paper analyzes how supervised fine-tuning (SFT) enhances language models' mathematical reasoning, identifying a ladder-like difficulty structure in the AIME24 dataset and tier-specific requirements for progression.", "motivation": "To understand the specific capabilities improved by SFT in mathematical reasoning tasks and how reasoning evolves across difficulty tiers.", "method": "Detailed analysis of model performance on the AIME24 dataset, categorizing questions into four tiers (Easy, Medium, Hard, Exh) and examining progression requirements.", "result": "Easy to Medium progression requires R1 reasoning with minimal SFT; Hard-tier accuracy plateaus at 65%; Exh-tier challenges remain unsolved. Dataset size scaling is more effective than curation.", "conclusion": "The study offers a roadmap for advancing language models in mathematical reasoning, emphasizing dataset size over curation and highlighting unresolved challenges in higher tiers."}}
{"id": "2504.11845", "pdf": "https://arxiv.org/pdf/2504.11845", "abs": "https://arxiv.org/abs/2504.11845", "authors": ["Jie Zhu", "Bo Peng", "Zhe Zhang", "Bingzheng Liu", "Jianjun Lei"], "title": "Boosting Multi-View Stereo with Depth Foundation Model in the Absence of Real-World Labels", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based Multi-View Stereo (MVS) methods have made remarkable progress\nin recent years. However, how to effectively train the network without using\nreal-world labels remains a challenging problem. In this paper, driven by the\nrecent advancements of vision foundation models, a novel method termed DFM-MVS,\nis proposed to leverage the depth foundation model to generate the effective\ndepth prior, so as to boost MVS in the absence of real-world labels.\nSpecifically, a depth prior-based pseudo-supervised training mechanism is\ndeveloped to simulate realistic stereo correspondences using the generated\ndepth prior, thereby constructing effective supervision for the MVS network.\nBesides, a depth prior-guided error correction strategy is presented to\nleverage the depth prior as guidance to mitigate the error propagation problem\ninherent in the widely-used coarse-to-fine network structure. Experimental\nresults on DTU and Tanks & Temples datasets demonstrate that the proposed\nDFM-MVS significantly outperforms existing MVS methods without using real-world\nlabels.", "AI": {"tldr": "DFM-MVS leverages a depth foundation model to generate depth priors for training MVS networks without real-world labels, improving performance on benchmarks.", "motivation": "Training MVS networks without real-world labels is challenging, and existing methods lack effective supervision.", "method": "DFM-MVS uses depth priors for pseudo-supervised training and error correction in a coarse-to-fine network structure.", "result": "Outperforms existing MVS methods on DTU and Tanks & Temples datasets without real-world labels.", "conclusion": "DFM-MVS effectively addresses the label-free training challenge in MVS by leveraging depth priors."}}
{"id": "2504.11990", "pdf": "https://arxiv.org/pdf/2504.11990", "abs": "https://arxiv.org/abs/2504.11990", "authors": ["Yechao Zhang", "Yuxuan Zhou", "Tianyu Li", "Minghui Li", "Shengshan Hu", "Wei Luo", "Leo Yu Zhang"], "title": "Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets", "categories": ["cs.LG", "cs.CR"], "comment": "To appear at IEEE Symposium on Security and Privacy 2025, 20 pages", "summary": "Transfer learning from pre-trained encoders has become essential in modern\nmachine learning, enabling efficient model adaptation across diverse tasks.\nHowever, this combination of pre-training and downstream adaptation creates an\nexpanded attack surface, exposing models to sophisticated backdoor embeddings\nat both the encoder and dataset levels--an area often overlooked in prior\nresearch. Additionally, the limited computational resources typically available\nto users of pre-trained encoders constrain the effectiveness of generic\nbackdoor defenses compared to end-to-end training from scratch. In this work,\nwe investigate how to mitigate potential backdoor risks in resource-constrained\ntransfer learning scenarios. Specifically, we conduct an exhaustive analysis of\nexisting defense strategies, revealing that many follow a reactive workflow\nbased on assumptions that do not scale to unknown threats, novel attack types,\nor different training paradigms. In response, we introduce a proactive mindset\nfocused on identifying clean elements and propose the Trusted Core (T-Core)\nBootstrapping framework, which emphasizes the importance of pinpointing\ntrustworthy data and neurons to enhance model security. Our empirical\nevaluations demonstrate the effectiveness and superiority of T-Core,\nspecifically assessing 5 encoder poisoning attacks, 7 dataset poisoning\nattacks, and 14 baseline defenses across five benchmark datasets, addressing\nfour scenarios of 3 potential backdoor threats.", "AI": {"tldr": "The paper addresses backdoor risks in transfer learning, proposing the Trusted Core (T-Core) Bootstrapping framework to enhance security by identifying trustworthy data and neurons.", "motivation": "Pre-trained encoders in transfer learning expose models to backdoor attacks, with existing defenses being reactive and ineffective under resource constraints.", "method": "The authors analyze existing defenses and introduce T-Core, a proactive framework focusing on clean elements (data and neurons) for security.", "result": "T-Core outperforms baselines, tested against 12 poisoning attacks and 14 defenses across five datasets, handling three backdoor threats.", "conclusion": "T-Core provides a robust solution for mitigating backdoor risks in resource-constrained transfer learning, emphasizing proactive security."}}
{"id": "2504.11844", "pdf": "https://arxiv.org/pdf/2504.11844", "abs": "https://arxiv.org/abs/2504.11844", "authors": ["Tom Everitt", "Cristina Garbacea", "Alexis Bellot", "Jonathan Richens", "Henry Papadatos", "Sim\u00e9on Campos", "Rohin Shah"], "title": "Evaluating the Goal-Directedness of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.", "AI": {"tldr": "The paper evaluates goal-directedness in LLMs, finding it consistent across tasks but not fully achieved, and suggests its use for monitoring progress and design choices.", "motivation": "To measure how effectively LLMs use their capabilities toward given goals, termed goal-directedness, to improve monitoring and design of agentic properties.", "method": "Evaluated goal-directedness using tasks requiring information gathering, cognitive effort, and plan execution, with subtasks to infer capabilities. Tested models from Google DeepMind, OpenAI, and Anthropic.", "result": "Goal-directedness is consistent across tasks, differs from task performance, and is moderately sensitive to motivational prompts. Most models are not fully goal-directed.", "conclusion": "Goal-directedness evaluations can enhance LLM progress monitoring and inform deliberate design of agentic properties."}}
{"id": "2504.11850", "pdf": "https://arxiv.org/pdf/2504.11850", "abs": "https://arxiv.org/abs/2504.11850", "authors": ["Finn Carter"], "title": "ACE: Attentional Concept Erasure in Diffusion Models", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Large text-to-image diffusion models have demonstrated remarkable image\nsynthesis capabilities, but their indiscriminate training on Internet-scale\ndata has led to learned concepts that enable harmful, copyrighted, or otherwise\nundesirable content generation. We address the task of concept erasure in\ndiffusion models, i.e., removing a specified concept from a pre-trained model\nsuch that prompting the concept (or related synonyms) no longer yields its\ndepiction, while preserving the model's ability to generate other content. We\npropose a novel method, Attentional Concept Erasure (ACE), that integrates a\nclosed-form attention manipulation with lightweight fine-tuning. Theoretically,\nwe formulate concept erasure as aligning the model's conditional distribution\non the target concept with a neutral distribution. Our approach identifies and\nnullifies concept-specific latent directions in the cross-attention modules via\na gated low-rank adaptation, followed by adversarially augmented fine-tuning to\nensure thorough erasure of the concept and its synonyms. Empirically, we\ndemonstrate on multiple benchmarks, including object classes, celebrity faces,\nexplicit content, and artistic styles, that ACE achieves state-of-the-art\nconcept removal efficacy and robustness. Compared to prior methods, ACE better\nbalances generality (erasing concept and related terms) and specificity\n(preserving unrelated content), scales to dozens of concepts, and is efficient,\nrequiring only a few seconds of adaptation per concept. We will release our\ncode to facilitate safer deployment of diffusion models.", "AI": {"tldr": "ACE is a novel method for erasing harmful or unwanted concepts from diffusion models while preserving other content, using attention manipulation and fine-tuning.", "motivation": "To address the issue of harmful or undesirable content generation in diffusion models by enabling selective concept removal.", "method": "Attentional Concept Erasure (ACE) combines closed-form attention manipulation with lightweight fine-tuning, aligning the model's conditional distribution on the target concept with a neutral distribution.", "result": "ACE achieves state-of-the-art concept removal efficacy and robustness across various benchmarks, balancing generality and specificity efficiently.", "conclusion": "ACE offers a scalable and efficient solution for safer deployment of diffusion models by enabling precise concept erasure."}}
{"id": "2504.11992", "pdf": "https://arxiv.org/pdf/2504.11992", "abs": "https://arxiv.org/abs/2504.11992", "authors": ["Pascal Schlachter", "Jonathan Fuss", "Bin Yang"], "title": "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025)", "summary": "A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis.", "AI": {"tldr": "The paper analyzes pseudo-labeling in online source-free universal domain adaptation (SF-UniDA), revealing gaps between current methods and ideal performance, and highlights the importance of pseudo-label accuracy over quantity.", "motivation": "To address the challenge of category shifts in online SF-UniDA and understand the impact of pseudo-labeling on adaptation outcomes.", "method": "Systematic analysis through controlled experiments with simulated pseudo-labeling, comparing contrastive and cross-entropy losses.", "result": "A significant gap exists between state-of-the-art and ideal pseudo-labeling performance; contrastive loss works well with moderate accuracy, while cross-entropy excels with near-perfect labels.", "conclusion": "Pseudo-label accuracy is crucial, and prioritizing high-confidence labels is beneficial, providing insights for future SF-UniDA advancements."}}
{"id": "2504.11889", "pdf": "https://arxiv.org/pdf/2504.11889", "abs": "https://arxiv.org/abs/2504.11889", "authors": ["Donghee Han", "Hwanjun Song", "Mun Yong Yi"], "title": "Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.", "AI": {"tldr": "A Query-to-Recommendation approach using LLMs improves recommendation efficiency, diversity, and performance, addressing challenges like inefficiency and sensitivity to item order.", "motivation": "Existing LLM-based recommendation methods struggle with inefficiency, sensitivity to item order, poor scalability, and unrealistic evaluation due to random negative sampling.", "method": "Proposes a Query-to-Recommendation approach where LLMs generate personalized queries to retrieve relevant items from the entire candidate pool, avoiding pre-selection.", "result": "Experiments show up to 57% improvement, with an average gain of 31%, demonstrating strong zero-shot performance and further gains when combined with existing models.", "conclusion": "The method enhances recommendation performance and diversity, works well for less popular items, and integrates seamlessly into existing systems without additional training."}}
{"id": "2504.11856", "pdf": "https://arxiv.org/pdf/2504.11856", "abs": "https://arxiv.org/abs/2504.11856", "authors": ["Zhenhuan Zhou", "Yuchen Zhang", "Along He", "Peng Wang", "Xueshuo Xie", "Tao Li"], "title": "Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation", "categories": ["cs.CV"], "comment": "12 pages, Initial submission time 25 December 2024, Now Under Review", "summary": "Root canal (RC) treatment is a highly delicate and technically complex\nprocedure in clinical practice, heavily influenced by the clinicians'\nexperience and subjective judgment. Deep learning has made significant\nadvancements in the field of computer-aided diagnosis (CAD) because it can\nprovide more objective and accurate diagnostic results. However, its\napplication in RC treatment is still relatively rare, mainly due to the lack of\npublic datasets in this field. To address this issue, in this paper, we\nestablished a First Molar Root Canal segmentation dataset called FMRC-2025.\nAdditionally, to alleviate the workload of manual annotation for dentists and\nfully leverage the unlabeled data, we designed a Cross-Frequency Collaborative\ntraining semi-supervised learning (SSL) Network called CFC-Net. It consists of\ntwo components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which\nintroduces two specialized students (SS) and one comprehensive teacher (CT) for\ncollaborative multi-frequency training. The CT and SS are trained on different\nfrequency components while fully integrating multi-frequency knowledge through\ncross and full frequency consistency supervisions. (2) Uncertainty-guided\nCross-Frequency Mix (UCF-Mix) mechanism enables the network to generate\nhigh-confidence pseudo-labels while learning to integrate multi-frequency\ninformation and maintaining the structural integrity of the targets. Extensive\nexperiments on FMRC-2025 and three public dental datasets demonstrate that\nCFC-MT is effective for RC segmentation and can also exhibit strong\ngeneralizability on other dental segmentation tasks, outperforming\nstate-of-the-art SSL medical image segmentation methods. Codes and dataset will\nbe released.", "AI": {"tldr": "The paper introduces a semi-supervised learning network (CFC-Net) for root canal segmentation, addressing the lack of public datasets with a new dataset (FMRC-2025) and leveraging unlabeled data.", "motivation": "Root canal treatment lacks objective diagnostic tools due to limited datasets and reliance on clinician experience. Deep learning can improve accuracy but requires labeled data.", "method": "Proposes CFC-Net with two components: CFC-MT for multi-frequency training and UCF-Mix for high-confidence pseudo-label generation.", "result": "CFC-Net outperforms state-of-the-art methods on FMRC-2025 and other dental datasets, demonstrating strong generalizability.", "conclusion": "CFC-Net effectively addresses dataset scarcity and improves root canal segmentation, with potential for broader dental applications."}}
{"id": "2504.11997", "pdf": "https://arxiv.org/pdf/2504.11997", "abs": "https://arxiv.org/abs/2504.11997", "authors": ["Kihyuk Hong", "Ambuj Tewari"], "title": "A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study reinforcement learning in infinite-horizon average-reward settings\nwith linear MDPs. Previous work addresses this problem by approximating the\naverage-reward setting by discounted setting and employing a value\niteration-based algorithm that uses clipping to constrain the span of the value\nfunction for improved statistical efficiency. However, the clipping procedure\nrequires computing the minimum of the value function over the entire state\nspace, which is prohibitive since the state space in linear MDP setting can be\nlarge or even infinite. In this paper, we introduce a value iteration method\nwith efficient clipping operation that only requires computing the minimum of\nvalue functions over the set of states visited by the algorithm. Our algorithm\nenjoys the same regret bound as the previous work while being computationally\nefficient, with computational complexity that is independent of the size of the\nstate space.", "AI": {"tldr": "A new value iteration method for reinforcement learning in linear MDPs avoids prohibitive computations by clipping over visited states only, matching prior regret bounds with improved efficiency.", "motivation": "Prior methods for average-reward reinforcement learning in linear MDPs require impractical computations over large or infinite state spaces.", "method": "Introduces a value iteration algorithm with efficient clipping, minimizing over visited states instead of the entire state space.", "result": "Achieves the same regret bound as previous work while being computationally efficient, independent of state space size.", "conclusion": "The proposed method is both statistically and computationally efficient, addressing a key limitation of prior approaches."}}
{"id": "2504.11942", "pdf": "https://arxiv.org/pdf/2504.11942", "abs": "https://arxiv.org/abs/2504.11942", "authors": ["Nada Shahin", "Leila Ismail"], "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "comment": null, "summary": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.", "AI": {"tldr": "The paper introduces ADAT, an Adaptive Transformer for sign language translation, addressing issues of fine-grained temporal dependency recognition and computational inefficiency. It outperforms baselines in accuracy and training efficiency.", "motivation": "Current Transformer-based sign language translation systems struggle with fine-grained temporal dependencies and high computational complexity, limiting their accuracy and efficiency.", "method": "The proposed ADAT incorporates enhanced feature extraction and adaptive feature weighting via a gating mechanism to improve contextual relevance and reduce training overhead.", "result": "ADAT improves BLEU-4 accuracy by 0.1% and reduces training time by 14.33% on PHOENIX14T, and achieves higher accuracy (8.7%) and faster training (2.8%) in sign-to-text tasks.", "conclusion": "ADAT effectively balances accuracy and efficiency in sign language translation, demonstrating superior performance over existing baselines."}}
{"id": "2504.11858", "pdf": "https://arxiv.org/pdf/2504.11858", "abs": "https://arxiv.org/abs/2504.11858", "authors": ["Jo\u00ebl Mathys", "Andreas Plesner", "Jorel Elmiger", "Roger Wattenhofer"], "title": "Synthetic Data for Blood Vessel Network Extraction", "categories": ["cs.CV"], "comment": "Presented at SynthData Workshop at ICLR 2025", "summary": "Blood vessel networks in the brain play a crucial role in stroke research,\nwhere understanding their topology is essential for analyzing blood flow\ndynamics. However, extracting detailed topological vessel network information\nfrom microscopy data remains a significant challenge, mainly due to the\nscarcity of labeled training data and the need for high topological accuracy.\nThis work combines synthetic data generation with deep learning to\nautomatically extract vessel networks as graphs from volumetric microscopy\ndata. To combat data scarcity, we introduce a comprehensive pipeline for\ngenerating large-scale synthetic datasets that mirror the characteristics of\nreal vessel networks. Our three-stage approach progresses from abstract graph\ngeneration through vessel mask creation to realistic medical image synthesis,\nincorporating biological constraints and imaging artifacts at each stage. Using\nthis synthetic data, we develop a two-stage deep learning pipeline of 3D\nU-Net-based models for node detection and edge prediction. Fine-tuning on real\nmicroscopy data shows promising adaptation, improving edge prediction F1 scores\nfrom 0.496 to 0.626 by training on merely 5 manually labeled samples. These\nresults suggest that automated vessel network extraction is becoming\npractically feasible, opening new possibilities for large-scale vascular\nanalysis in stroke research.", "AI": {"tldr": "The paper proposes a method combining synthetic data generation and deep learning to extract detailed vessel networks from microscopy data, addressing data scarcity and improving accuracy for stroke research.", "motivation": "Understanding brain vessel topology is critical for stroke research, but extracting detailed networks from microscopy data is challenging due to limited labeled data and the need for high accuracy.", "method": "A three-stage synthetic data generation pipeline creates realistic vessel networks, followed by a two-stage deep learning approach (3D U-Net models) for node detection and edge prediction, fine-tuned on real data.", "result": "Fine-tuning on just 5 labeled samples improved edge prediction F1 scores from 0.496 to 0.626, demonstrating practical feasibility.", "conclusion": "Automated vessel network extraction is now feasible, enabling large-scale vascular analysis in stroke research."}}
{"id": "2504.12011", "pdf": "https://arxiv.org/pdf/2504.12011", "abs": "https://arxiv.org/abs/2504.12011", "authors": ["Heesoo Jung", "Hogun Park"], "title": "Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Web Conference (WWW) 2025", "summary": "Self-supervised learning (SSL) in graphs has garnered significant attention,\nparticularly in employing Graph Neural Networks (GNNs) with pretext tasks\ninitially designed for other domains, such as contrastive learning and feature\nreconstruction. However, it remains uncertain whether these methods effectively\nreflect essential graph properties, precisely representation similarity with\nits neighbors. We observe that existing methods position opposite ends of a\nspectrum driven by the graph embedding smoothness, with each end corresponding\nto outperformance on specific downstream tasks. Decomposing the SSL objective\ninto three terms via an information-theoretic framework with a neighbor\nrepresentation variable reveals that this polarization stems from an imbalance\namong the terms, which existing methods may not effectively maintain. Further\ninsights suggest that balancing between the extremes can lead to improved\nperformance across a wider range of downstream tasks. A framework, BSG\n(Balancing Smoothness in Graph SSL), introduces novel loss functions designed\nto supplement the representation quality in graph-based SSL by balancing the\nderived three terms: neighbor loss, minimal loss, and divergence loss. We\npresent a theoretical analysis of the effects of these loss functions,\nhighlighting their significance from both the SSL and graph smoothness\nperspectives. Extensive experiments on multiple real-world datasets across node\nclassification and link prediction consistently demonstrate that BSG achieves\nstate-of-the-art performance, outperforming existing methods. Our\nimplementation code is available at https://github.com/steve30572/BSG.", "AI": {"tldr": "The paper introduces BSG, a framework for balancing smoothness in graph self-supervised learning (SSL), addressing limitations of existing methods by decomposing the SSL objective into three balanced terms.", "motivation": "Existing SSL methods in graphs, like contrastive learning, may not effectively capture essential graph properties, such as neighbor representation similarity, leading to polarized performance on downstream tasks.", "method": "The BSG framework decomposes the SSL objective into neighbor loss, minimal loss, and divergence loss, introducing novel loss functions to balance these terms. Theoretical analysis and experiments validate the approach.", "result": "BSG achieves state-of-the-art performance on node classification and link prediction tasks across multiple datasets, outperforming existing methods.", "conclusion": "Balancing the three SSL terms improves performance across diverse downstream tasks, demonstrating the effectiveness of the BSG framework in graph SSL."}}
{"id": "2504.12137", "pdf": "https://arxiv.org/pdf/2504.12137", "abs": "https://arxiv.org/abs/2504.12137", "authors": ["Laura Fieback", "Nishilkumar Balar", "Jakob Spiegelberg", "Hanno Gottschalk"], "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.", "AI": {"tldr": "The paper introduces Efficient Contrastive Decoding (ECD) to reduce hallucinations in Large Vision Language Models (LVLMs) by adjusting output distributions without additional training.", "motivation": "LVLMs often generate hallucinatory responses misaligned with visual inputs, necessitating a method to improve accuracy.", "method": "ECD uses probabilistic hallucination detection to contrast token probabilities and hallucination scores, refining output distributions.", "result": "ECD outperforms state-of-the-art methods in reducing hallucinations and computational efficiency on benchmark datasets.", "conclusion": "ECD is a versatile, training-free solution for mitigating hallucinations in LVLMs, enhancing performance and efficiency."}}
{"id": "2504.11872", "pdf": "https://arxiv.org/pdf/2504.11872", "abs": "https://arxiv.org/abs/2504.11872", "authors": ["Daiqi Liu", "Fuxin Fan", "Andreas Maier"], "title": "A Category-Fragment Segmentation Framework for Pelvic Fracture Segmentation in X-ray Images", "categories": ["cs.CV"], "comment": "5 pages, 2 figures, 1 table", "summary": "Pelvic fractures, often caused by high-impact trauma, frequently require\nsurgical intervention. Imaging techniques such as CT and 2D X-ray imaging are\nused to transfer the surgical plan to the operating room through image\nregistration, enabling quick intraoperative adjustments. Specifically,\nsegmenting pelvic fractures from 2D X-ray imaging can assist in accurately\npositioning bone fragments and guiding the placement of screws or metal plates.\nIn this study, we propose a novel deep learning-based category and fragment\nsegmentation (CFS) framework for the automatic segmentation of pelvic bone\nfragments in 2D X-ray images. The framework consists of three consecutive\nsteps: category segmentation, fragment segmentation, and post-processing. Our\nbest model achieves an IoU of 0.91 for anatomical structures and 0.78 for\nfracture segmentation. Results demonstrate that the CFS framework is effective\nand accurate.", "AI": {"tldr": "A deep learning framework (CFS) is proposed for automatic segmentation of pelvic bone fragments in 2D X-rays, achieving high accuracy.", "motivation": "Pelvic fractures require precise surgical planning, and current imaging methods need automation for better efficiency.", "method": "The CFS framework uses three steps: category segmentation, fragment segmentation, and post-processing.", "result": "The model achieves an IoU of 0.91 for anatomical structures and 0.78 for fracture segmentation.", "conclusion": "The CFS framework is effective and accurate for pelvic fracture segmentation in 2D X-rays."}}
{"id": "2504.12016", "pdf": "https://arxiv.org/pdf/2504.12016", "abs": "https://arxiv.org/abs/2504.12016", "authors": ["Arun Verma", "Xiaoqiang Lin", "Zhongxiang Dai", "Daniela Rus", "Bryan Kian Hsiang Low"], "title": "Active Human Feedback Collection via Neural Contextual Dueling Bandits", "categories": ["cs.LG"], "comment": "Accepted at ICLR 2025 Workshop on Bidirectional Human-AI Alignment\n  (BiAlign)", "summary": "Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB.", "AI": {"tldr": "Neural-ADB is a neural contextual dueling bandit algorithm for efficiently collecting human preference feedback when the reward function is non-linear, outperforming linear assumptions.", "motivation": "Existing methods assume linear reward functions, which are unrealistic for applications like online recommendation and LLM alignment. Neural-ADB addresses this gap.", "method": "Neural-ADB uses a neural contextual dueling bandit framework to handle non-linear reward functions, validated under the Bradley-Terry-Luce model.", "result": "Theoretical and experimental results show Neural-ADB's policy sub-optimality gap decreases sub-linearly with dataset growth.", "conclusion": "Neural-ADB provides a practical and principled solution for non-linear reward scenarios, validated by synthetic datasets."}}
{"id": "2504.12229", "pdf": "https://arxiv.org/pdf/2504.12229", "abs": "https://arxiv.org/abs/2504.12229", "authors": ["David Khachaturov", "Robert Mullins", "Ilia Shumailov", "Sumanth Dathathri"], "title": "Watermarking Needs Input Repetition Masking", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.", "AI": {"tldr": "The paper explores mimicry in text, where humans and LLMs unintentionally imitate synthetic text properties, challenging current watermarking and detection methods.", "motivation": "Concerns about LLM misuse (e.g., misinformation) led to countermeasures like detectors and watermarking, but unintentional mimicry by humans or unwatermarked LLMs may undermine these efforts.", "method": "Investigates conversational adaptation (mimicry) by humans and LLMs, including watermarking signals, in various settings.", "result": "Demonstrates that mimicry occurs, challenging assumptions and suggesting the need for lower false positives and longer word sequences in watermarking.", "conclusion": "Current watermarking methods may be unreliable due to mimicry; improvements are needed for long-term reliability."}}
{"id": "2504.11879", "pdf": "https://arxiv.org/pdf/2504.11879", "abs": "https://arxiv.org/abs/2504.11879", "authors": ["Yushuai Sun", "Zikun Zhou", "Dongmei Jiang", "Yaowei Wang", "Jun Yu", "Guangming Lu", "Wenjie Pei"], "title": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Asymmetric retrieval is a typical scenario in real-world retrieval systems,\nwhere compatible models of varying capacities are deployed on platforms with\ndifferent resource configurations. Existing methods generally train pre-defined\nnetworks or subnetworks with capacities specifically designed for\npre-determined platforms, using compatible learning. Nevertheless, these\nmethods suffer from limited flexibility for multi-platform deployment. For\nexample, when introducing a new platform into the retrieval systems, developers\nhave to train an additional model at an appropriate capacity that is compatible\nwith existing models via backward-compatible learning. In this paper, we\npropose a Prunable Network with self-compatibility, which allows developers to\ngenerate compatible subnetworks at any desired capacity through post-training\npruning. Thus it allows the creation of a sparse subnetwork matching the\nresources of the new platform without additional training. Specifically, we\noptimize both the architecture and weight of subnetworks at different\ncapacities within a dense network in compatible learning. We also design a\nconflict-aware gradient integration scheme to handle the gradient conflicts\nbetween the dense network and subnetworks during compatible learning. Extensive\nexperiments on diverse benchmarks and visual backbones demonstrate the\neffectiveness of our method. Our code and model are available at\nhttps://github.com/Bunny-Black/PrunNet.", "AI": {"tldr": "The paper introduces a Prunable Network with self-compatibility for flexible multi-platform deployment in asymmetric retrieval systems, eliminating the need for additional training when introducing new platforms.", "motivation": "Existing methods lack flexibility for multi-platform deployment, requiring additional training for new platforms. The paper aims to address this by enabling post-training pruning for compatible subnetworks.", "method": "The authors propose a Prunable Network optimized for architecture and weight of subnetworks at varying capacities. They introduce a conflict-aware gradient integration scheme to manage gradient conflicts during compatible learning.", "result": "Experiments on diverse benchmarks and visual backbones validate the method's effectiveness in generating compatible subnetworks without extra training.", "conclusion": "The Prunable Network offers a scalable solution for asymmetric retrieval systems, simplifying deployment across platforms with varying resource configurations."}}
{"id": "2504.12025", "pdf": "https://arxiv.org/pdf/2504.12025", "abs": "https://arxiv.org/abs/2504.12025", "authors": ["Yu Zhang", "Qingfeng Du", "Jiaqi Lv"], "title": "FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables decentralized model training across multiple\nparties while preserving privacy. However, most FL systems assume clients hold\nonly unimodal data, limiting their real-world applicability, as institutions\noften possess multimodal data. Moreover, the lack of labeled data further\nconstrains the performance of most FL methods. In this work, we propose FedEPA,\na novel FL framework for multimodal learning. FedEPA employs a personalized\nlocal model aggregation strategy that leverages labeled data on clients to\nlearn personalized aggregation weights, thereby alleviating the impact of data\nheterogeneity. We also propose an unsupervised modality alignment strategy that\nworks effectively with limited labeled data. Specifically, we decompose\nmultimodal features into aligned features and context features. We then employ\ncontrastive learning to align the aligned features across modalities, ensure\nthe independence between aligned features and context features within each\nmodality, and promote the diversity of context features. A multimodal feature\nfusion strategy is introduced to obtain a joint embedding. The experimental\nresults show that FedEPA significantly outperforms existing FL methods in\nmultimodal classification tasks under limited labeled data conditions.", "AI": {"tldr": "FedEPA is a novel FL framework for multimodal learning, addressing data heterogeneity and limited labeled data via personalized aggregation and unsupervised modality alignment.", "motivation": "Existing FL systems often assume unimodal client data and struggle with limited labeled data, limiting real-world applicability.", "method": "FedEPA uses personalized local model aggregation and unsupervised modality alignment, including contrastive learning and multimodal feature fusion.", "result": "FedEPA outperforms existing FL methods in multimodal classification tasks with limited labeled data.", "conclusion": "FedEPA effectively addresses challenges in multimodal FL, improving performance under data constraints."}}
{"id": "2504.12254", "pdf": "https://arxiv.org/pdf/2504.12254", "abs": "https://arxiv.org/abs/2504.12254", "authors": ["Mahmoud Salhab", "Marwan Elghitany", "Shameed Sait", "Syed Sibghat Ullah", "Mohammad Abusheikh", "Hasan Abusheikh"], "title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach attains state-of-the-art (SOTA)\nperformance, exceeding all previous efforts in the field of Arabic ASR on the\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.", "AI": {"tldr": "The paper presents a weakly supervised learning approach to train an Arabic ASR model using the Conformer architecture, achieving SOTA performance without costly manual transcriptions.", "motivation": "Developing high-performance ASR models for low-resource languages like Arabic is challenging due to the scarcity of labeled datasets.", "method": "The model is trained from scratch on 15,000 hours of weakly annotated speech data (MSA and DA) using the Conformer architecture.", "result": "The approach achieves state-of-the-art performance on standard benchmarks despite no human-verified labels.", "conclusion": "Weak supervision is a scalable, cost-efficient alternative to traditional supervised methods, enabling improved ASR in low-resource settings."}}
{"id": "2504.11893", "pdf": "https://arxiv.org/pdf/2504.11893", "abs": "https://arxiv.org/abs/2504.11893", "authors": ["Wei Sun", "Yanzhao Zhou", "Jianbin Jiao", "Yuan Li"], "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary 3D scene understanding is crucial for applications requiring\nnatural language-driven spatial interpretation, such as robotics and augmented\nreality. While 3D Gaussian Splatting (3DGS) offers a powerful representation\nfor scene reconstruction, integrating it with open-vocabulary frameworks\nreveals a key challenge: cross-view granularity inconsistency. This issue,\nstemming from 2D segmentation methods like SAM, results in inconsistent object\nsegmentations across views (e.g., a \"coffee set\" segmented as a single entity\nin one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based\nmethods often rely on isolated per-Gaussian feature learning, neglecting the\nspatial context needed for cohesive object reasoning, leading to fragmented\nrepresentations. We propose Context-Aware Gaussian Splatting (CAGS), a novel\nframework that addresses this challenge by incorporating spatial context into\n3DGS. CAGS constructs local graphs to propagate contextual features across\nGaussians, reducing noise from inconsistent granularity, employs mask-centric\ncontrastive learning to smooth SAM-derived features across views, and leverages\na precomputation strategy to reduce computational cost by precomputing\nneighborhood relationships, enabling efficient training in large-scale scenes.\nBy integrating spatial context, CAGS significantly improves 3D instance\nsegmentation and reduces fragmentation errors on datasets like LERF-OVS and\nScanNet, enabling robust language-guided 3D scene understanding.", "AI": {"tldr": "CAGS improves 3D scene understanding by integrating spatial context into 3D Gaussian Splatting, addressing cross-view granularity inconsistency and fragmentation errors.", "motivation": "Open-vocabulary 3D scene understanding is essential for robotics and AR, but current methods suffer from inconsistent object segmentations and fragmented representations due to isolated feature learning.", "method": "CAGS incorporates spatial context via local graphs, mask-centric contrastive learning, and a precomputation strategy for efficient training.", "result": "CAGS enhances 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet.", "conclusion": "CAGS enables robust language-guided 3D scene understanding by addressing key challenges in cross-view granularity and spatial context integration."}}
{"id": "2504.12075", "pdf": "https://arxiv.org/pdf/2504.12075", "abs": "https://arxiv.org/abs/2504.12075", "authors": ["Kiran K. Yalamanchi", "Pinaki Pal", "Balaji Mohan", "Abdullah S. AlRamadan", "Jihad A. Badra", "Yuanjiang Pei"], "title": "Generative Deep Learning Framework for Inverse Design of Fuels", "categories": ["cs.LG", "physics.chem-ph"], "comment": null, "summary": "In the present work, a generative deep learning framework combining a\nCo-optimized Variational Autoencoder (Co-VAE) architecture with quantitative\nstructure-property relationship (QSPR) techniques is developed to enable\naccelerated inverse design of fuels. The Co-VAE integrates a property\nprediction component coupled with the VAE latent space, enhancing molecular\nreconstruction and accurate estimation of Research Octane Number (RON) (chosen\nas the fuel property of interest). A subset of the GDB-13 database, enriched\nwith a curated RON database, is used for model training. Hyperparameter tuning\nis further utilized to optimize the balance among reconstruction fidelity,\nchemical validity, and RON prediction. An independent regression model is then\nused to refine RON prediction, while a differential evolution algorithm is\nemployed to efficiently navigate the VAE latent space and identify promising\nfuel molecule candidates with high RON. This methodology addresses the\nlimitations of traditional fuel screening approaches by capturing complex\nstructure-property relationships within a comprehensive latent representation.\nThe generative model provides a flexible tool for systematically exploring vast\nchemical spaces, paving the way for discovering fuels with superior anti-knock\nproperties. The demonstrated approach can be readily extended to incorporate\nadditional fuel properties and synthesizability criteria to enhance\napplicability and reliability for de novo design of new fuels.", "AI": {"tldr": "A generative deep learning framework (Co-VAE) combined with QSPR techniques accelerates inverse fuel design, optimizing molecular reconstruction and RON prediction.", "motivation": "To overcome limitations of traditional fuel screening by capturing complex structure-property relationships for discovering high-RON fuels.", "method": "Co-VAE integrates property prediction with VAE latent space, uses GDB-13 and RON data, hyperparameter tuning, regression refinement, and differential evolution for candidate identification.", "result": "The model efficiently explores chemical space, identifying high-RON fuel candidates and improving reconstruction and prediction accuracy.", "conclusion": "The framework is adaptable for additional fuel properties and synthesizability, enhancing de novo fuel design."}}
{"id": "2504.11895", "pdf": "https://arxiv.org/pdf/2504.11895", "abs": "https://arxiv.org/abs/2504.11895", "authors": ["Qishan Wang", "Jia Guo", "Shuyong Gao", "Haofen Wang", "Li Xiong", "Junjie Hu", "Hanqi Guo", "Wenqiang Zhang"], "title": "Search is All You Need for Few-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging\ntask in industrial inspection, where normal distribution modeling must be\naccomplished with only a few normal images. While existing approaches typically\nemploy multi-modal foundation models combining language and vision modalities\nfor prompt-guided anomaly detection, these methods often demand sophisticated\nprompt engineering and extensive manual tuning. In this paper, we demonstrate\nthat a straightforward nearest-neighbor search framework can surpass\nstate-of-the-art performance in both single-class and multi-class FSAD\nscenarios. Our proposed method, VisionAD, consists of four simple yet essential\ncomponents: (1) scalable vision foundation models that extract universal and\ndiscriminative features; (2) dual augmentation strategies - support\naugmentation to enhance feature matching adaptability and query augmentation to\naddress the oversights of single-view prediction; (3) multi-layer feature\nintegration that captures both low-frequency global context and high-frequency\nlocal details with minimal computational overhead; and (4) a class-aware visual\nmemory bank enabling efficient one-for-all multi-class detection. Extensive\nevaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate\nVisionAD's exceptional performance. Using only 1 normal images as support, our\nmethod achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%\nrespectively, outperforming current state-of-the-art approaches by significant\nmargins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior\nfew-shot capabilities of VisionAD make it particularly appealing for real-world\napplications where samples are scarce or expensive to obtain. Code is available\nat https://github.com/Qiqigeww/VisionAD.", "AI": {"tldr": "VisionAD, a simple nearest-neighbor framework, outperforms state-of-the-art FSAD methods with minimal training and superior few-shot capabilities.", "motivation": "Addressing the challenges of few-shot anomaly detection in industrial inspection, where existing methods require complex prompt engineering and tuning.", "method": "VisionAD uses scalable vision models, dual augmentation, multi-layer feature integration, and a class-aware memory bank for efficient anomaly detection.", "result": "Achieves AUROC scores of 97.4%, 94.8%, and 70.8% on MVTec-AD, VisA, and Real-IAD benchmarks, outperforming competitors by +1.6%, +3.2%, and +1.4%.", "conclusion": "VisionAD's training-free approach and high performance make it ideal for real-world applications with scarce data."}}
{"id": "2504.12086", "pdf": "https://arxiv.org/pdf/2504.12086", "abs": "https://arxiv.org/abs/2504.12086", "authors": ["Mohammadali Moghimi", "Sharu Theresa Jose", "Shana Moothedath"], "title": "Neural Contextual Bandits Under Delayed Feedback Constraints", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a new algorithm for neural contextual bandits (CBs) that\naddresses the challenge of delayed reward feedback, where the reward for a\nchosen action is revealed after a random, unknown delay. This scenario is\ncommon in applications such as online recommendation systems and clinical\ntrials, where reward feedback is delayed because the outcomes or results of a\nuser's actions (such as recommendations or treatment responses) take time to\nmanifest and be measured. The proposed algorithm, called Delayed NeuralUCB,\nuses an upper confidence bound (UCB)-based exploration strategy. Under the\nassumption of independent and identically distributed sub-exponential reward\ndelays, we derive an upper bound on the cumulative regret over a T-length\nhorizon. We further consider a variant of the algorithm, called Delayed\nNeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments\non real-world datasets, such as MNIST and Mushroom, along with comparisons to\nbenchmark approaches, demonstrate that the proposed algorithms effectively\nmanage varying delays and are well-suited for complex real-world scenarios.", "AI": {"tldr": "A new algorithm, Delayed NeuralUCB, addresses delayed reward feedback in neural contextual bandits, with a variant, Delayed NeuralTS, using Thompson Sampling. Both show effectiveness in real-world scenarios.", "motivation": "Delayed reward feedback is common in applications like online recommendations and clinical trials, where outcomes take time to manifest. Existing methods struggle with such delays.", "method": "The proposed Delayed NeuralUCB uses UCB-based exploration, while Delayed NeuralTS employs Thompson Sampling. Both handle random, unknown delays in reward feedback.", "result": "The algorithms achieve bounded cumulative regret under sub-exponential delay assumptions and perform well on real-world datasets like MNIST and Mushroom.", "conclusion": "The proposed algorithms effectively manage delayed feedback, making them suitable for complex real-world applications."}}
{"id": "2504.11896", "pdf": "https://arxiv.org/pdf/2504.11896", "abs": "https://arxiv.org/abs/2504.11896", "authors": ["Xingxing Yang", "Jie Chen", "Zaifeng Yang"], "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets.", "AI": {"tldr": "A novel low-light image enhancement method, PiCat, uses physics-informed priors and a Color-aware Transform (CAT) to handle lighting variations, combined with a Content-Noise Decomposition Network (CNDN) for noise reduction, outperforming existing methods.", "motivation": "Existing methods struggle with inconsistent color predictions and sensitivity to lighting variations, leading to unstable performance in diverse conditions.", "method": "Proposes PiCat, combining CAT for illumination-invariant descriptors and CNDN for noise reduction, leveraging physics-informed priors for robust enhancement.", "result": "PiCat outperforms state-of-the-art methods on five benchmark datasets.", "conclusion": "The PiCat framework effectively addresses challenges in low-light image enhancement by integrating physics-informed priors and noise reduction, achieving superior performance."}}
{"id": "2504.12151", "pdf": "https://arxiv.org/pdf/2504.12151", "abs": "https://arxiv.org/abs/2504.12151", "authors": ["Miaosen Luo", "Yuncheng Jiang", "Sijie Mai"], "title": "Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack\nof interpretability in the decision logic of multimodal fusion and modality\nimbalance caused by disparities in inter-modal information density. To address\nthese issues, we propose KAN-MCP, a novel framework that integrates the\ninterpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the\nMultimodal Clean Pareto (MCPareto) framework. First, KAN leverages its\nunivariate function decomposition to achieve transparent analysis of\ncross-modal interactions. This structural design allows direct inspection of\nfeature transformations without relying on external interpretation tools,\nthereby ensuring both high expressiveness and interpretability. Second, the\nproposed MCPareto enhances robustness by addressing modality imbalance and\nnoise interference. Specifically, we introduce the Dimensionality Reduction and\nDenoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises\nand reduces feature dimensionality. This approach provides KAN with\ndiscriminative low-dimensional inputs to reduce the modeling complexity of KAN\nwhile preserving critical sentiment-related information. Furthermore, MCPareto\ndynamically balances gradient contributions across modalities using the\npurified features output by DRD-MIB, ensuring lossless transmission of\nauxiliary signals and effectively alleviating modality imbalance. This synergy\nof interpretability and robustness not only achieves superior performance on\nbenchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers\nan intuitive visualization interface through KAN's interpretable architecture.", "AI": {"tldr": "KAN-MCP integrates interpretable Kolmogorov-Arnold Networks (KAN) with the robust Multimodal Clean Pareto (MCPareto) framework to address interpretability and modality imbalance in Multimodal Sentiment Analysis (MSA).", "motivation": "The challenges of interpretability in multimodal fusion and modality imbalance due to inter-modal information disparities drive the need for a transparent and robust solution.", "method": "KAN provides interpretable analysis of cross-modal interactions, while MCPareto enhances robustness via DRD-MIB for denoising and dimensionality reduction, and dynamic gradient balancing.", "result": "Superior performance on benchmark datasets (CMU-MOSI, CMU-MOSEI, CH-SIMS v2) with intuitive visualization through KAN's interpretable architecture.", "conclusion": "KAN-MCP successfully combines interpretability and robustness, offering a high-performing and transparent solution for MSA."}}
{"id": "2504.11914", "pdf": "https://arxiv.org/pdf/2504.11914", "abs": "https://arxiv.org/abs/2504.11914", "authors": ["Yuhao Chao", "Jie Liu", "Jie Tang", "Gangshan Wu"], "title": "AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Industrial Anomaly Detection (IAD) poses a formidable challenge due to the\nscarcity of defective samples, making it imperative to deploy models capable of\nrobust generalization to detect unseen anomalies effectively. Traditional\napproaches, often constrained by hand-crafted features or domain-specific\nexpert models, struggle to address this limitation, underscoring the need for a\nparadigm shift. We introduce AnomalyR1, a pioneering framework that leverages\nVLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional\ngeneralization and interpretability, to revolutionize IAD. By integrating MLLM\nwith Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned\nOutcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution\nthat autonomously processes inputs of image and domain knowledge, reasons\nthrough analysis, and generates precise anomaly localizations and masks. Based\non the latest multimodal IAD benchmark, our compact 3-billion-parameter model\noutperforms existing methods, establishing state-of-the-art results. As MLLM\ncapabilities continue to advance, this study is the first to deliver an\nend-to-end VLM-based IAD solution that demonstrates the transformative\npotential of ROAM-enhanced GRPO, positioning our framework as a forward-looking\ncornerstone for next-generation intelligent anomaly detection systems in\nindustrial applications with limited defective data.", "AI": {"tldr": "AnomalyR1 leverages VLM-R1 and GRPO with ROAM for end-to-end industrial anomaly detection, outperforming existing methods with limited defective data.", "motivation": "Addressing the challenge of scarce defective samples in IAD, traditional methods' limitations necessitate a paradigm shift.", "method": "Integrates VLM-R1 (MLLM) with GRPO and ROAM for autonomous anomaly localization and masking.", "result": "Achieves state-of-the-art performance on multimodal IAD benchmarks with a compact 3B-parameter model.", "conclusion": "Demonstrates the transformative potential of MLLM-based IAD, positioning AnomalyR1 as a cornerstone for future systems."}}
{"id": "2504.12156", "pdf": "https://arxiv.org/pdf/2504.12156", "abs": "https://arxiv.org/abs/2504.12156", "authors": ["Mustafa Cavus"], "title": "Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In many applications, especially those involving prediction, models may yield\nnear-optimal performance yet significantly disagree on individual-level\noutcomes. This phenomenon, known as predictive multiplicity, has been formally\ndefined in binary, probabilistic, and multi-target classification, and\nundermines the reliability of predictive systems. However, its implications\nremain unexplored in the context of survival analysis, which involves\nestimating the time until a failure or similar event while properly handling\ncensored data. We frame predictive multiplicity as a critical concern in\nsurvival-based models and introduce formal measures -- ambiguity, discrepancy,\nand obscurity -- to quantify it. This is particularly relevant for downstream\ntasks such as maintenance scheduling, where precise individual risk estimates\nare essential. Understanding and reporting predictive multiplicity helps build\ntrust in models deployed in high-stakes environments. We apply our methodology\nto benchmark datasets from predictive maintenance, extending the notion of\nmultiplicity to survival models. Our findings show that ambiguity steadily\nincreases, reaching up to 40-45% of observations; discrepancy is lower but\nexhibits a similar trend; and obscurity remains mild and concentrated in a few\nmodels. These results demonstrate that multiple accurate survival models may\nyield conflicting estimations of failure risk and degradation progression for\nthe same equipment. This highlights the need to explicitly measure and\ncommunicate predictive multiplicity to ensure reliable decision-making in\nprocess health management.", "AI": {"tldr": "The paper explores predictive multiplicity in survival analysis, introducing measures to quantify it and demonstrating its impact on model reliability in high-stakes applications like predictive maintenance.", "motivation": "Predictive multiplicity undermines model reliability, but its implications in survival analysis, crucial for tasks like maintenance scheduling, remain unexplored.", "method": "The authors define formal measures (ambiguity, discrepancy, obscurity) to quantify predictive multiplicity in survival models and apply them to benchmark datasets.", "result": "Findings show ambiguity reaches 40-45%, discrepancy follows a similar trend, and obscurity is mild but concentrated, indicating conflicting risk estimates across models.", "conclusion": "Predictive multiplicity must be measured and communicated to ensure reliable decision-making in survival-based applications."}}
{"id": "2504.11922", "pdf": "https://arxiv.org/pdf/2504.11922", "abs": "https://arxiv.org/abs/2504.11922", "authors": ["Lvpan Cai", "Haowei Wang", "Jiayi Ji", "YanShu ZhouMen", "Yiwei Ma", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach", "categories": ["cs.CV"], "comment": null, "summary": "The rise of AI-generated image editing tools has made localized forgeries\nincreasingly realistic, posing challenges for visual content integrity.\nAlthough recent efforts have explored localized AIGC detection, existing\ndatasets predominantly focus on object-level forgeries while overlooking\nbroader scene edits in regions such as sky or ground. To address these\nlimitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000\nlocally forged images with diverse scene-aware annotations, which are based on\nsemantic calibration to ensure high-quality samples. BR-Gen is constructed\nthrough a fully automated Perception-Creation-Evaluation pipeline to ensure\nsemantic coherence and visual realism. In addition, we further propose\n\\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that\nenhances the detection of localized forgeries by amplifying forgery-related\nfeatures across the entire image. NFA-ViT mines heterogeneous regions in\nimages, \\emph{i.e.}, potential edited areas, by noise fingerprints.\nSubsequently, attention mechanism is introduced to compel the interaction\nbetween normal and abnormal features, thereby propagating the generalization\ntraces throughout the entire image, allowing subtle forgeries to influence a\nbroader context and improving overall detection robustness. Extensive\nexperiments demonstrate that BR-Gen constructs entirely new scenarios that are\nnot covered by existing methods. Take a step further, NFA-ViT outperforms\nexisting methods on BR-Gen and generalizes well across current benchmarks. All\ndata and codes are available at https://github.com/clpbc/BR-Gen.", "AI": {"tldr": "The paper introduces BR-Gen, a dataset for detecting localized AI-generated image forgeries, and NFA-ViT, a transformer-based method to improve detection robustness.", "motivation": "Addressing the gap in datasets and methods for detecting scene-level AI-generated forgeries, which are often overlooked in favor of object-level forgeries.", "method": "BR-Gen is created via an automated pipeline for semantic coherence. NFA-ViT uses noise fingerprints and attention mechanisms to detect forgeries.", "result": "BR-Gen covers new scenarios, and NFA-ViT outperforms existing methods on it and other benchmarks.", "conclusion": "The work advances localized forgery detection with a novel dataset and method, demonstrating superior performance and generalization."}}
{"id": "2504.12181", "pdf": "https://arxiv.org/pdf/2504.12181", "abs": "https://arxiv.org/abs/2504.12181", "authors": ["Eunjeong Jeong", "Nikolaos Pappas"], "title": "Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "This paper is currently under review for presentation at a\n  peer-reviewed conference", "summary": "Federated Learning (FL) has emerged as a promising framework for distributed\nlearning, but its growing complexity has led to significant energy consumption,\nparticularly from computations on the client side. This challenge is especially\ncritical in energy-harvesting FL (EHFL) systems, where device availability\nfluctuates due to limited and time-varying energy resources. We propose\nFedBacys, a battery-aware FL framework that introduces cyclic client\nparticipation based on users' battery levels to cope with these issues.\nFedBacys enables clients to save energy and strategically perform local\ntraining just before their designated transmission time by clustering clients\nand scheduling their involvement sequentially. This design minimizes redundant\ncomputation, reduces system-wide energy usage, and improves learning stability.\nOur experiments demonstrate that FedBacys outperforms existing approaches in\nterms of energy efficiency and performance consistency, exhibiting robustness\neven under non-i.i.d. training data distributions and with very infrequent\nbattery charging. This work presents the first comprehensive evaluation of\ncyclic client participation in EHFL, incorporating both communication and\ncomputation costs into a unified, resource-aware scheduling strategy.", "AI": {"tldr": "FedBacys is a battery-aware FL framework that optimizes energy use by scheduling client participation based on battery levels, reducing redundant computations and improving stability.", "motivation": "Addressing high energy consumption in FL, especially in energy-harvesting systems with fluctuating device availability.", "method": "Cyclic client participation based on battery levels, clustering clients, and scheduling training before transmission to minimize energy use.", "result": "Outperforms existing methods in energy efficiency and performance consistency, even with non-i.i.d. data and infrequent charging.", "conclusion": "FedBacys offers a robust, resource-aware scheduling strategy for EHFL, balancing communication and computation costs."}}
{"id": "2504.11930", "pdf": "https://arxiv.org/pdf/2504.11930", "abs": "https://arxiv.org/abs/2504.11930", "authors": ["Hairui Ren", "Fan Tang", "He Zhao", "Zixuan Wang", "Dandan Guo", "Yi Chang"], "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning", "categories": ["cs.CV"], "comment": null, "summary": "Fine-tuning vision-language models (VLMs) with large amounts of unlabeled\ndata has recently garnered significant interest. However, a key challenge\nremains the lack of high-quality pseudo-labeled data. Current pseudo-labeling\nstrategies often struggle with mismatches between semantic and visual\ninformation, leading to sub-optimal performance of unsupervised prompt learning\n(UPL) methods. In this paper, we introduce a simple yet effective approach\ncalled \\textbf{A}ugmenting D\\textbf{i}scriminative \\textbf{R}ichness via\nDiffusions (AiR), toward learning a richer discriminating way to represent the\nclass comprehensively and thus facilitate classification. Specifically, our\napproach includes a pseudo-label generation module that leverages high-fidelity\nsynthetic samples to create an auxiliary classifier, which captures richer\nvisual variation, bridging text-image-pair classification to a more robust\nimage-image-pair classification. Additionally, we exploit the diversity of\ndiffusion-based synthetic samples to enhance prompt learning, providing greater\ninformation for semantic-visual alignment. Extensive experiments on five public\nbenchmarks, including RESISC45 and Flowers102, and across three learning\nparadigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and\nconsistent performance improvements over state-of-the-art unsupervised prompt\nlearning methods.", "AI": {"tldr": "AiR introduces a method to improve unsupervised prompt learning in VLMs by using high-fidelity synthetic samples for pseudo-labeling and enhancing semantic-visual alignment.", "motivation": "The lack of high-quality pseudo-labeled data and mismatches between semantic and visual information hinder unsupervised prompt learning in VLMs.", "method": "AiR leverages diffusion-based synthetic samples to create an auxiliary classifier, enriching visual variation and improving classification. It also enhances prompt learning by exploiting synthetic sample diversity.", "result": "AiR outperforms state-of-the-art methods on five benchmarks, including RESISC45 and Flowers102, across three learning paradigms.", "conclusion": "AiR effectively addresses pseudo-labeling challenges and improves unsupervised prompt learning performance in VLMs."}}
{"id": "2504.12262", "pdf": "https://arxiv.org/pdf/2504.12262", "abs": "https://arxiv.org/abs/2504.12262", "authors": ["David Keetae Park", "Xihaier Luo", "Guang Zhao", "Seungjun Lee", "Miruna Oprescu", "Shinjae Yoo"], "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 5 main figures, 3 tables, under review", "summary": "Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability.", "AI": {"tldr": "SCENT is a novel transformer-based framework for scalable spatiotemporal learning, unifying interpolation, reconstruction, and forecasting, with superior performance and scalability.", "motivation": "Addressing challenges in spatiotemporal learning, such as high dimensionality, irregular data, and scalability constraints, particularly in scientific domains.", "method": "SCENT uses a transformer-based encoder-processor-decoder with learnable queries, query-wise cross-attention, and sparse attention for scalability.", "result": "Demonstrates state-of-the-art performance in simulations and real-world experiments, with efficient evaluation at arbitrary resolutions.", "conclusion": "SCENT effectively tackles spatiotemporal learning challenges, offering a scalable and high-performing solution."}}
{"id": "2504.11946", "pdf": "https://arxiv.org/pdf/2504.11946", "abs": "https://arxiv.org/abs/2504.11946", "authors": ["Haoyang Wang", "Liming Liu", "Peiheng Wang", "Junlin Hao", "Jiangkai Wu", "Xinggong Zhang"], "title": "R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Mesh reconstruction from multi-view images is a fundamental problem in\ncomputer vision, but its performance degrades significantly under sparse-view\nconditions, especially in unseen regions where no ground-truth observations are\navailable. While recent advances in diffusion models have demonstrated strong\ncapabilities in synthesizing novel views from limited inputs, their outputs\noften suffer from visual artifacts and lack 3D consistency, posing challenges\nfor reliable mesh optimization. In this paper, we propose a novel framework\nthat leverages diffusion models to enhance sparse-view mesh reconstruction in a\nprincipled and reliable manner. To address the instability of diffusion\noutputs, we propose a Consensus Diffusion Module that filters unreliable\ngenerations via interquartile range (IQR) analysis and performs variance-aware\nimage fusion to produce robust pseudo-supervision. Building on this, we design\nan online reinforcement learning strategy based on the Upper Confidence Bound\n(UCB) to adaptively select the most informative viewpoints for enhancement,\nguided by diffusion loss. Finally, the fused images are used to jointly\nsupervise a NeRF-based model alongside sparse-view ground truth, ensuring\nconsistency across both geometry and appearance. Extensive experiments\ndemonstrate that our method achieves significant improvements in both geometric\nquality and rendering quality.", "AI": {"tldr": "A novel framework enhances sparse-view mesh reconstruction using diffusion models, addressing instability with a Consensus Diffusion Module and adaptive viewpoint selection via reinforcement learning.", "motivation": "Performance of mesh reconstruction degrades under sparse-view conditions, especially in unseen regions, and diffusion models often produce inconsistent outputs.", "method": "Proposes a Consensus Diffusion Module for reliable pseudo-supervision and an online reinforcement learning strategy for adaptive viewpoint selection, combined with NeRF-based supervision.", "result": "Significant improvements in geometric and rendering quality are demonstrated through extensive experiments.", "conclusion": "The framework effectively leverages diffusion models for robust and consistent sparse-view mesh reconstruction."}}
{"id": "2504.12270", "pdf": "https://arxiv.org/pdf/2504.12270", "abs": "https://arxiv.org/abs/2504.12270", "authors": ["ChenNingZhi Sheng", "Rafal Kustra", "Davide Chicco"], "title": "Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)", "categories": ["cs.LG", "stat.AP", "62H30"], "comment": "22 pages,3 figures,5 tables", "summary": "Purpose: The primary goal of this study is to explore the application of\nevaluation metrics to different clustering algorithms using the data provided\nfrom the Canadian Longitudinal Study (CLSA), focusing on cognitive features.\nThe objective of our work is to discover potential clinically relevant clusters\nthat contribute to the development of dementia over time-based on cognitive\nchanges. Method: The CLSA dataset includes 18,891 participants with data\navailable at both baseline and follow-up assessments, to which clustering\nalgorithms were applied. The clustering methodologies employed in this analysis\nare K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning\nAround Medoids (PAM). We use multiple evaluation metrics to assess our\nanalysis. For internal evaluation metrics, we use: Average silhouette Width,\nWithin and Between the sum of square Ratio (WB.Ratio), Entropy,\nCalinski-Harabasz Index (CH Index), and Separation Index. For clustering\ncomparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index\n(ARI), Rand Index (RI), and Variation Information. Results: Using evaluation\nmetrics to compare the results of the three clustering techniques, K-means and\nPartitioning Around Medoids (PAM) produced similar results. In contrast, there\nare significant differences between K-means clustering and Hierarchical\nClustering. Our study highlights the importance of the two internal evaluation\nmetrics: entropy and separation index. In between clustering comparison\nmetrics, the Adjusted Rand Index is a key tool. Conclusion: The study results\nhave the potential to contribute to understanding dementia. Researchers can\nalso benefit by applying the suggested evaluation metrics to other areas of\nhealthcare research. Overall, our study improves the understanding of using\nclustering techniques and evaluation metrics to reveal complex patterns in\nmedical data.", "AI": {"tldr": "The study evaluates clustering algorithms (K-means, Hierarchical Clustering, PAM) on CLSA cognitive data to identify dementia-related clusters, using various metrics. K-means and PAM performed similarly, while Hierarchical Clustering differed. Key metrics include entropy, separation index, and Adjusted Rand Index.", "motivation": "To identify clinically relevant clusters in cognitive data for dementia research and improve understanding of clustering techniques in healthcare.", "method": "Applied K-means, Hierarchical Clustering, and PAM to CLSA data (18,891 participants). Evaluated using internal (e.g., silhouette width, entropy) and comparison metrics (e.g., ARI).", "result": "K-means and PAM yielded similar results, differing from Hierarchical Clustering. Key metrics were entropy, separation index, and Adjusted Rand Index.", "conclusion": "The study aids dementia research and suggests applying these metrics in healthcare. It enhances understanding of clustering techniques for medical data."}}
{"id": "2504.11949", "pdf": "https://arxiv.org/pdf/2504.11949", "abs": "https://arxiv.org/abs/2504.11949", "authors": ["Jie Wang", "Chen Ye Gan", "Caoqi Wei", "Jiangtao Wen", "Yuxing Han"], "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments.", "AI": {"tldr": "Flow Intelligence introduces a motion-based approach for video feature matching, eliminating the need for spatial features or pretraining, and excels in robustness across diverse scenarios.", "motivation": "Traditional spatial feature matching fails with noisy, misaligned, or cross-modal data, and deep learning methods are limited by data and computational demands.", "method": "Extracts temporal motion signatures from pixel blocks across frames, focusing on motion patterns instead of spatial features.", "result": "Achieves invariance to translation, rotation, and scale, works cross-modally without pretraining, and outperforms traditional methods.", "conclusion": "Flow Intelligence enables robust, real-time video feature matching by leveraging motion, surpassing spatial-feature-based approaches."}}
{"id": "2504.11495", "pdf": "https://arxiv.org/pdf/2504.11495", "abs": "https://arxiv.org/abs/2504.11495", "authors": ["Yiting Wang", "Yunxin Fan", "Fei Liu"], "title": "Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Submitted to ICRA'25 Workshop of 3rd Robot-Assisted Medical Imaging", "summary": "Accurate modeling of tool-tissue interactions in robotic surgery requires\nprecise tracking of deformable tissues and integration of surgical domain\nknowledge. Traditional methods rely on labor-intensive annotations or rigid\nassumptions, limiting flexibility. We propose a framework combining sparse\nkeypoint tracking and probabilistic modeling that propagates expert-annotated\nlandmarks across endoscopic frames, even with large tissue deformations.\nClustered tissue keypoints enable dynamic local transformation construction via\nPCA, and tool poses, tracked similarly, are expressed relative to these frames.\nEmbedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)\nintegrates data-driven observations with labeled clinical expertise,\neffectively predicting relative tool-tissue poses and enhancing visual\nunderstanding of robotic surgical motions directly from video data.", "AI": {"tldr": "A framework combining sparse keypoint tracking and probabilistic modeling improves tool-tissue interaction modeling in robotic surgery by integrating expert annotations and dynamic tissue deformations.", "motivation": "Traditional methods for modeling tool-tissue interactions rely on rigid assumptions or labor-intensive annotations, limiting flexibility and accuracy.", "method": "The proposed framework uses sparse keypoint tracking, PCA for dynamic local transformations, and a Task-Parameterized Gaussian Mixture Model (TP-GMM) to integrate data-driven observations with clinical expertise.", "result": "The method effectively predicts relative tool-tissue poses and enhances visual understanding of robotic surgical motions directly from video data.", "conclusion": "The framework advances robotic surgery by combining expert knowledge with flexible, data-driven modeling of deformable tissues."}}
{"id": "2504.11967", "pdf": "https://arxiv.org/pdf/2504.11967", "abs": "https://arxiv.org/abs/2504.11967", "authors": ["Yifei Dong", "Fengyi Wu", "Sanjian Zhang", "Guangyu Chen", "Yuzhi Hu", "Masumi Yano", "Jingdong Sun", "Siyu Huang", "Feng Liu", "Qi Dai", "Zhi-Qi Cheng"], "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted at CVPR Workshop Anti-UAV 2025. 15 pages", "summary": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs.", "AI": {"tldr": "This survey examines anti-UAV technologies, focusing on classification, detection, and tracking, and evaluates state-of-the-art methods and benchmarks. It identifies gaps in real-time performance and swarm detection, urging innovation for next-gen defense systems.", "motivation": "UAVs are widely used but pose security risks, necessitating advanced anti-UAV solutions.", "method": "The survey reviews methodologies like diffusion-based synthesis, multi-modal fusion, and self-supervised learning, evaluating single and multi-sensor pipelines.", "result": "Gaps in real-time performance, stealth detection, and swarm scenarios are identified.", "conclusion": "The paper calls for robust, adaptive anti-UAV systems and highlights open research directions to guide future defense strategies."}}
{"id": "2504.11502", "pdf": "https://arxiv.org/pdf/2504.11502", "abs": "https://arxiv.org/abs/2504.11502", "authors": ["Jatin Nainani", "Chia-Tung Ho", "Anirudh Dhurka", "Haoxing Ren"], "title": "Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph", "categories": ["cs.SE", "cs.LG"], "comment": "7 pages, 7 figures, 2 tables", "summary": "Timing analysis is an essential and demanding verification method for Very\nLarge Scale Integrated (VLSI) circuit design and optimization. In addition, it\nalso serves as the cornerstone of the final sign-off, determining whether the\nchip is ready to be sent to the semiconductor foundry for fabrication.\nRecently, as the technology advance relentlessly, smaller metal pitches and the\nincreasing number of devices have led to greater challenges and longer\nturn-around-time for experienced human designers to debug timing issues from\nthe Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient\nand intelligent methodology is highly necessary and essential for debugging\ntiming issues and reduce the turnaround times. Recently, Large Language Models\n(LLMs) have shown great promise across various tasks in language understanding\nand interactive decision-making, incorporating reasoning and actions. In this\nwork, we propose a timing analysis agent, that is empowered by multi-LLMs task\nsolving, and incorporates a novel hierarchical planning and solving flow to\nautomate the analysis of timing reports from commercial tool. In addition, we\nbuild a Timing Debug Relation Graph (TDRG) that connects the reports with the\nrelationships of debug traces from experienced timing engineers. The timing\nanalysis agent employs the novel Agentic Retrieval Augmented Generation (RAG)\napproach, that includes agent and coding to retrieve data accurately, on the\ndeveloped TDRG. In our studies, the proposed timing analysis agent achieves an\naverage 98% pass-rate on a single-report benchmark and a 90% pass-rate for\nmulti-report benchmark from industrial designs, demonstrating its effectiveness\nand adaptability.", "AI": {"tldr": "A timing analysis agent using multi-LLMs and hierarchical planning automates VLSI timing report analysis, achieving high pass-rates (98% single-report, 90% multi-report).", "motivation": "Smaller metal pitches and more devices in VLSI design increase timing debug complexity, requiring efficient automation to reduce turnaround times.", "method": "Proposes a timing analysis agent with multi-LLMs, hierarchical planning, and a Timing Debug Relation Graph (TDRG) for automated report analysis.", "result": "Achieves 98% pass-rate for single-report and 90% for multi-report benchmarks in industrial designs.", "conclusion": "The agent effectively automates timing analysis, demonstrating adaptability and high accuracy in industrial settings."}}
{"id": "2504.11995", "pdf": "https://arxiv.org/pdf/2504.11995", "abs": "https://arxiv.org/abs/2504.11995", "authors": ["Rahima Khanam", "Muhammad Hussain"], "title": "A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions", "categories": ["cs.CV"], "comment": null, "summary": "The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources.", "AI": {"tldr": "YOLOv12 integrates attention mechanisms efficiently, improving real-time object detection with innovations like Area Attention and Residual Efficient Layer Aggregation Networks, while maintaining speed and accuracy.", "motivation": "To enhance YOLO's performance by incorporating attention mechanisms without compromising real-time capabilities, addressing computational overhead challenges.", "method": "Introduces Area Attention for efficient self-attention, Residual Efficient Layer Aggregation Networks for better feature aggregation, and FlashAttention for optimized memory access.", "result": "YOLOv12 outperforms prior YOLO versions and competitors in accuracy, inference speed, and computational efficiency.", "conclusion": "YOLOv12 advances real-time object detection by optimizing the latency-accuracy trade-off and computational resource usage."}}
{"id": "2504.11504", "pdf": "https://arxiv.org/pdf/2504.11504", "abs": "https://arxiv.org/abs/2504.11504", "authors": ["Woojin Kim", "Hyeoncheol Kim"], "title": "Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets", "categories": ["cs.CY", "cs.LG"], "comment": "12 pages, 6 figures, accepted to ITS2025", "summary": "As machine learning models are increasingly used in educational settings,\nfrom detecting at-risk students to predicting student performance, algorithmic\nbias and its potential impacts on students raise critical concerns about\nalgorithmic fairness. Although group fairness is widely explored in education,\nworks on individual fairness in a causal context are understudied, especially\non counterfactual fairness. This paper explores the notion of counterfactual\nfairness for educational data by conducting counterfactual fairness analysis of\nmachine learning models on benchmark educational datasets. We demonstrate that\ncounterfactual fairness provides meaningful insight into the causality of\nsensitive attributes and causal-based individual fairness in education.", "AI": {"tldr": "The paper explores counterfactual fairness in educational machine learning models, addressing algorithmic bias and its causal impacts on students.", "motivation": "Algorithmic bias in educational settings raises fairness concerns, especially regarding individual fairness in causal contexts like counterfactual fairness.", "method": "Counterfactual fairness analysis is conducted on benchmark educational datasets using machine learning models.", "result": "The study shows that counterfactual fairness offers insights into the causality of sensitive attributes and individual fairness in education.", "conclusion": "Counterfactual fairness is a valuable approach for understanding and addressing algorithmic bias in educational data."}}
{"id": "2504.11999", "pdf": "https://arxiv.org/pdf/2504.11999", "abs": "https://arxiv.org/abs/2504.11999", "authors": ["Mengyu Wang", "Hanbo Bi", "Yingchao Feng", "Linlin Xin", "Shuo Gong", "Tianqi Wang", "Zhiyuan Yan", "Peijin Wang", "Wenhui Diao", "Xian Sun"], "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Vision foundation models in remote sensing have been extensively studied due\nto their superior generalization on various downstream tasks. Synthetic\nAperture Radar (SAR) offers all-day, all-weather imaging capabilities,\nproviding significant advantages for Earth observation. However, establishing a\nfoundation model for SAR image interpretation inevitably encounters the\nchallenges of insufficient information utilization and poor interpretability.\nIn this paper, we propose a remote sensing foundation model based on\ncomplex-valued SAR data, which simulates the polarimetric decomposition process\nfor pre-training, i.e., characterizing pixel scattering intensity as a weighted\ncombination of scattering bases and scattering coefficients, thereby endowing\nthe foundation model with physical interpretability. Specifically, we construct\na series of scattering queries, each representing an independent and meaningful\nscattering basis, which interact with SAR features in the scattering query\ndecoder and output the corresponding scattering coefficient. To guide the\npre-training process, polarimetric decomposition loss and power\nself-supervision loss are constructed. The former aligns the predicted\ncoefficients with Yamaguchi coefficients, while the latter reconstructs power\nfrom the predicted coefficients and compares it to the input image's power. The\nperformance of our foundation model is validated on six typical downstream\ntasks, achieving state-of-the-art results. Notably, the foundation model can\nextract stable feature representations and exhibits strong generalization, even\nin data-scarce conditions.", "AI": {"tldr": "A foundation model for SAR image interpretation is proposed, using complex-valued data and polarimetric decomposition for pre-training, achieving state-of-the-art results on downstream tasks.", "motivation": "Addressing challenges of insufficient information utilization and poor interpretability in SAR image interpretation by leveraging polarimetric decomposition.", "method": "Constructs scattering queries to represent scattering bases, uses polarimetric decomposition loss and power self-supervision loss for pre-training.", "result": "Achieves state-of-the-art performance on six downstream tasks, with strong generalization even in data-scarce conditions.", "conclusion": "The foundation model provides interpretable and stable feature representations, demonstrating superior generalization for SAR image tasks."}}
{"id": "2504.11510", "pdf": "https://arxiv.org/pdf/2504.11510", "abs": "https://arxiv.org/abs/2504.11510", "authors": ["Xiaohua Feng", "Yuyuan Li", "Fengyuan Yu", "Ke Xiong", "Junjie Fang", "Li Zhang", "Tianyu Du", "Chaochao Chen"], "title": "RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.CR", "cs.CY", "cs.LG"], "comment": "17 pages", "summary": "In various networks and mobile applications, users are highly susceptible to\nattribute inference attacks, with particularly prevalent occurrences in\nrecommender systems. Attackers exploit partially exposed user profiles in\nrecommendation models, such as user embeddings, to infer private attributes of\ntarget users, such as gender and political views. The goal of defenders is to\nmitigate the effectiveness of these attacks while maintaining recommendation\nperformance. Most existing defense methods, such as differential privacy and\nattribute unlearning, focus on post-training settings, which limits their\ncapability of utilizing training data to preserve recommendation performance.\nAlthough adversarial training extends defenses to in-training settings, it\noften struggles with convergence due to unstable training processes. In this\npaper, we propose RAID, an in-training defense method against attribute\ninference attacks in recommender systems. In addition to the recommendation\nobjective, we define a defensive objective to ensure that the distribution of\nprotected attributes becomes independent of class labels, making users\nindistinguishable from attribute inference attacks. Specifically, this\ndefensive objective aims to solve a constrained Wasserstein barycenter problem\nto identify the centroid distribution that makes the attribute\nindistinguishable while complying with recommendation performance constraints.\nTo optimize our proposed objective, we use optimal transport to align users\nwith the centroid distribution. We conduct extensive experiments on four\nreal-world datasets to evaluate RAID. The experimental results validate the\neffectiveness of RAID and demonstrate its significant superiority over existing\nmethods in multiple aspects.", "AI": {"tldr": "RAID is an in-training defense method for recommender systems that protects against attribute inference attacks by making protected attributes indistinguishable while maintaining recommendation performance.", "motivation": "Attribute inference attacks exploit exposed user profiles in recommender systems, compromising privacy. Existing defenses are limited to post-training or suffer from unstable training. RAID addresses these gaps.", "method": "RAID defines a defensive objective to ensure protected attributes are independent of labels, solving a constrained Wasserstein barycenter problem using optimal transport to align users with a centroid distribution.", "result": "Experiments on four datasets show RAID effectively mitigates attacks and outperforms existing methods.", "conclusion": "RAID provides a robust in-training defense against attribute inference attacks without compromising recommendation quality."}}
{"id": "2504.12018", "pdf": "https://arxiv.org/pdf/2504.12018", "abs": "https://arxiv.org/abs/2504.12018", "authors": ["Xinli Yue", "JianHui Sun", "Junda Lu", "Liangchao Yao", "Fan Xia", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Yuetang Deng"], "title": "Instruction-augmented Multimodal Alignment for Image-Text and Element Matching", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop", "summary": "With the rapid advancement of text-to-image (T2I) generation models,\nassessing the semantic alignment between generated images and text descriptions\nhas become a significant research challenge. Current methods, including those\nbased on Visual Question Answering (VQA), still struggle with fine-grained\nassessments and precise quantification of image-text alignment. This paper\npresents an improved evaluation method named Instruction-augmented Multimodal\nAlignment for Image-Text and Element Matching (iMatch), which evaluates\nimage-text semantic alignment by fine-tuning multimodal large language models.\nWe introduce four innovative augmentation strategies: First, the QAlign\nstrategy creates a precise probabilistic mapping to convert discrete scores\nfrom multimodal large language models into continuous matching scores. Second,\na validation set augmentation strategy uses pseudo-labels from model\npredictions to expand training data, boosting the model's generalization\nperformance. Third, an element augmentation strategy integrates element\ncategory labels to refine the model's understanding of image-text matching.\nFourth, an image augmentation strategy employs techniques like random lighting\nto increase the model's robustness. Additionally, we propose prompt type\naugmentation and score perturbation strategies to further enhance the accuracy\nof element assessments. Our experimental results show that the iMatch method\nsignificantly surpasses existing methods, confirming its effectiveness and\npractical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025\nText to Image Generation Model Quality Assessment - Track 1 Image-Text\nAlignment.", "AI": {"tldr": "The paper introduces iMatch, an improved method for evaluating semantic alignment between text and generated images, using multimodal large language models and innovative augmentation strategies. It outperforms existing methods and won a CVPR competition.", "motivation": "Current methods for assessing image-text alignment, like VQA-based approaches, lack fine-grained precision. The need for a more accurate and robust evaluation method drives this research.", "method": "iMatch fine-tunes multimodal large language models with four augmentation strategies: QAlign for score conversion, validation set augmentation for data expansion, element augmentation for better matching, and image augmentation for robustness. Additional strategies like prompt type augmentation and score perturbation are also used.", "result": "iMatch significantly outperforms existing methods and won first place in the CVPR NTIRE 2025 competition for image-text alignment assessment.", "conclusion": "iMatch is an effective and practical solution for evaluating semantic alignment in text-to-image generation, validated by superior performance and competition success."}}
{"id": "2504.11516", "pdf": "https://arxiv.org/pdf/2504.11516", "abs": "https://arxiv.org/abs/2504.11516", "authors": ["Jiajun He", "Yuanqi Du", "Francisco Vargas", "Yuanqing Wang", "Carla P. Gomes", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Eric Vanden-Eijnden"], "title": "FEAT: Free energy Estimators with Adaptive Transport", "categories": ["stat.ML", "cs.LG", "physics.chem-ph", "physics.comp-ph"], "comment": "29 pages, 2 tables, 3 figures", "summary": "We present Free energy Estimators with Adaptive Transport (FEAT), a novel\nframework for free energy estimation -- a critical challenge across scientific\ndomains. FEAT leverages learned transports implemented via stochastic\ninterpolants and provides consistent, minimum-variance estimators based on\nescorted Jarzynski equality and controlled Crooks theorem, alongside\nvariational upper and lower bounds on free energy differences. Unifying\nequilibrium and non-equilibrium methods under a single theoretical framework,\nFEAT establishes a principled foundation for neural free energy calculations.\nExperimental validation on toy examples, molecular simulations, and quantum\nfield theory demonstrates improvements over existing learning-based methods.", "AI": {"tldr": "FEAT is a new framework for free energy estimation using learned transports and stochastic interpolants, offering consistent, minimum-variance estimators and unifying equilibrium/non-equilibrium methods.", "motivation": "Free energy estimation is a critical challenge in scientific domains, requiring improved accuracy and theoretical foundations.", "method": "FEAT uses learned transports via stochastic interpolants, leveraging escorted Jarzynski equality and controlled Crooks theorem, with variational bounds.", "result": "FEAT outperforms existing learning-based methods in toy examples, molecular simulations, and quantum field theory.", "conclusion": "FEAT provides a principled, unified framework for neural free energy calculations, validated by experimental results."}}
{"id": "2504.12020", "pdf": "https://arxiv.org/pdf/2504.12020", "abs": "https://arxiv.org/abs/2504.12020", "authors": ["Shiwei Gan", "Yafeng Yin", "Zhiwei Jiang", "Hongkai Wen", "Lei Xie", "Sanglu Lu"], "title": "MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (T-PAMI). This is a regular paper\n  submission", "summary": "Recent advances in sign language research have benefited from CNN-based\nbackbones, which are primarily transferred from traditional computer vision\ntasks (\\eg object identification, image recognition). However, these CNN-based\nbackbones usually excel at extracting features like contours and texture, but\nmay struggle with capturing sign-related features. In fact, sign language tasks\nrequire focusing on sign-related regions, including the collaboration between\ndifferent regions (\\eg left hand region and right hand region) and the\neffective content in a single region. To capture such region-related features,\nwe introduce MixSignGraph, which represents sign sequences as a group of mixed\ngraphs and designs the following three graph modules for feature extraction,\n\\ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and\nHierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the\ncorrelation of intra-frame cross-region features within one frame, \\ie focusing\non spatial features. The TSG module tracks the interaction of inter-frame\ncross-region features among adjacent frames, \\ie focusing on temporal features.\nThe HSG module aggregates the same-region features from different-granularity\nfeature maps of a frame, \\ie focusing on hierarchical features. In addition, to\nfurther improve the performance of sign language tasks without gloss\nannotations, we propose a simple yet counter-intuitive Text-driven CTC\nPre-training (TCP) method, which generates pseudo gloss labels from text labels\nfor model pre-training. Extensive experiments conducted on current five public\nsign language datasets demonstrate the superior performance of the proposed\nmodel. Notably, our model surpasses the SOTA models on multiple sign language\ntasks across several datasets, without relying on any additional cues.", "AI": {"tldr": "MixSignGraph, a graph-based method, improves sign language feature extraction by focusing on spatial, temporal, and hierarchical features, outperforming SOTA models without gloss annotations.", "motivation": "CNN-based backbones struggle with capturing sign-related features, necessitating a method to focus on region-specific and collaborative features in sign language.", "method": "MixSignGraph uses Local Sign Graph (LSG), Temporal Sign Graph (TSG), and Hierarchical Sign Graph (HSG) modules to extract spatial, temporal, and hierarchical features. A Text-driven CTC Pre-training (TCP) method generates pseudo gloss labels for pre-training.", "result": "The model outperforms SOTA models on multiple sign language tasks across five datasets without additional cues.", "conclusion": "MixSignGraph effectively captures sign-related features and improves performance, demonstrating its superiority in sign language tasks."}}
{"id": "2504.11519", "pdf": "https://arxiv.org/pdf/2504.11519", "abs": "https://arxiv.org/abs/2504.11519", "authors": ["Mohammad Farahmand", "Amoon Jamzad", "Fahimeh Fooladgar", "Laura Connolly", "Martin Kaufmann", "Kevin Yi Mi Ren", "John Rudan", "Doug McKay", "Gabor Fichtinger", "Parvin Mousavi"], "title": "FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry", "categories": ["physics.med-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "Purpose: Accurately classifying tissue margins during cancer surgeries is\ncrucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass\nSpectrometry (REIMS), a tool for real-time intraoperative margin assessment,\ngenerates spectra that require machine learning models to support clinical\ndecision-making. However, the scarcity of labeled data in surgical contexts\npresents a significant challenge. This study is the first to develop a\nfoundation model tailored specifically for REIMS data, addressing this\nlimitation and advancing real-time surgical margin assessment. Methods: We\npropose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is\nan adaptation of a foundation model originally designed for text-audio\nassociation, pretrained using our proposed supervised contrastive approach\nbased on triplet loss. An ablation study is performed to compare our proposed\nmodel against other models and pretraining methods. Results: Our proposed model\nsignificantly improves the classification performance, achieving\nstate-of-the-art performance with an AUROC of $82.4\\% \\pm 0.8$. The results\ndemonstrate the advantage of our proposed pretraining method and selected\nbackbone over the self-supervised and semi-supervised baselines and alternative\nmodels. Conclusion: Our findings demonstrate that foundation models, adapted\nand pretrained using our novel approach, can effectively classify REIMS data\neven with limited labeled examples. This highlights the viability of foundation\nmodels for enhancing real-time surgical margin assessment, particularly in\ndata-scarce clinical environments.", "AI": {"tldr": "The paper introduces FACT, a foundation model for classifying cancer tissue margins using REIMS data, achieving state-of-the-art performance with limited labeled data.", "motivation": "Accurate real-time margin assessment during cancer surgeries is critical, but labeled data scarcity in surgical contexts poses challenges.", "method": "FACT, a foundation model adapted from text-audio association, is pretrained using supervised contrastive learning with triplet loss, outperforming baselines.", "result": "FACT achieves an AUROC of 82.4% \u00b1 0.8, demonstrating superior performance over self-supervised and semi-supervised alternatives.", "conclusion": "Foundation models like FACT, pretrained with novel methods, can effectively classify REIMS data with limited labels, enhancing surgical margin assessment."}}
{"id": "2504.12021", "pdf": "https://arxiv.org/pdf/2504.12021", "abs": "https://arxiv.org/abs/2504.12021", "authors": ["Mohamad Dalal", "Artur Xarles", "Anthony Cioppa", "Silvio Giancola", "Marc Van Droogenbroeck", "Bernard Ghanem", "Albert Clap\u00e9s", "Sergio Escalera", "Thomas B. Moeslund"], "title": "Action Anticipation from SoccerNet Football Video Broadcasts", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "15 pages, 14 figures. To be published in the CVSports CVPR workshop", "summary": "Artificial intelligence has revolutionized the way we analyze sports videos,\nwhether to understand the actions of games in long untrimmed videos or to\nanticipate the player's motion in future frames. Despite these efforts, little\nattention has been given to anticipating game actions before they occur. In\nthis work, we introduce the task of action anticipation for football broadcast\nvideos, which consists in predicting future actions in unobserved future\nframes, within a five- or ten-second anticipation window. To benchmark this\ntask, we release a new dataset, namely the SoccerNet Ball Action Anticipation\ndataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a\nFootball Action ANticipation TRAnsformer (FAANTRA), a baseline method that\nadapts FUTR, a state-of-the-art action anticipation model, to predict\nball-related actions. To evaluate action anticipation, we introduce new\nmetrics, including mAP@$\\delta$, which evaluates the temporal precision of\npredicted future actions, as well as mAP@$\\infty$, which evaluates their\noccurrence within the anticipation window. We also conduct extensive ablation\nstudies to examine the impact of various task settings, input configurations,\nand model architectures. Experimental results highlight both the feasibility\nand challenges of action anticipation in football videos, providing valuable\ninsights into the design of predictive models for sports analytics. By\nforecasting actions before they unfold, our work will enable applications in\nautomated broadcasting, tactical analysis, and player decision-making. Our\ndataset and code are publicly available at\nhttps://github.com/MohamadDalal/FAANTRA.", "AI": {"tldr": "The paper introduces action anticipation in football videos, proposing a new dataset (SoccerNet Ball Action Anticipation) and a baseline method (FAANTRA) to predict ball-related actions. It evaluates performance with new metrics and highlights applications in sports analytics.", "motivation": "Little attention has been given to anticipating game actions before they occur, despite AI advancements in sports video analysis. This work aims to address this gap by predicting future actions in football broadcasts.", "method": "The paper proposes FAANTRA, a baseline method adapting FUTR for ball-related action anticipation. It introduces new metrics (mAP@\u03b4 and mAP@\u221e) and conducts ablation studies on task settings, inputs, and architectures.", "result": "Experimental results show feasibility and challenges in action anticipation, providing insights for predictive sports analytics models.", "conclusion": "The work enables applications in automated broadcasting, tactical analysis, and player decision-making by forecasting actions before they unfold. The dataset and code are publicly available."}}
{"id": "2504.11520", "pdf": "https://arxiv.org/pdf/2504.11520", "abs": "https://arxiv.org/abs/2504.11520", "authors": ["Adam Banda", "Charanjit K. Khosa", "Veronica Sanz"], "title": "Strengthening Anomaly Awareness", "categories": ["hep-ph", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "We present a refined version of the Anomaly Awareness framework for enhancing\nunsupervised anomaly detection. Our approach introduces minimal supervision\ninto Variational Autoencoders (VAEs) through a two-stage training strategy: the\nmodel is first trained in an unsupervised manner on background data, and then\nfine-tuned using a small sample of labeled anomalies to encourage larger\nreconstruction errors for anomalous samples.\n  We validate the method across diverse domains, including the MNIST dataset\nwith synthetic anomalies, network intrusion data from the CICIDS benchmark,\ncollider physics data from the LHCO2020 dataset, and simulated events from the\nStandard Model Effective Field Theory (SMEFT). The latter provides a realistic\nexample of subtle kinematic deviations in Higgs boson production. In all cases,\nthe model demonstrates improved sensitivity to unseen anomalies, achieving\nbetter separation between normal and anomalous samples. These results indicate\nthat even limited anomaly information, when incorporated through targeted\nfine-tuning, can substantially improve the generalization and performance of\nunsupervised models for anomaly detection.", "AI": {"tldr": "A refined Anomaly Awareness framework improves unsupervised anomaly detection by adding minimal supervision to VAEs through a two-stage training process, enhancing sensitivity to unseen anomalies.", "motivation": "To enhance unsupervised anomaly detection by incorporating limited labeled anomaly data to improve model generalization and performance.", "method": "A two-stage training strategy: unsupervised training on background data followed by fine-tuning with labeled anomalies to increase reconstruction errors for anomalies.", "result": "Improved sensitivity to unseen anomalies across diverse datasets (MNIST, CICIDS, LHCO2020, SMEFT), achieving better separation between normal and anomalous samples.", "conclusion": "Targeted fine-tuning with limited anomaly information significantly boosts the performance and generalization of unsupervised anomaly detection models."}}
{"id": "2504.12027", "pdf": "https://arxiv.org/pdf/2504.12027", "abs": "https://arxiv.org/abs/2504.12027", "authors": ["Bingyan Liu", "Chengyu Wang", "Tongtong Su", "Huan Ten", "Jun Huang", "Kailing Guo", "Kui Jia"], "title": "Understanding Attention Mechanism in Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets.", "AI": {"tldr": "The paper analyzes spatial and temporal attention blocks in diffusion-based T2V models, revealing their impact on video quality and structure. It proposes lightweight methods to enhance quality and enable text-guided editing.", "motivation": "Understanding how attention mechanisms in T2V models influence video synthesis, particularly quality and temporal consistency, is unclear.", "method": "An in-depth perturbation analysis of attention blocks using an information-theoretic approach.", "result": "Temporal and spatial attention maps affect video timing, layout, complexity, and aesthetics. High-entropy maps correlate with superior quality, while low-entropy maps relate to intra-frame structure.", "conclusion": "Proposed lightweight methods for attention matrix manipulation improve video quality and enable text-guided editing, validated across datasets."}}
{"id": "2504.11554", "pdf": "https://arxiv.org/pdf/2504.11554", "abs": "https://arxiv.org/abs/2504.11554", "authors": ["Chengkun Li", "Bobby Huggins", "Petrus Mikkola", "Luigi Acerbi"], "title": "Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted at the Proceedings track of the 7th Symposium on Advances in\n  Approximate Bayesian Inference (AABI 2025). 40 pages, 10 figures", "summary": "Bayesian inference with computationally expensive likelihood evaluations\nremains a significant challenge in many scientific domains. We propose\nnormalizing flow regression (NFR), a novel offline inference method for\napproximating posterior distributions. Unlike traditional surrogate approaches\nthat require additional sampling or inference steps, NFR directly yields a\ntractable posterior approximation through regression on existing log-density\nevaluations. We introduce training techniques specifically for flow regression,\nsuch as tailored priors and likelihood functions, to achieve robust posterior\nand model evidence estimation. We demonstrate NFR's effectiveness on synthetic\nbenchmarks and real-world applications from neuroscience and biology, showing\nsuperior or comparable performance to existing methods. NFR represents a\npromising approach for Bayesian inference when standard methods are\ncomputationally prohibitive or existing model evaluations can be recycled.", "AI": {"tldr": "NFR is a novel offline Bayesian inference method using normalizing flow regression to approximate posteriors without additional sampling, outperforming traditional methods.", "motivation": "Bayesian inference with expensive likelihood evaluations is challenging; NFR aims to simplify this by leveraging existing log-density evaluations.", "method": "NFR uses normalizing flow regression, tailored priors, and likelihood functions for robust posterior and evidence estimation.", "result": "NFR shows superior or comparable performance on synthetic benchmarks and real-world neuroscience and biology applications.", "conclusion": "NFR is a promising solution for computationally expensive Bayesian inference, especially when reusing model evaluations."}}
{"id": "2504.12029", "pdf": "https://arxiv.org/pdf/2504.12029", "abs": "https://arxiv.org/abs/2504.12029", "authors": ["Bingjie Gao", "Bo Zhang", "Li Niu"], "title": "Object Placement for Anything", "categories": ["cs.CV"], "comment": "accepted by ICME 2025", "summary": "Object placement aims to determine the appropriate placement (\\emph{e.g.},\nlocation and size) of a foreground object when placing it on the background\nimage. Most previous works are limited by small-scale labeled dataset, which\nhinders the real-world application of object placement. In this work, we devise\na semi-supervised framework which can exploit large-scale unlabeled dataset to\npromote the generalization ability of discriminative object placement models.\nThe discriminative models predict the rationality label for each foreground\nplacement given a foreground-background pair. To better leverage the labeled\ndata, under the semi-supervised framework, we further propose to transfer the\nknowledge of rationality variation, \\emph{i.e.}, whether the change of\nforeground placement would result in the change of rationality label, from\nlabeled data to unlabeled data. Extensive experiments demonstrate that our\nframework can effectively enhance the generalization ability of discriminative\nobject placement models.", "AI": {"tldr": "A semi-supervised framework is proposed to improve object placement models by leveraging large-scale unlabeled data and transferring knowledge of rationality variation.", "motivation": "Previous works are limited by small-scale labeled datasets, hindering real-world application of object placement.", "method": "A semi-supervised framework exploits unlabeled data and transfers knowledge of rationality variation from labeled to unlabeled data.", "result": "The framework effectively enhances the generalization ability of discriminative object placement models.", "conclusion": "The proposed method successfully addresses limitations of small datasets and improves model performance."}}
{"id": "2504.11555", "pdf": "https://arxiv.org/pdf/2504.11555", "abs": "https://arxiv.org/abs/2504.11555", "authors": ["Yahya Sattar", "Sunmook Choi", "Yassir Jedra", "Maryam Fazel", "Sarah Dean"], "title": "Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "stat.ML"], "comment": null, "summary": "We consider the problem of controlling a linear dynamical system from\nbilinear observations with minimal quadratic cost. Despite the similarity of\nthis problem to standard linear quadratic Gaussian (LQG) control, we show that\nwhen the observation model is bilinear, neither does the Separation Principle\nhold, nor is the optimal controller affine in the estimated state. Moreover,\nthe cost-to-go is non-convex in the control input. Hence, finding an analytical\nexpression for the optimal feedback controller is difficult in general. Under\ncertain settings, we show that the standard LQG controller locally maximizes\nthe cost instead of minimizing it. Furthermore, the optimal controllers\n(derived analytically) are not unique and are nonlinear in the estimated state.\nWe also introduce a notion of input-dependent observability and derive\nconditions under which the Kalman filter covariance remains bounded. We\nillustrate our theoretical results through numerical experiments in multiple\nsynthetic settings.", "AI": {"tldr": "The paper explores controlling a linear dynamical system with bilinear observations, revealing challenges like non-convex cost-to-go and non-affine optimal controllers, unlike standard LQG control.", "motivation": "The study aims to understand the complexities and deviations from standard LQG control when observations are bilinear, highlighting the lack of Separation Principle and non-affine optimal controllers.", "method": "The paper analyzes the problem theoretically, derives conditions for bounded Kalman filter covariance, and validates findings through numerical experiments in synthetic settings.", "result": "Results show the standard LQG controller can locally maximize cost, optimal controllers are nonlinear and non-unique, and input-dependent observability plays a key role.", "conclusion": "The study concludes that bilinear observations introduce significant challenges in control design, requiring nonlinear and non-unique solutions, with implications for practical applications."}}
{"id": "2504.12039", "pdf": "https://arxiv.org/pdf/2504.12039", "abs": "https://arxiv.org/abs/2504.12039", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.", "AI": {"tldr": "RadMamba, a lightweight Mamba SSM for radar-based HAR, achieves high accuracy with minimal parameters, outperforming existing models.", "motivation": "Address the computational demands of current radar-based HAR methods while maintaining accuracy.", "method": "Introduces RadMamba, a parameter-efficient Mamba SSM tailored for radar micro-Doppler signals.", "result": "Achieves 99.8% accuracy on Dataset DIAT with 1/400 parameters and 92.0% on Dataset CI4R with 1/10 parameters. Outperforms others by 3% on Dataset UoG2020 with only 6.7k parameters.", "conclusion": "RadMamba offers a lightweight, accurate solution for radar-based HAR, suitable for resource-constrained scenarios."}}
{"id": "2504.11570", "pdf": "https://arxiv.org/pdf/2504.11570", "abs": "https://arxiv.org/abs/2504.11570", "authors": ["Haozhe Lei", "Ya-Ting Yang", "Tao Li", "Zilin Bian", "Fan Zuo", "Sundeep Rangan", "Kaan Ozbay"], "title": "Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm\n(TAMPA), designed to improve real-time incident management during major events\nlike sports tournaments and concerts. Such events significantly stress\ntransportation networks, requiring efficient and adaptive patrol solutions.\nTAMPA integrates predictive traffic modeling and real-time complaint\nestimation, dynamically optimizing patrol deployment. Using dynamic\nprogramming, the algorithm continuously adjusts patrol strategies within short\nplanning windows, effectively balancing immediate response and efficient\nrouting. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects\nsignificant shifts in complaint patterns, triggering proactive adjustments in\npatrol routes. Theoretical analyses ensure performance remains closely aligned\nwith optimal solutions. Simulation results from an urban traffic network\ndemonstrate TAMPA's superior performance, showing improvements of approximately\n87.5\\% over stationary methods and 114.2\\% over random strategies. Future work\nincludes enhancing adaptability and incorporating digital twin technology for\nimproved predictive accuracy, particularly relevant for events like the 2026\nFIFA World Cup at MetLife Stadium.", "AI": {"tldr": "TAMPA is an adaptive patrolling algorithm for real-time incident management during major events, outperforming stationary and random methods by 87.5% and 114.2%, respectively.", "motivation": "Major events stress transportation networks, requiring efficient and adaptive patrol solutions.", "method": "TAMPA integrates predictive traffic modeling and real-time complaint estimation, using dynamic programming and the Dvoretzky-Kiefer-Wolfowitz inequality for adaptive patrol route adjustments.", "result": "Simulations show TAMPA improves performance by 87.5% over stationary methods and 114.2% over random strategies.", "conclusion": "TAMPA is effective for real-time incident management, with future enhancements planned for adaptability and predictive accuracy using digital twin technology."}}
{"id": "2504.12045", "pdf": "https://arxiv.org/pdf/2504.12045", "abs": "https://arxiv.org/abs/2504.12045", "authors": ["Jonas Myhre Schi\u00f8tt", "Viktor Sebastian Petersen", "Dimitrios P. Papadopoulos"], "title": "pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild", "categories": ["cs.CV", "cs.LG", "I.2.1; I.4.6"], "comment": "15 pages, 7 figures, to be published in SCIA 2025", "summary": "Computer vision models have seen increased usage in sports, and reinforcement\nlearning (RL) is famous for beating humans in strategic games such as Chess and\nGo. In this paper, we are interested in building upon these advances and\nexamining the game of classic 8-ball pool. We introduce pix2pockets, a\nfoundation for an RL-assisted pool coach. Given a single image of a pool table,\nwe first aim to detect the table and the balls and then propose the optimal\nshot suggestion. For the first task, we build a dataset with 195 diverse images\nwhere we manually annotate all balls and table dots, leading to 5748 object\nsegmentation masks. For the second task, we build a standardized RL environment\nthat allows easy development and benchmarking of any RL algorithm. Our object\ndetection model yields an AP50 of 91.2 while our ball location pipeline obtains\nan error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set\na baseline for the shot suggestion task and we show that all of them fail to\npocket all balls without making a foul move. We also present a simple baseline\nthat achieves a per-shot success rate of 94.7% and clears a full game in a\nsingle turn 30% of the time.", "AI": {"tldr": "The paper introduces pix2pockets, an RL-assisted pool coach system for detecting pool table objects and suggesting optimal shots, achieving high accuracy in detection and baseline performance in RL-based shot suggestions.", "motivation": "Leverage advances in computer vision and reinforcement learning to develop a system for assisting in the game of 8-ball pool, addressing object detection and optimal shot suggestions.", "method": "Created a dataset of 195 annotated images for object segmentation, developed an RL environment for benchmarking, and tested standard RL algorithms for shot suggestions.", "result": "Object detection achieved 91.2 AP50, ball location error was 0.4 cm. RL baselines failed to pocket all balls without fouls, but a simple baseline achieved 94.7% per-shot success and 30% full-game clearance.", "conclusion": "The system shows promise in object detection and shot suggestion, though RL algorithms need improvement for flawless gameplay."}}
{"id": "2504.11575", "pdf": "https://arxiv.org/pdf/2504.11575", "abs": "https://arxiv.org/abs/2504.11575", "authors": ["Furqan Rustam", "Islam Obaidat", "Anca Delia Jurcut"], "title": "MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment\n(M-En) networks presents significant challenges due to diverse malicious\ntraffic patterns and the evolving nature of cyber threats. Existing AI-based\ndetection systems struggle to adapt to new attack strategies and lack real-time\nattack detection capabilities with high accuracy and efficiency. This study\nproposes an online, continuous learning methodology for DDoS detection in M-En\nnetworks, enabling continuous model updates and real-time adaptation to\nemerging threats, including zero-day attacks. First, we develop a unique M-En\nnetwork dataset by setting up a realistic, real-time simulation using the NS-3\ntool, incorporating both victim and bot devices. DDoS attacks with varying\npacket sizes are simulated using the DDoSim application across IoT and\ntraditional IP-based environments under M-En network criteria. Our approach\nemploys a multi-level framework (MULTI-LF) featuring two machine learning\nmodels: a lightweight Model 1 (M1) trained on a selective, critical packet\ndataset for fast and efficient initial detection, and a more complex, highly\naccurate Model 2 (M2) trained on extensive data. When M1 exhibits low\nconfidence in its predictions, the decision is escalated to M2 for verification\nand potential fine-tuning of M1 using insights from M2. If both models\ndemonstrate low confidence, the system flags the incident for human\nintervention, facilitating model updates with human-verified categories to\nenhance adaptability to unseen attack patterns. We validate the MULTI-LF\nthrough real-world simulations, demonstrating superior classification accuracy\nof 0.999 and low prediction latency of 0.866 seconds compared to established\nbaselines. Furthermore, we evaluate performance in terms of memory usage (3.632\nMB) and CPU utilization (10.05%) in real-time scenarios.", "AI": {"tldr": "Proposes an online, continuous learning methodology for DDoS detection in Multi-Environment networks, using a multi-level framework with two ML models for real-time adaptation and high accuracy.", "motivation": "Existing AI-based DDoS detection systems struggle with adaptability to new attack strategies and lack real-time capabilities.", "method": "Develops a realistic M-En dataset using NS-3, employs a multi-level framework (MULTI-LF) with two ML models (M1 for fast detection, M2 for accuracy), and includes human intervention for model updates.", "result": "Achieves 0.999 classification accuracy, 0.866s prediction latency, 3.632MB memory usage, and 10.05% CPU utilization.", "conclusion": "The MULTI-LF framework effectively addresses real-time DDoS detection challenges in M-En networks with high accuracy and efficiency."}}
{"id": "2504.12048", "pdf": "https://arxiv.org/pdf/2504.12048", "abs": "https://arxiv.org/abs/2504.12048", "authors": ["Zirui Pan", "Xin Wang", "Yipeng Zhang", "Hong Chen", "Kwan Man Cheng", "Yaofei Wu", "Wenwu Zhu"], "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM", "categories": ["cs.CV"], "comment": "AAAI 2025 Poster", "summary": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io.", "AI": {"tldr": "The paper introduces Modular-Cam, a method for text-to-video generation that handles complex prompts by decomposing them into scenes and controlling camera movements.", "motivation": "Existing methods struggle with complex prompts involving dynamic scenes and camera-view changes, failing to decompose or transition smoothly.", "method": "Modular-Cam uses a large language model to analyze prompts, a temporal transformer for scene continuity, CamOperator for camera control, and AdaControlNet for consistency.", "result": "Experiments show Modular-Cam effectively generates multi-scene videos with fine-grained camera control.", "conclusion": "Modular-Cam addresses limitations of current methods, offering improved performance in complex text-to-video generation."}}
{"id": "2504.11609", "pdf": "https://arxiv.org/pdf/2504.11609", "abs": "https://arxiv.org/abs/2504.11609", "authors": ["Gemma E. Moran", "Bryon Aragam"], "title": "Towards Interpretable Deep Generative Models via Causal Representation Learning", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "comment": null, "summary": "Recent developments in generative artificial intelligence (AI) rely on\nmachine learning techniques such as deep learning and generative modeling to\nachieve state-of-the-art performance across wide-ranging domains. These\nmethods' surprising performance is due in part to their ability to learn\nimplicit \"representations'' of complex, multi-modal data. Unfortunately, deep\nneural networks are notoriously black boxes that obscure these representations,\nmaking them difficult to interpret or analyze. To resolve these difficulties,\none approach is to build new interpretable neural network models from the\nground up. This is the goal of the emerging field of causal representation\nlearning (CRL) that uses causality as a vector for building flexible,\ninterpretable, and transferable generative AI. CRL can be seen as a culmination\nof three intrinsically statistical problems: (i) latent variable models such as\nfactor analysis; (ii) causal graphical models with latent variables; and (iii)\nnonparametric statistics and deep learning. This paper reviews recent progress\nin CRL from a statistical perspective, focusing on connections to classical\nmodels and statistical and causal identifiablity results. This review also\nhighlights key application areas, implementation strategies, and open\nstatistical questions in CRL.", "AI": {"tldr": "The paper reviews causal representation learning (CRL), an emerging field aiming to create interpretable and transferable generative AI by integrating causality into neural networks. It connects CRL to classical statistical models and discusses its applications, methods, and open questions.", "motivation": "Deep neural networks' black-box nature obscures learned representations, making them hard to interpret. CRL addresses this by building interpretable models using causality.", "method": "CRL combines latent variable models, causal graphical models, and deep learning. The paper reviews progress, focusing on statistical connections, identifiability, and implementation strategies.", "result": "CRL offers a framework for flexible, interpretable, and transferable generative AI, bridging classical statistics and modern deep learning.", "conclusion": "CRL is a promising direction for interpretable AI, but open statistical questions remain. The paper highlights its potential and challenges."}}
{"id": "2504.12078", "pdf": "https://arxiv.org/pdf/2504.12078", "abs": "https://arxiv.org/abs/2504.12078", "authors": ["Trina De", "Adrian Urbanski", "Artur Yakimovich"], "title": "Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects", "categories": ["cs.CV", "q-bio.QM", "J.3; I.4"], "comment": "12 pages, 8 figures", "summary": "Biomedical images often contain objects known to be spatially correlated or\nnested due to their inherent properties, leading to semantic relations.\nExamples include cell nuclei being nested within eukaryotic cells and colonies\ngrowing exclusively within their culture dishes. While these semantic relations\nbear key importance, detection tasks are often formulated independently,\nrequiring multi-shot analysis pipelines. Importantly, spatial correlation could\nconstitute a fundamental prior facilitating learning of more meaningful\nrepresentations for tasks like instance segmentation. This knowledge has, thus\nfar, not been utilised by the biomedical computer vision community. We argue\nthat the instance segmentation of two or more categories of objects can be\nachieved in parallel. We achieve this via two architectures HydraStarDist (HSD)\nand the novel (HSD-WBR) based on the widely-used StarDist (SD), to take\nadvantage of the star-convexity of our target objects. HSD and HSD-WBR are\nconstructed to be capable of incorporating their interactions as constraints\ninto account. HSD implicitly incorporates spatial correlation priors based on\nobject interaction through a joint encoder. HSD-WBR further enforces the prior\nin a regularisation layer with the penalty we proposed named Within Boundary\nRegularisation Penalty (WBR). Both architectures achieve nested instance\nsegmentation in a single shot. We demonstrate their competitiveness based on\n$IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate\n(JTPR) compared to their baseline SD and Cellpose. Our approach can be further\nmodified to capture partial-inclusion/-exclusion in multi-object interactions\nin fluorescent or brightfield microscopy or digital imaging. Finally, our\nstrategy suggests gains by making this learning single-shot and computationally\nefficient.", "AI": {"tldr": "The paper proposes two architectures, HydraStarDist (HSD) and HSD-WBR, for nested instance segmentation in biomedical images by leveraging spatial correlations and semantic relations between objects.", "motivation": "Biomedical images often contain spatially correlated or nested objects, but existing methods treat detection tasks independently, missing opportunities for more efficient and meaningful representations.", "method": "HSD and HSD-WBR extend StarDist (SD) to incorporate spatial correlation priors. HSD uses a joint encoder, while HSD-WBR adds a regularization layer with a Within Boundary Regularisation Penalty (WBR).", "result": "Both architectures outperform baseline methods (SD and Cellpose) in nested instance segmentation, as measured by IoU_R, AP, and Joint TP rate (JTPR).", "conclusion": "The approach enables single-shot, computationally efficient segmentation and can be adapted for multi-object interactions in microscopy or digital imaging."}}
{"id": "2504.11610", "pdf": "https://arxiv.org/pdf/2504.11610", "abs": "https://arxiv.org/abs/2504.11610", "authors": ["Tianjian Yang", "Wei Vivian Li"], "title": "Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Background: The integration and analysis of multi-modal data are increasingly\nessential across various domains including bioinformatics. As the volume and\ncomplexity of such data grow, there is a pressing need for computational models\nthat not only integrate diverse modalities but also leverage their\ncomplementary information to improve clustering accuracy and insights,\nespecially when dealing with partial observations with missing data. Results:\nWe propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an\nunsupervised method for the integration and joint dimensionality reduction of\nmulti-modal data. GPCCA addresses key challenges in multi-modal data analysis\nby handling missing values within the model, enabling the integration of more\nthan two modalities, and identifying informative features while accounting for\ncorrelations within individual modalities. The model demonstrates robustness to\nvarious missing data patterns and provides low-dimensional embeddings that\nfacilitate downstream clustering and analysis. In a range of simulation\nsettings, GPCCA outperforms existing methods in capturing essential patterns\nacross modalities. Additionally, we demonstrate its applicability to\nmulti-omics data from TCGA cancer datasets and a multi-view image dataset.\nConclusion: GPCCA offers a useful framework for multi-modal data integration,\neffectively handling missing data and providing informative low-dimensional\nembeddings. Its performance across cancer genomics and multi-view image data\nhighlights its robustness and potential for broad application. To make the\nmethod accessible to the wider research community, we have released an R\npackage, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.", "AI": {"tldr": "GPCCA is an unsupervised method for integrating multi-modal data, handling missing values, and improving clustering accuracy.", "motivation": "The need for models to integrate diverse data modalities and handle missing data in bioinformatics and other domains.", "method": "Generalized Probabilistic Canonical Correlation Analysis (GPCCA), which integrates multi-modal data, handles missing values, and reduces dimensionality.", "result": "GPCCA outperforms existing methods in simulations and works well with cancer genomics and multi-view image data.", "conclusion": "GPCCA is robust and broadly applicable, with an available R package for wider use."}}
{"id": "2504.12080", "pdf": "https://arxiv.org/pdf/2504.12080", "abs": "https://arxiv.org/abs/2504.12080", "authors": ["Mengshi Qi", "Pengfei Zhu", "Xiangtai Li", "Xiaoyang Bi", "Lu Qi", "Huadong Ma", "Ming-Hsuan Yang"], "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.", "AI": {"tldr": "DC-SAM adapts SAM and SAM2 for in-context segmentation using prompt-tuning, achieving state-of-the-art results on COCO-20i, PASCAL-5i, and a new IC-VOS benchmark.", "motivation": "To address the gap in applying Segment Anything Models (SAM) to in-context segmentation for images and videos.", "method": "Proposes Dual Consistency SAM (DC-SAM) with prompt-tuning, feature fusion, cycle-consistent cross-attention, and a dual-branch design. Introduces a mask-tube training strategy and the IC-VOS benchmark for videos.", "result": "Achieves 55.5 mIoU on COCO-20i, 73.0 mIoU on PASCAL-5i, and 71.52 J&F on IC-VOS.", "conclusion": "DC-SAM effectively adapts SAM for in-context segmentation, with potential for video applications."}}
{"id": "2504.11621", "pdf": "https://arxiv.org/pdf/2504.11621", "abs": "https://arxiv.org/abs/2504.11621", "authors": ["Samin Aref", "Sanchaai Mathiyarasan"], "title": "Robust Markov stability for community detection at a scale learned based on the structure", "categories": ["cs.SI", "cond-mat.stat-mech", "cs.LG", "90C90, 90C10, 90C57, 90C59, 90C35, 05C15, 65K05", "I.2.6; G.2.2"], "comment": "This is the author copy of an article accepted for publication by\n  ACM. The publisher's verified version and full citation details are available\n  on the ACM website", "summary": "Community detection, the unsupervised task of clustering nodes of a graph,\nfinds applications across various fields. The common approaches for community\ndetection involve optimizing an objective function to partition the nodes into\ncommunities at a single scale of granularity. However, the single-scale\napproaches often fall short of producing partitions that are robust and at a\nsuitable scale. The existing algorithm, PyGenStability, returns multiple robust\npartitions for a network by optimizing the multi-scale Markov stability\nfunction. However, in cases where the suitable scale is not known or assumed by\nthe user, there is no principled method to select a single robust partition at\na suitable scale from the multiple partitions that PyGenStability produces. Our\nproposed method combines the Markov stability framework with a pre-trained\nmachine learning model for scale selection to obtain one robust partition at a\nscale that is learned based on the graph structure. This automatic scale\nselection involves using a gradient boosting model pre-trained on hand-crafted\nand embedding-based network features from a labeled dataset of 10k benchmark\nnetworks. This model was trained to predicts the scale value that maximizes the\nsimilarity of the output partition to the planted partition of the benchmark\nnetwork. Combining our scale selection algorithm with the PyGenStability\nalgorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale\ncommunity detection algorithm that returns one robust partition at a suitable\nscale without the need for any assumptions, input, or tweaking from the user.\nWe compare the performance of PO against 29 algorithms and show that it\noutperforms 25 other algorithms by statistically meaningful margins. Our\nresults facilitate choosing between community detection algorithms, among which\nPO stands out as the accurate, robust, and hyperparameter-free method.", "AI": {"tldr": "The paper introduces PyGenStabilityOne (PO), a hyperparameter-free multi-scale community detection algorithm that automatically selects a robust partition at a suitable scale using a pre-trained ML model.", "motivation": "Single-scale community detection methods often fail to produce robust and suitable partitions. Existing multi-scale methods like PyGenStability lack a principled way to select the best partition.", "method": "Combines Markov stability with a pre-trained gradient boosting model for scale selection, trained on 10k benchmark networks.", "result": "PO outperforms 25 out of 29 algorithms in performance comparisons, offering accuracy and robustness without user input.", "conclusion": "PO is a superior, hyperparameter-free method for community detection, eliminating the need for manual scale selection."}}
{"id": "2504.12083", "pdf": "https://arxiv.org/pdf/2504.12083", "abs": "https://arxiv.org/abs/2504.12083", "authors": ["Pritam Sarkar", "Ali Etemad"], "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.", "AI": {"tldr": "The paper proposes a self-alignment framework for Large Video Language Models (LVLMs) to improve their fine-grained temporal understanding and reduce errors, introducing a novel method called Refined Regularized Preference Optimization (RRPO).", "motivation": "LVLMs struggle with fine-grained temporal understanding, hallucination, and simple mistakes, limiting their real-world reliability.", "method": "The framework uses self-alignment with preferred/non-preferred response pairs and introduces RRPO for precise alignment and stable training.", "result": "RRPO outperforms Direct Preference Optimization (DPO), showing effectiveness in tasks like video hallucination and temporal reasoning.", "conclusion": "The self-alignment framework and RRPO enhance LVLMs' performance, addressing key challenges for reliable deployment."}}
{"id": "2504.11650", "pdf": "https://arxiv.org/pdf/2504.11650", "abs": "https://arxiv.org/abs/2504.11650", "authors": ["Shengyuan Yan", "Farzad Vazinram", "Zeynab Kaseb", "Lindsay Spoor", "Jochen Stiasny", "Betul Mamudi", "Amirhossein Heydarian Ardakani", "Ugochukwu Orji", "Pedro P. Vergara", "Yu Xiang", "Jerry Guo"], "title": "Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.NA", "cs.SY", "math.NA", "I.2.8"], "comment": "7 pages, 9 figures, 3 tables, 14 equations, 1 lemma, and 2 theorems.\n  ICT for Industry 2025 Alliander usecase workshop paper. Oral presentation of\n  this paper accepted and to be given on 16th April 2025 in ICT.OPEN 2025\n  conference of Netherlands in the Beatrix Theatre in Utrecht", "summary": "Power flow (PF) calculations are fundamental to power system analysis to\nensure stable and reliable grid operation. The Newton-Raphson (NR) method is\ncommonly used for PF analysis due to its rapid convergence when initialized\nproperly. However, as power grids operate closer to their capacity limits,\nill-conditioned cases and convergence issues pose significant challenges. This\nwork, therefore, addresses these challenges by proposing strategies to improve\nNR initialization, hence minimizing iterations and avoiding divergence. We\nexplore three approaches: (i) an analytical method that estimates the basin of\nattraction using mathematical bounds on voltages, (ii) Two data-driven models\nleveraging supervised learning or physics-informed neural networks (PINNs) to\npredict optimal initial guesses, and (iii) a reinforcement learning (RL)\napproach that incrementally adjusts voltages to accelerate convergence. These\nmethods are tested on benchmark systems. This research is particularly relevant\nfor modern power systems, where high penetration of renewables and\ndecentralized generation require robust and scalable PF solutions. In\nexperiments, all three proposed methods demonstrate a strong ability to provide\nan initial guess for Newton-Raphson method to converge with fewer steps. The\nfindings provide a pathway for more efficient real-time grid operations, which,\nin turn, support the transition toward smarter and more resilient electricity\nnetworks.", "AI": {"tldr": "The paper proposes three strategies to improve Newton-Raphson initialization for power flow calculations, addressing convergence challenges in modern power grids.", "motivation": "Power grids operating near capacity limits face ill-conditioned cases and convergence issues, necessitating robust solutions for stable grid operation.", "method": "Three approaches: (i) analytical bounds on voltages, (ii) data-driven models (supervised learning/PINNs), and (iii) reinforcement learning for incremental voltage adjustments.", "result": "All methods effectively reduce iterations and avoid divergence in benchmark systems.", "conclusion": "The findings enable more efficient real-time grid operations, supporting smarter and resilient electricity networks."}}
{"id": "2504.12088", "pdf": "https://arxiv.org/pdf/2504.12088", "abs": "https://arxiv.org/abs/2504.12088", "authors": ["Mirza Samad Ahmed Baig", "Syeda Anshrah Gillani", "Abdul Akbar Khan", "Shahid Munir Shah"], "title": "AttentionDrop: A Novel Regularization Method for Transformer Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "26 pages", "summary": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.", "AI": {"tldr": "AttentionDrop is a stochastic regularization method for Transformers, introducing three variants to mitigate overfitting by modifying self-attention distributions.", "motivation": "Transformers often overfit with limited or noisy data. AttentionDrop addresses this by regularizing self-attention.", "method": "Three variants: Hard Attention Masking, Blurred Attention Smoothing, and Consistency-Regularized AttentionDrop.", "result": "Improved regularization for Transformers, reducing overfitting.", "conclusion": "AttentionDrop effectively enhances Transformer robustness by diversifying attention patterns."}}
{"id": "2504.11667", "pdf": "https://arxiv.org/pdf/2504.11667", "abs": "https://arxiv.org/abs/2504.11667", "authors": ["Cemil Vahapoglu", "Timothy J. O'Shea", "Wan Liu", "Tamoghna Roy", "Sennur Ulukus"], "title": "Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "The literature is abundant with methodologies focusing on using transformer\narchitectures due to their prominence in wireless signal processing and their\ncapability to capture long-range dependencies via attention mechanisms. In\nparticular, depthwise separable convolutions enhance parameter efficiency for\nthe process of high-dimensional data characteristics of MIMO systems. In this\nwork, we introduce a novel unsupervised deep learning framework that integrates\ndepthwise separable convolutions and transformers to generate beamforming\nweights under imperfect channel state information (CSI) for a multi-user\nsingle-input multiple-output (MU-SIMO) system in dense urban environments. The\nprimary goal is to enhance throughput by maximizing sum-rate while ensuring\nreliable communication. Spectral efficiency and block error rate (BLER) are\nconsidered as performance metrics. Experiments are carried out under various\nconditions to compare the performance of the proposed NNBF framework against\nbaseline methods zero-forcing beamforming (ZFBF) and minimum mean square error\n(MMSE) beamforming. Experimental results demonstrate the superiority of the\nproposed framework over the baseline techniques.", "AI": {"tldr": "A novel unsupervised deep learning framework combining depthwise separable convolutions and transformers improves beamforming in MU-SIMO systems under imperfect CSI, outperforming ZFBF and MMSE baselines.", "motivation": "Enhancing throughput and reliable communication in dense urban MU-SIMO systems by addressing high-dimensional data challenges and imperfect CSI.", "method": "Integration of depthwise separable convolutions and transformers for unsupervised beamforming weight generation.", "result": "Proposed NNBF framework outperforms ZFBF and MMSE in spectral efficiency and BLER.", "conclusion": "The framework effectively maximizes sum-rate and ensures reliable communication, demonstrating superiority over traditional methods."}}
{"id": "2504.12100", "pdf": "https://arxiv.org/pdf/2504.12100", "abs": "https://arxiv.org/abs/2504.12100", "authors": ["Kaifeng Gao", "Siqi Chen", "Hanwang Zhang", "Jun Xiao", "Yueting Zhuang", "Qianru Sun"], "title": "Generalized Visual Relation Detection with Diffusion Models", "categories": ["cs.CV"], "comment": "Under review at IEEE TCSVT. The Appendix is provided additionally", "summary": "Visual relation detection (VRD) aims to identify relationships (or\ninteractions) between object pairs in an image. Although recent VRD models have\nachieved impressive performance, they are all restricted to pre-defined\nrelation categories, while failing to consider the semantic ambiguity\ncharacteristic of visual relations. Unlike objects, the appearance of visual\nrelations is always subtle and can be described by multiple predicate words\nfrom different perspectives, e.g., ``ride'' can be depicted as ``race'' and\n``sit on'', from the sports and spatial position views, respectively. To this\nend, we propose to model visual relations as continuous embeddings, and design\ndiffusion models to achieve generalized VRD in a conditional generative manner,\ntermed Diff-VRD. We model the diffusion process in a latent space and generate\nall possible relations in the image as an embedding sequence. During the\ngeneration, the visual and text embeddings of subject-object pairs serve as\nconditional signals and are injected via cross-attention. After the generation,\nwe design a subsequent matching stage to assign the relation words to\nsubject-object pairs by considering their semantic similarities. Benefiting\nfrom the diffusion-based generative process, our Diff-VRD is able to generate\nvisual relations beyond the pre-defined category labels of datasets. To\nproperly evaluate this generalized VRD task, we introduce two evaluation\nmetrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image\ncaptioning. Extensive experiments in both human-object interaction (HOI)\ndetection and scene graph generation (SGG) benchmarks attest to the superiority\nand effectiveness of Diff-VRD.", "AI": {"tldr": "Diff-VRD proposes a diffusion-based model for generalized visual relation detection (VRD), addressing semantic ambiguity by generating continuous relation embeddings beyond pre-defined categories.", "motivation": "Current VRD models are limited to pre-defined relation categories and fail to handle semantic ambiguity in visual relations.", "method": "Diff-VRD uses diffusion models to generate relation embeddings in a latent space, conditioned on visual and text embeddings of subject-object pairs, followed by a matching stage.", "result": "The model outperforms benchmarks in human-object interaction (HOI) detection and scene graph generation (SGG), demonstrating its effectiveness.", "conclusion": "Diff-VRD advances VRD by enabling generalized relation detection and introduces new evaluation metrics for this task."}}
{"id": "2504.11671", "pdf": "https://arxiv.org/pdf/2504.11671", "abs": "https://arxiv.org/abs/2504.11671", "authors": ["Ji Ma"], "title": "Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract ``vectors of variable\nvariations'' (e.g., ``male'' to ``female'') from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications.", "AI": {"tldr": "The paper explores how character and context influence LLM behavior in social science settings, proposing methods to probe and modify internal representations in a Dictator Game.", "motivation": "To understand and regulate how social concepts are encoded in LLMs, addressing gaps in behavior shaping for fairness and prosocial applications.", "method": "Extracts and manipulates \"vectors of variable variations\" from the LLM's internal state to alter decision-making in a Dictator Game.", "result": "Demonstrates that manipulating these vectors can significantly change how variables like gender influence the model's behavior.", "conclusion": "Provides a principled approach to studying and engineering social concepts in LLMs, with implications for alignment, debiasing, and social simulations."}}
{"id": "2504.12103", "pdf": "https://arxiv.org/pdf/2504.12103", "abs": "https://arxiv.org/abs/2504.12103", "authors": ["Tao Wen", "Jiepeng Wang", "Yabo Chen", "Shugong Xu", "Chi Zhang", "Xuelong Li"], "title": "Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image", "categories": ["cs.CV"], "comment": "Our project page: https://tele-ai.github.io/MetricSolver/", "summary": "Accurate and generalizable metric depth estimation is crucial for various\ncomputer vision applications but remains challenging due to the diverse depth\nscales encountered in indoor and outdoor environments. In this paper, we\nintroduce Metric-Solver, a novel sliding anchor-based metric depth estimation\nmethod that dynamically adapts to varying scene scales. Our approach leverages\nan anchor-based representation, where a reference depth serves as an anchor to\nseparate and normalize the scene depth into two components: scaled near-field\ndepth and tapered far-field depth. The anchor acts as a normalization factor,\nenabling the near-field depth to be normalized within a consistent range while\nmapping far-field depth smoothly toward zero. Through this approach, any depth\nfrom zero to infinity in the scene can be represented within a unified\nrepresentation, effectively eliminating the need to manually account for scene\nscale variations. More importantly, for the same scene, the anchor can slide\nalong the depth axis, dynamically adjusting to different depth scales. A\nsmaller anchor provides higher resolution in the near-field, improving depth\nprecision for closer objects while a larger anchor improves depth estimation in\nfar regions. This adaptability enables the model to handle depth predictions at\nvarying distances and ensure strong generalization across datasets. Our design\nenables a unified and adaptive depth representation across diverse\nenvironments. Extensive experiments demonstrate that Metric-Solver outperforms\nexisting methods in both accuracy and cross-dataset generalization.", "AI": {"tldr": "Metric-Solver introduces a sliding anchor-based method for metric depth estimation, dynamically adapting to varying scene scales for accurate and generalizable depth prediction.", "motivation": "Diverse depth scales in indoor and outdoor environments make accurate metric depth estimation challenging.", "method": "Uses an anchor-based representation to normalize scene depth into near-field and far-field components, dynamically adjusting the anchor for varying scales.", "result": "Outperforms existing methods in accuracy and cross-dataset generalization.", "conclusion": "Metric-Solver provides a unified and adaptive solution for metric depth estimation across diverse environments."}}
{"id": "2504.11714", "pdf": "https://arxiv.org/pdf/2504.11714", "abs": "https://arxiv.org/abs/2504.11714", "authors": ["Karthik Shivashankar", "Antonio Martini"], "title": "Unravelling Technical debt topics through Time, Programming Languages and Repository", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "This study explores the dynamic landscape of Technical Debt (TD) topics in\nsoftware engineering by examining its evolution across time, programming\nlanguages, and repositories. Despite the extensive research on identifying and\nquantifying TD, there remains a significant gap in understanding the diversity\nof TD topics and their temporal development. To address this, we have conducted\nan explorative analysis of TD data extracted from GitHub issues spanning from\n2015 to September 2023. We employed BERTopic for sophisticated topic modelling.\nThis study categorises the TD topics and tracks their progression over time.\nFurthermore, we have incorporated sentiment analysis for each identified topic,\nproviding a deeper insight into the perceptions and attitudes associated with\nthese topics. This offers a more nuanced understanding of the trends and shifts\nin TD topics through time, programming language, and repository.", "AI": {"tldr": "The study analyzes Technical Debt (TD) topics in software engineering over time, languages, and repositories using BERTopic and sentiment analysis.", "motivation": "To address the gap in understanding the diversity and temporal evolution of TD topics.", "method": "Explorative analysis of GitHub issues (2015-2023) using BERTopic for topic modeling and sentiment analysis.", "result": "Categorization of TD topics and their progression, along with sentiment insights.", "conclusion": "Provides nuanced understanding of TD trends and shifts over time, languages, and repositories."}}
{"id": "2504.12104", "pdf": "https://arxiv.org/pdf/2504.12104", "abs": "https://arxiv.org/abs/2504.12104", "authors": ["Shuo Li", "Fang Liu", "Zehua Hao", "Xinyi Wang", "Lingling Li", "Xu Liu", "Puhua Chen", "Wenping Ma"], "title": "Logits DeConfusion with CLIP for Few-Shot Learning", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "With its powerful visual-language alignment capability, CLIP performs well in\nzero-shot and few-shot learning tasks. However, we found in experiments that\nCLIP's logits suffer from serious inter-class confusion problems in downstream\ntasks, and the ambiguity between categories seriously affects the accuracy. To\naddress this challenge, we propose a novel method called Logits DeConfusion,\nwhich effectively learns and eliminates inter-class confusion in logits by\ncombining our Multi-level Adapter Fusion (MAF) module with our Inter-Class\nDeconfusion (ICD) module. Our MAF extracts features from different levels and\nfuses them uniformly to enhance feature representation. Our ICD learnably\neliminates inter-class confusion in logits with a residual structure.\nExperimental results show that our method can significantly improve the\nclassification performance and alleviate the inter-class confusion problem. The\ncode is available at https://github.com/LiShuo1001/LDC.", "AI": {"tldr": "The paper introduces Logits DeConfusion (LDC) to address CLIP's inter-class confusion in logits, improving classification accuracy via Multi-level Adapter Fusion (MAF) and Inter-Class Deconfusion (ICD) modules.", "motivation": "CLIP's logits exhibit inter-class confusion in downstream tasks, harming accuracy.", "method": "Proposes LDC with MAF for multi-level feature fusion and ICD for learnable logit deconfusion.", "result": "Significantly improves classification performance and reduces inter-class confusion.", "conclusion": "LDC effectively enhances CLIP's accuracy by addressing logit confusion."}}
{"id": "2504.11775", "pdf": "https://arxiv.org/pdf/2504.11775", "abs": "https://arxiv.org/abs/2504.11775", "authors": ["Tianhe Zhang", "Suhan Liu", "Peng Shi"], "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes", "categories": ["stat.ML", "cs.CY", "cs.LG", "q-fin.RM"], "comment": null, "summary": "Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.", "AI": {"tldr": "The paper addresses fairness in machine learning, focusing on insurance pricing, and proposes a method to ensure fairness without direct access to sensitive attributes.", "motivation": "Fairness in AI is critical, but insurance pricing has unique fairness challenges due to regulatory constraints and specialized definitions of fairness.", "method": "Proposes an efficient method for fair model construction in insurance, using privatized sensitive attributes and ensuring statistical guarantees.", "result": "The method adapts to transparency requirements, complies with regulations, and ensures fairness without direct access to sensitive data.", "conclusion": "The approach effectively balances fairness and regulatory demands in insurance pricing."}}
{"id": "2504.12121", "pdf": "https://arxiv.org/pdf/2504.12121", "abs": "https://arxiv.org/abs/2504.12121", "authors": ["Jose Francisco Diez-Pastor", "Francisco Javier Gonzalez-Moya", "Pedro Latorre-Carmona", "Francisco Javier Perez-Barber\u00eda", "Ludmila I. Kuncheva", "Antonio Canepa-Oneto", "Alvar Arnaiz-Gonz\u00e1lez", "Cesar Garcia-Osorio"], "title": "Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals", "categories": ["cs.CV"], "comment": "24 pages, 6 figures. Submitted to Computers and Geosciences", "summary": "Detection of spatial areas where biodiversity is at risk is of paramount\nimportance for the conservation and monitoring of ecosystems. Large terrestrial\nmammalian herbivores are keystone species as their activity not only has deep\neffects on soils, plants, and animals, but also shapes landscapes, as large\nherbivores act as allogenic ecosystem engineers. One key landscape feature that\nindicates intense herbivore activity and potentially impacts biodiversity is\nthe formation of grazing trails. Grazing trails are formed by the continuous\ntrampling activity of large herbivores that can produce complex networks of\ntracks of bare soil. Here, we evaluated different algorithms based on machine\nlearning techniques to identify grazing trails. Our goal is to automatically\ndetect potential areas with intense herbivory activity, which might be\nbeneficial for conservation and management plans.\n  We have applied five semantic segmentation methods combined with fourteen\nencoders aimed at mapping grazing trails on aerial images. Our results indicate\nthat in most cases the chosen methodology successfully mapped the trails,\nalthough there were a few instances where the actual trail structure was\nunderestimated. The UNet architecture with the MambaOut encoder was the best\narchitecture for mapping trails. The proposed approach could be applied to\ndevelop tools for mapping and monitoring temporal changes in these landscape\nstructures to support habitat conservation and land management programs. This\nis the first time, to the best of our knowledge, that competitive image\nsegmentation results are obtained for the detection and delineation of trails\nof large herbivorous mammals.", "AI": {"tldr": "The paper evaluates machine learning algorithms to detect grazing trails formed by large herbivores, identifying UNet with MambaOut as the best method for conservation and monitoring.", "motivation": "To detect areas of intense herbivore activity for ecosystem conservation and monitoring, focusing on grazing trails as indicators.", "method": "Applied five semantic segmentation methods combined with fourteen encoders on aerial images to map grazing trails.", "result": "UNet with MambaOut encoder performed best, successfully mapping trails, though some underestimation occurred.", "conclusion": "The approach can aid in mapping and monitoring trails for conservation, marking a first in competitive image segmentation for herbivore trail detection."}}
{"id": "2504.11792", "pdf": "https://arxiv.org/pdf/2504.11792", "abs": "https://arxiv.org/abs/2504.11792", "authors": ["Md Sultan Al Nahian", "Chris Delcher", "Daniel Harris", "Peter Akpunonu", "Ramakanth Kavuluru"], "title": "Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction.", "AI": {"tldr": "GPT-4o LLM outperforms traditional ML models in predicting drug overdose risk from insurance claims, even in zero-shot settings.", "motivation": "To leverage LLMs' ability to process long textual data and prior knowledge for improving drug overdose risk prediction.", "method": "Evaluate GPT-4o's performance in fine-tuned and zero-shot settings using longitudinal insurance claims records, comparing it to traditional ML models.", "result": "LLMs outperform traditional models in some settings and can predict risk without task-specific training.", "conclusion": "LLMs show promise for clinical decision support in drug overdose risk prediction."}}
{"id": "2504.12129", "pdf": "https://arxiv.org/pdf/2504.12129", "abs": "https://arxiv.org/abs/2504.12129", "authors": ["Songping Wang", "Yueming Lyu", "Shiqi Liu", "Ning Li", "Tong Tong", "Hao Sun", "Caifeng Shan"], "title": "Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright.", "AI": {"tldr": "The paper proposes a Hierarchical Anti-Aesthetic (HAA) framework to degrade the quality of maliciously customized diffusion models, protecting facial identity by disrupting aesthetics globally and locally.", "motivation": "Customized diffusion models pose risks to privacy and copyright. The paper addresses this by leveraging aesthetic degradation to hinder malicious use.", "method": "The HAA framework includes global and local anti-aesthetic branches, using reward mechanisms and losses to degrade image quality and disrupt facial identity.", "result": "HAA outperforms state-of-the-art methods in identity removal, effectively protecting facial privacy and copyright.", "conclusion": "The HAA framework provides a novel aesthetic-based solution to mitigate misuse of customized diffusion models, enhancing privacy and copyright protection."}}
{"id": "2504.11840", "pdf": "https://arxiv.org/pdf/2504.11840", "abs": "https://arxiv.org/abs/2504.11840", "authors": ["Huizhe Zhang", "Jintang Li", "Yuchang Zhu", "Liang Chen", "Zibin Zheng"], "title": "GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization", "categories": ["cs.NE", "cs.LG"], "comment": "work in progress", "summary": "Graph Transformers (GTs), which simultaneously integrate message-passing and\nself-attention mechanisms, have achieved promising empirical results in some\ngraph prediction tasks. Although these approaches show the potential of\nTransformers in capturing long-range graph topology information, issues\nconcerning the quadratic complexity and high computing energy consumption\nseverely limit the scalability of GTs on large-scale graphs. Recently, as\nbrain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the\ndevelopment of graph representation learning methods with lower computational\nand storage overhead through the unique event-driven spiking neurons. Inspired\nby these characteristics, we propose a linear-time Graph Transformer using\nSpiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ\nreconstructs codebooks based on rate coding outputs from spiking neurons, and\ninjects the codebooks into self-attention blocks to aggregate global\ninformation in linear complexity. Besides, spiking vector quantization\neffectively alleviates codebook collapse and the reliance on complex machinery\n(distance measure, auxiliary loss, etc.) present in previous vector\nquantization-based graph learning methods. In experiments, we compare GT-SVQ\nwith other state-of-the-art baselines on node classification datasets ranging\nfrom small to large. Experimental results show that GT-SVQ has achieved\ncompetitive performances on most datasets while maintaining up to 130x faster\ninference speed compared to other GTs.", "AI": {"tldr": "A linear-time Graph Transformer using Spiking Vector Quantization (GT-SVQ) is proposed for node classification, achieving competitive performance with faster inference speed.", "motivation": "Graph Transformers (GTs) face scalability issues due to quadratic complexity and high energy consumption. Spiking Neural Networks (SNNs) offer lower computational overhead, inspiring the integration of spiking neurons into GTs.", "method": "GT-SVQ reconstructs codebooks from spiking neuron outputs and injects them into self-attention blocks, enabling linear complexity. It also addresses codebook collapse and reduces reliance on complex machinery.", "result": "GT-SVQ achieves competitive performance on node classification tasks and is up to 130x faster in inference compared to other GTs.", "conclusion": "GT-SVQ effectively combines the strengths of GTs and SNNs, offering a scalable and efficient solution for graph representation learning."}}
{"id": "2504.12132", "pdf": "https://arxiv.org/pdf/2504.12132", "abs": "https://arxiv.org/abs/2504.12132", "authors": ["Linhao Qu", "Shiman Li", "Xiaoyuan Luo", "Shaolei Liu", "Qinhao Guo", "Manning Wang", "Zhijian Song"], "title": "Weakly Semi-supervised Whole Slide Image Classification by Two-level Cross Consistency Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Computer-aided Whole Slide Image (WSI) classification has the potential to\nenhance the accuracy and efficiency of clinical pathological diagnosis. It is\ncommonly formulated as a Multiple Instance Learning (MIL) problem, where each\nWSI is treated as a bag and the small patches extracted from the WSI are\nconsidered instances within that bag. However, obtaining labels for a large\nnumber of bags is a costly and time-consuming process, particularly when\nutilizing existing WSIs for new classification tasks. This limitation renders\nmost existing WSI classification methods ineffective. To address this issue, we\npropose a novel WSI classification problem setting, more aligned with clinical\npractice, termed Weakly Semi-supervised Whole slide image Classification\n(WSWC). In WSWC, a small number of bags are labeled, while a significant number\nof bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the\nabsence of patch labels, distinguishes it from typical semi-supervised image\nclassification problems, making existing algorithms for natural images\nunsuitable for directly solving the WSWC problem. In this paper, we present a\nconcise and efficient framework, named CroCo, to tackle the WSWC problem\nthrough two-level Cross Consistency supervision. CroCo comprises two\nheterogeneous classifier branches capable of performing both instance\nclassification and bag classification. The fundamental idea is to establish\ncross-consistency supervision at both the bag-level and instance-level between\nthe two branches during training. Extensive experiments conducted on four\ndatasets demonstrate that CroCo achieves superior bag classification and\ninstance classification performance compared to other comparative methods when\nlimited WSIs with bag labels are available. To the best of our knowledge, this\npaper presents for the first time the WSWC problem and gives a successful\nresolution.", "AI": {"tldr": "The paper introduces a weakly semi-supervised WSI classification (WSWC) problem and proposes CroCo, a framework using cross-consistency supervision to address it.", "motivation": "Existing WSI classification methods are ineffective due to the high cost of labeling large datasets, especially for new tasks. WSWC aligns better with clinical practice by using limited labeled data.", "method": "CroCo employs two heterogeneous classifier branches with cross-consistency supervision at bag and instance levels.", "result": "CroCo outperforms other methods in bag and instance classification on four datasets with limited labeled data.", "conclusion": "The paper successfully defines and solves the WSWC problem, offering a practical solution for clinical WSI classification."}}
{"id": "2504.11982", "pdf": "https://arxiv.org/pdf/2504.11982", "abs": "https://arxiv.org/abs/2504.11982", "authors": ["Alberto Bemporad", "Roland T\u00f3th"], "title": "Efficient identification of linear, parameter-varying, and nonlinear systems with noise models", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": "28 pages, 3 figures", "summary": "We present a general system identification procedure capable of estimating of\na broad spectrum of state-space dynamical models, including linear\ntime-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)\ndynamics, along with rather general classes of noise models. Similar to the LTI\ncase, we show that for this general class of model structures, including the NL\ncase, the model dynamics can be separated into a deterministic process and a\nstochastic noise part, allowing to seamlessly tune the complexity of the\ncombined model both in terms of nonlinearity and noise modeling. We\nparameterize the involved nonlinear functional relations by means of artificial\nneural-networks (ANNs), although alternative parametric nonlinear mappings can\nalso be used. To estimate the resulting model structures, we optimize a\nprediction-error-based criterion using an efficient combination of a\nconstrained quasi-Newton approach and automatic differentiation, achieving\ntraining times in the order of seconds compared to existing state-of-the-art\nANN methods which may require hours for models of similar complexity. We\nformally establish the consistency guarantees for the proposed approach and\ndemonstrate its superior estimation accuracy and computational efficiency on\nseveral benchmark LTI, LPV, and NL system identification problems.", "AI": {"tldr": "A system identification procedure for estimating diverse state-space dynamical models (LTI, LPV, NL) with general noise models, using ANNs for parameterization and achieving fast, accurate training.", "motivation": "To address the need for a versatile and efficient method for identifying a wide range of dynamical models, including nonlinear and noise-affected systems.", "method": "Parameterizes nonlinear functions with ANNs, optimizes a prediction-error criterion using a constrained quasi-Newton approach and automatic differentiation.", "result": "Achieves faster training (seconds vs. hours) and superior accuracy compared to state-of-the-art ANN methods.", "conclusion": "The proposed method is consistent, efficient, and accurate for LTI, LPV, and NL system identification."}}
{"id": "2504.12157", "pdf": "https://arxiv.org/pdf/2504.12157", "abs": "https://arxiv.org/abs/2504.12157", "authors": ["Xiaojun Ye", "Chun Wang", "Yiren Song", "Sheng Zhou", "Liangcheng Li", "Jiajun Bu"], "title": "FocusedAD: Character-centric Movie Audio Description", "categories": ["cs.CV", "I.2.10"], "comment": "Code and Demo link: https://github.com/Thorin215/FocusedAD", "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .", "AI": {"tldr": "FocusedAD is a novel framework for generating character-centric movie audio descriptions, addressing challenges like character identification and plot relevance. It outperforms benchmarks and includes tools for automated character query banks.", "motivation": "Movie Audio Description (AD) must narrate visual content for blind and visually impaired audiences, requiring plot-relevant narration with character names, which is challenging.", "method": "FocusedAD includes a Character Perception Module (CPM) for tracking characters, a Dynamic Prior Module (DPM) for contextual cues, and a Focused Caption Module (FCM) for enriched narrations. It also introduces an automated pipeline for character query banks.", "result": "FocusedAD achieves state-of-the-art performance on benchmarks like MAD-eval-Named and Cinepile-AD, including strong zero-shot results.", "conclusion": "FocusedAD effectively addresses the unique challenges of movie AD, offering a robust solution with released code and data."}}
{"id": "2504.12000", "pdf": "https://arxiv.org/pdf/2504.12000", "abs": "https://arxiv.org/abs/2504.12000", "authors": ["Thorben Markmann", "Michiel Straat", "Sebastian Peitz", "Barbara Hammer"], "title": "Control of Rayleigh-B\u00e9nard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "Data-driven flow control has significant potential for industry, energy\nsystems, and climate science. In this work, we study the effectiveness of\nReinforcement Learning (RL) for reducing convective heat transfer in the 2D\nRayleigh-B\\'enard Convection (RBC) system under increasing turbulence. We\ninvestigate the generalizability of control across varying initial conditions\nand turbulence levels and introduce a reward shaping technique to accelerate\nthe training. RL agents trained via single-agent Proximal Policy Optimization\n(PPO) are compared to linear proportional derivative (PD) controllers from\nclassical control theory. The RL agents reduced convection, measured by the\nNusselt Number, by up to 33% in moderately turbulent systems and 10% in highly\nturbulent settings, clearly outperforming PD control in all settings. The\nagents showed strong generalization performance across different initial\nconditions and to a significant extent, generalized to higher degrees of\nturbulence. The reward shaping improved sample efficiency and consistently\nstabilized the Nusselt Number to higher turbulence levels.", "AI": {"tldr": "RL outperforms PD control in reducing convective heat transfer in 2D RBC systems, showing strong generalization and improved efficiency with reward shaping.", "motivation": "To explore RL's effectiveness in reducing convective heat transfer in turbulent systems, comparing it to classical control methods.", "method": "Used single-agent PPO for RL, compared to linear PD controllers, with reward shaping to accelerate training.", "result": "RL reduced convection by up to 33% (moderate turbulence) and 10% (high turbulence), outperforming PD control and generalizing well.", "conclusion": "RL is effective for flow control in turbulent systems, with reward shaping enhancing training efficiency and performance."}}
{"id": "2504.12165", "pdf": "https://arxiv.org/pdf/2504.12165", "abs": "https://arxiv.org/abs/2504.12165", "authors": ["Yike Liu", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "CodingHomo: Bootstrapping Deep Homography With Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "Homography estimation is a fundamental task in computer vision with\napplications in diverse fields. Recent advances in deep learning have improved\nhomography estimation, particularly with unsupervised learning approaches,\noffering increased robustness and generalizability. However, accurately\npredicting homography, especially in complex motions, remains a challenge. In\nresponse, this work introduces a novel method leveraging video coding,\nparticularly by harnessing inherent motion vectors (MVs) present in videos. We\npresent CodingHomo, an unsupervised framework for homography estimation. Our\nframework features a Mask-Guided Fusion (MGF) module that identifies and\nutilizes beneficial features among the MVs, thereby enhancing the accuracy of\nhomography prediction. Additionally, the Mask-Guided Homography Estimation\n(MGHE) module is presented for eliminating undesired features in the\ncoarse-to-fine homography refinement process. CodingHomo outperforms existing\nstate-of-the-art unsupervised methods, delivering good robustness and\ngeneralizability. The code and dataset are available at:\n\\href{github}{https://github.com/liuyike422/CodingHomo", "AI": {"tldr": "CodingHomo is an unsupervised framework for homography estimation using video coding and motion vectors, outperforming existing methods with improved robustness and generalizability.", "motivation": "Accurate homography estimation in complex motions remains challenging despite advances in deep learning.", "method": "Leverages video coding and motion vectors (MVs), featuring Mask-Guided Fusion (MGF) and Mask-Guided Homography Estimation (MGHE) modules for enhanced accuracy.", "result": "Outperforms state-of-the-art unsupervised methods, offering good robustness and generalizability.", "conclusion": "CodingHomo provides a novel, effective approach for homography estimation, with publicly available code and dataset."}}
{"id": "2504.12051", "pdf": "https://arxiv.org/pdf/2504.12051", "abs": "https://arxiv.org/abs/2504.12051", "authors": ["Xhulja Shahini", "Jone Bartel", "Klaus Pohl"], "title": "On the calibration of Just-in-time Defect Prediction", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Just in time defect prediction (JIT DP) leverages ML to identify defect-prone\ncode commits, enabling quality assurance (QA) teams to allocate resources more\nefficiently by focusing on commits that are most likely to contain defects.\nAlthough JIT DP techniques have introduced improvements in terms of predictive\naccuracy, they are still susceptible to misclassification errors such as false\npositives and negatives. This can lead to wasted resources or undetected\ndefects, a particularly critical concern when QA resources are limited. To\nmitigate these challenges and preserve the practical utility of JIT DP tools,\nit becomes essential to estimate the reliability of the predictions, i.e.,\ncomputing confidence scores. Such scores can help practitioners determine the\ntrustworthiness of predictions and thus prioritize them efficiently. A simple\napproach to computing confidence scores is to extract, alongside each\nprediction, the corresponding prediction probabilities and use them as\nindicators of confidence. However, for these probabilities to reliably serve as\nconfidence scores, the predictive model must be well-calibrated. This means\nthat the prediction probabilities must accurately represent the true likelihood\nof each prediction being correct. Miscalibration, common in modern ML models,\ndistorts probability scores such that they do not align with the actual\ncorrectness probability. In this study, we evaluate the calibration of three\nJIT DP techniques to determine whether and to what extent they exhibit poor\ncalibration. Furthermore, we assess whether post-calibration methods can\nimprove the calibration of existing JIT defect prediction models. Our results\nreveal that all evaluated JIT DP models exhibit some level of miscalibration,\nwith ECE ranging from 2-35%. Furthermore, post-calibration methods do not\nconsistently improve the calibration.", "AI": {"tldr": "The paper evaluates the calibration of JIT defect prediction models, finding miscalibration issues and inconsistent improvements from post-calibration methods.", "motivation": "To address misclassification errors in JIT defect prediction by ensuring reliable confidence scores for predictions.", "method": "Evaluated calibration of three JIT DP techniques and tested post-calibration methods.", "result": "All models showed miscalibration (ECE 2-35%), with post-calibration not consistently effective.", "conclusion": "JIT DP models need better calibration methods to ensure reliable confidence scores."}}
{"id": "2504.12167", "pdf": "https://arxiv.org/pdf/2504.12167", "abs": "https://arxiv.org/abs/2504.12167", "authors": ["Yuan Luo", "Rudolf Hoffmann", "Yan Xia", "Olaf Wysocki", "Benedikt Schwab", "Thomas H. Kolbe", "Daniel Cremers"], "title": "RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning", "categories": ["cs.CV", "cs.LG"], "comment": "The paper accepted for CVPRW '25 (PBVS 2025 - the Perception Beyond\n  the Visible Spectrum)", "summary": "Semantic 3D city models are worldwide easy-accessible, providing accurate,\nobject-oriented, and semantic-rich 3D priors. To date, their potential to\nmitigate the noise impact on radar object detection remains under-explored. In\nthis paper, we first introduce a unique dataset, RadarCity, comprising 54K\nsynchronized radar-image pairs and semantic 3D city models. Moreover, we\npropose a novel neural network, RADLER, leveraging the effectiveness of\ncontrastive self-supervised learning (SSL) and semantic 3D city models to\nenhance radar object detection of pedestrians, cyclists, and cars.\nSpecifically, we first obtain the robust radar features via a SSL network in\nthe radar-image pretext task. We then use a simple yet effective feature fusion\nstrategy to incorporate semantic-depth features from semantic 3D city models.\nHaving prior 3D information as guidance, RADLER obtains more fine-grained\ndetails to enhance radar object detection. We extensively evaluate RADLER on\nthe collected RadarCity dataset and demonstrate average improvements of 5.46%\nin mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over\nprevious radar object detection methods. We believe this work will foster\nfurther research on semantic-guided and map-supported radar object detection.\nOur project page is publicly available\nathttps://gpp-communication.github.io/RADLER .", "AI": {"tldr": "The paper introduces RadarCity, a dataset with 54K radar-image pairs and 3D city models, and proposes RADLER, a neural network using contrastive SSL and 3D models to improve radar object detection.", "motivation": "To explore the underutilized potential of semantic 3D city models in mitigating noise for radar object detection.", "method": "Uses contrastive SSL for robust radar features and fuses semantic-depth features from 3D models to enhance detection.", "result": "RADLER improves mAP by 5.46% and mAR by 3.51% over prior methods.", "conclusion": "The work advances semantic-guided radar object detection and encourages further research in map-supported detection."}}
{"id": "2504.12063", "pdf": "https://arxiv.org/pdf/2504.12063", "abs": "https://arxiv.org/abs/2504.12063", "authors": ["Harrie Oosterhuis", "Rolf Jagerman", "Zhen Qin", "Xuanhui Wang"], "title": "Optimizing Compound Retrieval Systems", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "SIGIR 2025", "summary": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.", "AI": {"tldr": "The paper introduces compound retrieval systems, a broader class of retrieval systems that use multiple models, including LLMs, for better trade-offs between effectiveness and efficiency compared to cascading approaches.", "motivation": "Current cascading approaches in retrieval systems balance quality and computational costs but limit model interactions. The paper aims to explore broader interactions, especially with LLMs, for improved rankings.", "method": "Proposes compound retrieval systems, which learn where to apply component models and how to aggregate their predictions, combining BM25 with LLM relevance predictions.", "result": "Optimized compound systems outperform cascading approaches in effectiveness-efficiency trade-offs, even in self-supervised settings.", "conclusion": "The work encourages innovative thinking in model interactions for retrieval systems, showcasing the potential of compound approaches."}}
{"id": "2504.12186", "pdf": "https://arxiv.org/pdf/2504.12186", "abs": "https://arxiv.org/abs/2504.12186", "authors": ["Alejandro Newell", "Peiyun Hu", "Lahav Lipson", "Stephan R. Richter", "Vladlen Koltun"], "title": "CoMotion: Concurrent Multi-person 3D Motion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025, for code and weights go to\n  https://github.com/apple/ml-comotion", "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion", "AI": {"tldr": "A method for detecting and tracking 3D poses of multiple people from a monocular camera, handling occlusions and crowded scenes, with online tracking and state-of-the-art accuracy.", "motivation": "To address challenges in tracking detailed 3D poses in crowded, occluded scenes using a single camera.", "method": "Combines per-frame detection with a learned pose update for temporal coherence, avoiding detection matching across frames.", "result": "Achieves state-of-the-art accuracy in 3D pose estimation, with faster and more accurate multi-person tracking.", "conclusion": "The system is effective for real-time 3D pose tracking in complex scenarios, with publicly available code and weights."}}
{"id": "2504.12175", "pdf": "https://arxiv.org/pdf/2504.12175", "abs": "https://arxiv.org/abs/2504.12175", "authors": ["Yuling Jiao", "Yanming Lai", "Defeng Sun", "Yang Wang", "Bokai Yan"], "title": "Approximation Bounds for Transformer Networks with Application to Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We explore the approximation capabilities of Transformer networks for\nH\\\"older and Sobolev functions, and apply these results to address\nnonparametric regression estimation with dependent observations. First, we\nestablish novel upper bounds for standard Transformer networks approximating\nsequence-to-sequence mappings whose component functions are H\\\"older continuous\nwith smoothness index $\\gamma \\in (0,1]$. To achieve an approximation error\n$\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use\na fixed-depth Transformer network whose total number of parameters scales as\n$\\varepsilon^{-d_x n / \\gamma}$. This result not only extends existing findings\nto include the case $p = \\infty$, but also matches the best known upper bounds\non number of parameters previously obtained for fixed-depth FNNs and RNNs.\nSimilar bounds are also derived for Sobolev functions. Second, we derive\nexplicit convergence rates for the nonparametric regression problem under\nvarious $\\beta$-mixing data assumptions, which allow the dependence between\nobservations to weaken over time. Our bounds on the sample complexity impose no\nconstraints on weight magnitudes. Lastly, we propose a novel proof strategy to\nestablish approximation bounds, inspired by the Kolmogorov-Arnold\nrepresentation theorem. We show that if the self-attention layer in a\nTransformer can perform column averaging, the network can approximate\nsequence-to-sequence H\\\"older functions, offering new insights into the\ninterpretability of self-attention mechanisms.", "AI": {"tldr": "The paper analyzes Transformer networks' approximation capabilities for H\u00f6lder and Sobolev functions, extends results to nonparametric regression with dependent data, and introduces a novel proof strategy inspired by the Kolmogorov-Arnold theorem.", "motivation": "To understand and quantify the approximation power of Transformers for sequence-to-sequence mappings with H\u00f6lder and Sobolev functions, and apply this to nonparametric regression with dependent observations.", "method": "Establishes upper bounds for Transformers approximating H\u00f6lder and Sobolev functions, derives convergence rates for nonparametric regression under \u03b2-mixing data, and proposes a proof strategy using column averaging in self-attention.", "result": "Transformers achieve approximation error \u03b5 with parameter scaling \u03b5^(-d_x n / \u03b3), matching FNNs and RNNs. Explicit convergence rates are derived for regression, and self-attention's interpretability is enhanced.", "conclusion": "Transformers are effective for approximating H\u00f6lder and Sobolev functions, with applications in nonparametric regression. The proof strategy offers new insights into self-attention mechanisms."}}
{"id": "2504.12197", "pdf": "https://arxiv.org/pdf/2504.12197", "abs": "https://arxiv.org/abs/2504.12197", "authors": ["Mahdi Alehdaghi", "Rajarshi Bhattacharya", "Pourya Shamsolmoali", "Rafael M. O. Cruz", "Maguelonne Heritier", "Eric Granger"], "title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has provided considerable advancements for multimedia systems,\nyet the interpretability of deep models remains a challenge. State-of-the-art\npost-hoc explainability methods, such as GradCAM, provide visual interpretation\nbased on heatmaps but lack conceptual clarity. Prototype-based approaches, like\nProtoPNet and PIPNet, offer a more structured explanation but rely on fixed\npatches, limiting their robustness and semantic consistency.\n  To address these limitations, a part-prototypical concept mining network\n(PCMNet) is proposed that dynamically learns interpretable prototypes from\nmeaningful regions. PCMNet clusters prototypes into concept groups, creating\nsemantically grounded explanations without requiring additional annotations.\nThrough a joint process of unsupervised part discovery and concept activation\nvector extraction, PCMNet effectively captures discriminative concepts and\nmakes interpretable classification decisions.\n  Our extensive experiments comparing PCMNet against state-of-the-art methods\non multiple datasets show that it can provide a high level of interpretability,\nstability, and robustness under clean and occluded scenarios.", "AI": {"tldr": "PCMNet introduces a dynamic prototype-based method for interpretable deep learning, outperforming existing approaches in interpretability and robustness.", "motivation": "Addressing the lack of conceptual clarity and robustness in current explainability methods like GradCAM and prototype-based approaches.", "method": "PCMNet dynamically learns interpretable prototypes from meaningful regions, clustering them into concept groups without extra annotations.", "result": "PCMNet achieves high interpretability, stability, and robustness in clean and occluded scenarios, outperforming state-of-the-art methods.", "conclusion": "PCMNet offers a promising solution for interpretable deep learning by combining dynamic prototype learning and semantic grounding."}}
{"id": "2504.12189", "pdf": "https://arxiv.org/pdf/2504.12189", "abs": "https://arxiv.org/abs/2504.12189", "authors": ["Kiljae Lee", "Yuan Zhang"], "title": "Leave-One-Out Stable Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted at ICLR 2025", "summary": "Conformal prediction (CP) is an important tool for distribution-free\npredictive uncertainty quantification. Yet, a major challenge is to balance\ncomputational efficiency and prediction accuracy, particularly for multiple\npredictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),\na novel method to speed up full conformal using algorithmic stability without\nsample splitting. By leveraging leave-one-out stability, our method is much\nfaster in handling a large number of prediction requests compared to existing\nmethod RO-StabCP based on replace-one stability. We derived stability bounds\nfor several popular machine learning tools: regularized loss minimization (RLM)\nand stochastic gradient descent (SGD), as well as kernel method, neural\nnetworks and bagging. Our method is theoretically justified and demonstrates\nsuperior numerical performance on synthetic and real-world data. We applied our\nmethod to a screening problem, where its effective exploitation of training\ndata led to improved test power compared to state-of-the-art method based on\nsplit conformal.", "AI": {"tldr": "LOO-StabCP speeds up full conformal prediction using leave-one-out stability, outperforming RO-StabCP in efficiency and accuracy for multiple predictions.", "motivation": "Balancing computational efficiency and prediction accuracy in conformal prediction, especially for multiple predictions, is challenging.", "method": "Proposes Leave-One-Out Stable Conformal Prediction (LOO-StabCP), leveraging leave-one-out stability to avoid sample splitting and improve speed.", "result": "Theoretical stability bounds for RLM, SGD, kernel methods, neural networks, and bagging. Superior numerical performance on synthetic and real-world data.", "conclusion": "LOO-StabCP improves test power in screening problems by better utilizing training data compared to split conformal methods."}}
{"id": "2504.12215", "pdf": "https://arxiv.org/pdf/2504.12215", "abs": "https://arxiv.org/abs/2504.12215", "authors": ["Ilkin Sevgi Isler", "David Mohaisen", "Curtis Lisle", "Damla Turgut", "Ulas Bagci"], "title": "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 2 figures, to appear in IEEE ADSCA 2025", "summary": "Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing.", "AI": {"tldr": "A two-stage, uncertainty-guided framework improves tumor segmentation in thoracic CT by combining coarse localization with refined ROI segmentation and anatomical post-processing.", "motivation": "Challenges like boundary ambiguity, class imbalance, and anatomical variability make reliable tumor segmentation in thoracic CT difficult.", "method": "A coarse-to-fine approach: first-stage model localizes tumors, followed by anatomically informed filtering and second-stage segmentation with uncertainty-aware loss.", "result": "Improved Dice and Hausdorff scores, fewer false positives, and better spatial interpretability on private and public datasets (e.g., Dice improved from 0.4690 to 0.6447 on Orlando dataset).", "conclusion": "Combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines enhances robustness and clinical relevance of tumor delineation."}}
{"id": "2504.12210", "pdf": "https://arxiv.org/pdf/2504.12210", "abs": "https://arxiv.org/abs/2504.12210", "authors": ["Tingyang Sun", "Tuan Nguyen", "Ting He"], "title": "Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2408.04705", "summary": "Decentralized federated learning (DFL) is a promising machine learning\nparadigm for bringing artificial intelligence (AI) capabilities to the network\nedge. Running DFL on top of edge networks, however, faces severe performance\nchallenges due to the extensive parameter exchanges between agents. Most\nexisting solutions for these challenges were based on simplistic communication\nmodels, which cannot capture the case of learning over a multi-hop\nbandwidth-limited network. In this work, we address this problem by jointly\ndesigning the communication scheme for the overlay network formed by the agents\nand the mixing matrix that controls the communication demands between the\nagents. By carefully analyzing the properties of our problem, we cast each\ndesign problem into a tractable optimization and develop an efficient algorithm\nwith guaranteed performance. Our evaluations based on real topology and data\nshow that the proposed algorithm can reduce the total training time by over\n$80\\%$ compared to the baseline without sacrificing accuracy, while\nsignificantly improving the computational efficiency over the state of the art.", "AI": {"tldr": "The paper proposes a joint design of communication schemes and mixing matrices for decentralized federated learning (DFL) to improve performance in multi-hop bandwidth-limited edge networks, reducing training time by over 80% without accuracy loss.", "motivation": "DFL faces performance challenges due to extensive parameter exchanges in edge networks, and existing solutions lack realistic communication models for multi-hop bandwidth-limited scenarios.", "method": "The authors jointly design the communication scheme and mixing matrix, formulating each as a tractable optimization problem and developing an efficient algorithm with guaranteed performance.", "result": "Evaluations on real topology and data show an 80% reduction in total training time compared to baselines, with no accuracy loss and improved computational efficiency over state-of-the-art methods.", "conclusion": "The proposed approach effectively addresses DFL performance challenges in edge networks, offering significant efficiency gains while maintaining accuracy."}}
{"id": "2504.12222", "pdf": "https://arxiv.org/pdf/2504.12222", "abs": "https://arxiv.org/abs/2504.12222", "authors": ["Yike Liu", "Jianhui Zhang", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "Coding-Prior Guided Diffusion Network for Video Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "While recent video deblurring methods have advanced significantly, they often\noverlook two valuable prior information: (1) motion vectors (MVs) and coding\nresiduals (CRs) from video codecs, which provide efficient inter-frame\nalignment cues, and (2) the rich real-world knowledge embedded in pre-trained\ndiffusion generative models. We present CPGDNet, a novel two-stage framework\nthat effectively leverages both coding priors and generative diffusion priors\nfor high-quality deblurring. First, our coding-prior feature propagation (CPFP)\nmodule utilizes MVs for efficient frame alignment and CRs to generate attention\nmasks, addressing motion inaccuracies and texture variations. Second, a\ncoding-prior controlled generation (CPC) module network integrates coding\npriors into a pretrained diffusion model, guiding it to enhance critical\nregions and synthesize realistic details. Experiments demonstrate our method\nachieves state-of-the-art perceptual quality with up to 30% improvement in IQA\nmetrics. Both the code and the codingprior-augmented dataset will be\nopen-sourced.", "AI": {"tldr": "CPGDNet leverages coding priors (motion vectors, residuals) and diffusion generative models for superior video deblurring, achieving 30% better IQA metrics.", "motivation": "Existing methods ignore valuable coding priors and pre-trained diffusion models, limiting deblurring quality.", "method": "Two-stage framework: CPFP for alignment/attention using coding priors, and CPC to guide diffusion models for detail synthesis.", "result": "State-of-the-art perceptual quality with 30% improvement in IQA metrics.", "conclusion": "CPGDNet effectively combines coding and diffusion priors, setting a new benchmark in video deblurring."}}
{"id": "2504.12256", "pdf": "https://arxiv.org/pdf/2504.12256", "abs": "https://arxiv.org/abs/2504.12256", "authors": ["Andreas Plesner", "Turlan Kuzhagaliyev", "Roger Wattenhofer"], "title": "FLIP Reasoning Challenge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025", "summary": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.", "AI": {"tldr": "The paper introduces the FLIP dataset to evaluate AI reasoning, showing current models lag behind human performance.", "motivation": "To address the challenge of AI reasoning by creating a benchmark (FLIP) based on human verification tasks.", "method": "Uses the FLIP dataset with image-ordering tasks, testing VLMs and LLMs in zero-shot settings and ensembles.", "result": "Best models achieve 75.5%-77.9% accuracy vs. human 95.3%; ensembles improve to 85.2%.", "conclusion": "FLIP highlights AI reasoning gaps and the need for robust multimodal benchmarks."}}
{"id": "2504.12240", "pdf": "https://arxiv.org/pdf/2504.12240", "abs": "https://arxiv.org/abs/2504.12240", "authors": ["Junhao Zhuang", "Lingen Li", "Xuan Ju", "Zhaoyang Zhang", "Chun Yuan", "Ying Shan"], "title": "Cobra: Efficient Line Art COlorization with BRoAder References", "categories": ["cs.CV"], "comment": "Project page with code: https://zhuang2002.github.io/Cobra/", "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.", "AI": {"tldr": "Cobra is an efficient method for line art colorization in comics, using a Causal Sparse DiT architecture to handle extensive references and ensure consistency, improving speed and control.", "motivation": "The comic industry needs accurate, efficient, and flexible line art colorization, but existing diffusion models struggle with reference handling and latency.", "method": "Cobra employs a Causal Sparse DiT architecture with positional encodings, causal sparse attention, and Key-Value Cache to manage long-context references.", "result": "Cobra achieves high-quality colorization with over 200 references, enhancing speed and interactivity.", "conclusion": "Cobra meets industrial demands for efficient and controlled line art colorization, with released codes and models."}}
{"id": "2504.12284", "pdf": "https://arxiv.org/pdf/2504.12284", "abs": "https://arxiv.org/abs/2504.12284", "authors": ["Aditya Prakash", "Benjamin Lundell", "Dmitry Andreychuk", "David Forsyth", "Saurabh Gupta", "Harpreet Sawhney"], "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact", "summary": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.", "AI": {"tldr": "Predicting 3D hand motion and contact maps from RGB, text, and 3D contact points using a VQVAE and transformer-decoder model.", "motivation": "To address the novel problem of predicting interaction trajectories (hand motion and contact maps) from limited inputs (RGB, text, 3D contact point).", "method": "Uses (1) Interaction Codebook (VQVAE) to tokenize interaction trajectories, and (2) Interaction Predictor (transformer-decoder) to predict trajectories. Trained on HoloAssist dataset.", "result": "Outperforms transformer & diffusion baselines on a large, diverse benchmark, showing generalization across object/action categories, tasks, and scenes.", "conclusion": "The approach effectively predicts interaction trajectories, demonstrating robustness and scalability."}}
{"id": "2504.12259", "pdf": "https://arxiv.org/pdf/2504.12259", "abs": "https://arxiv.org/abs/2504.12259", "authors": ["Zhihang Yuan", "Rui Xie", "Yuzhang Shang", "Hanling Zhang", "Siyuan Wang", "Shengen Yan", "Guohao Dai", "Yu Wang"], "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformer(DiT)-based generation models have achieved remarkable\nsuccess in video generation. However, their inherent computational demands pose\nsignificant efficiency challenges. In this paper, we exploit the inherent\ntemporal non-uniformity of real-world videos and observe that videos exhibit\ndynamic information density, with high-motion segments demanding greater detail\npreservation than static scenes. Inspired by this temporal non-uniformity, we\npropose VGDFR, a training-free approach for Diffusion-based Video Generation\nwith Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements\nin latent space based on the motion frequency of the latent space content,\nusing fewer tokens for low-frequency segments while preserving detail in\nhigh-frequency segments. Specifically, our key contributions are: (1) A dynamic\nframe rate scheduler for DiT video generation that adaptively assigns frame\nrates for video segments. (2) A novel latent-space frame merging method to\nalign latent representations with their denoised counterparts before merging\nthose redundant in low-resolution space. (3) A preference analysis of Rotary\nPositional Embeddings (RoPE) across DiT layers, informing a tailored RoPE\nstrategy optimized for semantic and local information capture. Experiments show\nthat VGDFR can achieve a speedup up to 3x for video generation with minimal\nquality degradation.", "AI": {"tldr": "VGDFR introduces a training-free method for Diffusion-based Video Generation with Dynamic Latent Frame Rate, improving efficiency by adapting frame rates based on motion frequency.", "motivation": "Addressing the computational inefficiency of DiT-based video generation by leveraging temporal non-uniformity in videos.", "method": "Proposes a dynamic frame rate scheduler, latent-space frame merging, and optimized RoPE strategy.", "result": "Achieves up to 3x speedup in video generation with minimal quality loss.", "conclusion": "VGDFR effectively balances efficiency and quality in video generation by exploiting motion dynamics."}}
{"id": "2504.12292", "pdf": "https://arxiv.org/pdf/2504.12292", "abs": "https://arxiv.org/abs/2504.12292", "authors": ["Liam Schoneveld", "Zhe Chen", "Davide Davoli", "Jiapeng Tang", "Saimon Terazawa", "Ko Nishino", "Matthias Nie\u00dfner"], "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/", "summary": "Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.", "AI": {"tldr": "SHeaP improves 3D head reconstruction from monocular images using self-supervised learning with 2D Gaussians, outperforming existing methods in geometry and expression accuracy.", "motivation": "Accurate 3D head reconstruction from 2D data is challenging due to limited ground truth. Self-supervised learning from 2D videos is explored, but differentiable mesh rendering has limitations.", "method": "Predicts a 3DMM mesh and rigged Gaussians from an image, reanimates the avatar to match target frames, and backpropagates photometric losses to improve predictions.", "result": "Outperforms self-supervised methods on NoW benchmark for neutral faces and a new benchmark for expressions. Also excels in emotion classification.", "conclusion": "SHeaP's use of Gaussians enhances self-supervised 3D head reconstruction, achieving superior geometry and expression accuracy."}}
{"id": "2504.12264", "pdf": "https://arxiv.org/pdf/2504.12264", "abs": "https://arxiv.org/abs/2504.12264", "authors": ["Ayca Takmaz", "Cristiano Saltori", "Neehar Peri", "Tim Meinhardt", "Riccardo de Lutio", "Laura Leal-Taix\u00e9", "Aljo\u0161a O\u0161ep"], "title": "Towards Learning to Complete Anything in Lidar", "categories": ["cs.CV"], "comment": null, "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar", "AI": {"tldr": "CAL (Complete Anything in Lidar) is a zero-shot approach for Lidar-based shape completion, leveraging temporal context from multi-modal sensor sequences to mine object shapes and semantic features, enabling recognition beyond fixed class vocabularies.", "motivation": "Existing Lidar-based methods are limited to closed vocabularies, restricting their ability to complete and recognize objects in-the-wild. CAL aims to overcome this by using temporal context to mine object shapes and semantics.", "method": "The approach mines partial shape completions and semantic features from multi-modal sensor sequences, distilling them into a Lidar-only model for instance-level completion and recognition.", "result": "The distilled model infers full object shapes from partial observations and performs well on standard benchmarks for Semantic and Panoptic Scene Completion, localizing objects as 3D bounding boxes and recognizing objects beyond fixed vocabularies.", "conclusion": "CAL demonstrates the feasibility of zero-shot Lidar-based shape completion and recognition, extending capabilities beyond traditional closed-vocabulary methods."}}
{"id": "2504.12299", "pdf": "https://arxiv.org/pdf/2504.12299", "abs": "https://arxiv.org/abs/2504.12299", "authors": ["Marko Tot", "Shu Ishida", "Abdelhak Lemkhenter", "David Bignell", "Pallavi Choudhury", "Chris Lovett", "Luis Fran\u00e7a", "Matheus Ribeiro Furtado de Mendon\u00e7a", "Tarun Gupta", "Darren Gehring", "Sam Devlin", "Sergio Valcarcel Macua", "Raluca Georgescu"], "title": "Adapting a World Model for Trajectory Following in a 3D Game", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting.", "AI": {"tldr": "The paper explores using Inverse Dynamics Models (IDMs) with various encoders and policy heads for trajectory following in a 3D game, addressing distribution shift and stochasticity. It evaluates performance in diverse and low-data settings.", "motivation": "To improve imitation learning in complex environments like 3D games by addressing distribution shift and stochasticity, ensuring robust trajectory replication.", "method": "Applied IDMs with different encoders (e.g., DINOv2) and policy heads (GPT-style, MLP-style) in the game Bleeding Edge. Investigated future alignment strategies for mitigating aleatoric uncertainty.", "result": "GPT-style policy head with a scratch-trained encoder excelled in diverse data; DINOv2 with GPT-style performed best in low-data settings; both heads were comparable when pre-trained and fine-tuned.", "conclusion": "The optimal configuration for trajectory following depends on the data setting, with GPT-style heads showing versatility across scenarios."}}
{"id": "2504.12273", "pdf": "https://arxiv.org/pdf/2504.12273", "abs": "https://arxiv.org/abs/2504.12273", "authors": ["Zhuo He", "Paul Henderson", "Nicolas Pugeault"], "title": "Beyond Reconstruction: A Physics Based Neural Deferred Shader for Photo-realistic Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning based rendering has demonstrated major improvements for\nphoto-realistic image synthesis, applicable to various applications including\nvisual effects in movies and photo-realistic scene building in video games.\nHowever, a significant limitation is the difficulty of decomposing the\nillumination and material parameters, which limits such methods to reconstruct\nan input scene, without any possibility to control these parameters. This paper\nintroduces a novel physics based neural deferred shading pipeline to decompose\nthe data-driven rendering process, learn a generalizable shading function to\nproduce photo-realistic results for shading and relighting tasks, we also\nprovide a shadow estimator to efficiently mimic shadowing effect. Our model\nachieves improved performance compared to classical models and a state-of-art\nneural shading model, and enables generalizable photo-realistic shading from\narbitrary illumination input.", "AI": {"tldr": "A novel physics-based neural deferred shading pipeline is introduced to decompose illumination and material parameters, enabling photo-realistic shading and relighting with improved performance.", "motivation": "Overcome the limitation of decomposing illumination and material parameters in deep learning-based rendering, which restricts scene reconstruction control.", "method": "Proposes a physics-based neural deferred shading pipeline and a shadow estimator for efficient shadowing effects.", "result": "Achieves better performance than classical and state-of-the-art neural shading models, enabling generalizable photo-realistic shading from arbitrary illumination.", "conclusion": "The pipeline successfully decomposes rendering processes, offering control over shading and relighting with high realism."}}
{"id": "2504.12276", "pdf": "https://arxiv.org/pdf/2504.12276", "abs": "https://arxiv.org/abs/2504.12276", "authors": ["Lei Sun", "Hang Guo", "Bin Ren", "Luc Van Gool", "Radu Timofte", "Yawei Li", "Xiangyu Kong", "Hyunhee Park", "Xiaoxuan Yu", "Suejin Han", "Hakjae Jeon", "Jia Li", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Jingyu Ma", "Zhijuan Huang", "Huiyuan Fu", "Hongyuan Yu", "Boqi Zhang", "Jiawei Shi", "Heng Zhang", "Huadong Ma", "Deepak Kumar Tyagi", "Aman Kukretti", "Gajender Sharma", "Sriharsha Koundinya", "Asim Manna", "Jun Cheng", "Shan Tan", "Jun Liu", "Jiangwei Hao", "Jianping Luo", "Jie Lu", "Satya Narayan Tazi", "Arnim Gautam", "Aditi Pawar", "Aishwarya Joshi", "Akshay Dudhane", "Praful Hambadre", "Sachin Chaudhary", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Jiachen Tu", "Nikhil Akalwadi", "Vijayalaxmi Ashok Aralikatti", "Dheeraj Damodar Hegde", "G Gyaneshwar Rao", "Jatin Kalal", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Zhenyuan Lin", "Yubo Dong", "Weikun Li", "Anqi Li", "Ang Gao", "Weijun Yuan", "Zhan Li", "Ruting Deng", "Yihang Chen", "Yifan Deng", "Zhanglu Chen", "Boyang Yao", "Shuling Zheng", "Feng Zhang", "Zhiheng Fu", "Anas M. Ali", "Bilel Benjdira", "Wadii Boulila", "Jan Seny", "Pei Zhou", "Jianhua Hu", "K. L. Eddie Law", "Jaeho Lee", "M. J. Aashik Rasool", "Abdur Rehman", "SMA Sharif", "Seongwan Kim", "Alexandru Brateanu", "Raul Balmez", "Ciprian Orhei", "Cosmin Ancuti", "Zeyu Xiao", "Zhuoyuan Li", "Ziqi Wang", "Yanyan Wei", "Fei Wang", "Kun Li", "Shengeng Tang", "Yunkai Zhang", "Weirun Zhou", "Haoxuan Lu"], "title": "The Tenth NTIRE 2025 Image Denoising Challenge Report", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an overview of the NTIRE 2025 Image Denoising Challenge\n({\\sigma} = 50), highlighting the proposed methodologies and corresponding\nresults. The primary objective is to develop a network architecture capable of\nachieving high-quality denoising performance, quantitatively evaluated using\nPSNR, without constraints on computational complexity or model size. The task\nassumes independent additive white Gaussian noise (AWGN) with a fixed noise\nlevel of 50. A total of 290 participants registered for the challenge, with 20\nteams successfully submitting valid results, providing insights into the\ncurrent state-of-the-art in image denoising.", "AI": {"tldr": "Overview of NTIRE 2025 Image Denoising Challenge (\u03c3=50), focusing on methodologies, results, and state-of-the-art insights.", "motivation": "Develop a high-quality denoising network architecture evaluated by PSNR, without computational or size constraints.", "method": "Participants proposed solutions for denoising AWGN (\u03c3=50), with 20 teams submitting valid results.", "result": "290 participants registered; 20 teams provided insights into current state-of-the-art denoising.", "conclusion": "The challenge showcased advancements in image denoising, highlighting top-performing methodologies."}}
{"id": "2504.11493", "pdf": "https://arxiv.org/pdf/2504.11493", "abs": "https://arxiv.org/abs/2504.11493", "authors": ["Azizul Zahid", "Jie Fan", "Farong Wang", "Ashton Dy", "Sai Swaminathan", "Fei Liu"], "title": "Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "ICRA'25 Workshop: Human-Centered Robot Learning in the Era of Big\n  Data and Large Models", "summary": "Understanding action correspondence between humans and robots is essential\nfor evaluating alignment in decision-making, particularly in human-robot\ncollaboration and imitation learning within unstructured environments. We\npropose a multimodal demonstration learning framework that explicitly models\nhuman demonstrations from RGB video with robot demonstrations in voxelized\nRGB-D space. Focusing on the \"pick and place\" task from the RH20T dataset, we\nutilize data from 5 users across 10 diverse scenes. Our approach combines\nResNet-based visual encoding for human intention modeling and a Perceiver\nTransformer for voxel-based robot action prediction. After 2000 training\nepochs, the human model reaches 71.67% accuracy, and the robot model achieves\n71.8% accuracy, demonstrating the framework's potential for aligning complex,\nmultimodal human and robot behaviors in manipulation tasks.", "AI": {"tldr": "A multimodal framework aligns human and robot actions in pick-and-place tasks, achieving ~71% accuracy for both models.", "motivation": "To evaluate alignment in human-robot decision-making for collaboration and imitation learning in unstructured environments.", "method": "Uses RGB video for human demonstrations and voxelized RGB-D space for robot demonstrations, combining ResNet for human intention and Perceiver Transformer for robot action prediction.", "result": "Human model: 71.67% accuracy; robot model: 71.8% accuracy after 2000 training epochs.", "conclusion": "The framework shows promise for aligning complex, multimodal human-robot behaviors in manipulation tasks."}}
{"id": "2504.11509", "pdf": "https://arxiv.org/pdf/2504.11509", "abs": "https://arxiv.org/abs/2504.11509", "authors": ["Wenyi Zhang", "Ju Jia", "Xiaojun Jia", "Yihao Huang", "Xinfeng Li", "Cong Wu", "Lina Wang"], "title": "PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.", "AI": {"tldr": "Proposes PATFinger, a training-free fingerprinting scheme for verifying multimodal dataset usage, outperforming baselines by 30%.", "motivation": "Addresses gaps in cross-modal dataset verification, avoiding intrusive methods that degrade accuracy and non-intrusive methods with unstable behaviors.", "method": "Uses global optimal perturbation (GOP) and adaptive prompts to capture dataset-specific traits without model triggers.", "result": "PATFinger improves verification accuracy by 30% over existing methods.", "conclusion": "PATFinger offers a robust, training-free solution for cross-modal dataset verification."}}
{"id": "2504.11674", "pdf": "https://arxiv.org/pdf/2504.11674", "abs": "https://arxiv.org/abs/2504.11674", "authors": ["Sicong Pan", "Liren Jin", "Xuying Huang", "Cyrill Stachniss", "Marija Popovi\u0107", "Maren Bennewitz"], "title": "DM-OSVP++: One-Shot View Planning Using 3D Diffusion Models for Active RGB-Based Object Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Active object reconstruction is crucial for many robotic applications. A key\naspect in these scenarios is generating object-specific view configurations to\nobtain informative measurements for reconstruction. One-shot view planning\nenables efficient data collection by predicting all views at once, eliminating\nthe need for time-consuming online replanning. Our primary insight is to\nleverage the generative power of 3D diffusion models as valuable prior\ninformation. By conditioning on initial multi-view images, we exploit the\npriors from the 3D diffusion model to generate an approximate object model,\nserving as the foundation for our view planning. Our novel approach integrates\nthe geometric and textural distributions of the object model into the view\nplanning process, generating views that focus on the complex parts of the\nobject to be reconstructed. We validate the proposed active object\nreconstruction system through both simulation and real-world experiments,\ndemonstrating the effectiveness of using 3D diffusion priors for one-shot view\nplanning.", "AI": {"tldr": "The paper proposes a one-shot view planning method for active object reconstruction using 3D diffusion models as priors, improving efficiency by eliminating online replanning.", "motivation": "Active object reconstruction is essential for robotics, but generating informative views efficiently is challenging.", "method": "Leverages 3D diffusion models conditioned on initial multi-view images to generate an approximate object model, integrating geometric and textural distributions for view planning.", "result": "Validated in simulations and real-world experiments, showing effectiveness of 3D diffusion priors for one-shot view planning.", "conclusion": "The approach successfully integrates 3D diffusion priors to enhance active object reconstruction efficiency."}}
{"id": "2504.11698", "pdf": "https://arxiv.org/pdf/2504.11698", "abs": "https://arxiv.org/abs/2504.11698", "authors": ["Xingwu Ji", "Haochen Niu", "Dexin Duan", "Rendong Ying", "Fei Wen", "Peilin Liu"], "title": "An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World", "categories": ["cs.RO", "cs.CV"], "comment": "11 pages, 14 figures", "summary": "Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM.", "AI": {"tldr": "A self-supervised online adaptation framework for monocular visual odometry is proposed to improve generalization in diverse environments.", "motivation": "The diversity of open-world scenarios challenges the generalization of learned robotic navigation systems, leading to unreliable depth and pose estimation in novel environments.", "method": "The framework includes a lightweight depth estimation network with refiner modules, a self-supervised learning objective, and modules for sparse depth densification and dynamic consistency enhancement.", "result": "The method demonstrates robustness and generalization on urban, in-house datasets and a robot platform, outperforming state-of-the-art approaches.", "conclusion": "The proposed system effectively adapts to novel environments online, enhancing the reliability of visual odometry in diverse scenarios."}}
{"id": "2504.11734", "pdf": "https://arxiv.org/pdf/2504.11734", "abs": "https://arxiv.org/abs/2504.11734", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Recent Advance in 3D Object and Scene Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "34 pages, 6 figures", "summary": "In recent years, the demand for 3D content has grown exponentially with\nintelligent upgrading of interactive media, extended reality (XR), and\nMetaverse industries. In order to overcome the limitation of traditional manual\nmodeling approaches, such as labor-intensive workflows and prolonged production\ncycles, revolutionary advances have been achieved through the convergence of\nnovel 3D representation paradigms and artificial intelligence generative\ntechnologies. In this survey, we conduct a systematically review of the\ncutting-edge achievements in static 3D object and scene generation, as well as\nestablish a comprehensive technical framework through systematic\ncategorization. Specifically, we initiate our analysis with mainstream 3D\nobject representations, followed by in-depth exploration of two principal\ntechnical pathways in object generation: data-driven supervised learning\nmethods and deep generative model-based approaches. Regarding scene generation,\nwe focus on three dominant paradigms: layout-guided compositional synthesis, 2D\nprior-based scene generation, and rule-driven modeling. Finally, we critically\nexamine persistent challenges in 3D generation and propose potential research\ndirections for future investigation. This survey aims to provide readers with a\nstructured understanding of state-of-the-art 3D generation technologies while\ninspiring researchers to undertake more exploration in this domain.", "AI": {"tldr": "A survey on 3D content generation, covering object and scene creation methods, challenges, and future directions.", "motivation": "Address the limitations of manual 3D modeling by leveraging AI and novel 3D representations to meet growing industry demands.", "method": "Systematic review of static 3D object and scene generation, categorizing techniques like supervised learning, generative models, layout-guided synthesis, and rule-driven modeling.", "result": "Comprehensive framework for 3D generation technologies, highlighting advancements and persistent challenges.", "conclusion": "Provides structured insights into current 3D generation methods and suggests future research directions to inspire further exploration."}}
