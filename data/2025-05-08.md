<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.CV](#cs.CV) [Total: 113]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 8]
- [eess.IV](#eess.IV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/pdf/2505.03788)
*Trilok Padhi, Ramneet Kaur, Adam D. Cobb, Manoj Acharya, Anirban Roy, Colin Samplawski, Brian Matejek, Alexander M. Berenbeim, Nathaniel D. Bastian, Susmit Jha*

Main category: cs.CL

TL;DR: A novel method improves uncertainty calibration in multi-modal LLMs by using cross-modal consistency and temperature scaling, achieving better results on tasks like medical and visual question answering.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods for LLMs often report high confidence even when incorrect, leading to poor calibration. This work addresses this by leveraging cross-modal consistency.

Method: The approach grounds textual responses to visual inputs and uses a grounding model's confidence, calibrated via temperature scaling, to improve overall uncertainty calibration.

Result: Experiments on tasks like Slake and VQAv2 show significantly improved calibration with models like LLaVA-Med and LLaVA.

Conclusion: The proposed framework effectively enhances uncertainty calibration in multi-modal LLMs, validated across diverse tasks.

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


### [2] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/pdf/2505.03910)
*Gianluca Manzo, Julia Ive*

Main category: cs.CL

TL;DR: The paper explores using Bayesian Deep Learning to align predictive uncertainty in chest radiograph interpretation with human linguistic uncertainty, finding modest correlation and highlighting the need for refinement.


<details>
  <summary>Details</summary>
Motivation: To improve clinical workflows and decision-making by aligning machine uncertainty estimates with human uncertainty in medical imaging.

Method: Uses BERT, Monte Carlo Dropout, and Deep Ensembles to evaluate predictive uncertainty and compares it with rule-based linguistic uncertainty from radiology reports.

Result: Demonstrates good model performance but only a modest correlation between predictive and linguistic uncertainty.

Conclusion: Bayesian approximations provide useful uncertainty estimates, but further refinement is needed to better align with human uncertainty nuances in clinical settings.

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>


### [3] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/pdf/2505.03970)
*Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, Daniel E. Ho*

Main category: cs.CL

TL;DR: The paper introduces two legal RAG benchmarks (Bar Exam QA and Housing Statute QA) to address the lack of realistic benchmarks for legal AI systems, highlighting the challenges in legal RAG applications.


<details>
  <summary>Details</summary>
Motivation: The lack of realistic benchmarks for legal retrieval-augmented LLMs (RAG systems) hinders development. The paper aims to fill this gap by creating specialized benchmarks.

Method: Two novel legal RAG benchmarks were developed (Bar Exam QA and Housing Statute QA) using annotation processes mimicking legal research. Existing retriever pipelines were evaluated.

Result: Legal RAG systems face significant challenges, as shown by the performance of existing pipelines on the new benchmarks.

Conclusion: The benchmarks highlight the difficulty of legal RAG tasks, motivating further research in this area.

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>


### [4] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/pdf/2505.03973)
*Jiale Liu, Yifan Zeng, Shaokun Zhang, Chi Zhang, Malte HÃ¸jmark-Bertelsen, Marie Normann Gadeberg, Huazheng Wang, Qingyun Wu*

Main category: cs.CL

TL;DR: FGO (Fine-Grained Optimization) is a scalable framework for LLM-based optimization, dividing tasks into subsets to avoid context overflow and improve efficiency, outperforming existing methods by 1.6-8.6%.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of conventional LLM optimization, which struggles with large datasets due to context window overflow and degraded performance.

Method: Proposes FGO, which breaks large tasks into subsets, performs targeted optimizations, and merges results progressively.

Result: FGO outperforms existing methods by 1.6-8.6% and reduces prompt token usage by 56.3%, showing consistent gains across dataset sizes.

Conclusion: FGO provides a scalable and efficient solution for LLM-based optimization in agent systems.

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>


### [5] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/pdf/2505.03981)
*Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon*

Main category: cs.CL

TL;DR: X-Reasoner, a vision-language model post-trained on general-domain text, demonstrates generalizable reasoning across modalities and domains, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Proprietary models show strong multimodal reasoning, but open-source research focuses on text-only models. This paper explores if reasoning generalizes across modalities and domains.

Method: Two-stage approach: supervised fine-tuning with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards.

Result: X-Reasoner transfers reasoning to multimodal and out-of-domain settings, outperforming existing models. Continued training enhances specialized performance, leading to X-Reasoner-Med.

Conclusion: General-domain text-based post-training enables strong generalizable reasoning, with potential for domain-specific enhancement.

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>


### [6] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/pdf/2505.04016)
*Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding*

Main category: cs.CL

TL;DR: SLOT (Structured LLM Output Transformer) is a model-agnostic method to convert unstructured LLM outputs into structured formats, outperforming existing solutions in schema accuracy and content fidelity.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce outputs that don't adhere to predefined schemas, limiting their reliability in critical applications like agents and information extraction.

Method: SLOT uses a fine-tuned lightweight language model as a post-processing layer, combined with a systematic data curation pipeline and constrained decoding.

Result: Fine-tuned Mistral-7B with SLOT achieves 99.5% schema accuracy and 94.0% content similarity, surpassing Claude-3.5-Sonnet by significant margins. Compact models like Llama-3.2-1B also perform well.

Conclusion: SLOT enables reliable structured output generation across various LLMs, even in resource-constrained settings, outperforming proprietary models.

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>


### [7] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/pdf/2505.04072)
*Xu Huang, Yuefeng Huang, Weiwen Liu, Xingshan Zeng, Yasheng Wang, Ruiming Tang, Hong Xie, Defu Lian*

Main category: cs.CL

TL;DR: The paper introduces Personalized Tool Invocation (PTool) for LLMs, addressing user preferences and profile-dependent queries, and presents PTBench for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM tool invocation lacks personalization, ignoring user preferences and profile-based parameter inference.

Method: Proposes PTool, a data synthesis framework, and PTBench, a benchmark for personalized tool invocation. Fine-tunes open-source models.

Result: Demonstrates PTool's effectiveness and provides insights through model fine-tuning.

Conclusion: PTool and PTBench advance personalized tool invocation, with the benchmark publicly available.

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>


### [8] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/pdf/2505.04073)
*Mengxian Lyu, Xiaohan Li, Ziyi Chen, Jinqian Pan, Cheng Peng, Sankalp Talankar, Yonghui Wu*

Main category: cs.CL

TL;DR: A systematic review of 113 NLG-related medical publications, covering methods, applications, and challenges in healthcare.


<details>
  <summary>Details</summary>
Motivation: To address the need for a comprehensive review of NLG methods and applications in the medical domain, given the growing use of NLG in healthcare.

Method: Analyzed 113 publications from 3,988 NLG-related articles, focusing on data modality, model architecture, clinical applications, and evaluation methods, following PRISMA guidelines.

Result: Identified key NLG technologies, categorized clinical applications, and assessed capabilities, limitations, and challenges in medical NLG.

Conclusion: The review provides insights for future studies to leverage NLG for transforming medical discovery and healthcare.

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>


### [9] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/pdf/2505.04132)
*Mingruo Yuan, Ben Kao, Tien-Hsuan Wu, Michael M. K. Cheung, Henry W. H. Chan, Anne S. Y. Cheung, Felix W. H. Chan, Yongxi Chen*

Main category: cs.CL

TL;DR: The paper proposes a three-step method to make legal information accessible to laypersons by translating legal documents into simplified snippets, creating a Legal Question Bank (LQB), and designing an interactive recommender system. It highlights the use of GPT-3 for generating scalable legal questions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of making technical legal documents comprehensible and navigable for non-experts, ensuring better access to justice.

Method: A three-step approach: 1) Translate legal sections into layperson-friendly snippets (CLIC-pages), 2) Build a Legal Question Bank (LQB) using GPT-3 for scalable question generation, and 3) Develop an interactive CLIC Recommender (CRec) to link user queries to relevant CLIC-pages.

Result: Machine-generated questions (MGQs) are more scalable and diversified than human-composed questions (HCQs), though HCQs are more precise. A prototype of CRec demonstrates the approach's effectiveness.

Conclusion: The proposed method successfully bridges the gap between legal documents and laypersons, leveraging AI for scalability while maintaining precision through human input.

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [10] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/pdf/2505.04135)
*Vihaan Miriyala, Smrithi Bukkapatnam, Lavanya Prahallad*

Main category: cs.CL

TL;DR: CoT prompting with LLMs boosts sentiment categorization accuracy from 84% to 93% in app reviews.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis methods lack nuance in capturing user feedback.

Method: Compared CoT prompting vs. simple prompting on 2000 Amazon app reviews, validated against human judgments.

Result: CoT prompting improved accuracy from 84% to 93%.

Conclusion: Explicit reasoning via CoT enhances sentiment analysis performance.

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [11] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/pdf/2505.04146)
*Variath Madhupal Gautham Nair, Vishal Varma Dantuluri*

Main category: cs.CL

TL;DR: The paper introduces UTCB, a benchmark to test LLM vulnerabilities in image generation, revealing prompt-based jailbreaks and proposing a scalable evaluation method.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack robust content safety checks, making them susceptible to prompt-based jailbreaks, as shown by preliminary tests on platforms like ChatGPT and MetaAI.

Method: The UTCB benchmark uses structured prompt engineering, multilingual obfuscation, and evaluation with LLaMA-3, supporting zero-shot and fallback strategies, risk scoring, and automated tagging.

Result: The benchmark identifies vulnerabilities in LLMs, generating compromising images from simple prompts, and categorizes results into Bronze, Silver, and Gold tiers for verification.

Conclusion: UTCB provides a dynamic, scalable tool to evaluate and improve LLM safety in image generation, with plans for future updates to address evolving threats.

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>


### [12] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/pdf/2505.04152)
*Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel*

Main category: cs.CL

TL;DR: The paper explores using LLMs to analyze social signals in clinical dialogues, evaluating performance across architectures and prompting styles, and presents a system tracking 20 social signals.


<details>
  <summary>Details</summary>
Motivation: Effective patient-provider communication impacts health outcomes, and LLMs offer potential to automate analysis of social behaviors in clinical settings.

Method: Task-specific prompts were designed, and model performance was evaluated using an imbalanced annotated dataset of 20 social signals.

Result: The study presents the first system tracking all 20 social signals and identifies patterns in LLM behavior, offering insights for improving performance.

Conclusion: LLMs show promise for social signal processing in healthcare, with further analysis needed to optimize their use in clinical contexts.

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>


### [13] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/pdf/2505.04253)
*Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: Lightweight LLM-independent adaptive retrieval methods using external information match LLM-based performance with efficiency gains.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations, and RAG is costly and risky. Adaptive retrieval needs efficient, practical solutions beyond LLM-based uncertainty estimation.

Method: Introduced lightweight adaptive retrieval methods using 27 features grouped into 7 categories, evaluated on 6 QA datasets.

Result: Matched LLM-based performance while significantly improving efficiency.

Conclusion: External information can effectively enable adaptive retrieval without relying on LLMs.

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>


### [14] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/pdf/2505.04284)
*Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur*

Main category: cs.CL

TL;DR: The paper introduces a novel framework (GASCADE) and dataset (MCADRS) for summarizing adverse drug events in cancer treatment, leveraging LLMs and T5 models with alignment techniques for superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap in pharmacovigilance research focused on cancer, improving drug-related decision-making by summarizing patient-reported adverse drug events.

Method: Proposes the GASCADE framework, combining LLMs for information extraction and T5 for summarization, using the MCADRS dataset with alignment techniques like Direct Preference Optimization.

Result: GASCADE outperforms in experiments, validated by automated and human evaluations, enhancing drug decision-making and patient understanding.

Conclusion: The work advances personalized cancer care by improving ADE summarization, with publicly available code and dataset.

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>


### [15] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/pdf/2505.04388)
*Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard AyguadÃ©-Parra, Ulises CortÃ©s*

Main category: cs.CL

TL;DR: The paper introduces Aloe Beta, an open-source medical LLM, optimizing data preprocessing and training for safety and efficacy, setting a new evaluation standard and outperforming private alternatives.


<details>
  <summary>Details</summary>
Motivation: To advance open-source medical LLMs, ensuring public interest protection through improved safety, efficacy, and ethical alignment.

Method: Uses base models (Llama 3.1, Qwen 2.5) with synthetic data, Direct Preference Optimization (DPO) for alignment, and rigorous evaluation (four test types).

Result: Aloe Beta models excel in healthcare benchmarks, safety, and bias reduction, preferred by professionals and resilient to attacks.

Conclusion: Aloe Beta and its methodology advance open-source medical LLMs, setting new standards for ethical, high-performance models in healthcare.

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [16] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/pdf/2505.04393)
*David Exler, Mark Schutera, Markus Reischl, Luca Rettenberger*

Main category: cs.CL

TL;DR: The paper quantifies political bias in large language models (LLMs) using German Bundestag voting data, revealing left-leaning biases, influenced by model size, language, origin, and release date.


<details>
  <summary>Details</summary>
Motivation: To evaluate and address inherent biases in LLMs, which can spread misinformation and influence opinions, especially in political contexts.

Method: Quantified political bias using the Wahl-O-Mat score, comparing LLMs' alignment with German political parties and analyzing influencing factors.

Result: LLMs exhibit left-leaning biases, more pronounced in larger models. Language, origin, and release date also affect bias.

Conclusion: LLMs are prone to political bias, and developers must mitigate these biases to prevent undue influence on public opinion and decision-making.

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>


### [17] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/pdf/2505.04406)
*Aidar Valeev, Roman Garaev, Vadim Lomshakov, Irina Piontkovskaya, Vladimir Ivanov, Israel Adewuyi*

Main category: cs.CL

TL;DR: The paper introduces YABLoCo, a benchmark for evaluating LLMs on large-context code generation in C/C++ repositories with up to 2M LoC.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on small/medium code contexts, while real-world projects involve much larger repositories.

Method: The benchmark includes 215 functions from large repositories, with metadata, dependencies, docstrings, and call graphs.

Result: YABLoCo evaluates function body generation in C/C++ and provides a scalable pipeline and visualization tool.

Conclusion: The benchmark addresses the gap in evaluating LLMs for large-scale code generation in real-world projects.

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>


### [18] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/pdf/2505.04416)
*Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu*

Main category: cs.CL

TL;DR: OBLIVIATE is a framework for unlearning sensitive data in LLMs, preserving utility via token extraction, retain sets, and a tailored loss function with LoRA for efficiency.


<details>
  <summary>Details</summary>
Motivation: To mitigate risks of memorizing sensitive, copyrighted, or toxic content in LLMs.

Method: Extracts target tokens, builds retain sets, and fine-tunes with a loss function (masking, distillation, world fact) using LoRA.

Result: Effective in resisting membership inference attacks, minimizing impact on retained data, and maintaining robustness.

Conclusion: OBLIVIATE successfully unlearns targeted data while preserving model utility and efficiency.

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>


### [19] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/pdf/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TL;DR: The paper proposes automated linguistic anomaly detection to improve training dataset quality for generative models in creative tasks like poetry, comparing unsupervised and supervised methods and introducing the RUPOR dataset.


<details>
  <summary>Details</summary>
Motivation: Fluency defects in generated poems reduce their value, and training texts often lack quality control, necessitating better methods to manage dataset quality.

Method: The study compares unsupervised and supervised text anomaly detection approaches using synthetic and human-labeled datasets, including the newly introduced RUPOR dataset for Russian poems.

Result: The paper provides tools and insights, such as the RUPOR dataset and evaluation code, to enhance training dataset quality for creative generative models.

Conclusion: Automated linguistic anomaly detection can effectively improve dataset quality, benefiting generative models in creative domains.

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>


### [20] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/pdf/2505.04519)
*Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan*

Main category: cs.CL

TL;DR: The paper presents a method to efficiently train sparse large language models (LLMs) with Mixture of Experts (MoE) on Ascend NPUs, achieving high performance and resource utilization.


<details>
  <summary>Details</summary>
Motivation: The massive scale of trillion-parameter sparse LLMs poses challenges for software and hardware systems, motivating the need for efficient training methods on Ascend NPUs.

Method: The study leverages simulation to compare model hyperparameters, optimizes Expert Parallelism for communication, and improves memory efficiency. This leads to the development of Pangu Ultra MoE (718B parameters).

Result: Achieved 30.0% MFU on 6K Ascend NPUs, with performance comparable to DeepSeek R1, demonstrating efficient training of large-scale sparse LLMs.

Conclusion: The proposed recipe enables efficient training of MoE-based LLMs on Ascend NPUs, with insights for future model development.

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>


### [21] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/pdf/2505.04531)
*Josh McGiff, Nikola S. Nikolov*

Main category: cs.CL

TL;DR: A systematic review of strategies to address data scarcity in generative language modeling for low-resource languages, highlighting trends, challenges, and recommendations for equitable AI tools.


<details>
  <summary>Details</summary>
Motivation: To tackle linguistic inequality in NLP by focusing on low-resource languages, which are often overlooked in generative language modeling.

Method: Analyzed 54 studies to categorize and evaluate technical approaches like data augmentation, back-translation, multilingual training, and prompt engineering.

Result: Found reliance on transformer models, limited language coverage, and inconsistent evaluation methods.

Conclusion: Recommends broader language inclusion and outlines open challenges for equitable generative systems to support underrepresented languages.

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>


### [22] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/pdf/2505.04588)
*Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang*

Main category: cs.CL

TL;DR: ZeroSearch is a reinforcement learning framework that improves LLMs' search capabilities without real search engine interaction, addressing document quality and API cost issues.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of uncontrolled document quality and high API costs in RL-based search training for LLMs.

Method: Uses lightweight supervised fine-tuning to create a retrieval module, followed by curriculum-based RL training with degrading document quality.

Result: Achieves comparable or superior performance to real search engines with 3B, 7B, and 14B LLMs, and generalizes well across models.

Conclusion: ZeroSearch is a scalable and cost-effective solution for enhancing LLMs' search capabilities.

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


### [23] [Playing repeated games with Large Language Models](https://arxiv.org/pdf/2305.16867)
*Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz*

Main category: cs.CL

TL;DR: The paper uses behavioral game theory to study LLMs' cooperation and coordination in repeated games, showing strong performance in self-interested games but sub-optimality in coordination games. GPT-4's behavior can be modulated for better human interaction.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' social behavior in interactive settings using game theory, focusing on cooperation and coordination.

Method: LLMs play finitely repeated 2Ã2 games (e.g., Prisonerâs Dilemma, Battle of the Sexes) with other LLMs, human-like strategies, and actual humans.

Result: LLMs excel in self-interested games but struggle in coordination games. GPT-4's behavior improves with opponent info and social chain-of-thought (SCoT) strategies.

Conclusion: The study advances understanding of LLMs' social behavior and lays groundwork for machine behavioral game theory.

Abstract: LLMs are increasingly used in applications where they interact with humans
and other agents. We propose to use behavioural game theory to study LLM's
cooperation and coordination behaviour. We let different LLMs play finitely
repeated $2\times2$ games with each other, with human-like strategies, and
actual human players. Our results show that LLMs perform particularly well at
self-interested games like the iterated Prisoner's Dilemma family. However,
they behave sub-optimally in games that require coordination, like the Battle
of the Sexes. We verify that these behavioural signatures are stable across
robustness checks. We additionally show how GPT-4's behaviour can be modulated
by providing additional information about its opponent and by using a "social
chain-of-thought" (SCoT) strategy. This also leads to better scores and more
successful coordination when interacting with human players. These results
enrich our understanding of LLM's social behaviour and pave the way for a
behavioural game theory for machines.

</details>


### [24] [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/pdf/2308.15022)
*Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding*

Main category: cs.CL

TL;DR: Proposes recursive summarization for LLMs to improve long-term memory in conversations, enhancing response consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs like GPT-4 struggle with recalling past information in long conversations, leading to inconsistent responses.

Method: Recursively generates summaries/memory from dialogue contexts, using previous memory and new contexts to aid response generation.

Result: Improves response consistency in long-context conversations and complements existing long-context and retrieval-enhanced LLMs.

Conclusion: The method is a potential solution for modeling extremely long contexts in LLMs, with code to be released.

Abstract: Recently, large language models (LLMs), such as GPT-4, stand out remarkable
conversational abilities, enabling them to engage in dynamic and contextually
relevant dialogues across a wide range of topics. However, given a long
conversation, these chatbots fail to recall past information and tend to
generate inconsistent responses. To address this, we propose to recursively
generate summaries/ memory using large language models (LLMs) to enhance
long-term memory ability. Specifically, our method first stimulates LLMs to
memorize small dialogue contexts and then recursively produce new memory using
previous memory and following contexts. Finally, the chatbot can easily
generate a highly consistent response with the help of the latest memory. We
evaluate our method on both open and closed LLMs, and the experiments on the
widely-used public dataset show that our method can generate more consistent
responses in a long-context conversation. Also, we show that our strategy could
nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced
LLMs, bringing further long-term dialogue performance. Notably, our method is a
potential solution to enable the LLM to model the extremely long context. The
code and scripts will be released later.

</details>


### [25] [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/pdf/2403.19346)
*Jingyuan Ma, Damai Dai, Zihang Yuan, Rui li, Weilin Luo, Bin Wang, Qun Liu, Lei Sha, Zhifang Sui*

Main category: cs.CL

TL;DR: A new benchmark, Unreasonable Math Problem (UMP), evaluates LLMs' ability to handle unconventional math problems with flaws. State-of-the-art models like GPT-4o perform poorly (0.6 score), and reasoning models like DeepSeek-R1 are unstable. The study explores improvement strategies and limitations.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in recognizing and responding to unreasonable math problems, an underexplored area.

Method: Proposed the UMP benchmark with diverse unreasonable math questions, tested on 19 LLMs.

Result: GPT-4o scored 0.6 in UMP; reasoning models like DeepSeek-R1 were unstable.

Conclusion: LLMs struggle with unreasonable math problems, highlighting their limitations and the need for further research.

Abstract: Recent research have demonstrated LLMs' impressive performance in math and
reasoning. However, the capacity of LLMs to address math problems under
unconventional conditions, such as internal inconsistencies and flawed
assumptions, remains largely unexplored. In this paper, we propose a novel
benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to
recognize and respond to unreasonability in math problem. The benchmark
consists of a carefully curated collection of unreasonable math questions
across diverse types. Based on extensive experiments covering 19 LLMs, we
observe that even state-of-the-art models such as GPT-4o achieve only limited
performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone
to overthinking and unstable. We further explore strategies for improving the
recognition of unreasonable inputs, shedding light on both the possibility and
limitations of LLMs in this challenging setting.

</details>


### [26] [Re-ReST: Reflection-Reinforced Self-Training for Language Agents](https://arxiv.org/pdf/2406.01495)
*Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng*

Main category: cs.CL

TL;DR: The paper introduces Reflection-Reinforced Self-Training (Re-ReST) to improve language agents by refining low-quality self-generated samples using a reflector, achieving significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on costly human or stronger model annotations for finetuning language agents, the paper explores self-training and enhances it with reflection.

Method: Proposes Re-ReST, where a reflector refines low-quality self-generated samples using external feedback (e.g., unit test results), enriching the self-training dataset.

Result: Self-training improves baselines by 7.6% on HotpotQA and 28.4% on AlfWorld; Re-ReST further boosts performance by 2.0% and 14.1%, respectively.

Conclusion: Re-ReST effectively enhances self-training for language agents, demonstrating the reflector's efficiency and addressing limitations of prior reflection methods.

Abstract: Finetuning language agents with reasoning-action trajectories is effective,
but obtaining these trajectories from human annotations or stronger models is
costly and sometimes impractical. In this paper, we investigate the use of
self-training in language agents, which can generate supervision from the agent
itself, offering a promising alternative without relying on human or stronger
model demonstrations. Self-training, however, requires high-quality
model-generated samples, which are hard to obtain for challenging language
agent tasks. To address this, we present Reflection-Reinforced Self-Training
(Re-ReST), which uses a \textit{reflector} to refine low-quality generated
samples during self-training. The reflector takes the agent's output and
feedback from an external environment (e.g., unit test results in code
generation) to produce improved samples. This technique enhances the quality of
inferior samples and efficiently enriches the self-training dataset with
higher-quality samples. We conduct extensive experiments on open-source
language agents across tasks, including multi-hop question answering,
sequential decision-making, code generation, visual question answering, and
text-to-image generation. The results demonstrate the effectiveness of
self-training and Re-ReST in language agent tasks, with self-training improving
baselines by 7.6\% on HotpotQA and 28.4\% on AlfWorld, and Re-ReST further
boosting performance by 2.0\% and 14.1\%, respectively. Our studies also
confirm the efficiency of using a reflector to generate high-quality samples
for self-training. Moreover, we demonstrate a method to employ reflection
during inference without ground-truth feedback, addressing the limitation of
previous reflection work. Our code is released at
https://github.com/PlusLabNLP/Re-ReST.

</details>


### [27] [Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA](https://arxiv.org/pdf/2406.02044)
*Hussein Jawad, Yassine Chenik, Nicolas J. -B. Brunel*

Main category: cs.CL

TL;DR: QROA is a black-box jailbreak method for LLMs that identifies adversarial suffixes without needing internal model access or human templates, achieving over 80% ASR.


<details>
  <summary>Details</summary>
Motivation: To address security vulnerabilities in LLMs by exposing their susceptibility to adversarial manipulations.

Method: QROA frames the attack as an optimization bandit problem, using a surrogate model and token-level optimization. QROA-UNV extends this to find universal adversarial suffixes.

Result: Achieves over 80% Attack Success Rate (ASR) on multiple models.

Conclusion: Highlights critical LLM vulnerabilities, urging advanced defenses and robust safety evaluations for secure AI deployment.

Abstract: The rapid adoption of Large Language Models (LLMs) has exposed critical
security and ethical vulnerabilities, particularly their susceptibility to
adversarial manipulations. This paper introduces QROA, a novel black-box
jailbreak method designed to identify adversarial suffixes that can bypass LLM
alignment safeguards when appended to a malicious instruction. Unlike existing
suffix-based jailbreak approaches, QROA does not require access to the model's
logit or any other internal information. It also eliminates reliance on
human-crafted templates, operating solely through the standard query-response
interface of LLMs. By framing the attack as an optimization bandit problem,
QROA employs a surrogate model and token level optimization to efficiently
explore suffix variations. Furthermore, we propose QROA-UNV, an extension that
identifies universal adversarial suffixes for individual models, enabling
one-query jailbreaks across a wide range of instructions. Testing on multiple
models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings
highlight critical vulnerabilities, emphasize the need for advanced defenses,
and contribute to the development of more robust safety evaluations for secure
AI deployment. The code is made public on the following link:
https://github.com/qroa/QROA

</details>


### [28] [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://arxiv.org/pdf/2406.18501)
*Zhenghao Zhou, Robert Frank, R. Thomas McCoy*

Main category: cs.CL

TL;DR: The paper investigates whether in-context learning (ICL) in large language models (LLMs) is functionally equivalent to error-driven learning, using the inverse frequency effect (IFE) as evidence.


<details>
  <summary>Details</summary>
Motivation: To diagnose if ICL in LLMs involves error-driven learning mechanisms, inspired by the IFE observed in human psycholinguistics.

Method: Simulated structural priming with ICL and analyzed the IFE in LLMs, comparing effects across model sizes.

Result: LLMs exhibited the IFE, with stronger effects in larger models, suggesting ICL involves error-driven learning.

Conclusion: ICL in LLMs is a form of error-driven learning, aligning with human cognitive processing mechanisms.

Abstract: Large language models (LLMs) have shown the emergent capability of in-context
learning (ICL). One line of research has claimed that ICL is functionally
equivalent to gradient descent, a type of error-driven learning mechanism. In
this paper, we introduce a new way of diagnosing whether ICL is functionally
performing error-driven learning. Our approach is based on the inverse
frequency effect (IFE) -- a phenomenon in which an agent's behavior is
influenced to a greater degree when presented with improbable examples as
compared to more likely ones. The IFE has previously been identified in
psycholinguistics where humans exhibit the IFE in the context of structural
priming (the tendency for people to produce sentence structures they have
encountered recently). In that context, the IFE has been used as evidence that
human structural priming must involve error-driven learning mechanisms. In our
experiments, we simulated structural priming with ICL and found that LLMs
indeed display the IFE, with the effect being stronger in larger models. We
conclude that at least in the case we studied, ICL is indeed a type of
error-driven learning, supporting the hypothesis that an error signal is
implicitly computed in the forward pass during ICL. Our results suggest that
both humans and LLMs make use of error-driven processing mechanisms in on-line
processing.

</details>


### [29] [Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/pdf/2408.13184)
*Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng*

Main category: cs.CL

TL;DR: The paper introduces S2RCQL, a model combining Spatial-to-Relational Transformation and Curriculum Q-Learning to improve LLMs' spatial reasoning and path-planning by addressing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning in LLMs is crucial for embodied intelligence, but challenges like spatial and context inconsistency hallucinations hinder performance, especially in long-term path-planning tasks.

Method: Proposes S2RCQL: (1) Spatial-to-Relational Transformation converts spatial prompts into entity relations, leveraging LLMs' sequential thinking; (2) Q-learning-based path-planning corrects hallucinations using Q-values; (3) Reverse curriculum learning reduces task difficulty to accumulate experience.

Result: Experiments on ERNIE-Bot 4.0 show S2RCQL improves success and optimality rates by 23%-40% over advanced prompt engineering.

Conclusion: S2RCQL effectively mitigates hallucinations in LLMs, enhancing spatial reasoning and path-planning performance.

Abstract: Spatial reasoning in Large Language Models (LLMs) is the foundation for
embodied intelligence. However, even in simple maze environments, LLMs still
encounter challenges in long-term path-planning, primarily influenced by their
spatial hallucination and context inconsistency hallucination by long-term
reasoning. To address this challenge, this study proposes an innovative model,
Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To
address the spatial hallucination of LLMs, we propose the Spatial-to-Relational
approach, which transforms spatial prompts into entity relations and paths
representing entity relation chains. This approach fully taps the potential of
LLMs in terms of sequential thinking. As a result, we design a path-planning
algorithm based on Q-learning to mitigate the context inconsistency
hallucination, which enhances the reasoning ability of LLMs. Using the Q-value
of state-action as auxiliary information for prompts, we correct the
hallucinations of LLMs, thereby guiding LLMs to learn the optimal path.
Finally, we propose a reverse curriculum learning technique based on LLMs to
further mitigate the context inconsistency hallucination. LLMs can rapidly
accumulate successful experiences by reducing task difficulty and leveraging
them to tackle more complex tasks. We performed comprehensive experiments based
on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our
S2RCQL achieved a 23%--40% improvement in both success and optimality rates
compared with advanced prompt engineering.

</details>


### [30] [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/pdf/2411.02116)
*Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara*

Main category: cs.CL

TL;DR: The study compares human and LLM color-word associations, showing GPT-4o's highest accuracy but persistent gaps, especially in emotional categories.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to replicate human color-word associations, a fundamental aspect of cognition and design.

Method: Comparison of LLM generations (GPT-3 to GPT-4o) against human data from 10,000 Japanese participants, involving 17 colors and 80 words.

Result: GPT-4o performed best (50% accuracy) but showed category-specific variations, excelling in Rhythm/Landscape but struggling with Emotions.

Conclusion: LLMs align with humans in basic color discrimination but diverge in semantic memory structures for color-word associations.

Abstract: Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
have demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT-4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and 80
words (10 word from eight categories) in Japanese. Our findings reveal a clear
progression in LLM performance across generations, with GPT-4o achieving the
highest accuracy in predicting the best voted word for each color and category.
However, the highest median performance was approximately 50% even for GPT-4o
with visual inputs (chance level of 10%). Moreover, we found performance
variations across word categories and colors: while LLMs tended to excel in
categories such as Rhythm and Landscape, they struggled with categories such as
Emotions. Interestingly, color discrimination ability estimated from our
color-word association data showed high correlation with human color
discrimination patterns, consistent with previous studies. Thus, despite
reasonable alignment in basic color discrimination, humans and LLMs still
diverge systematically in the words they assign to those colors. Our study
highlights both the advancements in LLM capabilities and their persistent
limitations, raising the possibility of systematic differences in semantic
memory structures between humans and LLMs in representing color-word
associations.

</details>


### [31] [SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution](https://arxiv.org/pdf/2501.05040)
*Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, Kai Chen*

Main category: cs.CL

TL;DR: SWE-Fixer is an open-source framework for resolving GitHub issues using LLMs, featuring efficient retrieval and editing modules, and achieves competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches for GitHub issue resolution rely on proprietary models, limiting reproducibility and transparency. SWE-Fixer addresses this gap.

Method: SWE-Fixer uses a two-module system: a BM25-based retrieval module and a code editing module, trained on a custom 110K GitHub issues dataset.

Result: Achieves 22.0% and 30.2% on SWE-Bench Lite and Verified, improving to 24.7% and 32.8% with P2P filtering, while being more efficient.

Conclusion: SWE-Fixer is effective for real-world code-fixing, with plans to release the model, dataset, and code publicly.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a variety of complex tasks. One significant application of LLMs is in tackling
software engineering challenges, particularly in resolving real-world tasks on
GitHub by fixing code based on the issues reported by the users. However, many
current approaches rely on proprietary LLMs, which limits reproducibility,
accessibility, and transparency. The critical components of LLMs for addressing
software engineering issues and how their capabilities can be effectively
enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a
novel open-source framework designed to effectively and efficiently resolve
GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval
module and a code editing module. The retrieval module employs BM25 along with
a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the
code editing module utilizes the other model to generate patches for the
identified files. To mitigate the lack of publicly available datasets, we
compile an extensive dataset that includes 110K GitHub issues along with their
corresponding patches and train the two models of SWE-Fixer separately. We
assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving
competitive performance among open-source models with scores of 22.0% and
30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on
Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,
our approach requires only two model calls per instance, making it
significantly more efficient than existing methods. These results highlight the
effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make
our model, dataset, and code publicly available at
https://github.com/InternLM/SWE-Fixer.

</details>


### [32] [Estimating LLM Uncertainty with Logits](https://arxiv.org/pdf/2502.00290)
*Huan Ma, Jingdong Chen, Joey Tianyi Zhou, Guangyu Wang, Changqing Zhang*

Main category: cs.CL

TL;DR: LogTokU is a framework for estimating token uncertainty in LLMs, addressing the limitations of probability-based methods by leveraging evidence strength information.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations due to unreliable token generation when lacking knowledge. Existing uncertainty estimation methods, especially probability-based ones, fail to accurately identify token reliability.

Method: Proposes LogTokU, a framework using evidence modeling to decouple token uncertainty, enabling real-time estimation without multiple sampling.

Result: LogTokU shows significant effectiveness in estimating token uncertainty and guiding downstream tasks.

Conclusion: LogTokU provides a promising solution for real-time uncertainty estimation in LLMs, improving reliability.

Abstract: Over the past few years, Large Language Models (LLMs) have developed rapidly
and are widely applied in various domains. However, LLMs face the issue of
hallucinations, generating responses that may be unreliable when the models
lack relevant knowledge. To be aware of potential hallucinations, uncertainty
estimation methods have been introduced, and most of them have confirmed that
reliability lies in critical tokens. However, probability-based methods perform
poorly in identifying token reliability, limiting their practical utility. In
this paper, we reveal that the probability-based method fails to estimate token
reliability due to the loss of evidence strength information which is
accumulated in the training stage. Therefore, we present Logits-induced token
uncertainty (LogTokU), a framework for estimating decoupled token uncertainty
in LLMs, enabling real-time uncertainty estimation without requiring multiple
sampling processes. We employ evidence modeling to implement LogTokU and use
the estimated uncertainty to guide downstream tasks. The experimental results
demonstrate that LogTokU has significant effectiveness and promise.

</details>


### [33] [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/pdf/2503.01496)
*Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng*

Main category: cs.CL

TL;DR: Liger is a method to convert pretrained LLMs into gated linear recurrent models without extra parameters, using lightweight fine-tuning to restore performance.


<details>
  <summary>Details</summary>
Motivation: Pretraining non-standard architectures like linear recurrent models is costly, and current linearization methods require extensive fine-tuning or overlook gating mechanisms.

Method: Liger repurposes pretrained key matrix weights for gating, avoids extra parameters, and uses LoRA for lightweight fine-tuning. It also introduces Liger Attention for hybrid attention.

Result: Liger recovers 93% of Transformer performance with 0.02% pretraining tokens and achieves competitive benchmarks on 1B-8B parameter models.

Conclusion: Liger efficiently linearizes LLMs into gated recurrent structures without added parameters, maintaining performance and reducing training costs.

Abstract: Transformers with linear recurrent modeling offer linear-time training and
constant-memory inference. Despite their demonstrated efficiency and
performance, pretraining such non-standard architectures from scratch remains
costly and risky. The linearization of large language models (LLMs) transforms
pretrained standard models into linear recurrent structures, enabling more
efficient deployment. However, current linearization methods typically
introduce additional feature map modules that require extensive fine-tuning and
overlook the gating mechanisms used in state-of-the-art linear recurrent
models. To address these issues, this paper presents Liger, short for
Linearizing LLMs to gated recurrent structures. Liger is a novel approach for
converting pretrained LLMs into gated linear recurrent models without adding
extra parameters. It repurposes the pretrained key matrix weights to construct
diverse gating mechanisms, facilitating the formation of various gated
recurrent structures while avoiding the need to train additional components
from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),
Liger restores the performance of the linearized gated recurrent models to
match that of the original LLMs. Additionally, we introduce Liger Attention, an
intra-layer hybrid attention mechanism, which significantly recovers 93\% of
the Transformer-based LLM at 0.02\% pre-training tokens during the
linearization process, achieving competitive results across multiple
benchmarks, as validated on models ranging from 1B to 8B parameters. Code is
available at https://github.com/OpenSparseLLMs/Linearization.

</details>


### [34] [Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation](https://arxiv.org/pdf/2503.03186)
*Ben Hutchinson, Celeste RodrÃ­guez Louro, Glenys Collard, Ned Cooper*

Main category: cs.CL

TL;DR: The paper explores the potential and risks of using speech technologies to support Australian Aboriginal English, emphasizing participatory and culturally safe practices.


<details>
  <summary>Details</summary>
Motivation: The gap in language technology support for Indigenous contact varieties hinders participation in society and perpetuates minoritization.

Method: A case study on a real-world project integrating culturally appropriate and participatory design practices.

Result: Opportunities exist for supporting Indigenous languages, but risks must be mitigated through community participation.

Conclusion: Increased support for Indigenous languages is needed, with participatory and culturally safe practices.

Abstract: In Australia, post-contact language varieties, including creoles and local
varieties of international languages, emerged as a result of forced contact
between Indigenous communities and English speakers. These contact varieties
are widely used, yet are poorly supported by language technologies. This gap
presents barriers to participation in civil and economic society for Indigenous
communities using these varieties, and reproduces minoritisation of
contemporary Indigenous sociolinguistic identities. This paper concerns three
questions regarding this context. First, can speech technologies support
speakers of Australian Aboriginal English, a local indigenised variety of
English? Second, what risks are inherent in such a project? Third, what
technology development practices are appropriate for this context, and how can
researchers integrate meaningful community participation in order to mitigate
risks? We argue that opportunities do exist -- as well as risks -- and
demonstrate this through a case study exploring design practices in a
real-world project aiming to improve speech technologies for Australian
Aboriginal English. We discuss how we integrated culturally appropriate and
participatory processes throughout the project. We call for increased support
for languages used by Indigenous communities, including contact varieties,
which provide practical economic and socio-cultural benefits, provided that
participatory and culturally safe practices are enacted.

</details>


### [35] [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/pdf/2503.11280)
*Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung*

Main category: cs.CL

TL;DR: Multilingual LLMs show inconsistent cross-lingual alignments. A framework and metric (ILO) are proposed to evaluate shared and fragmented interlingual representations, revealing that single-language fine-tuning disrupts alignment unless early layers are frozen.


<details>
  <summary>Details</summary>
Motivation: To clarify whether multilingual LLMs develop unified interlingual representations or partially aligned constructs, given mixed evidence.

Method: Study 31 diverse languages, propose an interlingual representation framework, and introduce the ILO score to quantify alignment. Investigate the impact of single-language fine-tuning.

Result: Single-language fine-tuning disrupts alignment in early layers; freezing these layers preserves alignment and improves cross-lingual generalization.

Conclusion: Interlingual alignment is crucial for scalable multilingual learning, validated by the proposed framework and ILO metric.

Abstract: Large language models (LLMs) trained on massive multilingual datasets hint at
the formation of interlingual constructs--a shared subspace in the
representation space. However, evidence regarding this phenomenon is mixed,
leaving it unclear whether these models truly develop unified interlingual
representations, or present a partially aligned constructs. We explore 31
diverse languages varying on their resource-levels, typologies, and
geographical regions; and find that multilingual LLMs exhibit inconsistent
cross-lingual alignments. To address this, we propose an interlingual
representation framework identifying both the shared interlingual semantic
subspace and fragmented components, existed due to representational
limitations. We introduce Interlingual Local Overlap (ILO) score to quantify
interlingual alignment by comparing the local neighborhood structures of
high-dimensional representations. We utilize ILO to investigate the impact of
single-language fine-tuning on the interlingual representations in multilingual
LLMs. Our results indicate that training exclusively on a single language
disrupts the alignment in early layers, while freezing these layers preserves
the alignment of interlingual representations, leading to improved
cross-lingual generalization. These results validate our framework and metric
for evaluating interlingual representation, and further underscore that
interlingual alignment is crucial for scalable multilingual learning.

</details>


### [36] [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/pdf/2503.21813)
*Zhangcheng Qiang*

Main category: cs.CL

TL;DR: A new benchmark dataset, OAEI-LLM-T, is introduced to address hallucinations in LLM-based ontology matching, classifying them into categories and sub-categories for leaderboard construction and LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs pose challenges for ontology matching tasks, necessitating a dedicated dataset to study and mitigate these issues.

Method: The dataset, derived from OAEI TBox datasets, captures and classifies LLM hallucinations in ontology matching into two main categories and six sub-categories.

Result: OAEI-LLM-T enables the construction of an LLM leaderboard and supports fine-tuning of foundational LLMs for improved ontology matching.

Conclusion: The dataset provides a valuable tool for benchmarking and enhancing LLM performance in ontology matching by addressing hallucinations systematically.

Abstract: Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.
schema-matching) datasets in the Ontology Alignment Evaluation Initiative
(OAEI), capturing hallucinations of different LLMs performing OM tasks. These
OM-specific hallucinations are carefully classified into two primary categories
and six sub-categories. We showcase the usefulness of the dataset in
constructing the LLM leaderboard and fine-tuning foundational LLMs for
LLM-based OM systems.

</details>


### [37] [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/pdf/2504.18884)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: Ensemble strategy for LLMs improves sentiment analysis accuracy and robustness, reducing RMSE by 18.6%.


<details>
  <summary>Details</summary>
Motivation: Address variability and reproducibility issues in LLM results, inspired by human annotation's majority voting.

Method: Straightforward ensemble of multiple inferences using medium-sized LLMs.

Result: Ensemble outperforms single large-model attempts, reducing RMSE by 18.6%.

Conclusion: Ensemble strategy enhances LLM performance in sentiment analysis.

Abstract: With the advance of large language models (LLMs), LLMs have been utilized for
the various tasks. However, the issues of variability and reproducibility of
results from each trial of LLMs have been largely overlooked in existing
literature while actual human annotation uses majority voting to resolve
disagreements among annotators. Therefore, this study introduces the
straightforward ensemble strategy to a sentiment analysis using LLMs. As the
results, we demonstrate that the ensemble of multiple inference using
medium-sized LLMs produces more robust and accurate results than using a large
model with a single attempt with reducing RMSE by 18.6%.

</details>


### [38] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/pdf/2504.20752)
*Roman Abramov, Felix Steinbauer, Gjergji Kasneci*

Main category: cs.CL

TL;DR: Extending grokking to real-world data by augmenting knowledge graphs with synthetic data improves multi-hop reasoning accuracy in Transformers.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in multi-step factual reasoning in Transformers, especially with sparse real-world knowledge.

Method: Augmenting knowledge graphs with synthetic data to increase the ratio of inferred to atomic facts, enabling grokking.

Result: Achieves 95-100% accuracy on 2WikiMultiHopQA, surpassing baselines and matching state-of-the-art.

Conclusion: Grokking-based data augmentation enhances multi-hop reasoning, offering robust and interpretable factual reasoning in language models.

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>


### [39] [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/pdf/2505.00661)
*Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, JÃ¶rg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland*

Main category: cs.CL

TL;DR: The paper explores differences in generalization between in-context learning and fine-tuning in large language models, proposing a method to improve fine-tuning generalization by incorporating in-context inferences.


<details>
  <summary>Details</summary>
Motivation: Large language models often fail to generalize from fine-tuning, hindering practical applications, while in-context learning shows better generalization in some cases.

Method: Constructed novel datasets to test generalization, comparing in-context learning and fine-tuning, and proposed adding in-context inferences to fine-tuning data.

Result: In-context learning generalizes more flexibly than fine-tuning in data-matched settings, and the proposed method improves fine-tuning generalization.

Conclusion: The findings enhance understanding of learning biases in language models and offer practical improvements for their performance.

Abstract: Large language models exhibit exciting capabilities, yet can show
surprisingly narrow generalization from finetuning. E.g. they can fail to
generalize to simple reversals of relations they are trained on, or fail to
make simple logical deductions based on trained information. These failures to
generalize from fine-tuning can hinder practical application of these models.
On the other hand, language models' in-context learning shows different
inductive biases, and can generalize better in some cases. Here, we explore
these differences in generalization between in-context- and fine-tuning-based
learning. To do so, we constructed several novel datasets to evaluate and
improve models' abilities to generalize from finetuning data. The datasets are
designed to create clean tests of generalization, by isolating the knowledge in
the dataset from that in pretraining. We expose pretrained large models to
controlled subsets of the information in these datasets -- either in context,
or through fine-tuning -- and evaluate their performance on test sets that
require various types of generalization. We find overall that in data-matched
settings, in-context learning can generalize more flexibly than fine-tuning
(though we also find some qualifications of prior findings, such as cases when
fine-tuning can generalize to reversals embedded in a larger structure of
knowledge). We build on these findings to propose a method to enable improved
generalization from fine-tuning: adding in-context inferences to finetuning
data. We show that this method improves generalization across various splits of
our datasets and other benchmarks. Our results have implications for
understanding the inductive biases of different modes of learning in language
models, and practically improving their performance.

</details>


### [40] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/pdf/2505.00977)
*Yingquan Chen, Qianmu Li, Xiaocong Wu, Huifeng Li, Qing Chang*

Main category: cs.CL

TL;DR: The paper proposes a character-based diffusion embedding algorithm (CDEA) and XLNet to improve steganographic text quality by leveraging sensitive information properties and enhancing high-probability word selection.


<details>
  <summary>Details</summary>
Motivation: Existing models and embedding algorithms struggle with generating high-quality steganographic text due to limitations in text generation and ineffective mitigation of sensitive information's negative impacts.

Method: Introduces CDEA, which leverages sensitive information properties and character-level statistics to enhance high-probability word selection, combined with XLNet for long-sequence transformation.

Result: The combination of CDEA and XLNet significantly improves steganographic text quality, especially perceptual-imperceptibility.

Conclusion: CDEA and XLNet effectively address the challenges in steganographic text generation, enhancing quality and coherence.

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [41] [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/pdf/2505.02366)
*Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu*

Main category: cs.CL

TL;DR: The paper introduces JTCSE, a framework combining modulus constraints on semantic representations and cross-attention to improve unsupervised contrastive learning for sentence embeddings, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods ignore modulus features of semantic representations and suffer from sinking attention in BERT-like models, limiting performance.

Method: Proposes modulus constraints on semantic tensors and a cross-attention structure to enhance CLS token attention, forming the JTCSE framework.

Result: JTCSE outperforms baselines in seven semantic text similarity tasks and 130+ zero-shot downstream tasks.

Conclusion: JTCSE effectively addresses modulus and attention issues, achieving superior performance in unsupervised contrastive learning.

Abstract: Unsupervised contrastive learning has become a hot research topic in natural
language processing. Existing works usually aim at constraining the orientation
distribution of the representations of positive and negative samples in the
high-dimensional semantic space in contrastive learning, but the semantic
representation tensor possesses both modulus and orientation features, and the
existing works ignore the modulus feature of the representations and cause
insufficient contrastive learning. % Therefore, we firstly propose a training
objective that aims at modulus constraints on the semantic representation
tensor, to strengthen the alignment between the positive samples in contrastive
learning. Therefore, we first propose a training objective that is designed to
impose modulus constraints on the semantic representation tensor, to strengthen
the alignment between positive samples in contrastive learning. Then, the
BERT-like model suffers from the phenomenon of sinking attention, leading to a
lack of attention to CLS tokens that aggregate semantic information. In
response, we propose a cross-attention structure among the twin-tower ensemble
models to enhance the model's attention to CLS token and optimize the quality
of CLS Pooling. Combining the above two motivations, we propose a new
\textbf{J}oint \textbf{T}ensor representation modulus constraint and
\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence
\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven
semantic text similarity computation tasks, and the experimental results show
that JTCSE's twin-tower ensemble model and single-tower distillation model
outperform the other baselines and become the current SOTA. In addition, we
have conducted an extensive zero-shot downstream task evaluation, which shows
that JTCSE outperforms other baselines overall on more than 130 tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [42] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/pdf/2505.03821)
*Gracjan GÃ³ral, Alicja Ziarko, Piotr MiÅoÅ, MichaÅ Nauman, Maciej WoÅczyk, MichaÅ KosiÅski*

Main category: cs.CV

TL;DR: VLMs struggle with spatial reasoning and perspective-taking despite excelling in scene understanding, highlighting the need for improved geometric representations and training.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability in visual perspective taking using controlled tasks inspired by human tests.

Method: Used 144 unique visual tasks with systematic spatial variations and diagnostic questions to evaluate scene understanding, spatial reasoning, and perspective-taking.

Result: Models like GPT-4-Turbo and Claude Sonnet perform well in scene understanding but poorly in spatial reasoning and perspective-taking.

Conclusion: Future VLM development should integrate explicit geometric representations and tailored training to bridge the gap in complex visual tasks.

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [43] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/pdf/2505.03826)
*Minji Kang, Seongho Kim, Eunseo Go, Donghyeon Paek, Geon Lim, Muyoung Kim, Soyeun Kim, Sung Kyu Jang, Min Sup Choi, Woo Seok Kang, Jaehyun Kim, Jaekwang Kim, Hyeong-U Kim*

Main category: cs.CV

TL;DR: A machine learning-based framework for in-situ etch depth prediction in semiconductor manufacturing, using ANN and BNN for accuracy and uncertainty, and RGB data for cost-effective monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of conventional ex-situ methods (time delays, contamination risks) in monitoring etch depth and insulating material thickness.

Method: Proposes ML techniques: ANN for average etch depth prediction, BNN for uncertainty, and RGB data from DIC for input.

Result: ANN outperforms linear models; BNN provides reliable uncertainty estimates. RGB data shows strong performance without explicit process parameters.

Conclusion: Integration of DIC and ML offers a viable, cost-effective, real-time, in-situ monitoring solution for plasma etching, enhancing process stability and efficiency.

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [44] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/pdf/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: A survey analyzing benchmarks and evaluation methods for Video Large Language Models (VideoLLMs), highlighting trends, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a structured understanding of how to evaluate VideoLLMs and advance video understanding technologies.

Method: Examination of video understanding benchmarks, their characteristics, evaluation protocols, and limitations, along with analysis of various evaluation methodologies.

Result: Identifies performance trends of state-of-the-art VideoLLMs, key challenges in evaluation frameworks, and gaps in current benchmarks.

Conclusion: Proposes future research directions for improving benchmark design, metrics, and protocols, emphasizing diversity, multimodality, and interpretability.

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [45] [HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation](https://arxiv.org/pdf/2505.04276)
*Yajie Fu, Chaorui Huang, Junwei Li, Hui Kong, Yibin Tian, Huakang Li, Zhiyuan Zhang*

Main category: cs.CV

TL;DR: HDiffTG integrates Transformer, GCN, and diffusion models for 3D human pose estimation, achieving SOTA performance with robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and robustness in 3D human pose estimation by combining global (Transformer), local (GCN), and fine-tuning (diffusion model) techniques.

Method: Unified framework with Transformer for global dependencies, GCN for skeletal structures, and diffusion model for optimization. Lightweight optimizations reduce computational overhead.

Result: SOTA performance on MPI-INF-3DHP, high accuracy, efficiency, and robustness in noisy/occluded scenarios.

Conclusion: HDiffTG effectively balances global and local features, offering a lightweight, high-performance solution for 3D pose estimation.

Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that
integrates Transformer, Graph Convolutional Network (GCN), and diffusion model
into a unified framework. HDiffTG leverages the strengths of these techniques
to significantly improve pose estimation accuracy and robustness while
maintaining a lightweight design. The Transformer captures global
spatiotemporal dependencies, the GCN models local skeletal structures, and the
diffusion model provides step-by-step optimization for fine-tuning, achieving a
complementary balance between global and local features. This integration
enhances the model's ability to handle pose estimation under occlusions and in
complex scenarios. Furthermore, we introduce lightweight optimizations to the
integrated model and refine the objective function design to reduce
computational overhead without compromising performance. Evaluation results on
the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves
state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling
in both accuracy and computational efficiency. Additionally, the model exhibits
exceptional robustness in noisy and occluded environments. Source codes and
models are available at https://github.com/CirceJie/HDiffTG

</details>


### [46] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/pdf/2505.03832)
*Noor B. Tayfor, Tarik A. Rashid, Shko M. Qader, Bryar A. Hassan, Mohammed H. Abdalla, Jafar Majidpour, Aram M. Ahmed, Hussein M. Ali, Aso M. Aladdin, Abdulhady A. Abdullah, Ahmed S. Shamsaldin, Haval M. Sidqi, Abdulrahman Salih, Zaher M. Yaseen, Azad A. Ameen, Janmenjoy Nayak, Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: The paper reviews forensic techniques for detecting video forgery in surveillance footage, emphasizing the need for robust methods to ensure authenticity and legal credibility.


<details>
  <summary>Details</summary>
Motivation: The rise of video editing tools has made tampering with digital recordings easier, threatening the integrity of surveillance footage used in security and judicial processes.

Method: The study examines existing forensic techniques, including compression-based analysis, frame duplication detection, and machine learning-based approaches.

Result: The findings underscore the need for more advanced forensic methods to counter evolving forgery techniques.

Conclusion: Enhancing video forensic capabilities is crucial to maintaining the credibility and admissibility of surveillance recordings as legal evidence.

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [47] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/pdf/2505.04488)
*Ziyi Zhang, Zhen Sun, Zongmin Zhang, Zifan Peng, Yuemeng Zhao, Zichun Wang, Zeren Luo, Ruiting Zuo, Xinlei He*

Main category: cs.CV

TL;DR: The paper evaluates VideoLLMs for assisting visually impaired individuals in dynamic environments, introduces a benchmark dataset (VisAssistDaily), and highlights GPT-4o's high task success rate. It also identifies challenges like hazard perception and proposes solutions with SafeVid dataset and a polling mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-time perception in assistive technologies for visually impaired individuals in dynamic environments.

Method: Constructs VisAssistDaily benchmark dataset, evaluates VideoLLMs (including GPT-4o), conducts user studies, and introduces SafeVid dataset with a polling mechanism for hazard detection.

Result: GPT-4o achieves the highest task success rate. Challenges in hazard perception are identified, and a solution is proposed.

Conclusion: The work provides insights for improving assistive technologies for visually impaired individuals, emphasizing real-time hazard detection and dynamic environment adaptation.

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [48] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/pdf/2505.03833)
*Xuechao Wang, Sven Nomm, Junqing Huang, Kadri Medijainen, Aaro Toomela, Michael Ruzhansky*

Main category: cs.CV

TL;DR: PointExplainer is an explainable diagnostic tool for Parkinson's disease using hand-drawn signals, providing interpretable attributions without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deep neural network methods lack interpretability, hindering clinical trust in Parkinson's disease diagnosis.

Method: PointExplainer uses a diagnosis module (encoding hand-drawn signals into 3D point clouds) and an explanation module (training an interpretable surrogate model). It also introduces consistency measures for faithful explanations.

Result: Experiments on benchmark and new datasets show PointExplainer provides intuitive explanations without degrading diagnostic performance.

Conclusion: PointExplainer successfully addresses interpretability challenges in Parkinson's disease diagnosis while maintaining accuracy.

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [49] [Explainable Face Recognition via Improved Localization](https://arxiv.org/pdf/2505.03837)
*Rashik Shadman, Daqing Hou, Faraz Hussain, M G Sarwar Murshed*

Main category: cs.CV

TL;DR: The paper proposes an explainable face recognition system using Scaled Directed Divergence (SDD) for precise feature localization, enhancing transparency and trust in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based face recognition systems lack explainability, reducing user trust. This paper aims to address this gap by providing visual explanations for model decisions.

Method: The authors use SDD, a CAM-based discriminative localization technique, to fine-tune and highlight relevant facial features for model predictions.

Result: SDD-CAM outperforms traditional CAM by accurately and specifically localizing relevant facial features, improving transparency.

Conclusion: The SDD-based approach enhances trust in face recognition systems by providing clear, narrow visual explanations of model decisions.

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [50] [SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer](https://arxiv.org/pdf/2505.04394)
*Young-Hu Park, Rae-Hong Park, Hyung-Min Park*

Main category: cs.CV

TL;DR: The paper introduces SwinLip, a lightweight visual speech encoder for lip reading, using Swin Transformer and Conformer to reduce computational load while improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-based lip reading models are computationally complex and inefficient for spatio-temporal feature capture, hindering multi-modal applications.

Method: The authors apply Swin Transformer's hierarchical structure and window self-attention, integrating Conformer temporal embeddings for efficiency.

Result: SwinLip improves performance and speed in word/sentence recognition, achieving state-of-the-art results on Mandarin LRW-1000 with less computation.

Conclusion: SwinLip is an efficient, high-performance alternative to CNN-based lip reading models, validated on diverse datasets.

Abstract: This paper presents an efficient visual speech encoder for lip reading. While
most recent lip reading studies have been based on the ResNet architecture and
have achieved significant success, they are not sufficiently suitable for
efficiently capturing lip reading features due to high computational complexity
in modeling spatio-temporal information. Additionally, using a complex visual
model not only increases the complexity of lip reading models but also induces
delays in the overall network for multi-modal studies (e.g., audio-visual
speech recognition, speech enhancement, and speech separation). To overcome the
limitations of Convolutional Neural Network (CNN)-based models, we apply the
hierarchical structure and window self-attention of the Swin Transformer to lip
reading. We configure a new lightweight scale of the Swin Transformer suitable
for processing lip reading data and present the SwinLip visual speech encoder,
which efficiently reduces computational load by integrating modified
Convolution-augmented Transformer (Conformer) temporal embeddings with
conventional spatial embeddings in the hierarchical structure. Through
extensive experiments, we have validated that our SwinLip successfully improves
the performance and inference speed of the lip reading network when applied to
various backbones for word and sentence recognition, reducing computational
load. In particular, our SwinLip demonstrated robust performance in both
English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art
performance on the Mandarin LRW-1000 dataset with less computation compared to
the existing state-of-the-art model.

</details>


### [51] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/pdf/2505.03846)
*Kangsheng Wang, Yuhang Li, Chengwei Ye, Yufei Lin, Huanzhen Zhang, Bohan Hu, Linuo Xu, Shuyan Liu*

Main category: cs.CV

TL;DR: GAME, a Graph-Augmented Multimodal Encoder, effectively combines visual, auditory, and textual cues for personality prediction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of analyzing personality from short videos due to complex multimodal interactions.

Method: Uses a dual-branch Geo Two-Stream Network for visual cues, VGGish for audio, XLM-Roberta for text, and a Channel Attention-based Fusion module for integration.

Result: GAME consistently outperforms existing benchmarks.

Conclusion: GAME is effective and generalizable for personality prediction from multimodal data.

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [52] [VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling](https://arxiv.org/pdf/2406.04321)
*Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo*

Main category: cs.CV

TL;DR: VidMuse generates high-quality music from videos using local and global visual cues, outperforming existing models in audio quality and alignment.


<details>
  <summary>Details</summary>
Motivation: To study music generation conditioned solely on video, addressing the need for acoustically and semantically aligned audio tracks.

Method: Proposes VidMuse, a framework leveraging Long-Short-Term modeling and visual cues for music generation.

Result: VidMuse outperforms existing models in audio quality, diversity, and audio-visual alignment.

Conclusion: VidMuse is effective for video-conditioned music generation, with code and datasets publicly available.

Abstract: In this work, we systematically study music generation conditioned solely on
the video. First, we present a large-scale dataset comprising 360K video-music
pairs, including various genres such as movie trailers, advertisements, and
documentaries. Furthermore, we propose VidMuse, a simple framework for
generating music aligned with video inputs. VidMuse stands out by producing
high-fidelity music that is both acoustically and semantically aligned with the
video. By incorporating local and global visual cues, VidMuse enables the
creation of musically coherent audio tracks that consistently match the video
content through Long-Short-Term modeling. Through extensive experiments,
VidMuse outperforms existing models in terms of audio quality, diversity, and
audio-visual alignment. The code and datasets are available at
https://vidmuse.github.io/.

</details>


### [53] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/pdf/2505.03848)
*Janhavi Giri, Attila Lengyel, Don Kent, Edward Kibardin*

Main category: cs.CV

TL;DR: An advanced clustering framework combining deep Topological Data Analysis (TDA), self-supervised learning, and transfer learning is proposed for unsupervised image clustering in semiconductor manufacturing, improving defect identification and process monitoring.


<details>
  <summary>Details</summary>
Motivation: Semiconductor manufacturing produces large-scale image data for defect identification, but manual inspection and traditional clustering methods are inadequate for high-dimensional, unlabeled data.

Method: The framework integrates deep TDA to capture topological features, self-supervised learning for meaningful representations, and transfer learning for adaptability and scalability.

Result: Validated on synthetic and open-source datasets, the framework effectively clusters defect patterns and process variations.

Conclusion: The combination of TDA, self-supervised learning, and transfer learning offers a scalable solution for quality control in semiconductor manufacturing and similar domains.

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [54] [Question-Answering Dense Video Events](https://arxiv.org/pdf/2409.04388)
*Hangyu Qin, Junbin Xiao, Angela Yao*

Main category: cs.CV

TL;DR: The paper introduces a novel task of question-answering on dense video events, presents the DeVE-QA dataset, and proposes DeVi, a training-free MLLM approach that outperforms state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of answering and grounding dense-event questions in long videos, which current MLLMs struggle with.

Method: Proposes DeVi, featuring hierarchical captioning, temporal event memory, and self-consistency checking modules to detect, contextualize, and ground dense events.

Result: DeVi achieves a 4.8% and 2.1% accuracy increase on DeVE-QA and NExT-GQA, respectively.

Conclusion: DeVi effectively improves dense-event question answering and grounding, with plans to release data and code.

Abstract: This paper presents question-answering on dense video events, a novel task
that answers and grounds dense-event questions in long videos, thus challenging
MLLMs to faithfully comprehend and reason about multiple events over extended
periods of time. To facilitate the study, we construct DeVE-QA -- a dataset
featuring 78K questions about 26K events on 10.6K long videos. Our benchmarking
shows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we
propose DeVi, a novel training-free MLLM approach that highlights a
hierarchical captioning module, a temporal event memory module, and a
self-consistency checking module to respectively detect, contextualize and
memorize, and ground dense-events in long videos for question answering.
Extensive experiments show that DeVi is superior at answering dense-event
questions and grounding relevant video moments. Compared with existing MLLMs,
it achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on
DeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon
acceptance.

</details>


### [55] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/pdf/2505.03856)
*Tin MiÅ¡iÄ, Karlo KolediÄ, Fabio Bonsignorio, Ivan PetroviÄ, Ivan MarkoviÄ*

Main category: cs.CV

TL;DR: The paper presents an active inference-based model for covert and overt visual attention, tested using the Posner cueing task and a target focus task. Results show faster reaction times for exogenous/valid cues and behaviors like inhibition of return. Reflexive saccades are faster but less adaptable.


<details>
  <summary>Details</summary>
Motivation: To model how agents selectively attend to stimuli while filtering distractions, using active inference to optimize sensory precisions.

Method: The model dynamically optimizes sensory precisions based on environmental beliefs and input, tested in the Posner cueing task and a 2D target focus task, measuring reaction times.

Result: Exogenous/valid cues yield faster reaction times; the model shows inhibition of return. Reflexive saccades are quicker but less adaptable.

Conclusion: The active inference model effectively captures attentional dynamics, balancing speed and adaptability in covert and overt attention.

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [56] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/pdf/2505.03603)
*Y. B. Wang, S. Z. Zhou, J. F. Wu, T. Hu, J. N. Zhang, Y. Liu*

Main category: cs.CV

TL;DR: PAHA is an end-to-end audio-driven upper-body human animation framework using a diffusion model, addressing quality and consistency issues with PAR and PCE methods, and introducing CNAS dataset.


<details>
  <summary>Details</summary>
Motivation: Current methods suffer from long inference times and poor quality in specific regions due to lack of localized fine-grained supervision.

Method: Proposes PAR for dynamic loss adjustment and PCE for audio-motion consistency, with SG and DG for inference guidance.

Result: PAHA outperforms existing methods in audio-motion alignment and video evaluations.

Conclusion: PAHA advances audio-driven animation with improved quality and consistency, supported by the new CNAS dataset.

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [57] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/pdf/2505.03896)
*Shuang Zeng, Chee Hong Lee, Micky C Nnamdi, Wenqi Shi, J Ben Tamo, Lei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, May D. Wang, Yanye Lu, Qiushi Ren*

Main category: cs.CV

TL;DR: Proposes AttUKAN, a novel Attention U-shaped Kolmogorov-Arnold Network with Label-guided Pixel-wise Contrastive Loss for superior retinal vessel segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore fine-grained encoder features and focus only on decoder-label differences, limiting discriminative feature extraction.

Method: Integrates Attention Gates into Kolmogorov-Arnold Networks for enhanced sensitivity and interpretability, and introduces a novel contrastive loss for better feature discrimination.

Result: Achieves top F1 and MIoU scores on four public and one private dataset, outperforming 11 existing networks.

Conclusion: AttUKAN sets a new state-of-the-art in retinal vessel segmentation, validated by quantitative and qualitative results.

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [58] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/pdf/2505.03974)
*Nikhil M. Pawar, Jorge A. Prozzi, Feng Hong, Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: A framework combining CNN and ESPCNN improves infrastructure image resolution and reduces false alarms in distress detection.


<details>
  <summary>Details</summary>
Motivation: Address limitations of low-resolution drone images and high computational costs in super-resolution techniques for infrastructure management.

Method: Developed a framework using CNN for accurate distress classification and ESPCNN for efficient super-resolution of positive distress images.

Result: ESPCNN outperformed bicubic interpolation, reduced computational costs, and minimized false alarms. Visual inspection confirmed accurate crack detection.

Conclusion: The CNN-ESPCNN framework enhances distress detection accuracy and aids efficient infrastructure asset management.

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [59] [Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges](https://arxiv.org/pdf/2505.03991)
*Hao Xu, Arbind Agrahari Baniya, Sam Well, Mohamed Reda Bouadjenek, Richard Dazeley, Sunil Aryal*

Main category: cs.CV

TL;DR: A survey on video event detection in sports analytics, covering tasks like TAL, AS, and PES, with a focus on deep learning advancements, datasets, metrics, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To enhance sports analytics through automated event detection, improving performance analysis, viewer engagement, and broadcast efficiency.

Method: Review and categorization of datasets, evaluation metrics, and state-of-the-art techniques, including multi-modal and self-supervised learning approaches.

Result: Comprehensive overview of tasks, methodologies, and challenges in sports event detection, highlighting strengths and limitations.

Conclusion: Identifies open challenges and promising directions for future research to develop generalized, efficient, and robust event detection frameworks.

Abstract: Video event detection has become an essential component of sports analytics,
enabling automated identification of key moments and enhancing performance
analysis, viewer engagement, and broadcast efficiency. Recent advancements in
deep learning, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly improved accuracy and efficiency in Temporal
Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting
(PES). This survey provides a comprehensive overview of these three key tasks,
emphasizing their differences, applications, and the evolution of
methodological approaches. We thoroughly review and categorize existing
datasets and evaluation metrics specifically tailored for sports contexts,
highlighting the strengths and limitations of each. Furthermore, we analyze
state-of-the-art techniques, including multi-modal approaches that integrate
audio and visual information, methods utilizing self-supervised learning and
knowledge distillation, and approaches aimed at generalizing across multiple
sports. Finally, we discuss critical open challenges and outline promising
research directions toward developing more generalized, efficient, and robust
event detection frameworks applicable to diverse sports. This survey serves as
a foundation for future research on efficient, generalizable, and multi-modal
sports event detection.

</details>


### [60] [Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition](https://arxiv.org/pdf/2505.04502)
*Asma Baobaid, Mahmoud Meribout*

Main category: cs.CV

TL;DR: The paper proposes a method to maximize hardware engine usage in edge GPUs for video face detection and recognition, improving throughput and power efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance security and contactless access by optimizing concurrent task execution in edge GPUs, addressing limitations of prior works.

Method: Leverages concurrency and pipelining of tasks (face detection, recognition, video decoding) across all hardware engines in NVIDIA edge Orin GPUs.

Result: Achieved higher throughput, ~5% power savings (~300 mW), and real-time performance, with further gains from multi-stream processing.

Conclusion: Suggests hardware improvements for edge GPUs to reduce shuffle layers and further boost performance.

Abstract: Video face detection and recognition in public places at the edge is required
in several applications, such as security reinforcement and contactless access
to authorized venues. This paper aims to maximize the simultaneous usage of
hardware engines available in edge GPUs nowadays by leveraging the concurrency
and pipelining of tasks required for face detection and recognition. This also
includes the video decoding task, which is required in most face monitoring
applications as the video streams are usually carried via Gbps Ethernet
network. This constitutes an improvement over previous works where the tasks
are usually allocated to a single engine due to the lack of a unified and
automated framework that simultaneously explores all hardware engines. In
addition, previously, the input faces were usually embedded in still images or
within raw video streams that overlook the burst delay caused by the decoding
stage. The results on real-life video streams suggest that simultaneously using
all the hardware engines available in the recent NVIDIA edge Orin GPU, higher
throughput, and a slight saving of power consumption of around 300 mW,
accounting for around 5%, have been achieved while satisfying the real-time
performance constraint. The performance gets even higher by considering several
video streams simultaneously. Further performance improvement could have been
obtained if the number of shuffle layers that were created by the tensor RT
framework for the face recognition task was lower. Thus, the paper suggests
some hardware improvements to the existing edge GPU processors to enhance their
performance even higher.

</details>


### [61] [The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics](https://arxiv.org/pdf/2505.04006)
*Inamullah, Imran Razzak, Shoaib Jameel*

Main category: cs.CV

TL;DR: The paper explores how retinal imaging, enhanced by AI, serves as a non-invasive window for detecting and monitoring both ocular and systemic diseases, while discussing challenges and future directions in oculomics.


<details>
  <summary>Details</summary>
Motivation: The unique vascularized anatomy of the retina offers a non-invasive way to assess human health, driving the need for advanced imaging and AI integration.

Method: The survey reviews the evolution of retinal imaging techniques, AI-driven analysis, and the transition to oculomics.

Result: Retinal imaging combined with AI provides systemic health insights and non-invasive markers, advancing early disease detection and intervention.

Conclusion: Oculomics holds promise but faces hurdles; addressing research gaps and future directions is crucial for its progression.

Abstract: The unique vascularized anatomy of the human eye, encased in the retina,
provides an opportunity to act as a window for human health. The retinal
structure assists in assessing the early detection, monitoring of disease
progression and intervention for both ocular and non-ocular diseases. The
advancement in imaging technology leveraging Artificial Intelligence has seized
this opportunity to bridge the gap between the eye and human health. This track
paves the way for unveiling systemic health insight from the ocular system and
surrogating non-invasive markers for timely intervention and identification.
The new frontiers of oculomics in ophthalmology cover both ocular and systemic
diseases, and getting more attention to explore them. In this survey paper, we
explore the evolution of retinal imaging techniques, the dire need for the
integration of AI-driven analysis, and the shift of retinal imaging from
classical techniques to oculomics. We also discuss some hurdles that may be
faced in the progression of oculomics, highlighting the research gaps and
future directions.

</details>


### [62] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/pdf/2505.04524)
*Asma Baobaid, Mahmoud Meribout*

Main category: cs.CV

TL;DR: The paper proposes a hardware-software approach to optimize face detection and recognition on NVIDIA Jetson AGX Orin, achieving 290 FPS and reducing power consumption by 800 mW.


<details>
  <summary>Details</summary>
Motivation: Improving throughput and power consumption in face detection and recognition systems for public places, despite existing high-performance solutions.

Method: Combines simultaneous usage of all hardware engines in the Orin GPU and integrates a face tracker to avoid redundant recognition.

Result: Achieves 290 FPS on 1920x1080 frames with ~6 faces/frame and reduces power consumption by 800 mW.

Conclusion: The approach enables high-performance edge machine vision systems, ideal for multi-camera public monitoring.

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [63] [FoodTrack: Estimating Handheld Food Portions with Egocentric Video](https://arxiv.org/pdf/2505.04055)
*Ervin Wang, Yuhao Chen*

Main category: cs.CV

TL;DR: FoodTrack framework improves food volume tracking using egocentric video, achieving 7.01% error, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Accurate food tracking is vital for health monitoring, but traditional methods rely on restrictive conditions like specific camera angles or gesture recognition.

Method: FoodTrack uses egocentric video to measure food volume directly, handling occlusions and varying poses without relying on bite size assumptions.

Result: Achieves 7.01% absolute percentage loss, better than a previous 16.40% error under less flexible conditions.

Conclusion: FoodTrack offers a more accurate and adaptable solution for tracking food consumption.

Abstract: Accurately tracking food consumption is crucial for nutrition and health
monitoring. Traditional approaches typically require specific camera angles,
non-occluded images, or rely on gesture recognition to estimate intake, making
assumptions about bite size rather than directly measuring food volume. We
propose the FoodTrack framework for tracking and measuring the volume of
hand-held food items using egocentric video which is robust to hand occlusions
and flexible with varying camera and object poses. FoodTrack estimates food
volume directly, without relying on intake gestures or fixed assumptions about
bite size, offering a more accurate and adaptable solution for tracking food
consumption. We achieve absolute percentage loss of approximately 7.01% on a
handheld food object, improving upon a previous approach that achieved a 16.40%
mean absolute percentage error in its best case, under less flexible
conditions.

</details>


### [64] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/pdf/2505.04058)
*Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang*

Main category: cs.CV

TL;DR: A novel 2D-assisted 3D visual grounding framework improves object discrimination and relationship perception, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of distinguishing similar objects in 3D scenes using natural language due to the modality gap.

Method: Uses a dual-branch visual encoder with 2D pre-trained attributes and a cross-modal interaction module with graph attention for relationship fusion.

Result: Superior performance on benchmarks, especially in handling multiple similar distractors.

Conclusion: The framework effectively aligns 3D vision and language, enhancing object representation and relational learning.

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>


### [65] [SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation](https://arxiv.org/pdf/2505.04087)
*Zixuan Hu, Yichun Hu, Ling-Yu Duan*

Main category: cs.CV

TL;DR: SEVA is a novel TTA method that efficiently leverages data augmentations without extra computational cost, improving adaptation by optimizing an entropy loss upper bound and filtering harmful samples.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods underutilize reliable samples due to single-round entropy training, and augmentation strategies, while effective, are computationally expensive.

Method: SEVA introduces a theoretical framework to explore augmentation impacts, optimizing an upper bound of entropy loss in a single step and filtering harmful samples.

Result: SEVA demonstrates superior performance across various architectures and testing scenarios, meeting real-time TTA requirements.

Conclusion: SEVA effectively balances efficiency and performance, making it a practical solution for TTA challenges.

Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against
distribution shifts through rapid model adaptation during inference. While
existing TTA methods often rely on entropy-based unsupervised training and
achieve promising results, the common practice of a single round of entropy
training is typically unable to adequately utilize reliable samples, hindering
adaptation efficiency. In this paper, we discover augmentation strategies can
effectively unleash the potential of reliable samples, but the rapidly growing
computational cost impedes their real-time application. To address this
limitation, we propose a novel TTA approach named Single-step Ensemble of
Vicinal Augmentations (SEVA), which can take advantage of data augmentations
without increasing the computational burden. Specifically, instead of
explicitly utilizing the augmentation strategy to generate new data, SEVA
develops a theoretical framework to explore the impacts of multiple
augmentations on model adaptation and proposes to optimize an upper bound of
the entropy loss to integrate the effects of multiple rounds of augmentation
training into a single step. Furthermore, we discover and verify that using the
upper bound as the loss is more conducive to the selection mechanism, as it can
effectively filter out harmful samples that confuse the model. Combining these
two key advantages, the proposed efficient loss and a complementary selection
strategy can simultaneously boost the potential of reliable samples and meet
the stringent time requirements of TTA. The comprehensive experiments on
various network architectures across challenging testing scenarios demonstrate
impressive performances and the broad adaptability of SEVA. The code will be
publicly available.

</details>


### [66] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/pdf/2505.04088)
*Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He*

Main category: cs.CV

TL;DR: The paper proposes a Siamese Motion Mamba Tracker (SMMT) for thermal infrared (TIR) object tracking, addressing challenges like occlusion and motion blur with bidirectional state-space modeling and self-attention.


<details>
  <summary>Details</summary>
Motivation: TIR object tracking faces issues like occlusion, motion blur, and clutter, degrading tracker performance.

Method: SMMT integrates bidirectional state-space modeling and self-attention, uses a Siamese parameter-sharing strategy, and introduces a motion edge-aware regression loss.

Result: SMMT outperforms on benchmarks like LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017.

Conclusion: SMMT achieves superior TIR tracking performance by addressing key challenges with innovative modules and loss design.

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [67] [MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction](https://arxiv.org/pdf/2505.04105)
*Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim*

Main category: cs.CV

TL;DR: Proposes MAISY, a motion-aware image synthesis method using SAM and VS-SSIM loss to address limitations of GAN-based motion correction in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based methods overlook localized features and struggle with varying pixel intensities, luminance, and variance.

Method: Uses Segment Anything Model (SAM) to learn spatial patterns and introduces VS-SSIM loss for adaptive correction.

Result: Outperforms state-of-the-art with 40% PSNR, 10% SSIM, and 16% Dice improvements.

Conclusion: MAISY effectively corrects motion artifacts while preserving critical anatomical details.

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging.Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>


### [68] [One2Any: One-Reference 6D Pose Estimation for Any Object](https://arxiv.org/pdf/2505.04109)
*Mengya Liu, Siyuan Li, Ajad Chhatkuli, Prune Truong, Luc Van Gool, Federico Tombari*

Main category: cs.CV

TL;DR: One2Any is a novel method for 6D object pose estimation using a single RGB-D image, eliminating the need for 3D models, multi-view images, or category-specific training.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on complete 3D models, multi-view images, or category-specific training, limiting generalization to novel objects.

Method: One2Any uses an encoding-decoding framework: it encodes object shape, orientation, and texture into a Reference Object Pose Embedding (ROPE) from a single reference view, then decodes it into Reference Object Coordinates (ROC) for pose estimation.

Result: The method achieves state-of-the-art accuracy and robustness on benchmark datasets, outperforming methods requiring multi-view or CAD inputs.

Conclusion: One2Any demonstrates scalability and generalization to novel objects, offering a simpler and more efficient solution for 6D pose estimation.

Abstract: 6D object pose estimation remains challenging for many applications due to
dependencies on complete 3D models, multi-view images, or training limited to
specific object categories. These requirements make generalization to novel
objects difficult for which neither 3D models nor multi-view images may be
available. To address this, we propose a novel method One2Any that estimates
the relative 6-degrees of freedom (DOF) object pose using only a single
reference-single query RGB-D image, without prior knowledge of its 3D model,
multi-view data, or category constraints. We treat object pose estimation as an
encoding-decoding process, first, we obtain a comprehensive Reference Object
Pose Embedding (ROPE) that encodes an object shape, orientation, and texture
from a single reference view. Using this embedding, a U-Net-based pose decoding
module produces Reference Object Coordinate (ROC) for new views, enabling fast
and accurate pose estimation. This simple encoding-decoding framework allows
our model to be trained on any pair-wise pose data, enabling large-scale
training and demonstrating great scalability. Experiments on multiple benchmark
datasets demonstrate that our model generalizes well to novel objects,
achieving state-of-the-art accuracy and robustness even rivaling methods that
require multi-view or CAD inputs, at a fraction of compute.

</details>


### [69] [GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model](https://arxiv.org/pdf/2505.04119)
*Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou*

Main category: cs.CV

TL;DR: GAPrompt introduces a geometry-aware approach for efficient fine-tuning of 3D vision models, outperforming existing methods with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Fully fine-tuning pre-trained 3D models is resource-intensive, and current PEFT methods lack geometric awareness for point cloud data.

Method: Proposes GAPrompt with Point Prompt, Point Shift Prompter, and Prompt Propagation to enhance geometric adaptability.

Result: Achieves competitive performance with full fine-tuning while using only 2.19% of trainable parameters.

Conclusion: GAPrompt offers a parameter-efficient, geometry-aware solution for fine-tuning 3D vision models.

Abstract: Pre-trained 3D vision models have gained significant attention for their
promising performance on point cloud data. However, fully fine-tuning these
models for downstream tasks is computationally expensive and storage-intensive.
Existing parameter-efficient fine-tuning (PEFT) approaches, which focus
primarily on input token prompting, struggle to achieve competitive performance
due to their limited ability to capture the geometric information inherent in
point clouds. To address this challenge, we propose a novel Geometry-Aware
Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the
adaptability of 3D vision models. First, we introduce a Point Prompt that
serves as an auxiliary input alongside the original point cloud, explicitly
guiding the model to capture fine-grained geometric details. Additionally, we
present a Point Shift Prompter designed to extract global shape information
from the point cloud, enabling instance-specific geometric adjustments at the
input level. Moreover, our proposed Prompt Propagation mechanism incorporates
the shape information into the model's feature extraction process, further
strengthening its ability to capture essential geometric characteristics.
Extensive experiments demonstrate that GAPrompt significantly outperforms
state-of-the-art PEFT methods and achieves competitive results compared to full
fine-tuning on various benchmarks, while utilizing only 2.19% of trainable
parameters. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [70] [Vision Graph Prompting via Semantic Low-Rank Decomposition](https://arxiv.org/pdf/2505.04121)
*Zixiang Ai, Zichen Liu, Jiahuan Zhou*

Main category: cs.CV

TL;DR: ViG outperforms by using graph structures for images. VGP introduces a low-rank prompting method for ViG, improving transfer performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing prompting methods for Transformer-based models don't suit graph-based representations, limiting semantic modeling.

Method: Proposes Vision Graph Prompting (VGP), leveraging low-rank properties of graph components to integrate semantic and topological features.

Result: VGP significantly enhances ViG's transfer performance on downstream tasks, matching full fine-tuning with fewer parameters.

Conclusion: VGP effectively bridges the gap in graph-based prompting, offering a parameter-efficient solution for vision tasks.

Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as
graph structures, providing a more natural way to capture irregular semantic
patterns beyond traditional grid or sequence-based representations. To
efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning
techniques like visual prompting become increasingly essential. However,
existing prompting methods are primarily designed for Transformer-based models,
neglecting the rich topological relationships among nodes and edges in
graph-based representations, limiting their capacity to model complex
semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel
framework tailored for vision graph structures. Our core insight reveals that
semantically connected components in the graph exhibit low-rank properties.
Building on this observation, we introduce a semantic low-rank prompting method
that decomposes low-rank semantic features and integrates them with prompts on
vision graph topologies, capturing both global structural patterns and
fine-grained semantic dependencies. Extensive experiments demonstrate our
method significantly improves ViG's transfer performance on diverse downstream
tasks, achieving results comparable to full fine-tuning while maintaining
parameter efficiency. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [71] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/pdf/2505.04147)
*Lixing Niu, Jiapeng Li, Xingping Yu, Shu Wang, Ruining Feng, Bo Wu, Ping Wei, Yisen Wang, Lifeng Fan*

Main category: cs.CV

TL;DR: The paper introduces R^3-VQA, a comprehensive video dataset for social reasoning, evaluating LVLMs' capabilities and showing gaps in human-level reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing social reasoning tasks lack complexity, failing to mirror real-life challenges. The paper aims to bridge this gap with a detailed dataset.

Method: The authors created R^3-VQA, a dataset with fine-grained annotations of social events, mental states, and causal chains, including human and model-generated QAs.

Result: LVLMs fall short of human-level social reasoning, but ToM prompting improves their performance.

Conclusion: The dataset and findings highlight the need for better models in complex social reasoning.

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [72] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/pdf/2505.04150)
*Yu Yamaoka or Weng Ian Chan, Shigeto Seno, Soichiro Fukada, Hideo Matsuda*

Main category: cs.CV

TL;DR: The paper proposes OSLSP, a method for automated muscle tissue regeneration analysis using weakly supervised learning to address limitations of current LLP methods.


<details>
  <summary>Details</summary>
Motivation: Traditional muscle regeneration assessment relies on manual inspection, which lacks objectivity. Automated methods like LLP are limited by feature adaptation and loss of ordinal class information.

Method: The authors introduce OSLSP, a weakly supervised learning approach using similarity proportion loss and class proportion attention to preserve ordinal scale.

Result: OSLSP outperforms pre-trained and fine-tuning models in classifying skeletal muscle recovery stages.

Conclusion: OSLSP provides a quantitative, objective solution for muscle regeneration analysis, addressing key limitations of existing methods.

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [73] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.04175)
*Naphat Nithisopa, Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: A novel end-to-end framework combining ResNet and Vision Transformer with advanced techniques (Deformable Convolutions, Retrieval-Augmented Generation, CRF) achieves state-of-the-art OCR performance on six benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Text recognition in natural images is challenging but crucial for applications in computer vision and NLP.

Method: The framework uses Deformable Convolutions, adaptive dropout, and CRF for refined sequence modeling, replacing standard convolutions in ResNet and Vision Transformer.

Result: Achieves accuracies of 97.32% (IC13), 58.26% (IC15), 88.10% (SVT), 74.13% (IIIT5K), 82.17% (SVTP), and 66.67% (CUTE80), averaging 77.77%.

Conclusion: The method sets a new state-of-the-art, proving robustness across diverse datasets.

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [74] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/pdf/2505.04185)
*Hail Song, Wonsik Shin, Naeun Lee, Soomin Chung, Nojun Kwak, Woontack Woo*

Main category: cs.CV

TL;DR: S3D is a framework that converts 2D sketches into detailed 3D models using a U-Net-based architecture and style-alignment loss for improved fidelity.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating 3D models from sparse and ambiguous 2D sketches drives the need for a robust solution.

Method: Uses a U-Net encoder-decoder to create face segmentation masks, then generates 3D models with a style-alignment loss and sketch augmentation.

Result: S3D effectively produces high-quality 3D models from sketches, with enhanced reconstruction fidelity.

Conclusion: The framework demonstrates success in 3D model generation from sketches, with publicly available source code.

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [75] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/pdf/2505.04192)
*Trinh T. L. Vuong, Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA is a multimodal model for pathology that integrates single patch images, keyframe-extracted clips, and segmented videos to mimic pathologists' diagnostic process. It uses the VideoPath-Instruct dataset and knowledge transfer for training, achieving a benchmark in pathology video analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge visual narratives with diagnostic reasoning in pathology by mimicking the natural diagnostic process of pathologists.

Method: Integrates three image scenarios (single patch, keyframe clips, segmented videos) and uses the VideoPath-Instruct dataset for training, leveraging knowledge transfer from single-image datasets.

Result: Establishes a new benchmark in pathology video analysis, offering a foundation for AI systems supporting clinical decision-making.

Conclusion: VideoPath-LLaVA is a promising tool for enhancing diagnostic reasoning in pathology, with publicly available code, data, and model.

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [76] [SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios](https://arxiv.org/pdf/2505.04201)
*Ning Cheng, Jinan Xu, Jialing Chen, Wenjuan Han*

Main category: cs.CV

TL;DR: The paper introduces SToLa, a framework for integrating tactile sensing into multimodal reasoning, addressing modality discrepancy and data scarcity. It uses Mixture of Experts (MoE) and a new dataset for tactile commonsense reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable commonsense reasoning about the physical world by overcoming challenges in integrating tactile sensing, such as modality discrepancy and lack of diverse tactile data.

Method: Proposes SToLa, a Self-Adaptive Touch-Language framework using Mixture of Experts (MoE) to dynamically manage tactile and language modalities, alongside a new tactile commonsense reasoning dataset.

Result: SToLa shows competitive performance on benchmarks, demonstrating the effectiveness of MoE for multimodal management and tactile reasoning.

Conclusion: The framework and dataset advance tactile commonsense reasoning, proving the value of MoE in handling open-ended multimodal tasks.

Abstract: This paper explores the challenges of integrating tactile sensing into
intelligent systems for multimodal reasoning, particularly in enabling
commonsense reasoning about the open-ended physical world. We identify two key
challenges: modality discrepancy, where existing large touch-language models
often treat touch as a mere sub-modality of language, and open-ended tactile
data scarcity, where current datasets lack the diversity, open-endness and
complexity needed for reasoning. To overcome these challenges, we introduce
SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of
Experts (MoE) to dynamically process, unify, and manage tactile and language
modalities, capturing their unique characteristics. Crucially, we also present
a comprehensive tactile commonsense reasoning dataset and benchmark featuring
free-form questions and responses, 8 physical properties, 4 interactive
characteristics, and diverse commonsense knowledge. Experiments show SToLa
exhibits competitive performance compared to existing models on the PhysiCLeAR
benchmark and self-constructed datasets, proving the effectiveness of the
Mixture of Experts architecture in multimodal management and the performance
advantages for open-scenario tactile commonsense reasoning tasks.

</details>


### [77] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/pdf/2505.04207)
*Mustafa Yurdakul, Åakir Tasdemir*

Main category: cs.CV

TL;DR: The paper introduces a new RGB-D dataset (PothRGBD) and an improved YOLOv8-based model for accurate pothole detection and physical feature analysis, achieving higher precision, recall, and mAP than the standard YOLOv8n-seg model.


<details>
  <summary>Details</summary>
Motivation: Potholes cause safety and economic issues, but existing 2D RGB-based methods lack accuracy in analyzing physical characteristics. This work aims to improve detection and analysis using RGB-D data.

Method: A dataset of 1000 RGB-D images (PothRGBD) was created using an Intel RealSense D415 camera. The improved YOLOv8n-seg model incorporates DSConv, SimAM, and GELU for better segmentation and feature analysis.

Result: The proposed model achieved 93.7% precision, 90.4% recall, and 93.8% mAP@50, outperforming the standard YOLOv8n-seg model by 1.96%, 6.13%, and 2.07% respectively.

Conclusion: The model is lightweight, accurate, and suitable for real-time applications, providing a practical solution for intelligent transportation systems.

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [78] [CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models](https://arxiv.org/pdf/2505.04214)
*Fabian Wolf, Oliver TÃ¼selmann, Arthur Matei, Lukas Hennies, Christoph Rass, Gernot A. Fink*

Main category: cs.CV

TL;DR: The paper introduces a dataset (CM1) to evaluate few-shot capabilities of Large Vision Language Models (LVLMs) for extracting key-value info from handwritten documents, showing LVLMs outperform traditional methods with limited training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting key-value info from handwritten documents, especially with scarce annotated data, using LVLMs for mass digitization in archives.

Method: A novel dataset (CM1) of historic forms is used to benchmark LVLMs on extracting name and birthdate info, comparing them to a full-page extraction model under varying training set sizes.

Result: LVLMs outperform traditional full-page models when few training samples are available, leveraging their size and pretraining.

Conclusion: LVLMs are effective for few-shot extraction tasks in document analysis, offering advantages over classical methods in low-data scenarios.

Abstract: The automatic extraction of key-value information from handwritten documents
is a key challenge in document analysis. A reliable extraction is a
prerequisite for the mass digitization efforts of many archives. Large Vision
Language Models (LVLM) are a promising technology to tackle this problem
especially in scenarios where little annotated training data is available. In
this work, we present a novel dataset specifically designed to evaluate the
few-shot capabilities of LVLMs. The CM1 documents are a historic collection of
forms with handwritten entries created in Europe to administer the Care and
Maintenance program after World War Two. The dataset establishes three
benchmarks on extracting name and birthdate information and, furthermore,
considers different training set sizes. We provide baseline results for two
different LVLMs and compare performances to an established full-page extraction
model. While the traditional full-page model achieves highly competitive
performances, our experiments show that when only a few training samples are
available the considered LVLMs benefit from their size and heavy pretraining
and outperform the classical approach.

</details>


### [79] [A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation](https://arxiv.org/pdf/2505.04229)
*Theophilus Aidoo, Till Koebe, Akansh Maurya, Hewan Shrestha, Ingmar Weber*

Main category: cs.CV

TL;DR: A weak supervision framework uses 3m satellite imagery and coarse temporal labels to estimate parking lot occupancy, achieving high accuracy with minimal reliance on expensive high-resolution data.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity and high cost of labeled high-resolution imagery, especially in low-income regions, by proposing a cost-effective solution for remote sensing applications.

Method: Leverages coarse temporal labels (e.g., full on Saturdays, empty on Sundays) to train a pairwise comparison model using 3m resolution satellite imagery.

Result: Achieves an AUC of 0.92 on large parking lots, demonstrating high accuracy.

Conclusion: The framework is scalable for urban mobility analysis and adaptable for transit patterns and resource allocation in vulnerable communities, offering data-driven insights to improve well-being.

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>


### [80] [Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting](https://arxiv.org/pdf/2505.04262)
*Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li*

Main category: cs.CV

TL;DR: CSD improves 3D generation by coupling multi-view priors, addressing SDS's geometric inconsistencies and artifacts.


<details>
  <summary>Details</summary>
Motivation: SDS lacks multi-view correlation, leading to geometric flaws in 3D generation.

Method: CSD reformulates optimization as a multi-view joint problem, directly optimizes 3D Gaussian Splatting, and uses a deformable tetrahedral grid.

Result: CSD produces geometrically consistent 3D content with high-quality meshes.

Conclusion: CSD outperforms SDS in efficiency and quality for text-to-3D generation.

Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.

</details>


### [81] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/pdf/2505.04270)
*Yisen Feng, Haoyu Zhang, Meng Liu, Weili Guan, Liqiang Nie*

Main category: cs.CV

TL;DR: OSGNet improves egocentric video grounding by leveraging object information and shot movements, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook key egocentric video traits and fine-grained query details, limiting performance.

Method: OSGNet enriches video representation with object data and analyzes shot movements to align modalities.

Result: OSGNet achieves state-of-the-art results on three datasets.

Conclusion: OSGNet effectively addresses egocentric video grounding challenges, validated by superior performance.

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [82] [TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement](https://arxiv.org/pdf/2505.04281)
*Yi Li, Zhiyuan Zhang, Jiangnan Xia, Jianghan Cheng, Qilong Wu, Junwei Li, Yibin Tian, Hui Kong*

Main category: cs.CV

TL;DR: TS-Diff is a Two-Stage Diffusion Model for enhancing extremely low-light RAW images, using virtual cameras, CFI modules, and a color corrector, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in enhancing low-light RAW images, including noise, color shifts, and lack of generalizable models for diverse cameras.

Method: Uses a two-stage approach: pre-training with virtual cameras and CFI modules, followed by fine-tuning with real RAW data and structural reparameterization. Introduces a color corrector for consistency.

Result: Achieves top performance on QID, SID, and ELD datasets, excelling in denoising, generalization, and color consistency.

Conclusion: TS-Diff is robust and versatile, offering a practical solution for low-light imaging, with code and models publicly available.

Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing
extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes
noisy images by constructing multiple virtual cameras based on a noise space.
Camera Feature Integration (CFI) modules are then designed to enable the model
to learn generalizable features across diverse virtual cameras. During the
aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is
fine-tuned using a small amount of real RAW data to adapt to the noise
characteristics of specific cameras. A structural reparameterization technique
further simplifies CFI$^T$ for efficient deployment. To address color shifts
during the diffusion process, a color corrector is introduced to ensure color
consistency by dynamically adjusting global color distributions. Additionally,
a novel dataset, QID, is constructed, featuring quantifiable illumination
levels and a wide dynamic range, providing a comprehensive benchmark for
training and evaluation under extreme low-light conditions. Experimental
results demonstrate that TS-Diff achieves state-of-the-art performance on
multiple datasets, including QID, SID, and ELD, excelling in denoising,
generalization, and color consistency across various cameras and illumination
levels. These findings highlight the robustness and versatility of TS-Diff,
making it a practical solution for low-light imaging applications. Source codes
and models are available at https://github.com/CircccleK/TS-Diff

</details>


### [83] [MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition](https://arxiv.org/pdf/2505.04306)
*Qiannan Fan, Zhuoyang Li, Jitong Li, Chenyang Cao*

Main category: cs.CV

TL;DR: The paper proposes an identity-gated mixture of diffusion experts (MoDE) for occluded face recognition (OFR), improving performance by integrating multi-reconstructed faces and adapting predictions.


<details>
  <summary>Details</summary>
Motivation: Current OFR algorithms lack prior occlusion knowledge, leading to poor performance with real-world occlusions, impacting daily convenience.

Method: MoDE uses diffusion-based generative experts to estimate complete images for occluded faces, with an identity-gating network to evaluate and integrate predictions.

Result: Extensive experiments on public and wild datasets show superior performance for various occlusions compared to competing methods.

Conclusion: MoDE is an effective, plug-and-play solution for OFR, enhancing recognition accuracy for occluded faces.

Abstract: With the continuous impact of epidemics, people have become accustomed to
wearing masks. However, most current occluded face recognition (OFR) algorithms
lack prior knowledge of occlusions, resulting in poor performance when dealing
with occluded faces of varying types and severity in reality. Recognizing
occluded faces is still a significant challenge, which greatly affects the
convenience of people's daily lives. In this paper, we propose an
identity-gated mixture of diffusion experts (MoDE) for OFR. Each
diffusion-based generative expert estimates one possible complete image for
occluded faces. Considering the random sampling process of the diffusion model,
which introduces inevitable differences and variations between the inpainted
faces and the real ones. To ensemble effective information from
multi-reconstructed faces, we introduce an identity-gating network to evaluate
the contribution of each reconstructed face to the identity and adaptively
integrate the predictions in the decision space. Moreover, our MoDE is a
plug-and-play module for most existing face recognition models. Extensive
experiments on three public face datasets and two datasets in the wild validate
our advanced performance for various occlusions in comparison with the
competing methods.

</details>


### [84] [Multi-turn Consistent Image Editing](https://arxiv.org/pdf/2505.04320)
*Zijun Zhou, Yingying Deng, Xiangyu He, Weiming Dong, Fan Tang*

Main category: cs.CV

TL;DR: A multi-turn image editing framework improves iterative refinement, using flow matching and LQR for stability, and adaptive attention for coherence.


<details>
  <summary>Details</summary>
Motivation: Existing single-step image editing methods struggle with ambiguity, complex transformations, and progressive refinements, leading to inconsistent results.

Method: Proposes a multi-turn framework with flow matching for inversion, LQR for stable sampling, and adaptive attention highlighting for coherence.

Result: Significantly improves edit success rates and visual fidelity compared to existing methods.

Conclusion: The framework effectively addresses challenges in iterative image editing, offering better user satisfaction and consistency.

Abstract: Many real-world applications, such as interactive photo retouching, artistic
content creation, and product design, require flexible and iterative image
editing. However, existing image editing methods primarily focus on achieving
the desired modifications in a single step, which often struggles with
ambiguous user intent, complex transformations, or the need for progressive
refinements. As a result, these methods frequently produce inconsistent
outcomes or fail to meet user expectations. To address these challenges, we
propose a multi-turn image editing framework that enables users to iteratively
refine their edits, progressively achieving more satisfactory results. Our
approach leverages flow matching for accurate image inversion and a
dual-objective Linear Quadratic Regulators (LQR) for stable sampling,
effectively mitigating error accumulation. Additionally, by analyzing the
layer-wise roles of transformers, we introduce a adaptive attention
highlighting method that enhances editability while preserving multi-turn
coherence. Extensive experiments demonstrate that our framework significantly
improves edit success rates and visual fidelity compared to existing methods.

</details>


### [85] [CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion](https://arxiv.org/pdf/2505.04347)
*Yanyu Li, Pencheng Wan, Liang Han, Yaowei Wang, Liqiang Nie, Min Zhang*

Main category: cs.CV

TL;DR: CountDiffusion is a training-free framework to improve object quantity accuracy in text-to-image generation by correcting attention maps using a counting model.


<details>
  <summary>Details</summary>
Motivation: Training models for accurate object quantity in images is computationally expensive and conceptually challenging.

Method: CountDiffusion uses a two-stage process: predicting an intermediate image with a counting model, then correcting object quantity via attention map adjustments.

Result: CountDiffusion significantly enhances the ability of T2I models to generate images with correct object quantities.

Conclusion: The framework is effective, training-free, and compatible with existing diffusion-based T2I models.

Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to
generate images with accurate object quantity is still difficult due to the
high computational cost and the challenge of teaching models the abstract
concept of quantity. In this paper, we propose CountDiffusion, a training-free
framework aiming at generating images with correct object quantity from textual
descriptions. CountDiffusion consists of two stages. In the first stage, an
intermediate denoising result is generated by the diffusion model to predict
the final synthesized image with one-step denoising, and a counting model is
used to count the number of objects in this image. In the second stage, a
correction module is used to correct the object quantity by changing the
attention map of the object with universal guidance. The proposed
CountDiffusion can be plugged into any diffusion-based text-to-image (T2I)
generation models without further training. Experiment results demonstrate the
superiority of our proposed CountDiffusion, which improves the accurate object
quantity generation ability of T2I models by a large margin.

</details>


### [86] [WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing](https://arxiv.org/pdf/2505.04369)
*Jie Sun, Heng Liu, Yongzhen Wang, Xiao-Ping Zhang, Mingqiang Wei*

Main category: cs.CV

TL;DR: The paper introduces WDMamba, a novel dehazing framework leveraging a haze-specific wavelet degradation prior. It decomposes dehazing into low-frequency restoration and detail enhancement, using Mamba blocks for efficiency and self-guided contrastive regularization for improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of image dehazing by exploiting the observation that haze-related information primarily resides in low-frequency components, leading to a more effective restoration process.

Method: The method involves a two-stage coarse-to-fine strategy: low-frequency restoration using Mamba blocks for global structure reconstruction, followed by detail enhancement. Self-guided contrastive regularization is introduced during training to enhance detail retention.

Result: Extensive evaluations show WDMamba outperforms state-of-the-art methods in both qualitative and quantitative metrics on public dehazing benchmarks.

Conclusion: The proposed WDMamba framework effectively addresses image dehazing by leveraging wavelet degradation prior and a two-stage approach, achieving superior results.

Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior
observed through wavelet transform analysis, which shows that haze-related
information predominantly resides in low-frequency components. Exploiting this
insight, we propose a novel dehazing framework, WDMamba, which decomposes the
image dehazing task into two sequential stages: low-frequency restoration
followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to
effectively capture features specific to each stage of the dehazing process,
resulting in high-quality restored images. Specifically, in the low-frequency
restoration stage, we integrate Mamba blocks to reconstruct global structures
with linear complexity, efficiently removing overall haze and producing a
coarse restored image. Thereafter, the detail enhancement stage reinstates
fine-grained information that may have been overlooked during the previous
phase, culminating in the final dehazed output. Furthermore, to enhance detail
retention and achieve more natural dehazing, we introduce a self-guided
contrastive regularization during network training. By utilizing the coarse
restored output as a hard negative example, our model learns more
discriminative representations, substantially boosting the overall dehazing
performance. Extensive evaluations on public dehazing benchmarks demonstrate
that our method surpasses state-of-the-art approaches both qualitatively and
quantitatively. Code is available at https://github.com/SunJ000/WDMamba.

</details>


### [87] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/pdf/2505.04375)
*Moseli Mots'oehli, Hope Mogale, Kyungim Baek*

Main category: cs.CV

TL;DR: The study explores the impact of model size and label noise on vision transformers (ViTs) and Swin Transformers, finding larger ViTs (ViTl32) outperform smaller ones in accuracy and calibration under noise, while Swin Transformers are less robust. Active Learning helps moderately but harms calibration at high noise.


<details>
  <summary>Details</summary>
Motivation: To understand how model size and label noise affect vision transformers' performance and calibration, especially in resource-constrained settings.

Method: Evaluated four ViT and three Swin Transformer configurations on CIFAR10/100 under varying label noise rates, comparing accuracy and calibration.

Result: Larger ViTs (ViTl32) perform better under noise; Swin Transformers are less robust. Active Learning aids accuracy moderately but worsens calibration at high noise.

Conclusion: Larger ViTs are more practical under noise, while Swin Transformers struggle. Active Learning's benefits are limited by calibration trade-offs. Insights aid deployment in resource-constrained scenarios.

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [88] [Label-efficient Single Photon Images Classification via Active Learning](https://arxiv.org/pdf/2505.04376)
*Zili Zhang, Ziting Wen, Yiheng Qiang, Hongzhou Dong, Wenle Dong, Xinyang Li, Xiaofan Wang, Xiaoqiang Ren*

Main category: cs.CV

TL;DR: The paper introduces an active learning framework for single-photon image classification, using a condition-aware sampling strategy to reduce annotation costs while achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Semantic interpretation of single-photon images is underexplored due to high annotation costs and inefficient labeling.

Method: Proposes an imaging condition-aware sampling strategy with synthetic augmentation to selectively annotate informative samples.

Result: Achieves 97% accuracy on synthetic data with 1.5% labeled samples and 90.63% on real-world data with 8% labeled samples, outperforming baselines.

Conclusion: Active learning enables efficient classification of single-photon images, facilitating their large-scale real-world use.

Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme
environments through quantum-level photon detection technology. Current
research primarily focuses on reconstructing 3D scenes from sparse photon
events, whereas the semantic interpretation of single-photon images remains
underexplored, due to high annotation costs and inefficient labeling
strategies. This paper presents the first active learning framework for
single-photon image classification. The core contribution is an imaging
condition-aware sampling strategy that integrates synthetic augmentation to
model variability across imaging conditions. By identifying samples where the
model is both uncertain and sensitive to these conditions, the proposed method
selectively annotates only the most informative examples. Experiments on both
synthetic and real-world datasets show that our approach outperforms all
baselines and achieves high classification accuracy with significantly fewer
labeled samples. Specifically, our approach achieves 97% accuracy on synthetic
single-photon data using only 1.5% labeled samples. On real-world data, we
maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher
than the best-performing baseline. This illustrates that active learning
enables the same level of classification performance on single-photon images as
on classical images, opening doors to large-scale integration of single-photon
data in real-world applications.

</details>


### [89] [Tetrahedron-Net for Medical Image Registration](https://arxiv.org/pdf/2505.04380)
*Jinhai Xiang, Shuai Guo, Qianru Han, Dantong Shi, Xinwei He, Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces Tetrahedron-Net, a new architecture for medical image registration, adding an extra decoder to U-Net-like structures for improved feature interaction and performance.


<details>
  <summary>Details</summary>
Motivation: Current U-Net-like networks for medical image registration don't fully explore interactions within single encoder-decoder architectures, limiting representation capacity.

Method: Proposes Tetrahedron-Net, adding a second decoder to interact with both the encoder and original decoder, forming a 'Tetrahedron' structure.

Result: Experiments show superior performance on medical image registration benchmarks, with consistent gains when integrated into existing U-Net-like architectures.

Conclusion: The Tetrahedron-Net design is simple, effective, and generalizable, enhancing registration accuracy by improving feature interactions.

Abstract: Medical image registration plays a vital role in medical image processing.
Extracting expressive representations for medical images is crucial for
improving the registration quality. One common practice for this end is
constructing a convolutional backbone to enable interactions with skip
connections among feature extraction layers. The de facto structure, U-Net-like
networks, has attempted to design skip connections such as nested or full-scale
ones to connect one single encoder and one single decoder to improve its
representation capacity. Despite being effective, it still does not fully
explore interactions with a single encoder and decoder architectures. In this
paper, we embrace this observation and introduce a simple yet effective
alternative strategy to enhance the representations for registrations by
appending one additional decoder. The new decoder is designed to interact with
both the original encoder and decoder. In this way, it not only reuses feature
presentation from corresponding layers in the encoder but also interacts with
the original decoder to corporately give more accurate registration results.
The new architecture is concise yet generalized, with only one encoder and two
decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.
Three instantiations of Tetrahedron-Net are further constructed regarding the
different structures of the appended decoder. Our extensive experiments prove
that superior performance can be obtained on several representative benchmarks
of medical image registration. Finally, such a ``Tetrahedron'' design can also
be easily integrated into popular U-Net-like architectures including
VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.

</details>


### [90] [DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution](https://arxiv.org/pdf/2505.04384)
*Ming-Hui Liu, Xiao-Qian Liu, Xin Luo, Xin-Shun Xu*

Main category: cs.CV

TL;DR: The paper proposes DATA, a multi-DisentAnglement based conTrastive leArning framework, to improve generalization for open-world semi-supervised deepfake attribution by disentangling method-specific features and enhancing novel class discovery.


<details>
  <summary>Details</summary>
Motivation: Address limitations of previous methods that overfit to method-specific clues and struggle with novel classes in open-world scenarios.

Method: Introduces 'Orthonormal Deepfake Basis' to disentangle features and uses augmented-memory for contrastive learning to improve class boundaries.

Result: Achieves state-of-the-art performance with notable accuracy improvements (2.55%/5.7%) on the OSS-DFA benchmark.

Conclusion: DATA effectively enhances generalization and discrimination for deepfake attribution in open-world settings.

Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different
facial manipulation techniques, thereby mitigating the detrimental effects of
forgery content on the social order and personal reputations. However, previous
methods focus only on method-specific clues, which easily lead to overfitting,
while overlooking the crucial role of common forgery features. Additionally,
they struggle to distinguish between uncertain novel classes in more practical
open-world scenarios. To address these issues, in this paper we propose an
innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to
enhance the generalization ability on novel classes for the open-world
semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all
generation techniques can be abstracted into a similar architecture, DATA
defines the concept of 'Orthonormal Deepfake Basis' for the first time and
utilizes it to disentangle method-specific features, thereby reducing the
overfitting on forgery-irrelevant information. Furthermore, an augmented-memory
mechanism is designed to assist in novel class discovery and contrastive
learning, which aims to obtain clear class boundaries for the novel classes
through instance-level disentanglements. Additionally, to enhance the
standardization and discrimination of features, DATA uses bases contrastive
loss and center contrastive loss as auxiliaries for the aforementioned modules.
Extensive experimental evaluations show that DATA achieves state-of-the-art
performance on the OSS-DFA benchmark, e.g., there are notable accuracy
improvements in 2.55% / 5.7% under different settings, compared with the
existing methods.

</details>


### [91] [Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle](https://arxiv.org/pdf/2505.04392)
*Petr Jahoda, Jan Cech*

Main category: cs.CV

TL;DR: A novel method detects road anomalies by tracking a preceding vehicle, working in low visibility or dense traffic. It compensates for camera motion and performs in real time.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on direct observation, which is limited in low visibility or dense traffic. This approach aims to predict anomalies before encountering them.

Method: Uses visual tracking of a preceding vehicle and compensates for camera pitch rotation with an iterative robust estimator.

Result: Reliably detects road anomalies at a distance, even in challenging conditions, and operates in real time.

Conclusion: The method is versatile, effective, and suitable for real-time applications on standard hardware.

Abstract: A novel approach to detect road surface anomalies by visual tracking of a
preceding vehicle is proposed. The method is versatile, predicting any kind of
road anomalies, such as potholes, bumps, debris, etc., unlike direct
observation methods that rely on training visual detectors of those cases. The
method operates in low visibility conditions or in dense traffic where the
anomaly is occluded by a preceding vehicle. Anomalies are detected
predictively, i.e., before a vehicle encounters them, which allows to
pre-configure low-level vehicle systems (such as chassis) or to plan an
avoidance maneuver in case of autonomous driving. A challenge is that the
signal coming from camera-based tracking of a preceding vehicle may be weak and
disturbed by camera ego motion due to vibrations affecting the ego vehicle.
Therefore, we propose an efficient method to compensate camera pitch rotation
by an iterative robust estimator. Our experiments on both controlled setup and
normal traffic conditions show that road anomalies can be detected reliably at
a distance even in challenging cases where the ego vehicle traverses imperfect
road surfaces. The method is effective and performs in real time on standard
consumer hardware.

</details>


### [92] [Deep residual learning with product units](https://arxiv.org/pdf/2505.04397)
*Ziyuan Li, Uwe Jaekel, Babette Dellen*

Main category: cs.CV

TL;DR: PURe, a deep product-unit residual neural network, enhances expressiveness and efficiency by integrating product units into residual blocks, outperforming deeper ResNet models on multiple benchmarks with fewer parameters and faster convergence.


<details>
  <summary>Details</summary>
Motivation: To improve the expressiveness and parameter efficiency of deep convolutional networks by leveraging multiplicative feature interactions through product units, addressing limitations of standard summation neurons.

Method: PURe replaces conventional convolutional layers with 2D product units in residual blocks, eliminating nonlinear activation functions to preserve structural information.

Result: PURe achieves superior accuracy on Galaxy10 DECaLS (84.89%), ImageNet (top-1: 80.27%, top-5: 95.78%), and CIFAR-10 (95.01%) with fewer parameters and faster convergence than ResNet variants.

Conclusion: PURe balances accuracy, efficiency, and robustness, demonstrating the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [93] [MFSeg: Efficient Multi-frame 3D Semantic Segmentation](https://arxiv.org/pdf/2505.04408)
*Chengjie Huang, Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: MFSeg is an efficient multi-frame 3D semantic segmentation framework that reduces computational overhead while maintaining accuracy by feature-level aggregation and regularization.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and redundancy in existing multi-frame 3D semantic segmentation methods.

Method: Aggregates point cloud sequences at the feature level, regularizes feature extraction, and uses a lightweight MLP-based point decoder to avoid redundant upsampling.

Result: Outperforms existing methods on nuScenes and Waymo datasets, showing effectiveness and efficiency.

Conclusion: MFSeg is a promising solution for efficient and accurate multi-frame 3D semantic segmentation.

Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation
framework. By aggregating point cloud sequences at the feature level and
regularizing the feature extraction and aggregation process, MFSeg reduces
computational overhead while maintaining high accuracy. Moreover, by employing
a lightweight MLP-based point decoder, our method eliminates the need to
upsample redundant points from past frames. Experiments on the nuScenes and
Waymo datasets show that MFSeg outperforms existing methods, demonstrating its
effectiveness and efficiency.

</details>


### [94] [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/pdf/2505.04410)
*Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP improves CLIP for dense prediction by decoupling self-attention into 'content' and 'context' features, enhancing local discriminability and spatial consistency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of CLIP in dense prediction tasks due to poor local feature representation and spatial consistency.

Method: Proposes DeCLIP, decoupling self-attention into 'content' (aligned with image crops) and 'context' (retaining spatial correlations) features.

Result: DeCLIP outperforms existing methods in open-vocabulary dense prediction tasks like object detection and semantic segmentation.

Conclusion: DeCLIP effectively enhances CLIP for dense prediction, achieving superior performance in real-world scenarios.

Abstract: Dense visual prediction tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense prediction often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. The ``content'' features are aligned with image crop
representations to improve local discriminability, while ``context'' features
learn to retain the spatial correlations under the guidance of vision
foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP
significantly outperforms existing methods across multiple open-vocabulary
dense prediction tasks, including object detection and semantic segmentation.
Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.

</details>


### [95] [RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation](https://arxiv.org/pdf/2505.04424)
*Jing Hu, Chengming Feng, Shu Hu, Ming-Ching Chang, Xin Li, Xi Wu, Xin Wang*

Main category: cs.CV

TL;DR: A reinforcement learning-based framework, RLMiniStyler, is proposed for efficient and lightweight arbitrary style transfer, leveraging iterative feedback and multi-task learning for high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for style transfer are computationally expensive, prompting the need for a more efficient solution.

Method: Uses reinforcement learning to iteratively guide style transfer and an uncertainty-aware multi-task learning strategy to balance content and style.

Result: Validated as superior to state-of-the-art methods, generating high-quality stylized images at lower computational cost.

Conclusion: RLMiniStyler offers an efficient, lightweight solution for diverse style transfer with faster convergence and better performance.

Abstract: Arbitrary style transfer aims to apply the style of any given artistic image
to another content image. Still, existing deep learning-based methods often
require significant computational costs to generate diverse stylized results.
Motivated by this, we propose a novel reinforcement learning-based framework
for arbitrary style transfer RLMiniStyler. This framework leverages a unified
reinforcement learning policy to iteratively guide the style transfer process
by exploring and exploiting stylization feedback, generating smooth sequences
of stylized results while achieving model lightweight. Furthermore, we
introduce an uncertainty-aware multi-task learning strategy that automatically
adjusts loss weights to adapt to the content and style balance requirements at
different training stages, thereby accelerating model convergence. Through a
series of experiments across image various resolutions, we have validated the
advantages of RLMiniStyler over other state-of-the-art methods in generating
high-quality, diverse artistic image sequences at a lower cost. Codes are
available at https://github.com/fengxiaoming520/RLMiniStyler.

</details>


### [96] [RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance](https://arxiv.org/pdf/2311.18681)
*Chantal Pellegrini, Ege Ãzsoy, Benjamin Busam, Nassir Navab, Matthias Keicher*

Main category: cs.CV

TL;DR: RaDialog is a conversational AI for radiology report generation and interactive dialog, integrating visual and pathology data with an LLM, achieving state-of-the-art clinical correctness.


<details>
  <summary>Details</summary>
Motivation: To transform radiology by facilitating collaborative diagnostics, saving time, and improving report quality.

Method: Integrates visual features and pathology findings with an LLM, using parameter-efficient fine-tuning and a semi-automatically labeled dataset for chest X-ray tasks.

Result: Achieves state-of-the-art clinical correctness in report generation and excels in interactive tasks like report correction and Q&A.

Conclusion: RaDialog serves as a foundational step toward clinical dialog systems, with code publicly available.

Abstract: Conversational AI tools that can generate and discuss clinically correct
radiology reports for a given medical image have the potential to transform
radiology. Such a human-in-the-loop radiology assistant could facilitate a
collaborative diagnostic process, thus saving time and improving the quality of
reports. Towards this goal, we introduce RaDialog, the first thoroughly
evaluated and publicly available large vision-language model for radiology
report generation and interactive dialog. RaDialog effectively integrates
visual image features and structured pathology findings with a large language
model (LLM) while simultaneously adapting it to a specialized domain using
parameter-efficient fine-tuning. To keep the conversational abilities of the
underlying LLM, we propose a comprehensive, semi-automatically labeled,
image-grounded instruct dataset for chest X-ray radiology tasks. By training
with this dataset, our method achieves state-of-the-art clinical correctness in
report generation and shows impressive abilities in interactive tasks such as
correcting reports and answering questions, serving as a foundational step
toward clinical dialog systems. Our code is available on github:
https://github.com/ChantalMP/RaDialog.

</details>


### [97] [Learning Real Facial Concepts for Independent Deepfake Detection](https://arxiv.org/pdf/2505.04460)
*Ming-Hui Liu, Harry Cheng, Tianyi Wang, Xin Luo, Xin-Shun Xu*

Main category: cs.CV

TL;DR: RealID improves deepfake detection by learning real face concepts and independent classification, outperforming state-of-the-art methods by 1.74% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models generalize poorly due to reliance on forgery artifacts and limited real face understanding.

Method: RealID uses Real Concept Capture Module (RealC2) and Independent Dual-Decision Classifier (IDC) to learn real face concepts and classify independently.

Result: Achieves 1.74% higher average accuracy on five datasets compared to existing methods.

Conclusion: RealID enhances generalization by reducing reliance on forgery-irrelevant patterns and improving real face understanding.

Abstract: Deepfake detection models often struggle with generalization to unseen
datasets, manifesting as misclassifying real instances as fake in target
domains. This is primarily due to an overreliance on forgery artifacts and a
limited understanding of real faces. To address this challenge, we propose a
novel approach RealID to enhance generalization by learning a comprehensive
concept of real faces while assessing the probabilities of belonging to the
real and fake classes independently. RealID comprises two key modules: the Real
Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier
(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various
prototypes for real faces, allowing the model to capture a comprehensive
concept of real class. Meanwhile, IDC redefines the classification strategy by
making independent decisions based on the concept of the real class and the
presence of forgery artifacts. Through the combined effect of the above
modules, the influence of forgery-irrelevant patterns is alleviated, and
extensive experiments on five widely used datasets demonstrate that RealID
significantly outperforms existing state-of-the-art methods, achieving a 1.74%
improvement in average accuracy.

</details>


### [98] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/pdf/2505.04481)
*Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou*

Main category: cs.CV

TL;DR: The paper introduces CAD-Llama, a framework to enhance LLMs for generating parametric 3D CAD models by translating sequences into structured code and using adaptive pretraining.


<details>
  <summary>Details</summary>
Motivation: To expand LLMs' capabilities into domain-specific tasks like parametric CAD model generation, addressing their lack of exposure to parametric sequences and 3D awareness.

Method: Develops a hierarchical annotation pipeline and structured code format (SPCC), followed by adaptive pretraining and instruction tuning for CAD-specific tasks.

Result: CAD-Llama outperforms prior autoregressive methods and LLM baselines in generating parametric 3D CAD models.

Conclusion: The framework successfully equips LLMs with spatial knowledge for CAD tasks, demonstrating significant improvement over existing methods.

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>


### [99] [Vision-Language Models Create Cross-Modal Task Representations](https://arxiv.org/pdf/2410.22330)
*Grace Luo, Trevor Darrell, Amir Bar*

Main category: cs.CV

TL;DR: VLMs align inputs into shared task vectors, enabling cross-modal transfer and outperforming full-task prompting.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs process task information and align inputs across modalities.

Method: Measure cross-modal transfer of task vectors across tasks and architectures.

Result: Task vectors outperform full-task prompting and can transfer between base and fine-tuned models.

Conclusion: VLMs map modalities into common semantic representations via task vectors, simplifying processing.

Abstract: Autoregressive vision-language models (VLMs) can handle many tasks within a
single model, yet the representations that enable this capability remain
opaque. We find that VLMs align conceptually equivalent inputs into a shared
task vector, which is invariant to modality (text, image) and format (examples,
instruction), and may simplify VLM processing. We measure this alignment via
cross-modal transfer -- the ability of a task vector derived in one modality to
trigger the correct generation in another -- on a range of tasks and model
architectures. Although the task vector is highly compressed, we find that this
single vector outperforms prompting the model with the full task information,
unique to this cross-modal case. Furthermore, we show that task vectors can be
transferred from a base language model to its fine-tuned vision-language
counterpart, and that they can be derived solely from instructions without the
need for examples. Taken together, our findings shed light on how VLMs
internally process task information, and how they map different modalities into
common semantic representations. Project page:
https://vlm-cross-modal-reps.github.io.

</details>


### [100] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/pdf/2505.04485)
*Ali Alawieh, Alexandru P. Condurache*

Main category: cs.CV

TL;DR: FA-KPConv enhances KPConv by making it exactly invariant/equivariant to Euclidean transformations, improving performance in tasks like classification and registration, especially with limited data or random rotations.


<details>
  <summary>Details</summary>
Motivation: KPConv lacks exact invariance/equivariance to Euclidean transformations, requiring large datasets or augmentations. FA-KPConv addresses this by embedding geometric priors.

Method: FA-KPConv wraps KPConv layers with Frame Averaging, ensuring exact invariance/equivariance to translations, rotations, and reflections without adding parameters or losing input data.

Result: FA-KPConv improves performance in point cloud classification and registration, particularly with scarce training data or randomly rotated test data.

Conclusion: FA-KPConv effectively integrates geometric priors into KPConv, enhancing its robustness and accuracy in 3D point cloud tasks.

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>


### [101] [LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation](https://arxiv.org/pdf/2411.04997)
*Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu*

Main category: cs.CV

TL;DR: The paper proposes integrating LLMs into CLIP to enhance its capability for processing complex captions, using a novel caption-to-caption contrastive fine-tuning framework. This method outperforms existing approaches in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs' superior text understanding and open-world knowledge to improve CLIP's performance, especially for longer and more complex image captions.

Method: An efficient post-training strategy integrating LLMs into CLIP, using a caption-to-caption contrastive fine-tuning framework to address LLMs' autoregressive nature.

Result: Outperforms LoRA-based methods with nearly fourfold faster training and superior performance, surpassing models like CLIP, EVA02, and SigLip2 in various tasks.

Conclusion: The proposed approach significantly enhances CLIP's capabilities, demonstrating broad improvements in multimodal and cross-lingual retrieval tasks.

Abstract: CLIP is a foundational multimodal model that aligns image and text features
into a shared representation space via contrastive learning on large-scale
image-text pairs. Its effectiveness primarily stems from the use of natural
language as rich supervision. Motivated by the remarkable advancements in large
language models (LLMs), this work explores how LLMs' superior text
understanding and extensive open-world knowledge can enhance CLIP's capability,
especially for processing longer and more complex image captions. We propose an
efficient post-training strategy that integrates LLMs into pretrained CLIP. To
address the challenge posed by the autoregressive nature of LLMs, we introduce
a caption-to-caption contrastive fine-tuning framework, significantly enhancing
the discriminative quality of LLM outputs. Extensive experiments demonstrate
that our approach outperforms LoRA-based methods, achieving nearly fourfold
faster training with superior performance. Furthermore, we validate substantial
improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2
across various zero-shot multimodal retrieval tasks, cross-lingual retrieval
tasks, and multimodal language model pretraining.

</details>


### [102] [Efficient Flow Matching using Latent Variables](https://arxiv.org/pdf/2505.04486)
*Anirban Samaddar, Yixuan Sun, Viktor Nilsson, Sandeep Madireddy*

Main category: cs.CV

TL;DR: Latent-CFM improves flow matching models by incorporating multi-modal data structures using pretrained latent variable models, reducing training time and enhancing generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing flow matching models inefficiently learn high-dimensional data due to ignoring underlying structures, leading to suboptimal performance.

Method: Latent-CFM simplifies training/inference by leveraging pretrained deep latent variable models to handle multi-modal data.

Result: Latent-CFM achieves better generation quality with 50% less training, outperforms state-of-the-art models, and generates physically accurate samples.

Conclusion: Latent-CFM offers a scalable and efficient solution for flow matching, enabling conditional image generation and improved performance.

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [103] [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/pdf/2411.05261)
*Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Zhiling Yue, Yijian Gao, Junzhi Ning, Zhi Li, Simon Walsh, Guang Yang*

Main category: cs.CV

TL;DR: The paper introduces CVLM, a cyclic vision-language manipulator, to identify key image features in X-rays that influence AI-generated reports, improving transparency and reliability.


<details>
  <summary>Details</summary>
Motivation: Address the opaqueness and reliability concerns in AI-generated medical reports by clarifying which image features drive report changes.

Method: Proposes CVLM to generate manipulated X-rays and reports, enabling cyclic comparison to pinpoint critical features.

Result: CVLM identifies more precise and reliable features than existing methods, enhancing report transparency.

Conclusion: CVLM improves the interpretability and trustworthiness of AI-generated medical reports.

Abstract: Despite significant advancements in automated report generation, the
opaqueness of text interpretability continues to cast doubt on the reliability
of the content produced. This paper introduces a novel approach to identify
specific image features in X-ray images that influence the outputs of report
generation models. Specifically, we propose Cyclic Vision-Language Manipulator
CVLM, a module to generate a manipulated X-ray from an original X-ray and its
report from a designated report generator. The essence of CVLM is that cycling
manipulated X-rays to the report generator produces altered reports aligned
with the alterations pre-injected into the reports for X-ray generation,
achieving the term "cyclic manipulation". This process allows direct comparison
between original and manipulated X-rays, clarifying the critical image features
driving changes in reports and enabling model users to assess the reliability
of the generated texts. Empirical evaluations demonstrate that CVLM can
identify more precise and reliable features compared to existing explanation
methods, significantly enhancing the transparency and applicability of
AI-generated reports.

</details>


### [104] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/pdf/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: The paper introduces quantitative measures to evaluate the creativity of generative AI models, aiding users in selecting the right model for tasks, with results aligning with human intuition.


<details>
  <summary>Details</summary>
Motivation: To address the ongoing debate about the creativity of generative AI models by providing practical, measurable criteria for evaluation.

Method: Developed quantitative measures for creativity and tested them on popular image-to-image generation models.

Result: The proposed measures align with human intuition, suggesting their practical utility.

Conclusion: Quantitative measures can effectively evaluate AI creativity, helping users make informed model choices.

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [105] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/pdf/2505.04512)
*Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanCustom is a multi-modal video generation framework that enhances subject consistency and supports diverse input conditions like image, audio, video, and text, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for customized video generation struggle with identity consistency and limited input modalities, prompting the need for a more robust solution.

Method: The framework introduces a text-image fusion module, image ID enhancement, AudioNet for audio alignment, and a video-driven injection module for multi-modal conditioning.

Result: HunyuanCustom outperforms state-of-the-art methods in ID consistency, realism, and text-video alignment, validated across various scenarios.

Conclusion: Multi-modal conditioning and identity-preserving strategies effectively advance controllable video generation, as demonstrated by HunyuanCustom.

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>


### [106] [Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model](https://arxiv.org/pdf/2505.04522)
*Pengfei Guo, Can Zhao, Dong Yang, Yufan He, Vishwesh Nath, Ziyue Xu, Pedro R. A. S. Bassi, Zongwei Zhou, Benjamin D. Simon, Stephanie Anne Harmon, Baris Turkbey, Daguang Xu*

Main category: cs.CV

TL;DR: Text2CT generates 3D CT volumes from free-text descriptions using a diffusion model, outperforming fixed-format methods and achieving high anatomical fidelity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between semantic text inputs and detailed 3D CT scans for diagnostics and research.

Method: Uses a diffusion model with a novel prompt formulation to encode medical text into latent representations and decode them into high-resolution 3D CT scans.

Result: Superior performance in preserving anatomical fidelity and capturing intricate structures, achieving state-of-the-art results.

Conclusion: Text2CT offers promising applications in diagnostics and data augmentation, demonstrating the potential of free-text-to-3D-CT synthesis.

Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a
transformative opportunity in diagnostics and research. In this paper, we
introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual
descriptions using the diffusion model. Unlike previous methods that rely on
fixed-format text input, Text2CT employs a novel prompt formulation that
enables generation from diverse, free-text descriptions. The proposed framework
encodes medical text into latent representations and decodes them into
high-resolution 3D CT scans, effectively bridging the gap between semantic text
inputs and detailed volumetric representations in a unified 3D framework. Our
method demonstrates superior performance in preserving anatomical fidelity and
capturing intricate structures as described in the input text. Extensive
evaluations show that our approach achieves state-of-the-art results, offering
promising potential applications in diagnostics, and data augmentation.

</details>


### [107] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/pdf/2505.04526)
*Qi Zhou, Yukai Shi, Xiaojun Yang, Xiaoyu Xian, Lunjia Liao, Ruimao Zhang, Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes a Darkness-Free network (DFVO) for visible and infrared image fusion, addressing blurry and dim results in low-light conditions. It uses a cascaded multi-task approach with a latent-common feature extractor, achieving superior performance on the LLVIP dataset.


<details>
  <summary>Details</summary>
Motivation: Existing image fusion methods struggle with severe illumination degradation, leading to poor visual effects for autonomous driving. The goal is to create clearer, more informative fused images.

Method: DFVO employs a cascaded multi-task strategy with a latent-common feature extractor (LCFE), details-extraction module (DEM), and hyper cross-attention module (HCAM) to preserve texture and structural information.

Result: DFVO outperforms state-of-the-art methods, achieving 63.258 dB PSNR and 0.724 CC on the LLVIP dataset, with clearer and more evenly illuminated fusion results.

Conclusion: The proposed DFVO network effectively addresses illumination degradation in image fusion, providing superior results for high-level vision tasks.

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [108] [RAFT: Robust Augmentation of FeaTures for Image Segmentation](https://arxiv.org/pdf/2505.04529)
*Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin*

Main category: cs.CV

TL;DR: RAFT improves synthetic-to-real image segmentation with minimal labeled data, outperforming HALO on benchmarks like SYNTHIA->Cityscapes and GTAV->Cityscapes.


<details>
  <summary>Details</summary>
Motivation: Address the Syn2Real problem in image segmentation by reducing reliance on manual labeling and improving model adaptation.

Method: Proposes RAFT, a framework using data/feature augmentations and active learning for domain adaptation.

Result: Achieves mIoU improvements: 2.1%/79.9% (SYNTHIA->Cityscapes), 0.4%/78.2% (GTAV->Cityscapes), and 1.3%/73.2% (Cityscapes->ACDC).

Conclusion: RAFT effectively bridges the synthetic-to-real gap in image segmentation with minimal labeled data, outperforming prior methods.

Abstract: Image segmentation is a powerful computer vision technique for scene
understanding. However, real-world deployment is stymied by the need for
high-quality, meticulously labeled datasets. Synthetic data provides
high-quality labels while reducing the need for manual data collection and
annotation. However, deep neural networks trained on synthetic data often face
the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a
novel framework for adapting image segmentation models using minimal labeled
real-world data through data and feature augmentations, as well as active
learning. To validate RAFT, we perform experiments on the synthetic-to-real
"SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass
the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an
improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes
experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach
on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO,
with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the
effect of the allocated annotation budget and various components of RAFT upon
the final transfer mIoU.

</details>


### [109] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/pdf/2505.03414)
*Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel Features Matrix (FM) regularization method is proposed to enhance large vision-language models for target-unspecific tasks by preserving general knowledge and preventing overfitting.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods for large vision-language models struggle with target-unspecific tasks due to overfitting, which causes loss of general knowledge.

Method: The proposed method introduces a Features Matrix (FM) that captures diverse input semantics from a deep and fine perspective to preserve general knowledge.

Result: The FM is compatible with existing frameworks and significantly improves performance on target-unspecific tasks, achieving state-of-the-art results.

Conclusion: The FM regularization approach effectively mitigates overfitting and enhances generalizability in vision-language models.

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [110] [Registration of 3D Point Sets Using Exponential-based Similarity Matrix](https://arxiv.org/pdf/2505.04540)
*Ashutosh Singandhupe, Sanket Lokhande, Hung Manh La*

Main category: cs.CV

TL;DR: The paper introduces ESM-ICP, a robust variant of the ICP algorithm for point cloud registration, addressing challenges like large rotations and noise.


<details>
  <summary>Details</summary>
Motivation: Existing registration techniques struggle with large rotational differences and noise, leading to inaccurate 3D reconstructions.

Method: ESM-ICP uses a Gaussian-inspired exponential weighting scheme to dynamically adapt a similarity matrix for better alignment.

Result: ESM-ICP outperforms traditional and learning-based methods in handling large rotations and non-Gaussian noise.

Conclusion: ESM-ICP is a robust solution for point cloud registration, with public implementation available for reproducibility.

Abstract: Point cloud registration is a fundamental problem in computer vision and
robotics, involving the alignment of 3D point sets captured from varying
viewpoints using depth sensors such as LiDAR or structured light. In modern
robotic systems, especially those focused on mapping, it is essential to merge
multiple views of the same environment accurately. However, state-of-the-art
registration techniques often struggle when large rotational differences exist
between point sets or when the data is significantly corrupted by sensor noise.
These challenges can lead to misalignments and, consequently, to inaccurate or
distorted 3D reconstructions. In this work, we address both these limitations
by proposing a robust modification to the classic Iterative Closest Point (ICP)
algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),
integrates a Gaussian-inspired exponential weighting scheme to construct a
similarity matrix that dynamically adapts across iterations. This matrix
facilitates improved estimation of both rotational and translational components
during alignment. We demonstrate the robustness of ESM-ICP in two challenging
scenarios: (i) large rotational discrepancies between the source and target
point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show
that ESM-ICP outperforms traditional geometric registration techniques as well
as several recent learning-based methods. To encourage reproducibility and
community engagement, our full implementation is made publicly available on
GitHub. https://github.com/aralab-unr/ESM_ICP

</details>


### [111] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/pdf/2505.04575)
*Kunlun Xu, Xu Zou, Gang Hua, Jiahuan Zhou*

Main category: cs.CV

TL;DR: KA-Prompt addresses misalignment in domain-specific prompts for Domain Incremental Learning (DIL) by introducing componential alignment, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Random positioning of knowledge components in prompts causes conflicting integration and degraded predictions, necessitating alignment.

Method: KA-Prompt uses componential alignment in two phases: initial structure configuration and online alignment preservation.

Result: Extensive experiments show KA-Prompt improves learning and inference capacity in DIL benchmarks.

Conclusion: KA-Prompt effectively aligns domain-specific prompts, enhancing knowledge transfer and model performance.

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [112] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/pdf/2505.04586)
*Yuning Du, Jingshuai Liu, Rohan Dharmakumar, Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: A reinforcement learning framework improves MRI sampling for sequential diagnostic decisions, enabling affordable Point-of-Care MRI.


<details>
  <summary>Details</summary>
Motivation: To make MRI affordable and usable as a Point-of-Care device by reducing sampling complexity and cost.

Method: Multi-objective reinforcement learning adaptively optimizes k-space sampling for sequential diagnostic tasks.

Result: Competitive diagnostic performance with fewer samples in ACL sprain and cartilage thickness loss assessments.

Conclusion: The framework advances MRI towards comprehensive, affordable PoC use.

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


### [113] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/pdf/2505.04594)
*Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu*

Main category: cs.CV

TL;DR: MonoCoP improves monocular 3D object detection by sequentially predicting 3D attributes using a Chain-of-Prediction approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the inter-correlation of 3D attributes, limiting accuracy. MonoCoP addresses this by conditioning predictions on prior attributes.

Method: Uses AttributeNet for feature learning, constructs a Chain-of-Prediction for sequential feature propagation, and employs residual connections to retain earlier features.

Result: Achieves SoTA on KITTI and outperforms methods on Waymo and nuScenes datasets.

Conclusion: MonoCoP demonstrates the effectiveness of sequential and conditional attribute prediction for improving monocular 3D detection.

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [114] [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/pdf/2505.04601)
*Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, Cihang Xie*

Main category: cs.CV

TL;DR: OpenVision is a fully-open, cost-effective family of vision encoders that outperform or match OpenAI's CLIP, offering flexible trade-offs between capacity and efficiency for multimodal models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fully open alternatives to CLIP, OpenVision provides transparent training data and recipes, filling a gap in the field.

Method: OpenVision builds on existing frameworks like CLIPS and Recap-DataComp-1B, incorporating key insights to enhance encoder quality.

Result: OpenVision models (5.9M to 632.1M parameters) match or surpass CLIP's performance in multimodal frameworks like LLaVA, offering scalability.

Conclusion: OpenVision advances multimodal models by providing open, scalable, and efficient vision encoders, benefiting both research and practical deployments.

Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of
vision encoder for building multimodal foundation models. Although recent
alternatives such as SigLIP have begun to challenge this status quo, to our
knowledge none are fully open: their training data remains proprietary and/or
their training recipes are not released. This paper fills this gap with
OpenVision, a fully-open, cost-effective family of vision encoders that match
or surpass the performance of OpenAI's CLIP when integrated into multimodal
frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for
training framework and Recap-DataComp-1B for training data -- while revealing
multiple key insights in enhancing encoder quality and showcasing practical
benefits in advancing multimodal models. By releasing vision encoders spanning
from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible
trade-off between capacity and efficiency in building multimodal models: larger
models deliver enhanced multimodal performance, while smaller versions enable
lightweight, edge-ready multimodal deployments.

</details>


### [115] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/pdf/2505.04612)
*Jiahao Li, Haochen Wang, Muhammad Zubair Irshad, Igor Vasiljevic, Matthew R. Walter, Vitor Campagnolo Guizilini, Greg Shakhnarovich*

Main category: cs.CV

TL;DR: FastMap is a fast, GPU-friendly SfM method that outperforms COLMAP and GLOMAP in speed while maintaining pose accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like COLMAP and GLOMAP are slow due to poor parallelization and expensive optimization steps.

Method: FastMap uses GPU-friendly operations for easy parallelization and linear-time optimization steps.

Result: FastMap is 10-100x faster than COLMAP and GLOMAP with comparable accuracy.

Conclusion: FastMap offers a scalable and efficient solution for large-scale SfM tasks.

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is one to two orders of magnitude faster than
COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.

</details>


### [116] [Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait](https://arxiv.org/pdf/2505.04616)
*Feng Liu, Nicholas Chimitt, Lanqing Guo, Jitesh Jain, Aditya Kane, Minchul Kim, Wes Robbins, Yiyang Su, Dingqiang Ye, Xingguang Zhang, Jie Zhu, Siddharth Satyakam, Christopher Perry, Stanley H. Chan, Arun Ross, Humphrey Shi, Zhangyang Wang, Anil Jain, Xiaoming Liu*

Main category: cs.CV

TL;DR: FarSight is an end-to-end system for whole-body person recognition in challenging environments, integrating face, gait, and body shape cues. It outperforms prior methods with significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: The need for reliable biometric recognition in unconstrained, long-range surveillance scenarios with degraded conditions.

Method: FarSight combines multi-subject detection, video restoration, biometric feature encoding, and quality-guided fusion across modalities.

Result: Achieves 34.1% higher verification accuracy, 17.8% better identification, and 34.3% fewer errors in open-set tasks.

Conclusion: FarSight is a state-of-the-art solution for operational biometric recognition in real-world challenges.

Abstract: We address the problem of whole-body person recognition in unconstrained
environments. This problem arises in surveillance scenarios such as those in
the IARPA Biometric Recognition and Identification at Altitude and Range
(BRIAR) program, where biometric data is captured at long standoff distances,
elevated viewing angles, and under adverse atmospheric conditions (e.g.,
turbulence and high wind velocity). To this end, we propose FarSight, a unified
end-to-end system for person recognition that integrates complementary
biometric cues across face, gait, and body shape modalities. FarSight
incorporates novel algorithms across four core modules: multi-subject detection
and tracking, recognition-aware video restoration, modality-specific biometric
feature encoding, and quality-guided multi-modal fusion. These components are
designed to work cohesively under degraded image conditions, large pose and
scale variations, and cross-domain gaps. Extensive experiments on the BRIAR
dataset, one of the most comprehensive benchmarks for long-range, multi-modal
biometric recognition, demonstrate the effectiveness of FarSight. Compared to
our preliminary system, this system achieves a 34.1% absolute gain in 1:1
verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set
identification (Rank-20), and a 34.3% reduction in open-set identification
errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE
Face in Video Evaluation (FIVE), which conducts standardized face recognition
testing on the BRIAR dataset. These results establish FarSight as a
state-of-the-art solution for operational biometric recognition in challenging
real-world conditions.

</details>


### [117] [On Path to Multimodal Generalist: General-Level and General-Bench](https://arxiv.org/pdf/2505.04620)
*Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang*

Main category: cs.CV

TL;DR: The paper introduces General-Level, an evaluation framework for Multimodal Large Language Models (MLLMs), assessing their performance and generality across 5-scale levels. It challenges the assumption that higher task performance equals stronger MLLM capability, proposing Synergy as a core metric. The framework includes General-Bench, a comprehensive benchmark with 700 tasks and 325,800 instances, evaluating over 100 MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a standardized evaluation framework for MLLMs, questioning whether higher task performance truly reflects stronger capability or progress toward human-level AI.

Method: Introduces General-Level, a 5-scale evaluation framework, and General-Bench, a benchmark with diverse tasks and modalities. Synergy is used to measure consistency across comprehension, generation, and modalities.

Result: Evaluation of 100+ MLLMs reveals capability rankings and challenges in achieving genuine AI. The framework highlights gaps in current models.

Conclusion: The project aims to guide future research on multimodal foundation models, providing infrastructure to advance toward AGI.

Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid
growth, driven by the advanced capabilities of LLMs. Unlike earlier
specialists, existing MLLMs are evolving towards a Multimodal Generalist
paradigm. Initially limited to understanding multiple modalities, these models
have advanced to not only comprehend but also generate across modalities. Their
capabilities have expanded from coarse-grained to fine-grained multimodal
understanding and from supporting limited modalities to arbitrary ones. While
many benchmarks exist to assess MLLMs, a critical question arises: Can we
simply assume that higher performance across tasks indicates a stronger MLLM
capability, bringing us closer to human-level AI? We argue that the answer is
not as straightforward as it seems. This project introduces General-Level, an
evaluation framework that defines 5-scale levels of MLLM performance and
generality, offering a methodology to compare MLLMs and gauge the progress of
existing systems towards more robust multimodal generalists and, ultimately,
towards AGI. At the core of the framework is the concept of Synergy, which
measures whether models maintain consistent capabilities across comprehension
and generation, and across multiple modalities. To support this evaluation, we
present General-Bench, which encompasses a broader spectrum of skills,
modalities, formats, and capabilities, including over 700 tasks and 325,800
instances. The evaluation results that involve over 100 existing
state-of-the-art MLLMs uncover the capability rankings of generalists,
highlighting the challenges in reaching genuine AI. We expect this project to
pave the way for future research on next-generation multimodal foundation
models, providing a robust infrastructure to accelerate the realization of AGI.
Project page: https://generalist.top/

</details>


### [118] [Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence](https://arxiv.org/pdf/2202.03482)
*Frederik Pahde, Maximilian Dreyer, Leander Weber, Moritz Weckbecker, Christopher J. Anders, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin*

Main category: cs.CV

TL;DR: The paper critiques the separability-oriented computation of Concept Activation Vectors (CAVs) for diverging from true concept modeling due to distractor signals. It proposes pattern-based CAVs for more accurate concept directions and validates their superiority in alignment and applications like sensitivity testing and model correction.


<details>
  <summary>Details</summary>
Motivation: To address the discrepancy between separability-oriented CAVs and true concept modeling caused by distractor signals.

Method: Introduces pattern-based CAVs that focus solely on concept signals, avoiding distractor influences. Evaluates alignment with true concept directions and impact on applications.

Result: Demonstrates that pattern-based CAVs provide more accurate concept directions, improving performance in sensitivity testing and correcting shortcut behaviors in models.

Conclusion: Pattern-based CAVs outperform traditional methods by better aligning with true concept directions, enhancing their utility in applications.

Abstract: With a growing interest in understanding neural network prediction
strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool
for modeling human-understandable concepts in the latent space. Commonly, CAVs
are computed by leveraging linear classifiers optimizing the separability of
latent representations of samples with and without a given concept. However, in
this paper we show that such a separability-oriented computation leads to
solutions, which may diverge from the actual goal of precisely modeling the
concept direction. This discrepancy can be attributed to the significant
influence of distractor directions, i.e., signals unrelated to the concept,
which are picked up by filters (i.e., weights) of linear models to optimize
class-separability. To address this, we introduce pattern-based CAVs, solely
focussing on concept signals, thereby providing more accurate concept
directions. We evaluate various CAV methods in terms of their alignment with
the true concept direction and their impact on CAV applications, including
concept sensitivity testing and model correction for shortcut behavior caused
by data artifacts. We demonstrate the benefits of pattern-based CAVs using the
Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet,
EfficientNet, and Vision Transformer as model architectures.

</details>


### [119] [XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models](https://arxiv.org/pdf/2306.07971)
*Omkar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: XrayGPT is a conversational medical vision-language model for analyzing chest radiographs, combining a medical visual encoder with a fine-tuned LLM to enhance performance in the medical domain.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models lack sophistication in biomedical image understanding, while conversational medical models focus on text. XrayGPT bridges this gap by enabling visual conversation grounded in medical knowledge.

Method: Aligns a medical visual encoder (MedClip) with a fine-tuned LLM (Vicuna) using linear transformation and generates ~217k summaries from radiology reports for fine-tuning.

Result: XrayGPT achieves exceptional visual conversation abilities for chest radiographs, leveraging medical domain knowledge.

Conclusion: The model advances automated chest radiograph analysis and is open-sourced for further research.

Abstract: The latest breakthroughs in large vision-language models, such as Bard and
GPT-4, have showcased extraordinary abilities in performing a wide range of
tasks. Such models are trained on massive datasets comprising billions of
public image-text pairs with diverse tasks. However, their performance on
task-specific domains, such as radiology, is still under-investigated and
potentially limited due to a lack of sophistication in understanding biomedical
images. On the other hand, conversational medical models have exhibited
remarkable success but have mainly focused on text-based analysis. In this
paper, we introduce XrayGPT, a novel conversational medical vision-language
model that can analyze and answer open-ended questions about chest radiographs.
Specifically, we align both medical visual encoder (MedClip) with a fine-tuned
large language model (Vicuna), using a simple linear transformation. This
alignment enables our model to possess exceptional visual conversation
abilities, grounded in a deep understanding of radiographs and medical domain
knowledge. To enhance the performance of LLMs in the medical context, we
generate ~217k interactive and high-quality summaries from free-text radiology
reports. These summaries serve to enhance the performance of LLMs through the
fine-tuning process. Our approach opens up new avenues the research for
advancing the automated analysis of chest radiographs. Our open-source demos,
models, and instruction sets are available at:
https://github.com/mbzuai-oryx/XrayGPT.

</details>


### [120] [Illumination and Shadows in Head Rotation: experiments with Denoising Diffusion Models](https://arxiv.org/pdf/2308.06057)
*Andrea Asperti, Gabriele Colasuonno, Antonio Guerra*

Main category: cs.CV

TL;DR: The paper explores using denoising diffusion models to simulate continuous head rotation under varying lighting, leveraging latent space trajectories and additional labels from CelebA for improved realism.


<details>
  <summary>Details</summary>
Motivation: Enhancing image realism and reducing artifacts in computer vision by accurately modeling illumination and shadows during head rotation.

Method: Uses latent space trajectories in denoising diffusion models (DDIM), labels images by illumination direction, and computes trajectories via linear regression without additional training.

Result: Achieves head rotations up to Â±30 degrees while preserving distinct characteristics under challenging lighting.

Conclusion: Demonstrates the potential of latent space manipulation for realistic head rotation, contributing to representation learning and generative model semantics.

Abstract: Accurately modeling the effects of illumination and shadows during head
rotation is critical in computer vision for enhancing image realism and
reducing artifacts. This study delves into the latent space of denoising
diffusion models to identify compelling trajectories that can express
continuous head rotation under varying lighting conditions. A key contribution
of our work is the generation of additional labels from the CelebA
dataset,categorizing images into three groups based on prevalent illumination
direction: left, center, and right. These labels play a crucial role in our
approach, enabling more precise manipulations and improved handling of lighting
variations. Leveraging a recent embedding technique for Denoising Diffusion
Implicit Models (DDIM), our method achieves noteworthy manipulations,
encompassing a wide rotation angle of $\pm 30$ degrees, while preserving
individual distinct characteristics even under challenging illumination
conditions. Our methodology involves computing trajectories that approximate
clouds of latent representations of dataset samples with different yaw
rotations through linear regression. Specific trajectories are obtained by
analyzing subsets of data that share significant attributes with the source
image, including light direction. Notably, our approach does not require any
specific training of the generative model for the task of rotation; we merely
compute and follow specific trajectories in the latent space of a pre-trained
face generation model. This article showcases the potential of our approach and
its current limitations through a qualitative discussion of notable examples.
This study contributes to the ongoing advancements in representation learning
and the semantic investigation of the latent space of generative models.

</details>


### [121] [Uncertainty for SVBRDF Acquisition using Frequency Analysis](https://arxiv.org/pdf/2406.17774)
*Ruben Wiersma, Julien Philip, MiloÅ¡ HaÅ¡an, Krishna Mullia, Fujun Luan, Elmar Eisemann, Valentin Deschaintre*

Main category: cs.CV

TL;DR: The paper introduces a method to quantify uncertainty in SVBRDF acquisition using entropy and frequency domain analysis, enabling fast uncertainty mapping and improved reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address ambiguity in SVBRDF acquisition under uncontrolled conditions by quantifying uncertainty.

Method: Uses entropy and frequency domain analysis to accelerate uncertainty mapping, validated with a path tracer.

Result: Achieves competitive SVBRDF reconstruction, correlates error with uncertainty, and applies uncertainty maps for improved acquisition.

Conclusion: The method efficiently quantifies uncertainty and enhances SVBRDF acquisition through capture guidance and diffusion-based inpainting.

Abstract: This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view
captures. Under uncontrolled illumination and unstructured viewpoints, there is
no guarantee that the observations contain enough information to reconstruct
the appearance properties of a captured object. We study this ambiguity, or
uncertainty, using entropy and accelerate the analysis by using the frequency
domain, rather than the domain of incoming and outgoing viewing angles. The
result is a method that computes a map of uncertainty over an entire object
within a millisecond. We find that the frequency model allows us to recover
SVBRDF parameters with competitive performance, that the accelerated entropy
computation matches results with a physically-based path tracer, and that there
is a positive correlation between error and uncertainty. We then show that the
uncertainty map can be applied to improve SVBRDF acquisition using capture
guidance, sharing information on the surface, and using a diffusion model to
inpaint uncertain regions. Our code is available at
https://github.com/rubenwiersma/svbrdf_uncertainty.

</details>


### [122] [XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis](https://arxiv.org/pdf/2406.18360)
*Hao Li, Chenming Wu, Ming Yuan, Yan Zhang, Chen Zhao, Chunyu Song, Haocheng Feng, Errui Ding, Dingwen Zhang, Jingdong Wang*

Main category: cs.CV

TL;DR: A synthetic dataset for novel driving view synthesis is introduced to address the limitations of real-world data in autonomous driving simulations, revealing gaps in current NVS methods.


<details>
  <summary>Details</summary>
Motivation: Existing NVS methods fail to meet the demands of closed-loop simulations, especially for rendering views beyond the original trajectory (e.g., cross-lane views).

Method: A synthetic dataset with deviated trajectories (1-4 meters) is created, covering diverse conditions, to benchmark NVS methods in autonomous driving contexts.

Result: The dataset highlights the inadequacy of current NVS approaches in handling cross-lane or closed-loop simulation requirements.

Conclusion: The study underscores the need for improved NVS methods tailored to autonomous driving simulations, using the provided dataset as a benchmark.

Abstract: Comprehensive testing of autonomous systems through simulation is essential
to ensure the safety of autonomous driving vehicles. This requires the
generation of safety-critical scenarios that extend beyond the limitations of
real-world data collection, as many of these scenarios are rare or rarely
encountered on public roads. However, evaluating most existing novel view
synthesis (NVS) methods relies on sporadic sampling of image frames from the
training data, comparing the rendered images with ground-truth images.
Unfortunately, this evaluation protocol falls short of meeting the actual
requirements in closed-loop simulations. Specifically, the true application
demands the capability to render novel views that extend beyond the original
trajectory (such as cross-lane views), which are challenging to capture in the
real world. To address this, this paper presents a synthetic dataset for novel
driving view synthesis evaluation, which is specifically designed for
autonomous driving simulations. This unique dataset includes testing images
captured by deviating from the training trajectory by $1-4$ meters. It
comprises six sequences that cover various times and weather conditions. Each
sequence contains $450$ training images, $120$ testing images, and their
corresponding camera poses and intrinsic parameters. Leveraging this novel
dataset, we establish the first realistic benchmark for evaluating existing NVS
approaches under front-only and multicamera settings. The experimental findings
underscore the significant gap in current approaches, revealing their
inadequate ability to fulfill the demanding prerequisites of cross-lane or
closed-loop simulation.

</details>


### [123] [Image-GS: Content-Adaptive Image Representation via 2D Gaussians](https://arxiv.org/pdf/2407.01866)
*Yunxiang Zhang, Bingxuan Li, Alexandr Kuznetsov, Akshay Jindal, Stavros Diolatzis, Kenneth Chen, Anton Sochenov, Anton Kaplanyan, Qi Sun*

Main category: cs.CV

TL;DR: Image-GS introduces a content-adaptive image representation using 2D Gaussians, balancing visual fidelity and memory efficiency for real-time graphics.


<details>
  <summary>Details</summary>
Motivation: Existing methods use fixed data structures or compute-heavy implicit models, limiting practicality for real-time applications.

Method: Image-GS employs a custom differentiable renderer to adaptively allocate and optimize anisotropic, colored 2D Gaussians.

Result: It achieves efficient memory usage and visual quality, supports rapid random access, and works well for stylized images.

Conclusion: Image-GS is versatile, useful for texture compression, semantics-aware compression, and joint image tasks.

Abstract: Neural image representations have emerged as a promising approach for
encoding and rendering visual data. Combined with learning-based workflows,
they demonstrate impressive trade-offs between visual fidelity and memory
footprint. Existing methods in this domain, however, often rely on fixed data
structures that suboptimally allocate memory or compute-intensive implicit
models, hindering their practicality for real-time graphics applications.
  Inspired by recent advancements in radiance field rendering, we introduce
Image-GS, a content-adaptive image representation based on 2D Gaussians.
Leveraging a custom differentiable renderer, Image-GS reconstructs images by
adaptively allocating and progressively optimizing a group of anisotropic,
colored 2D Gaussians. It achieves a favorable balance between visual fidelity
and memory efficiency across a variety of stylized images frequently seen in
graphics workflows, especially for those showing non-uniformly distributed
features and in low-bitrate regimes. Moreover, it supports hardware-friendly
rapid random access for real-time usage, requiring only 0.3K MACs to decode a
pixel. Through error-guided progressive optimization, Image-GS naturally
constructs a smooth level-of-detail hierarchy. We demonstrate its versatility
with several applications, including texture compression, semantics-aware
compression, and joint image compression and restoration.

</details>


### [124] [Training-Free Sketch-Guided Diffusion with Latent Optimization](https://arxiv.org/pdf/2409.00313)
*Sandra Zhang Ding, Jiafeng Mao, Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: A training-free pipeline enhances text-to-image models by using sketches for precise control, leveraging cross-attention maps and latent optimization.


<details>
  <summary>Details</summary>
Motivation: To provide users with precise control over image generation in real-world content creation, addressing limitations of current text-to-image models.

Method: Uses cross-attention maps to track sketch features and introduces latent optimization to refine noisy latents during generation.

Result: Improved accuracy in generating images that closely follow the structure of input sketches.

Conclusion: The method offers greater user control and customization in content creation without additional training.

Abstract: Based on recent advanced diffusion models, Text-to-image (T2I) generation
models have demonstrated their capabilities to generate diverse and
high-quality images. However, leveraging their potential for real-world content
creation, particularly in providing users with precise control over the image
generation result, poses a significant challenge. In this paper, we propose an
innovative training-free pipeline that extends existing text-to-image
generation models to incorporate a sketch as an additional condition. To
generate new images with a layout and structure closely resembling the input
sketch, we find that these core features of a sketch can be tracked with the
cross-attention maps of diffusion models. We introduce latent optimization, a
method that refines the noisy latent at each intermediate step of the
generation process using cross-attention maps to ensure that the generated
images adhere closely to the desired structure outlined in the reference
sketch. Through latent optimization, our method enhances the accuracy of image
generation, offering users greater control and customization options in content
creation.

</details>


### [125] [Enhancing Test Time Adaptation with Few-shot Guidance](https://arxiv.org/pdf/2409.01341)
*Siqi Luo, Yi Xin, Yuntao Du, Zhongwei Wan, Tao Tan, Guangtao Zhai, Xiaohong Liu*

Main category: cs.CV

TL;DR: FS-TTA introduces a few-shot support set to improve Test Time Adaptation (TTA), reducing blind exploration in unseen target domains with a two-stage framework for better performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the unreliability of existing TTA methods in handling domain shifts between training and test data.

Method: A two-stage framework: (i) fine-tuning with few-shot support set and feature diversity augmentation, (ii) test time adaptation using prototype memory bank guidance.

Result: Superior performance and reliability demonstrated on three cross-domain classification benchmarks.

Conclusion: FS-TTA is a practical and effective solution for domain shift correction in real-world applications.

Abstract: Deep neural networks often encounter significant performance drops while
facing with domain shifts between training (source) and test (target) data. To
address this issue, Test Time Adaptation (TTA) methods have been proposed to
adapt pre-trained source model to handle out-of-distribution streaming target
data. Although these methods offer some relief, they lack a reliable mechanism
for domain shift correction, which can often be erratic in real-world
applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a
novel and practical setting that utilizes a few-shot support set on top of TTA.
Adhering to the principle of few inputs, big gains, FS-TTA reduces blind
exploration in unseen target domains. Furthermore, we propose a two-stage
framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source
model with few-shot support set, along with using feature diversity
augmentation module to avoid overfitting, (ii) implementing test time
adaptation based on prototype memory bank guidance to produce high quality
pseudo-label for model adaptation. Through extensive experiments on three
cross-domain classification benchmarks, we demonstrate the superior performance
and reliability of our FS-TTA and framework.

</details>


### [126] [Replace Anyone in Videos](https://arxiv.org/pdf/2409.19911)
*Xiang Wang, Shiwei Zhang, Haonan Qiu, Ruihang Chu, Zekun Li, Yingya Zhang, Changxin Gao, Yuehuan Wang, Chunhua Shen, Nong Sang*

Main category: cs.CV

TL;DR: ReplaceAnyone is a framework for localized human replacement and insertion in videos, using a diffusion-based approach with pose guidance and mask diversity for precise control.


<details>
  <summary>Details</summary>
Motivation: Precise control over human motion in videos, such as replacing or inserting individuals while preserving motion patterns, remains challenging despite advancements in diffusion models.

Method: The method uses an image-conditioned video inpainting paradigm with pose guidance, diverse masks, a hybrid inpainting encoder, and a two-phase optimization for training.

Result: ReplaceAnyone achieves realistic and coherent video content with seamless character replacement or insertion, maintaining desired motion and appearance.

Conclusion: The framework is effective and adaptable, working with both 3D-UNet and DiT-based models, and will be publicly available.

Abstract: The field of controllable human-centric video generation has witnessed
remarkable progress, particularly with the advent of diffusion models. However,
achieving precise and localized control over human motion in videos, such as
replacing or inserting individuals while preserving desired motion patterns,
still remains a formidable challenge. In this work, we present the
ReplaceAnyone framework, which focuses on localized human replacement and
insertion featuring intricate backgrounds. Specifically, we formulate this task
as an image-conditioned video inpainting paradigm with pose guidance, utilizing
a unified end-to-end video diffusion architecture that facilitates
image-conditioned video inpainting within masked regions. To prevent shape
leakage and enable granular local control, we introduce diverse mask forms
involving both regular and irregular shapes. Furthermore, we implement an
enriched visual guidance mechanism to enhance appearance alignment, a hybrid
inpainting encoder to further preserve the detailed background information in
the masked video, and a two-phase optimization methodology to simplify the
training difficulty. ReplaceAnyone enables seamless replacement or insertion of
characters while maintaining the desired pose motion and reference appearance
within a single framework. Extensive experimental results demonstrate the
effectiveness of our method in generating realistic and coherent video content.
The proposed ReplaceAnyone can be seamlessly applied not only to traditional
3D-UNet base models but also to DiT-based video models such as Wan2.1. The code
will be available at https://github.com/ali-vilab/UniAnimate-DiT.

</details>


### [127] [Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models](https://arxiv.org/pdf/2410.04634)
*Salma S. Abdel Magid, Weiwei Pan, Simon Warchol, Grace Guo, Junsik Kim, Mahia Rahman, Hanspeter Pfister*

Main category: cs.CV

TL;DR: Concept2Concept is a framework for auditing T2I models by characterizing their conditional distributions using interpretable concepts and metrics, aiding in human-understandable analysis.


<details>
  <summary>Details</summary>
Motivation: The need to audit T2I models for generating task-appropriate images and understanding prompt-content associations systematically.

Method: Proposes Concept2Concept, a framework using interpretable concepts and metrics to characterize conditional distributions of vision language models.

Result: Demonstrated through case studies of prompt distributions and implemented as an open-source interactive tool for non-technical users.

Conclusion: Concept2Concept provides a practical solution for auditing T2I models and datasets, making the process accessible to a broader audience.

Abstract: Text-to-image (T2I) models are increasingly used in impactful real-life
applications. As such, there is a growing need to audit these models to ensure
that they generate desirable, task-appropriate images. However, systematically
inspecting the associations between prompts and generated content in a
human-understandable way remains challenging. To address this, we propose
Concept2Concept, a framework where we characterize conditional distributions of
vision language models using interpretable concepts and metrics that can be
defined in terms of these concepts. This characterization allows us to use our
framework to audit models and prompt-datasets. To demonstrate, we investigate
several case studies of conditional distributions of prompts, such as
user-defined distributions or empirical, real-world distributions. Lastly, we
implement Concept2Concept as an open-source interactive visualization tool to
facilitate use by non-technical end-users. A demo is available at
https://tinyurl.com/Concept2ConceptDemo.

</details>


### [128] [DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation](https://arxiv.org/pdf/2412.03255)
*Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Yong Liu, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang*

Main category: cs.CV

TL;DR: DynamicControl is a novel framework for text-to-image diffusion models that dynamically manages multiple control signals, improving controllability and image quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods inefficiently handle or fix the number of conditions, limiting their ability to address complex or conflicting conditions in image synthesis.

Method: The framework uses a double-cycle controller for initial condition scoring, integrates a Multimodal Large Language Model (MLLM) for condition evaluation, and employs a parallel multi-control adapter to modulate ControlNet.

Result: DynamicControl outperforms existing methods in controllability, generation quality, and composability under diverse conditions.

Conclusion: The proposed framework effectively addresses the limitations of current methods by dynamically managing multiple conditions, enhancing text-to-image synthesis.

Abstract: To enhance the controllability of text-to-image diffusion models, current
ControlNet-like models have explored various control signals to dictate image
attributes. However, existing methods either handle conditions inefficiently or
use a fixed number of conditions, which does not fully address the complexity
of multiple conditions and their potential conflicts. This underscores the need
for innovative approaches to manage multiple conditions effectively for more
reliable and detailed image synthesis. To address this issue, we propose a
novel framework, DynamicControl, which supports dynamic combinations of diverse
control signals, allowing adaptive selection of different numbers and types of
conditions. Our approach begins with a double-cycle controller that generates
an initial real score sorting for all input conditions by leveraging
pre-trained conditional generation models and discriminative models. This
controller evaluates the similarity between extracted conditions and input
conditions, as well as the pixel-level similarity with the source image. Then,
we integrate a Multimodal Large Language Model (MLLM) to build an efficient
condition evaluator. This evaluator optimizes the ordering of conditions based
on the double-cycle controller's score ranking. Our method jointly optimizes
MLLMs and diffusion models, utilizing MLLMs' reasoning capabilities to
facilitate multi-condition text-to-image (T2I) tasks. The final sorted
conditions are fed into a parallel multi-control adapter, which learns feature
maps from dynamic visual conditions and integrates them to modulate ControlNet,
thereby enhancing control over generated images. Through both quantitative and
qualitative comparisons, DynamicControl demonstrates its superiority over
existing methods in terms of controllability, generation quality and
composability under various conditional controls.

</details>


### [129] [Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion](https://arxiv.org/pdf/2412.03413)
*Andrea Asperti, Ali Aydogdu, Angelo Greco, Fabio Merizzi, Pietro Miraglio, Beniamino Tartufoli, Alessandro Testa, Nadia Pinardi, Paolo Oddo*

Main category: cs.CV

TL;DR: Machine Learning models, particularly U-net, outperform traditional methods in reconstructing cloud-occluded SST data from MODIS Aqua images, reducing errors by 50%.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of filling cloud gaps in SST reconstructions from satellite images, improving accuracy over existing methods.

Method: Employed a U-net Convolutional Neural Network to reconstruct cloud-covered areas in MODIS Aqua nighttime L3 images, preserving observed values in cloud-free regions.

Result: U-net achieved 50% lower root mean square errors compared to traditional OI interpolation algorithms.

Conclusion: U-net is a highly precise and effective solution for SST reconstruction in cloud-affected satellite imagery.

Abstract: Sea Surface Temperature (SST) reconstructions from satellite images affected
by cloud gaps have been extensively documented in the past three decades. Here
we describe several Machine Learning models to fill the cloud-occluded areas
starting from MODIS Aqua nighttime L3 images. To tackle this challenge, we
employed a type of Convolutional Neural Network model (U-net) to reconstruct
cloud-covered portions of satellite imagery while preserving the integrity of
observed values in cloud-free areas. We demonstrate the outstanding precision
of U-net with respect to available products done using OI interpolation
algorithms. Our best-performing architecture show 50% lower root mean square
errors over established gap-filling methods.

</details>


### [130] [Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail](https://arxiv.org/pdf/2412.04472)
*Luca Bartolomei, Fabio Tosi, Matteo Poggi, Stefano Mattoccia*

Main category: cs.CV

TL;DR: Stereo Anywhere combines geometric constraints and monocular depth priors for stereo matching, achieving state-of-the-art zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To address challenges like textureless regions and occlusions by integrating stereo matching with learned contextual cues.

Method: Uses a dual-branch architecture with novel cost volume fusion mechanisms.

Result: Achieves top performance in zero-shot generalization, handling mirrors and transparencies robustly.

Conclusion: The framework outperforms existing solutions, demonstrating strong generalization and robustness.

Abstract: We introduce Stereo Anywhere, a novel stereo-matching framework that combines
geometric constraints with robust priors from monocular depth Vision Foundation
Models (VFMs). By elegantly coupling these complementary worlds through a
dual-branch architecture, we seamlessly integrate stereo matching with learned
contextual cues. Following this design, our framework introduces novel cost
volume fusion mechanisms that effectively handle critical challenges such as
textureless regions, occlusions, and non-Lambertian surfaces. Through our novel
optical illusion dataset, MonoTrap, and extensive evaluation across multiple
benchmarks, we demonstrate that our synthetic-only trained model achieves
state-of-the-art results in zero-shot generalization, significantly
outperforming existing solutions while showing remarkable robustness to
challenging cases such as mirrors and transparencies.

</details>


### [131] [Efficiency Meets Fidelity: A Novel Quantization Framework for Stable Diffusion](https://arxiv.org/pdf/2412.06661)
*Shuaiting Li, Juncan Deng, Zeyu Wang, Kedong Xu, Rongtao Deng, Hong Gu, Haibin Shen, Kejie Huang*

Main category: cs.CV

TL;DR: A new quantization framework for Stable Diffusion models ensures high-quality, consistent image generation while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the computational intensity and output consistency issues in quantized Stable Diffusion models for real-time applications.

Method: Proposes a Serial-to-Parallel pipeline with techniques like multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing.

Result: Superior performance in distribution similarity and visual fidelity under W4A8 quantization, with shorter training times.

Conclusion: The framework achieves high-fidelity generation comparable to floating-point models while maintaining quantization efficiency.

Abstract: Text-to-image generation via Stable Diffusion models (SDM) have demonstrated
remarkable capabilities. However, their computational intensity, particularly
in the iterative denoising process, hinders real-time deployment in
latency-sensitive applications. While Recent studies have explored
post-training quantization (PTQ) and quantization-aware training (QAT) methods
to compress Diffusion models, existing methods often overlook the consistency
between results generated by quantized models and those from floating-point
models. This consistency is paramount for professional applications where both
efficiency and output reliability are essential. To ensure that quantized SDM
generates high-quality and consistent images, we propose an efficient
quantization framework for SDM. Our framework introduces a Serial-to-Parallel
pipeline that simultaneously maintains training-inference consistency and
ensures optimization stability. Building upon this foundation, we further
develop several techniques including multi-timestep activation quantization,
time information precalculation, inter-layer distillation, and selective
freezing, to achieve high-fidelity generation in comparison to floating-point
models while maintaining quantization efficiency.
  Through comprehensive evaluation across multiple Stable Diffusion variants
(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over
state-of-the-art approaches with shorter training times. Under W4A8
quantization settings, we achieve significant improvements in both distribution
similarity and visual fidelity, while preserving a high image quality.

</details>


### [132] [SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation](https://arxiv.org/pdf/2412.11026)
*Hang Zhang, Zhuoling Li, Jun Liu*

Main category: cs.CV

TL;DR: SceneLLM leverages LLMs for dynamic Scene Graph Generation (SGG) by converting video frames into linguistic signals, enhancing spatial encoding, and fine-tuning with LoRA, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Dynamic scenes' spatio-temporal complexity makes SGG challenging; LLMs' reasoning capabilities inspire a novel approach to improve accuracy.

Method: Proposes SceneLLM with V2L mapping, SIA for spatial encoding, OT for implicit language signals, LoRA fine-tuning, and a transformer-based SGG predictor.

Result: Achieves state-of-the-art performance on the Action Genome benchmark, demonstrating effective dynamic scene understanding.

Conclusion: SceneLLM effectively integrates LLMs for dynamic SGG, offering a robust solution for complex spatio-temporal scene parsing.

Abstract: Dynamic scenes contain intricate spatio-temporal information, crucial for
mobile robots, UAVs, and autonomous driving systems to make informed decisions.
Parsing these scenes into semantic triplets <Subject-Predicate-Object> for
accurate Scene Graph Generation (SGG) is highly challenging due to the
fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities
of Large Language Models (LLMs), we propose SceneLLM, a novel framework that
leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework
introduces a Video-to-Language (V2L) mapping module that transforms video
frames into linguistic signals (scene tokens), making the input more
comprehensible for LLMs. To better encode spatial information, we devise a
Spatial Information Aggregation (SIA) scheme, inspired by the structure of
Chinese characters, which encodes spatial data into tokens. Using Optimal
Transport (OT), we generate an implicit language signal from the frame-level
token sequence that captures the video's spatio-temporal information. To
further improve the LLM's ability to process this implicit linguistic input, we
apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a
transformer-based SGG predictor to decode the LLM's reasoning and predict
semantic triplets. Our method achieves state-of-the-art results on the Action
Genome (AG) benchmark, and extensive experiments show the effectiveness of
SceneLLM in understanding and generating accurate dynamic scene graphs.

</details>


### [133] [Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling](https://arxiv.org/pdf/2501.04666)
*Nannan Li, Kevin J. Shih, Bryan A. Plummer*

Main category: cs.CV

TL;DR: The paper addresses challenges in virtual try-on by using synthetic data and a refinement model (EARSB) to improve garment matching and image quality.


<details>
  <summary>Details</summary>
Motivation: Limited paired (human, garment) training data and difficulties in generating accurate garment textures motivate the need for synthetic data and model refinement.

Method: Introduces a garment extraction model for synthetic data generation and EARSB for localized error correction in virtual try-on outputs.

Result: Synthetic data improves prior work, and EARSB enhances image quality, with user preference at 59%.

Conclusion: Combining synthetic data and EARSB effectively addresses virtual try-on challenges, improving performance and user satisfaction.

Abstract: Given an isolated garment image in a canonical product view and a separate
image of a person, the virtual try-on task aims to generate a new image of the
person wearing the target garment. Prior virtual try-on works face two major
challenges in achieving this goal: a) the paired (human, garment) training data
has limited availability; b) generating textures on the human that perfectly
match that of the prompted garment is difficult, often resulting in distorted
text and faded textures. Our work explores ways to tackle these issues through
both synthetic data as well as model refinement. We introduce a garment
extraction model that generates (human, synthetic garment) pairs from a single
image of a clothed individual. The synthetic pairs can then be used to augment
the training of virtual try-on. We also propose an Error-Aware Refinement-based
Schr\"odinger Bridge (EARSB) that surgically targets localized generation
errors for correcting the output of a base virtual try-on model. To identify
likely errors, we propose a weakly-supervised error classifier that localizes
regions for refinement, subsequently augmenting the Schr\"odinger Bridge's
noise schedule with its confidence heatmap. Experiments on VITON-HD and
DressCode-Upper demonstrate that our synthetic data augmentation enhances the
performance of prior work, while EARSB improves the overall image quality. In
user studies, our model is preferred by the users in an average of 59% of
cases.

</details>


### [134] [CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction](https://arxiv.org/pdf/2501.18504)
*Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa*

Main category: cs.CV

TL;DR: CLEAR combines LLMs and evolutionary computation to auto-generate and optimize cues for improved image recognition, outperforming human experts in specialized tasks like sustainability data extraction.


<details>
  <summary>Details</summary>
Motivation: Specialized image recognition tasks require domain expertise for prompt cues, limiting accuracy and scalability. CLEAR aims to automate and optimize this process.

Method: Uses LLMs and evolutionary computation to generate domain-specific representations and optimize textual cues via a genetic algorithm. Tests variable-length vs. fixed-length representations and refactors categorical to real-valued estimates.

Result: CLEAR achieves higher accuracy than human experts, reducing error rates by up to two orders of magnitude.

Conclusion: CLEAR effectively automates cue optimization for specialized image recognition, significantly improving accuracy and consistency.

Abstract: Large Language Model (LLM) image recognition is a powerful tool for
extracting data from images, but accuracy depends on providing sufficient cues
in the prompt - requiring a domain expert for specialized tasks. We introduce
Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a
combination of LLMs and evolutionary computation to generate and optimize cues
such that recognition of specialized features in images is improved. It
achieves this by auto-generating a novel domain-specific representation and
then using it to optimize suitable textual cues with a genetic algorithm. We
apply CLEAR to the real-world task of identifying sustainability data from
interior and exterior images of buildings. We investigate the effects of using
a variable-length representation compared to fixed-length and show how LLM
consistency can be improved by refactoring from categorical to real-valued
estimates. We show that CLEAR enables higher accuracy compared to expert human
recognition and human-authored prompts in every task with error rates improved
by up to two orders of magnitude and an ablation study evincing solution
concision.

</details>


### [135] [EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics](https://arxiv.org/pdf/2502.00205)
*Omar H. Khater, Abdul Jabbar Siddiqui, M. Shamim Hossain, Aiman El-Maleh*

Main category: cs.CV

TL;DR: EcoWeedNet is a lightweight, efficient model for weed detection in sustainable agriculture, achieving high accuracy with low computational cost and energy use.


<details>
  <summary>Details</summary>
Motivation: Weeds compete with crops for resources, impacting yields. Existing solutions are computationally expensive and less accurate.

Method: Proposes EcoWeedNet, a model optimized for weed detection, tested on the CottonWeedDet12 dataset.

Result: Achieves 95.2% mAP@0.5 with 4.21% of YOLOv4's parameters and 6.59% of its GFLOPs.

Conclusion: EcoWeedNet is deployable on low-power hardware, reducing carbon footprint and advancing sustainable agriculture.

Abstract: Sustainable agriculture plays a crucial role in ensuring world food security
for consumers. A critical challenge faced by sustainable precision agriculture
is weed growth, as weeds compete for essential resources with crops, such as
water, soil nutrients, and sunlight, which notably affect crop yields. The
adoption of automated computer vision technologies and ground agricultural
consumer electronic vehicles in precision agriculture offers sustainable,
low-carbon solutions. However, prior works suffer from issues such as low
accuracy and precision, as well as high computational expense. This work
proposes EcoWeedNet, a novel model that enhances weed detection performance
without introducing significant computational complexity, aligning with the
goals of low-carbon agricultural practices. The effectiveness of the proposed
model is demonstrated through comprehensive experiments on the CottonWeedDet12
benchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves
performance comparable to that of large models (mAP@0.5 = 95.2%), yet with
significantly fewer parameters (approximately 4.21% of the parameters of
YOLOv4), lower computational complexity and better computational efficiency
6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's
deployability on low-power consumer hardware, lower energy consumption, and
hence reduced carbon footprint, thereby emphasizing the application prospects
of EcoWeedNet in next-generation sustainable agriculture. These findings
provide the way forward for increased application of environmentally-friendly
agricultural consumer technologies.

</details>


### [136] [DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage Object Detection](https://arxiv.org/pdf/2502.11178)
*A. Enes Doruk, Hasan F. Ates*

Main category: cs.CV

TL;DR: DA-Mamba, a domain-adaptive Mamba-based one-stage object detection model, addresses limitations of 2D CNNs and transformers in domain adaptation by combining Mamba's efficient state-space modeling with attention mechanisms and introducing entropy-based knowledge distillation and gating attention for robust feature alignment.


<details>
  <summary>Details</summary>
Motivation: 2D CNNs struggle with long-range dependencies in domain adaptation, while transformers face computational complexity issues. The goal is to leverage Mamba's efficiency and global modeling for better cross-domain feature alignment.

Method: Combines Mamba's state-space modeling with attention mechanisms, introduces domain-adaptive spatial and channel-wise scanning, and proposes entropy-based knowledge distillation and gating attention to refine features and prevent overfitting.

Result: DA-Mamba achieves efficient and robust cross-domain feature alignment, addressing spatial and channel-wise variations while enhancing model generalization.

Conclusion: The proposed DA-Mamba model effectively overcomes the limitations of existing methods, offering a scalable and efficient solution for domain-adaptive object detection.

Abstract: Recent 2D CNN-based domain adaptation approaches struggle with long-range
dependencies due to limited receptive fields, making it difficult to adapt to
target domains with significant spatial distribution changes. While
transformer-based domain adaptation methods better capture distant
relationships through self-attention mechanisms that facilitate more effective
cross-domain feature alignment, their quadratic computational complexity makes
practical deployment challenging for object detection tasks across diverse
domains. Inspired by the global modeling and linear computation complexity of
the Mamba architecture, we present the first domain-adaptive Mamba-based
one-stage object detection model, termed DA-Mamba. Specifically, we combine
Mamba's efficient state-space modeling with attention mechanisms to address
domain-specific spatial and channel-wise variations. Our design leverages
domain-adaptive spatial and channel-wise scanning within the Mamba block to
extract highly transferable representations for efficient sequential
processing, while cross-attention modules generate long-range, mixed-domain
spatial features to enable robust soft alignment across domains. Besides,
motivated by the observation that hybrid architectures introduce feature noise
in domain adaptation tasks, we propose an entropy-based knowledge distillation
framework with margin ReLU, which adaptively refines multi-level
representations by suppressing irrelevant activations and aligning uncertainty
across source and target domains. Finally, to prevent overfitting caused by the
mixed-up features generated through cross-attention mechanisms, we propose
entropy-driven gating attention with random perturbations that simultaneously
refine target features and enhance model generalization.

</details>


### [137] [TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting](https://arxiv.org/pdf/2503.22676)
*Tony Yu, Yanlin Jin, Ashok Veeraraghavan, Akshat Dave, Guha Balakrishnan*

Main category: cs.CV

TL;DR: TranSplat is a 3D rendering algorithm for realistic cross-scene object transfer using Gaussian Splatting, addressing extraction and relighting challenges without explicit material estimation.


<details>
  <summary>Details</summary>
Motivation: To enable realistic cross-scene object transfer by solving challenges in 3D object extraction and relighting without needing explicit material properties.

Method: Uses Gaussian Splatting for 3D segmentation, user-guided insertion, and spherical harmonic analysis for relighting.

Result: Achieves excellent 3D object extractions and relighting, outperforming baselines in synthetic and real-world scenes.

Conclusion: TranSplat is effective but has limitations, as discussed.

Abstract: We present TranSplat, a 3D scene rendering algorithm that enables realistic
cross-scene object transfer (from a source to a target scene) based on the
Gaussian Splatting framework. Our approach addresses two critical challenges:
(1) precise 3D object extraction from the source scene, and (2) faithful
relighting of the transferred object in the target scene without explicit
material property estimation. TranSplat fits a splatting model to the source
scene, using 2D object masks to drive fine-grained 3D segmentation. Following
user-guided insertion of the object into the target scene, along with automatic
refinement of position and orientation, TranSplat derives per-Gaussian radiance
transfer functions via spherical harmonic analysis to adapt the object's
appearance to match the target scene's lighting environment. This relighting
strategy does not require explicitly estimating physical scene properties such
as BRDFs. Evaluated on several synthetic and real-world scenes and objects,
TranSplat yields excellent 3D object extractions and relighting performance
compared to recent baseline methods and visually convincing cross-scene object
transfers. We conclude by discussing the limitations of the approach.

</details>


### [138] [MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion](https://arxiv.org/pdf/2504.02287)
*Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide*

Main category: cs.CV

TL;DR: The paper introduces the MultiSensor-Home dataset and the MultiTSF method for multi-modal multi-view action recognition, addressing real-world challenges like distributed settings and asynchronous data.


<details>
  <summary>Details</summary>
Motivation: Current datasets and methods lack solutions for real-world challenges (e.g., wide-area distributed settings, asynchronous data) and struggle with inter-view relationships and spatial feature learning.

Method: Proposes MultiTSF, a Transformer-based fusion method with a human detection module to enhance spatial features and inter-view modeling.

Result: MultiTSF outperforms state-of-the-art methods on the MultiSensor-Home and MM-Office datasets, improving action recognition accuracy.

Conclusion: The MultiSensor-Home dataset and MultiTSF method advance real-world multi-modal multi-view action recognition, with code available for reproducibility.

Abstract: Multi-modal multi-view action recognition is a rapidly growing field in
computer vision, offering significant potential for applications in
surveillance. However, current datasets often fail to address real-world
challenges such as wide-area distributed settings, asynchronous data streams,
and the lack of frame-level annotations. Furthermore, existing methods face
difficulties in effectively modeling inter-view relationships and enhancing
spatial feature learning. In this paper, we introduce the MultiSensor-Home
dataset, a novel benchmark designed for comprehensive action recognition in
home environments, and also propose the Multi-modal Multi-view
Transformer-based Sensor Fusion (MultiTSF) method. The proposed
MultiSensor-Home dataset features untrimmed videos captured by distributed
sensors, providing high-resolution RGB and audio data along with detailed
multi-view frame-level action labels. The proposed MultiTSF method leverages a
Transformer-based fusion mechanism to dynamically model inter-view
relationships. Furthermore, the proposed method integrates a human detection
module to enhance spatial feature learning, guiding the model to prioritize
frames with human activity to enhance action the recognition accuracy.
Experiments on the proposed MultiSensor-Home and the existing MM-Office
datasets demonstrate the superiority of MultiTSF over the state-of-the-art
methods. Quantitative and qualitative results highlight the effectiveness of
the proposed method in advancing real-world multi-modal multi-view action
recognition. The source code is available at
https://github.com/thanhhff/MultiTSF.

</details>


### [139] [MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation](https://arxiv.org/pdf/2504.05184)
*Rayan Merghani Ahmed, Adnan Iltaf, Mohamed Elmanna, Gang Zhao, Hongliang Li, Yue Du, Bin Li, Shoujun Zhou*

Main category: cs.CV

TL;DR: A novel Supervised Prototypical Contrastive Loss (SPCL) is proposed to improve coronary DSA image segmentation by addressing high intra-class variance and class imbalance, implemented in an MSA-UNet3+ architecture, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of coronary DSA images is crucial for diagnosing and treating coronary artery diseases, but existing methods struggle with high intra-class variance and class imbalance.

Method: The SPCL loss combines supervised and prototypical contrastive learning to enhance feature differentiation and focus on hard-to-classify samples. The MSA-UNet3+ architecture includes a Multi-Scale Attention Encoder, Multi-Scale Dilated Bottleneck, and Contextual Attention Fusion Module for improved feature extraction and detail preservation.

Result: The method outperforms state-of-the-art techniques on a private dataset, achieving the highest Dice coefficient and F1-score while reducing ASD and ACD.

Conclusion: The proposed framework provides precise vessel segmentation, aiding in accurate diagnosis and treatment decisions, with code available for public use.

Abstract: Accurate segmentation of coronary Digital Subtraction Angiography images is
essential to diagnose and treat coronary artery diseases. Despite advances in
deep learning, challenges such as high intra-class variance and class imbalance
limit precise vessel delineation. Most existing approaches for coronary DSA
segmentation cannot address these issues. Also, existing segmentation network's
encoders do not directly generate semantic embeddings, which could enable the
decoder to reconstruct segmentation masks effectively from these well-defined
features. We propose a Supervised Prototypical Contrastive Loss that fuses
supervised and prototypical contrastive learning to enhance coronary DSA image
segmentation. The supervised contrastive loss enforces semantic embeddings in
the encoder, improving feature differentiation. The prototypical contrastive
loss allows the model to focus on the foreground class while alleviating the
high intra-class variance and class imbalance problems by concentrating only on
the hard-to-classify background samples. We implement the proposed SPCL loss
within an MSA-UNet3+: a Multi-Scale Attention-Enhanced UNet3+ architecture. The
architecture integrates key components: a Multi-Scale Attention Encoder and a
Multi-Scale Dilated Bottleneck designed to enhance multi-scale feature
extraction and a Contextual Attention Fusion Module built to keep fine-grained
details while improving contextual understanding. Experiments on a private
coronary DSA dataset show that MSA-UNet3+ outperforms state-of-the-art methods,
achieving the highest Dice coefficient and F1-score and significantly reducing
ASD and ACD. The developed framework provides clinicians with precise vessel
segmentation, enabling accurate identification of coronary stenosis and
supporting informed diagnostic and therapeutic decisions. The code will be
released at https://github.com/rayanmerghani/MSA-UNet3plus.

</details>


### [140] [Probability Density Geodesics in Image Diffusion Latent Space](https://arxiv.org/pdf/2504.06675)
*Qingtao Yu, Jaskirat Singh, Zhaoyuan Yang, Peter Henry Tu, Jing Zhang, Hongdong Li, Richard Hartley, Dylan Campbell*

Main category: cs.CV

TL;DR: The paper explores computing geodesics in diffusion latent space, leveraging probability density for path analysis and applications like image sequence interpolation.


<details>
  <summary>Details</summary>
Motivation: To study the structure of data space using diffusion models and geodesics, enabling analysis of high-density regions and practical applications.

Method: Algorithms for solving initial and boundary value problems in diffusion latent space, computing probability density and geodesic distance.

Result: Demonstrated analysis of video clips as geodesics and training-free image sequence interpolation/extrapolation.

Conclusion: Geodesics in diffusion latent space provide insights into data structure and enable practical applications without additional training.

Abstract: Diffusion models indirectly estimate the probability density over a data
space, which can be used to study its structure. In this work, we show that
geodesics can be computed in diffusion latent space, where the norm induced by
the spatially-varying inner product is inversely proportional to the
probability density. In this formulation, a path that traverses a high density
(that is, probable) region of image latent space is shorter than the equivalent
path through a low density region. We present algorithms for solving the
associated initial and boundary value problems and show how to compute the
probability density along the path and the geodesic distance between two
points. Using these techniques, we analyze how closely video clips approximate
geodesics in a pre-trained image diffusion space. Finally, we demonstrate how
these techniques can be applied to training-free image sequence interpolation
and extrapolation, given a pre-trained image diffusion model.

</details>


### [141] [PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection](https://arxiv.org/pdf/2504.08280)
*Xiong Li, Shulei Liu, Xingning Chen, Yisong Wu, Dong Zhu*

Main category: cs.CV

TL;DR: PNE-SGAN improves LiDAR loop closure detection by combining NDT-enhanced semantic graphs with probabilistic temporal filtering, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LiDAR loop closure detection lack robustness and accuracy due to coarse geometric representations and sensitivity to noise and viewpoint changes.

Method: PNE-SGAN uses NDT covariance matrices as geometric node features in a Graph Attention Network and integrates graph similarity scores into a probabilistic temporal filtering framework.

Result: PNE-SGAN achieves state-of-the-art performance on KITTI sequences (96.2% and 95.1% Average Precision) and excels in challenging bidirectional loop scenarios.

Conclusion: PNE-SGAN provides a highly accurate and robust solution for LiDAR loop closure detection, enhancing SLAM reliability in complex environments.

Abstract: LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous
Localization and Mapping (SLAM) but faces challenges in robustness and
accuracy. Existing methods, including semantic graph approaches, often suffer
from coarse geometric representations and lack temporal robustness against
noise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic
NDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.
PNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)
covariance matrices as rich, discriminative geometric node features, processed
via a Graph Attention Network (GAT). Crucially, it integrates graph similarity
scores into a probabilistic temporal filtering framework (modeled as an
HMM/Bayes filter), incorporating uncertain odometry for motion modeling and
utilizing forward-backward smoothing to effectively handle ambiguities.
Evaluations on challenging KITTI sequences (00 and 08) demonstrate
state-of-the-art performance, achieving Average Precision of 96.2\% and 95.1\%,
respectively. PNE-SGAN significantly outperforms existing methods, particularly
in difficult bidirectional loop scenarios where others falter. By synergizing
detailed NDT geometry with principled probabilistic temporal reasoning,
PNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing
SLAM reliability in complex, large-scale environments.

</details>


### [142] [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency](https://arxiv.org/pdf/2504.18589)
*Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao*

Main category: cs.CV

TL;DR: VCBENCH is a new benchmark for evaluating LVLMs on multimodal mathematical reasoning with visual dependencies, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook elementary-level math problems with visual dependencies, a key area for advancing AGI.

Method: VCBENCH includes 1,720 problems across six domains, using 6,697 images (avg. 3.9 per question) to test multi-image reasoning.

Result: Top LVLMs scored below 50% accuracy, showing major challenges in visual-mathematical integration.

Conclusion: VCBENCH highlights critical gaps in LVLMs and suggests future research directions.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly enhanced their ability to integrate visual and linguistic
information, achieving near-human proficiency in tasks like object recognition,
captioning, and visual question answering. However, current benchmarks
typically focus on knowledge-centric evaluations that assess domain-specific
expertise, often neglecting the core ability to reason about fundamental
mathematical elements and visual concepts. We identify a gap in evaluating
elementary-level math problems, which rely on explicit visual
dependencies-requiring models to discern, integrate, and reason across multiple
images while incorporating commonsense knowledge, all of which are crucial for
advancing toward broader AGI capabilities. To address this gap, we introduce
VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with
explicit visual dependencies. VCBENCH includes 1,720 problems across six
cognitive domains, featuring 6,697 images (averaging 3.9 per question) to
ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,
revealing substantial performance disparities, with even the top models unable
to exceed 50% accuracy. Our findings highlight the ongoing challenges in
visual-mathematical integration and suggest avenues for future LVLM
advancements.The project can be found at
https://alibaba-damo-academy.github.io/VCBench/.

</details>


### [143] [LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition](https://arxiv.org/pdf/2504.19186)
*Zhangshuo Qi, Luqi Cheng, Zijie Zhou, Guangming Xiong*

Main category: cs.CV

TL;DR: LRFusionPR improves autonomous driving place recognition by fusing LiDAR and radar data using a dual-branch network, enhancing accuracy and robustness in GPS-denied environments.


<details>
  <summary>Details</summary>
Motivation: Place recognition in GPS-denied environments is crucial for autonomous driving. LiDAR and radar fusion is promising but challenged by radar data noise, sparsity, and heterogeneous configurations.

Method: A dual-branch network fuses LiDAR and radar data in a unified polar coordinate BEV representation, using cross-attention for feature interaction and knowledge distillation for robustness.

Result: LRFusionPR achieves accurate place recognition and maintains robustness under varying weather conditions, validated on multiple datasets.

Conclusion: The proposed method effectively addresses challenges in LiDAR-radar fusion for place recognition, offering improved accuracy and robustness, with open-source code available.

Abstract: In autonomous driving, place recognition is critical for global localization
in GPS-denied environments. LiDAR and radar-based place recognition methods
have garnered increasing attention, as LiDAR provides precise ranging, whereas
radar excels in adverse weather resilience. However, effectively leveraging
LiDAR-radar fusion for place recognition remains challenging. The noisy and
sparse nature of radar data limits its potential to further improve recognition
accuracy. In addition, heterogeneous radar configurations complicate the
development of unified cross-modality fusion frameworks. In this paper, we
propose LRFusionPR, which improves recognition accuracy and robustness by
fusing LiDAR with either single-chip or scanning radar. Technically, a
dual-branch network is proposed to fuse different modalities within the unified
polar coordinate bird's eye view (BEV) representation. In the fusion branch,
cross-attention is utilized to perform cross-modality feature interactions. The
knowledge from the fusion branch is simultaneously transferred to the
distillation branch, which takes radar as its only input to further improve the
robustness. Ultimately, the descriptors from both branches are concatenated,
producing the multimodal global descriptor for place retrieval. Extensive
evaluations on multiple datasets demonstrate that our LRFusionPR achieves
accurate place recognition, while maintaining robustness under varying weather
conditions. Our open-source code will be released at
https://github.com/QiZS-BIT/LRFusionPR.

</details>


### [144] [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception](https://arxiv.org/pdf/2504.20468)
*Yuanchen Wu, Lu Zhang, Hang Yao, Junlong Du, Ke Yan, Shouhong Ding, Yunsheng Wu, Xiaoqiang Li*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in Large Vision-Language Models (LVLMs) by introducing 'Antidote,' a framework to mitigate counterfactual responses, and 'CP-Bench,' a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: LVLMs generate counterfactual responses, especially in counterfactual presupposition questions (CPQs), which current methods overlook.

Method: Proposes 'Antidote,' a synthetic data-driven post-training framework for self-correction and preference optimization.

Result: Antidote improves performance on CP-Bench by 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%.

Conclusion: Antidote effectively mitigates hallucinations without external supervision or catastrophic forgetting.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive results across
various cross-modal tasks. However, hallucinations, i.e., the models generating
counterfactual responses, remain a challenge. Though recent studies have
attempted to alleviate object perception hallucinations, they focus on the
models' response generation, and overlooking the task question itself. This
paper discusses the vulnerability of LVLMs in solving counterfactual
presupposition questions (CPQs), where the models are prone to accept the
presuppositions of counterfactual objects and produce severe hallucinatory
responses. To this end, we introduce "Antidote", a unified, synthetic
data-driven post-training framework for mitigating both types of hallucination
above. It leverages synthetic data to incorporate factual priors into questions
to achieve self-correction, and decouple the mitigation process into a
preference optimization problem. Furthermore, we construct "CP-Bench", a novel
benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce
factual responses. Applied to the LLaVA series, Antidote can simultaneously
enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR
by 30-50%, all without relying on external supervision from stronger LVLMs or
human feedback and introducing noticeable catastrophic forgetting issues.

</details>


### [145] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/pdf/2504.21226)
*Jiaqi Liu, Ran Tong, Aowei Shen, Shuzheng Li, Changlin Yang, Lisha Xu*

Main category: cs.CV

TL;DR: MemeBLIP2 is a lightweight multimodal system for detecting harmful memes by effectively combining image and text features, improving detection accuracy even for ironic or culturally specific content.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting harmful messages in memes, which combine visuals and text, often with subtle or culturally nuanced cues.

Method: Builds on BLIP-2, adding modules to align and fuse image and text representations in a shared space for better classification.

Result: Evaluated on PrideMM datasets, MemeBLIP2 captures subtle multimodal cues, enhancing harmful meme detection.

Conclusion: MemeBLIP2 effectively improves harmful meme detection by leveraging aligned multimodal features.

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>


### [146] [VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction](https://arxiv.org/pdf/2504.21718)
*Shiying Li, Xingqun Qi, Bingkun Yang, Chen Weile, Zezhao Tian, Muyi Sun, Qifeng Liu, Man Zhang, Zhenan Sun*

Main category: cs.CV

TL;DR: The paper introduces VividListener, a framework for expressive and controllable listener head dynamics in dialogue modeling, supported by a new dataset, ListenerX.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained control over motion variations and emotional intensity in long-sequence modeling, and there's a scarcity of large-scale paired speaker-listener corpora with multi-modal annotations.

Method: The authors propose VividListener, which includes a Responsive Interaction Module (RIM) for multi-modal interactive embeddings and Emotional Intensity Tags (EIT) for emotion intensity editing.

Result: VividListener achieves state-of-the-art performance on the ListenerX dataset, demonstrating expressive and controllable listener dynamics.

Conclusion: The framework and dataset address limitations in dialogue modeling, enabling fine-grained, expressive listener behavior.

Abstract: Generating responsive listener head dynamics with nuanced emotions and
expressive reactions is crucial for practical dialogue modeling in various
virtual avatar animations. Previous studies mainly focus on the direct
short-term production of listener behavior. They overlook the fine-grained
control over motion variations and emotional intensity, especially in
long-sequence modeling. Moreover, the lack of long-term and large-scale paired
speaker-listener corpora including head dynamics and fine-grained
multi-modality annotations (e.g., text-based expression descriptions, emotional
intensity) also limits the application of dialogue modeling.Therefore, we first
newly collect a large-scale multi-turn dataset of 3D dyadic conversation
containing more than 1.4M valid frames for multi-modal responsive interaction,
dubbed ListenerX. Additionally, we propose VividListener, a novel framework
enabling fine-grained, expressive and controllable listener dynamics modeling.
This framework leverages multi-modal conditions as guiding principles for
fostering coherent interactions between speakers and listeners.Specifically, we
design the Responsive Interaction Module (RIM) to adaptively represent the
multi-modal interactive embeddings. RIM ensures the listener dynamics achieve
fine-grained semantic coordination with textual descriptions and adjustments,
while preserving expressive reaction with speaker behavior. Meanwhile, we
design the Emotional Intensity Tags (EIT) for emotion intensity editing with
multi-modal information integration, applying to both text descriptions and
listener motion amplitude.Extensive experiments conducted on our newly
collected ListenerX dataset demonstrate that VividListener achieves
state-of-the-art performance, realizing expressive and controllable listener
dynamics.

</details>


### [147] [Token Coordinated Prompt Attention is Needed for Visual Prompting](https://arxiv.org/pdf/2505.02406)
*Zichen Liu, Xu Zou, Gang Hua, Jiahuan Zhou*

Main category: cs.CV

TL;DR: The paper introduces TCPA, a module for Vision Transformers that assigns specific prompts to different tokens (CLS and image tokens) to enhance feature diversity and discriminative power.


<details>
  <summary>Details</summary>
Motivation: Existing visual prompting methods treat all tokens uniformly, limiting ViT's representational capacity and causing biased features.

Method: TCPA disentangles prompts into CLS Prompts and Image Prompts, using attention mechanisms and a matching function for precise token-prompt interactions.

Result: Experiments show TCPA improves feature diversity and discriminative ability across benchmarks.

Conclusion: TCPA effectively addresses token-specific prompt interaction, enhancing ViT performance.

Abstract: Visual prompting techniques are widely used to efficiently fine-tune
pretrained Vision Transformers (ViT) by learning a small set of shared prompts
for all tokens. However, existing methods overlook the unique roles of
different tokens in conveying discriminative information and interact with all
tokens using the same prompts, thereby limiting the representational capacity
of ViT. This often leads to indistinguishable and biased prompt-extracted
features, hindering performance. To address this issue, we propose a
plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns
specific coordinated prompts to different tokens for attention-based
interactions. Firstly, recognizing the distinct functions of CLS and image
tokens-global information aggregation and local feature extraction, we
disentangle the prompts into CLS Prompts and Image Prompts, which interact
exclusively with CLS tokens and image tokens through attention mechanisms. This
enhances their respective discriminative abilities. Furthermore, as different
image tokens correspond to distinct image patches and contain diverse
information, we employ a matching function to automatically assign coordinated
prompts to individual tokens. This enables more precise attention interactions,
improving the diversity and representational capacity of the extracted
features. Extensive experiments across various benchmarks demonstrate that TCPA
significantly enhances the diversity and discriminative power of the extracted
features. The code is available at
https://github.com/zhoujiahuan1991/ICML2025-TCPA.

</details>


### [148] [Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction](https://arxiv.org/pdf/2505.02471)
*Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang*

Main category: cs.CV

TL;DR: Ming-Lite-Uni is an open-source multimodal framework unifying vision and language with a visual generator and autoregressive model, supporting text-to-image generation and image editing.


<details>
  <summary>Details</summary>
Motivation: To advance multimodal AI by integrating vision and language tasks, aligning with broader AGI goals.

Method: Combines MetaQueries and M2-omni frameworks with multi-scale tokens and representation alignment, using a fixed MLLM and learnable diffusion model.

Result: Demonstrates strong performance and fluid interaction, with open-sourced code and models.

Conclusion: Ming-Lite-Uni highlights the potential of unified models for AGI, with plans for further refinement.

Abstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a
newly designed unified visual generator and a native multimodal autoregressive
model tailored for unifying vision and language. Specifically, this project
provides an open-source implementation of the integrated MetaQueries and
M2-omni framework, while introducing the novel multi-scale learnable tokens and
multi-scale representation alignment strategy. By leveraging a fixed MLLM and a
learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to
perform both text-to-image generation and instruction based image editing
tasks, expanding their capabilities beyond pure visual understanding. Our
experimental results demonstrate the strong performance of Ming-Lite-Uni and
illustrate the impressive fluid nature of its interactive process. All code and
model weights are open-sourced to foster further exploration within the
community. Notably, this work aligns with concurrent multimodal AI milestones -
such as ChatGPT-4o with native image generation updated in March 25, 2025 -
underscoring the broader significance of unified models like Ming-Lite-Uni on
the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further
refined.

</details>


### [149] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/pdf/2505.03204)
*Liu Suxing, Byungwon Min*

Main category: cs.CV

TL;DR: Deep learning struggles with limited annotated data in breast cancer histopathology image classification.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of declining performance in deep learning models due to scarce annotated data in medical imaging.

Method: Not explicitly stated, but implies the use of deep learning for histopathology image classification.

Result: Performance declines with limited annotated data.

Conclusion: Limited annotated data is a critical challenge for deep learning in medical imaging.

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [150] [Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/pdf/2505.02567)
*Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang*

Main category: cs.CV

TL;DR: A survey on unifying multimodal understanding and image generation models, covering architectures, datasets, and challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between multimodal understanding (autoregressive-based) and image generation (diffusion-based) models by exploring unified frameworks.

Method: Reviews existing unified models, categorizing them into diffusion-based, autoregressive-based, and hybrid approaches, and analyzes their designs.

Result: Identifies key challenges (tokenization, cross-modal attention, data) and provides datasets/benchmarks for future research.

Conclusion: Aims to inspire further research and serve as a reference, with plans for regular updates.

Abstract: Recent years have seen remarkable progress in both multimodal understanding
models and image generation models. Despite their respective successes, these
two domains have evolved independently, leading to distinct architectural
paradigms: While autoregressive-based architectures have dominated multimodal
understanding, diffusion-based models have become the cornerstone of image
generation. Recently, there has been growing interest in developing unified
frameworks that integrate these tasks. The emergence of GPT-4o's new
capabilities exemplifies this trend, highlighting the potential for
unification. However, the architectural differences between the two domains
pose significant challenges. To provide a clear overview of current efforts
toward unification, we present a comprehensive survey aimed at guiding future
research. First, we introduce the foundational concepts and recent advancements
in multimodal understanding and text-to-image generation models. Next, we
review existing unified models, categorizing them into three main architectural
paradigms: diffusion-based, autoregressive-based, and hybrid approaches that
fuse autoregressive and diffusion mechanisms. For each category, we analyze the
structural designs and innovations introduced by related works. Additionally,
we compile datasets and benchmarks tailored for unified models, offering
resources for future exploration. Finally, we discuss the key challenges facing
this nascent field, including tokenization strategy, cross-modal attention, and
data. As this area is still in its early stages, we anticipate rapid
advancements and will regularly update this survey. Our goal is to inspire
further research and provide a valuable reference for the community. The
references associated with this survey are available on GitHub
(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).

</details>


### [151] [Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge](https://arxiv.org/pdf/2505.02784)
*Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline DÃ¤ndliker, CÃ©line Steger, MÃ©riam Koob, Yvan Gomez, Anton JakovÄiÄ, Melita KlaiÄ, Ana AdÅ¾iÄ, Pavel MarkoviÄ, Gracia GrabariÄ, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-RachmÃ¼hl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier LladÃ³, Simone Chiarella, Gerard MartÃ­-Juan, Miguel Ãngel GonzÃ¡lez Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, KeÃ¯n Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Å½iga Å piclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Milton O. Candela-Leal, Andrea Gondova, Sungmin You, Abdul Qayyum, Moona Mazher, Steven A Niederer, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: The FeTA Challenge 2024 benchmarked automated fetal brain MRI analysis, introducing biometry prediction and low-field MRI data. Segmentation accuracy nears inter-rater variability, while biometry methods struggled. Topological metrics and data diversity were highlighted as key for future improvements.


<details>
  <summary>Details</summary>
Motivation: To advance automated fetal brain MRI analysis by evaluating segmentation and biometry tasks, including low-field MRI data and topological metrics.

Method: Sixteen teams submitted segmentation methods, and seven participated in biometry prediction, evaluated on multi-centric datasets including low-field MRI.

Result: Segmentation accuracy plateaued near inter-rater variability. Biometry methods underperformed, and low-field MRI achieved high scores. Topological metrics revealed missed differences.

Conclusion: FeTA 2024 highlights the need for data-centric approaches, better topological evaluation, and diverse datasets for robust AI tools in fetal brain MRI.

Abstract: Accurate fetal brain tissue segmentation and biometric analysis are essential
for studying brain development in utero. The FeTA Challenge 2024 advanced
automated fetal brain MRI analysis by introducing biometry prediction as a new
task alongside tissue segmentation. For the first time, our diverse
multi-centric test set included data from a new low-field (0.55T) MRI dataset.
Evaluation metrics were also expanded to include the topology-specific Euler
characteristic difference (ED). Sixteen teams submitted segmentation methods,
most of which performed consistently across both high- and low-field scans.
However, longitudinal trends indicate that segmentation accuracy may be
reaching a plateau, with results now approaching inter-rater variability. The
ED metric uncovered topological differences that were missed by conventional
metrics, while the low-field dataset achieved the highest segmentation scores,
highlighting the potential of affordable imaging systems when paired with
high-quality reconstruction. Seven teams participated in the biometry task, but
most methods failed to outperform a simple baseline that predicted measurements
based solely on gestational age, underscoring the challenge of extracting
reliable biometric estimates from image data alone. Domain shift analysis
identified image quality as the most significant factor affecting model
generalization, with super-resolution pipelines also playing a substantial
role. Other factors, such as gestational age, pathology, and acquisition site,
had smaller, though still measurable, effects. Overall, FeTA 2024 offers a
comprehensive benchmark for multi-class segmentation and biometry estimation in
fetal brain MRI, underscoring the need for data-centric approaches, improved
topological evaluation, and greater dataset diversity to enable clinically
robust and generalizable AI tools.

</details>


### [152] [No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves](https://arxiv.org/pdf/2505.02831)
*Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang*

Main category: cs.CV

TL;DR: Self-Representation Alignment (SRA) improves diffusion transformers' performance by aligning latent representations internally, eliminating the need for external frameworks or pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing diffusion transformers require complex external representation frameworks or pre-trained models. SRA aims to simplify this by leveraging the model's inherent discriminative process.

Method: SRA aligns latent representations between earlier (noisier) and later (cleaner) layers of the diffusion transformer through self-distillation during generative training.

Result: SRA consistently improves performance for DiTs and SiTs, outperforming complex auxiliary frameworks and matching methods relying on external representation priors.

Conclusion: SRA is an effective, simple method for enhancing diffusion transformers without external dependencies, demonstrating strong performance and practicality.

Abstract: Recent studies have demonstrated that learning a meaningful internal
representation can both accelerate generative training and enhance generation
quality of the diffusion transformers. However, existing approaches necessitate
to either introduce an additional and complex representation training framework
or rely on a large-scale, pre-trained representation foundation model to
provide representation guidance during the original generative training
process. In this study, we posit that the unique discriminative process
inherent to diffusion transformers enables them to offer such guidance without
requiring external representation components. We therefore propose
Self-Representation Alignment (SRA), a simple yet straightforward method that
obtain representation guidance through a self-distillation manner.
Specifically, SRA aligns the output latent representation of the diffusion
transformer in earlier layer with higher noise to that in later layer with
lower noise to progressively enhance the overall representation learning during
only generative training process. Experimental results indicate that applying
SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA
not only significantly outperforms approaches relying on auxiliary, complex
representation training frameworks but also achieves performance comparable to
methods that heavily dependent on powerful external representation priors.

</details>


### [153] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/pdf/2505.03567)
*Zengli Luo, Canlong Zhang, Xiaochun Lu, Zhixin Li, Zhiwen Wang*

Main category: cs.CV

TL;DR: UPD-TBPS is a new framework for text-based pedestrian search that reduces uncertainties in detection and matching using multi-granularity uncertainty estimation, prototype-based decoupling, and cross-modal re-identification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-based pedestrian search struggle with uncertainties in complex scenes, leading to degraded performance.

Method: The framework includes Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID) to improve target identification and matching.

Result: Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets confirm the framework's effectiveness.

Conclusion: UPD-TBPS successfully addresses uncertainties in pedestrian search, enhancing detection and retrieval accuracy.

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>


### [154] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/pdf/2505.03631)
*Linhan Cao, Wei Sun, Kaiwei Zhang, Yicong Peng, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: A self-supervised learning framework for video quality assessment (VQA) is introduced, leveraging large-scale unlabeled web videos and iterative self-improvement to outperform supervised models.


<details>
  <summary>Details</summary>
Motivation: The reliance on manually annotated datasets for VQA is labor-intensive and limits generalization. This work aims to overcome these challenges using self-supervised learning.

Method: The approach uses a learning-to-rank paradigm on video pairs labeled via pseudo-labeling and synthetic distortions, with an iterative self-improvement strategy to refine annotations.

Result: The model achieves zero-shot performance matching supervised models, superior out-of-distribution generalization, and state-of-the-art results when fine-tuned.

Conclusion: The self-supervised framework effectively trains generalized VQA models, with datasets and code to be released for future research.

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [155] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/pdf/2505.03770)
*Mouad Abrini, Omri Abend, Dina Acklin, Henny Admoni, Gregor Aichinger, Nitay Alon, Zahra Ashktorab, Ashish Atreja, Moises Auron, Alexander Aufreiter, Raghav Awasthi, Soumya Banerjee, Joe M. Barnby, Rhea Basappa, Severin Bergsmann, Djallel Bouneffouf, Patrick Callaghan, Marc Cavazza, Thierry Chaminade, Sonia Chernova, Mohamed Chetouan, Moumita Choudhury, Axel Cleeremans, Jacek B. Cywinski, Fabio Cuzzolin, Hokin Deng, N'yoma Diamond, Camilla Di Pasquasio, Guillaume Dumas, Max van Duijn, Mahapatra Dwarikanath, Qingying Gao, Ashok Goel, Rebecca Goldstein, Matthew Gombolay, Gabriel Enrique Gonzalez, Amar Halilovic, Tobias Halmdienst, Mahimul Islam, Julian Jara-Ettinger, Natalie Kastel, Renana Keydar, Ashish K. Khanna, Mahdi Khoramshahi, JiHyun Kim, MiHyeon Kim, YoungBin Kim, Senka Krivic, Nikita Krasnytskyi, Arun Kumar, JuneHyoung Kwon, Eunju Lee, Shane Lee, Peter R. Lewis, Xue Li, Yijiang Li, Michal Lewandowski, Nathan Lloyd, Matthew B. Luebbers, Dezhi Luo, Haiyun Lyu, Dwarikanath Mahapatra, Kamal Maheshwari, Mallika Mainali, Piyush Mathur, Patrick Mederitsch, Shuwa Miura, Manuel Preston de Miranda, Reuth Mirsky, Shreya Mishra, Nina Moorman, Katelyn Morrison, John Muchovej, Bernhard Nessler, Felix Nessler, Hieu Minh Jord Nguyen, Abby Ortego, Francis A. Papay, Antoine Pasquali, Hamed Rahimi, Charumathi Raghu, Amanda Royka, Stefan Sarkadi, Jaelle Scheuerman, Simon Schmid, Paul Schrater, Anik Sen, Zahra Sheikhbahaee, Ke Shi, Reid Simmons, Nishant Singh, Mason O. Smith, Ramira van der Meulen, Anthia Solaki, Haoran Sun, Viktor Szolga, Matthew E. Taylor, Travis Taylor, Sanne Van Waveren, Juan David Vargas, Rineke Verbrugge, Eitan Wagner, Justin D. Weisz, Ximing Wen, William Yeoh, Wenlong Zhang, Michelle Zhao, Shlomo Zilberstein*

Main category: cs.AI

TL;DR: A curated anthology of papers from a 2025 AAAI workshop on advancing AI through Theory of Mind (ToM).


<details>
  <summary>Details</summary>
Motivation: To provide an open-access resource for the ToM and AI research community.

Method: Selection of papers presented at the AAAI 2025 workshop.

Result: A published volume of research contributions.

Conclusion: The volume serves as a valuable resource for advancing ToM in AI.

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>


### [156] [Design description of Wisdom Computing Persperctive](https://arxiv.org/pdf/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: A system combining AI and visualization to recognize handwritten matrices and display calculation steps, enhancing math learning.


<details>
  <summary>Details</summary>
Motivation: Addresses students' difficulty in understanding abstract math formulas and complex calculations.

Method: Uses Mamba backbone networks for handwriting recognition, YOLO for matrix extraction, CoordAttention for spatial accuracy, and Manim for animated step-by-step displays.

Result: Creates a modular, flexible system that dynamically generates animations for various math tasks, improving student comprehension.

Conclusion: The system is an intuitive, interactive educational tool that bridges the gap between knowledge and understanding in math.

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


### [157] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/pdf/2505.03941)
*Matan Shamir, Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAML introduces a metric learning approach for Goal Recognition (GR) using a Siamese network, enabling quick adaptation to new goals with minimal training.


<details>
  <summary>Details</summary>
Motivation: Traditional GR methods require costly domain models or extensive training for new goals, limiting flexibility and efficiency.

Method: GRAML employs a Siamese network and RNN to learn a metric in an embedding space, distinguishing observation traces by goal similarity.

Result: GRAML outperforms state-of-the-art GR methods in speed, flexibility, and runtime while maintaining accuracy.

Conclusion: GRAML automates model-learning for GR and efficiently adapts to new goals, offering a scalable and accurate solution.

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [158] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/pdf/2505.03947)
*Xiang Li, Yiyang Hao, Doug Fulop*

Main category: cs.AI

TL;DR: LLMs with RL post-training can play Atari's Frogger zero-shot, improve with in-context learning, and boost traditional RL methods via demonstrations.


<details>
  <summary>Details</summary>
Motivation: Develop general-purpose RL agents that adapt quickly to new tasks, addressing the inefficiency of current RL methods.

Method: Use reasoning LLMs with RL post-training for zero-shot play, study in-context learning, and bootstrap RL with LLM demonstrations.

Result: LLMs perform well in Frogger zero-shot, benefit from in-context learning, and enhance RL methods' efficiency.

Conclusion: LLMs show promise for efficient RL adaptation and can augment traditional RL training.

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>


### [159] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/pdf/2505.03961)
*Gerrit GroÃmann, Larisa Ivanova, Sai Leela Poduru, Mohaddeseh Tabrizian, Islam Mesabah, David A. Selby, Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: The study investigates how shared narratives influence LLM agents' collaboration in a public goods game, finding that common stories boost cooperation while differing stories or self-interest priming reduce it.


<details>
  <summary>Details</summary>
Motivation: To explore whether shared narratives, which drive human cooperation, can similarly encourage collaboration among LLM agents.

Method: A finitely repeated public goods game with LLM agents primed by teamwork stories, testing negotiation behavior under varying conditions.

Result: Common stories improved collaboration, while differing stories or self-interest priming led to egoistic behavior and reduced success rates.

Conclusion: Shared narratives can enhance LLM agent cooperation, suggesting implications for multi-agent system design and AI alignment.

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [160] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/pdf/2505.03985)
*Zirong Chen, Ziyan An, Jennifer Reynolds, Kristin Mullen, Stephen Martini, Meiyi Ma*

Main category: cs.AI

TL;DR: LogiDebrief automates 9-1-1 call debriefing using AI (STL and LLMs) for systematic performance evaluation, improving coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments.

Method: LogiDebrief formalizes call-taking requirements as logical specifications, using a three-step process: contextual understanding, STL-based runtime checking, and automated report generation.

Result: Deployed in Nashville, it debriefed 1,701 calls, saving 311.85 hours. Empirical evaluation confirms accuracy and effectiveness.

Conclusion: LogiDebrief enhances call-taking performance and efficiency, demonstrating real-world impact.

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>


### [161] [An alignment safety case sketch based on debate](https://arxiv.org/pdf/2505.03989)
*Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving*

Main category: cs.AI

TL;DR: The paper explores using debate between AI systems to ensure safety and honesty, addressing risks like research sabotage in AI companies.


<details>
  <summary>Details</summary>
Motivation: As AI systems surpass human capabilities, human oversight becomes challenging. Debate between AI systems is proposed to ensure alignment and prevent harmful actions.

Method: The paper outlines a debate-based training method for AI, ensuring honesty through exploration guarantees and online training.

Result: The safety case relies on four claims: debate proficiency, honesty from debate, stability during deployment, and error tolerance.

Conclusion: Open research problems remain to fully validate debate as a safety mechanism for AI systems.

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [162] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/pdf/2505.04019)
*Matteo Ceschin, Leonardo Arrighi, Luca Longo, Sylvio Barbon Junior*

Main category: cs.AI

TL;DR: The paper introduces a novel XAI method using Decision Predicate Graph (DPG) to globally explain outlier detection in Isolation Forest (iForest), enhancing interpretability and decision transparency.


<details>
  <summary>Details</summary>
Motivation: Understanding data pre-processing and model interpretability is crucial for robust ML solutions. iForest's effectiveness grows with complexity, but this complicates explainability, necessitating a global explanation method.

Method: The proposed method uses DPG to clarify ensemble logic, introducing the Inlier-Outlier Propagation Score (IOP-Score) to explain outlier identification and feature contributions.

Result: The approach improves iForest's explainability, offering insights into decision boundaries and feature usage, advancing the state-of-the-art in XAI for outlier detection.

Conclusion: The method promotes a fully explainable ML pipeline by detailing outlier identification logic and feature contributions, enhancing transparency in iForest.

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>


### [163] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/pdf/2505.04115)
*Luise Ge, Brendan Juba, Kris Nilsson*

Main category: cs.AI

TL;DR: The paper introduces a tractable first-order relational probabilistic inference method for hybrid variables, inspired by human reasoning, with polynomial-time lifted reasoning in bounded-degree settings.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing expressive power and computational tractability in reasoning under uncertainty, inspired by human reasoning.

Method: Extends sum-of-squares logic of expectation to relational settings, enabling lifted reasoning in bounded-degree fragments with polynomial-time complexity.

Result: Demonstrates tractable reasoning with hybrid variables, tight bounds provable by proofs of given degree and size, and completeness in sum-of-squares refutations.

Conclusion: The method successfully balances expressive power and tractability, offering a proof-theoretic framework for efficient probabilistic inference.

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>


### [164] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/pdf/2505.04310)
*Simo Alami C., Rim Kaddah, Jesse Read, Marie-Paule Cani*

Main category: cs.AI

TL;DR: A new DistRL architecture using normalizing flows for flexible return distributions, outperforming categorical and quantile methods on ATARI-5.


<details>
  <summary>Details</summary>
Motivation: Existing DistRL methods (e.g., C51) have limitations like fixed support or biased gradients, prompting a need for more flexible and efficient modeling.

Method: Uses normalizing flows for unbounded return distributions and introduces a novel surrogate for CramÃ©r distance to address gradient issues.

Result: Outperforms PDF-based models and remains competitive with quantile methods on ATARI-5, with higher parameter efficiency.

Conclusion: The proposed architecture offers superior flexibility and efficiency for modeling return distributions in DistRL.

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>


### [165] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/pdf/2505.04313)
*Stephen Richard Varey, Alessandro Di Stefano, The Anh Han*

Main category: cs.AI

TL;DR: KERAIA is a novel framework for symbolic knowledge engineering, addressing challenges in dynamic, complex environments by transforming human expertise into AI-tractable algorithms. It introduces innovations like Clouds of Knowledge and Dynamic Relations, validated through case studies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between unstructured human expertise and computationally tractable AI algorithms, overcoming limitations of static knowledge representation paradigms.

Method: KERAIA uses frame-based reasoning, Clouds of Knowledge, Dynamic Relations, and Lines of Thought for adaptive knowledge transformation. It includes KSYNTH language and GPPB for unified inference.

Result: Validated through case studies in naval warfare, industrial diagnostics, and RISK game, showing versatility and expressiveness.

Conclusion: KERAIA advances knowledge representation with dynamic, context-sensitive methods, ensuring transparency and practical applicability.

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>


### [166] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/pdf/2505.04317)
*Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang*

Main category: cs.AI

TL;DR: HCSP, a hierarchical RL framework, excels in 3v3 multi-drone volleyball by separating strategic and motion control, achieving high win rates and emergent team behaviors.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of long-horizon dependencies, inter-agent coupling, and quadrotor dynamics in competitive multi-agent tasks.

Method: Hierarchical Co-Self-Play (HCSP) with a three-stage pipeline: skill training, strategy learning via self-play, and joint fine-tuning.

Result: HCSP outperforms baselines with 82.9% win rate and shows emergent behaviors like role switching.

Conclusion: HCSP's hierarchical design and co-self-play are effective for complex multi-agent tasks.

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>


### [167] [Uncertain Machine Ethics Planning](https://arxiv.org/pdf/2505.04352)
*Simon Kolker, Louise A. Dennis, Ramon Fraga Pereira, Mengwei Xu*

Main category: cs.AI

TL;DR: The paper formalizes ethical decision-making under uncertainty using Multi-Moral Markov Decision Processes and a heuristic algorithm based on Multi-Objective AO*.


<details>
  <summary>Details</summary>
Motivation: Addressing conflicting moral theories (e.g., Utilitarianism, Deontology, Virtue Ethics) in machine ethics decisions and balancing them for long-term preferable outcomes.

Method: Develops a Multi-Moral Markov Decision Process and Multi-Moral Stochastic Shortest Path Problem, using a heuristic algorithm with Hypothetical Retrospection for ethical reasoning.

Result: Validated by a case study on the ethical dilemma of stealing insulin for someone in need.

Conclusion: The approach provides a framework for handling moral uncertainty and conflicting ethical theories in machine decision-making.

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>


### [168] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/pdf/2505.04480)
*Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park*

Main category: cs.AI

TL;DR: TrajEvo is a framework using LLMs and evolutionary algorithms to automate trajectory prediction heuristics, outperforming traditional and deep learning methods in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional heuristics lack accuracy, while deep learning methods are computationally expensive and lack explainability, limiting practical adoption.

Method: TrajEvo uses an evolutionary algorithm with Cross-Generation Elite Sampling and a Statistics Feedback Loop to refine heuristics from trajectory data.

Result: TrajEvo outperforms heuristic and deep learning methods on ETH-UCY and generalizes better to the unseen SDD dataset.

Conclusion: TrajEvo advances automated design of fast, explainable, and generalizable trajectory prediction heuristics, with open-source code for future research.

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>


### [169] [On some improvements to Unbounded Minimax](https://arxiv.org/pdf/2505.04525)
*Quentin Cohen-Solal, Tristan Cazenave*

Main category: cs.AI

TL;DR: Experimental evaluation of four modifications to Unbounded Best-First Minimax shows performance improvements in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of untested modifications on the Unbounded Best-First Minimax algorithm's efficiency.

Method: Tested four modifications: transposition tables, backpropagation strategy, heuristic function replacement, and completion technique.

Result: Transposition tables and completion techniques improved performance; heuristic replacement was beneficial only in costly settings.

Conclusion: Targeted modifications can enhance the algorithm's efficiency, though their effectiveness depends on context.

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>


### [170] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/pdf/2505.04528)
*Qi Liu, Xinhao Zheng, Renqiu Xia, Xingzhi Qi, Qinxiang Cao, Junchi Yan*

Main category: cs.AI

TL;DR: The paper introduces FPS and D-FPS frameworks for formal, process-verified problem-solving, evaluates them on benchmarks, and proposes RPE for answer correctness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a general formulation of problem-solving and the need for process-level verifiability in AI-based agents.

Method: Proposes FPS as a deterministic Markov decision process and D-FPS for decoupled solving and verification. Uses FTP environments and benchmarks like FormalMath500.

Result: Evaluated models solved up to 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

Conclusion: The frameworks are expressive, sound, and complete, offering a principled approach to verified problem-solving.

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>


### [171] [Qualitative Analysis of $Ï$-Regular Objectives on Robust MDPs](https://arxiv.org/pdf/2505.04539)
*Ali Asadi, Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, Ali Shafiee*

Main category: cs.AI

TL;DR: The paper studies qualitative problems for reachability and parity objectives in Robust Markov Decision Processes (RMDPs) without structural assumptions, presenting efficient oracle-based algorithms and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address the qualitative analysis problem in RMDPs for reachability and parity objectives without relying on structural assumptions like unichain or aperiodic properties.

Method: Develops efficient algorithms with oracle access to uncertainty sets for solving qualitative problems.

Result: Demonstrates effectiveness through experiments on classical RMDP examples, scaling to thousands of states.

Conclusion: The proposed oracle-based approach efficiently solves qualitative problems in RMDPs, validated by experimental results.

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>


### [172] [Learning to Play Two-Player Perfect-Information Games without Knowledge](https://arxiv.org/pdf/2008.01188)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: Proposes four techniques for learning game state evaluation functions via reinforcement, improving gameplay, and applying them to Hex to surpass Mohex 3HNN.


<details>
  <summary>Details</summary>
Motivation: To enhance reinforcement learning for game state evaluation without losing information and improve gameplay performance.

Method: 1. Generalization of tree bootstrapping for RL. 2. Modified minimax with unbounded depth. 3. Replacement of classic game gain with reinforcement heuristics. 4. New action selection distribution.

Result: Techniques improve gameplay; applied to Hex, surpassing Mohex 3HNN's level via self-play RL.

Conclusion: The proposed methods effectively enhance game state evaluation and performance in Hex.

Abstract: In this paper, several techniques for learning game state evaluation
functions by reinforcement are proposed. The first is a generalization of tree
bootstrapping (tree learning): it is adapted to the context of reinforcement
learning without knowledge based on non-linear functions. With this technique,
no information is lost during the reinforcement learning process. The second is
a modification of minimax with unbounded depth extending the best sequences of
actions to the terminal states. This modified search is intended to be used
during the learning process. The third is to replace the classic gain of a game
(+1 / -1) with a reinforcement heuristic. We study particular reinforcement
heuristics such as: quick wins and slow defeats ; scoring ; mobility or
presence. The four is a new action selection distribution. The conducted
experiments suggest that these techniques improve the level of play. Finally,
we apply these different techniques to design program-players to the game of
Hex (size 11 and 13) surpassing the level of Mohex 3HNN with reinforcement
learning from self-play without knowledge.

</details>


### [173] [From Latent to Lucid: Transforming Knowledge Graph Embeddings into Interpretable Structures with KGEPrisma](https://arxiv.org/pdf/2406.01759)
*Christoph Wehner, Chrysa Iliopoulou, Ute Schmid, Tarek R. Besold*

Main category: cs.AI

TL;DR: A post-hoc, local explainable AI method for Knowledge Graph Embedding (KGE) models decodes latent representations to provide interpretable insights without retraining.


<details>
  <summary>Details</summary>
Motivation: KGE models are opaque, making their predictions hard to explain. This work aims to bridge the gap between abstract embeddings and human-understandable explanations.

Method: The approach leverages the smoothness of embeddings to identify symbolic structures (triples) in subgraph neighborhoods, translating them into rules and facts.

Result: The method delivers faithful, localized explanations and supports diverse explanation types (rule-based, instance-based, analogy-based).

Conclusion: The proposed method enhances KGE model transparency and trustworthiness, enabling real-time explainability for large-scale knowledge graphs.

Abstract: In this paper, we introduce a post-hoc and local explainable AI method
tailored for Knowledge Graph Embedding (KGE) models. These models are essential
to Knowledge Graph Completion yet criticized for their opaque, black-box
nature. Despite their significant success in capturing the semantics of
knowledge graphs through high-dimensional latent representations, their
inherent complexity poses substantial challenges to explainability. While
existing methods like Kelpie use resource-intensive perturbation to explain KGE
models, our approach directly decodes the latent representations encoded by KGE
models, leveraging the smoothness of the embeddings, which follows the
principle that similar embeddings reflect similar behaviours within the
Knowledge Graph, meaning that nodes are similarly embedded because their graph
neighbourhood looks similar. This principle is commonly referred to as
smoothness. By identifying symbolic structures, in the form of triples, within
the subgraph neighborhoods of similarly embedded entities, our method
identifies the statistical regularities on which the models rely and translates
these insights into human-understandable symbolic rules and facts. This bridges
the gap between the abstract representations of KGE models and their predictive
outputs, offering clear, interpretable insights. Key contributions include a
novel post-hoc and local explainable AI method for KGE models that provides
immediate, faithful explanations without retraining, facilitating real-time
application on large-scale knowledge graphs. The method's flexibility enables
the generation of rule-based, instance-based, and analogy-based explanations,
meeting diverse user needs. Extensive evaluations show the effectiveness of our
approach in delivering faithful and well-localized explanations, enhancing the
transparency and trustworthiness of KGE models.

</details>


### [174] [Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate](https://arxiv.org/pdf/2502.12224)
*Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng*

Main category: cs.AI

TL;DR: Fate is an offloading system for MoE models, improving inference efficiency in edge scenarios by leveraging gate inputs for expert prefetching and a caching strategy.


<details>
  <summary>Details</summary>
Motivation: MoE models are underutilized in edge scenarios due to high memory demands and inaccurate expert predictions, which delay inference.

Method: Fate uses gate inputs from adjacent layers for expert prefetching, a shallow-favoring caching strategy, and tailored quantization for optimization.

Result: Fate achieves up to 4.5x and 1.9x speedups in prefill and decoding speeds, respectively, with a 99% expert hit rate.

Conclusion: Fate enables efficient MoE model inference in resource-constrained environments, with scalable performance improvements.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, and their application in edge scenarios has attracted
significant attention. However, sparse-activated Mixture-of-Experts (MoE)
models, which are well suited for edge scenarios, have received relatively
little attention due to their high memory demands. Offload-based methods have
been proposed to address this challenge, but they face difficulties with expert
prediction. Inaccurate expert predictions can result in prolonged inference
delays. To promote the application of MoE models in edge scenarios, we propose
Fate, an offloading system designed for MoE models to enable efficient
inference in resource-constrained environments. The key insight behind Fate is
that gate inputs from adjacent layers can be effectively used for expert
prefetching, achieving high prediction accuracy without additional GPU
overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy
that increases the expert hit rate to 99\%. Additionally, Fate integrates
tailored quantization strategies for cache optimization and IO efficiency.
Experimental results show that, compared to Load on Demand and Expert
Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in
prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,
while maintaining inference quality. Moreover, Fate's performance improvements
are scalable across different memory budgets.

</details>


### [175] [Generative AI in Transportation Planning: A Survey](https://arxiv.org/pdf/2503.07158)
*Longchao Da, Tiejin Chen, Zhuoheng Li, Shreyas Bachiraju, Huaiyuan Yao, Li Li, Yushun Dong, Xiyang Hu, Zhengzhong Tu, Dongjie Wang, Yue Zhao, Ben Zhou, Ram Pendyala, Benjamin Stabler, Yezhou Yang, Xuesong Zhou, Hua Wei*

Main category: cs.AI

TL;DR: A survey introduces a framework for using generative AI (GenAI) in transportation planning, categorizing applications by tasks and techniques, and addressing challenges like data scarcity and bias.


<details>
  <summary>Details</summary>
Motivation: The need for a systematic framework to guide GenAI adoption in transportation planning, bridging gaps between traditional methods and modern AI.

Method: Introduces a taxonomy categorizing GenAI applications into transportation tasks (descriptive, predictive, etc.) and computational techniques (data preparation, fine-tuning, etc.).

Result: Provides a comprehensive framework for GenAI in transportation, highlighting advancements and challenges like explainability and bias mitigation.

Conclusion: Aims to foster collaboration and innovation, ensuring ethical and impactful use of GenAI in transportation planning.

Abstract: The integration of generative artificial intelligence (GenAI) into
transportation planning has the potential to revolutionize tasks such as demand
forecasting, infrastructure design, policy evaluation, and traffic simulation.
However, there is a critical need for a systematic framework to guide the
adoption of GenAI in this interdisciplinary domain. In this survey, we, a
multidisciplinary team of researchers spanning computer science and
transportation engineering, present the first comprehensive framework for
leveraging GenAI in transportation planning. Specifically, we introduce a new
taxonomy that categorizes existing applications and methodologies into two
perspectives: transportation planning tasks and computational techniques. From
the transportation planning perspective, we examine the role of GenAI in
automating descriptive, predictive, generative, simulation, and explainable
tasks to enhance mobility systems. From the computational perspective, we
detail advancements in data preparation, domain-specific fine-tuning, and
inference strategies, such as retrieval-augmented generation and zero-shot
learning tailored to transportation applications. Additionally, we address
critical challenges, including data scarcity, explainability, bias mitigation,
and the development of domain-specific evaluation frameworks that align with
transportation goals like sustainability, equity, and system efficiency. This
survey aims to bridge the gap between traditional transportation planning
methodologies and modern AI techniques, fostering collaboration and innovation.
By addressing these challenges and opportunities, we seek to inspire future
research that ensures ethical, equitable, and impactful use of generative AI in
transportation planning.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [176] [Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment](https://arxiv.org/pdf/2505.04113)
*Xueyao Zhang, Yuancheng Wang, Chaoren Wang, Ziniu Li, Zhuo Chen, Zhizheng Wu*

Main category: cs.SD

TL;DR: The paper introduces the Intelligibility Preference Speech Dataset (INTP) and extends the Direct Preference Optimization (DPO) framework to improve zero-shot TTS systems, addressing challenges like tongue twisters and cross-lingual synthesis.


<details>
  <summary>Details</summary>
Motivation: Modern zero-shot TTS systems struggle with intelligibility in challenging scenarios, prompting the need for targeted improvements.

Method: Leverages preference alignment techniques and introduces INTP, extending DPO for diverse TTS architectures.

Result: INTP alignment improves intelligibility, naturalness, similarity, and audio quality across multiple TTS models, with weak-to-strong generalization demonstrated.

Conclusion: The approach shows promise for further iterative improvements, as validated by models like CosyVoice 2 and Ints.

Abstract: Modern zero-shot text-to-speech (TTS) systems, despite using extensive
pre-training, often struggle in challenging scenarios such as tongue twisters,
repeated words, code-switching, and cross-lingual synthesis, leading to
intelligibility issues. To address these limitations, this paper leverages
preference alignment techniques, which enable targeted construction of
out-of-pretraining-distribution data to enhance performance. We introduce a new
dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend
the Direct Preference Optimization (DPO) framework to accommodate diverse TTS
architectures. After INTP alignment, in addition to intelligibility, we observe
overall improvements including naturalness, similarity, and audio quality for
multiple TTS models across diverse domains. Based on that, we also verify the
weak-to-strong generalization ability of INTP for more intelligible models such
as CosyVoice 2 and Ints. Moreover, we showcase the potential for further
improvements through iterative alignment based on Ints. Audio samples are
available at https://intalign.github.io/.

</details>


### [177] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/pdf/2505.04451)
*Yohannis Telila, Tommaso Cucinotta, Davide Bacciu*

Main category: cs.SD

TL;DR: A method for automatic music transcription (AMT) using constant-Q transform and CNN to convert piano audio into a music score.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of transcribing polyphonic music, particularly classical piano, into a score representation.

Method: Uses constant-Q transform for feature extraction and a CNN model to process the audio signals.

Result: Transforms .wav piano audio files into a music score representation.

Conclusion: The proposed pipeline effectively converts piano audio into a score, demonstrating the potential of CNN in AMT.

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>


### [178] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/pdf/2505.04457)
*Shigeki Karita, Yuma Koizumi, Heiga Zen, Haruko Ishikawa, Robin Scheibler, Michiel Bacchiani*

Main category: cs.SD

TL;DR: Miipher-2 is a speech restoration model for cleaning training data in large-scale generative models, addressing challenges like generalization, efficiency, and no explicit conditioning. It uses a frozen USM for feature extraction and achieves high performance across languages.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and scalable cleaning of training data for large generative models, especially for unseen languages and without explicit conditioning.

Method: Utilizes a frozen USM for feature extraction, parallel adapters for clean feature prediction, and WaneFit vocoder for synthesis. Trained on multi-lingual data with degradations.

Result: Outperforms or matches conventional SR models in word-error-rate, speaker similarity, and sound quality. Efficiently processes million-hour datasets.

Conclusion: Miipher-2 is a scalable, efficient solution for speech data cleaning, suitable for large-scale applications.

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


### [179] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/pdf/2505.04621)
*Jessie Richter-Powell, Antonio Torralba, Jonathan Lorraine*

Main category: cs.SD

TL;DR: Audio-SDS extends Score Distillation Sampling (SDS) to audio diffusion models, enabling diverse tasks like impact sound simulation and source separation without specialized datasets.


<details>
  <summary>Details</summary>
Motivation: To generalize SDS, originally for text-to-3D generation, to audio tasks, leveraging generative priors for versatility.

Method: Uses a pretrained audio diffusion model with SDS to guide tasks like sound simulation, FM-synthesis calibration, and source separation.

Result: Demonstrates successful application across tasks, showing the method's adaptability.

Conclusion: Audio-SDS provides a versatile foundation for future generative prior-based audio tasks.

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>


### [180] [JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models](https://arxiv.org/pdf/2308.04729)
*Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, Alex Wang*

Main category: cs.SD

TL;DR: JEN-1 is a diffusion model for text-to-music generation, combining autoregressive and non-autoregressive training, outperforming state-of-the-art methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Text-to-music generation is challenging due to musical complexity and high sampling rates, with existing models lacking in quality and efficiency.

Method: JEN-1 uses a diffusion model with autoregressive and non-autoregressive training, enabling tasks like text-guided generation, inpainting, and continuation.

Result: JEN-1 achieves superior text-music alignment and music quality while remaining computationally efficient.

Conclusion: JEN-1 is a high-fidelity, versatile model for text-to-music generation, setting a new benchmark in the field.

Abstract: Music generation has attracted growing interest with the advancement of deep
generative models. However, generating music conditioned on textual
descriptions, known as text-to-music, remains challenging due to the complexity
of musical structures and high sampling rate requirements. Despite the task's
significance, prevailing generative models exhibit limitations in music
quality, computational efficiency, and generalization. This paper introduces
JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a
diffusion model incorporating both autoregressive and non-autoregressive
training. Through in-context learning, JEN-1 performs various generation tasks
including text-guided music generation, music inpainting, and continuation.
Evaluations demonstrate JEN-1's superior performance over state-of-the-art
methods in text-music alignment and music quality while maintaining
computational efficiency. Our demos are available at
https://jenmusic.ai/audio-demos

</details>


### [181] [Diverse Audio Embeddings -- Bringing Features Back Outperforms CLAP!](https://arxiv.org/pdf/2309.08751)
*Prateek Verma*

Main category: cs.SD

TL;DR: The paper explores combining domain-specific audio embeddings (e.g., pitch, timbre) with end-to-end learning to improve audio classification performance.


<details>
  <summary>Details</summary>
Motivation: To enhance audio classification by integrating domain-specific knowledge (e.g., pitch, timbre) with end-to-end models, surpassing the limitations of purely end-to-end approaches.

Method: Learns separate embeddings for diverse audio properties (pitch, timbre, neural representation) alongside an end-to-end architecture. Combines these embeddings for classification.

Result: Combining domain-specific embeddings with end-to-end learning significantly improves performance over using either approach alone.

Conclusion: Integrating domain expertise with end-to-end models yields robust, diverse representations, outperforming purely end-to-end methods.

Abstract: With the advent of modern AI architectures, a shift has happened towards
end-to-end architectures. This pivot has led to neural architectures being
trained without domain-specific biases/knowledge, optimized according to the
task. We in this paper, learn audio embeddings via diverse feature
representations, in this case, domain-specific. For the case of audio
classification over hundreds of categories of sound, we learn robust separate
embeddings for diverse audio properties such as pitch, timbre, and neural
representation, along with also learning it via an end-to-end architecture. We
observe handcrafted embeddings, e.g., pitch and timbre-based, although on their
own, are not able to beat a fully end-to-end representation, yet adding these
together with end-to-end embedding helps us, significantly improve performance.
This work would pave the way to bring some domain expertise with end-to-end
models to learn robust, diverse representations, surpassing the performance of
just training end-to-end models.

</details>


### [182] [Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets](https://arxiv.org/pdf/2503.22712)
*Zijun Jia, Jinsong Yu, Hongyu Long, Diyin Tang*

Main category: cs.SD

TL;DR: A risk-controlled prediction framework for speech emotion recognition (SER) is proposed to mitigate road rage by ensuring statistically rigorous guarantees on prediction accuracy, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Road rage, caused by emotional suppression and outbursts, threatens road safety. Current SER methods lack reliability and calibration for safety-critical applications.

Method: A novel framework uses a calibration set and a binary loss function to optimize prediction accuracy, controlled by a user-specified risk level. It includes extensions for small-batch online calibration and non-exchangeable environments.

Result: The framework consistently achieves a minimum coverage of 1 - Î± across models and datasets, maintaining robustness under varying conditions.

Conclusion: The proposed method provides reliable statistical guarantees for SER in dynamic environments, enhancing road safety by early emotion detection.

Abstract: Road rage, often triggered by emotional suppression and sudden outbursts,
significantly threatens road safety by causing collisions and aggressive
behavior. Speech emotion recognition technologies can mitigate this risk by
identifying negative emotions early and issuing timely alerts. However, current
SER methods, such as those based on hidden markov models and Long short-term
memory networks, primarily handle one-dimensional signals, frequently
experience overfitting, and lack calibration, limiting their safety-critical
effectiveness. We propose a novel risk-controlled prediction framework
providing statistically rigorous guarantees on prediction accuracy. This
approach employs a calibration set to define a binary loss function indicating
whether the true label is included in the prediction set. Using a data-driven
threshold $\beta$, we optimize a joint loss function to maintain an expected
test loss bounded by a user-specified risk level $\alpha$. Evaluations across
six baseline models and two benchmark datasets demonstrate our framework
consistently achieves a minimum coverage of $1 - \alpha$, effectively
controlling marginal error rates despite varying calibration-test split ratios
(e.g., 0.1). The robustness and generalizability of the framework are further
validated through an extension to small-batch online calibration under a local
exchangeability assumption. We construct a non-negative test martingale to
maintain prediction validity even in dynamic and non-exchangeable environments.
Cross-dataset tests confirm our method's ability to uphold reliable statistical
guarantees in realistic, evolving data scenarios.

</details>


### [183] [Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network](https://arxiv.org/pdf/2505.01880)
*Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo*

Main category: cs.SD

TL;DR: LOCO is a progressive audio-language co-learning network for audio temporal forgery localization (ATFL) under weak supervision, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing ATFL methods require costly fine-grained annotations, which are impractical in real-world scenarios. LOCO addresses this by leveraging weak supervision and self-supervision.

Method: LOCO uses an audio-language co-learning module to align semantics and forgery-aware prompts, a forgery localization module for proposals, and progressive refinement with pseudo labels and contrastive learning.

Result: LOCO achieves state-of-the-art performance on three public benchmarks.

Conclusion: LOCO effectively localizes audio forgeries under weak supervision, outperforming existing methods.

Abstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery
regions of the partial spoof audio that is purposefully modified. Existing ATFL
methods rely on training efficient networks using fine-grained annotations,
which are obtained costly and challenging in real-world scenarios. To meet this
challenge, in this paper, we propose a progressive audio-language co-learning
network (LOCO) that adopts co-learning and self-supervision manners to prompt
localization performance under weak supervision scenarios. Specifically, an
audio-language co-learning module is first designed to capture forgery
consensus features by aligning semantics from temporal and global perspectives.
In this module, forgery-aware prompts are constructed by using utterance-level
annotations together with learnable prompts, which can incorporate semantic
priors into temporal content features dynamically. In addition, a forgery
localization module is applied to produce forgery proposals based on fused
forgery-class activation sequences. Finally, a progressive refinement strategy
is introduced to generate pseudo frame-level labels and leverage supervised
semantic contrastive learning to amplify the semantic distinction between real
and fake content, thereby continuously optimizing forgery-aware features.
Extensive experiments show that the proposed LOCO achieves SOTA performance on
three public benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [184] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/pdf/2505.03774)
*Tao Yin, Chen Zhao, Xiaoyan Liu, Minglai Shao*

Main category: cs.LG

TL;DR: A novel method (OODHG) for detecting out-of-distribution (OOD) nodes in heterogeneous graphs, leveraging meta-path-based energy propagation and achieving superior performance in OOD detection and ID node classification.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs are often heterogeneous and involve distribution shifts, making OOD detection challenging. Existing research focuses on homogeneous graphs, leaving heterogeneous graphs underexplored.

Method: Proposes OODHG: learns node representations, calculates energy values for OOD detection, and classifies ID nodes. Uses meta-path-based energy propagation and energy constraints.

Result: OODHG outperforms baselines in OOD detection and accurately classifies ID nodes.

Conclusion: OODHG is simple, effective, and superior for OOD detection in heterogeneous graphs.

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>


### [185] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/pdf/2505.03775)
*Linqing Chen, Weilei Wang, Wentao Wu, Hanmeng Zhong*

Main category: cs.LG

TL;DR: The paper redefines hierarchical multi-label classification as Hierarchical Multi-Label Generation (HMG), using a generative framework with Probabilistic Level Constraints (PLC) to improve performance and control over model outputs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of hierarchical label relationships and large label counts in extreme multi-label classification, previous methods lacked precision in generative model outputs.

Method: Proposes a generative framework with PLC to generate hierarchical labels without preliminary steps like clustering, ensuring precise control over output count, length, and level.

Result: Achieves state-of-the-art performance in HMG and significantly better control over model outputs compared to prior work.

Conclusion: The proposed HMG framework with PLC effectively handles hierarchical label generation and outperforms existing methods in both performance and output control.

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>


### [186] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/pdf/2505.03776)
*Hansi Denis, Siegfried Mercelis, Ngoc-Quang Luong*

Main category: cs.LG

TL;DR: The paper introduces a novel Proximity Attention mechanism (PAPN) for route prediction in last-mile delivery and first-mile pickup, combining local and global attention to outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Optimizing parcel delivery and pickup requires accurate route prediction to enhance cost, resource efficiency, and service quality.

Method: Uses a Proximity Attention Encoder and Pointer Network decoder (PAPN) with local and global attention, trained on the LaDE dataset.

Result: Outperforms state-of-the-art supervised systems and competes with the best reinforcement learning method (DRL4Route).

Conclusion: PAPN demonstrates promise for route prediction in logistics, leveraging attention mechanisms for improved performance.

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>


### [187] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/pdf/2505.03777)
*LG AI Research, Sehyun Chun, Jiye Kim, Ahra Jo, Yeonsik Jo, Seungyul Oh, Seungjun Lee, Kwangrok Ryoo, Jongmin Lee, Seunghwan Kim, Byung Jun Kang, Soonyoung Lee, Jun Ha Park, Chanwoo Moon, Jiwon Ham, Haein Lee, Heejae Han, Jaeseung Byun, Soojong Do, Minju Ha, Dongyun Kim, Kyunghoon Bae, Woohyung Lim, Edward Hwayoung Lee, Yongmin Park, Jeongsang Yu, Gerrard Jeongwon Jo, Yeonjung Hong, Kyungjae Yoo, Sehui Han, Jaewan Lee, Changyoung Park, Kijeong Jeon, Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole is a vision-based deep learning framework for extracting chemical data from documents, outperforming existing tools and introducing a new benchmark and evaluation metric.


<details>
  <summary>Details</summary>
Motivation: The challenge of extracting molecular structures and reaction data from unstructured chemical formats and complex document layouts.

Method: MolMole unifies molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline.

Result: MolMole outperforms existing toolkits on both the introduced benchmark and public datasets.

Conclusion: The benchmark testset will be public, and MolMole will be accessible via an interactive demo, with commercial inquiries directed to LG AI Research.

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [188] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/pdf/2505.03778)
*Jonathan Viquerat, Paul Garnier, Amirhossein Bateni, Elie Hachem*

Main category: cs.LG

TL;DR: Dragonfly is a modular deep reinforcement learning library for easy experimentation, featuring JSON serialization and CPU-intensive environment support, with competitive benchmark performance.


<details>
  <summary>Details</summary>
Motivation: To simplify experimentation and development in deep reinforcement learning by providing a modular and maintainable framework.

Method: Uses JSON serialization for swapping building blocks and parameter sweeps, with features optimized for CPU-intensive tasks like numerical simulations.

Result: Performs favorably compared to standard agents in common benchmarks.

Conclusion: Dragonfly is an effective tool for modular and efficient deep reinforcement learning research and development.

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>


### [189] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/pdf/2505.04066)
*Tuochao Chen, Nicholas Batchelder, Alisa Liu, Noah Smith, Shyamnath Gollakota*

Main category: cs.LG

TL;DR: LlamaPIE is a real-time proactive assistant for enhancing human conversations via hearable devices, operating discreetly without explicit user invocation. It uses a two-model pipeline for context-aware assistance and is evaluated as effective and preferred over baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance human conversations by providing discreet, proactive assistance without interrupting the flow, addressing challenges like timing, conciseness, and real-time processing.

Method: A two-model pipeline: a small model decides when to respond, and a larger model generates the response. Built on a semi-synthetic dialogue dataset and implemented on Apple Silicon M2 hardware.

Result: User studies show strong preference for LlamaPIE over no assistance and reactive models, proving its effectiveness in live conversations.

Conclusion: LlamaPIE demonstrates the potential of proactive, unobtrusive AI assistance in enhancing real-time human interactions.

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>


### [190] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/pdf/2505.03779)
*Tao Liu, Tianyu Zhang, Yongxue Chen, Weiming Wang, Yu Jiang, Yuming Huang, Charlie C. L. Wang*

Main category: cs.LG

TL;DR: A neural network framework optimizes structural topology, curved layers, and fiber orientations in composites for strength and manufacturability, achieving 33.1% higher failure loads.


<details>
  <summary>Details</summary>
Motivation: To enhance anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability through an integrated optimization approach.

Method: Uses three implicit neural fields for shape, layer sequence, and fiber orientation, integrating design and manufacturability objectives into a differentiable optimization process.

Result: Composites show up to 33.1% improvement in failure loads compared to sequentially optimized designs.

Conclusion: The framework successfully co-optimizes mechanical strength and manufacturability, validated by physical experiments.

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>


### [191] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/pdf/2505.03781)
*Jin Yu, JaeHo Park, TaeJun Park, Gyurin Kim, JiHyun Lee, Min Sung Lee, Joon-myoung Kwon, Jeong Min Son, Yong-Yeon Jo*

Main category: cs.LG

TL;DR: A Zero-shot ECG diagnosis framework using RAG and expert-curated knowledge improves accuracy and explainability in ECG analysis, validated on the PTB-XL dataset.


<details>
  <summary>Details</summary>
Motivation: Generating reliable, evidence-based results in healthcare using RAG alone is challenging, necessitating expert-curated knowledge for accuracy.

Method: Proposes a Zero-shot ECG diagnosis framework combining RAG with expert-curated knowledge for ECG analysis.

Result: Effective performance on the PTB-XL dataset, demonstrating enhanced diagnostic accuracy and explainability.

Conclusion: The framework supports comprehensive ECG analysis and has potential applications beyond the tested dataset.

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>


### [192] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/pdf/2505.03783)
*Tian Chen, Shengping Liu, Li Liu, Heng Yong*

Main category: cs.LG

TL;DR: A novel Series-Parallel Multi-Network Architecture using PINNs is proposed to model closure terms in sparse data scenarios, enhancing generalizability for engineering applications.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of closure terms is challenging with sparse or incomplete data, necessitating widely applicable models.

Method: The approach integrates Physics-Informed Neural Networks (PINNs) with dedicated subnetworks to independently model unknown closure terms, incorporating physical constraints and heterogeneous data.

Result: The closure models are integrated into a PDE solver, enabling robust solutions for complex predictive simulations.

Conclusion: The proposed architecture improves generalizability and accuracy in modeling closure terms for engineering problems.

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>


### [193] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/pdf/2505.04161)
*Baida Zhang, Yakai Chen, Huichun Li, Zhenghu Zu*

Main category: cs.LG

TL;DR: A reinforcement learning framework is developed for optimizing intervention measures in infectious diseases using an individual agent-based model, validated through experiments and theory.


<details>
  <summary>Details</summary>
Motivation: The challenge of devising effective intervention measures during epidemics, with existing research limited by simplifications in disease models.

Method: A decision-making framework based on an individual agent-based transmission model (modified Covasim) using reinforcement learning to develop strategy functions, tested with multiple algorithms.

Result: The framework effectively suppresses epidemic expansion and maintains economic stability, validated experimentally and theoretically.

Conclusion: The study provides a robust methodological framework for public health strategies, addressing the complexity of disease transmission.

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>


### [194] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/pdf/2505.03784)
*Ahmed A. Metwally, A. Ali Heydari, Daniel McDuff, Alexandru Solot, Zeinab Esmaeilpour, Anthony Z Faranesh, Menglian Zhou, David B. Savage, Conor Heneghan, Shwetak Patel, Cathy Speed, Javier L. Prieto*

Main category: cs.LG

TL;DR: Deep neural networks predict insulin resistance using wearable and blood biomarker data, outperforming single-source methods, with high accuracy in vulnerable populations.


<details>
  <summary>Details</summary>
Motivation: Current methods for measuring insulin resistance are costly and inaccessible, limiting early intervention opportunities.

Method: Developed deep neural network models combining wearable device data and blood biomarkers to predict insulin resistance.

Result: Models achieved R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%, with higher accuracy in obese/sedentary participants.

Conclusion: This approach enables early detection of type 2 diabetes risk, facilitating preventative strategies.

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>


### [195] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/pdf/2505.04196)
*Sung Yoo Lim, Hyunsoo Yun, Prateek Bansal, Dong-Kyu Kim, Eui-Jin Kim*

Main category: cs.LG

TL;DR: A hybrid LLM-BN method improves synthetic population generation, achieving 95% feasibility and comparable diversity, outperforming traditional DGMs and proprietary LLMs.


<details>
  <summary>Details</summary>
Motivation: Ensuring valid downstream activity simulations in ABMs requires feasible and diverse synthetic populations, which existing DGMs struggle to balance.

Method: Fine-tuning LLMs with topological orderings from a Bayesian Network to control autoregressive generation.

Result: 95% feasibility (vs. ~80% in DGMs) with maintained diversity, scalable on standard hardware.

Conclusion: The lightweight, open-source LLM-BN approach enhances ABM reliability and reduces error propagation.

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>


### [196] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/pdf/2505.03785)
*Eleftherios Tzanis, Michail E. Klontzas*

Main category: cs.LG

TL;DR: mAIstro is an open-source, autonomous multi-agent framework for developing and deploying medical AI models without coding, using LLMs.


<details>
  <summary>Details</summary>
Motivation: To automate complex healthcare AI workflows and unify data analysis, model development, and inference.

Method: Modular architecture with natural language interface for tasks like data analysis, feature extraction, and model validation.

Result: Successfully executed tasks across 16 datasets, producing interpretable outputs and validated models.

Conclusion: mAIstro provides a reproducible, extensible foundation for clinical and research AI integration.

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>


### [197] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/pdf/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TL;DR: The study benchmarks a reasoning LLM (DeepSeek-R1) against non-reasoning LLMs in a text-to-SQL task, showing superior discrimination performance but limitations in generation.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of reasoning LLMs as discriminators compared to non-reasoning LLMs in planning frameworks.

Method: Benchmarked DeepSeek-R1 against non-reasoning LLMs using a novel method to extract soft scores from CoT outputs for fine-grained ranking.

Result: DeepSeek-R1 outperformed non-reasoning LLMs in discrimination but showed limitations in generation and logical capabilities.

Conclusion: Reasoning models excel as discriminators in LLM planning frameworks but underperform as generators, suggesting their optimal role is in discrimination.

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>


### [198] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/pdf/2505.03787)
*Zuraiz Baig, Sidra Nasir, Rizwan Ahmed Khan, Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TL;DR: Proposes lightweight 1D CNNs (ArrhythmiNet V1/V2) for efficient, real-time arrhythmia classification on edge devices, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for accurate, timely arrhythmia detection, overcoming limitations of manual ECG interpretation and existing deep learning models.

Method: Introduces two lightweight 1D CNNs inspired by MobileNet, optimized for edge devices, with Shapley and Grad-CAM for interpretability.

Result: Achieves 0.99 (V1) and 0.98 (V2) accuracy on MIT-BIH dataset, with small memory footprints (302.18 KB and 157.76 KB).

Conclusion: Demonstrates feasibility of interpretable, accurate, and efficient ECG monitoring systems for practical use.

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>


### [199] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/pdf/2505.04318)
*Jacob Glenn Ayers, Buvaneswari A. Ramanan, Manzoor A. Khan*

Main category: cs.LG

TL;DR: The paper proposes using the ÏÂ² Goodness of Fit Test as a drift detection meta-algorithm for neural networks to identify concept drift without examining inference outputs directly.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often face unreliable inference due to concept drift, and existing drift detection methods are underutilized. A versatile approach is needed to monitor models across diverse scenarios.

Method: The ÏÂ² Goodness of Fit Test is applied as a drift detection meta-algorithm on a multilayer perceptron, CNN, and transformer under simulated drift conditions.

Result: The method detects accuracy drops caused by concept drift without direct output analysis, enhancing model reliability.

Conclusion: The approach ensures continual evaluation of model reliability under varying conditions, improving safety in deep learning applications.

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>


### [200] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/pdf/2505.03789)
*Syoiti Ninomiya, Yuming Ma*

Main category: cs.LG

TL;DR: A new deep-learning architecture using high-order weak approximation for SDEs improves martingale learning and financial derivative pricing.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of learning martingales and pricing financial derivatives using deep learning.

Method: High-order weak approximation algorithms of explicit Runge-Kutta type, applied through iterative compositions and linear combinations of SDE vector fields.

Result: The architecture enables efficient learning of martingales and improves derivative pricing.

Conclusion: The proposed architecture successfully integrates high-order weak approximation with deep learning for financial applications.

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>


### [201] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/pdf/2505.03790)
*Yuren Zhang, Zhongnan Pu, Lei Jing*

Main category: cs.LG

TL;DR: A method combining Diffusion and Transformer models is proposed to generate high-quality augmented time-series data, addressing the lack of focus in this domain.


<details>
  <summary>Details</summary>
Motivation: Deep learning requires large datasets, but time-series data augmentation is underdeveloped compared to domains like images and speech.

Method: Uses a diffusion denoising model to generate initial time-step data, a Transformer for predicting subsequent actions, and a weighted loss function for convergence.

Result: The method produces high-quality augmented data, improving model performance compared to no augmentation or traditional methods.

Conclusion: The proposed approach effectively addresses the gap in time-series data augmentation, demonstrating its practical utility.

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>


### [202] [Practical Boolean Backpropagation](https://arxiv.org/pdf/2505.03791)
*Simon Golbert*

Main category: cs.LG

TL;DR: A method for purely Boolean backpropagation in neural networks, avoiding numerics, is introduced and tested.


<details>
  <summary>Details</summary>
Motivation: Boolean neural networks are hardware-efficient but lack exploration in purely Boolean training methods.

Method: Proposes a purely Boolean backpropagation method using a specific gate, operating in Boolean algebra without numerics.

Result: Initial experiments confirm the feasibility of the method.

Conclusion: The method shows promise for practical Boolean neural network training.

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>


### [203] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/pdf/2505.03792)
*Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, Bo An*

Main category: cs.LG

TL;DR: CoSo, a novel online fine-tuning method for VLM agents, improves RL exploration efficiency by dynamically assessing token influence using counterfactual reasoning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in online RL for VLM agents, such as open-ended textual action spaces and inefficient exploration.

Method: Proposes Counterfactual Soft Reinforcement Learning (CoSo), which prioritizes exploration of action-critical tokens while minimizing redundant ones.

Result: Theoretical and empirical results show CoSo enhances exploration efficiency and performance across diverse tasks.

Conclusion: CoSo is effective for fine-tuning VLM agents, offering improved exploration and performance in dynamic environments.

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [204] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/pdf/2505.03793)
*Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou*

Main category: cs.LG

TL;DR: A novel theoretical framework, LENSLLM, is proposed to model LLM fine-tuning dynamics for efficient and accurate model selection, achieving high accuracy and computational savings.


<details>
  <summary>Details</summary>
Motivation: The need for efficient LLM selection due to computational constraints and the lack of understanding of LLM fine-tuning dynamics.

Method: Derived a Hessian-based PAC-Bayes generalization bound and introduced LENSLLM, an NTK-based Rectified Scaling Model.

Result: Achieved up to 91.1% accuracy and reduced computational cost by 88.5%, outperforming 5 state-of-the-art methods.

Conclusion: LENSLLM provides an effective solution for LLM selection, with open-sourced implementation available.

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>


### [205] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/pdf/2505.03794)
*Ä°rfan IÅik, Ibrahim Karahan, Okan Erkaymaz*

Main category: cs.LG

TL;DR: An improved forward-backward splitting algorithm with two inertial parameters is proposed, showing weak convergence and superior performance in regression and classification tasks.


<details>
  <summary>Details</summary>
Motivation: To find a point in a real Hilbert space where the sum of a co-coercive operator and a maximal monotone operator vanishes, improving upon existing methods.

Method: The paper introduces a forward-backward splitting algorithm enhanced with two inertial parameters, tested under standard assumptions.

Result: The algorithm demonstrates weak convergence and outperforms existing methods in regression and data classification experiments.

Conclusion: The proposed algorithm is effective and superior to current alternatives for solving the targeted problem.

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>


### [206] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/pdf/2505.03797)
*Andrew Millard, Joshua Murphy, Simon Maskell, Zheng Zhao*

Main category: cs.LG

TL;DR: A new SMC-based training method for pBNNs improves scalability and outperforms state-of-the-art in predictive performance and optimal loss.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and scalability of partial Bayesian neural networks (pBNNs) using sequential Monte Carlo (SMC) samplers.

Method: Introduces a guided proposal and gradient-based Markov kernels for SMC-based training of pBNNs.

Result: Outperforms state-of-the-art in predictive performance and optimal loss, with better scalability and reduced training times.

Conclusion: The new SMC-based method for pBNNs is highly effective, scalable, and efficient, making it a competitive alternative to fully Bayesian approaches.

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>


### [207] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/pdf/2505.03798)
*Yiqing Shen, Hao Ding, Lalithkumar Seenivasan, Tianmin Shu, Mathias Unberath*

Main category: cs.LG

TL;DR: The paper proposes using digital twin (DT) representations instead of token representations in foundation models (FMs) to better capture real-world knowledge and address limitations like semantic coherence and causal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current FMs use token representations that fragment real-world data, limiting their ability to leverage domain knowledge and perform tasks like causal reasoning.

Method: The paper suggests adopting DT representations, which are outcome-driven digital replicas of physical processes, to encode domain knowledge and preserve continuous data.

Result: DT representations could overcome FM limitations by providing physically grounded, continuous representations.

Conclusion: The machine learning community should explore DT representations as an alternative to tokens for building more effective FMs.

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>


### [208] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/pdf/2505.03799)
*Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra*

Main category: cs.LG

TL;DR: Proposes SDM-InstructGLM, an instruction-tuned Graph Language Model for scalable graph processing without GNNs, using similarity-degree-based biased random walks.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of LLMs in graph tasks due to scalability and lack of graph-specific mechanisms, avoiding reliance on GNNs.

Method: Introduces a similarity-degree-based biased random walk to selectively sample and encode graph information, improving token efficiency.

Result: Enhances performance in node classification and link prediction, demonstrating scalable LLM-only graph processing.

Conclusion: Paves the way for GNN-free graph learning, enabling standalone LLM-based graph reasoning.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>


### [209] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/pdf/2505.03801)
*Changhai Zhou, Qian Qiao, Weizhong Zhang, Cheng Jin*

Main category: cs.LG

TL;DR: A two-stage LLM compression method is proposed, addressing challenges in low-rank and sparse composite approximation by optimizing global rank and sparsity while managing layer redundancy and component interaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for compressing LLMs struggle with interaction between low-rank and sparse matrices and uneven weight allocation across layers due to varying redundancy.

Method: The method uses robust PCA for decomposition in the first stage and probabilistic global optimization in the second stage to jointly identify low-rank and sparse structures.

Result: The approach outperforms state-of-the-art techniques in sparsification and composite approximation.

Conclusion: The proposed method effectively addresses key challenges in LLM compression, offering superior performance by optimizing global rank and sparsity.

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>


### [210] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/pdf/2505.03810)
*Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo*

Main category: cs.LG

TL;DR: A novel training-free method improves LLM quantization at low bit-widths using Walsh-Hadamard transform and grouped rotation, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current rotation-based PTQ methods for LLMs at very low bit-widths (e.g., 2-bit).

Method: Leverages Walsh-Hadamard transform with sequency ordering and proposes Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices.

Result: Significantly reduces quantization error, improves reasoning tasks, and enhances WikiText-2 PPL scores.

Conclusion: The method offers a robust, training-free solution for low-bit LLM quantization, even enhancing learned rotation techniques.

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>


### [211] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/pdf/2505.03802)
*Changhai Zhou, Yuhua Zhou, Qian Qiao, Weizhong Zhang, Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA combines low-bit quantization and LoRA for memory-efficient LLM fine-tuning. QR-Adaptor, a gradient-free method, jointly optimizes quantization and low-rank subspaces, outperforming SOTA methods by 4.89% on GSM8K.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to consistently improve performance due to separate optimization of quantization and low-rank subspaces, ignoring their synergy.

Method: Proposes QR-Adaptor, a unified strategy using partial calibration data to jointly search quantization components and low-rank subspaces, treating it as a discrete optimization problem.

Result: Achieves 4.89% accuracy improvement on GSM8K, sometimes outperforming 16-bit models while maintaining 4-bit memory usage.

Conclusion: QR-Adaptor effectively balances performance and memory efficiency, advancing quantized LLM fine-tuning.

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [212] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/pdf/2505.03803)
*Chen Xu, Yuxuan Yue, Zukang Xu, Xing Hu, Jiangyong Yu, Zhixuan Chen, Sifan Zhou, Zhihang Yuan, Dawei Yang*

Main category: cs.LG

TL;DR: RWKVQuant is a PTQ framework for RWKV models, addressing quantization challenges with adaptive techniques, achieving 3-bit quantization with minimal accuracy loss and speedup.


<details>
  <summary>Details</summary>
Motivation: RWKV, despite its Transformer-like performance, struggles with PTQ due to non-linear operators and uniform weight distribution, leading to performance degradation.

Method: RWKVQuant introduces a coarse-to-fine proxy for adaptive quantization and a codebook optimization algorithm for cluster-based quantization.

Result: Quantizes RWKV-6-14B to ~3-bit with <1% accuracy loss and 2.14x speedup.

Conclusion: RWKVQuant effectively addresses RWKV's PTQ challenges, enabling efficient deployment on resource-constrained devices.

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>


### [213] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/pdf/2505.03804)
*Xing Hu, Zhixuan Chen, Dawei Yang, Zukang Xu, Chen Xu, Zhihang Yuan, Sifan Zhou, Jiangyong Yu*

Main category: cs.LG

TL;DR: MoEQuant, a novel quantization framework, addresses memory overheads in MoE LLMs by tackling inter-expert and intra-expert imbalance challenges, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: MoE LLMs face memory overheads and accuracy degradation with standard PTQ due to sparse and dynamic routing, limiting practical deployment.

Method: Proposes MoEQuant with Expert-Balanced Self-Sampling (EBSS) for balanced calibration and Affinity-Guided Quantization (AGQ) to incorporate sample-expert affinities.

Result: MoEQuant achieves over 10 points accuracy gain in HumanEval for DeepSeekMoE-16B under 4-bit quantization, improving efficiency.

Conclusion: MoEQuant effectively addresses quantization challenges in MoE LLMs, enhancing performance and scalability.

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>


### [214] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/pdf/2505.03997)
*Prudhviraj Naidu, Zixian Wang, Leon Bergen, Ramamohan Paturi*

Main category: cs.LG

TL;DR: Transformer models show abrupt loss drops during training on algorithmic tasks, revealing hidden feature learning phases.


<details>
  <summary>Details</summary>
Motivation: To understand why Transformer models exhibit sudden performance improvements despite stagnant loss trends.

Method: Train Transformers on algorithmic tasks, analyze loss curves, and probe internal representations to identify feature learning phases.

Result: Models learn quiet features during stagnation, then acquire loud features coinciding with sharp loss drops. Ablation confirms causal role of features.

Conclusion: Next-token loss may not track incremental progress; key features develop covertly before triggering rapid gains.

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>


### [215] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/pdf/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TL;DR: Generalizing Randomized Uphill Climbing (RUC) into a model-agnostic feature optimization framework for multivariate time series forecasting, focusing on efficiency, interpretability, and societal impact.


<details>
  <summary>Details</summary>
Motivation: To improve forecasting tools by decoupling feature discovery from resource-intensive deep learning, enabling faster, more transparent, and energy-efficient solutions for resource-constrained institutions.

Method: Synthesizes feature programs via random operator composition, scores them with surrogate models on rolling windows, and filters instability using nested cross-validation and information-theoretic shrinkage.

Result: A lightweight framework promising faster iteration, lower energy use, and greater interpretability compared to GPU-heavy deep learning.

Conclusion: The approach offers a practical, transparent alternative to black-box models, benefiting institutions like energy regulators and climate risk NGOs.

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>


### [216] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.03806)
*Mehran Mazandarani, Marzieh Najariyan*

Main category: cs.LG

TL;DR: PrINNs integrate perception-based info into neural networks, extending PINNs to handle diverse perception forms and expert knowledge, enabling data-driven modeling in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: To bridge physics-based modeling and data-driven approaches by incorporating perception-based info and expert knowledge into neural networks.

Method: Uses loss functions to integrate perception precisiation (e.g., probability, fuzzy logic) and expert knowledge, introducing variants like MOEINNs, TKINNs, and FINNs.

Result: Enables modeling of dynamical systems, uncertain environments, and discovery of new differential equations without defuzzification.

Conclusion: PrINNs advance computational science by combining physics and perception, offering flexible and powerful modeling tools.

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>


### [217] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/pdf/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: A high-performing method for detecting harmful algal blooms using remote sensing data and AI models, combining tree-based and neural network approaches for robust classification.


<details>
  <summary>Details</summary>
Motivation: The growing threat of harmful algal blooms to water quality and public health necessitates efficient, accurate, and cost-effective detection methods.

Method: Integration of open-source remote sensing data (Sentinel-2, DEM, NOAA HRRR) with AI models (tree-based and neural network) into an ensemble for classifying algal bloom severity.

Result: The ensemble approach, combining tree models and a neural network, improved robustness and demonstrated effective use of diverse remote sensing inputs.

Conclusion: The method, initially developed for a U.S. competition, shows global potential and is openly available for adaptation, highlighting the synergy of remote sensing and AI for environmental challenges.

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [218] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/pdf/2505.03809)
*Suorong Yang, Peng Ye, Furao Shen, Dongzhan Zhou*

Main category: cs.LG

TL;DR: A novel framework unifies dynamic data selection and augmentation to improve training efficiency and model performance without sacrificing generalization.


<details>
  <summary>Details</summary>
Motivation: Dynamic data selection alone limits diversity, while combining it with augmentation lacks optimization. The goal is to exploit their synergies for better efficiency and performance.

Method: Proposes an online training framework that jointly estimates sample density and semantic consistency to select augmentation-suitable samples and exclude noisy data.

Result: Outperforms state-of-the-art methods, reducing training costs by 50% on ImageNet-1k with no performance loss, while enhancing noise resistance and robustness.

Conclusion: The unified approach effectively balances efficiency and performance, proving practical for real-world applications.

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [219] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/pdf/2505.03811)
*Surajit Chakrabarty, Rukma Talwadker, Tridib Mukherjee*

Main category: cs.LG

TL;DR: ScarceGAN improves rare sample identification in multi-dimensional longitudinal data by leveraging weak labels and semi-supervised GANs, achieving high recall with minimal noise.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like severe scarcity of positive samples, multi-class negative samples with uneven distributions, and weak label priors in unlabelled data.

Method: Reformulates semi-supervised GANs to accommodate weak labels, introduces a 'leeway' term for noisy samples, and modifies cost objectives for discriminator and generator.

Result: Achieves over 85% recall on scarce classes (60% improvement over vanilla GANs) and outperforms benchmarks in rare attack class identification (0.09% in KDDCUP99).

Conclusion: ScarceGAN effectively leverages weak labels and semi-supervised learning to identify rare samples, setting new benchmarks in imbalanced class identification.

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>


### [220] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/pdf/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TL;DR: A review of Information Filtering Networks (IFNs), covering their theory, methods, applications, and integration with modern machine learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-dimensional data modeling by leveraging IFNs for interpretable, efficient, and scalable solutions.

Method: Discusses IFN construction methodologies like TMFG and MFCF, which generate simplicial complexes for capturing multivariate dependencies.

Result: IFNs improve interpretability, computational efficiency, and predictive performance across fields like finance, biology, and AI.

Conclusion: IFNs bridge classical network theory with modern data-driven approaches and show potential in shaping deep learning architectures.

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>


### [221] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/pdf/2505.03818)
*Antonio Valerio Miceli-Barone, Vaishak Belle, Ali Payani*

Main category: cs.LG

TL;DR: A method called SInQ generates synthetic code reasoning training data using a generator and evaluator agent in a semi-adversarial setup, improving LLM performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex coding tasks requiring deep reasoning, and obtaining training data for such tasks is challenging.

Method: SInQ involves a generator agent creating semantically distinct program variants and an evaluator agent identifying divergent inputs, training each other semi-adversarially.

Result: The method improves performance on benchmarks like cross-language vulnerability detection and Python identifier swap tasks.

Conclusion: SInQ enables scalable synthetic data generation for training LLMs on complex reasoning tasks, with released code and data for replication.

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>


### [222] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/pdf/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: Two test-time fine-tuning methods improve uncertain model predictions by refining likely classes during inference, without auxiliary data.


<details>
  <summary>Details</summary>
Motivation: To enhance model predictions by addressing uncertainty without needing extra data.

Method: Introduces a focus on likely classes during inference and uses single-step gradient descent to refine predictions.

Result: Improved accuracy on high-uncertainty samples across text and image models with consistent hyperparameters.

Conclusion: The methods effectively refine predictions by reducing uncertainty and aligning probabilities with plausible outcomes.

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>


### [223] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/pdf/2505.03822)
*Hao Wu, Jialiang Wang*

Main category: cs.LG

TL;DR: The paper proposes a double regularized second-order latent factor (DRSLF) model to improve QoS prediction accuracy in cloud services by combining L1/L2-norm regularization and second-order optimization.


<details>
  <summary>Details</summary>
Motivation: Existing latent factor analysis models for QoS data rely on first-order optimizers and L2-norm regularization, leading to lower prediction accuracy.

Method: The DRSLF model integrates L1/L2-norm regularization and uses second-order information (Hessian-vector product) in conjugate gradient steps.

Result: Experiments on real-world QoS datasets show DRSLF outperforms baselines in low-rank representation.

Conclusion: DRSLF enhances QoS prediction accuracy by leveraging double regularization and second-order optimization.

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>


### [224] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/pdf/2505.03825)
*Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau*

Main category: cs.LG

TL;DR: ITA-CTF is a data-efficient framework for classifying multi-dimensional time series by combining contrastive tensor factorization with intelligent augmentations to handle low-data challenges.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning struggles with overfitting in low-data environments, making it hard to learn generalizable features for multi-dimensional time series classification.

Method: ITA-CTF uses contrastive tensor factorization (CTF) to learn core components and dependencies, enhanced by an ITA module that generates realistic augmentations guided by class prototypes.

Result: The method outperforms standard tensor factorization and DL benchmarks, achieving up to 18.7% improvement in performance across five classification tasks.

Conclusion: ITA-CTF effectively addresses low-data challenges by leveraging contrastive learning and intelligent augmentations, improving classification accuracy.

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [225] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/pdf/2505.03827)
*Xin Wang, Ling Feng, Huijun Zhang, Lei Cao, Kaisheng Zeng, Qi Li, Yang Ding, Yi Dai, David Clifton*

Main category: cs.LG

TL;DR: The paper introduces a meta-learning framework for estimating specific stressors from social media posts, addressing challenges like class diversity and limited data. It includes a meta-knowledge inheritance mechanism to prevent catastrophic forgetting and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Stress detection via social media is crucial for health, but existing work lacks specificity in identifying stressors. This study aims to estimate detailed stressors (e.g., exams, writing papers) despite challenges like class diversity and limited labeled data.

Method: A meta-learning-based framework enhanced by a meta-knowledge inheritance mechanism is proposed. It learns generic stressor contexts and generalizes well to new stressors with minimal labeled data.

Result: The model outperforms baselines, demonstrating strong generalization and prevention of catastrophic forgetting. A public dataset for stressor estimation is also released.

Conclusion: The framework effectively estimates stressors in a few-shot learning setting, advancing stress detection research. The public dataset supports further AI development for human well-being.

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>


### [226] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/pdf/2505.03849)
*Jonathan Gorard, Ammar Hakim, Hong Qin, Kyle Parfrey, Shantenu Jha*

Main category: cs.LG

TL;DR: The paper proposes a hybrid approach combining Monte Carlo sampling with formal verification methods to ensure physically and mathematically valid parameter spaces in high-dimensional inverse problems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-dimensional parameter scans and uncertainties in inverse problems in nuclear fusion and astrophysics.

Method: Combines Monte Carlo sampling with non-linear dimensionality reduction (e.g., autoencoders) and formal verification methods.

Result: Aims to construct parameter space restrictions with provable correctness while accounting for uncertainties.

Conclusion: Advocates for a hybrid approach to improve the reliability of parameter space exploration in complex inverse problems.

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


### [227] [Machine Learning: a Lecture Note](https://arxiv.org/pdf/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TL;DR: A lecture note for early-year master's and PhD students in data science, covering foundational machine learning topics, probabilistic unsupervised learning, and advanced topics like reinforcement learning and meta-learning.


<details>
  <summary>Details</summary>
Motivation: To prepare students with foundational machine learning concepts and equip them for advanced study and research in AI.

Method: Starts with basics (e.g., classification, backpropagation, SGD), explores probabilistic unsupervised learning (e.g., GANs, autoregressive models), and concludes with diverse advanced topics.

Result: Students gain a solid foundation in machine learning, enabling them to tackle more advanced topics and research.

Conclusion: The lecture note successfully bridges foundational knowledge to advanced machine learning, preparing students for further study and research in AI.

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>


### [228] [Explaining Anomalies with Tensor Networks](https://arxiv.org/pdf/2505.03911)
*Hans Hohenfeld, Marius Beuerle, Elie Mounzer*

Main category: cs.LG

TL;DR: The paper extends tensor networks for explainable anomaly detection to real-valued data and introduces tree tensor networks, demonstrating their effectiveness on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To broaden the application of tensor networks for explainable anomaly detection beyond discrete-valued tasks, enabling insights into real-world problems.

Method: Extends matrix product states to real-valued data and introduces tree tensor networks for anomaly detection, tested on three benchmarks.

Result: Shows adequate predictive performance and explainability of anomalies compared to baseline models.

Conclusion: Expands tensor networks' applicability and paves the way for future extensions to more complex architectures.

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>


### [229] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/pdf/2505.03923)
*Pedram Pad, Hadi Hammoud, Mohamad Dia, Nadim Maamari, L. Andrea Dunbar*

Main category: cs.LG

TL;DR: A novel, non-intrusive feature selection layer for neural networks automatically selects the most informative features during training, eliminating the need for retraining or hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing feature selection methods require post-selection retraining and extensive hyperparameter tuning, complicating adoption.

Method: Introduces a simple feature selection layer using trainable gains and Gaussian noise to automatically select features, with no changes to loss function or architecture.

Result: Achieves state-of-the-art performance on benchmarks and real-world datasets without hyperparameter search or retraining.

Conclusion: Demonstrates that simplicity and performance can coexist, providing an effective, straightforward tool for feature selection.

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>


### [230] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/pdf/2505.03949)
*John Christopher Tidwell, John Storm Tidwell*

Main category: cs.LG

TL;DR: A deep learning framework combining CNN, LSTM, and DQN for automated stock trading to overcome market noise and complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and direct RL struggle with market noise, complexity, and generalization in stock trading.

Method: Integrated framework using CNN for pattern recognition in technical indicators, LSTM for temporal dependencies, and DQN for optimal trading policy.

Result: Proposed solution aims to improve trading decisions by leveraging deep learning techniques.

Conclusion: The framework addresses limitations of traditional methods and RL, offering a robust approach for automated stock trading.

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>


### [231] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/pdf/2505.03953)
*Noah Schutte, Grigorii Veviurko, Krzysztof Postek, Neil Yorke-Smith*

Main category: cs.LG

TL;DR: The paper explores problem properties that justify using single-scenario deterministic proxies or distribution estimation in Decision-Focused Learning (DFL) for optimization under uncertainty. It introduces effective decision proxies with minimal learning complexity and demonstrates their success in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in understanding when to use single-scenario deterministic proxies versus distribution estimation in DFL, aiming to maximize decision quality rather than prediction accuracy.

Method: The study investigates problem properties to justify assumptions, proposes effective decision proxies for DFL, and tests them on problems with continuous/discrete variables and uncertainty in objectives/constraints.

Result: The presented approaches prove effective in experiments, showing minimal compromise on learning complexity while improving decision quality.

Conclusion: The paper concludes that understanding problem properties can guide the choice between single-scenario proxies and distribution estimation in DFL, offering practical solutions for optimization under uncertainty.

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>


### [232] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/pdf/2505.03955)
*Charupriya Sharma, IÃ±aki Estella Aguerri, Daniel Guimarans*

Main category: cs.LG

TL;DR: FlowRec introduces a network flow optimization approach for hierarchical forecast reconciliation, improving efficiency and scalability over traditional methods like MinT.


<details>
  <summary>Details</summary>
Motivation: Hierarchical forecasting requires coherent predictions across aggregation levels, but current methods are limited to tree structures and computationally expensive.

Method: FlowRec reformulates reconciliation as a network flow optimization, enabling generalized network structures and efficient updates.

Result: FlowRec achieves significant improvements in accuracy, runtime (3-40x), and memory usage (5-7x) over MinT.

Conclusion: FlowRec is a scalable and efficient solution for large-scale hierarchical forecasting, with optimality guarantees and dynamic update capabilities.

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>


### [233] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/pdf/2505.03977)
*Guilherme S. Imai Aldeia, Hengzhe Zhang, Geoffrey Bomarito, Miles Cranmer, Alcides Fonseca, Bogdan Burlacu, William G. La Cava, FabrÃ­cio Olivetti de FranÃ§a*

Main category: cs.LG

TL;DR: The paper introduces an updated version of SRBench, a benchmark for Symbolic Regression (SR), expanding methods, refining metrics, and analyzing trade-offs. It highlights the lack of a dominant algorithm and calls for community involvement to maintain SRBench as a living benchmark.


<details>
  <summary>Details</summary>
Motivation: Benchmarking SR methods is challenging due to diverse algorithms, datasets, and evaluation criteria. The paper aims to improve SRBench to better reflect the state-of-the-art in SR.

Method: The authors update SRBench by doubling the evaluated methods, refining metrics, and improving visualizations. They analyze trade-offs between complexity, accuracy, and energy consumption.

Result: No single algorithm performs best across all datasets. The paper emphasizes the need for standardized hyperparameter tuning and resource allocation.

Conclusion: The paper calls for community action to maintain SRBench, proposes deprecation criteria, and suggests best practices like adaptive tuning and energy efficiency.

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>


### [234] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/pdf/2505.03980)
*Aroon Sankoh, Victor Wickerhauser*

Main category: cs.LG

TL;DR: Comparison of MLE and RNN for estimating Ornstein-Uhlenbeck process parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical methods like MLE are compared with deep learning (RNN) for better accuracy in parameter estimation.

Method: Experiments comparing MLE and RNN for parameter estimation in the Ornstein-Uhlenbeck process.

Result: RNN may offer more precise estimators compared to MLE.

Conclusion: Deep learning (RNN) shows promise for improving parameter estimation in stochastic differential equations.

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>


### [235] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/pdf/2505.03983)
*Hengyuan Hu, Aniket Das, Dorsa Sadigh, Nima Anari*

Main category: cs.LG

TL;DR: DDPMs face slow inference due to sequential computation. This work leverages Stochastic Localization to enable optimization techniques like Autospeculative Decoding (ASD), achieving faster parallel runtime.


<details>
  <summary>Details</summary>
Motivation: Address the inference-time bottlenecks in DDPMs by exploiting their connection to Stochastic Localization.

Method: Reparametrize DDPM increments to satisfy exchangeability, then adapt Autospeculative Decoding (ASD) from autoregressive models.

Result: ASD achieves a theoretical $	ilde{O}(K^{rac{1}{3}})$ speedup and practical inference acceleration in DDPMs.

Conclusion: The exchangeability insight and ASD provide efficient optimization for DDPMs, reducing inference bottlenecks.

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>


### [236] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/pdf/2505.03992)
*Jarren Briscoe, Garrett Kepler, Daryl Deford, Assefaw Gebremedhin*

Main category: cs.LG

TL;DR: The paper highlights the impact of combinatorics-induced sample-size bias in classification metrics, proposing a model-agnostic correction technique to improve fairness in evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of sample-size bias in classification metrics, especially in social applications with disparate group sizes, ensuring fair and trustworthy model assessments.

Method: Analyzes bias in common classification metrics, identifies undefined cases, and proposes a model-agnostic technique for assessment and correction.

Result: Reveals significant bias due to combinatorics in metrics, challenges their efficacy in high-resolution bias assessment, and provides corrective measures.

Conclusion: The study advances fair classification methods by addressing combinatorics-induced bias and improving evaluation practices.

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>


### [237] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/pdf/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: The paper examines scaling issues in the muon optimizer's iterative orthogonalization at larger scales, showing theoretical and empirical evidence but no solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the scaling behavior of muon optimizer's hyper-parameters and identify potential issues at larger scales.

Method: Theoretical analysis and empirical validation on random matrices.

Result: Demonstrates that singular values of random matrices shrink with scale, impacting muon's orthogonalization.

Conclusion: Highlights a scaling issue in muon but does not propose remedies.

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


### [238] [Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners](https://arxiv.org/pdf/2410.02131)
*Hung Manh Pham, Aaqib Saeed, Dong Ma*

Main category: cs.LG

TL;DR: D-BETA is a novel framework for integrating ECG signals and textual reports using contrastive masked auto-encoder architecture, outperforming existing methods by 15% AUC in linear probing and 2% in zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Accurate ECG interpretation combined with textual reports can enhance clinical diagnostics, but modality disparities and data scarcity hinder robust cross-modal learning.

Method: D-BETA pre-trains ECG and text data with masked modality modeling, specialized loss functions, and improved negative sampling for cross-modal alignment.

Result: D-BETA achieves significant improvements (15% AUC in linear probing, 2% in zero-shot) over state-of-the-art models on five public datasets.

Conclusion: D-BETA effectively advances automated clinical diagnostics through robust multi-modal representations, with code and checkpoints publicly available.

Abstract: The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for
diagnosing cardiovascular diseases. Integrating ECG signals with accompanying
textual reports further holds immense potential to enhance clinical diagnostics
by combining physiological data and qualitative insights. However, this
integration faces significant challenges due to inherent modality disparities
and the scarcity of labeled data for robust cross-modal learning. To address
these obstacles, we propose D-BETA, a novel framework that pre-trains ECG and
text data using a contrastive masked auto-encoder architecture. D-BETA uniquely
combines the strengths of generative with boosted discriminative capabilities
to achieve robust cross-modal representations. This is accomplished through
masked modality modeling, specialized loss functions, and an improved negative
sampling strategy tailored for cross-modal alignment. Extensive experiments on
five public datasets across diverse downstream tasks demonstrate that D-BETA
significantly outperforms existing methods, achieving an average AUC
improvement of 15% in linear probing with only one percent of training data and
2% in zero-shot performance without requiring training data over
state-of-the-art models. These results highlight the effectiveness of D-BETA,
underscoring its potential to advance automated clinical diagnostics through
multi-modal representations. Our sample code and checkpoint are made available
at https://github.com/manhph2211/D-BETA.

</details>


### [239] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/pdf/2505.04046)
*Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, Dezhong Peng*

Main category: cs.LG

TL;DR: RDML addresses adversarial unreliability in trusted multi-view learning by disentangling clean and adversarial parts of data, recalibrating features, and using evidential attention.


<details>
  <summary>Details</summary>
Motivation: Existing trusted multi-view learning methods assume secure data, but adversarial perturbations in safety-sensitive applications (e.g., autonomous driving) undermine reliability.

Method: Proposes RDML: evidential disentanglement learning, feature recalibration, and view-level evidential attention to mitigate adversarial impacts.

Result: RDML outperforms state-of-the-art methods in multi-view classification under adversarial attacks.

Conclusion: RDML effectively enhances the reliability of multi-view learning in adversarial settings.

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>


### [240] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/pdf/2505.04075)
*Teddy Foley, Spencer Guo, Henry Josephson, Anqi Qu, Jack Sanderson*

Main category: cs.LG

TL;DR: The paper explores whether LLMs can advance without more compute, distinguishing between compute-dependent and compute-independent innovations, and introduces a metric (CEG) to quantify their impact.


<details>
  <summary>Details</summary>
Motivation: Motivated by regulatory efforts limiting high-performance hardware, the study investigates LLM progress in compute-constrained environments.

Method: A classification framework for innovations (compute-dependent vs. independent) is introduced, validated via small-scale GPT-2 experiments using CEG.

Result: Compute-independent innovations showed significant gains (CEG up to 3.5x), while compute-dependent ones were ineffective or harmful at small scales.

Conclusion: Algorithmic advancements can drive LLM progress under compute constraints, but compute availability remains crucial for certain innovations.

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>


### [241] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/pdf/2505.04083)
*Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele*

Main category: cs.LG

TL;DR: Plexus, a 3D parallel approach for full-graph training, scales to billion-edge graphs, outperforming existing methods with significant speedups and reduced time to solution.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of GPU memory limits and communication overhead in distributed full-graph training for GNNs.

Method: Proposes Plexus, a 3D parallel approach with optimizations like load balancing and a performance model for optimal configuration.

Result: Achieves speedups of 2.3x-12.5x and reduces time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on Frontier.

Conclusion: Plexus effectively scales GNN training to large graphs with superior performance and efficiency.

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>


### [242] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/pdf/2505.04104)
*Sarah Hartman, Cheng Soon Ong, Julia Powles, Petra Kuhnert*

Main category: cs.LG

TL;DR: The paper advocates for a responsible, application-driven approach (RAD-AI) to AI research, emphasizing context-specific, ethical, and adaptive methods for meaningful societal impact.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure AI research aligns with societal needs by addressing ethical, legal, and contextual challenges, fostering meaningful advances.

Method: A three-staged approach: (1) transdisciplinary teams and people-centered studies, (2) context-specific methods and ethical commitments, and (3) staged testbeds and community practice.

Result: Proposes RAD-AI as a framework to drive AI research that is adaptive, ethical, and contextually relevant.

Conclusion: RAD-AI can unlock new value by aligning AI research with societal needs and values, ensuring technical feasibility and ethical integrity.

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>


### [243] [Alpha Excel Benchmark](https://arxiv.org/pdf/2505.04110)
*David Noever, Forrest McKee*

Main category: cs.LG

TL;DR: A new benchmark for LLMs using Financial Modeling World Cup challenges, showing varied performance across tasks and highlighting practical business applications.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between academic AI benchmarks and real-world business tasks by leveraging Excel-based challenges.

Method: Converted 113 FMWC challenges into JSON formats and evaluated leading LLMs on this dataset.

Result: Models excelled in pattern recognition but struggled with complex numerical reasoning, revealing category-specific strengths.

Conclusion: The benchmark offers a practical framework for assessing LLMs in business contexts, aligning AI evaluation with real-world Excel proficiency.

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>


### [244] [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/pdf/2503.18892)
*Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He*

Main category: cs.LG

TL;DR: The paper investigates zero RL training across diverse base models, achieving improvements in reasoning accuracy and response length, while noting distinct training patterns and the emergence of cognitive behaviors like the 'aha moment.'


<details>
  <summary>Details</summary>
Motivation: To explore zero RL training beyond the Qwen2.5 model series, addressing limitations in representativeness and understanding diverse model behaviors.

Method: Leverages key design strategies like adjusting format rewards and controlling query difficulty, applied across 10 diverse base models.

Result: Substantial improvements in reasoning accuracy and response length, with observations of distinct training patterns and the 'aha moment' in small non-Qwen models.

Conclusion: Successful zero RL training is achievable with careful design, and the findings are shared openly to support further research.

Abstract: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can
naturally emerge through a simple reinforcement learning (RL) framework with
rule-based rewards, where the training may directly start from the base
models-a paradigm referred to as zero RL training. Most recent efforts to
reproduce zero RL training have primarily focused on the Qwen2.5 model series,
which may not be representative as we find the base models already exhibit
strong instruction-following and self-reflection abilities. In this work, we
investigate zero RL training across 10 diverse base models, spanning different
families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,
Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several
key design strategies-such as adjusting format reward and controlling query
difficulty-we achieve substantial improvements in both reasoning accuracy and
response length across most settings. However, by carefully monitoring the
training dynamics, we observe that different base models exhibit distinct
patterns during training. For instance, the increased response length does not
always correlate with the emergence of certain cognitive behaviors such as
verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for
the first time in small models not from the Qwen family. We share the key
designs that enable successful zero RL training, along with our findings and
practices. To facilitate further research, we open-source the code, models, and
analysis tools.

</details>


### [245] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/pdf/2505.04139)
*Hongyi Li, Jun Xu, William Ward Armstrong*

Main category: cs.LG

TL;DR: LHT is a non-iterative, statistically-driven oblique decision tree model for expressive and interpretable classification, with deterministic hyperplane construction and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable and theoretically grounded oblique decision tree model that avoids iterative optimization or heuristics.

Method: LHT directly computes hyperplane parameters from feature weights based on class differences, using a deterministic mechanism and piecewise linear membership functions in leaf nodes.

Result: LHT achieves competitive accuracy, has a time complexity of O(mnd), and provides interpretability through explicit feature weighting.

Conclusion: LHT is a practical, interpretable, and efficient alternative to traditional tree-based models, with theoretical guarantees and open-source implementation.

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>


### [246] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2505.04158)
*Yulong Wang, Yushuo Liu, Xiaoyi Duan, Kai Wang*

Main category: cs.LG

TL;DR: FilterTS is a novel multivariate time series forecasting model using frequency-domain filtering to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to capture complex periodic and trend components in multivariate time series, limiting prediction performance.

Method: FilterTS employs Dynamic Cross-Variable Filtering and Static Global Filtering Modules in the frequency domain, converting time-domain convolutions to multiplicative operations.

Result: FilterTS outperforms existing methods in accuracy and computational efficiency across eight real-world datasets.

Conclusion: FilterTS effectively addresses the limitations of current models by leveraging frequency-domain techniques for enhanced forecasting.

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>


### [247] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/pdf/2505.03335)
*Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces Absolute Zero (AZR), a self-evolving RLVR paradigm for language models that learns without external data, achieving SOTA performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability concerns of human supervision in RLVR and preparing for a future where human-provided tasks may limit superintelligent AI learning.

Method: AZR proposes and solves its own tasks using a code executor for validation and reward, eliminating reliance on external data.

Result: AZR outperforms zero-setting models using human-curated data, achieving SOTA in coding and math reasoning.

Conclusion: AZR demonstrates scalable, open-ended learning without human supervision, applicable across model scales and classes.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [248] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/pdf/2505.04163)
*Sungwon Han, Seungeon Lee, Meeyoung Cha, Sercan O Arik, Jinsung Yoon*

Main category: cs.LG

TL;DR: RAFT is a retrieval-augmented time series forecasting method that leverages historical data patterns to improve predictions, outperforming baselines by 86%.


<details>
  <summary>Details</summary>
Motivation: To enhance time series forecasting by providing inductive biases and complementing the model's learning capacity through retrieval of similar historical patterns.

Method: RAFT retrieves historical data candidates with patterns similar to the input and uses their future values alongside inputs for predictions.

Result: RAFT outperforms contemporary baselines on ten benchmark datasets with an average win ratio of 86%.

Conclusion: RAFT effectively augments forecasting models by retrieving and utilizing historical patterns, demonstrating superior performance.

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>


### [249] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/pdf/2505.04174)
*Ju-Hyung Lee, Yanqing Lu*

Main category: cs.LG

TL;DR: The paper introduces a cross-layer approach using an on-device LLM for wireless roaming, improving AP selection and dynamic threshold adjustment, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Wireless roaming is challenging due to sticky or excessive handovers in dynamic environments, and conventional methods often fail.

Method: Uses an on-device LLM for context-aware AP selection and dynamic threshold adjustment, optimized for latency and resource efficiency.

Result: Outperforms legacy heuristics and DRL baselines, balancing roaming stability and signal quality.

Conclusion: Demonstrates the potential of application-layer LLM reasoning for wireless control in edge systems.

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [250] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/pdf/2505.04167)
*Yulong Wang, Xiaofeng Hu, Xiaojian Cui, Kai Wang*

Main category: cs.LG

TL;DR: STRGCN is a Spatio-Temporal Relational Graph Convolutional Network designed for irregular multivariate time series (IMTS) without pre-alignment, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for IMTS rely on pre-alignment, which distorts data patterns and increases computational costs. STRGCN addresses these limitations by directly modeling asynchronous data.

Method: STRGCN represents IMTS as a fully connected graph, with observations as nodes, and uses a hierarchical 'Sandwich' structure to optimize embeddings.

Result: Experiments on four datasets show STRGCN achieves top accuracy with efficient memory usage and training speed.

Conclusion: STRGCN effectively handles IMTS by preserving asynchronous data nature and outperforms existing methods.

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>


### [251] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/pdf/2505.04173)
*Zixiao Wang, Wenqian Zhao, Yunheng Shen, Yang Bai, Guojin Chen, Farzan Farnia, Bei Yu*

Main category: cs.LG

TL;DR: DiffPattern-Flex introduces a novel method for reliable layout pattern generation using a discrete diffusion model and optimization-based legalization.


<details>
  <summary>Details</summary>
Motivation: Address concerns about legality guarantees in neural network-based layout generation by ensuring reliable and efficient pattern production.

Method: Combines a discrete diffusion model for diverse topology generation with an optimization-based white-box assessment for legality.

Result: Outperforms existing methods in benchmarks, producing reliable layout patterns efficiently.

Conclusion: DiffPattern-Flex is a robust solution for legal and efficient layout pattern generation.

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>


### [252] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/pdf/2505.04223)
*Sanghyeon Park, Soo-Mook Moon*

Main category: cs.LG

TL;DR: FRAIN introduces FastSync and SLERP to improve asynchronous federated learning, outperforming FedAvg, FedAsync, and BRAIN in stability and robustness under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing FL methods like client drift, performance drops under data heterogeneity, and synchronization overhead.

Method: FRAIN uses FastSync for efficient global model approximation and SLERP for parameter merging to preserve model directions.

Result: FRAIN achieves more stable and robust convergence than FedAvg, FedAsync, and BRAIN, especially in harsh environments.

Conclusion: FRAIN effectively mitigates issues in asynchronous FL, offering improved performance under non-IID data, delays, and malicious nodes.

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>


### [253] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/pdf/2505.04193)
*Bang You, Chenxu Wang, Huaping Liu*

Main category: cs.LG

TL;DR: The paper introduces a simplicity inductive bias in reinforcement learning by minimizing action trajectory entropy, leading to more robust and consistent policies.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning often captures spurious correlations, failing under perturbations. Simplicity is proposed as a solution for robustness.

Method: Minimizes trajectory entropy (bits to describe actions after observing states) using a variational action prediction model and an information-regularized reward function.

Result: Policies show cyclical, consistent action trajectories, outperforming state-of-the-art in robustness and performance.

Conclusion: Trajectory entropy minimization enhances policy simplicity and robustness, validated in high-dimensional tasks.

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>


### [254] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/pdf/2505.04200)
*Ahmed Sayeed Faruk, Jason Sulskis, Elena Zheleva*

Main category: cs.LG

TL;DR: The paper introduces cluster-based MAB algorithms to estimate treatment effects in networks with interference, outperforming vanilla MAB and RCT methods by balancing exploration-exploitation tradeoffs.


<details>
  <summary>Details</summary>
Motivation: A/B testing faces challenges like interference in social networks and performance loss, necessitating adaptive strategies for accurate treatment effect estimation.

Method: Two cluster-based MAB algorithms are proposed to learn total treatment effects while maximizing rewards, compared against vanilla MAB and RCT methods on semi-synthetic data.

Result: Cluster-based MABs achieve higher reward-action ratios than RCT methods without significant accuracy loss, while vanilla MABs suffer from spillover errors.

Conclusion: Cluster-based MABs offer an efficient alternative to RCTs for treatment effect estimation in networks with interference, balancing reward and accuracy.

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>


### [255] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/pdf/2505.04204)
*Mateo Lopez-Ledezma, Gissel Velarde*

Main category: cs.LG

TL;DR: The paper explores binary classification in cybersecurity, testing single classifiers and imbalance learning techniques, emphasizing the need for tailored approaches for each dataset.


<details>
  <summary>Details</summary>
Motivation: Cybersecurity requires automation due to large operational volumes, with many tasks framed as binary classification problems.

Method: Three experiments: evaluating single classifiers, testing sampling techniques, and assessing Self-Paced Ensembling with varying base classifiers.

Result: Imbalance learning techniques had mixed effects; best performers varied by dataset, highlighting the need for dataset-specific testing.

Conclusion: Tailored testing of classifiers and imbalance techniques is recommended for cybersecurity applications with imbalanced datasets.

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>


### [256] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/pdf/2505.04278)
*Weiwei Ye, Zhuopeng Xu, Ning Gui*

Main category: cs.LG

TL;DR: NsDiff introduces a diffusion-based framework using LSNM to model time-varying uncertainty in time series, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing DDPMs fail to capture non-stationary uncertainty due to fixed variance assumptions.

Method: NsDiff combines a diffusion-based generative model with a pre-trained estimator and an uncertainty-aware noise schedule.

Result: Experiments on nine datasets show NsDiff's superior performance.

Conclusion: NsDiff effectively models changing uncertainty, advancing probabilistic forecasting.

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>


### [257] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/pdf/2505.04241)
*Grzegorz Miebs, RafaÅ A. Bachorz*

Main category: cs.LG

TL;DR: A data-driven method predicts manufacturing steps and durations from 3D models using neural networks for scalable and precise scheduling.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for estimating production times are inadequate in dynamic or customized environments, necessitating a more adaptive approach.

Method: The approach renders 3D models into 2D images and uses a neural network (inspired by Generative Query Network) to map geometric features to time estimates for production steps.

Result: The method enables scalable, adaptive, and precise process planning across diverse product types.

Conclusion: This data-driven approach improves manufacturing scheduling accuracy in dynamic or customized production settings.

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>


### [258] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/pdf/2505.04263)
*Jan Blechschmidt, Tom-Christian Riemer, Max Winkler, Martin Stoll, Jan-F. Pietschmann*

Main category: cs.LG

TL;DR: A novel physics-informed deep learning method using DeepONet solves nonlinear drift-diffusion equations on metric graphs, applicable to diverse fields like biology and crowd dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods are inflexible for model design and parameter identification in drift-diffusion equations, prompting the need for a versatile deep learning approach.

Method: Three DeepONet models are trained for inflow, inner, and outflow edges, then coupled via edge-based domain decomposition to solve the drift-diffusion problem.

Result: The framework accurately evaluates graph-coupled physics models and is effective for optimization or inverse problems on such networks.

Conclusion: The approach is versatile and efficient for solving complex drift-diffusion equations on metric graphs, with potential for broader applications.

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>


### [259] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/pdf/2505.04335)
*Swagato Das, Arghya Pratihar, Swagatam Das*

Main category: cs.LG

TL;DR: HypeFCM improves fuzzy clustering in non-Euclidean spaces using hyperbolic geometry and a weight-based filtering mechanism, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods like FCM struggle with complex, high-dimensional, and non-Euclidean datasets due to Euclidean space assumptions.

Method: HypeFCM integrates fuzzy clustering with hyperbolic geometry (PoincarÃ© Disc model), initializes weights via Dirichlet distribution, and refines centroids iteratively.

Result: HypeFCM significantly outperforms conventional fuzzy clustering in non-Euclidean settings.

Conclusion: HypeFCM is robust and effective for clustering in non-Euclidean spaces, addressing limitations of traditional methods.

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>


### [260] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/pdf/2505.04340)
*Hong Jin, Kaicheng Zhou, Jie Yin, Lan You, Zhifeng Zhou*

Main category: cs.LG

TL;DR: MGA-HHN is a novel heterogeneous hypergraph neural network that addresses limitations of meta-path-based models by capturing high-order relations and mitigating over-squashing through multi-granular attention.


<details>
  <summary>Details</summary>
Motivation: Existing HeteGNNs fail to capture high-order node relations and suffer from over-squashing, limiting their performance.

Method: MGA-HHN constructs meta-path-based heterogeneous hypergraphs and employs a multi-granular attention mechanism at node and hyperedge levels.

Result: MGA-HHN outperforms state-of-the-art models in node classification, clustering, and visualization tasks.

Conclusion: MGA-HHN effectively addresses the limitations of prevailing HeteGNNs, offering improved performance and expressive node representations.

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>


### [261] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2505.04338)
*Zichen Liu, Wei Zhang, Christof SchÃ¼tte, Tiejun Li*

Main category: cs.LG

TL;DR: RDDPMs enable generative modeling on submanifolds using only function evaluations and first-order derivatives, avoiding reliance on extensive geometric data.


<details>
  <summary>Details</summary>
Motivation: Existing methods require detailed geometric information, limiting applicability. RDDPMs aim to generalize to more manifolds.

Method: Uses a projection scheme requiring only function value and first-order derivative evaluations.

Result: Demonstrated effectiveness on datasets, including high-dimensional manifolds like SO(10) and molecular configurations.

Conclusion: RDDPMs offer a versatile and practical approach for generative modeling on diverse manifolds.

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>


### [262] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/pdf/2505.04339)
*Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu*

Main category: cs.LG

TL;DR: AR-DBSCAN enhances DBSCAN by using multi-agent reinforcement learning to adaptively handle varying density scales, improving clustering accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: DBSCAN struggles with datasets of varying density scales, limiting its effectiveness in real-world applications.

Method: AR-DBSCAN models data as a two-level encoding tree, assigns agents to density partitions, and uses reinforcement learning for parameter optimization.

Result: AR-DBSCAN improves clustering accuracy by up to 144.1% (NMI) and 175.3% (ARI) and robustly finds optimal parameters.

Conclusion: AR-DBSCAN effectively addresses DBSCAN's limitations, offering adaptive and robust clustering for diverse datasets.

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>


### [263] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/pdf/2505.04346)
*Arghya Pratihar, Kushal Bose, Swagatam Das*

Main category: cs.LG

TL;DR: The paper proposes a topological clustering algorithm using Vietoris-Rips complex and Betti number filtration to improve performance on complex datasets.


<details>
  <summary>Details</summary>
Motivation: Existing topological clustering algorithms fail to fully exploit topological structures and perform inconsistently on complex datasets.

Method: The algorithm identifies topologically similar neighbors using Vietoris-Rips complex and Betti number filtration, and introduces Betti sequences to capture essential features.

Result: Experiments on synthetic and real-world datasets show the algorithm outperforms existing topology-based methods.

Conclusion: The proposed method effectively clusters complex, intertwined shapes, addressing limitations of current approaches.

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>


### [264] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/pdf/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TL;DR: The paper explores Deep Learning techniques to address energy transition challenges, focusing on residential energy use and road transport.


<details>
  <summary>Details</summary>
Motivation: The urgency to mitigate climate change and reduce CO2 emissions, coupled with the slow pace of the energy transition, drives the need for alternative solutions.

Method: Develops novel Deep Learning tools: Non-Intrusive Load Monitoring for residential energy reduction and Deep Reinforcement Learning for optimizing EV charging.

Result: Proposes actionable tools to reduce energy consumption and emissions in key sectors.

Conclusion: Deep Learning can accelerate the energy transition by targeting high-demand domains like residential energy and transport.

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>


### [265] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/pdf/2505.04435)
*Vahideh Hayyolalam, Ãznur Ãzkasap*

Main category: cs.LG

TL;DR: FedBWO reduces communication in Federated Learning by transmitting performance scores instead of model weights, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: FL clients face communication bottlenecks due to high data transmission; FedBWO aims to reduce this by optimizing updates.

Method: Uses Federated Black Widow Optimization (FedBWO) to transmit performance scores and improve local model updates.

Result: FedBWO boosts global model accuracy by 21% over FedAvg and 12% over FedGWO, while cutting communication costs.

Conclusion: FedBWO is effective for resource-constrained FL, enhancing performance and reducing communication overhead.

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>


### [266] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/pdf/2505.04371)
*Filipe Santos, JoÃ£o Paulo Fernandes, LuÃ­s Macedo*

Main category: cs.LG

TL;DR: The paper applies a flag-based RL exploration policy to Connect Four, comparing classical and quantum versions, showing improved performance over epsilon-greedy but identical win rates between the two.


<details>
  <summary>Details</summary>
Motivation: To generalize the flag-based RL exploration policy by testing it in Connect Four, a different setting than Checkers, and to study its performance under more complex scenarios like going second.

Method: Classical and quantum RL agents were trained and tested against a Randomized Negamax opponent, tracking flagged action iterations and win rates.

Result: Flagged policies outperformed epsilon-greedy, with quantum agents sampling flagged actions faster, but win rates were identical for classical and quantum versions.

Conclusion: The flag-based approach is effective, but the simplicity of the training scenario may explain the identical win rates between classical and quantum methods.

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>


### [267] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/pdf/2505.04461)
*Pengfei Jiao, Hongjiang Chen, Xuan Guo, Zhidong Zhao, Dongxiao He, Di Jin*

Main category: cs.LG

TL;DR: The paper discusses temporal interaction graph representation learning (TIGRL), its importance, methods, and future directions.


<details>
  <summary>Details</summary>
Motivation: Temporal interaction graphs (TIGs) model dynamic systems, and TIGRL aims to embed nodes effectively for downstream tasks.

Method: Proposes a taxonomy of TIGRL methods based on information types and curates datasets for empirical research.

Result: Provides a systematic categorization of TIGRL methods and resources for further study.

Conclusion: Identifies open challenges and future research directions to advance TIGRL.

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>


### [268] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/pdf/2505.04389)
*Jenni Lampainen, Kaisa Joki, Napsu Karmitsa, Marko M. MÃ¤kelÃ¤*

Main category: cs.LG

TL;DR: Clust-Splitter is an efficient algorithm for large-scale clustering using nonsmooth optimization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of clustering very large datasets efficiently and effectively.

Method: Uses a sequence of nonsmooth optimization problems and the limited memory bundle method with an incremental approach.

Result: Demonstrates high efficiency and solution quality comparable to best existing methods.

Conclusion: Clust-Splitter is a robust solution for large-scale clustering tasks.

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>


### [269] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/pdf/2505.04464)
*Louis Ohl, Fredrik Lindsten*

Main category: cs.LG

TL;DR: The paper proposes a new method to evaluate clustering models by ranking them based on their alignment with a consensus matrix, outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Current clustering evaluation metrics struggle with diverse cluster definitions and constraints, limiting their effectiveness.

Method: The method constructs a discriminative ordering using ensemble clustering, comparing model connectivity to a consensus matrix.

Result: Tests on synthetic data show the method ranks models matching the consensus well, and it outperforms other scoring methods for diverse clustering algorithms.

Conclusion: The proposed ranking score is effective for evaluating clustering models with varied definitions and constraints.

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>


### [270] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/pdf/2505.04396)
*Jingnan Wang, Jie Chao, Shangshang Yang, Congyi Nai, Kaijun Ren, Kefeng Deng, Xi Chen, Yaxin Liu, Hanqiuzi Wen, Ziniu Xiao, Lifeng Zhang, Xiaodong Wang, Jiping Guan, Baoxiang Pan*

Main category: cs.LG

TL;DR: A method combines learned high-resolution climatological data with coarse-grid forecasts for accurate, efficient wind power weather predictions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in renewable energy planning due to scale inconsistency, process errors, and high computational costs of traditional weather forecasting.

Method: Learn climatological distribution from high-resolution simulations and combine it with coarse-grid forecasts to produce fine-grained, large ensemble forecasts.

Result: Outperforms existing methods in accuracy and efficiency, with significant computational cost reduction (1 GPU hour vs. 1000s of CPU hours).

Conclusion: The method enables more efficient and reliable renewable energy planning by reducing costs while maintaining accuracy.

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [271] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/pdf/2505.04468)
*Hyeju Shin, Kyudan Jung, Seongwon Yun, Juyoung Yun*

Main category: cs.LG

TL;DR: FFTKF combines frequency-domain noise shaping and Kalman filtering to improve DP-SGD performance while maintaining privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Address the performance degradation in DP-SGD due to added noise, aiming to preserve model utility.

Method: Integrates frequency-domain noise shaping with Kalman filtering, using a high-frequency mask and scalar-gain Kalman filter.

Result: Outperforms DP-SGD and DiSK in test accuracy across multiple datasets with CNNs, Wide ResNets, and Vision Transformers.

Conclusion: FFTKF maintains privacy guarantees while achieving better utility through reduced noise and controlled bias.

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>


### [272] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/pdf/2505.04412)
*Ren Wang, Pengcheng Zhou*

Main category: cs.LG

TL;DR: An AutoEncoder-based method integrates manifold reconstruction and learning to improve dimensionality reduction for noisy data, outperforming t-SNE, UMAP, and Topological AutoEncoders.


<details>
  <summary>Details</summary>
Motivation: Existing manifold learning methods often fail to balance local details and global topology in noisy data, leading to distorted embeddings.

Method: The proposed method combines a manifold reconstruction layer with topological and geometric regularizations during dimensionality reduction, enhancing mutual training benefits.

Result: Outperforms baselines (t-SNE, UMAP, Topological AutoEncoders) in discovering and preserving manifold structures, validated by visual and quantitative metrics.

Conclusion: Combining manifold reconstruction with learning is key for reliable latent manifold representation, especially in noisy real-world data.

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>


### [273] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/pdf/2505.04417)
*Georg A. Gottwald, Shuigen Liu, Youssef Marzouk, Sebastian Reich, Xin T. Tong*

Main category: cs.LG

TL;DR: Localized diffusion models exploit low-dimensional structure to reduce sample complexity and avoid the curse of dimensionality, balancing statistical and localization errors for better performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face challenges due to high-dimensional score functions, which suffer from the curse of dimensionality. Understanding and leveraging low-dimensional structure (like locality) can improve efficiency.

Method: Proposes a localized diffusion model using a localized score matching loss and a localized hypothesis space to train the score function.

Result: Localization reduces sample complexity and circumvents the curse of dimensionality, with a trade-off in localization error. A moderate localization radius balances errors for optimal performance.

Conclusion: Localized diffusion models offer improved efficiency and parallel training potential, making them suitable for large-scale applications.

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>


### [274] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/pdf/2505.04440)
*Xiaozheng Qu, Zhaochuan Li, Zhuang Qi, Xiang Li, Haibei Huang, Lei Meng, Xiangxu Meng*

Main category: cs.LG

TL;DR: IR-ART improves Fuzzy ART by iteratively refining clusters, enhancing robustness to vigilance parameter changes without added complexity.


<details>
  <summary>Details</summary>
Motivation: Fuzzy ART's clustering performance is sensitive to the vigilance parameter, limiting its practicality for non-expert users. Existing solutions add complexity, contradicting the algorithm's simplicity.

Method: IR-ART integrates three phases: Cluster Stability Detection, Unstable Cluster Deletion, and Vigilance Region Expansion, iteratively refining clusters.

Result: IR-ART improves tolerance to suboptimal vigilance parameters while maintaining simplicity, validated on 15 datasets.

Conclusion: IR-ART is a practical solution for non-expert users, offering self-optimization through iterative refinement.

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>


### [275] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/pdf/2505.04558)
*Wenzhao Liu, Haoran Li, Congying Han, Zicheng Zhang, Anqi Li, Tiande Guo*

Main category: cs.LG

TL;DR: The paper introduces Purity Law (PuLa) for TSP, showing edge prevalence grows with vertex sparsity, and proposes PUPO to align neural solutions with PuLa, improving generalization without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Generalization in neural TSP solvers is challenging due to lack of robust principles for universal patterns.

Method: Uncover PuLa, a structural principle for TSP, and propose PUPO to align neural solutions with PuLa during training.

Result: PUPO enhances generalization of neural solvers without additional inference overhead.

Conclusion: PuLa and PUPO provide a principled way to improve neural TSP solvers' generalization.

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [276] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/pdf/2505.04441)
*Mirazul Haque, Petr Babkin, Farima Farmahinifarahani, Manuela Veloso*

Main category: cs.LG

TL;DR: LLM-based APR augmented with execution traces shows limited improvement, with effectiveness decreasing as trace complexity increases. Optimized prompts outperform trace-free ones, and traces complement LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based APR methods ignore runtime behavior, so this work aims to enhance APR by incorporating execution traces.

Method: Augment standard APR prompts with program execution traces, evaluated using GPT models on three APR datasets.

Result: Limited performance improvement (2/6 configurations); effectiveness decreases with trace complexity. Optimized prompts outperform trace-free ones.

Conclusion: Execution traces complement LLM reasoning, and optimized prompts are more effective than trace-free or finetuning approaches.

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>


### [277] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/pdf/2505.04619)
*Abdulaziz Almuzairee, Rohan Patil, Dwait Bhatt, Henrik I. Christensen*

Main category: cs.LG

TL;DR: The paper introduces the Merge And Disentanglement (MAD) algorithm to efficiently merge multiple camera views for robust visual servoing, balancing computational efficiency and deployment cost.


<details>
  <summary>Details</summary>
Motivation: Multiple cameras improve robustness in visual servoing but are computationally expensive. The goal is to optimize sample efficiency while ensuring lightweight deployment.

Method: The MAD algorithm merges multiple views for efficiency and augments with single-view features for lightweight deployment.

Result: The approach is validated on Meta-World and ManiSkill3, showing efficiency and robustness.

Conclusion: MAD provides a practical solution for robust visual servoing by balancing computational efficiency and deployment cost.

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


### [278] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/pdf/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TL;DR: Malicious RL fine-tuning can dismantle safety guardrails in language models efficiently. Reward Neutralization is introduced as a defense, maintaining safety under attack.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of language models to RL fine-tuning attacks, which bypass existing defenses and escalate harmful outputs.

Method: Introduces Reward Neutralization, a defense framework that trains models to reject malicious reward signals with minimal-information responses.

Result: Reward Neutralization maintains harmful scores â¤2 after 200 attack steps, outperforming standard models.

Conclusion: Robust defense against RL fine-tuning attacks is achievable, closing a security gap for open-weight models.

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>


### [279] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/pdf/2505.04471)
*Vincent Souveton, SÃ©bastien Terrana*

Main category: cs.LG

TL;DR: A novel method using Hamiltonian-informed Normalizing Flows is introduced to model the Vlasov-Poisson equations, enabling fast sampling and interpretable potential learning.


<details>
  <summary>Details</summary>
Motivation: The complexity of the Vlasov-Poisson equations and lack of analytical solutions necessitate numerical methods. This work aims to provide a faster and interpretable alternative.

Method: The approach uses Fixed-Kinetic Neural Hamiltonian Flows to transform an initial Gaussian distribution into the final state via invertible, volume-preserving transformations derived from Hamiltonian dynamics.

Result: The model successfully learns the final distribution from initial states and generalizes to unseen intermediate states, providing insights into system evolution.

Conclusion: The method offers a fast, interpretable, and generalizable solution for modeling Hamiltonian systems like the Vlasov-Poisson equations.

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>


### [280] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/pdf/2505.04535)
*Michail Theologitis, Vasilis Samoladas, Antonios Deligiannakis*

Main category: cs.LG

TL;DR: FDA-Opt is a new algorithm combining FedOpt and FDA, improving federated learning for fine-tuning language models without extra tuning.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) struggles with rigid communication of large model parameters. FedOpt and FDA have limitations in synchronization and tuning.

Method: Introduces FDA-Opt, a unified algorithm combining FedOpt and FDA principles, resolving their drawbacks.

Result: Outperforms FedOpt in fine-tuning LMs across NLP tasks, even with competitor-optimized settings.

Conclusion: FDA-Opt is a practical, superior drop-in replacement for FedOpt in FL systems, requiring no additional configuration.

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>


### [281] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/pdf/2505.04608)
*Drew Prinster, Xing Han, Anqi Liu, Suchi Saria*

Main category: cs.LG

TL;DR: The paper proposes weighted conformal test martingales (WCTMs) for online monitoring of AI/ML systems to detect unexpected data distribution changes, improving adaptability and performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Responsible AI/ML deployment in high-stakes settings requires post-deployment monitoring to detect unsafe behavior, but current methods are limited in scope and adaptability.

Method: The authors introduce WCTMs, a generalization of conformal test martingales, to monitor any unexpected changepoints while controlling false alarms. They also propose specific algorithms for online adaptation to mild shifts and detection of severe shifts.

Result: WCTMs demonstrate improved performance on real-world datasets compared to state-of-the-art baselines.

Conclusion: The proposed WCTMs expand the capabilities of monitoring methods, enabling better adaptability and detection of harmful shifts in AI/ML systems.

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>


### [282] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $Î±$-$Î²$-Divergence](https://arxiv.org/pdf/2505.04560)
*Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang*

Main category: cs.LG

TL;DR: ABKD introduces a framework using Î±-Î²-divergence to balance the Hardness-Concentration and Confidence-Concentration effects in Knowledge Distillation, outperforming FKLD and RKLD.


<details>
  <summary>Details</summary>
Motivation: The core challenge in KD is balancing two mode-concentration effects: Hardness-Concentration (focus on large errors) and Confidence-Concentration (focus on high student confidence). FKLD and RKLD handle these effects poorly, either too weakly or too strongly.

Method: Proposes ABKD, a framework with Î±-Î²-divergence, to smoothly interpolate between FKLD and RKLD, balancing the two effects.

Result: ABKD achieves an effective trade-off between the two effects, validated by experiments on 17 datasets with 12 teacher-student settings.

Conclusion: ABKD provides a superior balance in KD, leveraging teacher distribution more effectively than FKLD or RKLD alone.

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [283] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/pdf/2505.04566)
*Lucas R. C. Farias, Talita P. Silva, Pedro H. M. Araujo*

Main category: cs.LG

TL;DR: A multitask LSTM model predicts arboviral outbreaks and case counts for dengue, chikungunya, and Zika in Recife, Brazil, using historical data. Longer input windows improve dengue forecasting, while intermediate windows optimize outbreak detection.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable epidemic forecasting in data-limited public health settings by jointly predicting outbreaks and case counts for multiple diseases.

Method: Uses LSTM networks with a sliding window strategy (60, 90, 120 days) and hyperparameter optimization via Keras Tuner. Evaluated with time series cross-validation and a 2023 test set.

Result: Longer windows enhance dengue case forecasting, while intermediate windows improve outbreak detection. The multitask model performs well across diseases and tasks.

Conclusion: The unified multitask approach is feasible and advantageous for epidemic forecasting in resource-limited scenarios.

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>


### [284] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/pdf/2505.04599)
*Michael Crawshaw, Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>


### [285] [Testing Juntas Optimally with Samples](https://arxiv.org/pdf/2505.04604)
*Lorenzo Beretta, Nathaniel Harms, Caleb Koch*

Main category: cs.LG

TL;DR: Tight bounds for distribution-free k-junta testing and feature selection, showing no large gap between tolerant testing and learning.


<details>
  <summary>Details</summary>
Motivation: To establish the first tight bounds for testing a natural class of Boolean functions in the distribution-free sample-based model and explore implications for feature selection and tolerant testing.

Method: Proving upper and lower bounds on the number of samples required for k-junta testing and analyzing the feature selection problem.

Result: Tight bounds of Î(1/Ïµ(â(2^k log(n choose k)) + log(n choose k))) for k-junta testing and Î©(2^{(1-o(1))k} + log(n choose k)) for tolerant testing.

Conclusion: The study provides foundational insights into the sample complexity of junta testing and its relationship with learning, highlighting the absence of a large gap in tolerant testing.

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>


### [286] [Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems](https://arxiv.org/pdf/2302.03669)
*Ming Zhu, Xiao-Yang Liu, Sem Borst, Anwar Walid*

Main category: cs.LG

TL;DR: The paper explores deep reinforcement learning (DRL) for adaptive traffic light control, showing scalability and optimal 'greenwave' behavior in grid networks.


<details>
  <summary>Details</summary>
Motivation: To improve traffic efficiency and reduce congestion in intelligent transportation systems using scalable DRL methods.

Method: Uses DQN for single intersections and DDPG for grid networks, demonstrating 'greenwave' behavior.

Result: DQN delivers optimal control in single intersections; DDPG produces 'greenwave' in grids, proven optimal.

Conclusion: DRL is scalable and effective for traffic light control, with 'greenwave' emerging as optimal behavior.

Abstract: Smart traffic lights in intelligent transportation systems (ITSs) are
envisioned to greatly increase traffic efficiency and reduce congestion. Deep
reinforcement learning (DRL) is a promising approach to adaptively control
traffic lights based on the real-time traffic situation in a road network.
However, conventional methods may suffer from poor scalability. In this paper,
we investigate deep reinforcement learning to control traffic lights, and both
theoretical analysis and numerical experiments show that the intelligent
behavior ``greenwave" (i.e., a vehicle will see a progressive cascade of green
lights, and not have to brake at any intersection) emerges naturally a grid
road network, which is proved to be the optimal policy in an avenue with
multiple cross streets. As a first step, we use two DRL algorithms for the
traffic light control problems in two scenarios. In a single road intersection,
we verify that the deep Q-network (DQN) algorithm delivers a thresholding
policy; and in a grid road network, we adopt the deep deterministic policy
gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN
algorithm delivers the optimal control, and the DDPG algorithm with passive
observations has the capability to produce on its own a high-level intelligent
behavior in a grid road network, namely, the ``greenwave" policy emerges. We
also verify the ``greenwave" patterns in a $5 \times 10$ grid road network.
Thirdly, the ``greenwave" patterns demonstrate that DRL algorithms produce
favorable solutions since the ``greenwave" policy shown in experiment results
is proved to be optimal in a specified traffic model (an avenue with multiple
cross streets). The delivered policies both in a single road intersection and a
grid road network demonstrate the scalability of DRL algorithms.

</details>


### [287] [Meta-Learning Loss Functions for Deep Neural Networks](https://arxiv.org/pdf/2406.09713)
*Christian Raymond*

Main category: cs.LG

TL;DR: Meta-learning improves AI performance by leveraging past experiences, focusing on optimizing loss functions for better learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Humans learn quickly from few examples, while AI systems require extensive data. Meta-learning addresses this gap by embedding inductive biases from past tasks.

Method: Explores meta-learning, particularly optimizing loss functions, to enhance learning efficiency and performance.

Result: Meta-learning, especially loss function optimization, can significantly improve AI learning systems.

Conclusion: Optimizing loss functions through meta-learning is a promising approach to enhance AI performance and efficiency.

Abstract: Humans can often quickly and efficiently solve complex new learning tasks
given only a small set of examples. In contrast, modern artificially
intelligent systems often require thousands or millions of observations in
order to solve even the most basic tasks. Meta-learning aims to resolve this
issue by leveraging past experiences from similar learning tasks to embed the
appropriate inductive biases into the learning system. Historically methods for
meta-learning components such as optimizers, parameter initializations, and
more have led to significant performance increases. This thesis aims to explore
the concept of meta-learning to improve performance, through the
often-overlooked component of the loss function. The loss function is a vital
component of a learning system, as it represents the primary learning
objective, where success is determined and quantified by the system's ability
to optimize for that objective successfully.

</details>


### [288] [A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification](https://arxiv.org/pdf/2410.22377)
*Flavio Corradini, Flavio Gerosa, Marco Gori, Carlo Lucheroni, Marco Piangerelli, Martina Zannotti*

Main category: cs.LG

TL;DR: A systematic review of spatio-temporal GNNs for time series analysis, covering models, datasets, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of GNNs for time series classification and forecasting, aiding future research.

Method: Conducted a database search, reviewed over 150 papers, and analyzed models, datasets, and results.

Result: First detailed comparison of spatio-temporal GNN models across domains, highlighting key findings and resources.

Conclusion: Identifies limitations like reproducibility and scalability, guiding future research in spatio-temporal GNNs.

Abstract: In recent years, spatio-temporal graph neural networks (GNNs) have attracted
considerable interest in the field of time series analysis, due to their
ability to capture dependencies among variables and across time points. The
objective of the presented systematic literature review is hence to provide a
comprehensive overview of the various modeling approaches and application
domains of GNNs for time series classification and forecasting. A database
search was conducted, and over 150 journal papers were selected for a detailed
examination of the current state-of-the-art in the field. This examination is
intended to offer to the reader a comprehensive collection of proposed models,
links to related source code, available datasets, benchmark models, and fitting
results. All this information is hoped to assist researchers in future studies.
To the best of our knowledge, this is the first systematic literature review
presenting a detailed comparison of the results of current spatio-temporal GNN
models in different domains. In addition, in its final part this review
discusses current limitations and challenges in the application of
spatio-temporal GNNs, such as comparability, reproducibility, explainability,
poor information capacity, and scalability.

</details>


### [289] [VecCity: A Taxonomy-guided Library for Map Entity Representation Learning](https://arxiv.org/pdf/2411.00874)
*Wentao Zhang, Jingyuan Wang, Yifan Yang, Leong Hou U*

Main category: cs.LG

TL;DR: The paper introduces VecCity, a taxonomy-driven library for MapRL, addressing fragmentation and lack of benchmarks by organizing models functionally and providing standardized evaluation tools.


<details>
  <summary>Details</summary>
Motivation: Existing MapRL research is fragmented by entity type and lacks unified benchmarks, hindering progress and reusability.

Method: Proposes a functional taxonomy for MapRL models and develops VecCity, a library with modular components for encoding, pre-training, fine-tuning, and evaluation.

Result: VecCity integrates datasets from nine cities, reproduces 21 models, and establishes standardized benchmarks, demonstrating effectiveness in streamlining development.

Conclusion: VecCity advances MapRL research by promoting modularity and reusability, offering a unified framework for innovation.

Abstract: Electronic maps consist of diverse entities, such as points of interest
(POIs), road networks, and land parcels, playing a vital role in applications
like ITS and LBS. Map entity representation learning (MapRL) generates
versatile and reusable data representations, providing essential tools for
efficiently managing and utilizing map entity data. Despite the progress in
MapRL, two key challenges constrain further development. First, existing
research is fragmented, with models classified by the type of map entity,
limiting the reusability of techniques across different tasks. Second, the lack
of unified benchmarks makes systematic evaluation and comparison of models
difficult. To address these challenges, we propose a novel taxonomy for MapRL
that organizes models based on functional module-such as encoders, pre-training
tasks, and downstream tasks-rather than by entity type. Building on this
taxonomy, we present a taxonomy-driven library, VecCity, which offers
easy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation.
The library integrates datasets from nine cities and reproduces 21 mainstream
MapRL models, establishing the first standardized benchmarks for the field.
VecCity also allows users to modify and extend models through modular
components, facilitating seamless experimentation. Our comprehensive
experiments cover multiple types of map entities and evaluate 21 VecCity
pre-built models across various downstream tasks. Experimental results
demonstrate the effectiveness of VecCity in streamlining model development and
provide insights into the impact of various components on performance. By
promoting modular design and reusability, VecCity offers a unified framework to
advance research and innovation in MapRL. The code is available at
https://github.com/Bigscity-VecCity/VecCity.

</details>


### [290] [Enhanced Photovoltaic Power Forecasting: An iTransformer and LSTM-Based Model Integrating Temporal and Covariate Interactions](https://arxiv.org/pdf/2412.02302)
*Guang Wu, Yun Wang, Qian Zhou, Ziyang Zhang*

Main category: cs.LG

TL;DR: A novel model combining iTransformer, LSTM, and cross-attention with KAN mapping improves PV power forecasting accuracy by capturing seasonal variations.


<details>
  <summary>Details</summary>
Motivation: Accurate PV power forecasting is essential for grid integration and energy management, but existing models struggle with complex relationships and temporal dynamics.

Method: The model uses iTransformer for target variable feature extraction, LSTM for covariate features, cross-attention for fusion, and KAN mapping for enhanced representation.

Result: Experiments on Australian datasets show the model effectively captures seasonal variations and improves forecasting accuracy.

Conclusion: The proposed model addresses limitations of existing methods and enhances PV power forecasting performance.

Abstract: Accurate photovoltaic (PV) power forecasting is critical for integrating
renewable energy sources into the grid, optimizing real-time energy management,
and ensuring energy reliability amidst increasing demand. However, existing
models often struggle with effectively capturing the complex relationships
between target variables and covariates, as well as the interactions between
temporal dynamics and multivariate data, leading to suboptimal forecasting
accuracy. To address these challenges, we propose a novel model architecture
that leverages the iTransformer for feature extraction from target variables
and employs long short-term memory (LSTM) to extract features from covariates.
A cross-attention mechanism is integrated to fuse the outputs of both models,
followed by a Kolmogorov-Arnold network (KAN) mapping for enhanced
representation. The effectiveness of the proposed model is validated using
publicly available datasets from Australia, with experiments conducted across
four seasons. Results demonstrate that the proposed model effectively capture
seasonal variations in PV power generation and improve forecasting accuracy.

</details>


### [291] [Vision-Language Model Selection and Reuse for Downstream Adaptation](https://arxiv.org/pdf/2501.18271)
*Hao-Zhe Tan, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo*

Main category: cs.LG

TL;DR: The paper introduces Model Label Learning (MLL), a method to efficiently select and reuse Vision-Language Models (VLMs) for downstream tasks by labeling, selecting, and reusing models based on their specialties.


<details>
  <summary>Details</summary>
Motivation: Selecting the best pre-trained VLM for specific tasks is challenging due to performance variability and evaluation limitations.

Method: MLL involves three modules: model labeling (describing VLM specialties), model selection (matching task requirements), and model reuse (ensemble application).

Result: The method is computationally efficient and scalable, validated on a benchmark of 49 VLMs and 17 datasets.

Conclusion: MLL effectively addresses the challenge of VLM selection and reuse, demonstrating promising results.

Abstract: Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular
across various visual tasks, and several open-sourced VLM variants have been
released. However, selecting the best-performing pre-trained VLM for a specific
downstream task is challenging since no single VLM can achieve promising
performance on all downstream tasks, and evaluating all available VLMs is
impossible due to time and data limitations. To address this problem, this
paper proposes a novel paradigm to select and reuse VLM for downstream tasks,
called Model Label Learning (MLL). The proposal contains three key modules:
\emph{model labeling}, which assigns labels to each VLM to describe their
specialty and utility; \emph{model selection}, which matches the requirements
of the target task with model labels; and \emph{model reuse}, which applies
selected VLMs to the target task in an ensemble manner. The proposal is highly
computationally efficient and growable since the model labeling process is
completed target task independent and the ability could grow with the number of
candidate VLMs. We also introduce a new benchmark for evaluating VLM selection
methods, including 49 VLMs and 17 target task datasets. Experimental results
clearly demonstrate the effectiveness of the proposed method for selecting and
reusing VLMs.

</details>


### [292] [LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty](https://arxiv.org/pdf/2503.18314)
*Christoforos N. Spartalis, Theodoros Semertzidis, Efstratios Gavves, Petros Daras*

Main category: cs.LG

TL;DR: LoTUS is a Machine Unlearning (MU) method that removes training sample influence from pre-trained models without retraining, outperforming baselines in efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To avoid costly retraining from scratch while eliminating the influence of specific training samples in pre-trained models.

Method: LoTUS smooths model prediction probabilities up to an information-theoretic bound, mitigating over-confidence from data memorization.

Result: Outperforms eight baselines on five datasets, including ImageNet1k, and introduces the RF-JSD metric for real-world evaluation.

Conclusion: LoTUS is efficient and effective for MU, especially in large-scale scenarios like ImageNet1k.

Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the
influence of training samples from pre-trained models, avoiding retraining from
scratch. LoTUS smooths the prediction probabilities of the model up to an
information-theoretic bound, mitigating its over-confidence stemming from data
memorization. We evaluate LoTUS on Transformer and ResNet18 models against
eight baselines across five public datasets. Beyond established MU benchmarks,
we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining
is impractical, simulating real-world conditions. Moreover, we introduce the
novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable
evaluation under real-world conditions. The experimental results show that
LoTUS outperforms state-of-the-art methods in terms of both efficiency and
effectiveness. Code: https://github.com/cspartalis/LoTUS.

</details>


### [293] [Machine Learning Cryptanalysis of a Quantum Random Number Generator](https://arxiv.org/pdf/1905.02342)
*Nhan Duy Truong, Jing Yan Haw, Syed Muhamad Assad, Ping Koy Lam, Omid Kavehei*

Main category: cs.LG

TL;DR: The paper investigates the impact of deterministic classical noise in optical continuous variable QRNGs using ML, showing ML's potential to benchmark RNG quality.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks exploit environmental info to predict RNG outputs, compromising QRNG integrity due to classical noise.

Method: Developed a predictive ML analysis to detect correlations from deterministic noise in QRNG stages, followed by filtering and randomness extraction.

Result: ML detects noise correlations; QRNG becomes robust after filtering. ML also benchmarks QRNG against congruential RNG.

Conclusion: ML is effective for assessing RNG quality, highlighting QRNG robustness post-processing.

Abstract: Random number generators (RNGs) that are crucial for cryptographic
applications have been the subject of adversarial attacks. These attacks
exploit environmental information to predict generated random numbers that are
supposed to be truly random and unpredictable. Though quantum random number
generators (QRNGs) are based on the intrinsic indeterministic nature of quantum
properties, the presence of classical noise in the measurement process
compromises the integrity of a QRNG. In this paper, we develop a predictive
machine learning (ML) analysis to investigate the impact of deterministic
classical noise in different stages of an optical continuous variable QRNG. Our
ML model successfully detects inherent correlations when the deterministic
noise sources are prominent. After appropriate filtering and randomness
extraction processes are introduced, our QRNG system, in turn, demonstrates its
robustness against ML. We further demonstrate the robustness of our ML approach
by applying it to uniformly distributed random numbers from the QRNG and a
congruential RNG. Hence, our result shows that ML has potentials in
benchmarking the quality of RNG devices.

</details>


### [294] [FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training](https://arxiv.org/pdf/2504.03783)
*Haoyuan Li, Mathias Funk, Jindong Wang, Aaqib Saeed*

Main category: cs.LG

TL;DR: FAST, a two-pass Federated Active Learning framework, reduces communication costs and annotator effort by using foundation models for weak labeling and refining uncertain samples, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High annotation costs and communication-intensive processes limit real-world Federated Active Learning (FAL) deployments, especially in cross-silo settings.

Method: FAST employs a two-pass approach: weak labeling with foundation models followed by refinement of uncertain samples, integrating steps into a streamlined workflow.

Result: FAST outperforms existing FAL methods by 4.36% on average and reduces communication rounds eightfold under a 5% labeling budget.

Conclusion: FAST offers an efficient solution for FAL by minimizing communication and annotation overhead while maintaining performance.

Abstract: Federated Active Learning (FAL) has emerged as a promising framework to
leverage large quantities of unlabeled data across distributed clients while
preserving data privacy. However, real-world deployments remain limited by high
annotation costs and communication-intensive sampling processes, particularly
in a cross-silo setting, when clients possess substantial local datasets. This
paper addresses the crucial question: What is the best practice to reduce
communication costs in human-in-the-loop learning with minimal annotator
effort? Existing FAL methods typically rely on iterative annotation processes
that separate active sampling from federated updates, leading to multiple
rounds of expensive communication and annotation. In response, we introduce
FAST, a two-pass FAL framework that harnesses foundation models for weak
labeling in a preliminary pass, followed by a refinement pass focused
exclusively on the most uncertain samples. By leveraging representation
knowledge from foundation models and integrating refinement steps into a
streamlined workflow, FAST substantially reduces the overhead incurred by
iterative active sampling. Extensive experiments on diverse medical and natural
image benchmarks demonstrate that FAST outperforms existing FAL methods by an
average of 4.36% while reducing communication rounds eightfold under a limited
5% labeling budget.

</details>


### [295] [A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods](https://arxiv.org/pdf/2304.01215)
*Alessio Brini, Elisa Giovannini, Elia Smaniotto*

Main category: cs.LG

TL;DR: Machine learning models forecast honey production in Italy, using weather data to identify key drivers and improve risk management for beekeepers.


<details>
  <summary>Details</summary>
Motivation: Address production fluctuations in beekeeping due to climate change by predicting honey yields using weather variables.

Method: Train and compare linear and nonlinear machine learning models on 2022 hive-level and weather data, then ensemble models for accuracy.

Result: Identified key weather-related drivers of honey production; ensemble models may improve forecast accuracy.

Conclusion: Insights aid beekeepers in risk management and could inform insurance products for poor harvests.

Abstract: The beekeeping sector has experienced significant production fluctuations in
recent years, largely due to increasingly frequent adverse weather events
linked to climate change. These events can severely affect the environment,
reducing its suitability for bee activity. We conduct a forecasting analysis of
honey production across Italy using a range of machine learning models, with a
particular focus on weather-related variables as key predictors. Our analysis
relies on a dataset collected in 2022, which combines hive-level observations
with detailed weather data. We train and compare several linear and nonlinear
models, evaluating both their predictive accuracy and interpretability. By
examining model explanations, we identify the main drivers of honey production.
We also ensemble models from different families to assess whether combining
predictions improves forecast accuracy. These insights support beekeepers in
managing production risks and may inform the development of insurance products
against unexpected losses due to poor harvests.

</details>


### [296] [Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals](https://arxiv.org/pdf/2504.06987)
*Sanyam Paresh Shah, Abdullah Mamun, Shovito Barua Soumma, Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: The paper proposes MetaBoost, a hybrid ML framework for predicting Metabolic Syndrome (MetS), addressing data imbalance and methodological issues. It outperforms existing techniques and identifies key risk factors like blood glucose and triglycerides.


<details>
  <summary>Details</summary>
Motivation: MetS prediction is challenging due to class imbalance, data scarcity, and inconsistent methods. The study aims to improve accuracy and provide actionable insights.

Method: Evaluated ML models (XGBoost, Random Forest, TabNet) with data balancing techniques (ROS, SMOTE, ADASYN, CTGAN) and introduced MetaBoost, a hybrid framework. Conducted counterfactual and probabilistic analyses.

Result: MetaBoost improved accuracy by 1.87%. Blood glucose and triglycerides were the most modified and predictive features.

Conclusion: The study enhances MetS prediction methodology and offers clinical insights, demonstrating ML's potential in reducing MetS-related health burdens.

Abstract: Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that
significantly increases the risk of cardiovascular diseases and type 2
diabetes. Despite its global prevalence, accurate prediction of MetS remains
challenging due to issues such as class imbalance, data scarcity, and
methodological inconsistencies in existing studies. In this paper, we address
these challenges by systematically evaluating and optimizing machine learning
(ML) models for MetS prediction, leveraging advanced data balancing techniques
and counterfactual analysis. Multiple ML models, including XGBoost, Random
Forest, TabNet, etc., were trained and compared under various data balancing
techniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN.
Additionally, we introduce MetaBoost, a novel hybrid framework that integrates
SMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted
averaging and iterative weight tuning to enhance the model's performance
(achieving up to a 1.87% accuracy improvement over individual balancing
techniques). A comprehensive counterfactual analysis is conducted to quantify
the feature-level changes required to shift individuals from high-risk to
low-risk categories. The results indicate that blood glucose (50.3%) and
triglycerides (46.7%) were the most frequently modified features, highlighting
their clinical significance in MetS risk reduction. Additionally, probabilistic
analysis shows elevated blood glucose (85.5% likelihood) and triglycerides
(74.9% posterior probability) as the strongest predictors. This study not only
advances the methodological rigor of MetS prediction but also provides
actionable insights for clinicians and researchers, highlighting the potential
of ML in mitigating the public health burden of metabolic syndrome.

</details>


### [297] [Disjunctive Branch-And-Bound for Certifiably Optimal Low-Rank Matrix Completion](https://arxiv.org/pdf/2305.12292)
*Dimitris Bertsimas, Ryan Cory-Wright, Sean Lo, Jean Pauphilet*

Main category: cs.LG

TL;DR: The paper proposes a method for low-rank matrix completion with optimality guarantees, using convex relaxations and branch-and-bound to achieve certifiable optimality.


<details>
  <summary>Details</summary>
Motivation: Existing matrix completion methods lack optimality guarantees despite being scalable and effective. The authors aim to address this gap.

Method: Reformulates matrix completion as convex problems over projection matrices, introduces disjunctive branch-and-bound, and derives new convex relaxations by decomposing low-rank matrices.

Result: Numerical experiments show a two-order reduction in optimality gap and solve problems to certifiable optimality for specific dimensions. Test error improves by 2%--50%.

Conclusion: The proposed method achieves certifiable optimality in low-rank matrix completion, significantly outperforming existing heuristics.

Abstract: Low-rank matrix completion consists of computing a matrix of minimal
complexity that recovers a given set of observations as accurately as possible.
Unfortunately, existing methods for matrix completion are heuristics that,
while highly scalable and often identifying high-quality solutions, do not
possess any optimality guarantees. We reexamine matrix completion with an
optimality-oriented eye. We reformulate low-rank matrix completion problems as
convex problems over the non-convex set of projection matrices and implement a
disjunctive branch-and-bound scheme that solves them to certifiable optimality.
Further, we derive a novel and often near-exact class of convex relaxations by
decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing
that two-by-two minors in each rank-one matrix have determinant zero. In
numerical experiments, our new convex relaxations decrease the optimality gap
by two orders of magnitude compared to existing attempts, and our disjunctive
branch-and-bound scheme solves $n \times m$ rank-$r$ matrix completion problems
to certifiable optimality or near optimality in hours for $\max \{m, n\} \leq
2500$ and $r \leq 5$. Moreover, this improvement in the training error
translates into an average $2\%$--$50\%$ improvement in the test set error.

</details>


### [298] [A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks](https://arxiv.org/pdf/2504.12806)
*Georgios Papadopoulos, Shaltiel Eloul, Yash Satsangi, Jamie Heredge, Niraj Kumar, Chun-Fu Chen, Marco Pistoia*

Main category: cs.LG

TL;DR: A method to reconstruct input data from VQNN gradients using gradient inversion, finite difference, and adaptive filtering, optimized with Kalman filter for convergence.


<details>
  <summary>Details</summary>
Motivation: VQNNs' loss landscapes have exponentially growing local minima, making gradient-based information recovery harder than in classical NNs.

Method: Combines gradient inversion, finite difference, adaptive low-pass filtering, and Kalman filter optimization.

Result: Successfully inverts batch-trained data if the VQNN is over-parameterized.

Conclusion: The scheme effectively recovers input data from VQNN gradients despite challenging loss landscapes.

Abstract: The loss landscape of Variational Quantum Neural Networks (VQNNs) is
characterized by local minima that grow exponentially with increasing qubits.
Because of this, it is more challenging to recover information from model
gradients during training compared to classical Neural Networks (NNs). In this
paper we present a numerical scheme that successfully reconstructs input
training, real-world, practical data from trainable VQNNs' gradients. Our
scheme is based on gradient inversion that works by combining gradients
estimation with the finite difference method and adaptive low-pass filtering.
The scheme is further optimized with Kalman filter to obtain efficient
convergence. Our experiments show that our algorithm can invert even
batch-trained data, given the VQNN model is sufficiently over-parameterized.

</details>


### [299] [Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models](https://arxiv.org/pdf/2308.12563)
*Thi Kieu Khanh Ho, Narges Armanfard*

Main category: cs.LG

TL;DR: TSAD-C is a novel unsupervised time-series anomaly detection method that handles noisy training data, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Real-world anomaly detection often involves noisy training data, a challenge overlooked in controlled academic settings.

Method: TSAD-C uses three modules: Decontaminator for noise removal, Long-range Dependency Modeling for capturing patterns, and Anomaly Scoring for detection.

Result: TSAD-C outperforms existing methods on four diverse datasets, setting a new benchmark.

Conclusion: TSAD-C advances unsupervised TSAD by effectively addressing noisy training data, achieving state-of-the-art performance.

Abstract: Mainstream unsupervised anomaly detection algorithms often excel in academic
datasets, yet their real-world performance is restricted due to the controlled
experimental conditions involving clean training data. Addressing the challenge
of training with noise, a prevalent issue in practical anomaly detection, is
frequently overlooked. In a pioneering endeavor, this study delves into the
realm of label-level noise within sensory time-series anomaly detection (TSAD).
This paper presents a novel and practical end-to-end unsupervised TSAD when the
training data is contaminated with anomalies. The introduced approach, called
TSAD-C, is devoid of access to abnormality labels during the training phase.
TSAD-C encompasses three core modules: a Decontaminator to rectify anomalies
(aka noise) present during training, a Long-range Variable Dependency Modeling
module to capture long-term intra- and inter-variable dependencies within the
decontaminated data that is considered as a surrogate of the pure normal data,
and an Anomaly Scoring module to detect anomalies from all types. Our extensive
experiments conducted on four reliable and diverse datasets conclusively
demonstrate that TSAD-C surpasses existing methodologies, thus establishing a
new state-of-the-art in the TSAD field.

</details>


### [300] [Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder](https://arxiv.org/pdf/2310.10745)
*Priyam Gupta, Peter J. Schmid, Denis Sipp, Taraneh Sayadi, Georgios Rigas*

Main category: cs.LG

TL;DR: The paper introduces the Mori-Zwanzig autoencoder (MZ-AE) to improve Koopman operator approximation by combining nonlinear autoencoders with non-Markovian corrections, enhancing predictive accuracy for complex dynamics.


<details>
  <summary>Details</summary>
Motivation: Data-driven methods for approximating the Koopman operator face challenges like observable selection and dimensionality reduction, limiting their accuracy for complex systems.

Method: The MZ-AE method uses a nonlinear autoencoder to extract observables and integrates a non-Markovian correction via the Mori-Zwanzig formalism for robust approximation.

Result: The approach improves predictive accuracy for flow around a cylinder and offers low-dimensional approximations for Kuramoto-Sivashinsky dynamics with strong short- and long-term performance.

Conclusion: MZ-AE bridges data-driven techniques and Koopman theory, advancing the understanding and prediction of nonlinear dynamics.

Abstract: The Koopman operator presents an attractive approach to achieve global
linearization of nonlinear systems, making it a valuable method for simplifying
the understanding of complex dynamics. While data-driven methodologies have
exhibited promise in approximating finite Koopman operators, they grapple with
various challenges, such as the judicious selection of observables,
dimensionality reduction, and the ability to predict complex system behaviours
accurately. This study presents a novel approach termed Mori-Zwanzig
autoencoder (MZ-AE) to robustly approximate the Koopman operator in
low-dimensional spaces. The proposed method leverages a nonlinear autoencoder
to extract key observables for approximating a finite invariant Koopman
subspace and integrates a non-Markovian correction mechanism using the
Mori-Zwanzig formalism. Consequently, this approach yields an approximate
closure of the dynamics within the latent manifold of the nonlinear
autoencoder, thereby enhancing the accuracy and stability of the Koopman
operator approximation. Demonstrations showcase the technique's improved
predictive capability for flow around a cylinder. It also provides a low
dimensional approximation for Kuramoto-Sivashinsky (KS) with promising
short-term predictability and robust long-term statistical performance. By
bridging the gap between data-driven techniques and the mathematical
foundations of Koopman theory, MZ-AE offers a promising avenue for improved
understanding and prediction of complex nonlinear dynamics.

</details>


### [301] [Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse](https://arxiv.org/pdf/2311.05139)
*Ruijie Jiang, Thuan Nguyen, Shuchin Aeron, Prakash Ishwar*

Main category: cs.LG

TL;DR: The paper proves that Supervised, Hard-Supervised, and Unsupervised Contrastive Learning losses are minimized by Neural-Collapse (NC) representations, with simpler proofs and broader applicability than prior work. Empirical results show Adam optimization with hard-negatives and feature normalization achieves NC, while their absence leads to Dimensional-Collapse.


<details>
  <summary>Details</summary>
Motivation: To theoretically and empirically demonstrate that contrastive learning losses (SCL, HSCL, UCL) lead to Neural-Collapse (NC) representations, and to highlight the role of hard-negatives and feature normalization in avoiding Dimensional-Collapse (DC).

Method: Theoretical proofs for loss minimization under NC conditions, empirical validation using Adam optimization with hard-negatives and feature normalization, and comparison of results with and without these components.

Result: Losses for SCL, HSCL, and UCL are minimized by NC representations. Hard-negatives and feature normalization are crucial for achieving NC, while their absence leads to DC.

Conclusion: Hard-negative sampling and feature normalization are essential for contrastive learning to achieve NC. The work opens theoretical problems for future research.

Abstract: For a widely-studied data model and general loss and sample-hardening
functions we prove that the losses of Supervised Contrastive Learning (SCL),
Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by
representations that exhibit Neural-Collapse (NC), i.e., the class means form
an Equiangular Tight Frame (ETF) and data from the same class are mapped to the
same representation. We also prove that for any representation mapping, the
HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and
UCL losses. In contrast to existing literature, our theoretical results for SCL
do not require class-conditional independence of augmented views and work for a
general loss function class that includes the widely used InfoNCE loss
function. Moreover, our proofs are simpler, compact, and transparent. Similar
to existing literature, our theoretical claims also hold for the practical
scenario where batching is used for optimization. We empirically demonstrate,
for the first time, that Adam optimization (with batching) of HSCL and HUCL
losses with random initialization and suitable hardness levels can indeed
converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature
normalization. Without incorporating hard-negatives or feature normalization,
however, the representations learned via Adam suffer from Dimensional-Collapse
(DC) and fail to attain the NC-geometry. These results exemplify the role of
hard-negative sampling in contrastive representation learning and we conclude
with several open theoretical problems for future work. The code can be found
at https://github.com/rjiang03/HCL/tree/main

</details>


### [302] [TransAxx: Efficient Transformers with Approximate Computing](https://arxiv.org/pdf/2402.07545)
*Dimitrios Danopoulos, Georgios Zervakis, Dimitrios Soudris, JÃ¶rg Henkel*

Main category: cs.LG

TL;DR: TransAxx, a PyTorch-based framework, enables approximate arithmetic for ViT models, improving efficiency without compromising performance.


<details>
  <summary>Details</summary>
Motivation: ViT models are computationally demanding, limiting their use on low-power devices. Approximate multipliers, though used in DNN accelerators, haven't been explored for ViTs.

Method: TransAxx integrates approximate arithmetic for ViTs, analyzes sensitivity on ImageNet, and uses MCTS to optimize hardware configurations.

Result: The framework achieves significant accuracy-power trade-offs, enhancing efficiency for ViT models.

Conclusion: TransAxx effectively addresses ViT computational demands, offering a practical solution for low-power applications.

Abstract: Vision Transformer (ViT) models which were recently introduced by the
transformer architecture have shown to be very competitive and often become a
popular alternative to Convolutional Neural Networks (CNNs). However, the high
computational requirements of these models limit their practical applicability
especially on low-power devices. Current state-of-the-art employs approximate
multipliers to address the highly increased compute demands of DNN accelerators
but no prior research has explored their use on ViT models. In this work we
propose TransAxx, a framework based on the popular PyTorch library that enables
fast inherent support for approximate arithmetic to seamlessly evaluate the
impact of approximate computing on DNNs such as ViT models. Using TransAxx we
analyze the sensitivity of transformer models on the ImageNet dataset to
approximate multiplications and perform approximate-aware finetuning to regain
accuracy. Furthermore, we propose a methodology to generate approximate
accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)
algorithm to efficiently search the space of possible configurations using a
hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy
of our methodology in achieving significant trade-offs between accuracy and
power, resulting in substantial gains without compromising on performance.

</details>


### [303] [Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows](https://arxiv.org/pdf/2405.17211)
*Shuhao Cao, Francesco Brarda, Ruipeng Li, Yuanzhe Xi*

Main category: cs.LG

TL;DR: A new learning framework improves Fourier Neural Operators (FNOs) for spatiotemporal PDEs by introducing spectral fine-tuning and a convex loss function, enhancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neural networks for PDEs are costly and lack accuracy; the paper aims to address these issues with a novel framework.

Method: Proposes spatiotemporal adaptation for FNOs, spectral fine-tuning without frequency truncation, and a convex loss function using a negative Sobolev norm.

Result: Significant improvements in computational efficiency and accuracy on Navier-Stokes benchmarks compared to traditional methods.

Conclusion: The framework advances operator learning for PDEs, offering a practical and efficient solution with publicly available code.

Abstract: Recent advancements in operator-type neural networks have shown promising
results in approximating the solutions of spatiotemporal Partial Differential
Equations (PDEs). However, these neural networks often entail considerable
training expenses, and may not always achieve the desired accuracy required in
many scientific and engineering disciplines. In this paper, we propose a new
learning framework to address these issues. A new spatiotemporal adaptation is
proposed to generalize any Fourier Neural Operator (FNO) variant to learn maps
between Bochner spaces, which can perform an arbitrary-length temporal
super-resolution for the first time. To better exploit this capacity, a new
paradigm is proposed to refine the commonly adopted end-to-end neural operator
training and evaluations with the help from the wisdom from traditional
numerical PDE theory and techniques. Specifically, in the learning problems for
the turbulent flow modeled by the Navier-Stokes Equations (NSE), the proposed
paradigm trains an FNO only for a few epochs. Then, only the newly proposed
spatiotemporal spectral convolution layer is fine-tuned without the frequency
truncation. The spectral fine-tuning loss function uses a negative Sobolev norm
for the first time in operator learning, defined through a reliable
functional-type a posteriori error estimator whose evaluation is exact thanks
to the Parseval identity. Moreover, unlike the difficult nonconvex optimization
problems in the end-to-end training, this fine-tuning loss is convex. Numerical
experiments on commonly used NSE benchmarks demonstrate significant
improvements in both computational efficiency and accuracy, compared to
end-to-end evaluation and traditional numerical PDE solvers under certain
conditions. The source code is publicly available at
https://github.com/scaomath/torch-cfd.

</details>


### [304] [Mixed-Curvature Decision Trees and Random Forests](https://arxiv.org/pdf/2406.05227)
*Philippe Chlenski, Quentin Chu, Itsik Pe'er*

Main category: cs.LG

TL;DR: Extends decision trees and random forests to product space manifolds for improved classification and regression.


<details>
  <summary>Details</summary>
Motivation: Existing classifiers for product spaces are limited to single linear boundaries, and no regressors exist.

Method: Extends decision tree and random forest algorithms to handle Cartesian products of Euclidean, hyperspherical, and hyperbolic manifolds.

Result: Demonstrates superior accuracy over Euclidean methods in various constant-curvature and product manifolds.

Conclusion: Provides a simple, expressive solution for classification and regression in product manifolds.

Abstract: We extend decision tree and random forest algorithms to product space
manifolds: Cartesian products of Euclidean, hyperspherical, and hyperbolic
manifolds. Such spaces have extremely expressive geometries capable of
representing many arrangements of distances with low metric distortion. To
date, all classifiers for product spaces fit a single linear decision boundary,
and no regressor has been described. Our method enables a simple, expressive
method for classification and regression in product manifolds. We demonstrate
the superior accuracy of our tool compared to Euclidean methods operating in
the ambient space or the tangent plane of the manifold across a range of
constant-curvature and product manifolds. Code for our implementation and
experiments is available at https://github.com/pchlenski/embedders.

</details>


### [305] [Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks](https://arxiv.org/pdf/2505.02369)
*Juyoung Yun*

Main category: cs.LG

TL;DR: ZSharp improves SAM by using Z-score normalization and percentile-based filtering to focus on statistically significant gradient components, enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: SAM perturbs parameters using the entire gradient vector, including less significant components, which may not optimize sharpness effectively.

Method: ZSharp applies layer-wise Z-score normalization and percentile filtering to select significant gradient components for perturbation, retaining SAM's two-phase structure.

Result: ZSharp outperforms SAM, ASAM, and Friendly-SAM in test accuracy across CIFAR-10, CIFAR-100, and Tiny-ImageNet with various models.

Conclusion: ZSharp's gradient filtering enhances sharpness sensitivity, leading to better generalization in neural network training.

Abstract: Sharpness-Aware Minimization (SAM) improves neural network generalization by
optimizing the worst-case loss within a neighborhood of parameters, yet it
perturbs parameters using the entire gradient vector, including components with
low statistical significance. We introduce ZSharp, a refined sharpness-aware
optimization method that incorporates layer-wise Z-score normalization followed
by percentile-based filtering. This process selects only the most statistically
significant gradient components-those with large standardized magnitudes-for
constructing the perturbation direction. ZSharp retains the standard two-phase
SAM structure of ascent and descent while modifying the ascent step to focus on
sharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10,
CIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and
Vision Transformers. Across all architectures and datasets, ZSharp consistently
achieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These
results indicate that Z-score-based gradient filtering can enhance the
sharpness sensitivity of the update direction, leading to improved
generalization in deep neural network training.

</details>


### [306] [Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing](https://arxiv.org/pdf/2409.16832)
*Lyudong Jin, Ming Tang, Jiayu Pan, Meng Zhang, Hao Wang*

Main category: cs.LG

TL;DR: The paper proposes a fractional reinforcement learning framework to optimize task updating and offloading in mobile edge computing, reducing Age of Information (AoI) by up to 52.6%.


<details>
  <summary>Details</summary>
Motivation: To address the timeliness challenges in computational-intensive updates for cyber-physical systems (CPS) using mobile edge computing (MEC).

Method: Develops fractional single-agent and multi-agent RL frameworks, including an asynchronous model-free algorithm for semi-Markov games.

Result: Achieves a 52.6% reduction in average AoI compared to baseline algorithms.

Conclusion: The proposed framework effectively minimizes AoI in dynamic edge computing environments, demonstrating significant performance improvements.

Abstract: In the realm of emerging real-time networked applications like cyber-physical
systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for
evaluating the timeliness. To meet the high computational demands, such as
those in intelligent manufacturing within CPS, mobile edge computing (MEC)
presents a promising solution for optimizing computing and reducing AoI. In
this work, we study the timeliness of computational-intensive updates and
explores jointly optimize the task updating and offloading policies to minimize
AoI. Specifically, we consider edge load dynamics and formulate a task
scheduling problem to minimize the expected time-average AoI. The fractional
objective introduced by AoI and the semi-Markov game nature of the problem
render this challenge particularly difficult, with existing approaches not
directly applicable. To this end, we present a comprehensive framework to
fractional reinforcement learning (RL). We first introduce a fractional
single-agent RL framework and prove its linear convergence. We then extend this
to a fractional multi-agent RL framework with a convergence analysis. To tackle
the challenge of asynchronous control in semi-Markov game, we further design an
asynchronous model-free fractional multi-agent RL algorithm, where each device
makes scheduling decisions with the hybrid action space without knowing the
system dynamics and decisions of other devices. Experimental results show that
our proposed algorithms reduce the average AoI by up to 52.6% compared with the
best baseline algorithm in our experiments.

</details>


### [307] [Batched Bayesian optimization by maximizing the probability of including the optimum](https://arxiv.org/pdf/2410.06333)
*Jenna Fromer, Runzhong Wang, Mrunali Manjrekar, Austin Tripp, JosÃ© Miguel HernÃ¡ndez-Lobato, Connor W. Coley*

Main category: cs.LG

TL;DR: Proposes qPO, a batched Bayesian optimization method for molecular design, focusing on pure exploitation to efficiently identify top compounds.


<details>
  <summary>Details</summary>
Motivation: Existing batch BO methods balance exploration-exploitation but require approximations. qPO aims to simplify by focusing on pure exploitation.

Method: qPO maximizes the probability the batch includes the true optimum, using additive scores to avoid combinatorial challenges.

Result: Empirical evidence shows qPO is competitive with state-of-the-art batch BO methods.

Conclusion: qPO offers a simpler, effective alternative for batched BO in molecular design, complementing existing methods.

Abstract: Batched Bayesian optimization (BO) can accelerate molecular design by
efficiently identifying top-performing compounds from a large chemical library.
Existing acquisition strategies for batch design in BO aim to balance
exploration and exploitation. This often involves optimizing non-additive batch
acquisition functions, necessitating approximation via myopic construction
and/or diversity heuristics. In this work, we propose an acquisition strategy
for discrete optimization that is motivated by pure exploitation, qPO
(multipoint Probability of Optimality). qPO maximizes the probability that the
batch includes the true optimum, which is expressible as the sum over
individual acquisition scores and thereby circumvents the combinatorial
challenge of optimizing a batch acquisition function. We differentiate the
proposed strategy from parallel Thompson sampling and discuss how it implicitly
captures diversity. Finally, we apply our method to the model-guided
exploration of large chemical libraries and provide empirical evidence that it
is competitive with and complements other state-of-the-art methods in batched
Bayesian optimization.

</details>


### [308] [Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling](https://arxiv.org/pdf/2410.06397)
*Matthew X. Burns, Qingyuan Hou, Michael C. Huang*

Main category: cs.LG

TL;DR: The paper provides non-asymptotic convergence guarantees for hybrid large-neighborhood local search (LNLS) algorithms in analog dynamical accelerators (DXs) by linking them to block Langevin Diffusion (BLD).


<details>
  <summary>Details</summary>
Motivation: Hybrid LNLS lacks non-asymptotic convergence guarantees and principled hyperparameter selection, limiting its practical use in cross-device training and inference.

Method: The work reduces hybrid LNLS to BLD algorithms, adapting classical sampling theory to prove exponential KL-divergence convergence for randomized and cyclic block selection strategies. It also bounds 2-Wasserstein bias under finite device variation.

Result: The paper establishes explicit bounds on performance in terms of device variation, step duration, noise strength, and function parameters, providing a closed-form expression for hyperparameter tuning.

Conclusion: The BLD model bridges theory and analog computing, offering practical insights for hyperparameter selection and performance optimization in hybrid LNLS.

Abstract: Analog dynamical accelerators (DXs) are a growing sub-field in computer
architecture research, offering order-of-magnitude gains in power efficiency
and latency over traditional digital methods in several machine learning,
optimization, and sampling tasks. However, limited-capacity accelerators
require hybrid analog/digital algorithms to solve real-world problems, commonly
using large-neighborhood local search (LNLS) frameworks. Unlike fully digital
algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no
principled hyperparameter selection schemes, particularly limiting cross-device
training and inference.
  In this work, we provide non-asymptotic convergence guarantees for hybrid
LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools
from classical sampling theory, we prove exponential KL-divergence convergence
for randomized and cyclic block selection strategies using ideal DXs. With
finite device variation, we provide explicit bounds on the 2-Wasserstein bias
in terms of step duration, noise strength, and function parameters. Our BLD
model provides a key link between established theory and novel computing
platforms, and our theoretical results provide a closed-form expression linking
device variation, algorithm hyperparameters, and performance.

</details>


### [309] [Conditional Lagrangian Wasserstein Flow for Time Series Imputation](https://arxiv.org/pdf/2410.07550)
*Weizhu Qian, Dalin Zhang, Yan Zhao, Yunyao Cheng*

Main category: cs.LG

TL;DR: A novel method, Conditional Lagrangian Wasserstein Flow (CLWF), is proposed for time series imputation, addressing slow convergence in diffusion models by minimizing kinetic energy and integrating a task-specific potential function.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of diffusion model-based imputation methods, particularly slow convergence during inference.

Method: CLWF learns velocity by minimizing kinetic energy, following Lagrangian mechanics, and integrates a time-dependent denoising autoencoder to estimate the gradient of a potential function, reducing sampling variance.

Result: The method demonstrates competitive performance against state-of-the-art imputation approaches.

Conclusion: CLWF is an effective solution for time series imputation, outperforming existing methods.

Abstract: Time series imputation is important for numerous real-world applications. To
overcome the limitations of diffusion model-based imputation methods, e.g.,
slow convergence in inference, we propose a novel method for time series
imputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF).
Following the principle of least action in Lagrangian mechanics, we learn the
velocity by minimizing the corresponding kinetic energy. Moreover, to enhance
the model's performance, we estimate the gradient of a task-specific potential
function using a time-dependent denoising autoencoder and integrate it into the
base estimator to reduce the sampling variance. Finally, the proposed method
demonstrates competitive performance compared to other state-of-the-art
imputation approaches.

</details>


### [310] [Reward-free World Models for Online Imitation Learning](https://arxiv.org/pdf/2410.14081)
*Shangzhe Li, Zhiao Huang, Hao Su*

Main category: cs.LG

TL;DR: A novel online imitation learning method using reward-free world models in latent spaces achieves stable, expert-level performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Prior online IL methods struggle with high-dimensional inputs and complex dynamics, necessitating a more efficient and stable approach.

Method: Leverages reward-free world models in latent spaces, uses inverse soft-Q learning for optimization, and employs learned latent dynamics for planning.

Result: Demonstrates superior performance on benchmarks like DMControl, MyoSuite, and ManiSkill2.

Conclusion: The proposed method effectively addresses instability and inefficiency in complex tasks, outperforming existing approaches.

Abstract: Imitation learning (IL) enables agents to acquire skills directly from expert
demonstrations, providing a compelling alternative to reinforcement learning.
However, prior online IL approaches struggle with complex tasks characterized
by high-dimensional inputs and complex dynamics. In this work, we propose a
novel approach to online imitation learning that leverages reward-free world
models. Our method learns environmental dynamics entirely in latent spaces
without reconstruction, enabling efficient and accurate modeling. We adopt the
inverse soft-Q learning objective, reformulating the optimization process in
the Q-policy space to mitigate the instability associated with traditional
optimization in the reward-policy space. By employing a learned latent dynamics
model and planning for control, our approach consistently achieves stable,
expert-level performance in tasks with high-dimensional observation or action
spaces and intricate dynamics. We evaluate our method on a diverse set of
benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating
superior empirical performance compared to existing approaches.

</details>


### [311] [Harnessing Causality in Reinforcement Learning With Bagged Decision Times](https://arxiv.org/pdf/2410.14659)
*Daiqi Gao, Hsin-Yu Lai, Predrag Klasnja, Susan A. Murphy*

Main category: cs.LG

TL;DR: The paper proposes an online RL algorithm for problems with bagged decision times, using causal DAGs to handle non-Markovian transitions and formulating it as a periodic MDP.


<details>
  <summary>Details</summary>
Motivation: Addressing RL problems where actions within a finite sequence (bag) jointly impact a single reward, common in scenarios like mobile health interventions.

Method: Utilizes expert-provided causal DAGs to construct states as Bayesian sufficient statistics, transforming the problem into a periodic MDP. Generalizes Bellman equations for stationary MDPs to periodic ones.

Result: The constructed state achieves the maximal optimal value function. The method is validated on testbed variants from real mobile health data.

Conclusion: The approach effectively handles non-Markovian and non-stationary transitions in bagged decision-time problems, demonstrating practical utility in mobile health.

Abstract: We consider reinforcement learning (RL) for a class of problems with bagged
decision times. A bag contains a finite sequence of consecutive decision times.
The transition dynamics are non-Markovian and non-stationary within a bag. All
actions within a bag jointly impact a single reward, observed at the end of the
bag. For example, in mobile health, multiple activity suggestions in a day
collectively affect a user's daily commitment to being active. Our goal is to
develop an online RL algorithm to maximize the discounted sum of the
bag-specific rewards. To handle non-Markovian transitions within a bag, we
utilize an expert-provided causal directed acyclic graph (DAG). Based on the
DAG, we construct states as a dynamical Bayesian sufficient statistic of the
observed history, which results in Markov state transitions within and across
bags. We then formulate this problem as a periodic Markov decision process
(MDP) that allows non-stationarity within a period. An online RL algorithm
based on Bellman equations for stationary MDPs is generalized to handle
periodic MDPs. We show that our constructed state achieves the maximal optimal
value function among all state constructions for a periodic MDP. Finally, we
evaluate the proposed method on testbed variants built from real data in a
mobile health clinical trial.

</details>


### [312] [SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics](https://arxiv.org/pdf/2412.01124)
*Qingtian Zhu, Yumin Zheng, Yuling Sang, Yifan Zhan, Ziyan Zhu, Jun Ding, Yinqiang Zheng*

Main category: cs.LG

TL;DR: SUICA is a tool using Implicit Neural Representations (INRs) and a graph-augmented Autoencoder to model Spatial Transcriptomics (ST) data continuously and compactly, improving spatial density and gene expression.


<details>
  <summary>Details</summary>
Motivation: ST data is challenging due to its discrete spatial distribution and high dimensionality. SUICA aims to model it effectively.

Method: SUICA combines INRs with a graph-augmented Autoencoder, uses regression-by-classification for skewed data, and employs classification-based loss functions.

Result: SUICA outperforms conventional INR variants and SOTA methods in numerical fidelity, statistical correlation, and bio-conservation. It also enriches gene signatures.

Conclusion: SUICA effectively models ST data, enhancing spatial and gene expression analysis, and benefits downstream applications.

Abstract: Spatial Transcriptomics (ST) is a method that captures gene expression
profiles aligned with spatial coordinates. The discrete spatial distribution
and the super-high dimensional sequencing results make ST data challenging to
be modeled effectively. In this paper, we manage to model ST in a continuous
and compact manner by the proposed tool, SUICA, empowered by the great
approximation capability of Implicit Neural Representations (INRs) that can
enhance both the spatial density and the gene expression. Concretely within the
proposed SUICA, we incorporate a graph-augmented Autoencoder to effectively
model the context information of the unstructured spots and provide informative
embeddings that are structure-aware for spatial mapping. We also tackle the
extremely skewed distribution in a regression-by-classification fashion and
enforce classification-based loss functions for the optimization of SUICA. By
extensive experiments of a wide range of common ST platforms under varying
degradations, SUICA outperforms both conventional INR variants and SOTA methods
regarding numerical fidelity, statistical correlation, and bio-conservation.
The prediction by SUICA also showcases amplified gene signatures that enriches
the bio-conservation of the raw data and benefits subsequent analysis. The code
is available at https://github.com/Szym29/SUICA.

</details>


### [313] [Information-Geometric Barycenters for Bayesian Federated Learning](https://arxiv.org/pdf/2412.11646)
*Nour Jamoussi, Giuseppe Serra, Photios A. Stavrou, Marios Kountouris*

Main category: cs.LG

TL;DR: The paper reinterprets federated learning (FL) aggregation as finding the barycenter of local posteriors using divergence metrics, proposing BA-BFL, which matches state-of-the-art performance while providing geometric insights.


<details>
  <summary>Details</summary>
Motivation: FL's averaging approach may not align with Bayesian inference, prompting a need for a unifying framework that generalizes existing methods and offers theoretical clarity.

Method: The study adopts an information-geometric perspective to reinterpret FL aggregation, proposes BA-BFL for non-convex settings, and compares it with statistical techniques in non-IID scenarios.

Result: BA-BFL achieves performance comparable to state-of-the-art methods and provides a geometric interpretation of aggregation. It also explores Bayesian layers' impact in hybrid models.

Conclusion: The framework generalizes FL methods, BA-BFL is effective in non-convex and non-IID settings, and Bayesian layers enhance uncertainty quantification.

Abstract: Federated learning (FL) is a widely used and impactful distributed
optimization framework that achieves consensus through averaging locally
trained models. While effective, this approach may not align well with Bayesian
inference, where the model space has the structure of a distribution space.
Taking an information-geometric perspective, we reinterpret FL aggregation as
the problem of finding the barycenter of local posteriors using a prespecified
divergence metric, minimizing the average discrepancy across clients. This
perspective provides a unifying framework that generalizes many existing
methods and offers crisp insights into their theoretical underpinnings. We then
propose BA-BFL, an algorithm that retains the convergence properties of
Federated Averaging in non-convex settings. In non-independent and identically
distributed scenarios, we conduct extensive comparisons with statistical
aggregation techniques, showing that BA-BFL achieves performance comparable to
state-of-the-art methods while offering a geometric interpretation of the
aggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep
Learning, exploring the impact of Bayesian layers on uncertainty quantification
and model calibration.

</details>


### [314] [Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence](https://arxiv.org/pdf/2412.13779)
*Yichen Li, Yuying Wang, Haozhao Wang, Yining Qi, Tianzhe Xiao, Ruixuan Li*

Main category: cs.LG

TL;DR: FedSSI is a regularization-based method for Continual Federated Learning (CFL) that avoids rehearsal, reduces computational overhead, and handles data heterogeneity effectively.


<details>
  <summary>Details</summary>
Motivation: Current CFL methods rely on rehearsal, which is memory-intensive and may violate privacy. The goal is to develop a cost-efficient solution without rehearsal.

Method: FedSSI adapts synaptic intelligence for CFL, specifically addressing heterogeneous data. It avoids rehearsal and reduces computational costs.

Result: FedSSI outperforms state-of-the-art methods, demonstrating effectiveness in heterogeneous data settings.

Conclusion: FedSSI is a simple yet effective solution for CFL, offering privacy preservation and computational efficiency.

Abstract: Continual Federated Learning (CFL) allows distributed devices to
collaboratively learn novel concepts from continuously shifting training data
while avoiding knowledge forgetting of previously seen tasks. To tackle this
challenge, most current CFL approaches rely on extensive rehearsal of previous
data. Despite effectiveness, rehearsal comes at a cost to memory, and it may
also violate data privacy. Considering these, we seek to apply regularization
techniques to CFL by considering their cost-efficient properties that do not
require sample caching or rehearsal. Specifically, we first apply traditional
regularization techniques to CFL and observe that existing regularization
techniques, especially synaptic intelligence, can achieve promising results
under homogeneous data distribution but fail when the data is heterogeneous.
Based on this observation, we propose a simple yet effective regularization
algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the
CFL with heterogeneous data settings. FedSSI can not only reduce computational
overhead without rehearsal but also address the data heterogeneity issue.
Extensive experiments show that FedSSI achieves superior performance compared
to state-of-the-art methods.

</details>


### [315] [Towards Robust Incremental Learning under Ambiguous Supervision](https://arxiv.org/pdf/2501.13584)
*Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, Haobo Wang*

Main category: cs.LG

TL;DR: The paper introduces Incremental Partial Label Learning (IPLL) to address annotation uncertainty in dynamic learning systems. It proposes the PGDR algorithm to mitigate label ambiguity and forgetting, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Annotation uncertainty and high costs in dynamic learning systems motivate the need for weakly-supervised learning like IPLL.

Method: PGDR uses prototype-guided disambiguation and memory replay to handle label ambiguity and catastrophic forgetting.

Result: PGDR achieves balanced class perception and less forgetting, outperforming in experiments.

Conclusion: IPLL with PGDR effectively addresses annotation challenges and forgetting in incremental learning.

Abstract: Traditional Incremental Learning (IL) targets to handle sequential
fully-supervised learning problems where novel classes emerge from time to
time. However, due to inherent annotation uncertainty and ambiguity, collecting
high-quality annotated data in a dynamic learning system can be extremely
expensive. To mitigate this problem, we propose a novel weakly-supervised
learning paradigm called Incremental Partial Label Learning (IPLL), where the
sequentially arrived data relate to a set of candidate labels rather than the
ground truth. Technically, we develop the Prototype-Guided Disambiguation and
Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to
mitigate two intertwined challenges in IPLL, i.e., label ambiguity and
catastrophic forgetting. To handle the former, PGDR encapsulates a
momentum-based pseudo-labeling algorithm along with prototype-guided
initialization, resulting in a balanced perception of classes. To alleviate
forgetting, we develop a memory replay technique that collects
well-disambiguated samples while maintaining representativeness and diversity.
By jointly distilling knowledge from curated memory data, our framework
exhibits a great disambiguation ability for samples of new tasks and achieves
less forgetting of knowledge. Extensive experiments demonstrate that PGDR
achieves superior

</details>


### [316] [Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression](https://arxiv.org/pdf/2501.13790)
*Michael Crawshaw, Blake Woodworth, Mingrui Liu*

Main category: cs.LG

TL;DR: Local Gradient Descent variants for distributed logistic regression achieve faster convergence ($O(1/KR)$) with large stepsizes, outperforming prior guarantees ($\Omega(1/R)$).


<details>
  <summary>Details</summary>
Motivation: To demonstrate the benefits of local updates in distributed logistic regression with heterogeneous data, which existing analyses fail to capture.

Method: Analyzed two variants of Local Gradient Descent, focusing on large stepsizes ($\eta \gg 1/K$) for logistic regression.

Result: Convergence rate of $O(1/KR)$ for $K$ local steps and sufficient $R$ rounds, improving over prior $\Omega(1/R)$ guarantees.

Conclusion: Large stepsizes enable faster convergence, highlighting the advantage of local updates in distributed settings.

Abstract: We analyze two variants of Local Gradient Descent applied to distributed
logistic regression with heterogeneous, separable data and show convergence at
the rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication
rounds. In contrast, all existing convergence guarantees for Local GD applied
to any problem are at least $\Omega(1/R)$, meaning they fail to show the
benefit of local updates. The key to our improved guarantee is showing progress
on the logistic regression objective when using a large stepsize $\eta \gg
1/K$, whereas prior analysis depends on $\eta \leq 1/K$.

</details>


### [317] [Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework](https://arxiv.org/pdf/2502.00846)
*Terje Mildner, Oliver Hamelijnck, Paris Giampouras, Theodoros Damoulas*

Main category: cs.LG

TL;DR: FedGVI is a robust probabilistic Federated Learning framework addressing model misspecification with calibrated uncertainty, outperforming existing methods in robustness and predictive performance.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in frequentist and Bayesian FL by providing unbiased predictions and calibrated uncertainty under model misspecification.

Method: Generalizes Partitioned Variational Inference with robust and conjugate updates, reducing client-side computational complexity.

Result: Theoretical guarantees include fixed-point convergence, optimal cavity distribution, and robustness to likelihood misspecification. Empirical results show improved robustness and predictive performance.

Conclusion: FedGVI effectively addresses FL challenges, offering theoretical and empirical advantages over prior methods.

Abstract: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that
is robust to both prior and likelihood misspecification. FedGVI addresses
limitations in both frequentist and Bayesian FL by providing unbiased
predictions under model misspecification, with calibrated uncertainty
quantification. Our approach generalises previous FL approaches, specifically
Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and
conjugate updates, decreasing computational complexity at the clients. We offer
theoretical analysis in terms of fixed-point convergence, optimality of the
cavity distribution, and provable robustness to likelihood misspecification.
Further, we empirically demonstrate the effectiveness of FedGVI in terms of
improved robustness and predictive performance on multiple synthetic and real
world classification data sets.

</details>


### [318] [SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital Twins for AI Training](https://arxiv.org/pdf/2502.21187)
*Fakrul Islam Tushar, Lavsen Dahal, Cindy McCabe, Fong Chi Ho, Paul Segars, Ehsan Abadi, Kyle J. Lafata, Ehsan Samei, Joseph Y. Lo*

Main category: cs.LG

TL;DR: SYN-LUNGS generates high-quality 3D CT images with annotations to address data scarcity in AI lung cancer screening, improving model performance by 10% in detection and 2-9% in segmentation/classification.


<details>
  <summary>Details</summary>
Motivation: AI models for lung cancer screening face data scarcity, limiting generalizability and clinical use. Generative models help but are hindered by training data variability.

Method: SYN-LUNGS combines XCAT3 phantoms for digital twins, X-Lesions for nodule simulation, and DukeSim for CT image formation, creating a dataset of 3,072 nodule images from 1,044 CT scans.

Result: Models trained on clinical + simulated data outperformed clinical-only models, with 10% better detection, 2-9% improvements in segmentation/classification, and enhanced synthesis.

Conclusion: SYN-LUNGS offers a scalable solution for AI development, improving rare disease representation and model reliability through anatomy-informed simulations.

Abstract: AI models for lung cancer screening are limited by data scarcity, impacting
generalizability and clinical applicability. Generative models address this
issue but are constrained by training data variability. We introduce SYN-LUNGS,
a framework for generating high-quality 3D CT images with detailed annotations.
SYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for
nodule simulation (varying size, location, and appearance), and DukeSim for CT
image formation with vendor and parameter variability. The dataset includes
3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174
digital twins. Models trained on clinical + simulated data outperform clinical
only models, achieving 10% improvement in detection, 2-9% in segmentation and
classification, and enhanced synthesis. By incorporating anatomy-informed
simulations, SYN-LUNGS provides a scalable approach for AI model development,
particularly in rare disease representation and improving model reliability.

</details>


### [319] [Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters](https://arxiv.org/pdf/2503.18216)
*Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu*

Main category: cs.LG

TL;DR: RaNA adapters improve LLM inference efficiency by combining low-rank matrix decompositions and adaptive masking, outperforming neuron-adaptive methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of neuron-adaptive techniques in Transformers, such as reliance on sparse activations and costly masking.

Method: Propose Adaptive Rank Allocation framework and RaNA adapter, applying low-rank decompositions and adaptive masking to linear layers.

Result: Improves perplexity by 7 points and accuracy by 8 percentage-points while reducing FLOPs by ~44%.

Conclusion: RaNA is a robust solution for efficient inference in modern Transformer architectures.

Abstract: Large Language Models (LLMs) are computationally intensive, particularly
during inference. Neuron-adaptive techniques, which selectively activate
neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer
from limitations in modern Transformers. These include reliance on sparse
activations, incompatibility with attention layers, and the use of costly
neuron masking techniques. To address these issues, we propose the Adaptive
Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)
adapter. RaNA adapters leverage rank adapters, which operate on linear layers
by applying both low-rank matrix decompositions and adaptive masking to
efficiently allocate compute without depending on activation sparsity. This
enables RaNA to be generally applied to MLPs and linear components of attention
modules, while eliminating the need for expensive maskers found in
neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA
improves perplexity by up to 7 points and increases accuracy by up to 8
percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art
Transformer architectures. These results position RaNA as a robust solution for
improving inference efficiency in modern Transformer architectures.

</details>


### [320] [Mitigating Many-Shot Jailbreaking](https://arxiv.org/pdf/2504.09604)
*Christopher M. Ackerman, Nina Panickssery*

Main category: cs.LG

TL;DR: The paper explores mitigation techniques for Many-shot jailbreaking (MSJ) attacks on LLMs, showing combined fine-tuning and input sanitization significantly reduces attack effectiveness while preserving model performance.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of modern LLMs to MSJ attacks, which exploit long context windows to bypass safety training.

Method: Probes the effectiveness of fine-tuning and input sanitization, individually and combined, against MSJ attacks.

Result: Combined techniques significantly reduce MSJ attack effectiveness without harming benign tasks.

Conclusion: The proposed approach could effectively mitigate MSJ vulnerabilities if integrated into post-training safety measures.

Abstract: Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the
long context windows of modern LLMs to circumvent model safety training by
including in the prompt many examples of a "fake" assistant responding
inappropriately before the final request. With enough examples, the model's
in-context learning abilities override its safety training, and it responds as
if it were the "fake" assistant. In this work, we probe the effectiveness of
different fine-tuning and input sanitization approaches on mitigating MSJ
attacks, alone and in combination. We find incremental mitigation effectiveness
for each, and show that the combined techniques significantly reduce the
effectiveness of MSJ attacks, while retaining model performance in benign
in-context learning and conversational tasks. We suggest that our approach
could meaningfully ameliorate this vulnerability if incorporated into model
safety post-training.

</details>


### [321] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/pdf/2505.02222)
*Essential AI, :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani*

Main category: cs.LG

TL;DR: Muon, a second-order optimizer, outperforms AdamW in data efficiency at large batch sizes, enabling economical training. Combined with muP, it offers efficient hyperparameter transfer with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To improve training efficiency and data retention at large batch sizes beyond the critical batch size, surpassing AdamW's limitations.

Method: Muon, a second-order optimizer, is combined with the maximal update parameterization (muP) and a telescoping algorithm to handle errors in muP.

Result: Muon retains data efficiency at large batch sizes, is computationally efficient, and works well with models up to four billion parameters.

Conclusion: Muon expands the Pareto frontier over AdamW, offering a more economical and efficient training solution, especially at large scales.

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>


### [322] [Smooth Quadratic Prediction Markets](https://arxiv.org/pdf/2505.02959)
*Enrique Nueve, Bo Waggoner*

Main category: cs.LG

TL;DR: The paper introduces the Smooth Quadratic Prediction Market, which generalizes steepest gradient descent, improving worst-case monetary loss while retaining key guarantees. It also explores trading behavior under constraints and adaptive liquidity.


<details>
  <summary>Details</summary>
Motivation: To explore if other learning algorithms beyond Follow-The-Regularized-Leader can inspire prediction market designs, improving upon Duality-based Cost Function Market Makers.

Method: Decomposes and modifies the DCFMM pricing mechanism to propose the Smooth Quadratic Prediction Market, incentivizing steepest gradient descent. Examines trading under bounded budgets and buy-only constraints.

Result: The new market improves worst-case monetary loss for AD securities while preserving key axioms. Trading behavior insights under constraints are provided.

Conclusion: Future designs can separate price update rules from fee structures while maintaining guarantees, as demonstrated by the Smooth Quadratic Prediction Market.

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic Prediction Market. Our
results suggest future designs where the price update rule is separate from the
fee structure, yet guarantees are preserved.

</details>


### [323] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/pdf/2505.03595)
*Sidharth S. Menon, Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net is a neural surrogate for solving high-dimensional PDEs efficiently, overcoming the curse of dimensionality with high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: High-dimensional PDEs are computationally intractable due to exponential complexity growth, especially on hypercubic domains. Traditional methods fail, necessitating a scalable solution.

Method: Anant-Net integrates high-dimensional boundary conditions and minimizes PDE residuals at collocation points, using Kolmogorov-Arnold networks for interpretability.

Result: Anant-Net solves 300-dimensional PDEs on a single GPU in hours, outperforming state-of-the-art methods in accuracy and runtime.

Conclusion: Anant-Net is an accurate, interpretable, and scalable framework for high-dimensional PDEs, setting a new benchmark in the field.

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [324] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/pdf/2505.03712)
*Deming Sheng, Ricardo Henao*

Main category: cs.LG

TL;DR: A parametric survival analysis method using the Asymmetric Laplace Distribution (ALD) is proposed, outperforming existing parametric and nonparametric models in accuracy, discrimination, and calibration.


<details>
  <summary>Details</summary>
Motivation: To address limitations of nonparametric survival analysis models by introducing a parametric approach with closed-form calculations for event summaries.

Method: Uses the ALD distribution for parametric modeling, optimized via maximum likelihood to learn individual-level parameters (location, scale, asymmetry).

Result: Demonstrates superior performance in accuracy, discrimination, and calibration on synthetic and real-world datasets.

Conclusion: The ALD-based parametric method is a robust alternative to existing survival analysis models.

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [325] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/pdf/2505.03864)
*Qiaomu Li, Ying Xie*

Main category: cs.MA

TL;DR: The paper analyzes the challenges of integrating Google's A2A and Anthropic's MCP for multi-agent AI systems, focusing on semantic interoperability, security, governance, and practical implications.


<details>
  <summary>Details</summary>
Motivation: To address the emergent challenges of combining A2A and MCP for seamless multi-agent collaboration and tool interaction.

Method: Critical analysis of the integration's benefits, dependencies, and trade-offs, including security, privacy, debugging, and semantic negotiation.

Result: Identifies key challenges like security vulnerabilities, privacy issues, and debugging difficulties, while acknowledging the foundational potential of A2A+MCP.

Conclusion: A2A+MCP provides a vital foundation but requires advancements to manage complexities for full potential realization.

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>


### [326] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/pdf/2505.04364)
*Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun*

Main category: cs.MA

TL;DR: SwarmBench is introduced to evaluate LLMs' swarm intelligence in decentralized MAS under local constraints, revealing performance variations and coordination challenges.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' emergent coordination in MAS under strict, swarm-like constraints, addressing gaps in existing benchmarks.

Method: SwarmBench, a configurable 2D grid benchmark with five MAS tasks, evaluates LLMs using local perception and communication.

Result: LLMs show performance variations and struggle with robust planning under local information constraints.

Conclusion: SwarmBench aids reproducible research into LLM-based MAS coordination and Embodied MAS theory, released as an open toolkit.

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [327] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/pdf/2505.04379)
*Mohammad Elayan, Wissam Kontar*

Main category: cs.MA

TL;DR: The paper analyzes consensus in transportation systems with AVs, using TGSIM data to evaluate safety, interaction, and performance metrics. Full consensus is rare (1.63% of AV-VRU interactions), emphasizing the need for balanced AV models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving consensus in safety, interaction quality, and traffic performance in mixed-traffic systems with AVs.

Method: Empirical analysis using high-resolution trajectory data from TGSIM, evaluating metrics like TTC, PET, deceleration, headways, and string stability.

Result: Full consensus across safety, interaction, and performance is rare (1.63% of AV-VRU interactions).

Conclusion: AV models must explicitly balance multi-dimensional performance in mixed-traffic environments.

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>


### [328] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/pdf/2505.04579)
*StÃ©phane Aroca-Ouellette, Miguel Aroca-Ouellette, Katharina von der Wense, Alessandro Roncone*

Main category: cs.MA

TL;DR: HA$^2$ improves zero-shot coordination in collaborative tasks by using hierarchical reinforcement learning to mimic human-like shared task abstractions, outperforming baselines and state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents struggle with quick adaptation to new teammates due to lack of shared task abstractions, a key human capability.

Method: HA$^2$ leverages hierarchical reinforcement learning to create structured collaboration akin to humans.

Result: HA$^2$ shows significant improvement over baselines and state-of-the-art methods in the Overcooked environment, with better resilience to changes.

Conclusion: HA$^2$ effectively bridges the gap in zero-shot coordination by emulating human collaboration strategies.

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>


### [329] [Towards a HIPAA Compliant Agentic AI System in Healthcare](https://arxiv.org/pdf/2504.17669)
*Subash Neupane, Sudip Mittal, Shahram Rahimi*

Main category: cs.MA

TL;DR: A HIPAA-compliant Agentic AI framework for clinical workflows ensures regulatory compliance via dynamic policy enforcement, PHI governance, sanitization, and audit trails.


<details>
  <summary>Details</summary>
Motivation: To address the need for regulatory compliance (e.g., HIPAA) in AI-driven clinical workflows handling sensitive healthcare data.

Method: Integrates ABAC for PHI governance, a hybrid PHI sanitization pipeline (regex + BERT), and immutable audit trails.

Result: A framework that enforces compliance while autonomously handling sensitive healthcare data.

Conclusion: The proposed framework enables safe adoption of Agentic AI in clinical settings by ensuring HIPAA compliance.

Abstract: Agentic AI systems powered by Large Language Models (LLMs) as their
foundational reasoning engine, are transforming clinical workflows such as
medical report generation and clinical summarization by autonomously analyzing
sensitive healthcare data and executing decisions with minimal human oversight.
However, their adoption demands strict compliance with regulatory frameworks
such as Health Insurance Portability and Accountability Act (HIPAA),
particularly when handling Protected Health Information (PHI). This
work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that
enforces regulatory compliance through dynamic, context-aware policy
enforcement. Our framework integrates three core mechanisms: (1)
Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid
PHI sanitization pipeline combining regex patterns and BERT-based model to
minimize leakage, and (3) immutable audit trails for compliance verification.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [330] [RFNNS: Robust Fixed Neural Network Steganography with Popular Deep Generative Models](https://arxiv.org/pdf/2505.04116)
*Yu Cheng, Jiuan Zhou, Jiawei Chen, Zhaoxia Yin, Xinpeng Zhang*

Main category: cs.MM

TL;DR: RFNNS improves Fixed Neural Network Steganography by enhancing stego image quality and robustness against attacks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing FNNS methods cause high distortion and poor robustness in stego images, limiting security and practicality.

Method: RFNNS uses texture-aware localization and robust perturbation generation to embed secret data in less perceptible areas and withstand attacks.

Result: RFNNS achieves better security (imperceptibility, anti-steganalysis) and robustness against common attacks like JPEG compression and noise.

Conclusion: RFNNS outperforms SOTA FNNS methods, offering practical and secure steganography.

Abstract: Image steganography is a technique that conceals secret information in a
cover image to achieve covert communication. Recent research has demonstrated
that Fixed Neural Network Steganography (FNNS) exhibits significant practical
advantages, as it enables stable and efficient steganographic embedding and
extraction without requiring neural network training. However, the stego image
generated by existing FNNS methods suffers from considerable distortion and
exhibits poor robustness, severely reducing the security and practicality of
steganography. To address the aforementioned issues, we propose a Robust Fixed
Neural Network Steganography (RFNNS). In RFNNS, we introduce a texture-aware
localization technique to add perturbations carrying secret image information
to complex texture areas that are less perceptible to the human eye, thereby
ensuring the quality of the stego image. To enhance robustness, a robust
steganographic perturbation generation (RSPG) strategy is designed, which
enables slight perturbations to be accurately decoded even after common image
attacks. Subsequently, the generated robust perturbations are combined with the
AI-generated cover image to produce the stego image. The receiver only needs to
share the secret key and employ the same decoding network structure to
accurately extract the secret image from the attacked stego image. Experimental
results demonstrate that RFNNS achieves enhanced performance in terms of
security, including imperceptibility and anti-steganalysis performance.
Furthermore, RFNNS demonstrates superior robustness against common image
attacks, such as JPEG compression, Gaussian noise, and contrast adjustment,
across diverse embedding capacities, outperforming existing SOTA FNNS methods.

</details>


### [331] [Securing Immersive 360 Video Streams through Attribute-Based Selective Encryption](https://arxiv.org/pdf/2505.04466)
*Mohammad Waquas Usmani, Susmit Shannigrahi, Michael Zink*

Main category: cs.MM

TL;DR: A novel ABE-based framework for secure 360Â° video streaming reduces computational overhead while maintaining security and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and scalability issues of HTTPS in securing high-bitrate 360Â° video streams.

Method: Integrates ABE with selective encryption, focusing on viewport-adaptive encryption for dynamic frame protection.

Result: Reduced computational load, improved cache hit rates, and maintained visual quality comparable to HTTPS.

Conclusion: The proposed ABE-based framework efficiently secures 360Â° video streaming with minimal overhead.

Abstract: Delivering high-quality, secure 360{\deg} video content introduces unique
challenges, primarily due to the high bitrates and interactive demands of
immersive media. Traditional HTTPS-based methods, although widely used, face
limitations in computational efficiency and scalability when securing these
high-resolution streams. To address these issues, this paper proposes a novel
framework integrating Attribute-Based Encryption (ABE) with selective
encryption techniques tailored specifically for tiled 360{\deg} video
streaming. Our approach employs selective encryption of frames at varying
levels to reduce computational overhead while ensuring robust protection
against unauthorized access.
  Moreover, we explore viewport-adaptive encryption, dynamically encrypting
more frames within tiles occupying larger portions of the viewer's field of
view. This targeted method significantly enhances security in critical viewing
areas without unnecessary overhead in peripheral regions. We deploy and
evaluate our proposed approach using the CloudLab testbed, comparing its
performance against traditional HTTPS streaming. Experimental results
demonstrate that our ABE-based model achieves reduced computational load on
intermediate caches, improves cache hit rates, and maintains comparable visual
quality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).

</details>


### [332] [EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment](https://arxiv.org/pdf/2504.16405)
*Lancheng Gao, Ziheng Jia, Yunhao Zeng, Wei Sun, Yiming Zhang, Wei Zhou, Guangtao Zhai, Xiongkuo Min*

Main category: cs.MM

TL;DR: EEmo-Bench is a new benchmark for evaluating multi-modal large language models (MLLMs) on image-evoked emotions, using diverse tasks and emotional attributes like Valence-Arousal-Dominance (VAD).


<details>
  <summary>Details</summary>
Motivation: Current evaluations of MLLMs' emotion understanding are coarse-grained; EEmo-Bench aims to provide a systematic and comprehensive assessment.

Method: The benchmark includes 1,960 annotated images and four tasks (Perception, Ranking, Description, Assessment) with 6,773 QA pairs, tested on 19 MLLMs.

Result: Some proprietary and large-scale open-source MLLMs perform well overall, but analytical capabilities in certain dimensions are lacking.

Conclusion: EEmo-Bench advances research on MLLMs' emotion perception and understanding, crucial for applications like human-machine interaction.

Abstract: The furnishing of multi-modal large language models (MLLMs) has led to the
emergence of numerous benchmark studies, particularly those evaluating their
perception and understanding capabilities. Among these, understanding
image-evoked emotions aims to enhance MLLMs' empathy, with significant
applications such as human-machine interaction and advertising recommendations.
However, current evaluations of this MLLM capability remain coarse-grained, and
a systematic and comprehensive assessment is still lacking. To this end, we
introduce EEmo-Bench, a novel benchmark dedicated to the analysis of the evoked
emotions in images across diverse content categories. Our core contributions
include: 1) Regarding the diversity of the evoked emotions, we adopt an emotion
ranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional
attributes for emotional assessment. In line with this methodology, 1,960
images are collected and manually annotated. 2) We design four tasks to
evaluate MLLMs' ability to capture the evoked emotions by single images and
their associated attributes: Perception, Ranking, Description, and Assessment.
Additionally, image-pairwise analysis is introduced to investigate the model's
proficiency in performing joint and comparative analysis. In total, we collect
6,773 question-answer pairs and perform a thorough assessment on 19
commonly-used MLLMs. The results indicate that while some proprietary and
large-scale open-source MLLMs achieve promising overall performance, the
analytical capabilities in certain evaluation dimensions remain suboptimal. Our
EEmo-Bench paves the path for further research aimed at enhancing the
comprehensive perceiving and understanding capabilities of MLLMs concerning
image-evoked emotions, which is crucial for machine-centric emotion perception
and understanding.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [333] [Aliasing Reduction in Neural Amp Modeling by Smoothing Activations](https://arxiv.org/pdf/2505.04082)
*Ryota Sato, Julius O. Smith III*

Main category: eess.AS

TL;DR: The paper explores novel activation functions to reduce aliasing in neural-network-based audio hardware emulations, introducing the Aliasing-to-Signal Ratio (ASR) metric for assessment.


<details>
  <summary>Details</summary>
Motivation: Address aliasing artifacts in neural amplifier models caused by nonlinear activation functions.

Method: Investigate modified activation functions and introduce ASR to measure aliasing. Compare with Error-to-Signal Ratio (ESR).

Result: Smoother activation functions reduce ASR without significantly increasing ESR, improving modeling accuracy.

Conclusion: Improved activation functions can mitigate aliasing in neural amp models while maintaining accuracy.

Abstract: The increasing demand for high-quality digital emulations of analog audio
hardware such as vintage guitar amplifiers has led to numerous works in
neural-network-based black-box modeling, with deep learning architectures like
WaveNet showing promising results. However, a key limitation in all of these
models is the aliasing artifacts that arise from the use of nonlinear
activation functions in neural networks. In this paper, we investigate novel
and modified activation functions aimed at mitigating aliasing within neural
amplifier models. Supporting this, we introduce a novel metric, the
Aliasing-to-Signal Ratio (ASR), which quantitatively assesses the level of
aliasing with high accuracy. Measuring also the conventional Error-to-Signal
Ratio (ESR), we conducted studies on a range of preexisting and modern
activation functions with varying stretch factors. Our findings confirmed that
activation functions with smoother curves tend to achieve lower ASR values,
indicating a noticeable reduction in aliasing. Notably, this improvement in
aliasing reduction was achievable without a substantial increase in ESR,
demonstrating the potential for high modeling accuracy with reduced aliasing in
neural amp models.

</details>


### [334] [Robust Speech Recognition with SchrÃ¶dinger Bridge-Based Speech Enhancement](https://arxiv.org/pdf/2505.04237)
*Rauf Nasretdinov, Roman Korostik, Ante JukiÄ*

Main category: eess.AS

TL;DR: Generative speech enhancement using SchrÃ¶dinger bridge improves ASR robustness in noisy/reverberant conditions, reducing word error rate by 40% vs. unprocessed speech and 8% vs. predictive baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance ASR model robustness in challenging acoustic environments (noisy and reverberant conditions).

Method: Employ a SchrÃ¶dinger bridge-based speech enhancement model, analyze scaling and sampling methods, and compare with predictive/diffusion baselines using pre-trained ASR models.

Result: Significant reduction in word error rate: 40% relative to unprocessed speech and 8% relative to predictive baselines.

Conclusion: The SchrÃ¶dinger bridge approach is effective for improving ASR performance in noisy/reverberant environments.

Abstract: In this work, we investigate application of generative speech enhancement to
improve the robustness of ASR models in noisy and reverberant conditions. We
employ a recently-proposed speech enhancement model based on Schr\"odinger
bridge, which has been shown to perform well compared to diffusion-based
approaches. We analyze the impact of model scaling and different sampling
methods on the ASR performance. Furthermore, we compare the considered model
with predictive and diffusion-based baselines and analyze the speech
recognition performance when using different pre-trained ASR models. The
proposed approach significantly reduces the word error rate, reducing it by
approximately 40% relative to the unprocessed speech signals and by
approximately 8% relative to a similarly sized predictive approach.

</details>


### [335] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/pdf/2505.04382)
*Anton Selitskiy, Maitreya Kocharekar*

Main category: eess.AS

TL;DR: The paper introduces a vector-based voice conversion method using discrete optimal transport for alignment, showing high quality and effectiveness, and highlights a potential misuse in audio generation.


<details>
  <summary>Details</summary>
Motivation: To improve voice conversion by aligning audio embeddings between speakers using discrete optimal transport.

Method: Employ discrete optimal transport mapping for aligning audio embeddings in voice conversion.

Result: Demonstrates high quality and effectiveness of the method, with potential misuse in audio generation.

Conclusion: Discrete optimal transport is effective for voice conversion but may have unintended consequences in synthetic audio classification.

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>


### [336] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/pdf/2505.04419)
*Sumit Kumar, Parampreet Singh, Vipul Arora*

Main category: eess.AS

TL;DR: The paper introduces ROD, a dataset for detecting vocal ornaments in Indian classical music, and proposes a deep time-series model for ornamentation detection, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Ornamentations are crucial for melodic expression but lack annotated datasets and specialized models, hindering research in MIR applications like pedagogy and genre classification.

Method: The authors create the ROD dataset with expert annotations and develop a deep time-series model for ornament detection, ensuring boundary preservation in audio chunking.

Result: Experiments show the proposed model outperforms the baseline CRNN on both the ROD dataset and a separate annotated concert dataset.

Conclusion: The ROD dataset and the proposed model advance ornamentation detection in MIR, with potential applications in music analysis and generation.

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>


### [337] [Accelerating Audio Research with Robotic Dummy Heads](https://arxiv.org/pdf/2505.04548)
*Austin Lu, Kanad Sarkar, Yongjie Zhuang, Leo Lin, Ryan M Corey, Andrew C Singer*

Main category: eess.AS

TL;DR: A robotic dummy head combines acoustic realism and mobility to automate audio experiments and serve as a moving sound source, validated through experiments and open-sourced for research.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between static acoustic mannequins and dynamic robotic platforms for advanced audio research.

Method: Developed a robotic dummy head capable of movement, speech, and listening, with quiet motors for dynamic experiments.

Result: Validated high-quality audio data collection and demonstrated utility in adaptive binaural beamforming studies.

Conclusion: The device accelerates audio research by enabling dynamic experiments and is open-sourced to foster innovation.

Abstract: This work introduces a robotic dummy head that fuses the acoustic realism of
conventional audiological mannequins with the mobility of robots. The proposed
device is capable of moving, talking, and listening as people do, and can be
used to automate spatially-stationary audio experiments, thus accelerating the
pace of audio research. Critically, the device may also be used as a moving
sound source in dynamic experiments, due to its quiet motor. This feature
differentiates our work from previous robotic acoustic research platforms.
Validation that the robot enables high quality audio data collection is
provided through various experiments and acoustic measurements. These
experiments also demonstrate how the robot might be used to study adaptive
binaural beamforming. Design files are provided as open-source to stimulate
novel audio research.

</details>


### [338] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.04623)
*Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, Pheng-Ann Heng*

Main category: eess.AS

TL;DR: EchoInk-R1, a reinforcement learning framework, improves cross-modal reasoning in MLLMs, achieving 85.77% accuracy on AVQA-R1-6K, outperforming the base model.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with structured cross-modal reasoning, especially integrating audio and visual signals.

Method: Built on Qwen2.5-Omni-7B and optimized with GRPO, EchoInk-R1 tackles multiple-choice QA over audio-image pairs using the AVQA-R1-6K dataset.

Result: EchoInk-R1-7B achieves 85.77% accuracy, outperforming the base model (80.53%) with just 562 RL steps. It also shows reflective reasoning.

Conclusion: Lightweight RL fine-tuning enhances cross-modal reasoning, unifying audio, visual, and textual modalities for open-world reasoning.

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


### [339] [SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection](https://arxiv.org/pdf/2408.17432)
*Ismail Rasim Ulgen, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman*

Main category: eess.AS

TL;DR: SelectTTS is a low-complexity method for multi-speaker TTS that selects frames from target speakers and uses SSL features, achieving performance comparable to state-of-the-art models with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Existing multi-speaker TTS methods are complex and resource-intensive, limiting accessibility. SelectTTS aims to simplify this process.

Method: SelectTTS selects frames from target speakers and decodes them using SSL features, reducing model complexity.

Result: SelectTTS matches state-of-the-art performance with 8x fewer parameters and 270x less training data.

Conclusion: SelectTTS offers a simpler, resource-efficient alternative for multi-speaker TTS, enabling wider accessibility.

Abstract: Synthesizing the voices of unseen speakers remains a persisting challenge in
multi-speaker text-to-speech (TTS). Existing methods model speaker
characteristics through speaker conditioning during training, leading to
increased model complexity and limiting reproducibility and accessibility. A
lower-complexity method would enable speech synthesis research with limited
computational and data resources to reach to a wider use. To this end, we
propose SelectTTS, a simple and effective alternative. SelectTTS selects
appropriate frames from the target speaker and decodes them using frame-level
self-supervised learning (SSL) features. We demonstrate that this approach can
effectively capture speaker characteristics for unseen speakers and achieves
performance comparable to state-of-the-art multi-speaker TTS frameworks on both
objective and subjective metrics. By directly selecting frames from the target
speaker's speech, SelectTTS enables generalization to unseen speakers with
significantly lower model complexity. Compared to baselines such as XTTS-v2 and
VALL-E, SelectTTS achieves better speaker similarity while reducing model
parameters by over 8x and training data requirements by 270x.

</details>


### [340] [mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition](https://arxiv.org/pdf/2502.01547)
*Andrew Rouditchenko, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass*

Main category: eess.AS

TL;DR: mWhisper-Flamingo combines pre-trained audio (Whisper) and video (AV-HuBERT) models for multilingual AVSR, using decoder modality dropout to improve noisy performance. It achieves state-of-the-art results on MuAViC.


<details>
  <summary>Details</summary>
Motivation: Most AVSR methods are limited to English due to lack of multilingual video data, hindering performance in noisy conditions.

Method: Combines Whisper and AV-HuBERT with decoder modality dropout for training on paired and separate audio/visual inputs.

Result: Achieves state-of-the-art WER on MuAViC (9 languages) and outperforms audio-only Whisper in noisy conditions.

Conclusion: mWhisper-Flamingo effectively addresses multilingual AVSR challenges, improving noisy performance with innovative training.

Abstract: Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio
and can improve performance in noise, but most methods are trained only on
English data. One limitation is the lack of large-scale multilingual video
data, which makes it hard to train models from scratch. In this work, we
propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of
a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable
better multi-modal integration and improve the noisy multilingual performance,
we introduce decoder modality dropout where the model is trained both on paired
audio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo
achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.
Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on
all languages in noisy conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [341] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/pdf/2505.03838)
*Ting Yu Tsai, An Yu, Meghana Spurthi Maadugundu, Ishrat Jahan Mohima, Umme Habiba Barsha, Mei-Hwa F. Chen, Balakrishnan Prabhakaran, Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac is a web-based AI platform for 4D cardiac image segmentation and disease classification, achieving high accuracy and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To provide precise and effective processing of cardiac imaging data for better identification and management of cardiovascular diseases.

Method: Uses deep learning models for segmentation and a two-step classification pipeline, trained on the ACDC dataset.

Result: Segmentation accuracy of 92.6% and classification accuracy of 98% in five disease categories.

Conclusion: IntelliCardiac is a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis.

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [342] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/pdf/2505.03844)
*SolÃ¨ne DebuysÃ¨re, Nicolas TrouvÃ©, Nathan Letheule, Olivier LÃ©vÃªque, Elise Colin*

Main category: eess.IV

TL;DR: A novel AI-driven method transforms satellite SAR imagery into airborne SAR representations using a pre-trained latent diffusion model, addressing data scarcity in remote sensing.


<details>
  <summary>Details</summary>
Motivation: The lack of open-source, labeled SAR datasets hinders the use of foundation models in remote sensing. Synthetic image generation is proposed to augment scarce data.

Method: Utilizes spatial conditioning in a 3.5B-parameter latent diffusion model, trained on 110k SAR images from ONERA's archival data, to convert satellite SAR to airborne SAR.

Result: The pipeline effectively bridges realism between simulated and real images, advancing SAR imaging technology.

Conclusion: This work pioneers a new approach in SAR imaging, demonstrating AI's potential to overcome data limitations in remote sensing.

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [343] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/pdf/2505.03845)
*Ioannis Kyprakis, Vasileios Skaramagkas, Iro Boura, Georgios Karamanis, Dimitrios I. Fotiadis, Zinovia Kefalopoulou, Cleanthe Spanaki, Manolis Tsiknakis*

Main category: eess.IV

TL;DR: Deep learning models analyze facial videos to detect depressive symptoms in Parkinson's disease patients, with Video Swin Tiny performing best (94% accuracy).


<details>
  <summary>Details</summary>
Motivation: Depressive symptoms in PD are underdiagnosed due to overlapping motor features; this study aims to improve detection using facial video analysis.

Method: Tested ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention layers on 1,875 videos from 178 PD patients, assessing GDS scores and medication states.

Result: Video Swin Tiny achieved 94% accuracy in binary classification and 87.1% in multiclass tasks for depressive symptom detection.

Conclusion: Facial video analysis with deep learning, especially Video Swin Tiny, effectively detects depressive symptoms in PD patients.

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [344] [Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification](https://arxiv.org/pdf/2505.04003)
*Feng Gao, Sheng Liu, Chuanzheng Gong, Xiaowei Zhou, Jiayi Wang, Junyu Dong, Qian Du*

Main category: eess.IV

TL;DR: PICNet improves land cover classification by addressing inter-frequency feature coupling and complementary information inconsistency in multi-source remote sensing data.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy and reliability in land cover classification by leveraging complementary information from HSI and SAR/LiDAR data.

Method: Uses a Prototype-based Information Compensation Network (PICNet) with frequency interaction and prototype-based compensation modules.

Result: Outperforms state-of-the-art methods on three public datasets.

Conclusion: PICNet effectively addresses challenges in multi-source data classification, demonstrating superior performance.

Abstract: Multi-source remote sensing data joint classification aims to provide
accuracy and reliability of land cover classification by leveraging the
complementary information from multiple data sources. Existing methods confront
two challenges: inter-frequency multi-source feature coupling and inconsistency
of complementary information exploration. To solve these issues, we present a
Prototype-based Information Compensation Network (PICNet) for land cover
classification based on HSI and SAR/LiDAR data. Specifically, we first design a
frequency interaction module to enhance the inter-frequency coupling in
multi-source feature extraction. The multi-source features are first decoupled
into high- and low-frequency components. Then, these features are recoupled to
achieve efficient inter-frequency communication. Afterward, we design a
prototype-based information compensation module to model the global
multi-source complementary information. Two sets of learnable modality
prototypes are introduced to represent the global modality information of
multi-source data. Subsequently, cross-modal feature integration and alignment
are achieved through cross-attention computation between the modality-specific
prototype vectors and the raw feature representations. Extensive experiments on
three public datasets demonstrate the significant superiority of our PICNet
over state-of-the-art methods. The codes are available at
https://github.com/oucailab/PICNet.

</details>


### [345] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/pdf/2505.04097)
*Thien Nhan Vo, Bac Nam Ho, Thanh Xuan Truong*

Main category: eess.IV

TL;DR: A 3D CNN model for classifying Alzheimer's in brain MRI scans achieved high accuracy (0.912) and AUC (0.961), outperforming resizing alone by ~0.027. Sensitivity and specificity were >0.90, showing the value of simple augmentation.


<details>
  <summary>Details</summary>
Motivation: To improve Alzheimer's classification in T1-weighted brain MRI scans using a 3D CNN with noise injection and cross-validation.

Method: Developed a 3D CNN with convolution, pooling, batch normalization, ReLU layers, and sigmoid output, using stochastic noise injection and five-fold cross-validation.

Result: Achieved 0.912 accuracy, 0.961 AUC, and >0.90 sensitivity/specificity, outperforming resizing by ~0.027.

Conclusion: Simple augmentation is effective for 3D MRI classification, motivating future work on advanced methods like 3D U-Net and vision transformers.

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


### [346] [A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings](https://arxiv.org/pdf/2505.04172)
*Iankai Tang, Kegang Wang, Yingke Ding, Jiatong Ji, Zeyu Wang, Xiyuxing Zhang, Ping Chen, Yuanchun Shi, Yuntao Wang*

Main category: eess.IV

TL;DR: The paper introduces Ï-Ring, an open-source dataset for ring-based cardiovascular monitoring, and evaluates methods for estimating heart rate, respiratory rate, oxygen saturation, and blood pressure.


<details>
  <summary>Details</summary>
Motivation: There is a lack of publicly available datasets and standardized tools for reliable cardiovascular parameter estimation using smart rings.

Method: The authors present Ï-Ring, a dataset with photoplethysmography and accelerometer data from 34 subjects, and evaluate physics-based and deep learning methods using their RingTool toolkit.

Result: The proposed methods outperform commercial rings, achieving low MAE values for cardiovascular parameters.

Conclusion: The open-sourced dataset and toolkit aim to advance research in ring-based cardiovascular health sensing.

Abstract: Smart rings offer a convenient way to continuously and unobtrusively monitor
cardiovascular physiological signals. However, a gap remains between the ring
hardware and reliable methods for estimating cardiovascular parameters, partly
due to the lack of publicly available datasets and standardized analysis tools.
In this work, we present $\tau$-Ring, the first open-source ring-based dataset
designed for cardiovascular physiological sensing. The dataset comprises
photoplethysmography signals (infrared and red channels) and 3-axis
accelerometer data collected from two rings (reflective and transmissive
optical paths), with 28.21 hours of raw data from 34 subjects across seven
activities. $\tau$-Ring encompasses both stationary and motion scenarios, as
well as stimulus-evoked abnormal physiological states, annotated with four
ground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood
pressure. Using our proposed RingTool toolkit, we evaluated three widely-used
physics-based methods and four cutting-edge deep learning approaches. Our
results show superior performance compared to commercial rings, achieving best
MAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\%
for oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood
pressure estimation. The open-sourced dataset and toolkit aim to foster further
research and community-driven advances in ring-based cardiovascular health
sensing.

</details>


### [347] [Enhanced SCanNet with CBAM and Dice Loss for Semantic Change Detection](https://arxiv.org/pdf/2505.04199)
*Athulya Ratnayake, Buddhi Wijenayake, Praveen Sumanasekara, Roshan Godaliyadda, Vijitha Herath, Parakrama Ekanayake*

Main category: eess.IV

TL;DR: The paper proposes enhancing SCanNet with CBAM and Dice loss to improve semantic change detection in remote sensing by addressing noise, class boundaries, and imbalance.


<details>
  <summary>Details</summary>
Motivation: Current SCD models struggle with noisy inputs, subtle class boundaries, and class imbalance, despite advancements like transformers.

Method: Integrates CBAM (channel and spatial attention) and Dice loss into SCanNet for better feature representation and handling class imbalance.

Result: Quantitative and qualitative experiments on SECOND dataset show improved accuracy, clearer boundaries, and better small-change recovery.

Conclusion: CBAM and Dice loss effectively enhance feature representation and address class imbalance in SCD tasks.

Abstract: Semantic Change Detection (SCD) in remote sensing imagery requires accurately
identifying land-cover changes across multi-temporal image pairs. Despite
substantial advancements, including the introduction of transformer-based
architectures, current SCD models continue to struggle with challenges such as
noisy inputs, subtle class boundaries, and significant class imbalance. In this
study, we propose enhancing the Semantic Change Network (SCanNet) by
integrating the Convolutional Block Attention Module (CBAM) and employing Dice
loss during training. CBAM sequentially applies channel attention to highlight
feature maps with the most meaningful content, followed by spatial attention to
pinpoint critical regions within these maps. This sequential approach ensures
precise suppression of irrelevant features and spatial noise, resulting in more
accurate and robust detection performance compared to attention mechanisms that
apply both processes simultaneously or independently. Dice loss, designed
explicitly for handling class imbalance, further boosts sensitivity to minority
change classes. Quantitative experiments conducted on the SECOND dataset
demonstrate consistent improvements. Qualitative analysis confirms these
improvements, showing clearer segmentation boundaries and more accurate
recovery of small-change regions. These findings highlight the effectiveness of
attention mechanisms and Dice loss in improving feature representation and
addressing class imbalance in semantic change detection tasks.

</details>


### [348] [HYAMD High-Resolution Fundus Image Dataset for age related macular degeneration (AMD) Diagnosis](https://arxiv.org/pdf/2505.04230)
*Meishar Meisel, Benjamin A. Cohen, Meital Baskin, Beatrice Tiosano, Joachim A. Behar, Eran Berkowitz*

Main category: eess.IV

TL;DR: The HYAMD dataset is a longitudinal collection of 1,560 fundus images from 325 patients, including AMD and DR cases, with gold-standard annotations for AMD identification using machine learning.


<details>
  <summary>Details</summary>
Motivation: To provide an open-access retinal dataset from an Israeli sample to support AMD identification and research using machine learning models.

Method: The dataset includes DFIs from AMD and DR patients, with diagnoses confirmed by clinical evaluations, OCT, and angiography. Images were captured using a Topcon camera.

Result: HYAMD is the first open-access Israeli retinal dataset, offering high-quality annotated images for AMD research.

Conclusion: HYAMD serves as a valuable resource for advancing AMD identification and machine learning applications in ophthalmology.

Abstract: The Hillel Yaffe Age Related Macular Degeneration (HYAMD) dataset is a
longitudinal collection of 1,560 Digital Fundus Images (DFIs) from 325 patients
examined at the Hillel Yaffe Medical Center (Hadera, Israel) between 2021 and
2024. The dataset includes an AMD cohort of 147 patients (aged 54-94) with
varying stages of AMD and a control group of 190 diabetic retinopathy (DR)
patients (aged 24-92). AMD diagnoses were based on comprehensive clinical
ophthalmic evaluations, supported by Optical Coherence Tomography (OCT) and OCT
angiography. Non-AMD DFIs were sourced from DR patients without concurrent AMD,
diagnosed using macular OCT, fluorescein angiography, and widefield imaging.
HYAMD provides gold-standard annotations, ensuring AMD labels were assigned
following a full clinical assessment. Images were captured with a DRI OCT
Triton (Topcon) camera, offering a 45 deg field of view and 1960 x 1934 pixel
resolution. To the best of our knowledge, HYAMD is the first open-access
retinal dataset from an Israeli sample, designed to support AMD identification
using machine learning models.

</details>


### [349] [Adaptive Aggregation Weights for Federated Segmentation of Pancreas MRI](https://arxiv.org/pdf/2410.22530)
*Hongyi Pan, Gorkem Durak, Zheyuan Zhang, Yavuz Taktak, Elif Keles, Halil Ertugrul Aktas, Alpay Medetalibeyoglu, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Rajesh N. Keswani, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Michael G. Goggins, Michael B. Wallace, Ziyue Xu, Ulas Bagci*

Main category: eess.IV

TL;DR: A novel FL method with adaptive aggregation weights improves pancreas MRI segmentation accuracy across diverse datasets while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Traditional FL methods like FedAvg struggle with domain generalization due to variations in imaging protocols and demographics, especially in pancreas MRI segmentation.

Method: Introduces adaptive aggregation weights to dynamically adjust client contributions during model aggregation, addressing domain-specific differences.

Result: The approach enhances segmentation accuracy, reduces domain shift impact, and shows significant improvements across multiple hospitals.

Conclusion: The proposed method outperforms conventional FL techniques in handling heterogeneity while maintaining privacy.

Abstract: Federated learning (FL) enables collaborative model training across
institutions without sharing sensitive data, making it an attractive solution
for medical imaging tasks. However, traditional FL methods, such as Federated
Averaging (FedAvg), face difficulties in generalizing across domains due to
variations in imaging protocols and patient demographics across institutions.
This challenge is particularly evident in pancreas MRI segmentation, where
anatomical variability and imaging artifacts significantly impact performance.
In this paper, we conduct a comprehensive evaluation of FL algorithms for
pancreas MRI segmentation and introduce a novel approach that incorporates
adaptive aggregation weights. By dynamically adjusting the contribution of each
client during model aggregation, our method accounts for domain-specific
differences and improves generalization across heterogeneous datasets.
Experimental results demonstrate that our approach enhances segmentation
accuracy and reduces the impact of domain shift compared to conventional FL
methods while maintaining privacy-preserving capabilities. Significant
performance improvements are observed across multiple hospitals (centers).

</details>


### [350] [Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation](https://arxiv.org/pdf/2502.03501)
*Wang Xinyi, Kang Hongyu, Wei Peishan, Shuai Li, Yu Sun, Sai Kit Lam, Yongping Zheng*

Main category: eess.IV

TL;DR: The paper introduces Proxy Prompt (PP) for automated prompting and improved human-model interactions in SAM/SAM2, achieving state-of-the-art results with minimal training data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of automated prompting and enhance human-model interactions for SAM/SAM2, promoting their clinical adoption.

Method: Proposes PP, auto-generated using non-target data with pre-annotated masks, and a 3-step context-selection strategy. Includes a contextual colorization module for enhanced interactions.

Result: Achieves state-of-the-art performance on four datasets, comparable to fully-trained models, even with only 16 image masks.

Conclusion: PP effectively improves segmentation and human-model interactions, demonstrating strong performance with minimal training data.

Abstract: In this paper, we aim to address the unmet demand for automated prompting and
enhanced human-model interactions of SAM and SAM2 for the sake of promoting
their widespread clinical adoption. Specifically, we propose Proxy Prompt (PP),
auto-generated by leveraging non-target data with a pre-annotated mask. We
devise a novel 3-step context-selection strategy for adaptively selecting the
most representative contextual information from non-target data via vision
mamba and selective maps, empowering the guiding capability of non-target
image-mask pairs for segmentation on target image/video data. To reinforce
human-model interactions in PP, we further propose a contextual colorization
module via a dual-reverse cross-attention to enhance interactions between
target features and contextual-embedding with amplifying distinctive features
of user-defined object(s). Via extensive evaluations, our method achieves
state-of-the-art performance on four public datasets and yields comparable
results with fully-trained models, even when trained with only 16 image masks.

</details>
