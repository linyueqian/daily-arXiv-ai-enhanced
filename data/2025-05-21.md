<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 228]
- [cs.CV](#cs.CV) [Total: 167]
- [cs.AI](#cs.AI) [Total: 98]
- [cs.SD](#cs.SD) [Total: 24]
- [cs.LG](#cs.LG) [Total: 246]
- [cs.MA](#cs.MA) [Total: 10]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 27]
- [eess.IV](#eess.IV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/pdf/2505.13480)
*Avinash Patil, Siru Tao, Amardeep Gedhu*

Main category: cs.CL

TL;DR: The study evaluates how well large language models (LLMs) like Claude, GPT, Mistral, and LLaMA perform in assessing suicide risk using the C-SSRS scale, finding Claude and GPT align closely with human judgments, while Mistral has the lowest error. Ethical concerns and human oversight are emphasized.


<details>
  <summary>Details</summary>
Motivation: Suicide prevention is a public health challenge, and with people potentially disclosing suicidal thoughts to AI instead of humans, assessing LLMs' capability for risk evaluation is crucial.

Method: The study tests zero-shot performance of six LLMs (Claude, GPT, Mistral, LLaMA) in classifying posts on a 7-point C-SSRS severity scale, comparing results to human annotations.

Result: Claude and GPT align closely with human annotations; Mistral has the lowest ordinal prediction error. Misclassifications are usually between adjacent severity levels.

Conclusion: LLMs show promise for suicide risk assessment but require human oversight, transparency, and cautious deployment due to ethical concerns.

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [2] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/pdf/2505.13483)
*Xingyuan Lu, Yuxi Liu, Dongyu Zhang, Zhiyao Wu, Jing Ren, Feng Xia*

Main category: cs.CL

TL;DR: A multimodal dataset in Chinese for metaphorical advertisements is introduced to address gaps in fine-grained emotion classification research, featuring 5,000 annotated text-image pairs.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the lack of multimodal metaphorical emotion datasets and the English-centric focus in existing research, recognizing the complexity of emotion classification in multimodal metaphors.

Method: The researchers constructed a dataset of 5,000 Chinese text-image pairs from metaphorical advertisements, annotated for metaphor occurrence, domain relations, and fine-grained emotions.

Result: The dataset includes annotations for 10 emotion categories and is publicly available for further research.

Conclusion: This work fills a critical gap in multimodal metaphor research and supports advancements in emotional intelligence studies, particularly for non-English languages.

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [3] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/pdf/2505.13487)
*Ashwin Kumar, Yuzi He, Aram H. Markosyan, Bobbie Chern, Imanol Arrieta-Ibarra*

Main category: cs.CL

TL;DR: The paper investigates prefix bias in LLM-based reward models trained on human feedback data, revealing biases in racial and gender dimensions, and proposes a data augmentation method to mitigate them.


<details>
  <summary>Details</summary>
Motivation: To address underexplored biases in reward models derived from human preference datasets, particularly prefix bias triggered by minor query variations.

Method: Introduces novel methods to detect and evaluate prefix bias, tests these on diverse datasets and model architectures, and proposes a data augmentation strategy for mitigation.

Result: Significant biases in reward models across racial and gender dimensions were found, with the proposed augmentation method effectively reducing bias impact.

Conclusion: Highlights the need for bias-aware dataset design and evaluation to ensure fair and reliable reward models, contributing to AI fairness.

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [4] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/pdf/2505.13488)
*Federico Germani, Giovanni Spitale*

Main category: cs.CL

TL;DR: LLMs show high agreement in blind evaluations but biases emerge when source framing is introduced, particularly against Chinese sources.


<details>
  <summary>Details</summary>
Motivation: To assess the consistency, bias, and robustness of LLM evaluations of text, especially under framing effects.

Method: Systematic evaluation of 4,800 statements by four LLMs, manipulating source attribution (LLM vs. human of specified nationality).

Result: High inter- and intra-model agreement in blind conditions; breakdown when source framing is introduced, with bias against Chinese sources.

Conclusion: Framing effects significantly impact LLM evaluations, raising concerns about neutrality and fairness in LLM-mediated systems.

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [5] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/pdf/2505.13491)
*Aakash Gupta, Nataraj Das*

Main category: cs.CL

TL;DR: The paper proposes a GPT-3-based framework for abstractive summarization of product reviews to help users avoid decision paralysis by providing pros and cons.


<details>
  <summary>Details</summary>
Motivation: The surge in e-commerce post-pandemic has led to overwhelming product reviews, causing decision paralysis for buyers. Existing tools lack nuanced understanding.

Method: Fine-tunes a generative pre-trained transformer (GPT-3, Curie engine) for abstractive summarization, leveraging 13B parameters to capture review relationships.

Result: Generates pros and cons summaries with 'common-sense' insights, aiding quicker and better decision-making for users.

Conclusion: The framework effectively reduces review overload by providing meaningful summaries, empowering users to make informed decisions.

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [6] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/pdf/2505.13492)
*Weiming Zhang, Lingyue Fu, Qingyao Li, Kounianhua Du, Jianghao Lin, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu*

Main category: cs.CL

TL;DR: LLM4CD leverages large language models (LLMs) to enhance cognitive diagnosis (CD) by incorporating semantic relationships, addressing the limitations of ID-based methods and cold-start problems with new students/exercises.


<details>
  <summary>Details</summary>
Motivation: Current CD methods rely on ID relationships, ignoring semantic data and struggling with new entries in intelligent tutoring systems. LLMs offer potential to overcome these issues with open-world knowledge.

Method: LLM4CD uses LLMs to create cognitively expressive textual representations and introduces a bi-level encoder framework (macro-level cognitive text encoder and micro-level knowledge state encoder) to replace ID embeddings with semantic representations.

Result: The method outperforms previous CD models on real-world datasets, demonstrating effectiveness in incorporating semantic information.

Conclusion: LLM4CD successfully integrates LLMs to enrich CD tasks with semantic data, improving performance and handling new entries effectively.

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [7] [Memory-Centric Embodied Question Answer](https://arxiv.org/pdf/2505.13948)
*Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia*

Main category: cs.CL

TL;DR: MemoryEQA enhances Embodied Question Answering by prioritizing memory interaction across modules, improving efficiency and accuracy in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing EQA frameworks are planner-centric, limiting memory module interaction. MemoryEQA addresses this by fully integrating memory into all modules.

Method: Proposes a multi-modal hierarchical memory mechanism (global and local memory) and leverages a large language model to adapt memory for module inputs.

Result: Achieves a 19.8% performance gain on the MT-HM3D dataset, demonstrating superior memory capability in complex tasks.

Conclusion: MemoryEQA's memory-centric approach significantly improves EQA performance, highlighting the importance of memory in handling complex, multi-target tasks.

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [8] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/pdf/2505.13498)
*Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen*

Main category: cs.CL

TL;DR: IRLBench is a new benchmark for evaluating LLMs in multilingual settings, focusing on English and endangered Irish, revealing performance gaps and cultural biases.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored performance of LLMs in low-resource and multilingual settings, particularly for endangered languages like Irish, and overcoming biases in existing benchmarks.

Method: Developed IRLBench using 2024 Irish Leaving Certificate exams, framing tasks as long-form generation and leveraging official marking schemes for comprehensive evaluation.

Result: LLMs perform worse in Irish (55.8% correctness) vs. English (76.2%), with valid Irish responses under 80% of the time.

Conclusion: IRLBench highlights the need for robust, culturally aware multilingual AI development, with released resources to support future research.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [9] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/pdf/2505.13500)
*Prithviraj Singh Shahani, Matthias Scheutz*

Main category: cs.CL

TL;DR: The paper investigates the robustness of safety fine-tuning in LLMs by injecting Gaussian noise, revealing vulnerabilities in current safety alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Understanding the resilience of safety guardrails in LLMs under perturbation, as their robustness is poorly understood.

Method: Systematically injecting Gaussian noise into model activations and evaluating its impact on harmful-output rates and reasoning.

Result: Gaussian noise increases harmful-output rates by up to 27%, deeper safety fine-tuning offers no extra protection, and chain-of-thought reasoning remains intact.

Conclusion: Current safety alignment techniques are vulnerable; reasoning-based and reinforcement learning approaches are promising for robust AI safety systems.

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [10] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.13506)
*Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, Chenyang Tu*

Main category: cs.CL

TL;DR: EcoSafeRAG enhances RAG security by detecting malicious content without relying on LLM internal knowledge, improving performance and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the security vulnerabilities introduced by RAG, such as corpus poisoning, without conflicting with RAG's design by avoiding reliance on LLM internal knowledge.

Method: Uses sentence-level processing and bait-guided context diversity detection to analyze candidate documents for malicious content.

Result: Achieves state-of-the-art security, improves clean-scenario performance, and reduces operational costs (1.2× latency, 48%-80% token reduction vs. Vanilla RAG).

Conclusion: EcoSafeRAG effectively bridges the security gap in RAG systems while maintaining efficiency and performance.

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [11] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/pdf/2505.14272)
*Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser*

Main category: cs.CL

TL;DR: The paper proposes a cross-lingual transfer learning method using nearest-neighbor retrieval to augment limited labeled hate speech data in low-resource languages, improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Labeled hate speech data is scarce and costly to collect, especially for low-resource languages, necessitating efficient methods to enhance detection with minimal data.

Method: Leverages nearest-neighbor retrieval to augment small labeled datasets in target languages by retrieving relevant examples from a multilingual hate speech pool. Uses maximum marginal relevance to reduce redundancy.

Result: Outperforms models trained on target language data alone and often surpasses state-of-the-art, achieving high performance with as few as 200 retrieved instances.

Conclusion: The approach is data-efficient, scalable, and adaptable to new languages and tasks, demonstrating significant improvements in hate speech detection.

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [12] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/pdf/2505.13508)
*Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You*

Main category: cs.CL

TL;DR: Time-R1 is a framework enhancing a 3B-parameter LLM with comprehensive temporal abilities—understanding, prediction, and creative generation—via a three-stage RL curriculum, outperforming much larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs lack robust temporal intelligence, struggling with integrating past reasoning and future predictions, while existing methods target isolated skills with poor generalization.

Method: A three-stage RL curriculum with dynamic rule-based rewards: (1) foundational temporal understanding, (2) future event prediction, (3) creative scenario generation without fine-tuning.

Result: Time-R1 outperforms models 200 times larger (e.g., 671B DeepSeek-R1) on future event prediction and creative generation benchmarks.

Conclusion: Progressive RL fine-tuning enables smaller models to achieve superior temporal performance, offering a scalable path to time-aware AI. Time-Bench dataset and Time-R1 checkpoints are released for research.

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [13] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/pdf/2505.13514)
*Shuxun Wang, Qingyu Yin, Chak Tou Leong, Qiang Zhang, Linyi Yang*

Main category: cs.CL

TL;DR: The paper investigates how induction heads in LLMs cause repetitive outputs (repetition curse) and proposes mitigation techniques.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind the repetition curse in LLMs, focusing on induction heads' role.

Method: Analyzes induction heads' dominance in output logits and proposes attention head regularization to reduce their toxicity.

Result: Identifies induction heads as key drivers of repetition and suggests mitigation strategies.

Conclusion: Provides insights into LLM design and training, offering techniques to improve output diversity.

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [14] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/pdf/2505.13527)
*Jingyu Peng, Maolin Wang, Nan Wang, Xiangyu Zhao, Jiatong Li, Kai Zhang, Qi Liu*

Main category: cs.CL

TL;DR: LogiBreak is a black-box jailbreak method that converts harmful prompts into logical expressions to bypass LLM safety systems, exploiting distributional gaps between alignment and logic-based inputs.


<details>
  <summary>Details</summary>
Motivation: Current safety mechanisms for LLMs are vulnerable to jailbreak attacks due to distributional discrepancies between alignment and malicious prompts.

Method: LogiBreak translates harmful natural language prompts into formal logical expressions to exploit the gap in alignment data and logic-based inputs.

Result: LogiBreak is effective in bypassing safety constraints across multilingual jailbreak datasets in various settings.

Conclusion: The method highlights vulnerabilities in LLM safety systems and suggests a need for improved alignment techniques.

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [15] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/pdf/2505.13554)
*Zhanglin Wu, Daimeng Wei, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Zongyao Li, Yuanchang Luo, Jinlong Yang, Zhiqiang Rao, Hao Yang*

Main category: cs.CL

TL;DR: The paper explores integrating NMT and LLM for translation, proposing a scheduling policy to minimize LLM usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in translation but suffer from high costs and latency, often performing comparably to NMT systems. A hybrid approach is needed.

Method: A novel decider leveraging source sentence features is proposed to schedule between NMT and LLM usage, optimizing performance and efficiency.

Result: Experiments show optimal translation performance with minimal LLM usage, validating the decider's effectiveness.

Conclusion: Integrating NMT and LLM with a smart scheduling policy balances performance and efficiency, making LLM usage practical.

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [16] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/pdf/2505.13559)
*Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, Eng Siong Chng*

Main category: cs.CL

TL;DR: CS-Sum evaluates LLMs' ability to understand code-switched (CS) dialogues through summarization, revealing subtle but critical errors despite high automated scores.


<details>
  <summary>Details</summary>
Motivation: Code-switching (CS) challenges LLMs, but its comprehensibility is underexplored. CS-Sum aims to assess this gap.

Method: CS-Sum benchmarks CS dialogue summarization across Mandarin-English, Tamil-English, and Malay-English, evaluating ten LLMs with few-shot, translate-summarize, and fine-tuning approaches.

Result: LLMs score high on metrics but make subtle errors altering dialogue meaning, with error rates varying by language pair and model.

Conclusion: Specialized training on CS data is needed, as current LLMs struggle with nuanced CS comprehension.

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [17] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/pdf/2505.13628)
*Nathaniel Krasner, Nicholas Lanuzo, Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: Multilingual alignment of sentence representations using visual information instead of bitexts, enabling efficient alignment for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To explore if visual information can replace bitexts for multilingual alignment, especially for low-resource languages.

Method: Using multilingual image-caption alignment to implicitly align text representations across languages, even those unseen during pretraining.

Result: Aligned representations work for cross-lingual NLU and bitext retrieval.

Conclusion: Visual information can effectively bridge the gap for multilingual alignment, offering a scalable solution for low-resource languages.

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [18] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/pdf/2505.13657)
*Charles J. Torres, Richard Futrell*

Main category: cs.CL

TL;DR: The paper introduces a unified, script-agnostic metric for orthographic transparency using mutual compressibility between orthographic and phonological strings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified metric for orthographic transparency across different scripts.

Method: Quantifies transparency using mutual compressibility, combining irregular spellings and rule complexity, estimated via neural sequence models.

Result: Evaluated 22 languages, confirming intuitive rankings of script transparency.

Conclusion: Mutual compressibility provides a simple, principled, and general measure for orthographic transparency.

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [19] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/pdf/2505.13706)
*Julia Jose, Rachel Greenstadt*

Main category: cs.CL

TL;DR: GPT-4 outperforms GPT-3.5 and Claude 3 Opus in detecting propaganda techniques but falls short of RoBERTa-CRF. LLMs excel in detecting specific techniques like name-calling, appeal to fear, and flag-waving compared to MGN.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of Large Language Models (LLMs) in detecting propaganda techniques in news articles, comparing them with transformer-based models.

Method: Comparison of LLMs (GPT-4, GPT-3.5, Claude 3 Opus) with transformer-based models (RoBERTa-CRF, MGN) on detecting propaganda techniques.

Result: GPT-4 shows superior F1 scores (0.16) over GPT-3.5 and Claude 3 Opus but underperforms RoBERTa-CRF (0.67). LLMs outperform MGN in detecting specific techniques like name-calling, appeal to fear, and flag-waving.

Conclusion: While LLMs, especially GPT-4, show promise in detecting propaganda, they do not surpass specialized models like RoBERTa-CRF, indicating room for improvement.

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [20] [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/pdf/2505.06149)
*Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser*

Main category: cs.CL

TL;DR: The paper evaluates multilingual LLMs for hate speech detection, finding that zero-shot and few-shot prompting underperform fine-tuned models but generalize better, with prompt design being crucial.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how multilingual LLMs perform in hate speech detection across diverse languages using zero-shot and few-shot prompting.

Method: Evaluated LLM prompting techniques (zero-shot, few-shot) across eight non-English languages, comparing them to fine-tuned encoder models.

Result: Zero-shot and few-shot prompting lag behind fine-tuned models in real-world evaluations but generalize better in functional tests. Prompt design is critical and language-specific.

Conclusion: Multilingual LLMs show promise for hate speech detection but require tailored prompting techniques for optimal performance across languages.

Abstract: Despite growing interest in automated hate speech detection, most existing
approaches overlook the linguistic diversity of online content. Multilingual
instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ
offer promising capabilities across languages, but their effectiveness in
identifying hate speech through zero-shot and few-shot prompting remains
underexplored. This work evaluates LLM prompting-based detection across eight
non-English languages, utilizing several prompting techniques and comparing
them to fine-tuned encoder models. We show that while zero-shot and few-shot
prompting lag behind fine-tuned encoder models on most of the real-world
evaluation sets, they achieve better generalization on functional tests for
hate speech detection. Our study also reveals that prompt design plays a
critical role, with each language often requiring customized prompting
techniques to maximize performance.

</details>


### [21] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/pdf/2505.13725)
*Yu Guo, Dong Jin, Shenghao Ye, Shuangwu Chen, Jian Yang, Xiaobin Tan*

Main category: cs.CL

TL;DR: SQLForge enhances text-to-SQL reasoning in LLMs by synthesizing reliable and diverse data, achieving state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The performance gap between open-source and closed-source LLMs in text-to-SQL tasks motivates the development of SQLForge.

Method: SQLForge improves data reliability via SQL syntax constraints and reverse translation, and boosts diversity through template enrichment and iterative domain exploration. Fine-tuning models on this data yields SQLForge-LM.

Result: SQLForge-LM achieves 85.7% EX accuracy on Spider Dev and 59.8% on BIRD Dev, outperforming other open-source models.

Conclusion: SQLForge effectively narrows the performance gap between open-source and closed-source LLMs in text-to-SQL tasks.

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [22] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/pdf/2505.13761)
*Jacob Kleiman, Kevin Frank, Sindy Campagna*

Main category: cs.CL

TL;DR: A framework combining simulations and LLMs to make complex simulations accessible via intuitive language interactions while grounding LLMs in accurate real-world dynamics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between complex simulations (inaccessible to non-technical users) and LLMs (lacking structured causal understanding) by integrating their strengths.

Method: Developed a simulation agent framework that merges LLMs' conversational abilities with simulations' structured accuracy.

Result: Enabled seamless interaction with simulations via LLMs while grounding LLMs in real-world dynamics.

Conclusion: The framework offers a robust, generalizable solution for empirical validation across diverse domains.

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [23] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/pdf/2505.13772)
*Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros*

Main category: cs.CL

TL;DR: Llama-Krikri-8B is a Greek-optimized LLM based on Llama 3.1-8B, excelling in language understanding, generation, and code tasks, with support for Modern/Ancient Greek and English.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality Greek language models by leveraging Meta's Llama 3.1-8B and extensive Greek data training.

Method: Extensive training on Greek data, multi-stage post-training with human/synthetic data (e.g., MAGPIE), and evaluation using novel Greek benchmarks.

Result: Outperforms comparable Greek and multilingual LLMs in understanding, generation, and code tasks.

Conclusion: Llama-Krikri-8B is a state-of-the-art Greek LLM with broad linguistic and computational capabilities.

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [24] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/pdf/2505.13792)
*Siddhant Bhambri, Upasana Biswas, Subbarao Kambhampati*

Main category: cs.CL

TL;DR: The paper explores evaluating reasoning traces in Knowledge Distillation (KD) for smaller language models (SLMs) in QA tasks, finding that correct traces don't always lead to correct final answers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating the faithfulness of reasoning traces in KD for SLMs, given their verbosity and interpretability issues.

Method: Uses rule-based problem decomposition to break queries into structured sub-problems, generating interpretable traces. Tested on Open Book QA with Classification and Information Retrieval steps.

Result: Correct reasoning traces don't guarantee correct final answers, and there's low correlation between trace and final answer correctness.

Conclusion: Challenges the assumption that reasoning traces improve SLMs' performance via KD, highlighting the need for better evaluation methods.

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [25] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/pdf/2505.13840)
*Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye*

Main category: cs.CL

TL;DR: EfficientLLM is a benchmark and empirical study evaluating efficiency techniques for LLMs, covering pretraining, fine-tuning, and inference, with insights on trade-offs, task-dependency, and cross-modality generalization.


<details>
  <summary>Details</summary>
Motivation: The growing compute, energy, and monetary costs of LLMs necessitate efficient techniques to balance performance and resource usage.

Method: Systematic evaluation of 100+ model-technique pairs across architecture pretraining, fine-tuning, and inference, using six metrics on a production-class cluster.

Result: Key insights include trade-offs (e.g., MoE reduces FLOPs but increases VRAM), task-dependent optima (e.g., MQA for memory-latency), and cross-modality generalization.

Conclusion: EfficientLLM provides actionable guidance for optimizing efficiency in next-generation foundation models, with open-sourced resources for broader adoption.

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [26] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/pdf/2505.13844)
*Congchi Yin, Yongpeng Zhang, Xuyun Wen, Piji Li*

Main category: cs.CL

TL;DR: The paper explores improving alignment between language models and human brain activity by integrating associative memory, showing enhanced alignment in relevant brain regions and better performance with fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To enhance the alignment between language models and human brain activity during speech processing by leveraging associative memory.

Method: Mapping language model activations to brain activity, expanding text stimuli with simulated associative memory, and fine-tuning models using the Association dataset.

Result: Improved alignment in brain regions related to associative memory and better performance with fine-tuned large language models.

Conclusion: Integrating associative memory improves language model-brain alignment, with fine-tuned models showing superior results.

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [27] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/pdf/2505.13855)
*Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch*

Main category: cs.CL

TL;DR: DoGEN (Domain Gating Ensemble Networks) improves machine-generated text detection by adapting to unseen domains using ensemble models and domain classifier weights, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The increasing capability of language models necessitates robust detection of machine-generated text, but current detectors fail to adapt to new domains and models.

Method: DoGEN ensembles domain expert detector models with weights from a domain classifier to adapt to unseen domains.

Result: DoGEN achieves top performance in in-domain detection and outperforms larger models in out-of-domain detection.

Conclusion: DoGEN advances domain-adaptive AI detection, with released code and models to support future research.

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [28] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/pdf/2505.14286)
*Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill*

Main category: cs.CL

TL;DR: Speech LLMs combining pre-trained encoders and large language models are vulnerable to universal adversarial attacks, including selective triggers based on input attributes.


<details>
  <summary>Details</summary>
Motivation: To examine vulnerabilities in speech LLMs due to their flexibility, focusing on universal adversarial attacks.

Method: Prepend a fixed adversarial audio segment to inputs, testing attacks causing no output or task modification, then extending to selective triggers based on attributes like speaker gender or language.

Result: Critical vulnerabilities found in Qwen2-Audio and Granite-Speech, suggesting broader susceptibility in similar models.

Conclusion: Speech LLMs need more robust training and adversarial resistance to mitigate these vulnerabilities.

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [29] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/pdf/2505.13866)
*Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim*

Main category: cs.CL

TL;DR: RPC accelerates inference in reasoning-focused language models by compressing KV cache, improving throughput with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Long reasoning paths in language models increase memory usage and reduce throughput, limiting practical deployment.

Method: Proposes Reasoning Path Compression (RPC), a training-free method that compresses KV cache by retaining high-importance entries based on recent queries.

Result: RPC improves QwQ-32B's throughput by 1.60× with only a 1.2% accuracy drop on AIME 2024.

Conclusion: Semantic sparsity in reasoning paths can be exploited for efficient compression, aiding practical LLM deployment.

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [30] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/pdf/2505.13886)
*Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang*

Main category: cs.CL

TL;DR: Code2Logic uses game code to synthesize multimodal reasoning data, creating the GameQA dataset to improve Vision Language Models (VLMs) efficiently.


<details>
  <summary>Details</summary>
Motivation: Scarcity of visual-language CoT data limits VLM reasoning; game code offers a cost-effective alternative.

Method: Leverage LLMs to adapt game code for automatic reasoning data synthesis via code execution.

Result: GameQA dataset (30 games, 158 tasks) improves VLM performance, e.g., Qwen2.5-VL-7B by 2.33%.

Conclusion: Code2Logic provides scalable, effective data synthesis for VLMs, demonstrating cross-domain generalization.

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [31] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/pdf/2505.13890)
*Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang*

Main category: cs.CL

TL;DR: The paper introduces a graph-based framework to analyze reasoning in LLMs, revealing structural properties' impact on accuracy and prompting strategies' influence on reasoning structure.


<details>
  <summary>Details</summary>
Motivation: To address the counterintuitive and unstable behaviors of Reasoning LLMs (RLMs) and improve understanding of their reasoning processes.

Method: Clusters Chain-of-Thought (CoT) outputs into reasoning steps and constructs directed reasoning graphs to analyze dependencies.

Result: Structural properties (e.g., exploration density, branching) correlate with reasoning accuracy, and prompting strategies reshape reasoning structure.

Conclusion: The framework enables quantitative evaluation of reasoning quality and offers insights for prompt engineering and LLM cognitive analysis.

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [32] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/pdf/2505.13893)
*Yuanyi Wang, Zhaoyi Yan, Yiming Zhang, Qi Zhou, Yanggan Gu, Fei Wu, Hongxia Yang*

Main category: cs.CL

TL;DR: InfiGFusion is a structure-aware fusion framework for LLMs, using Graph-on-Logits Distillation to model semantic dependencies, improving fusion quality and outperforming SOTA models.


<details>
  <summary>Details</summary>
Motivation: Existing logit-based fusion methods ignore semantic dependencies, which are crucial for aligning diverse model behaviors.

Method: Proposes InfiGFusion with Graph-on-Logits Distillation, forming a co-activation graph and using a scalable approximation for efficiency.

Result: Outperforms SOTA models on 11 benchmarks, with significant gains in complex reasoning tasks.

Conclusion: InfiGFusion effectively models semantic dependencies, enhancing fusion quality and performance in diverse tasks.

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [33] [Let's Verify Math Questions Step by Step](https://arxiv.org/pdf/2505.13903)
*Chengyu Shen, Zhen Hao Wong, Runming He, Hao Liang, Meiyi Qiang, Zimo Meng, Zhengyang Zhao, Bohan Zeng, Zhengzhou Zhu, Bin Cui, Wentao Zhang*

Main category: cs.CL

TL;DR: MathQ-Verify is a five-stage pipeline to filter ill-posed math questions, improving dataset quality and verification performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on correct reasoning paths but overlook question validity, leading to unreliable datasets.

Method: A five-stage pipeline: format validation, formalization, contradiction detection, completeness check, and lightweight model voting.

Result: Achieves state-of-the-art performance, improving F1 by 25 points, with 90% precision and 63% recall.

Conclusion: MathQ-Verify provides a scalable, accurate solution for curating reliable math datasets, reducing noise and computation.

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [34] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/pdf/2505.13908)
*Ajitesh Bankula, Praney Bankula*

Main category: cs.CL

TL;DR: The paper explores how language family proximity and morphological similarity impact cross-lingual transfer in multilingual NLP, comparing model performance and reviewing linguistic distance metrics.


<details>
  <summary>Details</summary>
Motivation: To understand how language families and morphology influence the effectiveness of cross-lingual transfer in multilingual NLP models.

Method: Investigates transfer performance by analyzing language family proximity and morphological similarity, and reviews linguistic distance metrics.

Result: Finds correlations between linguistic distance metrics and transfer outcomes, and discusses emerging approaches to improve transfer.

Conclusion: Highlights the importance of linguistic factors in cross-lingual transfer and suggests integrating typological and morphological information into model pre-training.

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [35] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/pdf/2505.13913)
*Hiram Ring*

Main category: cs.CL

TL;DR: The paper explores word order change in languages, proposing a universal mechanism based on a large dataset. Findings support a 'Min-Max' theory balancing processing and information structure pressures.


<details>
  <summary>Details</summary>
Motivation: To resolve disagreements between innate and functional theories of language evolution by identifying a universal mechanism for word order change.

Method: Analysis of a large parallel dataset (1,500+ languages, 133 families, 111 isolates) to correlate word class length with word order.

Result: Word class length correlates with word order, supporting processing theories, and predicts historical change better than descent or language area.

Conclusion: An integrated 'Min-Max' theory aligns with efficiency and information-theoretic models, suggesting competing pressures drive language evolution.

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [36] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/pdf/2505.13936)
*Saydul Akbar Murad, Ashim Dahal, Nick Rahimi*

Main category: cs.CL

TL;DR: The paper introduces R1 Translator, a model combining bidirectional LSTM and transformer-based decoder to improve EEG-to-text decoding, outperforming existing models in ROUGE, CER, and WER metrics.


<details>
  <summary>Details</summary>
Motivation: To address performance limitations in EEG-to-text decoding models, bridging the gap between human brain signals and language processing.

Method: R1 Translator uses a bidirectional LSTM encoder and pretrained transformer decoder to process EEG embeddings for text generation.

Result: R1 outperforms T5 and Brain Translator in ROUGE-1 (38.00%), ROUGE-L (32.51%), CER (0.5795), and WER (0.7280).

Conclusion: R1 Translator significantly improves EEG-to-text decoding performance, offering a robust solution for brain-language processing.

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [37] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/pdf/2505.13944)
*Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, Nam Le, Linh Ngo Van*

Main category: cs.CL

TL;DR: WAVE++ is a novel prompt-based method for Continual Relation Extraction (CRE) that addresses challenges like task identity accuracy and catastrophic forgetting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Memory-based CRE approaches raise privacy and memory concerns, while current prompt-based methods struggle with task identity and forgetting issues.

Method: WAVE++ uses task-specific prompt pools inspired by prefix-tuning, incorporates label descriptions, and integrates a generative model to avoid explicit data storage.

Result: WAVE++ outperforms state-of-the-art prompt-based and rehearsal-based methods in CRE.

Conclusion: WAVE++ provides a robust, flexible, and privacy-preserving solution for CRE, with publicly available code.

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [38] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/pdf/2505.13949)
*Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, Zheng Hu*

Main category: cs.CL

TL;DR: FlashThink introduces a verification model to shorten LLM reasoning content without losing accuracy, reducing reasoning length by ~77% for tested models.


<details>
  <summary>Details</summary>
Motivation: LLMs generate excessively long reasoning content, causing computational overhead, even when shorter reasoning suffices for correct answers.

Method: A verification model identifies when to stop reasoning early, ensuring correctness while reducing content length.

Result: FlashThink reduces reasoning content by ~77% for Deepseek-R1 and QwQ-32B models without accuracy loss.

Conclusion: Early reasoning exit via verification models can make LLM reasoning more efficient without sacrificing accuracy.

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [39] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/pdf/2505.13963)
*Qianli Wang, Mingyang Wang, Nils Feldhus, Simon Ostermann, Yuan Cao, Hinrich Schütze, Sebastian Möller, Vera Schmitt*

Main category: cs.CL

TL;DR: Quantization impacts LLM explainability and interpretability unpredictably, varying by method, approach, and evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored effects of quantization on LLM explainability and interpretability, crucial for understanding model decisions.

Method: Comprehensive experiments with three quantization techniques, two explainability methods, and two interpretability approaches, plus a user study.

Result: Quantization's effect on transparency is inconsistent, sometimes degrading or improving it, depending on configuration.

Conclusion: Quantization unpredictably affects model transparency, cautioning its use in transparency-critical applications.

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [40] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/pdf/2505.13965)
*Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Xiang Liu, Xuming Hu*

Main category: cs.CL

TL;DR: CAFES is a collaborative multi-agent framework for Automated Essay Scoring (AES) that improves evaluation generalizability and human alignment, achieving a 21% QWK improvement.


<details>
  <summary>Details</summary>
Motivation: Traditional AES methods lack generalizability and multimodal perception, while MLLM-based approaches produce misaligned scores. CAFES addresses these gaps.

Method: CAFES uses three agents: Initial Scorer, Feedback Pool Manager, and Reflective Scorer, working collaboratively to refine scores iteratively.

Result: Experiments show a 21% improvement in QWK, particularly in grammatical and lexical diversity.

Conclusion: CAFES advances AES by enhancing human alignment and multimodal evaluation, with code to be released.

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [41] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/pdf/2505.13972)
*Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt*

Main category: cs.CL

TL;DR: Judge models independent of and not fine-tuned to the generator model provide the most reliable label flipping evaluations for counterfactual data augmentation (CDA), but human intervention is still needed.


<details>
  <summary>Details</summary>
Motivation: Inconsistencies in judge model selection for evaluating counterfactuals in CDA prompted investigation into the relationship between generator and judge models.

Method: Experiments with two LLM-based methods, three datasets, five generator models, and 15 judge models, plus a user study (n=90).

Result: Independent, non-fine-tuned judge models yield reliable evaluations, but the gap between automated results and human judgment remains large.

Conclusion: Automated CDA pipelines may be insufficient without human intervention.

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [42] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/pdf/2505.13973)
*Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl Razi, Aris Sotiras, Yi Su, Yalin Wang*

Main category: cs.CL

TL;DR: RL-based tuning, especially GRPO, improves MLLMs but faces challenges in medical tasks. The study explores four key factors for effective RL tuning in medical VQA, showing GRPO outperforms SFT.


<details>
  <summary>Details</summary>
Motivation: Aligning model responses with clinical expectations in medical tasks.

Method: Investigates base model initialization, medical semantic alignment, length-based rewards, and bias impact through experiments.

Result: GRPO-based RL tuning outperforms SFT in accuracy and reasoning quality.

Conclusion: Provides insights for domain-specific fine-tuning of medical MLLMs, highlighting GRPO's superiority.

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [43] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/pdf/2505.13975)
*Yuxuan Jiang, Dawei Li, Frank Ferraro*

Main category: cs.CL

TL;DR: DRP improves efficiency of Large Reasoning Models by pruning verbose reasoning traces and distilling pruned paths into a student model, achieving better token efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LRMs often produce excessively verbose reasoning traces, leading to inefficiency. DRP aims to address this by combining pruning and distillation.

Method: DRP uses a teacher model for skill-aware step decomposition and content pruning, then distills pruned reasoning paths into a student model.

Result: DRP reduces token usage (e.g., GSM8K: 917 to 328) while improving accuracy (91.7% to 94.1%) and achieves 43% token reduction on AIME with no drop in performance.

Conclusion: Aligning training CoTs with the student's reasoning capacity is key for effective knowledge transfer and performance gains.

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [44] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/pdf/2505.13979)
*Maya Srikanth, Run Chen, Julia Hirschberg*

Main category: cs.CL

TL;DR: Multimodal empathy detection models struggle with conflicting cues. Analyzing disagreements between unimodal and multimodal predictions reveals ambiguity and annotator uncertainty, highlighting challenges in fusion and human-model consistency.


<details>
  <summary>Details</summary>
Motivation: To understand why multimodal models fail when modalities provide conflicting cues and to improve empathy detection robustness.

Method: Fine-tuned models for text, audio, and video, combined with a gated fusion model, were used to analyze disagreements between unimodal and multimodal predictions.

Result: Disagreements often reflect ambiguity, with dominant signals in one modality misleading fusion. Humans and models show inconsistent benefits from multimodal input.

Conclusion: Disagreement serves as a diagnostic signal for challenging examples, aiding in improving empathy detection system robustness.

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [45] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/pdf/2505.13988)
*Linxin Song, Taiwei Shi, Jieyu Zhao*

Main category: cs.CL

TL;DR: RFT improves LLM reasoning but harms trustworthiness by reducing refusal rates (hallucination tax). SUM dataset helps restore refusal behavior with minimal trade-offs.


<details>
  <summary>Details</summary>
Motivation: To study the side effect of RFT on model trustworthiness, specifically the hallucination tax, where models confidently answer unanswerable questions.

Method: Introduce SUM, a dataset of unanswerable math problems, and test RFT's impact on refusal rates. Incorporate SUM during RFT to restore refusal behavior.

Result: Standard RFT reduces refusal rates by 80%, increasing hallucinations. Adding 10% SUM during RFT restores refusal behavior with minimal accuracy loss.

Conclusion: Incorporating SUM during RFT improves model trustworthiness by enhancing refusal behavior and generalization to other tasks.

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [46] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/pdf/2505.13990)
*Tingfeng Hui, Pengyu Zhu, Bowen Ping, Ling Tang, Yaqi Zhang, Sen Su*

Main category: cs.CL

TL;DR: DecIF is an autonomous framework for generating high-quality instruction-following data using LLMs, leveraging decomposition and meta-information to ensure diversity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on external resources, limiting flexibility and generalizability. DecIF addresses this by autonomously generating instruction data.

Method: DecIF uses decomposition to guide LLMs in generating meta-information and response constraints, ensuring well-structured instructions. It also detects and resolves inconsistencies.

Result: DecIF outperforms existing approaches in instruction-following tasks, demonstrating flexibility, scalability, and generalizability.

Conclusion: DecIF is a robust solution for synthesizing high-quality instruction data autonomously, enhancing LLM capabilities.

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [47] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/pdf/2505.13995)
*Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, Dan Jurafsky*

Main category: cs.CL

TL;DR: The paper addresses sycophancy in LLMs, introducing a theory of social sycophancy and a framework (ELEPHANT) to evaluate it. Results show LLMs exhibit high sycophancy rates, which is hard to mitigate.


<details>
  <summary>Details</summary>
Motivation: Existing work overlooks sycophancy in ambiguous contexts like advice-seeking, where it can reinforce harmful beliefs. This paper aims to fill this gap.

Method: Introduces ELEPHANT, a framework to evaluate social sycophancy across five face-preserving behaviors, tested on two datasets (OEQ and AITA).

Result: LLMs show high sycophancy: 47% more face-preservation than humans on OEQ and 42% inappropriate affirmation on AITA. Sycophancy is rewarded in preference datasets and hard to mitigate.

Conclusion: The paper provides theoretical and empirical tools to understand and address social sycophancy in LLMs, highlighting its prevalence and challenges.

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [48] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/pdf/2505.14009)
*Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, Linqi Song*

Main category: cs.CL

TL;DR: ACM is a model merging framework that uses activation-guided consensus to improve efficiency and accuracy in combining LLMs, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods overlook functional heterogeneity in neural layers, limiting their effectiveness.

Method: ACM determines layer-specific merging coefficients using mutual information between activations, avoiding gradient computations or training.

Result: ACM reduces response length by 55.3% and improves reasoning accuracy by 1.3 points in Qwen-7B models.

Conclusion: ACM is a plug-and-play solution for efficient and accurate model merging, with publicly available code.

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [49] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/pdf/2505.14015)
*Tai D. Nguyen, Long H. Pham, Jun Sun*

Main category: cs.CL

TL;DR: AutoLaw is a dynamic framework for enhancing legal compliance in LLMs by combining adversarial data generation and a jury-inspired deliberation process, outperforming static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing legal evaluation benchmarks lack adaptability to diverse local contexts, limiting their utility in evolving regulatory landscapes.

Method: AutoLaw uses adversarial data generation and a jury-inspired deliberation process with ranked LLM-based jurors to simulate judicial decision-making.

Result: AutoLaw improves LLM discrimination and boosts violation detection rates across benchmarks like Law-SG, Case-SG, and Unfair-TOS.

Conclusion: AutoLaw offers a scalable, context-aware solution for evaluating and enhancing LLMs in legally sensitive applications.

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [50] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/pdf/2505.14045)
*Yingli Shen, Wen Lai, Shuo Wang, Kangyang Luo, Alexander Fraser, Maosong Sun*

Main category: cs.CL

TL;DR: The paper introduces TED2025, a large-scale multi-way parallel corpus for 113 languages, and demonstrates its effectiveness in improving multilingual LLMs through continued pretraining and instruction tuning.


<details>
  <summary>Details</summary>
Motivation: Unaligned multilingual data limits cross-lingual semantics, while multi-way parallel data (like TED2025) offers stronger consistency and better performance for LLMs.

Method: The study uses TED2025, a corpus with up to 50 aligned languages, to explore strategies for pretraining and instruction tuning, analyzing key factors.

Result: Models trained on multi-way parallel data outperform those on unaligned data across six multilingual benchmarks.

Conclusion: Multi-way parallel data, exemplified by TED2025, significantly enhances multilingual LLM performance compared to unaligned data.

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [51] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/pdf/2505.14052)
*Wei Jiang, Anying Fu, Youling Zhang*

Main category: cs.CL

TL;DR: MAMA Pruning is a novel method combining Movement and Magnitude Analysis to prune large language models effectively, reducing size and complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods cause performance degradation or require extensive retraining. The goal is to create smaller, faster models without sacrificing quality.

Method: Uses weights and bias from pre-training and GRPO rewards from post-training as pruning indicators.

Result: Outperforms state-of-the-art methods across pruning levels and tasks.

Conclusion: MAMA Pruning achieves efficient model compression with minimal performance loss.

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [52] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/pdf/2505.14070)
*Feiyu Duan, Xuemiao Zhang, Sirui Wang, Haoran Que, Yuqi Liu, Wenge Rong, Xunliang Cai*

Main category: cs.CL

TL;DR: The paper introduces a High-Knowledge Scorer (HKS) to select high-quality training data for LLMs by focusing on knowledge richness, improving model performance in knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods overlook knowledge richness in text corpora, leading to knowledge scarcity in pre-trained models.

Method: Proposes HKS, a gradient-free method using knowledge density and coverage metrics, and a multi-domain knowledge element pool for data selection.

Result: Training on high-knowledge data improves model performance in knowledge-intensive and general tasks, enhancing both generic and domain-specific capabilities.

Conclusion: HKS effectively addresses knowledge scarcity in LLMs by selecting knowledge-rich data, boosting model performance across tasks.

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [53] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/pdf/2505.14079)
*Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei*

Main category: cs.CL

TL;DR: BAR, a backward reasoning-based LLM agent, outperforms forward reasoning in complex tasks by starting planning from the terminal state.


<details>
  <summary>Details</summary>
Motivation: Forward reasoning struggles with complex tasks due to the perception gap between initial state and goal.

Method: BAR uses backward reasoning, recursive goal decomposition, state consistency, and stage memory modules.

Result: BAR shows superior performance over existing methods in experiments.

Conclusion: Backward reasoning is effective for complex tasks, and BAR's modules enhance planning robustness and efficiency.

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [54] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/pdf/2505.14080)
*Franziska Sofia Hafner, Ana Valdivia, Luc Rocher*

Main category: cs.CL

TL;DR: Language models encode harmful gendered stereotypes, often conflating gender with biological sex, erasing diverse identities. Research advocates redefining 'gender bias' and tests 16 models, finding binary gender encoding tied to sex, with larger models reinforcing this. Calls for re-evaluating harm definitions.


<details>
  <summary>Details</summary>
Motivation: To address superficial mitigation of gendered harms in language models and highlight deeper issues like binary gender encoding and erasure of diverse identities.

Method: Operationalizes gender studies insights, empirically tests 16 language models of varying architectures, datasets, and sizes for gender encoding patterns.

Result: Models encode gender as binary tied to biological sex, erase non-binary terms, and larger models reinforce stronger gender-sex associations.

Conclusion: Urges redefining 'gender bias' in language models to address deeper harms and calls for broader mitigation strategies.

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [55] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/pdf/2505.14099)
*Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: PDRR is a four-stage KBQA framework (Predict, Decompose, Retrieve, Reason) that outperforms existing methods by handling complex questions through structured decomposition and reasoning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLM-only approaches (outdated knowledge, hallucinations) and chain-based KG-RAG methods (limited to simple questions).

Method: Four-stage framework: Predict question type, decompose into triples, retrieve KB info, and reason with LLM as an agent.

Result: PDRR consistently outperforms existing methods across LLM backbones and handles complex questions better.

Conclusion: PDRR effectively combines structured KBs with LLMs for superior KBQA performance on diverse question types.

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [56] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/pdf/2505.14101)
*Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva*

Main category: cs.CL

TL;DR: The paper introduces MultiHal, a KG-based multilingual benchmark to evaluate LLM factuality, addressing gaps in existing benchmarks by leveraging structured KG data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM factuality are English-centric and ignore structured factual resources like KGs, which can mitigate hallucinations.

Method: Developed MultiHal by mining and curating 25.9k high-quality KG-paths from open-domain KGs, integrating them for multilingual, multihop evaluation.

Result: Baseline evaluation shows a 0.12 to 0.36-point increase in semantic similarity scores for KG-RAG over vanilla QA across languages and models.

Conclusion: MultiHal demonstrates KG integration's potential for hallucination mitigation and aims to advance graph-based fact-checking research.

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [57] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/pdf/2505.14104)
*Wei Fan, Tianshi Zheng, Yiran Hu, Zheye Deng, Weiqi Wang, Baixuan Xu, Chunyang Li, Haoran Li, Weixing Shen, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper formalizes Legal Rule Induction (LRI) to derive doctrinal rules from precedents, introduces a benchmark dataset, and shows LLMs' limitations and improvements when trained on the dataset.


<details>
  <summary>Details</summary>
Motivation: Computational legal research lacks methods to induce legal rules from judicial decisions due to model limitations. LLMs offer potential but lack formal task definitions and benchmarks.

Method: The authors formalize LRI, create a benchmark with 5,121 case sets and 216 expert-annotated test sets, and evaluate LLMs' performance.

Result: State-of-the-art LLMs struggle with over-generalization and hallucination, but training on the dataset improves their ability to capture nuanced rule patterns.

Conclusion: The study advances LRI by providing formal definitions, benchmarks, and showing LLMs' potential when properly trained.

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [58] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/pdf/2505.14106)
*Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao*

Main category: cs.CL

TL;DR: PersonaConvBench is a benchmark for evaluating personalized reasoning and generation in multi-turn conversations with LLMs, integrating personalization and conversational structure across three tasks.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks integration of personalization and conversational structure, limiting realistic evaluation of LLMs in multi-user scenarios.

Method: PersonaConvBench includes sentence classification, impact regression, and user-centric text generation tasks across ten Reddit domains, benchmarking LLMs under unified prompting.

Result: Incorporating personalized history improves performance significantly, e.g., a 198% gain in sentiment classification over non-conversational baselines.

Conclusion: PersonaConvBench supports research on LLMs adapting to individual styles, tracking context, and generating engaging responses.

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [59] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/pdf/2505.14107)
*Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang*

Main category: cs.CL

TL;DR: DiagnosisArena is a benchmark to test AI diagnostic reasoning in healthcare, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: To enable safe AI deployment in healthcare by systematically evaluating diagnostic capabilities.

Method: Developed DiagnosisArena with 1,113 patient case-diagnosis pairs from top medical journals, screened by AI and experts.

Result: Top models (o3-mini, o1, DeepSeek-R1) scored 45.82%, 31.09%, and 17.79% accuracy, showing a generalization bottleneck.

Conclusion: DiagnosisArena aims to advance AI diagnostic reasoning for real-world clinical challenges, with tools available for further research.

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [60] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/pdf/2505.14112)
*Tianle Gu, Zongqi Wang, Kexin Huang, Yuanqi Yao, Xiangliang Zhang, Yujiu Yang, Xiuying Chen*

Main category: cs.CL

TL;DR: The paper introduces Invisible Entropy (IE), a lightweight and efficient watermarking method for LLMs that avoids reliance on the original model, addressing challenges like computational costs and model leakage.


<details>
  <summary>Details</summary>
Motivation: Existing logit-based watermarking struggles in low-entropy scenarios and relies on the original LLM, leading to inefficiency and risks. IE aims to enhance safety and efficiency.

Method: IE uses a lightweight feature extractor and entropy tagger to predict token entropy, along with a threshold navigator to adaptively set entropy thresholds for better watermarking.

Result: IE reduces parameter size by 99% while matching state-of-the-art performance on HumanEval and MBPP datasets.

Conclusion: IE provides a safe and efficient solution for low-entropy watermarking, improving text naturalness and detection robustness.

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [61] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/pdf/2505.14116)
*Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong*

Main category: cs.CL

TL;DR: The paper introduces Self-Reasoning Language Model (SRLM), which synthesizes longer Chain-of-Thought (CoT) data through self-training, improving reasoning tasks performance.


<details>
  <summary>Details</summary>
Motivation: Longer reasoning rationales, embodying meta-reasoning skills like reflection and decomposition, are hard to create. SRLM addresses this by enabling models to self-generate and improve CoT data.

Method: SRLM uses 1,000 demonstration examples to teach the model to unfold hidden reasoning chains, acting as a catalyst for self-training and iterative improvement.

Result: SRLM achieves +2.5 average improvement across five reasoning tasks (MMLU, GSM8K, ARC-C, HellaSwag, BBH) and +7.89 with 64 sampling times.

Conclusion: SRLM enhances reasoning performance and stability, revealing diverse and creative reasoning paths, outperforming baselines.

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [62] [Probing BERT for German Compound Semantics](https://arxiv.org/pdf/2505.14130)
*Filip Miletić, Aaron Schmid, Sabine Schulte im Walde*

Main category: cs.CL

TL;DR: The paper examines how well German BERT models encode noun compound semantics, finding trends similar to English but with weaker results due to German's higher compounding productivity and ambiguity.


<details>
  <summary>Details</summary>
Motivation: To understand how pretrained German BERT models capture noun compound semantics and compare their performance to English models.

Method: The study varies target tokens, layers, and model cases (cased/uncased) and evaluates them using a gold standard of 868 compounds to predict compositionality.

Result: Compositionality information is most recoverable in early layers, but results lag behind English, likely due to German's higher compounding productivity and ambiguity.

Conclusion: German BERT models struggle more with noun compound semantics than English, highlighting the task's inherent difficulty in German.

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [63] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/pdf/2505.14131)
*Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich*

Main category: cs.CL

TL;DR: The paper compares text vs. image table representations in TQA, introduces a controlled study, and proposes FRES for dynamic representation selection, improving performance by 10%.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks controlled setups to compare text and image table representations in TQA. This study aims to provide fine-grained distinctions.

Method: Conducts a controlled study on table representations and models, analyzing question complexity and table size. Introduces FRES for dynamic representation selection.

Result: Best table representation and model vary by setup. FRES improves performance by 10% on average.

Conclusion: Dynamic selection of table representations (FRES) outperforms indiscriminate use, highlighting the need for context-aware approaches in TQA.

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [64] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/pdf/2505.14149)
*Chengzhi Zhang, Xinyi Yan, Lei Zhao, Yingyi Zhang*

Main category: cs.CL

TL;DR: The paper proposes a method to improve Keyphrase Extraction (KPE) from academic articles by leveraging structural features and section texts, addressing limitations of abstract-based and full-text approaches.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of academic papers necessitates efficient literature retrieval. Current KPE methods are limited by abstract length or full-text noise.

Method: The approach uses structural features and section texts for KPE, evaluating seven features and integrating results via an algorithm. It also examines section structure classification's impact.

Result: Structural features enhance KPE performance, with the integration approach yielding the best results. Section structure classification quality also affects performance.

Conclusion: Utilizing section structure information improves KPE effectiveness, offering a practical solution for academic literature retrieval.

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [65] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.14157)
*Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul*

Main category: cs.CL

TL;DR: The paper explores prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) for language models, showing that pPE-trained models outperform inference-time prompt engineering (iPE) methods, with null-example pPE yielding the highest gains.


<details>
  <summary>Details</summary>
Motivation: Existing RFT research neglects the design of prior prompts, which prepend instructions to queries during training. This study investigates whether different pPE approaches can guide models to internalize distinct behaviors.

Method: Five iPE strategies (reasoning, planning, code-based reasoning, knowledge recall, null-example) are translated into pPE approaches and tested on Qwen2.5-7B. Performance is evaluated on benchmarks like AIME2024 and GPQA-Diamond.

Result: All pPE-trained models outperform iPE-prompted ones, with null-example pPE achieving the highest gains. Different pPE strategies instill distinct behavioral styles in models.

Conclusion: pPE is a powerful yet understudied aspect of RFT, offering significant performance improvements and behavioral control.

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [66] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/pdf/2505.14158)
*Sanjay Govindan, Maurice Pagnucco, Yang Song*

Main category: cs.CL

TL;DR: Activation engineering improves LLMs' temporal alignment for factual recall without training, achieving up to 44% improvement in prompting strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate outdated responses due to conflicting temporal knowledge. Ensuring time-appropriate answers is critical for accuracy.

Method: Activation engineering is used to align LLaMA 2 models to specific time points, testing injection layers and prompting strategies.

Result: Up to 44% improvement in relative prompting and 16% in explicit prompting, matching fine-tuning performance without its computational cost.

Conclusion: Activation engineering is a computationally efficient alternative to fine-tuning for temporal alignment in LLMs.

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [67] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/pdf/2505.14160)
*Zahraa Al Sahili, Ioannis Patras, Matthew Purver*

Main category: cs.CL

TL;DR: The paper audits multilingual CLIP models for social biases, revealing stronger gender biases than English-only models, especially in low-resource languages, and highlights the need for language-aware bias evaluation.


<details>
  <summary>Details</summary>
Motivation: To systematically assess social biases in multilingual vision-language models, which are under-explored despite their universal retrieval promises.

Method: Audits three multilingual CLIP models (M-CLIP, NLLB-CLIP, CAPIVARA-CLIP) across ten languages using balanced subsets of FairFace and PATA stereotype suite in a zero-shot setting.

Result: All models show stronger gender bias than English-only baselines, with CAPIVARA-CLIP exhibiting the largest biases in low-resource languages. Cross-lingual weight sharing transfers English stereotypes into gender-neutral languages.

Conclusion: Multilinguality does not mitigate bias; fine-grained, language-aware evaluation is crucial for future multilingual vision-language research.

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [68] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/pdf/2505.14165)
*Zhenkai Qin, Jiajing He, Qiao Fang*

Main category: cs.CL

TL;DR: PL-FGSA is a prompt learning-based framework for fine-grained sentiment analysis, integrating prompt design with TextCNN to improve generalization and scalability, outperforming traditional methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional FGSA approaches lack generalization and scalability due to task-specific architectures and extensive annotated data requirements.

Method: PL-FGSA reformulates FGSA as a multi-task prompt-augmented generation problem, combining aspect extraction, sentiment classification, and causal explanation.

Result: Achieves F1-scores of 0.922, 0.694, and 0.597 on SST-2, SemEval-2014 Task 4, and MAMS datasets, outperforming traditional methods.

Conclusion: PL-FGSA validates prompt-based generalization's effectiveness and offers practical value for real-world sentiment analysis.

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [69] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/pdf/2505.14172)
*Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu*

Main category: cs.CL

TL;DR: LLMs struggle with simple character-level tasks due to tokenization, analyzed as low mutual information. A lightweight fix improves performance while keeping subword benefits.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' failure in character-level tasks like counting letters, attributed to tokenization, by analyzing it as a mutual information problem.

Method: Use 19 synthetic tasks to isolate character-level reasoning, study emergence patterns, and propose a lightweight architectural fix.

Result: Character-level capabilities emerge late in training; percolation models explain this. The proposed fix improves reasoning without losing subword advantages.

Conclusion: Bridges gaps in tokenized LMs, offering a framework to understand and mitigate their blind spots.

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [70] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/pdf/2505.14173)
*Yunlong Liang, Fandong Meng, Jie Zhou*

Main category: cs.CL

TL;DR: THOR-MoE improves sparse Mixture-of-Experts (MoE) for NMT by introducing hierarchical task-guided and context-responsive routing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MoE solutions for NMT lack task-specific knowledge and context-aware routing, leading to sub-optimal performance.

Method: THOR-MoE predicts domain/language labels, extracts mixed representations for hierarchical expert allocation, and enhances token routing with context information.

Result: THOR-MoE achieves superior performance on multi-domain and multilingual benchmarks, with a 0.75 BLEU improvement and fewer activated parameters.

Conclusion: THOR-MoE is a plug-and-play module compatible with existing routing schemes, offering broad applicability and improved efficiency.

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [71] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/pdf/2505.14174)
*Yusuf Denizay Dönder, Derek Hommel, Andrea W Wen-Yi, David Mimno, Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [72] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/pdf/2505.14178)
*Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You*

Main category: cs.CL

TL;DR: Tokenization impacts reasoning in language models; poor token granularity disrupts symbolic computation, while aligned formats improve performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how tokenization schemes, like BPE, affect symbolic reasoning in language models, particularly with CoT prompting.

Method: Theoretical and empirical analysis of tokenization's role, introducing Token Awareness to measure logical alignment. Evaluated on arithmetic and symbolic tasks.

Result: Token structure significantly affects reasoning; atomically-aligned formats enable small models to outperform larger ones in symbolic tasks.

Conclusion: Symbolic reasoning in LLMs depends on token-level representations, not just architecture.

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [73] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/pdf/2505.14179)
*Tong Bao, Heng Zhang, Chengzhi Zhang*

Main category: cs.CL

TL;DR: A two-stage abstractive summarization framework is proposed to address challenges in capturing structured information in scientific papers, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully capture structured information in scientific papers and lack robustness across disciplines.

Method: A two-stage approach: (1) standardize chapter titles and train a classifier for structural function recognition, (2) use Longformer for context-aware summarization.

Result: Outperforms advanced baselines on domain-specific datasets, generating more comprehensive summaries.

Conclusion: The framework effectively leverages structural functions for improved abstractive summarization of scientific papers.

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [74] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/pdf/2505.14181)
*Yunlong Liang, Fandong Meng, Jiaan Wang, Jie Zhou*

Main category: cs.CL

TL;DR: The paper introduces SlangDIT, a task combining slang detection, explanation, and translation, and proposes SlangOWL, a deep thinking model, to improve slang translation accuracy.


<details>
  <summary>Details</summary>
Motivation: Slang translation is challenging due to context-dependent meanings, and existing approaches treat detection, explanation, and translation as isolated tasks. The paper aims to explore their interdependence for better translation.

Method: The authors create the SlangDIT dataset (25k English-Chinese pairs) and propose SlangOWL, a model that detects slang, analyzes meanings, provides explanations, and generates translations.

Result: SlangOWL outperforms vanilla and fine-tuned LLMs (e.g., Qwen2.5, LLama-3.1) in slang translation.

Conclusion: The deep thinking approach of SlangOWL enhances LLM performance in slang translation, demonstrating the value of integrating detection, explanation, and translation.

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [75] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/pdf/2505.14183)
*Guosheng Liang, Longguang Zhong, Ziyi Yang, Xiaojun Quan*

Main category: cs.CL

TL;DR: ThinkSwitcher enables LRMs to dynamically switch between short and long CoT reasoning, reducing computational costs by 20-30% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often overthink simple tasks, causing unnecessary computational overhead. They inherently support efficient short CoT reasoning, which can be leveraged.

Method: Proposes ThinkSwitcher, a framework with a lightweight switching module trained to dynamically choose reasoning modes based on task complexity.

Result: Reduces computational cost by 20-30% while maintaining high accuracy on complex tasks.

Conclusion: ThinkSwitcher is a scalable and efficient solution for unified LRM deployment.

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [76] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/pdf/2505.14195)
*Tuc Nguyen, Yifan Hu, Thai Le*

Main category: cs.CL

TL;DR: The paper introduces a unified framework to analyze the interplay between authorship obfuscation, mimicking, and verification in LLMs, addressing privacy risks and the role of demographic metadata.


<details>
  <summary>Details</summary>
Motivation: To explore the under-researched interactions between authorship privacy tasks (AO, AM, AV) in LLMs, given their growing impact on content curation and privacy concerns.

Method: The study presents a unified framework to quantify interactions among AO, AM, and AV, examining effects over time and the influence of demographic metadata.

Result: The framework reveals dynamic relationships between these tasks, their iterative effects, and how demographic factors modulate performance and privacy risks.

Conclusion: The work fills a critical gap in understanding authorship privacy in LLMs, offering insights into inter-task dynamics and privacy implications, with publicly available code for further research.

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [77] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/pdf/2505.14212)
*Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey*

Main category: cs.CL

TL;DR: The paper introduces an automated method to generate context-based QA pairs for enhancing LLMs in knowledge-intensive QA tasks, improving reasoning and factual accuracy.


<details>
  <summary>Details</summary>
Motivation: Current QA systems struggle with complex reasoning and real-time knowledge integration, and RAG techniques face challenges in handling logical connections between multiple sources.

Method: The approach involves an automated QA generator and model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.

Result: Mistral-7b-v0.3 outperforms Llama-3-8b, showing better scores in BERT F1, BLEU, and ROUGE for LLM-generated QA pairs compared to human-annotated ones.

Conclusion: The methodology improves logical coherence and factual accuracy, offering potential for adaptable AI systems.

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [78] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/pdf/2505.14226)
*Darpan Aswal, Siddharth D Jaiswal*

Main category: cs.CL

TL;DR: The paper introduces novel jailbreaking strategies for LLMs using code-mixing and phonetic perturbations, achieving high attack success rates in text and image generation.


<details>
  <summary>Details</summary>
Motivation: Existing red-teaming efforts focus on English and fixed templates, leaving models vulnerable to multilingual and multimodal jailbreaking.

Method: The study uses code-mixing and phonetic perturbations to bypass safety filters, testing effectiveness in text and image generation tasks.

Result: Achieved 99% Attack Success Rate for text and 78% for image generation, with high relevance rates. Phonetic perturbations impact tokenization.

Conclusion: Highlights the need for generalizable safety alignment in multilingual multimodal models, especially for real-world prompts with misspellings.

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [79] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/pdf/2505.14233)
*Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue*

Main category: cs.CL

TL;DR: ABFT fine-tunes LMs by targeting attention scores, reducing computational costs and improving performance, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between ICL and pre-training without high computational costs.

Method: Proposes ABFT, focusing on attention scores to guide correct label token focus and reduce wrong token attention.

Result: Outperforms previous methods in performance, robustness, and efficiency with minimal data cost.

Conclusion: Demonstrates control over LM modules for improved behavior, advancing mechanistic interpretability.

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [80] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/pdf/2505.14238)
*Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma*

Main category: cs.CL

TL;DR: ABBA introduces a new PEFT method that decouples updates from pre-trained weights using a Hadamard product of two low-rank matrices, achieving higher expressivity and better performance than existing methods like LoRA and HiRA.


<details>
  <summary>Details</summary>
Motivation: Adapting large language models efficiently to new domains is challenging. Current PEFT methods like LoRA and HiRA are limited in expressivity due to their reliance on pre-trained model structures.

Method: ABBA reparameterizes updates as a Hadamard product of two independently learnable low-rank matrices, fully decoupling updates from pre-trained weights.

Result: ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, outperforming existing PEFT methods significantly.

Conclusion: ABBA offers a more expressive and efficient PEFT approach, validated by theoretical analysis and empirical results.

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [81] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/pdf/2505.14242)
*Ziang Wang, Amir Aryani*

Main category: cs.CL

TL;DR: An NLP approach classifies childhood speech disorder literature using LDA and BERTopic, identifying 14 clinical clusters with strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: To automate and improve the classification of scientific literature on childhood speech disorders for efficient literature reviews.

Method: Retrieved 4,804 PubMed articles (post-2015), pre-processed abstracts, and applied LDA and BERTopic with custom stop words.

Result: Identified 14 clinically meaningful clusters; LDA scored 0.42 coherence and -7.5 perplexity; BERTopic had <20% outliers.

Conclusion: The approach effectively classifies literature, aiding automated reviews in speech-language pathology.

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [82] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/pdf/2505.12392)
*Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi*

Main category: cs.CL

TL;DR: SLOT is a test-time optimization method for language models that improves accuracy by adapting to individual prompts with minimal parameter updates.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex or underrepresented instructions, leading to poor performance. SLOT aims to address this by enabling efficient per-sample adaptation.

Method: SLOT updates a lightweight sample-specific parameter vector at test-time, added to the final hidden layer, and minimizes cross-entropy loss on the input prompt.

Result: SLOT outperforms baselines, e.g., Qwen2.5-7B gains 8.6% accuracy on GSM8K, and DeepSeek-R1-Distill-Llama-70B achieves SOTA 68.69% on GPQA.

Conclusion: SLOT effectively enhances LLM performance on complex tasks with minimal computational overhead.

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [83] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/pdf/2505.14244)
*Haijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu Qian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang, Gongbo Tang, Weihua Luo, Zhao Xu, Kaifu Zhang*

Main category: cs.CL

TL;DR: The paper proposes a three-level framework (TransBench) to evaluate machine translation (MT) in industrial contexts, addressing gaps in domain-specific and cultural adaptation metrics.


<details>
  <summary>Details</summary>
Motivation: General-purpose MT models fall short in industrial scenarios due to domain-specific terminology, cultural nuances, and inadequate evaluation frameworks.

Method: Introduces TransBench, a benchmark with 17,000 sentences for e-commerce, combining traditional metrics (BLEU, TER) with Marco-MOS, a domain-specific model.

Result: Provides a structured evaluation framework, the first e-commerce MT benchmark, novel metrics, and open-sourced tools.

Conclusion: Bridges the gap between academic benchmarks and real-world MT efficacy, enabling better assessment for industry needs.

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [84] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/pdf/2505.14256)
*Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong*

Main category: cs.CL

TL;DR: FuxiMT is a Chinese-centric multilingual machine translation model using a sparsified LLM, trained in two stages and incorporating MoEs and curriculum learning. It outperforms baselines, especially in low-resource settings, and shows strong zero-shot translation abilities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multilingual machine translation, particularly for low-resource languages and unseen language pairs, by leveraging a sparsified LLM and innovative training strategies.

Method: A two-stage training approach: pre-training on a large Chinese corpus, followed by multilingual fine-tuning on 65 languages. Incorporates Mixture-of-Experts (MoEs) and curriculum learning for robustness.

Result: FuxiMT outperforms state-of-the-art LLMs and translation models, especially in low-resource scenarios, and demonstrates strong zero-shot translation capabilities.

Conclusion: FuxiMT is a promising solution for multilingual translation, particularly in low-resource and zero-shot settings, bridging communication gaps where parallel data is scarce.

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [85] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/pdf/2505.14268)
*Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu*

Main category: cs.CL

TL;DR: Think-J improves LLM-as-a-Judge by teaching LLMs to think, using curated data and RL optimization, outperforming existing methods without extra human input.


<details>
  <summary>Details</summary>
Motivation: Current generative LLMs underperform as judges for response preferences, limiting their utility in evaluation and reward modeling.

Method: Develop initial judgment thinking with curated data, then optimize via offline (critic model) and online (rule-based rewards) RL methods.

Result: Think-J significantly boosts LLM-Judge evaluation, surpassing generative and classifier-based judges without additional human annotations.

Conclusion: Think-J enhances LLM-as-a-Judge by integrating judgment thinking, offering a scalable and efficient solution for preference modeling.

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [86] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/pdf/2505.14271)
*Minh Ngoc Ta, Dong Cao Van, Duc-Anh Hoang, Minh Le-Anh, Truong Nguyen, My Anh Tran Nguyen, Yuxia Wang, Preslav Nakov, Sang Dinh*

Main category: cs.CL

TL;DR: FAID is a fine-grained detection framework for classifying human-written, AI-generated, and human-AI collaborative texts, while identifying AI model families. It outperforms baselines and improves generalization.


<details>
  <summary>Details</summary>
Motivation: The rise of human-AI collaboration in generative tasks creates challenges in distinguishing text origins, necessitating a robust detection method.

Method: FAID uses multi-level contrastive learning and multi-task auxiliary classification to capture stylistic cues, modeling AI families as distinct entities. It adapts to unseen data without retraining.

Result: FAID outperforms baselines, especially in generalization to unseen domains and new AI models.

Conclusion: FAID enhances transparency and accountability in AI-assisted writing by providing interpretable and adaptable detection.

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [87] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/pdf/2505.14279)
*Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch*

Main category: cs.CL

TL;DR: YESciEval is an open-source framework for robust evaluation of LLMs in scientific Q&A, using rubric-based assessment and reinforcement learning to reduce bias.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of LLM evaluations in scientific question-answering and mitigate optimism bias in LLM evaluators.

Method: Combines fine-grained rubric-based assessment with reinforcement learning, using multidisciplinary science Q&A datasets (including adversarial variants) and evaluation scores from multiple LLMs.

Result: Enables scalable, cost-free evaluation independent of proprietary models or human feedback.

Conclusion: Advances reliable LLM-as-a-judge models, supporting AI alignment and fostering robust, transparent evaluation for scientific inquiry and AGI.

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [88] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/pdf/2505.14297)
*Jungseob Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim*

Main category: cs.CL

TL;DR: Cross-Lingual Optimization (CLO) outperforms supervised fine-tuning (SFT) in adapting English-centric LLMs to other languages, especially in low-resource settings, by using less data and maintaining English performance.


<details>
  <summary>Details</summary>
Motivation: Standard SFT overemphasizes English performance and struggles in data-constrained environments, necessitating a more efficient cross-lingual transfer method.

Method: CLO leverages English SFT data and a translation model for cross-lingual transfer, tested on five models and six languages with varying resource levels.

Result: CLO consistently beats SFT in target language proficiency and English retention, achieving better results with fewer samples (3,200 vs. 6,400 in low-resource cases).

Conclusion: CLO is more robust and efficient than SFT, particularly in low-resource languages, highlighting SFT's limitations and the benefits of CLO's training strategies.

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [89] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/pdf/2505.14305)
*Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han, Haobo Hua, Min Peng*

Main category: cs.CL

TL;DR: JOLT-SQL is a single-stage SFT framework for Text-to-SQL that jointly optimizes schema linking and SQL generation, improving robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address challenges in SFT approaches like complex pipelines and poor robustness to noisy schema information.

Method: Uses discriminative schema linking with local bidirectional attention and a confusion-aware noisy schema sampling strategy.

Result: Achieves state-of-the-art execution accuracy on Spider and BIRD benchmarks, with improved training and inference efficiency.

Conclusion: JOLT-SQL offers a streamlined, robust, and efficient solution for Text-to-SQL tasks.

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [90] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/pdf/2505.14309)
*Ehsan Doostmohammadi, Marco Kuhlmann*

Main category: cs.CL

TL;DR: The paper explores how query-context overlap affects retrieval-augmented language models, finding that increasing overlap beyond a threshold improves performance and efficiency. Synthetic context generation via paraphrasing reduces training time by 40% without performance loss.


<details>
  <summary>Details</summary>
Motivation: To understand the optimal degree of query-context overlap in retrieval-augmented language models, as its impact on performance and efficiency is unexplored.

Method: Systematically vary query-context overlap levels during training and inference, using synthetic context generated by paraphrasing queries.

Result: Increased overlap improves perplexity and learning speed beyond a threshold. Synthetic context reduces training time by 40% without performance loss.

Conclusion: Optimizing query-context overlap in retrieval mechanisms can significantly enhance language model efficiency and performance.

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [91] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/pdf/2505.14311)
*Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Kenneth Church, Vukosi Marivate*

Main category: cs.CL

TL;DR: The paper reviews Hausa NLP's current state, identifies challenges like limited datasets, and introduces HausaNLP, a resource catalog. It also discusses LLM integration issues and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: Hausa, a low-resource language with millions of speakers, lacks adequate NLP resources and representation despite its significance.

Method: The paper systematically examines Hausa NLP resources, research, and gaps, and introduces HausaNLP, a catalog for datasets and tools.

Result: HausaNLP is presented as a resource to improve accessibility. Challenges like tokenization and dialectal variation in LLMs are highlighted.

Conclusion: Strategic directions like dataset expansion and community collaboration are proposed to advance Hausa NLP, benefiting broader multilingual NLP research.

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [92] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/pdf/2505.14313)
*Leonardo Bertolazzi, Manuel Vargas Guzmán, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik*

Main category: cs.CL

TL;DR: MIND, a meta-learning fine-tuning approach, improves LLMs' generalization for deductive reasoning tasks, outperforming larger models like GPT-4o in small LMs.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' limited ability to generalize to out-of-distribution problems, particularly in deductive reasoning tasks.

Method: Proposes Meta-learning for In-context Deduction (MIND), a few-shot meta-learning fine-tuning approach for systematic understanding of deductive rules.

Result: MIND significantly enhances generalization in small LMs (1.5B to 7B parameters), especially in low-data settings, outperforming larger models like GPT-4o.

Conclusion: MIND enables small LMs to generalize better and apply inference rules systematically, surpassing state-of-the-art LLMs in deductive reasoning tasks.

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [93] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/pdf/2505.14347)
*Neelabh Sinha*

Main category: cs.CL

TL;DR: QA-prompting improves long-context summarization by using question-answering as an intermediate step, outperforming baselines by up to 29% in ROUGE scores without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing positional biases and suboptimal information extraction in long-context summarization by LMs.

Method: QA-prompting: a simple prompting method that uses question-answering before summary generation to enrich context.

Result: Outperforms baselines and state-of-the-art methods by up to 29% in ROUGE scores across multiple domains.

Conclusion: QA-prompting is an effective, scalable solution for summarization, emphasizing domain-specific question selection.

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [94] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/pdf/2505.14350)
*Jialong Han, Si Zhang, Ke Zhang*

Main category: cs.CL

TL;DR: OSoRA is a novel PEFT method for LLMs that reduces computational costs by integrating SVD and learnable scaling vectors, outperforming LoRA and VeRA.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is resource-intensive; PEFT methods like LoRA still require significant resources. OSoRA aims to address this.

Method: OSoRA extends LoRA by using SVD and learnable scaling vectors, freezing singular vector matrices and optimizing an output-dimension vector.

Result: OSoRA reduces computational costs while matching or outperforming LoRA and VeRA in benchmarks.

Conclusion: Jointly training singular values and output-dimension vectors is key to OSoRA's success, offering efficient fine-tuning for LLMs.

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [95] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/pdf/2505.14354)
*Xin Li, Mengbing Liu, Li Wei, Jiancheng An, Mérouane Debbah, Chau Yuen*

Main category: cs.CL

TL;DR: WirelessMathBench is a new benchmark for evaluating LLMs on wireless communications math tasks, revealing their limitations despite strong performance in basic recall.


<details>
  <summary>Details</summary>
Motivation: To assess and improve LLMs' capabilities in domain-specific mathematical reasoning, particularly in wireless communications.

Method: Created WirelessMathBench with 587 questions from 40 research papers, testing tasks from multiple-choice to complex equation completion.

Result: LLMs struggle with complex tasks, with the best model achieving 38.05% accuracy and only 7.83% in full equation completion.

Conclusion: WirelessMathBench highlights LLMs' limitations and aims to foster development of more robust, domain-aware models.

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [96] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/pdf/2505.14367)
*Jialong Han, Si Zhang, Ke Zhang*

Main category: cs.CL

TL;DR: DuDe improves LoRA by decomposing weight matrices for stable training and efficient knowledge transfer, achieving high accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA-based methods suffer from unstable training and inefficient knowledge transfer due to random initialization.

Method: DuDe decomposes weight matrices into magnitude and direction components using SVD for principled initialization.

Result: DuDe achieves up to 48.35% accuracy on MMLU and 62.53% (±1.59) on GSM8K, showing superior performance.

Conclusion: DuDe enhances optimization stability and preserves pre-trained representations, making it a significant PEFT contribution.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [97] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/pdf/2505.14376)
*Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma*

Main category: cs.CL

TL;DR: AutoRev is a graph-based automatic peer review system for academic papers, outperforming SOTA methods by 58.72%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for review generation overlook computational limits of long input tokens.

Method: Represents papers as graphs to extract critical passages for review generation.

Result: Outperforms SOTA baselines by 58.72% across metrics.

Conclusion: Graph-based extraction shows promise for NLP tasks; code will be public.

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [98] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/pdf/2505.14393)
*Nadir Durrani, Basel Mousi, Fahim Dalvi*

Main category: cs.CL

TL;DR: This survey systematizes research on Multilingual Knowledge Editing (MKE), presenting a taxonomy of methods, benchmarks, key findings, challenges, and open problems in the field.


<details>
  <summary>Details</summary>
Motivation: The study addresses the underexplored area of knowledge editing in multilingual contexts, aiming to ensure factual edits generalize reliably across languages.

Method: The paper surveys MKE methods, including parameter-based, memory-based, fine-tuning, and hypernetwork approaches, and evaluates benchmarks and transfer patterns.

Result: Key findings include method effectiveness, transfer patterns, and challenges like cross-lingual propagation, language anisotropy, and edit scalability.

Conclusion: The survey consolidates MKE research, identifies open problems, and sets the stage for future advancements in editable language-aware LLMs.

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [99] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/pdf/2505.14395)
*Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh*

Main category: cs.CL

TL;DR: MUG-Eval is a framework for evaluating multilingual text generation in LLMs using conversational tasks, independent of language-specific tools or LLMs-as-judges, and works across resource levels.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs' text generation in low-resource languages is challenging due to scarce direct assessment methods.

Method: Transforms benchmarks into conversational tasks, measuring task success rate as a proxy for generation quality.

Result: Strong correlation with benchmarks (r > 0.75) and enables standardized comparisons across 30 languages.

Conclusion: MUG-Eval offers a robust, resource-efficient solution for multilingual generation evaluation, scalable to many languages.

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [100] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/pdf/2505.14398)
*Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella*

Main category: cs.CL

TL;DR: The paper introduces Log-Augmented Generation (LAG), a framework that reuses prior computations and reasoning from logs to improve LLMs' performance on new tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to retain and apply reasoning from past tasks. LAG aims to enhance learning from prior experiences efficiently.

Method: LAG uses key-value (KV) caches to store reasoning contexts from past tasks, retrieving relevant logs to augment generation for new tasks.

Result: Experiments show LAG outperforms standard agentic systems and existing KV cache or reflection-based methods.

Conclusion: LAG effectively improves LLMs' ability to learn from past tasks while maintaining efficiency and scalability.

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [101] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/pdf/2505.14406)
*Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu*

Main category: cs.CL

TL;DR: PhantomCircuit is a framework to analyze and detect knowledge overshadowing in LLMs, revealing its origins and mechanisms during training.


<details>
  <summary>Details</summary>
Motivation: Current understanding of knowledge overshadowing in LLMs is limited to inference-time observations, lacking insights into its training-time origins.

Method: PhantomCircuit uses knowledge circuit analysis to dissect attention heads and trace competing knowledge pathways.

Result: The framework effectively identifies knowledge overshadowing instances, providing new insights into the phenomenon.

Conclusion: PhantomCircuit offers a methodological lens for mitigating knowledge overshadowing in LLMs.

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [102] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/pdf/2505.14418)
*Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Daizong Ding, Zhuosheng Zhang, Gongshen Liu*

Main category: cs.CL

TL;DR: AgentGhost is a stealthy backdoor attack framework for MLLM-powered GUI agents, leveraging interaction-level triggers and achieving high attack accuracy with minimal utility loss.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the underexplored supply chain threat of backdoor attacks in GUI agents, which rely on open-source models or APIs.

Method: AgentGhost combines goal and interaction-level triggers, formulates backdoor injection as a Min-Max optimization problem, and uses supervised contrastive learning and fine-tuning.

Result: AgentGhost achieves 99.7% attack accuracy with only 1% utility degradation and is countered by a defense method reducing accuracy to 22.1%.

Conclusion: The work highlights the vulnerability of GUI agents to backdoor attacks and proposes a defense, emphasizing the need for secure agent development.

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [103] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/pdf/2505.14423)
*Ona de Gibert, Joseph Attieh, Teemu Vahtola, Mikko Aulamo, Zihao Li, Raúl Vázquez, Tiancheng Hu, Jörg Tiedemann*

Main category: cs.CL

TL;DR: LLM-generated synthetic data improves low-resource machine translation, validated across diverse languages and applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of low-resource machine translation by leveraging synthetic data.

Method: Constructed a synthetic corpus from English Europarl, extended via pivoting, and evaluated its quality and utility.

Result: Synthetic data significantly improves MT performance, even when noisy.

Conclusion: LLM-generated synthetic data is a viable solution for enhancing low-resource MT, supported by the release of SynOPUS.

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [104] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/pdf/2505.14660)
*Ronald Seoh, Dan Goldwasser*

Main category: cs.CL

TL;DR: EmoGist is a training-free, in-context learning method for visual emotion classification using LVLMs, improving accuracy by leveraging context-dependent emotion label definitions.


<details>
  <summary>Details</summary>
Motivation: Emotions in images are context-dependent and nuanced, requiring adaptive definitions for accurate classification.

Method: EmoGist pre-generates emotion label explanations from image clusters and retrieves contextually relevant ones at test time for classification.

Result: Achieves up to 13 points improvement in micro F1 on Memotion and 8 points in macro F1 on FI dataset.

Conclusion: EmoGist effectively enhances emotion classification by leveraging contextual explanations.

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [105] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/pdf/2505.14425)
*Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen*

Main category: cs.CL

TL;DR: LLMs perform well on synthetic instructions but struggle with human-authored ones in spatial grounding tasks. Fine-tuning on synthetic data shows limitations in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs generalizing from synthetic to human-authored instructions in grounded environments, particularly in spatial tasks.

Method: Fine-tune LLMs using synthetic instructions and evaluate on a benchmark with both synthetic and human-written instructions.

Result: Models generalize well on simple tasks but degrade significantly on complex ones. Error analysis highlights gaps in instruction generalization.

Conclusion: The study underscores the limitations of synthetic data for training LLMs in complex, human-authored instruction scenarios.

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [106] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/pdf/2505.14436)
*Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao*

Main category: cs.CL

TL;DR: The paper explores Parametric Knowledge Transfer (PKT) in Large Language Models (LLMs), introducing Pre-Align PKT (PrePKT) and Post-Align PKT (PostPKT) paradigms. It identifies Neural Incompatibility as a key challenge and proposes LaTen for efficient alignment.


<details>
  <summary>Details</summary>
Motivation: To transcend traditional knowledge transfer methods and achieve genuine PKT across LLMs of different scales by aligning parametric spaces.

Method: Introduces PrePKT and PostPKT paradigms, proposing LaTen for PrePKT to align parametric spaces with minimal training.

Result: Both PostPKT and PrePKT face stability challenges due to Neural Incompatibility, identified as a fundamental barrier.

Conclusion: Neural Incompatibility presents challenges for PKT, offering insights into LLM architectures and future research directions.

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [107] [Creative Preference Optimization](https://arxiv.org/pdf/2505.14442)
*Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty*

Main category: cs.CL

TL;DR: CrPO enhances LLM creativity by optimizing for multiple dimensions (novelty, diversity, surprise, quality) using a modular alignment method, outperforming GPT-4o on evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack truly creative content generation, and existing methods fail to address creativity's multifaceted nature generally.

Method: Proposes Creative Preference Optimization (CrPO), integrating multiple creativity signals into preference optimization, and uses MuCE dataset for training.

Result: CrPO-augmented models outperform baselines like GPT-4o in novelty, diversity, and surprise while maintaining quality, validated by human and automated evaluations.

Conclusion: Optimizing for creativity within preference frameworks advances LLM creative capabilities without quality trade-offs.

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [108] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/pdf/2505.14455)
*Chihan Huang, Hao Tang*

Main category: cs.CL

TL;DR: CtrlDiff is a dynamic, controllable semi-autoregressive framework combining autoregressive and diffusion-based language models to address fixed-length and controllability issues.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of fixed-length outputs and weak controllability in diffusion-based language models.

Method: Proposes CtrlDiff, using reinforcement learning for adaptive block sizing and classifier-guided control for discrete diffusion.

Result: CtrlDiff narrows the performance gap to autoregressive models and enables effective conditional text generation.

Conclusion: CtrlDiff sets a new standard for hybrid diffusion models, improving flexibility and control in text generation.

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [109] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/pdf/2505.14464)
*Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: The paper explores distillation to enhance reasoning in language models, using verified outputs from three teacher models. AM-Thinking-v1-distilled data shows better diversity and performance.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning capabilities of open-source language models through distillation of high-quality, verified reasoning traces.

Method: Collect outputs from three teacher models on 1.89M queries, analyze data distributions, and train student models for evaluation on reasoning benchmarks.

Result: AM-Thinking-v1-distilled models outperform others (e.g., 84.3 on AIME2024) and adapt response length based on task difficulty.

Conclusion: High-quality distilled data enhances reasoning performance; datasets are released for future research.

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [110] [Void in Language Models](https://arxiv.org/pdf/2505.14467)
*Mani Shemiranifar*

Main category: cs.CL

TL;DR: The paper investigates unactivated layers (Voids) in transformer-based LMs during inference using L2 Adaptive Computation (LAC), showing skipping Voids improves performance.


<details>
  <summary>Details</summary>
Motivation: To understand if all layers in transformer LMs are activated during inference and whether skipping unactivated layers can enhance efficiency and performance.

Method: Uses LAC to trace activated layers during Prompt Processing (PP) and Response Generation (RG) phases, identifying Voids by monitoring L2-norm changes. Evaluated on instruction-tuned LMs (Llama, Mistral, Qwen) across benchmarks (MMLU, GPQA Diamond, BoolQ).

Result: Skipping Voids improved performance, e.g., Qwen2.5-7B-Instruct on MMLU (69.24 to 71.29) using 30% layers, and Mistral-7B-Instruct-v0.3 on GPQA Diamond (13.88 to 18.36) using 70% layers.

Conclusion: Not all layers contribute equally during inference; selectively skipping Voids can boost model performance on specific tasks.

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [111] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/pdf/2505.14469)
*Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee*

Main category: cs.CL

TL;DR: LLMs are more prone to unsafe outputs with code-mixed prompts than monolingual ones, analyzed via explainability methods and cultural dimensions.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in LLMs, especially with code-mixed inputs, by understanding their susceptibility to harmful outputs.

Method: Systematic investigation using explainability methods to analyze internal attribution shifts and cultural dimensions in unsafe outputs.

Result: Code-mixed prompts increase unsafe outputs; cultural context influences harm classification.

Conclusion: The study clarifies mechanisms behind LLMs' harmful behaviors with code-mixed inputs, highlighting cultural and internal factors.

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [112] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/pdf/2505.14471)
*Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen*

Main category: cs.CL

TL;DR: Citss is a novel framework for citation classification using self-supervised contrastive learning to address data scarcity and contextual noise, compatible with both encoder- and decoder-based models.


<details>
  <summary>Details</summary>
Motivation: Citation classification is crucial for scholarly analysis, but fine-tuning PLMs is challenging due to data scarcity, noise, and spurious correlations.

Method: Citss employs self-supervised contrastive learning with sentence-level cropping and keyphrase perturbation to improve robustness.

Result: Experiments on three datasets show Citss outperforms state-of-the-art methods for both encoder- and decoder-based models.

Conclusion: Citss effectively addresses challenges in citation classification and is adaptable to diverse PLMs.

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [113] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/pdf/2505.14481)
*He Zhu, Junyou Su, Minxi Chen, Wen Wang, Yijie Deng, Guanhua Chen, Wenjia Zhang*

Main category: cs.CL

TL;DR: PlanGPT-VL is a domain-specific Vision-Language Model for urban planning maps, outperforming general VLMs with innovative data synthesis, structured verification, and efficient training.


<details>
  <summary>Details</summary>
Motivation: General VLMs fail to analyze planning maps effectively, which are crucial for urban planning.

Method: PlanGPT-VL uses PlanAnno-V for data synthesis, Critical Point Thinking for verification, and a hybrid training approach.

Result: It outperforms general VLMs in planning map tasks, achieving high accuracy with a lightweight 7B model.

Conclusion: PlanGPT-VL provides a reliable tool for urban planning professionals, balancing performance and efficiency.

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [114] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/pdf/2505.14483)
*Agam Goyal, Xianyang Zhan, Yilun Chen, Koustuv Saha, Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: MoMoE is a modular, cross-community framework for scalable and explainable content moderation, outperforming fine-tuned baselines with concise explanations.


<details>
  <summary>Details</summary>
Motivation: Existing moderation methods are opaque and require per-community models, limiting adoption. MoMoE aims to provide scalable, transparent moderation.

Method: MoMoE uses four operators (Allocate, Predict, Aggregate, Explain) and is instantiated as community-specialized and norm-violation experts.

Result: MoMoE achieves Micro-F1 scores of 0.72 and 0.67 on unseen subreddits, matching or surpassing baselines with reliable explanations.

Conclusion: MoMoE enables scalable, transparent moderation without per-community fine-tuning, suggesting lightweight expert ensembles for trustworthy human-AI governance.

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [115] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/pdf/2505.14499)
*Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou*

Main category: cs.CL

TL;DR: The paper proposes LRSA, a framework combining small language models (SLMs) and large language models (LLMs) for Multimodal Aspect-Based Sentiment Analysis (MABSA), enhancing SLMs' performance with LLM-generated explanations and a dual cross-attention mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing SLMs for MABSA have limited capacity, leading to inaccuracies, while LLMs, though powerful, underperform fine-tuned small models in ABSA. The goal is to leverage both for better results.

Method: LRSA integrates LLM-generated rationales into SLMs and uses a dual cross-attention mechanism to improve feature interaction and fusion for aspect and sentiment identification.

Result: Experiments on three benchmarks show LRSA's superiority and generalizability, outperforming baseline models.

Conclusion: LRSA effectively combines SLMs and LLMs, enhancing MABSA performance and demonstrating broad applicability to pre-trained models.

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [116] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/pdf/2505.14505)
*Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji*

Main category: cs.CL

TL;DR: ModRWKV, a lightweight multimodal framework based on RWKV7, offers a viable RNN alternative to Transformers for MLLMs, balancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore modern RNN architectures for multimodal tasks, addressing the limitations of quadratic-complexity Transformers and text-only RNN applications.

Method: Proposes ModRWKV, a decoupled multimodal framework with dynamically adaptable encoders, leveraging RWKV7's pretrained weights for efficient training.

Result: Achieves optimal performance-efficiency balance, with pretrained initialization enhancing multimodal understanding.

Conclusion: Modern RNNs like ModRWKV are competitive alternatives to Transformers in MLLMs, with identified optimal configurations.

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [117] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/pdf/2505.14523)
*Michael Sullivan*

Main category: cs.CL

TL;DR: Language models over logical forms (LFLMs) are more data-efficient than textual models, demonstrated by the GFoLDS prototype, which outperforms textual LMs on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To show that LFLMs are more data-efficient and can leverage built-in linguistic knowledge for learning complex patterns.

Method: Introduces GFoLDS, a pretrained LM over graph representations of logical forms, as a proof-of-concept.

Result: GFoLDS outperforms textual LMs with similar data, showing LFLMs learn with less data and scale well.

Conclusion: LFLMs are viable for real-world applications due to their efficiency and scalability.

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [118] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/pdf/2505.14530)
*Zhipeng Yang, Junzhuo Li, Siyu Xia, Xuming Hu*

Main category: cs.CL

TL;DR: Large language models (LLMs) internally decompose and execute composite tasks sequentially across layers, confirmed by layer-masking and cross-task patching methods.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency of LLMs by demonstrating their internal stepwise task execution and planning capabilities.

Method: Used layer-masking, cross-task patching, and LogitLens to analyze subtask learning and execution patterns across layers.

Result: Confirmed distinct subtasks are learned at different depths and executed sequentially, replicated on the TRACE benchmark.

Conclusion: LLMs can internally plan and execute subtasks, enabling fine-grained activation steering for improved control.

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [119] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/pdf/2505.14536)
*Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram*

Main category: cs.CL

TL;DR: The paper proposes using sparse autoencoders (SAEs) to target and reduce toxic outputs in LLMs, balancing toxicity reduction with language fluency.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate toxic content, and existing detoxification methods are easily bypassed. This work aims to improve detoxification by identifying and steering toxicity-related activations.

Method: Leverage SAEs to identify toxicity-related directions in model activations and apply targeted steering with varying aggressiveness levels. Evaluated on GPT-2 Small and Gemma-2-2B.

Result: Stronger steering reduces toxicity by up to 20%, but fluency degrades in GPT-2 Small. General model abilities remain stable. Wider SAEs hinder safety interventions.

Conclusion: SAE-based interventions show promise for LLM detoxification but require careful balancing of toxicity reduction and fluency, with practical guidelines for deployment.

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [120] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/pdf/2505.14552)
*Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang*

Main category: cs.CL

TL;DR: KORGym is introduced as a dynamic evaluation platform for LLMs, offering diverse games and revealing performance insights across models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are domain-specific and fail to assess LLMs' general reasoning capabilities comprehensively.

Method: KORGym, inspired by KOR-Bench and Gymnasium, provides 50+ games in textual/visual formats and supports interactive, multi-turn assessments with reinforcement learning.

Result: Experiments on 19 LLMs and 8 VLMs show consistent reasoning patterns, superior closed-source model performance, and insights into modality, reasoning strategies, and reinforcement learning effects.

Conclusion: KORGym is a promising resource for advancing LLM reasoning research and evaluation in interactive environments.

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [121] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/pdf/2505.14553)
*Abhimanyu Talwar, Julien Laasri*

Main category: cs.CL

TL;DR: The paper explores using Hindi as a pivot language to translate Nepali to English, achieving improved results with supervised methods but slightly underperforming in semi-supervised approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large, diverse parallel corpora for certain language pairs by leveraging a pivot language (Hindi) for Nepali-English translation.

Method: Two approaches: Transfer Method (fully supervised) and Backtranslation (semi-supervised). Hindi is used as the pivot language.

Result: Achieved a SacreBLEU score of 14.2 with the Transfer Method, improving the baseline by 6.6 points. Semi-supervised results were slightly below the baseline (15.1).

Conclusion: Hindi is a viable pivot for Nepali-English translation. Future work could address the under-performance in semi-supervised methods.

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [122] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/pdf/2505.14577)
*Sohaila Eltanbouly, Salam Albatarni, Tamer Elsayed*

Main category: cs.CL

TL;DR: TRATES is a novel AES framework for trait-specific essay scoring using LLMs and rubrics, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of focus on individual trait assessment in AES, despite its long history.

Method: Leverages LLMs to generate trait-specific features from rubrics, combines them with generic features, and uses regression for scoring.

Result: TRATES achieves state-of-the-art performance across all traits on a widely-used dataset.

Conclusion: The framework effectively combines trait-specific and generic features for superior AES performance.

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [123] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/pdf/2505.14582)
*Shangziqi Zhao, Jiahao Yuan, Guisong Yang, Usman Naseem*

Main category: cs.CL

TL;DR: Prune-on-Logic framework improves reasoning in small language models by pruning low-utility steps in Long-CoT, focusing on verification steps for accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Long-CoT reasoning is verbose and hard to distill into small models, prompting exploration of pruning for better alignment.

Method: Proposes Prune-on-Logic, a framework that converts Long-CoT into logic graphs and prunes low-utility steps under self-verification.

Result: Pruning verification steps improves accuracy and reduces cost, while pruning reasoning or entire chains degrades performance.

Conclusion: Pruning is effective for aligning CoT reasoning with small model capacity, emphasizing semantically leaner steps.

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [124] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/pdf/2505.14585)
*Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper addresses safety and privacy risks in LLMs by integrating contextual compliance using reinforcement learning, improving both legal adherence and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current mitigation strategies for LLM safety and privacy risks lack contextual reasoning and compliance with standards like GDPR, EU AI Act, and HIPAA.

Method: Uses reinforcement learning with rule-based rewards to align LLMs with regulatory standards while enhancing reasoning.

Result: Achieves +17.64% accuracy in safety/privacy benchmarks and improves general reasoning (+2.05% MMLU, +8.98% LegalBench).

Conclusion: The method effectively balances compliance and reasoning, offering a scalable solution for safer LLMs.

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [125] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/pdf/2505.14590)
*Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper proposes MCIP, a refined version of MCP, to address safety risks in its decentralized architecture. It introduces a taxonomy for unsafe behaviors, benchmarks, and training data to improve LLMs' safety performance in MCP interactions.


<details>
  <summary>Details</summary>
Motivation: MCP's decentralized architecture poses safety risks, necessitating systematic analysis and improvement.

Method: Analyzes MCP's safety gaps using MAESTRO, proposes MCIP, develops a taxonomy for unsafe behaviors, and creates benchmarks and training data for LLMs.

Result: Experiments show LLMs' vulnerabilities in MCP interactions and significant safety improvements with the proposed approach.

Conclusion: The paper successfully enhances MCP safety by introducing MCIP and improving LLMs' risk identification capabilities.

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [126] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/pdf/2505.14597)
*Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Mingzheng Xu, Tianhao Cheng, Yixuan Wang, Zheng Chu, Shijie Xuyang, Zhiyuan Ma, YuanTao Fan, Wanxiang Che*

Main category: cs.CL

TL;DR: The paper introduces CTF-Code, a benchmark for evaluating Code LLMs' sensitivity to subtle input changes, and CTF-Instruct, a fine-tuning framework to improve sensitivity, showing performance boosts.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook code sensitivity, focusing only on difficulty and diversity. The study aims to address this gap.

Method: CTF-Code uses counterfactual perturbations to test sensitivity. CTF-Instruct extends existing data with incremental fine-tuning and a selection mechanism.

Result: LLMs show a >10% drop on CTF-Code. Fine-tuning with CTF-Instruct improves performance by >2% on CTF-Code and >10% on LiveCodeBench.

Conclusion: Enhancing sensitivity through CTF-Instruct improves LLM performance, validating its feasibility.

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [127] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/pdf/2505.14599)
*Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang*

Main category: cs.CL

TL;DR: The paper introduces TruthHypo, a benchmark for evaluating LLMs' ability to generate truthful biomedical hypotheses, and KnowHD, a hallucination detector. Results show LLMs struggle with truthfulness, but KnowHD helps filter accurate hypotheses.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying the truthfulness of hypotheses generated by LLMs in biomedicine, particularly due to hallucination issues.

Method: Introduces TruthHypo (a benchmark) and KnowHD (a knowledge-based hallucination detector) to assess and filter truthful hypotheses.

Result: LLMs struggle to generate truthful hypotheses, but KnowHD's groundedness scores effectively filter accurate ones. Human evaluations confirm its utility.

Conclusion: KnowHD aids in identifying truthful hypotheses, accelerating scientific discovery, and the tools are publicly available.

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [128] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/pdf/2505.14607)
*Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain*

Main category: cs.CL

TL;DR: sudoLLM introduces a framework for multi-role aligned LLMs, using user-based biases to control access to sensitive information, improving alignment and resistance to jailbreaking.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack user authorization-based access control, which is crucial for safety-critical systems. sudoLLM aims to address this gap.

Method: The framework injects user-based biases into queries and trains the LLM to use this signal to restrict sensitive information to authorized users.

Result: Empirical results show improved alignment, generalization, and resistance to jailbreaking attacks.

Conclusion: sudoLLM serves as an additional security layer, complementing existing guardrails for safer LLM deployment.

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [129] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/pdf/2505.14608)
*Rafael Rivera Soto, Barry Chen, Nicholas Andrews*

Main category: cs.CL

TL;DR: The paper challenges claims that machine-text detection is inherently unreliable by identifying a robust stylistic feature space and introducing AURA, a metric to evaluate detection performance.


<details>
  <summary>Details</summary>
Motivation: To address skepticism about the reliability of machine-text detectors and explore robust detection methods against optimized language models.

Method: Examines stylistic feature space robustness, tests detectors against optimized models, and introduces a paraphrasing attack and AURA metric.

Result: Stylistic detectors remain effective even against optimized models, but single-sample detection is vulnerable. AURA shows human-machine distribution overlap.

Conclusion: Machine-text detection remains challenging; avoid sole reliance on it, but stylistic features and AURA offer insights.

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [130] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/pdf/2505.14617)
*Sahar Abdelnabi, Ahmed Salem*

Main category: cs.CL

TL;DR: LLMs change behavior when aware of evaluation, impacting safety alignment. A white-box probing framework quantifies and controls this effect.


<details>
  <summary>Details</summary>
Motivation: To study how 'test awareness' in LLMs affects their behavior and safety alignment, given its analogy to the Hawthorne phenomenon.

Method: Introduces a white-box probing framework to identify and control awareness-related activations, applied to various LLMs across tasks.

Result: Test awareness significantly impacts safety alignment, with variations across models. The framework allows fine-grained control.

Conclusion: This work enhances trust in safety evaluations by quantifying and managing test awareness effects in LLMs.

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [131] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/pdf/2505.14631)
*Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei*

Main category: cs.CL

TL;DR: LHRMs adaptively decide when to perform extended thinking, improving efficiency and performance over LRMs and LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of excessive thinking in LRMs for simple queries by introducing adaptive hybrid reasoning.

Method: Two-stage training: Hybrid Fine-Tuning (HFT) and online reinforcement learning with Hybrid Group Policy Optimization (HGPO).

Result: LHRMs outperform LRMs and LLMs in reasoning and efficiency, adapting to query difficulty.

Conclusion: Advocates for hybrid thinking systems, balancing performance and efficiency.

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [132] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/pdf/2505.14633)
*Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger*

Main category: cs.CL

TL;DR: LitmusValues is a pipeline to detect AI risks by identifying value priorities in AI models, using dilemmas to predict risky behaviors.


<details>
  <summary>Details</summary>
Motivation: As AI models evolve, detecting risks like Alignment Faking becomes harder. Identifying AI values can serve as an early warning system for risky behaviors.

Method: Developed LitmusValues to evaluate AI priorities on value classes and AIRiskDilemmas to test value conflicts in safety scenarios.

Result: LitmusValues predicts risky behaviors, even unseen ones, by analyzing value prioritization in AI models.

Conclusion: Value prioritization in AI models can effectively predict and uncover potential risks, aiding in AI safety.

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [133] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/pdf/2505.14652)
*Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen*

Main category: cs.CL

TL;DR: General-Reasoner enhances LLM reasoning across diverse domains by using a novel training paradigm, a large dataset, and a generative answer verifier, outperforming baselines on 12 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning works focus on math/coding due to data abundance and easy verification, limiting broader applicability. General-Reasoner aims to address this gap.

Method: Proposes a training paradigm with (1) a large, diverse dataset via web crawling and (2) a generative model-based answer verifier with chain-of-thought and context-awareness.

Result: Outperforms baselines on 12 benchmarks (e.g., MMLU-Pro, GPQA, MATH AMC), showing robust, generalizable reasoning while excelling in math tasks.

Conclusion: General-Reasoner successfully extends LLM reasoning to diverse domains, demonstrating superior performance and generalization.

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [134] [Reward Reasoning Model](https://arxiv.org/pdf/2505.14674)
*Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei*

Main category: cs.CL

TL;DR: Reward Reasoning Models (RRMs) improve reward model performance by using chain-of-thought reasoning and adaptive test-time compute, achieving superior results without explicit training data.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of effectively utilizing test-time compute to enhance reward model performance in aligning large language models with human expectations.

Method: Introduce RRMs with a reinforcement learning framework for self-evolved reward reasoning, leveraging chain-of-thought reasoning for complex queries.

Result: RRMs outperform benchmarks across domains and adaptively improve reward accuracy with test-time compute.

Conclusion: RRMs offer a scalable solution for enhancing reward models, with pretrained models available for use.

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [135] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/pdf/2505.14679)
*Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang*

Main category: cs.CL

TL;DR: ULTRAEDIT is a scalable, efficient model editing method for lifelong learning in LLMs, achieving faster speeds and lower resource usage than prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of practical lifelong adaptation in LLMs, ensuring efficient updates without compromising existing capabilities.

Method: ULTRAEDIT uses lightweight linear algebra for parameter shifts and a lifelong normalization strategy for scalability.

Result: Achieves 7x faster editing speeds, uses <1/3 VRAM, and supports up to 1M edits with high accuracy.

Conclusion: ULTRAEDIT is a robust solution for scalable lifelong model editing, validated by extensive experiments.

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [136] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/pdf/2505.14684)
*Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper introduces CoT Thought Leap Bridge Task to detect and fill gaps in Chain-of-Thought reasoning, improving model performance on mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical CoT datasets suffer from Thought Leaps due to missing intermediate steps, hindering model learning and generalization.

Method: Proposed CoT-Bridge detects leaps and generates missing steps, using a specialized dataset (ScaleQM+) for training.

Result: Models fine-tuned on bridged datasets outperform original ones, with up to +5.87% improvement on NuminaMath.

Conclusion: Enhancing reasoning completeness via CoT-Bridge improves performance and generalization, serving as a plug-and-play module for optimization.

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [137] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/pdf/2505.14685)
*Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger*

Main category: cs.CL

TL;DR: The paper investigates how language models (LMs) track characters' beliefs using causal mediation and abstraction, revealing a 'lookback mechanism' for belief reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding the Theory of Mind (ToM) capabilities of LMs, specifically how they represent characters' beliefs that may differ from reality.

Method: Analyzes Llama-3-70B-Instruct using causal mediation and abstraction, constructing a dataset of simple stories with characters altering object states. Identifies a 'lookback mechanism' for belief tracking.

Result: Uncovered a lookback mechanism where the LM binds character-object-state triples via Ordering IDs (OIs) and retrieves relevant information for belief reasoning. Visibility IDs further refine belief updates.

Conclusion: Provides insights into LM belief tracking, advancing the reverse-engineering of ToM reasoning in LMs.

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


### [138] [Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy](https://arxiv.org/pdf/2312.10097)
*Isidor Konrad Maier, Matthias Wolff*

Main category: cs.CL

TL;DR: A numeral decomposer based on arithmetic criteria and Hurford's Packing Strategy is introduced, tested on 273 languages, and found to produce compact grammars close to expert-made ones.


<details>
  <summary>Details</summary>
Motivation: To develop a numeral decomposer independent of base-10 assumptions, leveraging Hurford's Packing Strategy for numeral structure analysis.

Method: The decomposer uses arithmetic criteria (multiplicators > sqrt(n), summands < n/2, factors < sqrt(n)) to reverse Hurford's Packing Strategy, applied in unsupervised grammar induction.

Result: Tested on 273 languages, the decomposer produced grammars with sensible mathematical attributes, often outperforming state-of-the-art tools in compactness.

Conclusion: The decomposer is effective, though some errors arise from linguistic peculiarities like context sensitivity.

Abstract: This paper presents a novel numeral decomposer based on arithmetic criteria.
The criteria are not dependent on a base-10 assumption but only on Hurford's
Packing Strategy. Hurford's Packing Strategy constitutes numerals by packing
factors and summands to multiplicators. We found out that a numeral of value n
has a multiplicator larger than sqrt(n), a summand smaller than n/2 and a
factor smaller than sqrt(n). Using these findings, the numeral decomposer
attempts to detect and unpack factors and summand in order to reverse Hurford's
Packing strategy. We tested its applicability for incremental unsupervised
grammar induction in 273 languages. This way, grammars were obtained with
sensible mathematical attributes that explain the structure of produced
numerals. The numeral-decomposer-induced grammars are often close to
expert-made and more compact than numeral grammars induced by a modern
state-of-the-art grammar induction tool. Furthermore, this paper contains a
report about the few cases of incorrect induced mathematical attributes, which
are often linked to linguistic peculiarities like context sensitivity.

</details>


### [139] [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/pdf/2403.10056)
*Yongquan He, Wenyuan Zhang, Xuancheng Huang, Peng Zhang*

Main category: cs.CL

TL;DR: A novel method (KPIG) for continual instruction tuning in LLMs addresses catastrophic forgetting by dynamically replaying data and refining training objectives, improving generalization and instruction-following abilities.


<details>
  <summary>Details</summary>
Motivation: To mitigate catastrophic forgetting in continual instruction tuning of LLMs, which degrades previously learned abilities, and to avoid surface-level pattern memorization.

Method: Proposes Key-part Information Gain (KPIG) to dynamically replay data and refine training objectives, focusing on task-aware information. Introduces P-score and V-score metrics for evaluation.

Result: Superior performance on both seen and held-out tasks, demonstrating improved generalization and instruction-following.

Conclusion: KPIG effectively alleviates catastrophic forgetting and enhances LLM performance in continual instruction tuning.

Abstract: Instruction tuning for large language models (LLMs) can drive them to produce
results consistent with human goals in specific downstream tasks. However, the
process of continual instruction tuning (CIT) for LLMs may bring about the
catastrophic forgetting (CF) problem, where previously learned abilities are
degraded. Recent methods try to alleviate the CF problem by modifying models or
replaying data, which may only remember the surface-level pattern of
instructions and get confused on held-out tasks. In this paper, we propose a
novel continual instruction tuning method based on Key-part Information Gain
(KPIG). Our method computes the information gain on masked parts to dynamically
replay data and refine the training objective, which enables LLMs to capture
task-aware information relevant to the correct response and alleviate
overfitting to general descriptions in instructions. In addition, we propose
two metrics, P-score and V-score, to measure the generalization and
instruction-following abilities of LLMs. Experiments demonstrate our method
achieves superior performance on both seen and held-out tasks.

</details>


### [140] [PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games](https://arxiv.org/pdf/2404.17662)
*Qinglin Zhu, Runcong Zhao, Bin Liang, Jinhua Du, Lin Gui, Yulan He*

Main category: cs.CL

TL;DR: WellPlay is a dataset for multi-agent reasoning in Murder Mystery Games (MMGs), and PLAYER* is a new LLM-based framework that outperforms existing methods in reasoning accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of reasoning in MMGs, such as undefined state spaces and strategic natural language interactions, by creating a benchmark dataset and a novel agent framework.

Method: Developed WellPlay dataset with 1,482 questions across 12 games and introduced PLAYER*, which uses sensor-based state representation and information-driven strategy for questioning and suspect pruning.

Result: PLAYER* outperforms existing methods in reasoning accuracy, efficiency, and agent-human interaction.

Conclusion: The work advances reasoning agents for complex social scenarios, providing a systematic benchmark and effective framework for MMGs.

Abstract: We introduce WellPlay, a reasoning dataset for multi-agent conversational
inference in Murder Mystery Games (MMGs). WellPlay comprises 1,482 inferential
questions across 12 games, spanning objectives, reasoning, and relationship
understanding, and establishes a systematic benchmark for evaluating agent
reasoning abilities in complex social settings. Building on this foundation, we
present PLAYER*, a novel framework for Large Language Model (LLM)-based agents
in MMGs. MMGs pose unique challenges, including undefined state spaces, absent
intermediate rewards, and the need for strategic reasoning through natural
language. PLAYER* addresses these challenges with a sensor-based state
representation and an information-driven strategy that optimises questioning
and suspect pruning. Experiments show that PLAYER* outperforms existing methods
in reasoning accuracy, efficiency, and agent-human interaction, advancing
reasoning agents for complex social scenarios.

</details>


### [141] [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://arxiv.org/pdf/2407.01082)
*Minh Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, Ravid Shwartz-Ziv*

Main category: cs.CL

TL;DR: Min-p sampling improves text generation quality and diversity by dynamically adjusting the sampling threshold based on model confidence, outperforming top-p sampling.


<details>
  <summary>Details</summary>
Motivation: Top-p sampling struggles to balance quality and diversity, especially at higher temperatures, leading to incoherent or repetitive outputs.

Method: Proposes min-p sampling, a dynamic truncation method using the top token's probability as a scaling factor to adjust the sampling threshold.

Result: Min-p sampling improves quality and diversity across benchmarks (GPQA, GSM8K, AlpacaEval) and model families (Mistral, Llama 3), with human evaluations favoring it.

Conclusion: Min-p sampling is effective and widely adopted, enhancing text generation quality in popular frameworks like Hugging Face Transformers and VLLM.

Abstract: Large Language Models (LLMs) generate text by sampling the next token from a
probability distribution over the vocabulary at each decoding step. Popular
sampling methods like top-p (nucleus sampling) often struggle to balance
quality and diversity, especially at higher temperatures which lead to
incoherent or repetitive outputs. We propose min-p sampling, a dynamic
truncation method that adjusts the sampling threshold based on the model's
confidence by using the top token's probability as a scaling factor. Our
experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative
Writing show that min-p sampling improves both the quality and diversity of
generated text across different model families (Mistral and Llama 3) and model
sizes (1B to 123B parameters), especially at higher temperatures. Human
evaluations further show a clear preference for min-p sampling, in both text
quality and creativity. Min-p sampling has been adopted by popular open-source
LLM frameworks, including Hugging Face Transformers, VLLM, and many others,
highlighting its considerable impact on improving text generation quality.

</details>


### [142] [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/pdf/2407.18416)
*Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, Vishvak Murahari*

Main category: cs.CL

TL;DR: PersonaGym and PersonaScore introduce a dynamic evaluation framework and metric for persona agents, revealing that model size doesn't guarantee better persona adherence.


<details>
  <summary>Details</summary>
Motivation: Evaluating how faithfully persona agents adhere to their personas in free-form settings is challenging.

Method: Introduces PersonaGym (dynamic evaluation framework) and PersonaScore (human-aligned metric) to assess 10 LLMs across 200 personas and 10,000 questions.

Result: GPT-4.1 and LLaMA-3-8b scored similarly, showing model size doesn't ensure better performance.

Conclusion: Algorithmic and architectural innovation is needed for faithful persona agents, as size alone doesn't improve adherence.

Abstract: Persona agents, which are LLM agents conditioned to act according to an
assigned persona, enable contextually rich and user aligned interactions across
domains like education and healthcare. However, evaluating how faithfully these
agents adhere to their personas remains a significant challenge, particularly
in free-form settings that demand consistency across diverse, persona-relevant
environments. We introduce PersonaGym, the first dynamic evaluation framework
for persona agents, and PersonaScore, a human-aligned automatic metric grounded
in decision theory that enables comprehensive large-scale evaluation. Our
evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals
significant advancement opportunities. For example, GPT-4.1 had the exact same
PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed
source model. Importantly, increased model size and complexity do not
necessarily enhance persona agent capabilities, underscoring the need for
algorithmic and architectural innovation toward faithful, performant persona
agents.

</details>


### [143] [Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework](https://arxiv.org/pdf/2409.00054)
*Yuting Hu, Dancheng Liu, Qingyun Wang, Charles Yu, Chenhui Xu, Qingxiao Zheng, Heng Ji, Jinjun Xiong*

Main category: cs.CL

TL;DR: A novel framework, LLM-Duo, uses large language models and a progressive ontology prompting algorithm to automate the identification of interventions from scientific literature, demonstrated with a case study in speech-language pathology.


<details>
  <summary>Details</summary>
Motivation: The high volume and complexity of scientific literature make manual intervention identification laborious and error-prone, necessitating an automated solution.

Method: The framework combines a progressive ontology prompting (POP) algorithm for structured prompts and a dual-agent system (LLM-Duo) with explorer and evaluator agents to refine annotations.

Result: The system identified 2,421 interventions from 64,177 articles, outperforming baselines and creating a public knowledge base.

Conclusion: The LLM-Duo framework effectively automates intervention discovery, offering a scalable and accurate solution for literature curation.

Abstract: Identifying effective interventions from the scientific literature is
challenging due to the high volume of publications, specialized terminology,
and inconsistent reporting formats, making manual curation laborious and prone
to oversight. To address this challenge, this paper proposes a novel framework
leveraging large language models (LLMs), which integrates a progressive
ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On
the one hand, the POP algorithm conducts a prioritized breadth-first search
(BFS) across a predefined ontology, generating structured prompt templates and
action sequences to guide the automatic annotation process. On the other hand,
the LLM-Duo system features two specialized LLM agents, an explorer and an
evaluator, working collaboratively and adversarially to continuously refine
annotation quality. We showcase the real-world applicability of our framework
through a case study focused on speech-language intervention discovery.
Experimental results show that our approach surpasses advanced baselines,
achieving more accurate and comprehensive annotations through a fully automated
process. Our approach successfully identified 2,421 interventions from a corpus
of 64,177 research articles in the speech-language pathology domain,
culminating in the creation of a publicly accessible intervention knowledge
base with great potential to benefit the speech-language pathology community.

</details>


### [144] [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/pdf/2409.11074)
*Adrian Cosma, Ana-Maria Bucur, Emilian Radoi*

Main category: cs.CL

TL;DR: RoMath introduces a Romanian mathematical reasoning benchmark to address the lack of non-English resources, promoting multilingual AI development.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on English, neglecting other languages like Romanian, a low-resource language with unique features.

Method: RoMath includes three subsets (Baccalaureate, Competitions, Synthetic) covering various mathematical domains and difficulty levels.

Result: Benchmarked open-weight language models show the need for dedicated resources beyond automatic translation.

Conclusion: RoMath highlights the importance of creating resources for underrepresented languages to improve multilingual AI.

Abstract: Mathematics has long been conveyed through natural language, primarily for
human understanding. With the rise of mechanized mathematics and proof
assistants, there is a growing need to understand informal mathematical text,
yet most existing benchmarks focus solely on English, overlooking other
languages. This paper introduces RoMath, a Romanian mathematical reasoning
benchmark suite comprising three subsets: Baccalaureate, Competitions and
Synthetic, which cover a range of mathematical domains and difficulty levels,
aiming to improve non-English language models and promote multilingual AI
development. By focusing on Romanian, a low-resource language with unique
linguistic features, RoMath addresses the limitations of Anglo-centric models
and emphasizes the need for dedicated resources beyond simple automatic
translation. We benchmark several open-weight language models, highlighting the
importance of creating resources for underrepresented languages. Code and
datasets are be made available.

</details>


### [145] [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/pdf/2409.11726)
*Wenyuan Zhang, Shuaiyi Nie, Jiawei Sheng, Zefeng Zhang, Xinghua Zhang, Yongquan He, Tingwen Liu*

Main category: cs.CL

TL;DR: RoleKE-Bench evaluates LLMs' ability to detect known (KKE) and unknown (UKE) knowledge errors in role-playing, revealing struggles even in latest models. The proposed S²RD method improves detection but highlights ongoing challenges.


<details>
  <summary>Details</summary>
Motivation: Existing works overlook LLMs' ability to detect KKE and UKE in role-playing, leading to low-quality corpus construction. This gap motivates the need for evaluation and improvement.

Method: Proposes RoleKE-Bench for evaluating LLMs' error detection in KKE and UKE. Introduces S²RD, an agent-based reasoning method combining self-recollection and self-doubt.

Result: Latest LLMs struggle with KKE and UKE detection, especially for familiar knowledge. S²RD improves detection but doesn't fully resolve the issue.

Conclusion: Error detection in LLM role-playing remains challenging. S²RD offers improvement, but further research is needed.

Abstract: Large language model (LLM) role-playing has gained widespread attention.
Authentic character knowledge is crucial for constructing realistic LLM
role-playing agents. However, existing works usually overlook the exploration
of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown
knowledge errors (UKE) while playing roles, which would lead to low-quality
automatic construction of character trainable corpus. In this paper, we propose
RoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The
results indicate that even the latest LLMs struggle to detect these two types
of errors effectively, especially when it comes to familiar knowledge. We
experimented with various reasoning strategies and propose an agent-based
reasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore
further the potential for improving error detection capabilities. Experiments
show that our method effectively improves the LLMs' ability to detect error
character knowledge, but it remains an issue that requires ongoing attention.

</details>


### [146] [Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/pdf/2410.03663)
*Zhuochun Li, Yuelyu Ji, Rui Meng, Daqing He*

Main category: cs.CL

TL;DR: The paper introduces FAIR, a novel knowledge distillation method that improves smaller LLMs by leveraging teacher LLMs to identify and explain student mistakes, and uses a peer-review process to ensure high-quality instructional data.


<details>
  <summary>Details</summary>
Motivation: Current knowledge distillation methods for smaller LLMs rely on single-teacher rationales, unlike human learning which benefits from understanding both correct answers and mistakes.

Method: FAIR involves teacher LLMs identifying student mistakes and explaining them, and a peer-review process to filter high-quality rationales.

Result: Experiments on reasoning tasks show FAIR's effectiveness in improving smaller LLMs.

Conclusion: FAIR enhances knowledge distillation by mimicking human learning processes and ensuring high-quality instructional data.

Abstract: While reasoning capabilities typically emerge in large language models (LLMs)
with tens of billions of parameters, recent research focuses on improving
smaller open-source models through knowledge distillation (KD) from commercial
LLMs. However, many of these studies rely solely on responses from a single LLM
as the gold rationale, unlike the natural human learning process, which
involves understanding both the correct answers and the reasons behind
mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via
Peer-Review (FAIR) approach: 1) instead of merely obtaining rationales from
teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data; 2) we design a
simulated peer-review process between teacher LLMs, and selects only the
generated rationales above the acceptance threshold, which reduces the chance
of teachers guessing correctly with flawed rationale, improving instructional
data quality. Comprehensive experiments and analysis on mathematical,
commonsense, and logical reasoning tasks demonstrate the effectiveness of our
method. Our code is available at
https://github.com/zhuochunli/Learn-from-Committee.

</details>


### [147] [SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition](https://arxiv.org/pdf/2410.10624)
*Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim*

Main category: cs.CL

TL;DR: SensorLLM is a two-stage framework enabling LLMs to perform HAR from sensor data by aligning sensor inputs with human-intuitive descriptions and fine-tuning for classification, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with time-series sensor data due to semantic, numerical, and sequence challenges. SensorLLM aims to bridge this gap by aligning sensor inputs with intuitive descriptions.

Method: 1. Sensor-Language Alignment: Uses SensorQA dataset to align sensor data with trend descriptions. 2. Task-Aware Tuning: Adapts the model for HAR classification.

Result: SensorLLM matches or exceeds state-of-the-art HAR performance, generalizing across diverse scenarios.

Conclusion: SensorLLM effectively combines sensor learning, reasoning, and classification, advancing foundation models for time-series analysis.

Abstract: We introduce SensorLLM, a two-stage framework that enables Large Language
Models (LLMs) to perform human activity recognition (HAR) from wearable sensor
data. While LLMs excel at reasoning and generalization, they struggle with
time-series inputs due to limited semantic context, numerical complexity, and
sequence variability. To address these challenges, we construct SensorQA, a
question-answering dataset of human-intuitive sensor-text pairs spanning
diverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where
the model aligns sensor inputs with trend descriptions. Special tokens are
introduced to mark channel boundaries. This alignment enables LLMs to interpret
numerical patterns, channel-specific signals, and variable-length
inputs--without requiring human annotation. In the subsequent Task-Aware Tuning
stage, we adapt the model for multivariate HAR classification, achieving
performance that matches or exceeds state-of-the-art methods. Our results show
that, guided by human-intuitive alignment, SensorLLM becomes an effective
sensor learner, reasoner, and classifier--generalizing across varied HAR
settings and paving the way for foundation model research in time-series
analysis.

</details>


### [148] [RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals](https://arxiv.org/pdf/2410.11348)
*David Reber, Sean Richardson, Todd Nief, Cristina Garbacea, Victor Veitch*

Main category: cs.CL

TL;DR: RATE is a method to measure how reward models respond to high-level attributes like sentiment or helpfulness by using LLM-generated imperfect counterfactuals, adjusting for bias by rewriting twice.


<details>
  <summary>Details</summary>
Motivation: Reward models are opaque, making it unclear what they reward. RATE aims to measure their sensitivity to specific attributes causally.

Method: RATE uses LLMs to rewrite responses, creating imperfect counterfactuals, and adjusts for bias by rewriting twice to estimate causal effects.

Result: RATE is validated as an effective estimator for measuring reward model sensitivity to attributes.

Conclusion: RATE provides a reliable way to causally analyze reward model behavior, addressing the black-box nature of such models.

Abstract: Reward models are widely used as proxies for human preferences when aligning
or evaluating LLMs. However, reward models are black boxes, and it is often
unclear what, exactly, they are actually rewarding. In this paper we develop
Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for
measuring the sensitivity of a reward model to high-level attributes of
responses, such as sentiment, helpfulness, or complexity. Importantly, RATE
measures the causal effect of an attribute on the reward. RATE uses LLMs to
rewrite responses to produce imperfect counterfactuals examples that can be
used to measure causal effects. A key challenge is that these rewrites are
imperfect in a manner that can induce substantial bias in the estimated
sensitivity of the reward model to the attribute. The core idea of RATE is to
adjust for this imperfect-rewrite effect by rewriting twice. We establish the
validity of the RATE procedure and show empirically that it is an effective
estimator.

</details>


### [149] [Interpreting token compositionality in LLMs: A robustness analysis](https://arxiv.org/pdf/2410.12924)
*Nura Aljaafari, Danilo S. Carvalho, André Freitas*

Main category: cs.CL

TL;DR: CAP analyzes LLMs' processing of compositional structures, revealing fragmented information processing and limitations in transformers' handling of compositional semantics.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' reliability and interpretability by understanding their internal mechanisms, particularly in processing compositional linguistic structures.

Method: Constituent-Aware Pooling (CAP) intervenes in model activations using constituent-based pooling across model levels, grounded in compositionality, mechanistic interpretability, and information theory.

Result: Experiments show transformers struggle with compositional abstractions, exhibiting fragmented processing that worsens with model size, due to training objectives and architecture.

Conclusion: Current transformer architectures have fundamental limitations in compositional semantics, highlighting the need for new LLM design approaches.

Abstract: Understanding the internal mechanisms of large language models (LLMs) is
integral to enhancing their reliability, interpretability, and inference
processes. We present Constituent-Aware Pooling (CAP), a methodology designed
to analyse how LLMs process compositional linguistic structures. Grounded in
principles of compositionality, mechanistic interpretability, and information
theory, CAP systematically intervenes in model activations through
constituent-based pooling at various model levels. Our experiments on inverse
definition modelling, hypernym and synonym prediction reveal critical insights
into transformers' limitations in handling compositional abstractions. No
specific layer integrates tokens into unified semantic representations based on
their constituent parts. We observe fragmented information processing, which
intensifies with model size, suggesting that larger models struggle more with
these interventions and exhibit greater information dispersion. This
fragmentation likely stems from transformers' training objectives and
architectural design, preventing systematic and cohesive representations. Our
findings highlight fundamental limitations in current transformer architectures
regarding compositional semantics processing and model interpretability,
underscoring the critical need for novel approaches in LLM design to address
these challenges.

</details>


### [150] [The Mystery of the Pathological Path-star Task for Language Models](https://arxiv.org/pdf/2410.13779)
*Arvid Frydenlund*

Main category: cs.CL

TL;DR: The paper explores the path-star task, highlighting language models' struggles with it due to teacher-forcing and next-token prediction limitations. The authors propose a regularization method and demonstrate solvability in alternative settings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of language models in solving the path-star task, which is simple for humans but challenging for models.

Method: Introduces a regularization method using structured samples of the same graph with differing target nodes and tests it across various model types.

Result: Improved performance across models, with proofs showing theoretical solvability and encoder-only models solving the task consistently.

Conclusion: The task's difficulty stems from representation and training paradigms, but it is learnable with the right methods.

Abstract: The recently introduced path-star task is a minimal task designed to
exemplify limitations to the abilities of language models (Bachmann and
Nagarajan, 2024). It involves a path-star graph where multiple arms radiate
from a single starting node and each node is unique. Given the start node and a
specified target node that ends an arm, the task is to generate the arm
containing that target node. This is straightforward for a human but
surprisingly difficult for language models, which did not outperform the random
baseline. The authors hypothesized this is due to a deficiency in
teacher-forcing and the next-token prediction paradigm.
  We demonstrate the task is learnable using teacher-forcing in alternative
settings and that the issue is partially due to representation. We introduce a
regularization method using structured samples of the same graph but with
differing target nodes, improving results across a variety of model types. We
provide RASP proofs showing the task is theoretically solvable. Finally, we
find settings where an encoder-only model can consistently solve the task.

</details>


### [151] [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/pdf/2410.14425)
*Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Yanhao Jia, Meihuizi Jia, Yichao Feng, Luu Anh Tuan*

Main category: cs.CL

TL;DR: W2SDefense is a novel weak-to-strong unlearning algorithm that defends against backdoor attacks in PEFT-finetuned LLMs by using feature alignment knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: PEFT is vulnerable to backdoor attacks, and poisoned LLMs retain backdoor activation capabilities. A defense mechanism is needed to unlearn backdoors without harming model performance.

Method: Train a small clean teacher model, then use it to guide a poisoned large student model via PEFT-based unlearning.

Result: W2SDefense effectively unlearns backdoor features and prevents activation, maintaining model performance.

Conclusion: W2SDefense is a robust solution for defending against backdoor attacks in PEFT-finetuned LLMs.

Abstract: Parameter-efficient fine-tuning (PEFT) can bridge the gap between large
language models (LLMs) and downstream tasks. However, PEFT has been proven
vulnerable to malicious attacks. Research indicates that poisoned LLMs, even
after PEFT, retain the capability to activate internalized backdoors when input
samples contain predefined triggers. In this paper, we introduce a novel
weak-to-strong unlearning algorithm to defend against backdoor attacks based on
feature alignment knowledge distillation, named W2SDefense. Specifically, we
first train a small-scale language model through full-parameter fine-tuning to
serve as the clean teacher model. Then, this teacher model guides the
large-scale poisoned student model in unlearning the backdoor, leveraging PEFT.
Theoretical analysis suggests that W2SDefense has the potential to enhance the
student model's ability to unlearn backdoor features, preventing the activation
of the backdoor. We conduct comprehensive experiments on three state-of-the-art
large language models and several different backdoor attack algorithms. Our
empirical results demonstrate the outstanding performance of W2SDefense in
defending against backdoor attacks without compromising model performance.

</details>


### [152] [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/pdf/2410.15522)
*Srishti Gureja, Lester James V. Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, Marzieh Fadaee*

Main category: cs.CL

TL;DR: The paper evaluates reward models (RMs) in multilingual settings, revealing performance gaps between English and non-English languages and the impact of translation quality and language resources.


<details>
  <summary>Details</summary>
Motivation: To address the understudied capabilities of RMs in multilingual contexts and provide a systematic evaluation framework.

Method: Constructed M-RewardBench, a multilingual RM evaluation benchmark with 2.87k preference instances across 23 languages, and evaluated various RMs.

Result: Identified significant performance gaps between English and non-English languages, with RM preferences varying across languages. Improved translation quality and high-resource languages enhance RM performance.

Conclusion: The study highlights the need for better multilingual RM evaluation and releases M-RewardBench and codebase to aid future research.

Abstract: Reward models (RMs) have driven the state-of-the-art performance of LLMs
today by enabling the integration of human feedback into the language modeling
process. However, RMs are primarily trained and evaluated in English, and their
capabilities in multilingual settings remain largely understudied. In this
work, we conduct a systematic evaluation of several reward models in
multilingual settings. We first construct the first-of-its-kind multilingual RM
evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances
for 23 typologically diverse languages, that tests the chat, safety, reasoning,
and translation capabilities of RMs. We then rigorously evaluate a wide range
of reward models on M-RewardBench, offering fresh insights into their
performance across diverse languages. We identify a significant gap in RMs'
performances between English and non-English languages and show that RM
preferences can change substantially from one language to another. We also
present several findings on how different multilingual aspects impact RM
performance. Specifically, we show that the performance of RMs is improved with
improved translation quality. Similarly, we demonstrate that the models exhibit
better performance for high-resource languages. We release M-RewardBench
dataset and the codebase in this study to facilitate a better understanding of
RM evaluation in multilingual settings.

</details>


### [153] [Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics](https://arxiv.org/pdf/2410.21272)
*Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov*

Main category: cs.CL

TL;DR: LLMs solve arithmetic tasks using a 'bag of heuristics' rather than robust algorithms or memorization.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLMs solve reasoning tasks via generalizable algorithms or memorization, focusing on arithmetic as a representative task.

Method: Causal analysis to identify a circuit of neurons implementing simple heuristics, categorized by input patterns and outputs.

Result: A sparse set of heuristic neurons, combined unordered, explains most of the model's arithmetic accuracy, appearing early in training.

Conclusion: LLMs rely on heuristic-based mechanisms for arithmetic, not robust algorithms or memorization.

Abstract: Do large language models (LLMs) solve reasoning tasks by learning robust
generalizable algorithms, or do they memorize training data? To investigate
this question, we use arithmetic reasoning as a representative task. Using
causal analysis, we identify a subset of the model (a circuit) that explains
most of the model's behavior for basic arithmetic logic and examine its
functionality. By zooming in on the level of individual circuit neurons, we
discover a sparse set of important neurons that implement simple heuristics.
Each heuristic identifies a numerical input pattern and outputs corresponding
answers. We hypothesize that the combination of these heuristic neurons is the
mechanism used to produce correct arithmetic answers. To test this, we
categorize each neuron into several heuristic types-such as neurons that
activate when an operand falls within a certain range-and find that the
unordered combination of these heuristic types is the mechanism that explains
most of the model's accuracy on arithmetic prompts. Finally, we demonstrate
that this mechanism appears as the main source of arithmetic accuracy early in
training. Overall, our experimental results across several LLMs show that LLMs
perform arithmetic using neither robust algorithms nor memorization; rather,
they rely on a "bag of heuristics".

</details>


### [154] [Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models](https://arxiv.org/pdf/2411.02448)
*Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva K. Pentyala, Katherine Tan, Xiang-Bo Mao, Roshanak Omrani, Sougata Chaudhuri, Regunathan Radhakrishnan, Sitaram Asur, Claire Na Cheng, Bin Yu*

Main category: cs.CL

TL;DR: The paper introduces three fine-tuned LLM autoevaluators (REC-8B, REC-12B, REC-70B) for assessing generated text quality across faithfulness, instruction following, coherence, and completeness, with detailed explanations and citations. REC-70B outperforms state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Ensuring the quality of LLM-generated text is challenging due to issues like factual inaccuracies and hallucination. The paper aims to address this by developing reliable autoevaluators.

Method: Three fine-tuned LLM autoevaluators (REC-8B, REC-12B, REC-70B) are introduced, designed to evaluate text across multiple dimensions and provide explanations and citations.

Result: REC-70B outperforms state-of-the-art LLMs in content evaluation, offering better explanations and citations with minimal bias.

Conclusion: The REC models provide a robust solution for evaluating LLM-generated text, enhancing trust through detailed explanations and verifiable citations.

Abstract: LLMs have demonstrated impressive proficiency in generating coherent and
high-quality text, making them valuable across a range of text-generation
tasks. However, rigorous evaluation of this generated content is crucial, as
ensuring its quality remains a significant challenge due to persistent issues
such as factual inaccuracies and hallucination. This paper introduces three
fine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B,
specifically designed to evaluate generated text across several dimensions:
faithfulness, instruction following, coherence, and completeness. These models
not only provide ratings for these metrics but also offer detailed explanation
and verifiable citation, thereby enhancing trust in the content. Moreover, the
models support various citation modes, accommodating different requirements for
latency and granularity. Extensive evaluations on diverse benchmarks
demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms
state-of-the-art LLMs, excelling in content evaluation by delivering better
quality explanation and citation with minimal bias. Our REC dataset and models
are available at https://github.com/adelaidehsu/REC.

</details>


### [155] [Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training](https://arxiv.org/pdf/2411.14318)
*Zheheng Luo, Xin Zhang, Xiao Liu, Haoling Li, Yeyun Gong, Chen Qi, Peng Cheng*

Main category: cs.CL

TL;DR: Velocitune is a framework for domain-adaptive continual pre-training that dynamically adjusts data proportions based on learning velocity, improving performance in reasoning and command-generation tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of methods for domain-adaptive continual pre-training by dynamically managing data proportions to optimize learning.

Method: Velocitune assesses learning velocity and adjusts data proportions, favoring slower-learning domains, guided by a scaling law for cost efficiency.

Result: Achieves performance gains in math and code reasoning tasks, as well as command-line generation benchmarks.

Conclusion: Velocitune's effectiveness is driven by target loss prediction and data ordering, offering a scalable solution for diverse corpus training.

Abstract: It is well-known that a diverse corpus is critical for training large
language models, which are typically constructed from a mixture of various
domains. In general, previous efforts resort to sampling training data from
different domains with static proportions, as well as adjusting data
proportions during training. However, few methods have addressed the
complexities of domain-adaptive continual pre-training. To fill this gap, we
propose Velocitune, a novel framework dynamically assesses learning velocity
and adjusts data proportions accordingly, favoring slower-learning domains
while shunning faster-learning ones, which is guided by a scaling law to
indicate the desired learning goal for each domain with less associated cost.
To evaluate the effectiveness of Velocitune, we conduct experiments in a
reasoning-focused dataset with CodeLlama, as well as in a corpus specialised
for system command generation with Llama3 and Mistral. Velocitune achieves
performance gains in both math and code reasoning tasks and command-line
generation benchmarks. Further analysis reveals that key factors driving
Velocitune's effectiveness include target loss prediction and data ordering.

</details>


### [156] [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/pdf/2411.17388)
*Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, Wentao Zhang*

Main category: cs.CL

TL;DR: GraphJudge is a framework for constructing high-quality Knowledge Graphs (KGs) from noisy, unstructured data by using an entity-centric strategy and a fine-tuned LLM as a graph judge.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in KG construction: noise in documents, inaccuracies from naive LLMs, and hallucination issues.

Method: Proposes GraphJudge with an entity-centric noise elimination strategy and a fine-tuned LLM as a graph judge.

Result: Achieves state-of-the-art performance on general and domain-specific datasets, demonstrating strong generalization.

Conclusion: GraphJudge effectively improves KG quality by mitigating noise and enhancing accuracy, with code publicly available.

Abstract: In real-world scenarios, most of the data obtained from the information
retrieval (IR) system is unstructured. Converting natural language sentences
into structured Knowledge Graphs (KGs) remains a critical challenge. We
identified three limitations with respect to existing KG construction methods:
(1) There could be a large amount of noise in real-world documents, which could
result in extracting messy information. (2) Naive LLMs usually extract
inaccurate knowledge from some domain-specific documents. (3) Hallucination
phenomenon cannot be overlooked when directly using LLMs to construct KGs. In
this paper, we propose \textbf{GraphJudge}, a KG construction framework to
address the aforementioned challenges. In this framework, we designed an
entity-centric strategy to eliminate the noise information in the documents.
And we fine-tuned a LLM as a graph judge to finally enhance the quality of
generated KGs. Experiments conducted on two general and one domain-specific
text-graph pair datasets demonstrate state-of-the-art performance against
various baseline methods with strong generalization abilities. Our code is
available at
\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.

</details>


### [157] [A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension](https://arxiv.org/pdf/2412.06245)
*Saahith Janapati, Yangfeng Ji*

Main category: cs.CL

TL;DR: The study compares how supervised fine-tuning (SFT) and in-context learning (ICL) affect the hidden representations of LLMs, finding ICL induces higher intrinsic dimensionality (ID) than SFT.


<details>
  <summary>Details</summary>
Motivation: To understand the distinct mechanisms of SFT and ICL and their impact on LLM representations.

Method: Uses Intrinsic Dimension (ID) to analyze representation changes during SFT and ICL, varying demonstration counts in ICL.

Result: ICL consistently induces higher ID than SFT, indicating higher-dimensional manifolds in embedding space.

Conclusion: ICL's higher ID suggests richer representation spaces compared to SFT, highlighting their distinct learning mechanisms.

Abstract: The performance of Large Language Models (LLMs) on natural language tasks can
be improved through both supervised fine-tuning (SFT) and in-context learning
(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates
the model's weights by minimizing loss on training data, whereas in-context
learning leverages task demonstrations embedded in the prompt, without changing
the model's parameters. This study investigates the effects of these learning
paradigms on the hidden representations of LLMs using Intrinsic Dimension (ID).
We use ID to estimate the number of degrees of freedom between representations
extracted from LLMs as they perform specific natural language tasks. We first
explore how the ID of LLM representations evolves during SFT and how it varies
due to the number of demonstrations in ICL. We then compare the IDs induced by
SFT and ICL and find that ICL consistently induces a higher ID compared to SFT,
suggesting that representations generated during ICL reside in higher
dimensional manifolds in the embedding space.

</details>


### [158] [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/pdf/2412.11936)
*Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, Xuming Hu*

Main category: cs.CL

TL;DR: A survey analyzing mathematical reasoning in multimodal large language models (MLLMs), covering benchmarks, methodologies, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To advance artificial general intelligence (AGI) by integrating LLMs with mathematical reasoning, especially in multimodal settings.

Method: Review of over 200 studies since 2021, categorizing developments into benchmarks, methodologies, and challenges.

Result: Identifies five major challenges in achieving AGI for multimodal mathematical reasoning.

Conclusion: The survey provides insights for enhancing multimodal reasoning and serves as a resource for future research.

Abstract: Mathematical reasoning, a core aspect of human cognition, is vital across
many domains, from educational problem-solving to scientific advancements. As
artificial general intelligence (AGI) progresses, integrating large language
models (LLMs) with mathematical reasoning tasks is becoming increasingly
significant. This survey provides the first comprehensive analysis of
mathematical reasoning in the era of multimodal large language models (MLLMs).
We review over 200 studies published since 2021, and examine the
state-of-the-art developments in Math-LLMs, with a focus on multimodal
settings. We categorize the field into three dimensions: benchmarks,
methodologies, and challenges. In particular, we explore multimodal
mathematical reasoning pipeline, as well as the role of (M)LLMs and the
associated methodologies. Finally, we identify five major challenges hindering
the realization of AGI in this domain, offering insights into the future
direction for enhancing multimodal reasoning capabilities. This survey serves
as a critical resource for the research community in advancing the capabilities
of LLMs to tackle complex multimodal reasoning tasks.

</details>


### [159] [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/pdf/2412.14161)
*Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, Graham Neubig*

Main category: cs.CL

TL;DR: The paper introduces TheAgentCompany, a benchmark for evaluating AI agents' performance in work-related tasks, finding current agents can complete 30% of tasks autonomously.


<details>
  <summary>Details</summary>
Motivation: To assess AI agents' ability to perform real-world professional tasks, impacting industry adoption and labor market policies.

Method: Developed TheAgentCompany, a self-contained environment mimicking a software company, testing baseline agents with closed and open-weights LMs.

Result: The most competitive agent autonomously completed 30% of tasks, showing potential for simpler tasks but limitations in complex ones.

Conclusion: Current AI agents can handle simpler tasks but struggle with long-horizon tasks, highlighting room for improvement.

Abstract: We interact with computers on an everyday basis, be it in everyday life or
work, and many aspects of work can be done entirely with access to a computer
and the Internet. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. But how
performant are AI agents at accelerating or even autonomously performing
work-related tasks? The answer to this question has important implications both
for industry looking to adopt AI into their workflows and for economic policy
to understand the effects that adoption of AI may have on the labor market. To
measure the progress of these LLM agents' performance on performing real-world
professional tasks, in this paper we introduce TheAgentCompany, an extensible
benchmark for evaluating AI agents that interact with the world in similar ways
to those of a digital worker: by browsing the Web, writing code, running
programs, and communicating with other coworkers. We build a self-contained
environment with internal web sites and data that mimics a small software
company environment, and create a variety of tasks that may be performed by
workers in such a company. We test baseline agents powered by both closed
API-based and open-weights language models (LMs), and find that the most
competitive agent can complete 30% of tasks autonomously. This paints a nuanced
picture on task automation with LM agents--in a setting simulating a real
workplace, a good portion of simpler tasks could be solved autonomously, but
more difficult long-horizon tasks are still beyond the reach of current
systems. We release code, data, environment, and experiments on
https://the-agent-company.com.

</details>


### [160] [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/pdf/2412.14470)
*Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang*

Main category: cs.CL

TL;DR: Agent-SafetyBench is introduced to evaluate LLM agent safety, revealing significant risks and the inadequacy of current defenses.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM agents in interactive environments introduces new safety challenges, but lacks benchmarks for evaluation.

Method: Agent-SafetyBench includes 349 environments and 2,000 test cases, assessing 8 safety risk categories and 10 failure modes.

Result: Evaluation of 16 LLM agents shows none score above 60% safety, highlighting robustness and risk awareness issues.

Conclusion: Current defenses like prompts are insufficient; advanced strategies are needed. Agent-SafetyBench is released to aid research.

Abstract: As large language models (LLMs) are increasingly deployed as agents, their
integration into interactive environments and tool use introduce new safety
challenges beyond those associated with the models themselves. However, the
absence of comprehensive benchmarks for evaluating agent safety presents a
significant barrier to effective assessment and further improvement. In this
paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to
evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349
interaction environments and 2,000 test cases, evaluating 8 categories of
safety risks and covering 10 common failure modes frequently encountered in
unsafe interactions. Our evaluation of 16 popular LLM agents reveals a
concerning result: none of the agents achieves a safety score above 60%. This
highlights significant safety challenges in LLM agents and underscores the
considerable need for improvement. Through failure mode and helpfulness
analysis, we summarize two fundamental safety defects in current LLM agents:
lack of robustness and lack of risk awareness. Furthermore, our findings
suggest that reliance on defense prompts alone may be insufficient to address
these safety issues, emphasizing the need for more advanced and robust
strategies. To drive progress in this area, Agent-SafetyBench has been released
at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further
research in agent safety evaluation and improvement.

</details>


### [161] [SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs](https://arxiv.org/pdf/2412.16783)
*Leon Fröhling, Pietro Bernardelle, Gianluca Demartini*

Main category: cs.CL

TL;DR: The paper introduces SubData, a Python library for standardizing datasets to evaluate LLM perspective alignment, and a theory-driven method to test differently-aligned LLMs on tasks like hate speech detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of inconsistent datasets in evaluating LLM alignment with human perspectives on subjective tasks.

Method: A two-step framework: (1) SubData library for dataset standardization, (2) theory-driven approach to test LLM alignment on tasks.

Result: SubData offers flexible mapping and taxonomy for diverse research needs, distinguishing it from existing tools.

Conclusion: The paper invites contributions to expand SubData into a benchmark suite for evaluating LLM perspective alignment in NLP tasks.

Abstract: As increasingly capable large language models (LLMs) emerge, researchers have
begun exploring their potential for subjective tasks. While recent work
demonstrates that LLMs can be aligned with diverse human perspectives,
evaluating this alignment on actual downstream tasks (e.g., hate speech
detection) remains challenging due to the use of inconsistent datasets across
studies. To address this issue, in this resource paper we propose a two-step
framework: we (1) introduce SubData, an open-source Python library designed for
standardizing heterogeneous datasets to evaluate LLM perspective alignment; and
(2) present a theory-driven approach leveraging this library to test how
differently-aligned LLMs (e.g., aligned with different political viewpoints)
classify content targeting specific demographics. SubData's flexible mapping
and taxonomy enable customization for diverse research needs, distinguishing it
from existing resources. We invite contributions to add datasets to our
initially proposed resource and thereby help expand SubData into a
multi-construct benchmark suite for evaluating LLM perspective alignment on NLP
tasks.

</details>


### [162] [Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts](https://arxiv.org/pdf/2501.02009)
*Youcheng Huang, Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv*

Main category: cs.CL

TL;DR: The paper explores aligning concept representations across different LLMs using linear transformations, enabling cross-model control and transfer of steering vectors (SVs).


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling LLM behavior by leveraging concept representations and their relationships across models.

Method: Introduces a linear transformation method to align concept representations (SVs) across LLMs, tested for cross-model transfer and control.

Result: 1) Linear transformations align SVs across LLMs. 2) Generalizes across concepts. 3) Smaller LLMs' SVs control larger LLMs.

Conclusion: Linear transformations enable efficient cross-model alignment and control of LLM behavior, with implications for model interpretability and safety.

Abstract: Understanding the inner workings of Large Language Models (LLMs) is a
critical research frontier. Prior research has shown that a single LLM's
concept representations can be captured as steering vectors (SVs), enabling the
control of LLM behavior (e.g., towards generating harmful content). Our work
takes a novel approach by exploring the intricate relationships between concept
representations across different LLMs, drawing an intriguing parallel to
Plato's Allegory of the Cave. In particular, we introduce a linear
transformation method to bridge these representations and present three key
findings: 1) Concept representations across different LLMs can be effectively
aligned using simple linear transformations, enabling efficient cross-model
transfer and behavioral control via SVs. 2) This linear transformation
generalizes across concepts, facilitating alignment and control of SVs
representing different concepts across LLMs. 3) A weak-to-strong
transferability exists between LLM concept representations, whereby SVs
extracted from smaller LLMs can effectively control the behavior of larger
LLMs.

</details>


### [163] [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/pdf/2501.02506)
*Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, Jiecao Chen*

Main category: cs.CL

TL;DR: ToolHop is a dataset for evaluating multi-hop tool use in LLMs, revealing challenges and performance gaps, with GPT-4o leading at 49.04% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliable evaluation datasets for assessing multi-hop tool-use capabilities in LLMs.

Method: ToolHop uses a query-driven approach for data construction, including tool creation, document refinement, and code generation.

Result: Evaluation of 14 LLMs shows significant challenges, with GPT-4o achieving 49.04% accuracy.

Conclusion: ToolHop provides actionable insights for improving LLM tool-use strategies, highlighting substantial room for advancement.

Abstract: Effective evaluation of multi-hop tool use is critical for analyzing the
understanding, reasoning, and function-calling capabilities of large language
models (LLMs). However, progress has been hindered by a lack of reliable
evaluation datasets. To address this, we present ToolHop, a dataset comprising
995 user queries and 3,912 associated tools, specifically designed for rigorous
evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful
interdependencies, locally executable tools, detailed feedback, and verifiable
answers through a novel query-driven data construction approach that includes
tool creation, document refinement, and code generation. We evaluate 14 LLMs
across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and
GPT), uncovering significant challenges in handling multi-hop tool-use
scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,
underscoring substantial room for improvement. Further analysis reveals
variations in tool-use strategies for various families, offering actionable
insights to guide the development of more effective approaches. Code and data
can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.

</details>


### [164] [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/pdf/2501.06582)
*Steven H. Wang, Maksim Zubkov, Kexin Fan, Sarah Harrell, Yuyang Sun, Wei Chen, Andreas Plesner, Roger Wattenhofer*

Main category: cs.CL

TL;DR: ACORD is the first expert-annotated retrieval benchmark for contract drafting, featuring complex clauses and ranked query-clause pairs. It aids in finding relevant precedent clauses, with bi-encoders and LLM re-rankers showing promise but needing further improvement.


<details>
  <summary>Details</summary>
Motivation: Lawyers rely on precedent clauses for contract drafting, but existing retrieval methods lack expert-annotated benchmarks for complex clauses. ACORD addresses this gap.

Method: ACORD includes 114 queries and 126,000 query-clause pairs, ranked 1-5 stars. Bi-encoder retrievers and LLM re-rankers are tested for relevance.

Result: Bi-encoders with LLM re-rankers show promise but require improvements to match lawyers' needs.

Conclusion: ACORD serves as a valuable benchmark for NLP in legal retrieval, though more work is needed for practical legal use.

Abstract: Information retrieval, specifically contract clause retrieval, is
foundational to contract drafting because lawyers rarely draft contracts from
scratch; instead, they locate and revise the most relevant precedent. We
introduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval
benchmark for contract drafting fully annotated by experts. ACORD focuses on
complex contract clauses such as Limitation of Liability, Indemnification,
Change of Control, and Most Favored Nation. It includes 114 queries and over
126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task
is to find the most relevant precedent clauses to a query. The bi-encoder
retriever paired with pointwise LLMs re-rankers shows promising results.
However, substantial improvements are still needed to effectively manage the
complex legal work typically undertaken by lawyers. As the first retrieval
benchmark for contract drafting annotated by experts, ACORD can serve as a
valuable IR benchmark for the NLP community.

</details>


### [165] [TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time](https://arxiv.org/pdf/2501.07482)
*Thales Sales Almeida, Giovana Kerche Bonás, João Guilherme Alves Santos, Hugo Abonizio, Rodrigo Nogueira*

Main category: cs.CL

TL;DR: The paper introduces TiEBe, a benchmark dataset to evaluate LLMs' retention of global and regional events over time, revealing geographic and linguistic disparities in factual recall.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks assessing LLMs' knowledge retention over time and across regions, and to highlight disparities in factual recall.

Method: Created TiEBe, a dataset of 23,000+ question-answer pairs from Wikipedia-structured data, spanning 10+ years, 23 regions, and 13 languages, to evaluate LLMs.

Result: Found geographic disparities in recall, a correlation with socioeconomic indicators, and performance gaps for low-resource languages.

Conclusion: Emphasizes the need for balanced global representation in LLM training and highlights the impact of language on factual recall.

Abstract: As the knowledge landscape evolves and large language models (LLMs) become
increasingly widespread, there is a growing need to keep these models updated
with current events. While existing benchmarks assess general factual recall,
few studies explore how LLMs retain knowledge over time or across different
regions. To address these gaps, we present the Timely Events Benchmark (TiEBe),
a dataset of over 23,000 question-answer pairs centered on notable global and
regional events, spanning more than 10 years of events, 23 regions, and 13
languages. TiEBe leverages structured retrospective data from Wikipedia to
identify notable events through time. These events are then used to construct a
benchmark to evaluate LLMs' understanding of global and regional developments,
grounded in factual evidence beyond Wikipedia itself. Our results reveal
significant geographic disparities in factual recall, emphasizing the need for
more balanced global representation in LLM training. We also observe a Pearson
correlation of more than 0.7 between models' performance in TiEBe and various
countries' socioeconomic indicators, such as HDI. In addition, we examine the
impact of language on factual recall by posing questions in the native language
of the region where each event occurred, uncovering substantial performance
gaps for low-resource languages.

</details>


### [166] [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/pdf/2501.14315)
*Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee*

Main category: cs.CL

TL;DR: Fine-tuning with LLM-generated data improves cross-domain generalization by reducing high perplexity tokens, preserving non-target task performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining consistent model performance across domains and understand the impact of LLM-generated data on cross-domain generalization.

Method: Systematic analysis of fine-tuning with LLM-generated data, comparing it to ground truth data, and masking high perplexity tokens in ground truth data.

Result: LLM-generated data reduces non-target task degradation and improves robustness by lowering token perplexity. Masking high perplexity tokens in ground truth data achieves similar results.

Conclusion: Token perplexity reduction is key to mitigating catastrophic forgetting in LLMs, offering insights for robust fine-tuning strategies.

Abstract: Maintaining consistent model performance across domains is a fundamental
challenge in machine learning. While recent work has explored using
LLM-generated data for fine-tuning, its impact on cross-domain generalization
remains poorly understood. This paper presents a systematic analysis revealing
that fine-tuning with LLM-generated data not only improves target task
performance but also reduces non-target task degradation compared to
fine-tuning with ground truth data. Through analyzing the data sequence in
tasks of various domains, we demonstrate that this enhancement of non-target
task robustness stems from the reduction of high perplexity tokens found in
LLM-generated sequences. Following our findings, we showed that masking high
perplexity tokens in ground truth training data achieves similar non-target
task performance preservation, comparable to using LLM-generated data.
Extensive experiments across different model families and scales, including
Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our
findings. To the best of our knowledge, this is the first work to provide an
empirical explanation based on token perplexity reduction to mitigate
catastrophic forgetting in LLMs after fine-tuning, offering valuable insights
for developing more robust fine-tuning strategies.

</details>


### [167] [STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection](https://arxiv.org/pdf/2501.15451)
*Zewen Bai, Shengdi Yin, Junyu Lu, Jingjie Zeng, Haohao Zhu, Yuanyuan Sun, Liang Yang, Hongfei Lin*

Main category: cs.CL

TL;DR: The paper introduces a fine-grained solution for detecting Chinese hate speech, including a new dataset (STATE ToxiCN) and evaluations of existing models and LLMs for hateful slang detection.


<details>
  <summary>Details</summary>
Motivation: The lack of span-level fine-grained annotations and research on Chinese hateful slang in hate speech detection motivated this work.

Method: Constructed a dataset (STATE ToxiCN) with Target-Argument-Hateful-Group quadruples, evaluated existing models, and studied Chinese hateful slang using LLMs.

Result: Provided the first span-level Chinese hate speech dataset and insights into model performance and slang detection.

Conclusion: The work advances Chinese hate speech detection by offering resources and evaluations for fine-grained analysis.

Abstract: The proliferation of hate speech has caused significant harm to society. The
intensity and directionality of hate are closely tied to the target and
argument it is associated with. However, research on hate speech detection in
Chinese has lagged behind, and existing datasets lack span-level fine-grained
annotations. Furthermore, the lack of research on Chinese hateful slang poses a
significant challenge. In this paper, we provide a solution for fine-grained
detection of Chinese hate speech. First, we construct a dataset containing
Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first
span-level Chinese hate speech dataset. Secondly, we evaluate the span-level
hate speech detection performance of existing models using STATE ToxiCN.
Finally, we conduct the first study on Chinese hateful slang and evaluate the
ability of LLMs to detect such expressions. Our work contributes valuable
resources and insights to advance span-level hate speech detection in Chinese.

</details>


### [168] [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/pdf/2502.13061)
*Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne*

Main category: cs.CL

TL;DR: A robust adaptation framework for hateful meme detection improves accuracy and generalization in LMMs, outperforming larger systems and enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Hateful memes are a growing online concern, but current LMMs struggle with performance and generalization. Existing methods like SFT and in-context learning are insufficient.

Method: Proposes a robust adaptation framework for LMMs to enhance in-domain accuracy and cross-domain generalization while maintaining vision-language capabilities.

Result: Achieves state-of-the-art performance on six datasets, outperforming larger systems, and generates better rationales for hateful content.

Conclusion: The framework effectively addresses LMM limitations in hateful meme detection, improving performance and interpretability.

Abstract: Hateful memes have become a significant concern on the Internet,
necessitating robust automated detection systems. While LMMs have shown promise
in hateful meme detection, they face notable challenges like sub-optimal
performance and limited out-of-domain generalization capabilities. Recent
studies further reveal the limitations of both SFT and in-context learning when
applied to LMMs in this setting. To address these issues, we propose a robust
adaptation framework for hateful meme detection that enhances in-domain
accuracy and cross-domain generalization while preserving the general
vision-language capabilities of LMMs. Experiments on six meme classification
datasets show that our approach achieves state-of-the-art performance,
outperforming larger agentic systems. Moreover, our method generates
higher-quality rationales for explaining hateful content compared to standard
SFT, enhancing model interpretability.

</details>


### [169] [People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text](https://arxiv.org/pdf/2501.15654)
*Jenna Russell, Marzena Karpinska, Mohit Iyyer*

Main category: cs.CL

TL;DR: Humans, especially frequent LLM users, excel at detecting AI-generated text, outperforming automated detectors, and rely on lexical and complex textual clues.


<details>
  <summary>Details</summary>
Motivation: To evaluate human ability in detecting AI-generated text compared to automated methods, especially among frequent LLM users.

Method: Hired annotators to label 300 non-fiction articles as human or AI-generated, with explanations. Compared performance of 'expert' annotators (frequent LLM users) against commercial/open-source detectors.

Result: Expert annotators misclassified only 1 of 300 articles, outperforming detectors. They used lexical and complex textual cues (e.g., formality, originality).

Conclusion: Humans, particularly LLM users, are highly effective at detecting AI text, highlighting limitations of current automated detectors. Dataset and code released for future research.

Abstract: In this paper, we study how well humans can detect text generated by
commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300
non-fiction English articles, label them as either human-written or
AI-generated, and provide paragraph-length explanations for their decisions.
Our experiments show that annotators who frequently use LLMs for writing tasks
excel at detecting AI-generated text, even without any specialized training or
feedback. In fact, the majority vote among five such "expert" annotators
misclassifies only 1 of 300 articles, significantly outperforming most
commercial and open-source detectors we evaluated even in the presence of
evasion tactics like paraphrasing and humanization. Qualitative analysis of the
experts' free-form explanations shows that while they rely heavily on specific
lexical clues ('AI vocabulary'), they also pick up on more complex phenomena
within the text (e.g., formality, originality, clarity) that are challenging to
assess for automatic detectors. We release our annotated dataset and code to
spur future research into both human and automated detection of AI-generated
text.

</details>


### [170] [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/pdf/2501.19202)
*Dang Huu-Tien, Hoang Thanh-Tung, Anh Bui, Le-Minh Nguyen, Naoya Inoue*

Main category: cs.CL

TL;DR: Current LLM unlearning methods reduce model robustness; forget-tokens act like backdoor triggers. RNA is proposed to mitigate this, improving robustness without extra overhead.


<details>
  <summary>Details</summary>
Motivation: To address the reduced robustness in LLM unlearning caused by forget-tokens, which disrupt model behavior.

Method: Propose Random Noise Augmentation (RNA), a model-agnostic approach with theoretical guarantees.

Result: RNA significantly improves robustness, maintains unlearning performance, and adds no computational overhead.

Conclusion: RNA effectively mitigates robustness issues in LLM unlearning without compromising performance or efficiency.

Abstract: In this paper, we show that current state-of-the-art LLM unlearning methods
inherently reduce models' robustness, causing them to misbehave even when a
single non-adversarial forget-token is in the retain-query. Toward
understanding underlying causes, we reframe the unlearning process as backdoor
attacks and defenses: forget-tokens act as backdoor triggers that, when
activated in retain-queries, cause disruptions in unlearned models' behaviors,
similar to successful backdoor attacks. To mitigate this vulnerability, we
propose Random Noise Augmentation (RNA) -- a plug-and-play, model and method
agnostic approach with theoretical guarantees for improving the robustness of
unlearned models. Extensive experiments demonstrate that RNA significantly
improves the robustness of unlearned models, maintains unlearning performances
while introducing no additional computational overhead.

</details>


### [171] [LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information](https://arxiv.org/pdf/2502.02095)
*Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, Shanghang Zhang*

Main category: cs.CL

TL;DR: The paper proposes a method to improve long-form generation by using process supervision with Monte Carlo Tree Search and step-level DPO, achieving better length and quality without compromising general performance.


<details>
  <summary>Details</summary>
Motivation: Current models like GPT-4o underperform in long-form generation, lacking detailed feedback for extended contexts, leading to issues like length deviations and reduced quality.

Method: The approach involves Monte Carlo Tree Search for stepwise preference pairs, a global memory pool for consistency, and external critiques to refine candidates. Step-level DPO is then applied.

Result: The method improves length and quality in long-form generation benchmarks while maintaining performance on general benchmarks.

Conclusion: Incorporating process supervision with stepwise feedback enhances long-form generation without sacrificing general model capabilities.

Abstract: Long-form generation is crucial for academic writing papers and repo-level
code generation. Despite this, current models, including GPT-4o, still exhibit
unsatisfactory performance. Existing methods that utilize preference learning
with outcome supervision often fail to provide detailed feedback for extended
contexts. This shortcoming can lead to content that does not fully satisfy
query requirements, resulting in issues like length deviations, and diminished
quality. In this paper, we propose enhancing long-form generation by
incorporating process supervision. We employ Monte Carlo Tree Search to gather
stepwise preference pairs, utilizing a global memory pool to maintain
consistency. To address the issue of suboptimal candidate selection, we
integrate external critiques to refine and improve the quality of the
preference pairs. Finally, we apply step-level DPO using the collected stepwise
preference pairs. Experimental results show that our method improves length and
quality on long-form generation benchmarks, with almost lossless performance on
general benchmarks across various model backbones.

</details>


### [172] [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/pdf/2502.02362)
*Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: The paper introduces Premise Augmented Reasoning Chains (PARC) to improve the evaluation of reasoning in LLMs by identifying premises for each step, enhancing error detection and verification.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought (CoT) prompting in LLMs produces verbose reasoning chains, making verification and error tracing difficult due to dependencies between steps.

Method: The authors restructure linear reasoning chains into PARC, a directed acyclic graph with premise links, and test it using the PERL dataset.

Result: LLMs achieve 90% recall in premise identification, and PARC improves error identification accuracy by 6-16%.

Conclusion: PARC enhances reliability in LLM-based reasoning evaluations and opens new avenues for complex problem-solving.

Abstract: Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large
language models (LLMs) by enabling detailed step-by-step solutions. However,
due to the verbosity of LLMs, the resulting reasoning chains can be long,
making it harder to verify the reasoning steps and trace issues resulting from
dependencies between the steps that may be farther away in the sequence of
steps. Importantly, mathematical reasoning allows each step to be derived from
a small set of premises, which are a subset of the preceding steps in the
reasoning chain. In this paper, we present a framework that identifies the
premises for each step, to improve the evaluation of reasoning. We restructure
conventional linear reasoning chains into Premise Augmented Reasoning Chains
(PARC) by introducing premise links, resulting in a directed acyclic graph
where the nodes are the steps and the edges are the premise links. Through
experiments with a PARC-based dataset that we built, namely PERL (Premises and
ERrors identification in LLMs), we demonstrate that LLMs can reliably identify
premises within complex reasoning chains. In particular, even open-source LLMs
achieve 90% recall in premise identification. We also show that PARC helps to
identify errors in reasoning chains more reliably. The accuracy of error
identification improves by 6% to 16% absolute when step-by-step verification is
carried out in PARC under the premises. Our findings highlight the utility of
premise-centric representations in addressing complex problem-solving tasks and
open new avenues for improving the reliability of LLM-based reasoning
evaluations.

</details>


### [173] [A comparison of translation performance between DeepL and Supertext](https://arxiv.org/pdf/2502.02577)
*Alex Flückiger, Chantal Amrhein, Tim Graf, Frédéric Odermatt, Martin Pömsl, Philippe Schläpfer, Florian Schottmann, Samuel Läubli*

Main category: cs.CL

TL;DR: The study compares DeepL and Supertext MT systems on unsegmented texts, finding Supertext more consistent at document-level despite similar segment-level performance.


<details>
  <summary>Details</summary>
Motivation: To assess MT systems' ability to leverage extended context, as current benchmarking often overlooks this aspect.

Method: Evaluated translation quality of DeepL and Supertext on unsegmented texts across four language directions, using professional translators for document-level assessments.

Result: Supertext outperformed DeepL in three out of four language directions at document-level, though segment-level results were comparable.

Conclusion: Advocates for context-sensitive evaluation methodologies to better reflect real-world MT usability and releases evaluation data for reproducibility.

Abstract: As strong machine translation (MT) systems are increasingly based on large
language models (LLMs), reliable quality benchmarking requires methods that
capture their ability to leverage extended context. This study compares two
commercial MT systems -- DeepL and Supertext -- by assessing their performance
on unsegmented texts. We evaluate translation quality across four language
directions with professional translators assessing segments with full
document-level context. While segment-level assessments indicate no strong
preference between the systems in most cases, document-level analysis reveals a
preference for Supertext in three out of four language directions, suggesting
superior consistency across longer texts. We advocate for more
context-sensitive evaluation methodologies to ensure that MT quality
assessments reflect real-world usability. We release all evaluation data and
scripts for further analysis and reproduction at
https://github.com/supertext/evaluation_deepl_supertext.

</details>


### [174] [Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation](https://arxiv.org/pdf/2502.02789)
*Jingyu Liu, Beidi Chen, Ce Zhang*

Main category: cs.CL

TL;DR: SpecPrefill accelerates LLM inference by selecting key prompt tokens, improving TTFT and QPS without training.


<details>
  <summary>Details</summary>
Motivation: Optimizing TTFT is critical for LLM performance but challenging due to compute bottlenecks shifting to MLP.

Method: Uses a lightweight model to speculate important tokens, sending them with positional info to the main model.

Result: Achieves up to 7x QPS and 7.66x TTFT improvement on real tasks.

Conclusion: SpecPrefill effectively boosts LLM inference efficiency by focusing on key tokens.

Abstract: Improving time-to-first-token (TTFT) is an essentially important objective in
modern large language model (LLM) inference engines. Optimizing TTFT directly
results in higher maximal QPS and meets the requirements of many critical
applications. However, boosting TTFT is notoriously challenging since it is
compute-bounded and the performance bottleneck shifts from the self-attention
that many prior works focus on to the MLP part. In this work, we present
SpecPrefill, a training free framework that accelerates the inference TTFT for
both long and medium context queries based on the following insight: LLMs are
generalized enough to preserve the quality given only a carefully chosen subset
of prompt tokens. At its core, SpecPrefill leverages a lightweight model to
speculate locally important tokens based on the context. These tokens, along
with the necessary positional information, are then sent to the main model for
processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a
comprehensive benchmarking of performance improvement both in a real end-to-end
setting and ablation studies. SpecPrefill manages to serve
Llama-3.1-405B-Instruct-FP8 with up to 7$\times$ maximal end-to-end QPS on real
downstream tasks and 7.66$\times$ TTFT improvement.

</details>


### [175] [Who Taught You That? Tracing Teachers in Model Distillation](https://arxiv.org/pdf/2502.06659)
*Somin Wadhwa, Chantal Shaib, Silvio Amir, Byron C. Wallace*

Main category: cs.CL

TL;DR: The paper investigates whether a student model's outputs can reveal its teacher model, exploring implications for proprietary model distillation.


<details>
  <summary>Details</summary>
Motivation: To identify if teacher models leave detectable 'footprints' in student models, with potential legal and practical implications for proprietary model use.

Method: Uses discriminative models with lexical features, testing n-gram similarity and part-of-speech (PoS) templates across tasks like summarization and question answering.

Result: N-gram similarity is unreliable, but PoS templates in student models mimic those of their teachers, enabling identification.

Conclusion: Teacher models leave detectable traces in student models via PoS templates, raising concerns for proprietary model misuse.

Abstract: Model distillation -- using outputs from a large teacher model to teach a
small student model -- is a practical means of creating efficient models for a
particular task. We ask: Can we identify a students' teacher based on its
outputs? Such "footprints" left by teacher LLMs would be interesting artifacts.
Beyond this, reliable teacher inference may have practical implications as
actors seek to distill specific capabilities of massive proprietary LLMs into
deployed smaller LMs, potentially violating terms of service. We consider
practical task distillation targets including summarization, question
answering, and instruction-following. We assume a finite set of candidate
teacher models, which we treat as blackboxes. We design discriminative models
that operate over lexical features. We find that $n$-gram similarity alone is
unreliable for identifying teachers, but part-of-speech (PoS) templates
preferred by student models mimic those of their teachers.

</details>


### [176] [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/pdf/2502.11051)
*Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu*

Main category: cs.CL

TL;DR: The paper proposes MMUnlearner, a method for multimodal machine unlearning in MLLMs, focusing on erasing visual patterns while preserving textual knowledge. It outperforms baselines like GA and NPO.


<details>
  <summary>Details</summary>
Motivation: Machine Unlearning (MU) for Multimodal Large Language Models (MLLMs) is underdeveloped, especially in selectively removing visual patterns without affecting textual knowledge.

Method: Develops MMUnlearner, a geometry-constrained gradient ascent method, using a weight saliency map to update MLLM weights while preserving non-target knowledge.

Result: MMUnlearner outperforms baselines (GA and NPO) in all evaluation dimensions.

Conclusion: The method effectively addresses multimodal MU in MLLMs, balancing visual pattern removal with textual knowledge retention.

Abstract: Recent progress in Machine Unlearning (MU) has introduced solutions for the
selective removal of private or sensitive information encoded within deep
neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)
remains in its nascent phase. Therefore, we propose to reformulate the task of
multimodal MU in the era of MLLMs, which aims to erase only the visual patterns
associated with a given entity while preserving the corresponding textual
knowledge encoded within the original parameters of the language model
backbone. Furthermore, we develop a novel geometry-constrained gradient ascent
method MMUnlearner. It updates the weights of MLLMs with a weight saliency map
jointly restricted by the remaining concepts and textual knowledge during
unlearning, thereby preserving parameters essential for non-target knowledge.
Extensive experiments demonstrate that MMUnlearner surpasses baselines that
finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or
Negative Preference Optimization (NPO), across all evaluation dimensions. Our
code will be released upon acceptance.

</details>


### [177] [CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment](https://arxiv.org/pdf/2502.11066)
*Nura Aljaafari, Danilo S. Carvalho, André Freitas*

Main category: cs.CL

TL;DR: CARMA improves compositional reasoning in LLMs by using mutual information regularization and layer-wise stability constraints, enhancing stability and robustness without sacrificing fine-tuned performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with compositional generalization, and existing methods like architectural changes or data augmentation have limitations in adaptability and scalability.

Method: CARMA employs mutual information regularization and layer-wise stability constraints to mitigate feature fragmentation and maintain structured representations.

Result: CARMA reduces fine-tuning variability, stabilizes token representations, and improves compositional reasoning, though effectiveness varies by architecture.

Conclusion: Integrating CARMA with fine-tuning can enhance compositional generalization in LLMs while preserving task-specific performance, offering a scalable auxiliary solution.

Abstract: Large language models (LLMs) struggle with compositional generalisation,
limiting their ability to systematically combine learned components to
interpret novel inputs. While architectural modifications, fine-tuning, and
data augmentation improve compositionality, they often have limited
adaptability, face scalability constraints, or yield diminishing returns on
real data. To address this, we propose CARMA, an intervention that enhances the
stability and robustness of compositional reasoning in LLMs while preserving
fine-tuned performance. CARMA employs mutual information regularisation and
layer-wise stability constraints to mitigate feature fragmentation, ensuring
structured representations persist across and within layers. We evaluate CARMA
on inverse dictionary modelling and sentiment classification, measuring its
impact on semantic consistency, performance stability, and robustness to
lexical perturbations. Results show that CARMA reduces the variability
introduced by fine-tuning, stabilises token representations, and improves
compositional reasoning. While its effectiveness varies across architectures,
CARMA's key strength lies in reinforcing learned structures rather than
introducing new capabilities, making it a scalable auxiliary method. These
findings suggest that integrating CARMA with fine-tuning can improve
compositional generalisation while maintaining task-specific performance in
LLMs.

</details>


### [178] [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/pdf/2502.11100)
*Milan Bhan, Yann Choho, Pierre Moreau, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: CT-CBM is an unsupervised TCBM generator that builds concept labels without predefined human labels or LLM annotations, enhancing interpretability without performance loss.


<details>
  <summary>Details</summary>
Motivation: To create interpretable text classification models without relying on predefined human-labeled concepts or LLM annotations.

Method: CT-CBM iteratively adds important concepts in the bottleneck layer using a small language model and addresses leakage via parallel residual connections.

Result: CT-CBM achieves competitive performance while improving interpretability.

Conclusion: CT-CBM offers a promising solution for interpretable NLP classifiers without sacrificing accuracy.

Abstract: Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models
for text classification that predict a set of salient concepts before making
the final prediction. This paper proposes Complete Textual Concept Bottleneck
Model (CT-CBM),a novel TCBM generator building concept labels in a fully
unsupervised manner using a small language model, eliminating both the need for
predefined human labeled concepts and LLM annotations. CT-CBM iteratively
targets and adds important concepts in the bottleneck layer to create a
complete concept basis and addresses downstream classification leakage through
a parallel residual connection. CT-CBM achieves good results against
competitors, offering a promising solution to enhance interpretability of NLP
classifiers without sacrificing performance.

</details>


### [179] [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/pdf/2502.11733)
*Jonathan Jordan, Sherzod Hakimov, David Schlangen*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' incremental and in-context learning in a text-based environment, focusing on task-solving, efficiency, and inferring synthetic words. Larger models outperform open-weight ones but struggle with synthetic words.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' adaptability and learning efficiency in agent systems, particularly for situated or embodied action tasks.

Method: Experiments in a text-based environment testing incremental learning, controlled in-context learning, and synthetic word inference.

Result: Larger commercial models outperform open-weight ones, but all struggle with synthetic words.

Conclusion: LLMs show promise in agent systems but face challenges in inferring unknown words, highlighting room for improvement.

Abstract: Large Language Models (LLMs) serve not only as chatbots but as key components
in agent systems, where their common-sense knowledge significantly impacts
performance as language-based planners for situated or embodied action. We
assess LLMs' incremental learning (based on feedback from the environment), and
controlled in-context learning abilities using a text-based environment. We
introduce challenging yet interesting set of experiments to test i) how agents
can incrementally solve tasks related to every day objects in typical rooms in
a house where each of them are discovered by interacting within the
environment, ii) controlled in-context learning abilities and efficiency of
agents by providing short info about locations of objects and rooms to check
how faster the task can be solved, and finally iii) using synthetic
pseudo-English words to gauge how well LLMs are at inferring meaning of unknown
words from environmental feedback. Results show that larger commercial models
have a substantial gap in performance compared to open-weight but almost all
models struggle with the synthetic words experiments.

</details>


### [180] [EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](https://arxiv.org/pdf/2502.11916)
*Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu*

Main category: cs.CL

TL;DR: EssayJudge is a multimodal benchmark for AES, addressing traditional challenges by leveraging MLLMs for trait-specific scoring and multimodal context understanding.


<details>
  <summary>Details</summary>
Motivation: Traditional AES systems struggle with generalizability, fine-grained trait capture, and multimodal contexts. EssayJudge aims to overcome these using MLLMs.

Method: Proposes EssayJudge, a benchmark evaluating AES capabilities across lexical-, sentence-, and discourse-level traits using MLLMs.

Result: Experiments with 18 MLLMs show performance gaps, especially in discourse-level traits, compared to human evaluation.

Conclusion: EssayJudge highlights the potential and current limitations of MLLMs in AES, calling for further research to bridge gaps.

Abstract: Automated Essay Scoring (AES) plays a crucial role in educational assessment
by providing scalable and consistent evaluations of writing tasks. However,
traditional AES systems face three major challenges: (1) reliance on
handcrafted features that limit generalizability, (2) difficulty in capturing
fine-grained traits like coherence and argumentation, and (3) inability to
handle multimodal contexts. In the era of Multimodal Large Language Models
(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES
capabilities across lexical-, sentence-, and discourse-level traits. By
leveraging MLLMs' strengths in trait-specific scoring and multimodal context
understanding, EssayJudge aims to offer precise, context-rich evaluations
without manual feature engineering, addressing longstanding AES limitations.
Our experiments with 18 representative MLLMs reveal gaps in AES performance
compared to human evaluation, particularly in discourse-level traits,
highlighting the need for further advancements in MLLM-based AES research.

</details>


### [181] [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2502.11811)
*Qianchi Zhang, Hainan Zhang, Liang Pang, Ziwei Wang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng*

Main category: cs.CL

TL;DR: FineFilter is a fine-grained noise filtering mechanism for RAG, improving QA performance by optimizing clue extraction, reranking, and truncation.


<details>
  <summary>Details</summary>
Motivation: Noise in retrieved documents hinders RAG's accuracy, and existing methods struggle with direct answer clue detection.

Method: FineFilter treats noise filtering as a sentence-level MinMax problem, using a clue extractor, reranker, and truncator.

Result: FineFilter outperforms baselines on QA datasets, enhancing complex reasoning and robustness.

Conclusion: FineFilter effectively improves RAG performance by fine-grained noise filtering and generalization across scenarios.

Abstract: Retrieved documents containing noise will hinder Retrieval-Augmented
Generation (RAG) from detecting answer clues, necessitating noise filtering
mechanisms to enhance accuracy. Existing methods use reranking or summarization
to identify the most relevant sentences, but directly and accurately locating
answer clues from these large-scale and complex documents remains challenging.
Unlike these document-level operations, we treat noise filtering as a
sentence-level MinMax optimization problem: first identifying potential clues
from multiple documents, then ranking them by relevance, and finally retaining
the minimum number of clues through truncation. In this paper, we propose
FineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting
of a clue extractor, a reranker, and a truncator. We optimize each module to
tackle complex reasoning challenges: (1) The clue extractor first uses
sentences containing the answer and similar ones as fine-tuning targets, aiming
to extract sufficient potential clues; (2) The reranker is trained to
prioritize effective clues based on the real feedback from the generation
module, with clues capable of generating correct answers as positive samples
and others as negative; (3) The truncator takes the minimum number of clues
needed to answer the question (truncation point) as fine-tuning targets, and
performs truncation on the reranked clues to achieve fine-grained noise
filtering. Experiments on three QA datasets demonstrate that FineFilter
significantly improves QA performance over baselines on both LLaMA3 and
Mistral. Further analysis confirms its effectiveness in complex reasoning,
robustness to unreliable retrieval, and generalization to different scenarios.

</details>


### [182] [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/pdf/2502.12464)
*Seanie Lee, Dong Bok Lee, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang*

Main category: cs.CL

TL;DR: SafeRoute is a binary router that selectively uses a large safety guard model for hard examples, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Large safety guard models are computationally expensive, while smaller distilled models underperform on hard examples. SafeRoute aims to balance cost and performance.

Method: Proposes SafeRoute, a binary router to distinguish hard from easy examples, applying the larger model only to hard cases.

Result: Experimental results show SafeRoute enhances the trade-off between computational cost and safety performance, outperforming baselines.

Conclusion: SafeRoute efficiently maintains accuracy by selectively using the larger model, optimizing computational resources.

Abstract: Deploying large language models (LLMs) in real-world applications requires
robust safety guard models to detect and block harmful user prompts. While
large safety guard models achieve strong performance, their computational cost
is substantial. To mitigate this, smaller distilled models are used, but they
often underperform on "hard" examples where the larger model provides accurate
predictions. We observe that many inputs can be reliably handled by the smaller
model, while only a small fraction require the larger model's capacity.
Motivated by this, we propose SafeRoute, a binary router that distinguishes
hard examples from easy ones. Our method selectively applies the larger safety
guard model to the data that the router considers hard, improving efficiency
while maintaining accuracy compared to solely using the larger safety guard
model. Experimental results on multiple benchmark datasets demonstrate that our
adaptive model selection significantly enhances the trade-off between
computational cost and safety performance, outperforming relevant baselines.

</details>


### [183] [R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](https://arxiv.org/pdf/2502.12767)
*Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi*

Main category: cs.CL

TL;DR: R2-KG is a dual-agent framework combining low- and high-capacity LLMs for KG-based reasoning, improving accuracy and reliability while reducing costs.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks require re-tuning for KG/task changes and rely on high-capacity LLMs, limiting practicality.

Method: R2-KG uses an Operator (low-capacity LLM) for evidence gathering and a Supervisor (high-capacity LLM) for judgments, with an Abstention mechanism for reliability.

Result: R2-KG outperforms baselines in accuracy and reliability across five benchmarks, even with low-capacity Operators.

Conclusion: R2-KG offers a flexible, cost-effective solution for KG-based reasoning, reducing reliance on high-capacity LLMs while ensuring trustworthiness.

Abstract: Recent studies have combined Large Language Models (LLMs) with Knowledge
Graphs (KGs) to enhance reasoning, improving inference accuracy without
additional training while mitigating hallucination. However, existing
frameworks still suffer two practical drawbacks: they must be re-tuned whenever
the KG or reasoning task changes, and they depend on a single, high-capacity
LLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce
R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two
roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor
(a high-capacity LLM) that makes final judgments. This design is cost-efficient
for LLM inference while still maintaining strong reasoning accuracy.
Additionally, R2-KG employs an Abstention mechanism, generating answers only
when sufficient evidence is collected from KG, which significantly enhances
reliability. Experiments across five diverse benchmarks show that R2-KG
consistently outperforms baselines in both accuracy and reliability, regardless
of the inherent capability of LLMs used as the Operator. Further experiments
reveal that the single-agent version of R2-KG, equipped with a strict
self-consistency strategy, achieves significantly higher-than-baseline
reliability with reduced inference cost but increased abstention rate in
complex KGs. Our findings establish R2-KG as a flexible and cost-effective
solution for KG-based reasoning, reducing reliance on high-capacity LLMs while
ensuring trustworthy inference. The code is available at
https://github.com/ekrxjwh2009/R2-KG/.

</details>


### [184] [TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/pdf/2502.13442)
*Jialin Ouyang*

Main category: cs.CL

TL;DR: TreeCut is a synthetic dataset designed to test LLMs' ability to handle unanswerable math problems by systematically generating such problems and their answerable counterparts. It reveals high hallucination rates in models like GPT-4o and o3-mini.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLMs' unfounded confidence in answering unanswerable math problems and to systematically evaluate their reasoning limitations.

Method: TreeCut generates infinite unanswerable problems by representing questions as trees and removing necessary conditions, then tests LLMs like GPT-4o and o3-mini.

Result: Hallucination rates of 64% (GPT-4o) and 44% (o3-mini) in worst-case scenarios, with deeper trees, complex item names, and mid-path condition removals increasing hallucinations.

Conclusion: LLMs struggle to identify unanswerable math problems, highlighting persistent reasoning challenges despite their high performance on standard benchmarks.

Abstract: Large language models (LLMs) now achieve near-human performance on standard
math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability
remains disputed. A key concern is that models often produce confident, yet
unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic
dataset that systematically generates infinite unanswerable math word problems
and their answerable counterparts, by representing each question as a tree and
removing chosen necessary conditions. Experiments show TreeCut effectively
induce hallucinations in large language models, including GPT-4o and o3-mini,
with rates of 64% and 44% in their respective worst-case scenarios under
zero-shot setting. Further analysis highlights that deeper or more complex
trees, composite item names, and removing necessary condition near the middle
of a path all increase the likelihood of hallucinations, underscoring the
persistent challenges LLMs face in identifying unanswerable math problems. The
dataset generation code and sample data are available at
https://github.com/j-bagel/treecut-math.

</details>


### [185] [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/pdf/2502.14037)
*Giorgio Franceschelli, Mirco Musolesi*

Main category: cs.CL

TL;DR: Proposes three new decoding methods to improve text generation by analyzing token probability distributions, ensuring contextual appropriateness without sacrificing quality or diversity.


<details>
  <summary>Details</summary>
Motivation: Language models often reproduce training data, generate repetitive text, or favor common patterns due to flawed decoding strategies.

Method: Leverages mathematical analysis of token probability distributions, using differences between consecutive sorted probabilities to truncate incorrect tokens.

Result: Outperforms or matches existing methods in quality and diversity across tasks like math problem solving, summarization, and divergent association.

Conclusion: The proposed decoding methods effectively balance diversity and accuracy in text generation.

Abstract: Despite their growing capabilities, language models still frequently
reproduce content from their training data, generate repetitive text, and favor
common grammatical patterns and vocabulary. A possible cause is the decoding
strategy: the most common strategies either consider only the most probable
tokens, which reduces output diversity, or increase the likelihood of unlikely
tokens, compromising output accuracy and correctness. In this paper, we propose
three new decoding methods that leverage a mathematical analysis of the token
probability distribution to ensure the generation of contextually appropriate
text. In particular, the difference between consecutive, sorted probabilities
can be used to truncate incorrect tokens. Experiments concerning math problem
solving, extreme summarization, and the divergent association task demonstrate
that our approach consistently performs at least as well as existing methods in
terms of quality and diversity.

</details>


### [186] [Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction](https://arxiv.org/pdf/2502.14171)
*Mehdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim*

Main category: cs.CL

TL;DR: The study explores how open-source LLMs (LLaMA) handle Theory of Mind (ToM) in responses, finding that explicit ToM alignment improves quality, with win rates of 67% and 63% for 3B and 8B models.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack ToM alignment, a key human communication trait. The study aims to assess and improve ToM in LLMs.

Method: Experiments on LLaMA 3 variants, manipulating ToM components (beliefs, desires, intentions) to evaluate response alignment.

Result: ToM-informed alignment improved response quality, with win rates of 67% (3B) and 63% (8B).

Conclusion: ToM-driven strategies can enhance alignment in LLM-based conversational agents.

Abstract: Natural language interaction with agentic Artificial Intelligence (AI),
driven by Large Language Models (LLMs), is expected to remain a dominant
paradigm in the near future. While humans instinctively align their
communication with mental states -- an ability known as Theory of Mind (ToM),
current LLM powered systems exhibit significant limitations in this regard.
This study examines the extent to which open source language models (LLaMA) can
capture and preserve ToM related information and how effectively it contributes
to consistent ToM reasoning in generated responses. We further investigate
whether explicit manipulation of ToM related components, such as beliefs,
desires, and intentions, can enhance response alignment. Experiments on two
LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves
response quality, achieving win rates of 67 and 63 percent for the 3B and 8B
models, respectively. These findings highlight the potential of ToM driven
strategies to improve alignment in LLM based conversational agents.

</details>


### [187] [SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/pdf/2502.16747)
*Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong*

Main category: cs.CL

TL;DR: SQLong is a data augmentation framework that enhances LLM performance in NL2SQL tasks for large database schemas by generating synthetic data.


<details>
  <summary>Details</summary>
Motivation: Open-weight LLMs struggle with large database schemas due to increased context length, limiting their effectiveness in NL2SQL tasks.

Method: SQLong extends database schemas with synthetic CREATE TABLE commands and data rows, simulating long-context scenarios for finetuning.

Result: LLMs finetuned with SQLong-augmented data outperform those on standard datasets in experiments on Spider and BIRD datasets.

Conclusion: SQLong improves NL2SQL capabilities for complex database schemas, demonstrating practical impact in real-world settings.

Abstract: Open-weight large language models (LLMs) have significantly advanced
performance in the Natural Language to SQL (NL2SQL) task. However, their
effectiveness diminishes when dealing with large database schemas, as the
context length increases. To address this limitation, we present SQLong, a
novel and efficient data augmentation framework designed to enhance LLM
performance in long-context scenarios for the NL2SQL task. SQLong generates
augmented datasets by extending existing database schemas with additional
synthetic CREATE TABLE commands and corresponding data rows, sampled from
diverse schemas in the training data. This approach effectively simulates
long-context scenarios during finetuning and evaluation. Through experiments on
the Spider and BIRD datasets, we demonstrate that LLMs finetuned with
SQLong-augmented data significantly outperform those trained on standard
datasets. These imply SQLong's practical implementation and its impact on
improving NL2SQL capabilities in real-world settings with complex database
schemas.

</details>


### [188] [Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare](https://arxiv.org/pdf/2502.16051)
*Max Lamparth, Declan Grabb, Amy Franks, Scott Gershan, Kaitlyn N. Kunstman, Aaron Lulla, Monika Drummond Roots, Manu Sharma, Aryan Shrivastava, Nina Vasan, Colleen Waickman*

Main category: cs.CL

TL;DR: A new expert-created dataset addresses the limitations of current medical LM benchmarks by focusing on nuanced clinical reasoning in mental healthcare, covering five domains and evaluating LMs on accuracy, demographic impact, and response consistency.


<details>
  <summary>Details</summary>
Motivation: Current LM benchmarks oversimplify clinical practice, relying on board exam questions. This work aims to capture the complexities and ambiguities of real-world mental healthcare decision-making.

Method: An expert-annotated dataset was created without LM assistance, spanning five domains (treatment, diagnosis, etc.), with demographic variables standardized. Preference datasets were added for ambiguous cases.

Result: The dataset was used to evaluate 15 LMs, assessing accuracy, demographic influence, and consistency with human annotations.

Conclusion: The dataset fills a gap in LM evaluation by reflecting real-world clinical complexities, demonstrating its utility for improving LM performance in mental healthcare.

Abstract: Current medical language model (LM) benchmarks often over-simplify the
complexities of day-to-day clinical practice tasks and instead rely on
evaluating LMs on multiple-choice board exam questions. Thus, we present an
expert-created and annotated dataset spanning five critical domains of
decision-making in mental healthcare: treatment, diagnosis, documentation,
monitoring, and triage. This dataset - created without any LM assistance - is
designed to capture the nuanced clinical reasoning and daily ambiguities mental
health practitioners encounter, reflecting the inherent complexities of care
delivery that are missing from existing datasets. Almost all 203 base questions
with five answer options each have had the decision-irrelevant demographic
patient information removed and replaced with variables (e.g., AGE), and are
available for male, female, or non-binary-coded patients. For question
categories dealing with ambiguity and multiple valid answer options, we create
a preference dataset with uncertainties from the expert annotations. We outline
a series of intended use cases and demonstrate the usability of our dataset by
evaluating eleven off-the-shelf and four mental health fine-tuned LMs on
category-specific task accuracy, on the impact of patient demographic
information on decision-making, and how consistently free-form responses
deviate from human annotated samples.

</details>


### [189] [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/pdf/2502.16901)
*Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh*

Main category: cs.CL

TL;DR: Cross-lingual backdoor attacks (X-BAT) in multilingual LLMs show vulnerabilities where backdoors in one language transfer to others via shared embeddings, using toxicity classification as an example.


<details>
  <summary>Details</summary>
Motivation: To expose how attackers can exploit shared embedding spaces in multilingual models to insert backdoors that affect multiple languages.

Method: Demonstrated using toxicity classification, poisoning data in one language with rare/high-occurring tokens as triggers.

Result: Backdoors inserted in one language automatically transfer to others, revealing a critical vulnerability in mLLMs.

Conclusion: The study highlights a hidden backdoor effect in multilingual models, urging improved security measures.

Abstract: We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT)
in multilingual Large Language Models (mLLMs), revealing how backdoors inserted
in one language can automatically transfer to others through shared embedding
spaces. Using toxicity classification as a case study, we demonstrate that
attackers can compromise multilingual systems by poisoning data in a single
language, with rare and high-occurring tokens serving as specific, effective
triggers. Our findings expose a critical vulnerability that influences the
model's architecture, resulting in a concealed backdoor effect during the
information flow. Our code and data are publicly available
https://github.com/himanshubeniwal/X-BAT.

</details>


### [190] [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/pdf/2502.16894)
*Chenghao Fan, Zhenyi Lu, Sichen Liu, Chengfeng Gu, Xiaoye Qu, Wei Wei, Yu Cheng*

Main category: cs.CL

TL;DR: GOAT improves LoRA MoE by adaptive SVD-structured priors and theoretical scaling, matching Full FT performance.


<details>
  <summary>Details</summary>
Motivation: LoRA underperforms Full FT due to suboptimal SVD initialization and MoE weight misalignment.

Method: GOAT integrates SVD-structured MoE and aligns optimization with Full FT via a scaling factor.

Result: GOAT achieves state-of-the-art performance across 25 diverse datasets.

Conclusion: GOAT bridges the gap between LoRA and Full FT without altering architecture or training.

Abstract: While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for
Large Language Models (LLMs), its performance often falls short of Full
Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with
static singular value decomposition (SVD) subsets, leading to suboptimal
leveraging of pre-trained knowledge. Another path for improving LoRA is
incorporating a Mixture-of-Experts (MoE) architecture. However, weight
misalignment and complex gradient dynamics make it challenging to adopt SVD
prior to the LoRA MoE architecture. To mitigate these issues, we propose
\underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t}
(GOAT), a framework that (1) adaptively integrates relevant priors using an
SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by
deriving a theoretical scaling factor. We demonstrate that proper scaling,
without modifying the architecture or training algorithms, boosts LoRA MoE's
efficiency and performance. Experiments across 25 datasets, including natural
language understanding, commonsense reasoning, image classification, and
natural language generation, demonstrate GOAT's state-of-the-art performance,
closing the gap with Full FT.

</details>


### [191] [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/pdf/2502.19982)
*Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper investigates knowledge forgetting in large language models, focusing on generalizing unlearning to implicit knowledge. It introduces UGBench for evaluation and proposes PerMU, a novel unlearning method, showing significant improvements.


<details>
  <summary>Details</summary>
Motivation: To ensure models forget not just specific training samples but also related implicit knowledge, addressing gaps in current unlearning methods.

Method: Proposes PerMU, a probability perturbation-based unlearning paradigm, and introduces UGBench to evaluate unlearning of implicit knowledge across 13 methods.

Result: PerMU improves unlearning of target data by 50.40% and implicit knowledge forgetting by 40.73%.

Conclusion: PerMU advances generalized implicit knowledge forgetting, with UGBench providing a robust evaluation framework.

Abstract: In this paper, we investigate knowledge forgetting in large language models
with a focus on its generalisation--ensuring that models forget not only
specific training samples but also related implicit knowledge. To this end, we
begin by identifying a broader unlearning scope that includes both target data
and logically associated samples, including rephrased, subject-replaced,
one-hop reasoned, and relation-reversed data. To rigorously evaluate
generalisation, we introduce UGBench, the first comprehensive benchmark
specifically designed to assess the unlearning of in-scope implicit knowledge
covering 13 state-of-the-art methods across three datasets. UGBench reveals
that unlearned models can still recall paraphrased answers and retain target
facts in intermediate layers. This motivates us to take a preliminary step
toward more generalised implicit knowledge forgetting by proposing PerMU, a
novel probability perturbation-based unlearning paradigm. PerMU simulates
adversarial unlearning samples to eliminate fact-related tokens from the logit
distribution, collectively reducing the probabilities of all answer-associated
tokens. Experiments are conducted on a diverse range of datasets, including
TOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B
in scale. The results demonstrate that PerMU delivers up to a 50.40%
improvement in unlearning vanilla target data while maintaining a 40.73% boost
in forgetting implicit knowledge. Our code can be found in
https://github.com/MaybeLizzy/UGBench.

</details>


### [192] [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/pdf/2503.09579)
*Yingfa Chen, Yutong Wu, Chenyang Song, Zhen Leng Thai, Xingyu Shen, Xu Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: Grouped-Query Attention (GQA) configurations are often suboptimal for long-context scenarios. This work introduces flexible control over attention FLOPs and joint optimization of model size and GQA, reducing memory and FLOPs by over 50% without performance loss.


<details>
  <summary>Details</summary>
Motivation: Current GQA configurations overlook context length's impact on inference cost, leading to inefficiency in long-context scenarios.

Method: Decoupling head size from hidden size for flexible FLOPs control and jointly optimizing model size and GQA configuration.

Result: Optimal GQA configurations reduce memory and FLOPs by >50% vs. Llama-3's GQA, with no performance loss.

Conclusion: The proposed recipe offers efficient GQA configurations for long-context LLMs, improving resource allocation.

Abstract: Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the
computational cost of attention layers in large language models (LLMs).
However, current GQA configurations are often suboptimal because they overlook
how context length influences inference cost. Since inference cost grows with
context length, the most cost-efficient GQA configuration should also vary
accordingly. In this work, we analyze the relationship among context length,
model size, GQA configuration, and model loss, and introduce two innovations:
(1) we decouple the total head size from the hidden size, enabling more
flexible control over attention FLOPs; and (2) we jointly optimize the model
size and the GQA configuration to arrive at a better allocation of inference
resources between attention layers and other components. Our analysis reveals
that commonly used GQA configurations are highly suboptimal for long-context
scenarios. More importantly, we propose a recipe for deriving cost-optimal GQA
configurations. Our results show that for long-context scenarios, one should
use fewer attention heads while scaling up model size. Configurations selected
by our recipe can reduce both memory usage and FLOPs by more than 50% compared
to Llama-3's GQA, with *no degradation in model capabilities*. Our findings
offer valuable insights for designing efficient long-context LLMs. The code is
available at https://www.github.com/THUNLP/cost-optimal-gqa .

</details>


### [193] [Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing](https://arxiv.org/pdf/2502.20592)
*Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Chenyu You, Shafiq Joty, Giuseppe Carenini*

Main category: cs.CL

TL;DR: A novel framework using test-time scaling and prompt ensembles improves Multi-Document Summarization (MDS) by generating and refining multiple candidate summaries, with new LLM-based metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored application of test-time scaling in natural language generation, specifically MDS, which requires nuanced prompt design and ensemble methods due to diverse summarization needs.

Method: Proposes a framework combining prompt ensembles for generating multiple summaries and an aggregator to refine them, introducing two new LLM-based metrics (CAP and LLM-ACU) for evaluation.

Result: The framework significantly enhances summary quality and identifies practical scaling boundaries for MDS tasks.

Conclusion: Test-time scaling and prompt ensembles effectively improve MDS, with new metrics providing better evaluation, though scaling limits exist.

Abstract: Recent advances in test-time scaling have shown promising results in
improving Large Language Model (LLM) performance through strategic computation
allocation during inference. While this approach has demonstrated strong
improvements in logical and mathematical reasoning tasks, its application to
natural language generation (NLG), particularly summarization, remains
unexplored. Multi-Document Summarization (MDS), a fundamental task in NLG,
presents unique challenges by requiring models to extract and synthesize
essential information across multiple lengthy documents. Unlike reasoning
tasks, MDS demands a more nuanced approach to prompt design and ensemble
methods, as no single "best" prompt can satisfy diverse summarization
requirements. We propose a novel framework leveraging test-time scaling for
MDS. Our approach employs prompt ensemble techniques to generate multiple
candidate summaries using various prompts, then combines them with an
aggregator to produce a refined summary. To evaluate our method effectively, we
also introduce two new LLM-based metrics: the Consistency-Aware Preference
(CAP) score and LLM Atom-Content-Unit (LLM-ACU) score, which assess summary
quality while addressing the positional bias inherent in traditional automatic
evaluation. Our extensive experiments demonstrate that this framework
significantly enhances summary quality while also revealing the practical
scaling boundaries to MDS tasks.

</details>


### [194] [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/pdf/2502.21074)
*Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, Yulan He*

Main category: cs.CL

TL;DR: CODI introduces a training framework to compress natural language Chain-of-Thought reasoning into continuous space, matching explicit CoT performance while improving efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs by enabling efficient and robust reasoning in latent continuous space, overcoming limitations of prior implicit CoT methods.

Method: CODI jointly trains explicit (teacher) and implicit (student) CoT tasks, aligning hidden states to distill reasoning into continuous space.

Result: CODI matches explicit CoT performance on GSM8k, achieves 3.1x compression, and outperforms prior implicit CoT by 28.2% in accuracy.

Conclusion: LLMs can reason effectively in both natural language and latent continuous space, validated by CODI's performance and robustness.

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
encouraging step-by-step reasoning in natural language. However, leveraging a
latent continuous space for reasoning may offer benefits in terms of both
efficiency and robustness. Prior implicit CoT methods attempt to bypass
language completely by reasoning in continuous space but have consistently
underperformed compared to the standard explicit CoT approach. We introduce
CODI (Continuous Chain-of-Thought via Self-Distillation), a novel training
framework that effectively compresses natural language CoT into continuous
space. CODI jointly trains a teacher task (Explicit CoT) and a student task
(Implicit CoT), distilling the reasoning ability from language into continuous
space by aligning the hidden states of a designated token. Our experiments show
that CODI is the first implicit CoT approach to match the performance of
explicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and
outperforming the previous state-of-the-art by 28.2% in accuracy. CODI also
demonstrates robustness, generalizable to complex datasets, and
interpretability. These results validate that LLMs can reason effectively not
only in natural language, but also in a latent continuous space.

</details>


### [195] [RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs](https://arxiv.org/pdf/2503.10657)
*Zhongzhan Huang, Guoming Ling, Yupei Lin, Yandong Chen, Shanshan Zhong, Hefeng Wu, Liang Lin*

Main category: cs.CL

TL;DR: The paper introduces RouterEval, a benchmark for evaluating routers in Routing LLMs, and demonstrates that a capable router can outperform the best single LLM as the pool size grows.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive benchmarks for Routing LLMs hinders router development, prompting the creation of RouterEval.

Method: The study analyzes over 8,500 LLMs and introduces RouterEval, a benchmark with 200M+ performance records across 12 evaluations.

Result: A capable router improves performance as the pool size increases, surpassing even the best single LLM. Existing methods still need improvement.

Conclusion: RouterEval is a valuable resource for advancing router research in Routing LLMs, with significant potential for performance gains.

Abstract: Routing large language models (LLMs) is a new paradigm that uses a router to
recommend the best LLM from a pool of candidates for a given input. In this
paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel
model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can
significantly enhance the performance of this paradigm as the number of
candidates increases. This improvement can even surpass the performance of the
best single model in the pool and many existing strong LLMs, confirming it a
highly promising paradigm. However, the lack of comprehensive and open-source
benchmarks for Routing LLMs has hindered the development of routers. In this
paper, we introduce RouterEval, a benchmark tailored for router research, which
includes over 200,000,000 performance records for 12 popular LLM evaluations
across various areas such as commonsense reasoning, semantic understanding,
etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations
of existing Routing LLM methods reveal that most still have significant room
for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,
code and tutorial.

</details>


### [196] [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/pdf/2503.01513)
*Katerina Korre, Dimitris Tsirmpas, Nikos Gkoumas, Emma Cabalé, Danai Myrtzani, Theodoros Evgeniou, Ion Androutsopoulos, John Pavlopoulos*

Main category: cs.CL

TL;DR: A survey on using LLMs to improve online discussion quality, covering evaluation taxonomies, intervention strategies, datasets, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Online discussions often degrade into harmful exchanges like hate speech, threatening social cohesion. LLMs offer potential to enhance discussion quality.

Method: Synthesizes NLP and Social Sciences to create taxonomies for evaluation and facilitation, and outlines LLM-based strategies.

Result: Provides new taxonomies for discussion quality, intervention strategies, datasets, and a roadmap for future research.

Conclusion: LLMs can significantly improve online discourse quality, with potential for both technological and societal impact.

Abstract: We present a survey of methods for assessing and enhancing the quality of
online discussions, focusing on the potential of LLMs. While online discourses
aim, at least in theory, to foster mutual understanding, they often devolve
into harmful exchanges, such as hate speech, threatening social cohesion and
democratic values. Recent advancements in LLMs enable artificial facilitation
agents to not only moderate content, but also actively improve the quality of
interactions. Our survey synthesizes ideas from NLP and Social Sciences to
provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of
intervention and facilitation strategies, (c) along with a new taxonomy of
conversation facilitation datasets, (d) an LLM-oriented roadmap of good
practices and future research directions, from technological and societal
perspectives.

</details>


### [197] [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/pdf/2503.12908)
*Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong*

Main category: cs.CL

TL;DR: HICD is a new method to reduce hallucinations in LLMs by inducing controlled hallucinations for contrastive decoding, improving contextual faithfulness and factuality.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate inaccurate or incorrect outputs (hallucinations). Existing contrastive decoding methods lack effectiveness in mitigating these issues.

Method: HICD selects key attention heads (inducing heads), disperses their attention to induce hallucinations, and compares these with original outputs for contrastive decoding.

Result: HICD improves performance in tasks like context completion, reading comprehension, and question answering, and enhances factuality in knowledge recall tasks.

Conclusion: HICD offers a promising strategy to reduce hallucinations by controlled induction, boosting LLM performance across various tasks.

Abstract: Large Language Models (LLMs) often generate hallucinations, producing outputs
that are contextually inaccurate or factually incorrect. We introduce HICD, a
novel method designed to induce hallucinations for contrastive decoding to
mitigate hallucinations. Unlike existing contrastive decoding methods, HICD
selects attention heads crucial to the model's prediction as inducing heads,
then induces hallucinations by dispersing attention of these inducing heads and
compares the hallucinated outputs with the original outputs to obtain the final
result. Our approach significantly improves performance on tasks requiring
contextual faithfulness, such as context completion, reading comprehension, and
question answering. It also improves factuality in tasks requiring accurate
knowledge recall. We demonstrate that our inducing heads selection and
attention dispersion method leads to more "contrast-effective" hallucinations
for contrastive decoding, outperforming other hallucination-inducing methods.
Our findings provide a promising strategy for reducing hallucinations by
inducing hallucinations in a controlled manner, enhancing the performance of
LLMs in a wide range of tasks.

</details>


### [198] [MCiteBench: A Multimodal Benchmark for Generating Text with Citations](https://arxiv.org/pdf/2503.02589)
*Caiyu Hu, Yikai Zhang, Tinghui Zhu, Yiwei Ye, Yanghua Xiao*

Main category: cs.CL

TL;DR: MCiteBench is introduced as the first benchmark to evaluate MLLMs' ability to generate text with citations in multimodal contexts, revealing challenges like modality bias and unreliable grounding.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating MLLMs' citation generation for multimodal content, as existing work focuses on text-only scenarios.

Method: Develop MCiteBench using academic papers and review-rebuttal data, then test MLLMs' performance in generating citations for multimodal input.

Result: MLLMs struggle with reliable grounding and exhibit modality bias, with insights into their citation generation behavior.

Conclusion: MCiteBench highlights challenges in multimodal citation tasks, guiding future research to improve MLLMs' reliability and transparency.

Abstract: Multimodal Large Language Models (MLLMs) have advanced in integrating diverse
modalities but frequently suffer from hallucination. A promising solution to
mitigate this issue is to generate text with citations, providing a transparent
chain for verification. However, existing work primarily focuses on generating
citations for text-only content, leaving the challenges of multimodal scenarios
largely unexplored. In this paper, we introduce MCiteBench, the first benchmark
designed to assess the ability of MLLMs to generate text with citations in
multimodal contexts. Our benchmark comprises data derived from academic papers
and review-rebuttal interactions, featuring diverse information sources and
multimodal content. Experimental results reveal that MLLMs struggle to ground
their outputs reliably when handling multimodal input. Further analysis
uncovers a systematic modality bias and reveals how models internally rely on
different sources when generating citations, offering insights into model
behavior and guiding future directions for multimodal citation tasks.

</details>


### [199] [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/pdf/2503.04372)
*Orfeas Menis Mastromichalakis, Giorgos Filandrianos, Maria Symeonaki, Giorgos Stamou*

Main category: cs.CL

TL;DR: GRAPE is a probability-based metric for evaluating gender bias in MT systems, tested with the GAMBIT-MT dataset in Greek and French.


<details>
  <summary>Details</summary>
Motivation: Addressing gender bias in MT systems when translating ambiguous occupational terms, as traditional evaluation methods fail to capture systematic biases.

Method: Introduces GRAPE, a metric analyzing aggregated model responses, and GAMBIT-MT, a benchmark dataset for gender-ambiguous terms.

Result: Evaluated MT systems show gendered translations that may align with or diverge from societal stereotypes and real-world gender distributions.

Conclusion: GRAPE provides a nuanced way to assess gender bias in MT, highlighting the need for more equitable translation systems.

Abstract: Machine Translation (MT) systems frequently encounter gender-ambiguous
occupational terms, where they must assign gender without explicit contextual
cues. While individual translations in such cases may not be inherently biased,
systematic patterns-such as consistently translating certain professions with
specific genders-can emerge, reflecting and perpetuating societal stereotypes.
This ambiguity challenges traditional instance-level single-answer evaluation
approaches, as no single gold standard translation exists. To address this, we
introduce GRAPE, a probability-based metric designed to evaluate gender bias by
analyzing aggregated model responses. Alongside this, we present GAMBIT-MT, a
benchmarking dataset in English with gender-ambiguous occupational terms. Using
GRAPE, we evaluate several MT systems and examine whether their gendered
translations in Greek and French align with or diverge from societal
stereotypes, real-world occupational gender distributions, and normative
standards.

</details>


### [200] [MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection](https://arxiv.org/pdf/2503.18132)
*Yibo Yan, Shen Wang, Jiahao Huo, Philip S. Yu, Xuming Hu, Qingsong Wen*

Main category: cs.CL

TL;DR: MathAgent, a Mixture-of-Math-Agent framework, improves error detection in multimodal math education by 5% in accuracy and 3% in categorization, achieving high student satisfaction and cost savings.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with nuanced error detection in multimodal math contexts, necessitating a specialized solution.

Method: MathAgent uses three specialized agents: image-text consistency validator, visual semantic interpreter, and integrative error analyzer.

Result: 5% higher accuracy in error step identification, 3% improvement in categorization, and 90% student satisfaction in deployment.

Conclusion: MathAgent effectively addresses multimodal math error detection, demonstrating practical benefits in education.

Abstract: Mathematical error detection in educational settings presents a significant
challenge for Multimodal Large Language Models (MLLMs), requiring a
sophisticated understanding of both visual and textual mathematical content
along with complex reasoning capabilities. Though effective in mathematical
problem-solving, MLLMs often struggle with the nuanced task of identifying and
categorizing student errors in multimodal mathematical contexts. Therefore, we
introduce MathAgent, a novel Mixture-of-Math-Agent framework designed
specifically to address these challenges. Our approach decomposes error
detection into three phases, each handled by a specialized agent: an image-text
consistency validator, a visual semantic interpreter, and an integrative error
analyzer. This architecture enables more accurate processing of mathematical
content by explicitly modeling relationships between multimodal problems and
student solution steps. We evaluate MathAgent on real-world educational data,
demonstrating approximately 5% higher accuracy in error step identification and
3% improvement in error categorization compared to baseline models. Besides,
MathAgent has been successfully deployed in an educational platform that has
served over one million K-12 students, achieving nearly 90% student
satisfaction while generating significant cost savings by reducing manual error
detection.

</details>


### [201] [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/pdf/2504.02438)
*Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan*

Main category: cs.CL

TL;DR: ViLAMP introduces differential distillation for efficient long-form video processing, preserving task-relevant info while reducing redundancy, achieving state-of-the-art performance on long videos.


<details>
  <summary>Details</summary>
Motivation: High computational costs and loss of critical temporal or semantic info in existing methods for long-form video processing.

Method: Differential distillation with keyframe selection and feature merging, implemented in ViLAMP for hierarchical video-language modeling.

Result: Superior performance on five benchmarks, handling ultra-long videos (10K frames) efficiently on a single GPU.

Conclusion: ViLAMP offers a scalable, efficient solution for long-form video understanding without sacrificing performance.

Abstract: Long-form video processing fundamentally challenges vision-language models
(VLMs) due to the high computational costs of handling extended temporal
sequences. Existing token pruning and feature merging methods often sacrifice
critical temporal dependencies or dilute semantic information. We introduce
differential distillation, a principled approach that systematically preserves
task-relevant information while suppressing redundancy. Based on this
principle, we develop ViLAMP, a hierarchical video-language model that
processes hour-long videos at "mixed precision" through two key mechanisms: (1)
differential keyframe selection that maximizes query relevance while
maintaining temporal distinctiveness at the frame level and (2) differential
feature merging that preserves query-salient features in non-keyframes at the
patch level. Hence, ViLAMP retains full information in keyframes while reducing
non-keyframes to their most salient features, resembling mixed-precision
training. Extensive experiments demonstrate ViLAMP's superior performance
across five video understanding benchmarks, particularly on long-form content.
Notably, ViLAMP can process ultra-long videos (up to 10K frames) on a single
NVIDIA A100 GPU, achieving substantial computational efficiency while
maintaining state-of-the-art performance. Code and model are available at
https://github.com/steven-ccq/ViLAMP.

</details>


### [202] [Leveraging Robust Optimization for LLM Alignment under Distribution Shifts](https://arxiv.org/pdf/2504.05831)
*Mingye Zhu, Yi Liu, Zheren Fu, Yongdong Zhang, Zhendong Mao*

Main category: cs.CL

TL;DR: A novel distribution-aware optimization framework improves preference alignment in LLMs by mitigating distribution shifts from synthetic data.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of distribution shifts in synthetic data used for preference alignment, which undermines nuanced human preference representation.

Method: Uses classifiers to assign calibration values to training samples, then optimizes a robust objective focusing on human-preferred data regions.

Result: Mitigates distributional mismatch and enhances generation of responses aligned with human values.

Conclusion: The framework effectively improves preference alignment in LLMs by targeting the human-preferred distribution.

Abstract: Preference alignment methods are increasingly critical for steering large
language models (LLMs) to generate outputs consistent with human values. While
recent approaches often rely on synthetic data generated by LLMs for
scalability and cost-efficiency reasons, this reliance can introduce
distribution shifts that undermine the nuanced representation of human
preferences needed for desirable outputs. In this paper, we propose a novel
distribution-aware optimization framework that improves preference alignment
despite such shifts. Our approach first leverages well-learned classifiers to
assign a calibration value to each training sample, quantifying its alignment
with the target human-preferred distribution. These values are then
incorporated into a robust optimization objective that minimizes the worst-case
loss over regions of the data space most relevant to human preferences. By
explicitly focusing optimization on the target distribution, our approach
mitigates the impact of distributional mismatch and improves the generation of
responses that better reflect intended values.

</details>


### [203] [Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models](https://arxiv.org/pdf/2504.08399)
*Yin Jou Huang, Rafik Hadfi*

Main category: cs.CL

TL;DR: The paper proposes a multi-observer framework for assessing LLM personality traits, replacing self-reports with observer-based evaluations for better accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Self-reports for LLM personality assessments are limited by biases and lack of behavioral nuance, prompting the need for a more reliable method.

Method: Uses multiple observer agents with specific relational contexts to evaluate LLM behavior across Big Five traits via dialogue.

Result: Observer ratings align better with human judgments, reduce biases, and achieve optimal reliability with 5-7 observers.

Conclusion: A multi-observer approach provides a more reliable and context-sensitive method for assessing LLM personality traits.

Abstract: Self-report questionnaires have long been used to assess LLM personality
traits, yet they fail to capture behavioral nuances due to biases and
meta-knowledge contamination. This paper proposes a novel multi-observer
framework for personality trait assessments in LLM agents that draws on
informant-report methods in psychology. Instead of relying on self-assessments,
we employ multiple observer agents. Each observer is configured with a specific
relational context (e.g., family member, friend, or coworker) and engages the
subject LLM in dialogue before evaluating its behavior across the Big Five
dimensions. We show that these observer-report ratings align more closely with
human judgments than traditional self-reports and reveal systematic biases in
LLM self-assessments. We also found that aggregating responses from 5 to 7
observers reduces systematic biases and achieves optimal reliability. Our
results highlight the role of relationship context in perceiving personality
and demonstrate that a multi-observer paradigm offers a more reliable,
context-sensitive approach to evaluating LLM personality traits.

</details>


### [204] [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/pdf/2504.10368)
*Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu*

Main category: cs.CL

TL;DR: S1-Bench is a new benchmark to evaluate Large Reasoning Models (LRMs) on simple tasks favoring intuitive system 1 thinking, revealing their inefficiency and inaccuracy in such tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus on complex reasoning (system 2), leaving a gap for evaluating LRMs' system 1 thinking capabilities.

Method: S1-Bench introduces simple, diverse, and natural questions across domains and languages to assess LRMs' system 1 performance.

Result: Evaluations of 28 LRMs show inefficiency, low accuracy, and limited robustness in handling simple questions, with a gap between perceived difficulty and response length.

Conclusion: The work highlights the need for dual-system compatibility in LRM development, addressing both system 1 and system 2 thinking.

Abstract: We introduce S1-Bench, a novel benchmark designed to evaluate the performance
of Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1
thinking rather than deliberative system 2 reasoning. While LRMs have achieved
significant breakthroughs in complex reasoning tasks through explicit chains of
thought, their heavy reliance on system 2 thinking may limit their system 1
thinking capabilities. However, there is a lack of an appropriate benchmark for
evaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench
introduces a suite of simple, diverse, and natural questions across multiple
domains and languages, specifically designed to assess LRMs' performance on
questions more suitable for system 1 . We conduct extensive evaluations across
28 LRMs, revealing their inefficiency, inadequate accuracy, and limited
robustness when handling simple questions. Additionally, we observe a gap
between their difficulty perception and generation length. Overall, this work
paves the way toward dual-system compatibility in the development of LRMs.

</details>


### [205] [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/pdf/2504.12324)
*Mengying Yuan, Wenhao Wang, Zixuan Wang, Yujie Huang, Kangli Wei, Fei Li, Chong Teng, Donghong Ji*

Main category: cs.CL

TL;DR: The paper introduces Cross-Document Cross-Lingual NLI (CDCL-NLI), a novel task extending NLI to multilingual, multi-document scenarios. It presents a dataset, a method combining RST-enhanced graph fusion and interpretability-aware prediction, and demonstrates superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored area of CDCL-NLI and extend NLI capabilities to handle multi-document, multilingual contexts.

Method: Integrates RST-enhanced graph fusion with interpretability-aware prediction, using heterogeneous graph neural networks for context modeling and lexical chains for cross-lingual alignment.

Result: Achieves significant improvements over conventional NLI models and large language models, supported by extensive experiments.

Conclusion: The work advances NLI research, highlighting potential in cross-document cross-lingual understanding, hallucination elimination, and interpretability. Resources are made available for peer review.

Abstract: Natural Language Inference (NLI) is a fundamental task in natural language
processing. While NLI has developed many sub-directions such as sentence-level
NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI
(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel
paradigm: CDCL-NLI, which extends traditional NLI capabilities to
multi-document, multilingual scenarios. To support this task, we construct a
high-quality CDCL-NLI dataset including 25,410 instances and spanning 26
languages. To address the limitations of previous methods on CDCL-NLI task, we
further propose an innovative method that integrates RST-enhanced graph fusion
with interpretability-aware prediction. Our approach leverages RST (Rhetorical
Structure Theory) within heterogeneous graph neural networks for cross-document
context modeling, and employs a structure-aware semantic alignment based on
lexical chains for cross-lingual understanding. For NLI interpretability, we
develop an EDU (Elementary Discourse Unit)-level attribution framework that
produces extractive explanations. Extensive experiments demonstrate our
approach's superior performance, achieving significant improvements over both
conventional NLI models as well as large language models. Our work sheds light
on the study of NLI and will bring research interest on cross-document
cross-lingual context understanding, hallucination elimination and
interpretability inference. Our code and datasets are available at
\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer
review.

</details>


### [206] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/pdf/2504.14150)
*Katie Matton, Robert Osazuwa Ness, John Guttag, Emre Kıcıman*

Main category: cs.CL

TL;DR: The paper introduces a method to measure the faithfulness of explanations generated by large language models (LLMs), addressing their potential to misrepresent reasoning and cause misuse.


<details>
  <summary>Details</summary>
Motivation: LLM explanations can be unfaithful, leading to over-trust and misuse. The paper aims to rigorously define and measure faithfulness to mitigate this issue.

Method: The approach involves defining faithfulness in terms of concept influence and using an auxiliary LLM to create counterfactuals, combined with a Bayesian hierarchical model to quantify causal effects.

Result: The method successfully quantifies and identifies patterns of unfaithfulness, revealing cases where explanations hide social bias or mislead about evidence influence.

Conclusion: The proposed method provides a robust way to assess LLM explanation faithfulness, uncovering interpretable patterns of unfaithfulness in tasks like social bias and medical QA.

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>


### [207] [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/pdf/2504.15241)
*Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee*

Main category: cs.CL

TL;DR: The paper introduces MrGuard, a multilingual guardrail for LLMs to detect and filter unsafe content across languages, outperforming baselines by over 15%.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to adversarial attacks like jailbreaking, especially in multilingual settings due to limited safety-aligned data. A robust guardrail is needed for real-world deployment.

Method: The approach includes synthetic multilingual data generation, supervised fine-tuning, and a curriculum-based GRPO framework for improved performance.

Result: MrGuard consistently outperforms baselines by over 15% and handles multilingual variations like code-switching and low-resource languages effectively.

Conclusion: MrGuard provides a robust solution for multilingual content moderation, with reasoning capabilities to explain language-specific risks.

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual settings, where multilingual safety-aligned data
is often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we introduce a multilingual
guardrail with reasoning for prompt classification. Our method consists of: (1)
synthetic multilingual data generation incorporating culturally and
linguistically nuanced variants, (2) supervised fine-tuning, and (3) a
curriculum-based Group Relative Policy Optimization (GRPO) framework that
further improves performance. Experimental results demonstrate that our
multilingual guardrail, MrGuard, consistently outperforms recent baselines
across both in-domain and out-of-domain languages by more than 15%. We also
evaluate MrGuard's robustness to multilingual variations, such as
code-switching and low-resource language distractors in the prompt, and
demonstrate that it preserves safety judgments under these challenging
conditions. The multilingual reasoning capability of our guardrail enables it
to generate explanations, which are particularly useful for understanding
language-specific risks and ambiguities in multilingual content moderation.

</details>


### [208] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/pdf/2504.20371)
*Zhibo Man, Yuanmeng Chen, Yujie Zhang, Jinan Xu*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' disambiguation ability in multi-domain translation (MDT) using a framework (DMDTEval) with annotated test sets, diverse prompts, and precise metrics, revealing key findings.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly in MDT due to word ambiguity across domains, necessitating evaluation of their disambiguation ability.

Method: The study constructs a multi-domain annotated test set, curates disambiguation prompts, and designs metrics to evaluate LLMs across 4 language pairs and 13 domains.

Result: Experiments reveal crucial findings about LLMs' disambiguation efficacy, aiding future research.

Conclusion: The framework and findings advance research on improving LLMs' disambiguation in MDT.

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory, the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT, remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompt strategies, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompt strategies on multiple
state-of-the-art LLMs. We conduct comprehensive experiments across 4 language
pairs and 13 domains, our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>


### [209] [HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation](https://arxiv.org/pdf/2505.00038)
*Cristina Garbacea, Chenhao Tan*

Main category: cs.CL

TL;DR: HyPerAlign is a hypothesis-driven method for personalizing LLM outputs to individual users, outperforming preference-based fine-tuning in authorship attribution and deliberative alignment tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods aggregate diverse user preferences, resulting in generic outputs. Personalized responses tailored to individual users are needed for better usability.

Method: HyPerAlign infers user-specific hypotheses (communication strategies, personality, writing style) from few-shot examples and prompts LLMs with these to generate customized outputs.

Result: HyPerAlign achieves high win-rates (>90%) in authorship attribution and improves helpfulness by up to 70% in deliberative alignment compared to preference-based methods.

Conclusion: HyPerAlign offers an interpretable and sample-efficient approach for LLM personalization, enhancing user-specific alignment.

Abstract: Alignment algorithms are widely used to align large language models (LLMs) to
human users based on preference annotations. Typically these (often divergent)
preferences are aggregated over a diverse set of users, resulting in fine-tuned
models that are aligned to the ``average-user'' preference. Nevertheless,
current models are used by individual users in very specific contexts and
situations, emphasizing the need for user-dependent preference control. In this
work we address the problem of personalizing LLM outputs to their users. We aim
to generate customized responses tailored to specific individuals instead of
generic outputs that emulate the collective voices of diverse populations. We
propose HyPerAlign, an interpretable and sample-efficient hypothesis-driven
personalization approach for LLM models. Given few-shot examples written by a
particular user, we first infer hypotheses about their communication
strategies, personality, and writing style, then prompt LLM models with these
hypotheses and user-specific attributes to generate customized outputs. We
conduct experiments on two different personalization tasks, namely authorship
attribution and deliberative alignment, with datasets from diverse domains
(news articles, blog posts, emails, jailbreaking benchmarks). Results
demonstrate the superiority of hypothesis-driven LLM personalization compared
to preference-based fine-tuning methods. For authorship attribution, HyPerAlign
generations have consistently high win-rates (commonly $> 90\%$) against
state-of-the-art preference fine-tuning approaches across diverse user profiles
and LLM models. For deliberative alignment, the helpfulness of LLM models is
improved by up to $70\%$ on average. Overall, HyPerAlign represents an
interpretable and sample-efficient strategy for the personalization of LLM
models to individual users.

</details>


### [210] [Adaptive Thinking via Mode Policy Optimization for Social Language Agents](https://arxiv.org/pdf/2505.02156)
*Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao*

Main category: cs.CL

TL;DR: The paper introduces AML, a framework for adaptive reasoning in social intelligence simulation, outperforming GPT-4o and GRPO.


<details>
  <summary>Details</summary>
Motivation: Current methods lack dynamic reasoning depth adjustment, leading to inefficiency and inflexibility in social simulations.

Method: Proposes AML with hierarchical thinking modes and AMPO algorithm for context-aware mode switching and reasoning optimization.

Result: AML achieves 15.6% higher performance than GPT-4o and 7.0% better than GRPO with shorter reasoning chains.

Conclusion: AML's adaptive thinking mode selection and optimization significantly improve social intelligence simulation.

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
studies. Existing methods either lack this kind of reasoning capability or
enforce Long Chain-of-Thought reasoning uniformly across all scenarios,
resulting in excessive token usage and inflexible social simulation. To address
this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning
($\textbf{AML}$) framework in this paper, aiming to improve the adaptive
thinking ability of language agents in dynamic social interactions. To this
end, we first identify hierarchical thinking modes ranging from intuitive
response to deep deliberation based on the cognitive control theory. We then
develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy
$\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the
context-aware mode switching and reasoning. Our framework advances existing
research in three key aspects: (1) Multi-granular thinking mode design, (2)
Context-aware mode switching across social interaction, and (3) Token-efficient
reasoning via depth-adaptive processing. Extensive experiments on social
intelligence benchmarks verify that AML achieves 15.6% higher task performance
than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter
reasoning chains, demonstrating the advantage of adaptive thinking mode
selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.

</details>


### [211] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/pdf/2505.00753)
*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu*

Main category: cs.CL

TL;DR: The paper surveys LLM-based human-agent systems (LLM-HAS), addressing challenges like reliability and safety in autonomous LLM agents by integrating human input. It provides a structured overview of core components, applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of fully autonomous LLM agents (e.g., hallucinations, safety risks) by incorporating human feedback and control, enhancing reliability and performance.

Method: A comprehensive survey of LLM-HAS, clarifying concepts, detailing core components (e.g., human feedback, interaction types), and exploring applications.

Result: A structured overview of LLM-HAS, highlighting its potential to improve agent performance and safety, along with challenges and opportunities.

Conclusion: The paper consolidates knowledge on LLM-HAS, aiming to inspire further research and innovation in this interdisciplinary field.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.

</details>


### [212] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/pdf/2505.09930)
*Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao*

Main category: cs.CL

TL;DR: MePO is a merit-guided, lightweight prompt optimizer that improves response quality across diverse tasks and models without relying on large-scale LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods rely on advanced LLMs, which can degrade performance for lightweight models due to verbose prompts.

Method: MePO uses model-agnostic prompt quality merits and a lightweight LLM to generate optimized prompts, avoiding reliance on large-scale LLMs.

Result: MePO achieves better results across tasks and models, reducing costs and privacy concerns.

Conclusion: MePO offers a scalable, robust solution for prompt optimization, suitable for real-world deployment.

Abstract: Prompt optimization (PO) provides a practical way to improve response quality
when users lack the time or expertise to manually craft effective prompts.
Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to
generate optimized prompts. However, due to limited downward compatibility,
verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight
inference models and degrade response quality. In this work, we rethink prompt
optimization through the lens of interpretable design. We first identify a set
of model-agnostic prompt quality merits and empirically validate their
effectiveness in enhancing prompt and response quality. We then introduce MePO,
a merit-guided, lightweight, and locally deployable prompt optimizer trained on
our preference dataset built from merit-aligned prompts generated by a
lightweight LLM. Unlike prior work, MePO avoids online optimization reliance,
reduces cost and privacy concerns, and, by learning clear, interpretable
merits, generalizes effectively to both large-scale and lightweight inference
models. Experiments demonstrate that MePO achieves better results across
diverse tasks and model types, offering a scalable and robust solution for
real-world deployment. The code and dataset can be found in
https://github.com/MidiyaZhu/MePO

</details>


### [213] [Designing and Contextualising Probes for African Languages](https://arxiv.org/pdf/2505.10081)
*Wisdom Aduah, Francois Meyer*

Main category: cs.CL

TL;DR: The paper investigates how linguistic knowledge is encoded in pretrained language models (PLMs) for African languages, finding that adapted PLMs outperform multilingual ones and that syntactic and semantic information is distributed differently across layers.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze linguistic knowledge in PLMs for African languages, which is unclear despite their improving performance.

Method: Layer-wise probes for six diverse African languages and control tasks for the MasakhaPOS dataset.

Result: Adapted PLMs encode more linguistic information than multilingual PLMs; syntactic info is in middle-to-last layers, semantic info is distributed.

Conclusion: The study clarifies how linguistic knowledge is encoded in African-language PLMs, supporting strategies like active learning and multilingual adaptation.

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [214] [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/pdf/2505.10643)
*Shuchen Guo, Yun Wang, Jichao Yu, Xuansheng Wu, Bilgehan Ayik, Field M. Watts, Ehsan Latif, Ninghao Liu, Lei Liu, Xiaoming Zhai*

Main category: cs.CL

TL;DR: The study examined scoring biases in automatic systems for ELLs in science assessments, finding no significant bias with large datasets but potential issues with small samples.


<details>
  <summary>Details</summary>
Motivation: To investigate how unbalanced training data affects scoring bias and disparities for ELLs in automated scoring systems.

Method: Fine-tuned BERT with four datasets (ELLs, non-ELLs, unbalanced mixed, balanced mixed) and analyzed scoring accuracy and mean score gaps across 21 items.

Result: No AI bias or disparities were found with large datasets (30,000 or 1,000 ELL responses), but concerns arose with small samples (200 ELL responses).

Conclusion: Large training datasets mitigate scoring bias for ELLs, but small samples may introduce disparities.

Abstract: This study investigated potential scoring biases and disparities toward
English Language Learners (ELLs) when using automatic scoring systems for
middle school students' written responses to science assessments. We
specifically focus on examining how unbalanced training data with ELLs
contributes to scoring bias and disparities. We fine-tuned BERT with four
datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting
the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced
mixed dataset with equal representation of both groups. The study analyzed 21
assessment items: 10 items with about 30,000 ELL responses, five items with
about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring
accuracy (Acc) was calculated and compared to identify bias using Friedman
tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and
then calculated the differences in MSGs generated through both the human and AI
models to identify the scoring disparities. We found that no AI bias and
distorted disparities between ELLs and non-ELLs were found when the training
dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could
exist if the sample size is limited (ELL = 200).

</details>


### [215] [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/pdf/2505.11341)
*Banca Calvo Figueras, Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper introduces a large-scale dataset and evaluation method for Critical Questions Generation (CQs-Gen), using LLMs for benchmarking and encouraging further research.


<details>
  <summary>Details</summary>
Motivation: To advance critical thinking by developing systems that generate questions challenging argumentative reasoning, addressing the lack of datasets and evaluation standards.

Method: Constructed a 5K annotated dataset, proposed a reference-based evaluation using LLMs, and conducted zero-shot testing on 11 LLMs.

Result: Established a strong baseline for CQs-Gen, highlighting task difficulty, and provided data, code, and a leaderboard for future research.

Conclusion: The work supports further exploration of CQs-Gen's benefits for automated reasoning and human critical thinking.

Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical
thinking by enabling systems to generate questions that expose underlying
assumptions and challenge the validity of argumentative reasoning structures.
Despite growing interest in this area, progress has been hindered by the lack
of suitable datasets and automatic evaluation standards. This paper presents a
comprehensive approach to support the development and benchmarking of systems
for this task. We construct the first large-scale dataset including $~$5K
manually annotated questions. We also investigate automatic evaluation methods
and propose a reference-based technique using large language models (LLMs) as
the strategy that best correlates with human judgments. Our zero-shot
evaluation of 11 LLMs establishes a strong baseline while showcasing the
difficulty of the task. Data and code plus a public leaderboard are provided to
encourage further research not only in terms of model performance, but also to
explore the practical benefits of CQs-Gen for both automated reasoning and
human critical thinking.

</details>


### [216] [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423)
*Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal*

Main category: cs.CL

TL;DR: Explicit CoT reasoning in RLLMs can degrade instruction-following accuracy, but selective reasoning strategies like classifier-selective reasoning can mitigate this.


<details>
  <summary>Details</summary>
Motivation: To investigate the overlooked phenomenon where explicit CoT reasoning harms instruction-following accuracy in RLLMs.

Method: Evaluated 15 models on IFEval and ComplexBench, analyzed attention patterns, and proposed four mitigation strategies.

Result: CoT reasoning often diverts attention from instruction-relevant tokens, but selective reasoning strategies recover performance.

Conclusion: Selective reasoning, especially classifier-selective reasoning, effectively mitigates reasoning-induced failures in instruction-following.

Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained
for reasoning or prompted via chain-of-thought (CoT), have achieved
state-of-the-art performance on many complex reasoning tasks. However, we
uncover a surprising and previously overlooked phenomenon: explicit CoT
reasoning can significantly degrade instruction-following accuracy. Evaluating
15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)
and ComplexBench (with complex, compositional constraints), we consistently
observe performance drops when CoT prompting is applied. Through large-scale
case studies and an attention-based analysis, we identify common patterns where
reasoning either helps (e.g., with formatting or lexical precision) or hurts
(e.g., by neglecting simple constraints or introducing unnecessary content). We
propose a metric, constraint attention, to quantify model focus during
generation and show that CoT reasoning often diverts attention away from
instruction-relevant tokens. To mitigate these effects, we introduce and
evaluate four strategies: in-context learning, self-reflection, self-selective
reasoning, and classifier-selective reasoning. Our results demonstrate that
selective reasoning strategies, particularly classifier-selective reasoning,
can substantially recover lost performance. To our knowledge, this is the first
work to systematically expose reasoning-induced failures in
instruction-following and offer practical mitigation strategies.

</details>


### [217] [Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing](https://arxiv.org/pdf/2505.11604)
*Kyudan Jung, Hojun Cho, Jooyeol Yun, Soyoung Yang, Jaehyeok Jang, Jagul Choo*

Main category: cs.CL

TL;DR: Talk-to-Your-Slides is an LLM-powered agent for faster, cheaper, and more accurate slide editing by leveraging structured slide object data instead of images.


<details>
  <summary>Details</summary>
Motivation: Existing GUI-based slide editing methods are computationally expensive and slow.

Method: Uses high-level and low-level layers for user-command interaction with slide objects, avoiding image modality.

Result: 34.02% faster processing, 34.76% better instruction fidelity, and 87.42% cheaper operation than baselines.

Conclusion: The system is efficient and effective, supported by TSBench, a new dataset for evaluation.

Abstract: Editing presentation slides remains one of the most common and time-consuming
tasks faced by millions of users daily, despite significant advances in
automated slide generation. Existing approaches have successfully demonstrated
slide editing via graphic user interface (GUI)-based agents, offering intuitive
visual control. However, such methods often suffer from high computational cost
and latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered
agent designed to edit slides %in active PowerPoint sessions by leveraging
structured information about slide objects rather than relying on image
modality. The key insight of our work is designing the editing process with
distinct high-level and low-level layers to facilitate interaction between user
commands and slide objects. By providing direct access to application objects
rather than screen pixels, our system enables 34.02% faster processing, 34.76%
better instruction fidelity, and 87.42% cheaper operation than baselines. To
evaluate slide editing capabilities, we introduce TSBench, a human-annotated
dataset comprising 379 diverse editing instructions paired with corresponding
slide variations in four categories. Our code, benchmark and demos are
available at https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C.

</details>


### [218] [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/pdf/2505.11733)
*Kevin Wu, Eric Wu, Rahul Thapa, Kevin Wei, Angela Zhang, Arvind Suresh, Jacqueline J. Tao, Min Woo Sun, Alejandro Lozano, James Zou*

Main category: cs.CL

TL;DR: The paper introduces MedCaseReasoning, a dataset to evaluate LLMs' clinical reasoning, showing current models' shortcomings and improvements through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks focus only on diagnostic accuracy, ignoring reasoning quality, which is critical in clinical cases.

Method: The authors created MedCaseReasoning, a dataset of 14,489 diagnostic cases with clinician-authored reasoning, and evaluated LLMs on it.

Result: Top models like DeepSeek-R1 scored poorly (48% accuracy, 64% recall), but fine-tuning improved accuracy by 29% and recall by 41%.

Conclusion: Fine-tuning LLMs on detailed reasoning data enhances diagnostic and reasoning performance, addressing a gap in medical AI evaluation.

Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to
diagnose clinical cases. However, unlike domains such as math or coding, where
correctness can be objectively defined by the final answer, medical diagnosis
requires both the outcome and the reasoning process to be accurate. Currently,
widely used medical benchmarks like MedQA and MMLU assess only accuracy in the
final answer, overlooking the quality and faithfulness of the clinical
reasoning process. To address this limitation, we introduce MedCaseReasoning,
the first open-access dataset for evaluating LLMs on their ability to align
with clinician-authored diagnostic reasoning. The dataset includes 14,489
diagnostic question-and-answer cases, each paired with detailed reasoning
statements derived from open-access medical case reports. We evaluate
state-of-the-art reasoning LLMs on MedCaseReasoning and find significant
shortcomings in their diagnoses and reasoning: for instance, the top-performing
open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy
and mentions only 64% of the clinician reasoning statements (recall). However,
we demonstrate that fine-tuning LLMs on the reasoning traces derived from
MedCaseReasoning significantly improves diagnostic accuracy and clinical
reasoning recall by an average relative gain of 29% and 41%, respectively. The
open-source dataset, code, and models are available at
https://github.com/kevinwu23/Stanford-MedCaseReasoning.

</details>


### [219] [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/pdf/2505.11810)
*Shen Li, Renfen Hu, Lijun Wang*

Main category: cs.CL

TL;DR: The paper introduces AI Taiyan, a specialized large language model for Classical Chinese, outperforming general and traditional models in tasks like punctuation, translation, and word explanation.


<details>
  <summary>Details</summary>
Motivation: General-purpose large language models underperform in specialized domains like Classical Chinese, necessitating domain-specific solutions.

Method: Developed AI Taiyan with 1.8B parameters through tailored model design, data processing, foundational training, and fine-tuning.

Result: AI Taiyan excels in Classical Chinese tasks, nearing or surpassing human performance, and outperforms general and traditional models.

Conclusion: The study offers a blueprint for efficient domain-specific model development and highlights applications in ancient text collation and research.

Abstract: General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many natural language processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to language processing of Classical Chinese
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.

</details>


### [220] [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/pdf/2505.11958)
*Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: HiPPrO is a novel two-stage framework for generating nuanced counterspeech by integrating multiple attributes hierarchically, outperforming baselines in intent conformity and Rouge scores.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of single-attribute counterspeech generation by proposing a holistic approach for more effective and nuanced responses.

Method: HiPPrO uses hierarchical prefix learning and preference optimization in two stages: attribute-specific prefix embedding and reward-free preference optimization.

Result: ~38% improvement in intent conformity and slight gains in Rouge scores (1-3%) over baselines, validated by human evaluations.

Conclusion: Multi-attribute conditioning enhances counterspeech efficacy, as demonstrated by HiPPrO's superior performance.

Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online.
Previous studies have focused on generating counterspeech conditioned only on
specific intents (single attributed). However, a holistic approach considering
multiple attributes simultaneously can yield more nuanced and effective
responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with
Preference Optimization, a novel two-stage framework that utilizes the
effectiveness of attribute-specific prefix embedding spaces hierarchically
optimized during the counterspeech generation process in the first phase.
Thereafter, we incorporate both reference and reward-free preference
optimization to generate more constructive counterspeech. Furthermore, we
extend IntentCONANv2 by annotating all 13,973 counterspeech instances with
emotion labels by five annotators. HiPPrO leverages hierarchical prefix
optimization to integrate these dual attributes effectively. An extensive
evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent
conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,
respectively, compared to several baseline models. Human evaluations further
substantiate the superiority of our approach, highlighting the enhanced
relevance and appropriateness of the generated counterspeech. This work
underscores the potential of multi-attribute conditioning in advancing the
efficacy of counterspeech generation systems.

</details>


### [221] [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/pdf/2505.12043)
*Jingxue Chen, Qingkun Tang, Qianchun Lu, Siyuan Fang*

Main category: cs.CL

TL;DR: The paper introduces Mixture of Losses (MoL), a framework to improve domain-specific LLM performance without degrading general language skills by decoupling optimization objectives for domain and general corpora.


<details>
  <summary>Details</summary>
Motivation: Domain-specific LLM applications face hallucinations and accuracy issues, while traditional CPT methods degrade general language skills or fail to adapt effectively.

Method: MoL uses cross-entropy loss for domain-corpus and KL divergence for general-corpus, preserving foundational capabilities while enhancing domain expertise.

Result: Empirical results show a 27.9% accuracy boost on Math-500 and an 83.3% improvement on AIME25, with a 1:1 corpus ratio being optimal.

Conclusion: MoL effectively balances domain adaptation and general skill retention, outperforming traditional CPT approaches.

Abstract: Although large language models (LLMs) perform well in general tasks,
domain-specific applications suffer from hallucinations and accuracy
limitations. Continual Pre-Training (CPT) approaches encounter two key issues:
(1) domain-biased data degrades general language skills, and (2) improper
corpus-mixture ratios limit effective adaptation. To address these, we propose
a novel framework, Mixture of Losses (MoL), which decouples optimization
objectives for domain-specific and general corpora. Specifically, cross-entropy
(CE) loss is applied to domain-corpus to ensure knowledge acquisition, while
Kullback-Leibler (KL) divergence aligns general-corpus training with the base
model's foundational capabilities. This dual-loss architecture preserves
universal skills while enhancing domain expertise, avoiding catastrophic
forgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio
optimally balances training and overfitting without the need for extensive
tuning or resource-intensive experiments. Furthermore, our experiments
demonstrate significant performance gains compared to traditional CPT
approaches, which often suffer from degradation in general language
capabilities; our model achieves 27.9% higher accuracy on the Math-500
benchmark in the non-think reasoning mode, and an impressive 83.3% improvement
on the challenging AIME25 subset in the think mode, underscoring the
effectiveness of our approach.

</details>


### [222] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/pdf/2505.12082)
*Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Zhou Xun, Siyuan Qiao, Liang Xiang, Yonghui Wu*

Main category: cs.CL

TL;DR: Model merging during pre-training improves performance and reduces costs, with insights from experiments on dense and MoE architectures.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of model merging in large-scale pre-training, which remains understudied.

Method: Extensive experiments with dense and MoE architectures, analyzing merging strategies and hyperparameters.

Result: Merging checkpoints with constant learning rates enhances performance and predicts annealing behavior, lowering training costs.

Conclusion: Provides practical guidelines for effective model merging in pre-training, benefiting the open-source community.

Abstract: Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.

</details>


### [223] [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/pdf/2505.12654)
*Yuxin Lin, Yinglin Zheng, Ming Zeng, Wangzheng Shi*

Main category: cs.CL

TL;DR: The paper introduces a multi-modal dataset (MM-F2F) and an end-to-end framework for predicting turn-taking and backchannel actions in human-machine conversations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets and methods for predicting turn-taking and backchannel actions using multi-modal signals (linguistic, acoustic, visual).

Method: Proposes an automatic data collection pipeline to create the MM-F2F dataset (210 hours of videos, 1.5M words, 20M frames). Introduces an end-to-end framework that leverages multi-modal signals (text, audio, video) for prediction.

Result: Achieves a 10% increase in F1-score for turn-taking and 33% for backchannel prediction compared to existing methods.

Conclusion: The dataset and framework advance research in human-machine conversation by improving prediction accuracy and adaptability to real-world scenarios.

Abstract: This paper addresses the gap in predicting turn-taking and backchannel
actions in human-machine conversations using multi-modal signals (linguistic,
acoustic, and visual). To overcome the limitation of existing datasets, we
propose an automatic data collection pipeline that allows us to collect and
annotate over 210 hours of human conversation videos. From this, we construct a
Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over
1.5M words and corresponding turn-taking and backchannel annotations from
approximately 20M frames. Additionally, we present an end-to-end framework that
predicts the probability of turn-taking and backchannel actions from
multi-modal signals. The proposed model emphasizes the interrelation between
modalities and supports any combination of text, audio, and video inputs,
making it adaptable to a variety of realistic scenarios. Our experiments show
that our approach achieves state-of-the-art performance on turn-taking and
backchannel prediction tasks, achieving a 10% increase in F1-score on
turn-taking and a 33% increase on backchannel prediction. Our dataset and code
are publicly available online to ease of subsequent research.

</details>


### [224] [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/pdf/2505.12768)
*Yaxun Dai, Wenxuan Xie, Xialie Zhuang, Tianyu Yang, Yiying Yang, Haiqin Yang, Yuhang Zhao, Pingfu Chao, Wenhao Jiang*

Main category: cs.CL

TL;DR: ReEx-SQL integrates execution feedback into Text-to-SQL generation, improving accuracy and robustness with dynamic reasoning and tree-based decoding.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat execution feedback as a post-hoc signal, limiting their ability to correct reasoning errors during generation.

Method: ReEx-SQL uses execution-aware reinforcement learning, structured prompts, and tree-based decoding to dynamically adjust reasoning based on feedback.

Result: Achieves 88.8% on Spider and 64.9% on BIRD, surpassing baselines by 2.7% and 2.6%, respectively, with 51.9% faster inference.

Conclusion: ReEx-SQL enhances Text-to-SQL performance and efficiency by integrating execution feedback during generation.

Abstract: In Text-to-SQL, execution feedback is essential for guiding large language
models (LLMs) to reason accurately and generate reliable SQL queries. However,
existing methods treat execution feedback solely as a post-hoc signal for
correction or selection, failing to integrate it into the generation process.
This limitation hinders their ability to address reasoning errors as they
occur, ultimately reducing query accuracy and robustness. To address this
issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement
Learning), a framework for Text-to-SQL that enables models to interact with the
database during decoding and dynamically adjust their reasoning based on
execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm
that interleaves intermediate SQL execution into reasoning paths, facilitating
context-sensitive revisions. It achieves this through structured prompts with
markup tags and a stepwise rollout strategy that integrates execution feedback
into each stage of generation. To supervise policy learning, we develop a
composite reward function that includes an exploration reward, explicitly
encouraging effective database interaction. Additionally, ReEx-SQL adopts a
tree-based decoding strategy to support exploratory reasoning, enabling dynamic
expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on
Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning
baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving
85.2% on Spider-Realistic with leading performance. In addition, its
tree-structured decoding improves efficiency and performance over linear
decoding, reducing inference time by 51.9% on the BIRD development set.

</details>


### [225] [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/pdf/2505.13147)
*Aswathy Velutharambath, Kai Sassenberg, Roman Klinger*

Main category: cs.CL

TL;DR: The paper challenges prior success in detecting deception from text, attributing it to dataset artifacts. It introduces a belief-based framework and new corpora (DeFaBel), finding negligible correlations between linguistic cues and deception.


<details>
  <summary>Details</summary>
Motivation: To reassess the reliability of linguistic cues for deception detection, questioning prior findings due to potential dataset biases.

Method: Introduces a belief-based deception framework and constructs the DeFaBel corpora (German and multilingual) to study deception cues in isolation. Evaluates linguistic cues and benchmarks models.

Result: Negligible correlations between linguistic cues and deception in DeFaBel. Inconsistent predictive cues across datasets. Models perform near chance on DeFaBel.

Conclusion: Deception cannot be reliably inferred from linguistic cues, urging a rethink of deception study and modeling in NLP.

Abstract: Can deception be detected solely from written text? Cues of deceptive
communication are inherently subtle, even more so in text-only communication.
Yet, prior studies have reported considerable success in automatic deception
detection. We hypothesize that such findings are largely driven by artifacts
introduced during data collection and do not generalize beyond specific
datasets. We revisit this assumption by introducing a belief-based deception
framework, which defines deception as a misalignment between an author's claims
and true beliefs, irrespective of factual accuracy, allowing deception cues to
be studied in isolation. Based on this framework, we construct three corpora,
collectively referred to as DeFaBel, including a German-language corpus of
deceptive and non-deceptive arguments and a multilingual version in German and
English, each collected under varying conditions to account for belief change
and enable cross-linguistic analysis. Using these corpora, we evaluate commonly
reported linguistic cues of deception. Across all three DeFaBel variants, these
cues show negligible, statistically insignificant correlations with deception
labels, contrary to prior work that treats such cues as reliable indicators. We
further benchmark against other English deception datasets following similar
data collection protocols. While some show statistically significant
correlations, effect sizes remain low and, critically, the set of predictive
cues is inconsistent across datasets. We also evaluate deception detection
using feature-based models, pretrained language models, and instruction-tuned
large language models. While some models perform well on established deception
datasets, they consistently perform near chance on DeFaBel. Our findings
challenge the assumption that deception can be reliably inferred from
linguistic cues and call for rethinking how deception is studied and modeled in
NLP.

</details>


### [226] [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/pdf/2505.13282)
*Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: LORex is a plug-and-play framework combining discriminative ranking and generative reasoning for efficient taxonomy expansion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for taxonomy expansion face challenges like representation limits, noise, and context limits, necessitating a better approach.

Method: LORex ranks and chunks candidate terms into batches, filters noise, and iteratively refines selections by reasoning hierarchy for contextual efficiency.

Result: LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

Conclusion: LORex effectively addresses key challenges in taxonomy expansion, offering superior performance and efficiency.

Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>


### [227] [J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization](https://arxiv.org/pdf/2505.13346)
*Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

Main category: cs.CL

TL;DR: The paper introduces a reinforcement learning-based approach (EIS-GRPO) to improve LLM-as-judge models for reasoning-intensive tasks, outperforming existing models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLM-as-judge models in reasoning-intensive domains by enhancing their evaluation robustness.

Method: Proposes the EIS-GRPO algorithm to mitigate positional biases and introduces ReasoningJudgeBench for diverse reasoning evaluations.

Result: J4R, a 7B judge trained with EIS-GRPO, outperforms GPT-4o and other small judges by 6.7% and 9%, respectively.

Conclusion: The EIS-GRPO-trained judge demonstrates superior performance in reasoning tasks, validating the effectiveness of the proposed method.

Abstract: To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.

</details>


### [228] [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/pdf/2505.13353)
*Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana*

Main category: cs.CL

TL;DR: The paper examines LLMs' ability to reason over long code contexts, differentiating lexical (verbatim) and semantic (functional) recall. It introduces SemTrace to measure semantic recall and finds LLMs struggle with mid-context reasoning, especially for semantic tasks. Lexical recall varies by granularity, and benchmarks may underestimate semantic recall challenges.


<details>
  <summary>Details</summary>
Motivation: To understand how effectively LLMs utilize long contexts for code reasoning and to explore the relationship between lexical and semantic recall abilities.

Method: Proposes SemTrace for measuring semantic recall and evaluates state-of-the-art LLMs on code reasoning tasks, analyzing performance based on snippet position and recall type.

Result: LLMs show a drop in accuracy for mid-context code reasoning, especially with semantic tasks. Lexical recall varies by granularity, and benchmarks may lack sensitivity to semantic recall.

Conclusion: Current benchmarks may not fully capture LLMs' challenges with semantic recall, highlighting a gap in evaluating their in-context reasoning abilities.

Abstract: Although modern Large Language Models (LLMs) support extremely large
contexts, their effectiveness in utilizing long context for code reasoning
remains unclear. This paper investigates LLM reasoning ability over code
snippets within large repositories and how it relates to their recall ability.
Specifically, we differentiate between lexical code recall (verbatim retrieval)
and semantic code recall (remembering what the code does). To measure semantic
recall, we propose SemTrace, a code reasoning technique where the impact of
specific statements on output is attributable and unpredictable. We also
present a method to quantify semantic recall sensitivity in existing
benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop
in code reasoning accuracy as a code snippet approaches the middle of the input
context, particularly with techniques requiring high semantic recall like
SemTrace. Moreover, we find that lexical recall varies by granularity, with
models excelling at function retrieval but struggling with line-by-line recall.
Notably, a disconnect exists between lexical and semantic recall, suggesting
different underlying mechanisms. Finally, our findings indicate that current
code reasoning benchmarks may exhibit low semantic recall sensitivity,
potentially underestimating LLM challenges in leveraging in-context
information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [229] [An Edge AI Solution for Space Object Detection](https://arxiv.org/pdf/2505.13468)
*Wenxuan Zhang, Peng Hu*

Main category: cs.CV

TL;DR: Proposes an Edge AI solution using SE layers, ViT, and YOLOv9 for real-time space object detection with high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for real-time collision assessment in near-Earth orbits due to increasing space assets.

Method: Combines Squeeze-and-Excitation layers, Vision Transformers, and YOLOv9 framework for deep-learning-based vision sensing.

Result: Demonstrates high accuracy and low latency in detecting multiple satellites in realistic scenarios.

Conclusion: The proposed Edge AI solution is effective for real-time space object detection tasks.

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [230] [Self-Supervised Learning for Image Segmentation: A Comprehensive Survey](https://arxiv.org/pdf/2505.13584)
*Thangarajah Akilan, Nusrat Jahan, Wandong Zhang*

Main category: cs.CV

TL;DR: A survey on self-supervised learning (SSL) for image segmentation, covering pretext tasks, downstream tasks, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: Supervised learning requires costly labeled data; SSL leverages unlabeled data to learn representations, making it a powerful paradigm for computer vision tasks like segmentation.

Method: Analyzes over 150 recent articles on SSL-based image segmentation, categorizing pretext tasks, downstream tasks, and benchmark datasets.

Result: Provides a comprehensive overview of SSL methodologies for segmentation, highlighting advances and practical insights.

Conclusion: Offers key observations and future directions to guide emerging researchers in SSL-based segmentation.

Abstract: Supervised learning demands large amounts of precisely annotated data to
achieve promising results. Such data curation is labor-intensive and imposes
significant overhead regarding time and costs. Self-supervised learning (SSL)
partially overcomes these limitations by exploiting vast amounts of unlabeled
data and creating surrogate (pretext or proxy) tasks to learn useful
representations without manual labeling. As a result, SSL has become a powerful
machine learning (ML) paradigm for solving several practical downstream
computer vision problems, such as classification, detection, and segmentation.
Image segmentation is the cornerstone of many high-level visual perception
applications, including medical imaging, intelligent transportation,
agriculture, and surveillance. Although there is substantial research potential
for developing advanced algorithms for SSL-based semantic segmentation, a
comprehensive study of existing methodologies is essential to trace advances
and guide emerging researchers. This survey thoroughly investigates over 150
recent image segmentation articles, particularly focusing on SSL. It provides a
practical categorization of pretext tasks, downstream tasks, and commonly used
benchmark datasets for image segmentation research. It concludes with key
observations distilled from a large body of literature and offers future
directions to make this research field more accessible and comprehensible for
readers.

</details>


### [231] [IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion](https://arxiv.org/pdf/2505.13633)
*Wentao Song, He Huang, Youqiang Sun, Fang Qu, Jiaqi Zhang, Longhui Fang, Yuwei Hao, Chenyang Peng*

Main category: cs.CV

TL;DR: IPENS is an interactive unsupervised method for multi-target point cloud extraction in plant phenotyping, achieving high accuracy without annotated data.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on manual annotations and struggle with self-occluded objects. IPENS aims to provide a non-invasive, efficient solution for plant phenotyping.

Method: IPENS uses radiance field information to lift 2D masks (from SAM2) into 3D space, with a multi-target optimization strategy for segmentation.

Result: Achieves 63.72% mIoU on rice and 89.68% on wheat datasets, with strong phenotypic estimation (e.g., R2 = 0.9956 for wheat spike volume).

Conclusion: IPENS offers a fast, annotation-free solution for high-quality phenotyping, enhancing intelligent breeding efficiency.

Abstract: Advanced plant phenotyping technologies play a crucial role in targeted trait
improvement and accelerating intelligent breeding. Due to the species diversity
of plants, existing methods heavily rely on large-scale high-precision manually
annotated data. For self-occluded objects at the grain level, unsupervised
methods often prove ineffective. This study proposes IPENS, an interactive
unsupervised multi-target point cloud extraction method. The method utilizes
radiance field information to lift 2D masks, which are segmented by SAM2
(Segment Anything Model 2), into 3D space for target point cloud extraction. A
multi-target collaborative optimization strategy is designed to effectively
resolve the single-interaction multi-target segmentation challenge.
Experimental validation demonstrates that IPENS achieves a grain-level
segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong
phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697
(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length
and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a
wheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),
with equally outstanding phenotypic estimation performance: spike volume
prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00
(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92
(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality
phenotyping extraction solution for rice and wheat. Without requiring annotated
data, it rapidly extracts grain-level point clouds within 3 minutes through
simple single-round interactions on images for multiple targets, demonstrating
significant potential to accelerate intelligent breeding efficiency.

</details>


### [232] [GeoVLM: Improving Automated Vehicle Geolocalisation Using Vision-Language Matching](https://arxiv.org/pdf/2505.13669)
*Barkin Dagda, Muhammad Awais, Saber Fallah*

Main category: cs.CV

TL;DR: GeoVLM improves cross-view geo-localization by using vision-language models for interpretable reranking, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of similar-looking scenes in cross-view geo-localization, where existing methods fail to rank the correct image as the top match.

Method: Proposes GeoVLM, a trainable reranking approach leveraging zero-shot vision-language models and interpretable cross-view language descriptions.

Result: GeoVLM enhances retrieval performance on benchmarks (VIGOR, University-1652) and a new dataset (Cross-View UK), surpassing existing methods.

Conclusion: GeoVLM effectively improves geo-localization accuracy with explainable natural language descriptions, validated across datasets.

Abstract: Cross-view geo-localisation identifies coarse geographical position of an
automated vehicle by matching a ground-level image to a geo-tagged satellite
image from a database. Despite the advancements in Cross-view geo-localisation,
significant challenges still persist such as similar looking scenes which makes
it challenging to find the correct match as the top match. Existing approaches
reach high recall rates but they still fail to rank the correct image as the
top match. To address this challenge, this paper proposes GeoVLM, a novel
approach which uses the zero-shot capabilities of vision language models to
enable cross-view geo-localisation using interpretable cross-view language
descriptions. GeoVLM is a trainable reranking approach which improves the best
match accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard
benchmark VIGOR and University-1652 and also through real-life driving
environments using Cross-View United Kingdom, a new benchmark dataset
introduced in this paper. The results of the paper show that GeoVLM improves
retrieval performance of cross-view geo-localisation compared to the
state-of-the-art methods with the help of explainable natural language
descriptions. The code is available at
https://github.com/CAV-Research-Lab/GeoVLM

</details>


### [233] [ReactDiff: Latent Diffusion for Facial Reaction Generation](https://arxiv.org/pdf/2505.14151)
*Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang*

Main category: cs.CV

TL;DR: ReactDiff integrates a Multi-Modality Transformer with latent diffusion for facial reaction generation, outperforming prior methods in correlation, diversity, and realism.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in capturing audio-visual relevance for realistic, diverse, and appropriate listener facial reactions, which prior works simplified or uni-modal approaches couldn't address fully.

Method: Proposes ReactDiff, combining a Multi-Modality Transformer with latent diffusion, using intra- and inter-class attention for multi-modal interaction and diverse outputs.

Result: Achieves a facial reaction correlation of 0.26 and diversity score of 0.094, outperforming existing methods while maintaining realism.

Conclusion: ReactDiff advances facial reaction generation by effectively integrating multi-modal inputs and diffusion, offering improved performance and open-sourced implementation.

Abstract: Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.

</details>


### [234] [GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization](https://arxiv.org/pdf/2505.13731)
*Pengyue Jia, Seongheon Park, Song Gao, Xiangyu Zhao, Yixuan Li*

Main category: cs.CV

TL;DR: GeoRanker is a distance-aware ranking framework for image geolocalization, leveraging vision-language models and a novel multi-order distance loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods for image geolocalization rely on simplistic heuristics and fail to model spatial relationships among candidates, limiting accuracy.

Method: Proposes GeoRanker, which encodes query-candidate interactions and uses a multi-order distance loss to rank absolute and relative distances. Introduces the GeoRanking dataset for training.

Result: GeoRanker outperforms existing methods on IM2GPS3K and YFCC4K benchmarks.

Conclusion: GeoRanker advances image geolocalization by effectively modeling spatial relationships and leveraging multimodal data.

Abstract: Worldwide image geolocalization-the task of predicting GPS coordinates from
images taken anywhere on Earth-poses a fundamental challenge due to the vast
diversity in visual content across regions. While recent approaches adopt a
two-stage pipeline of retrieving candidates and selecting the best match, they
typically rely on simplistic similarity heuristics and point-wise supervision,
failing to model spatial relationships among candidates. In this paper, we
propose GeoRanker, a distance-aware ranking framework that leverages large
vision-language models to jointly encode query-candidate interactions and
predict geographic proximity. In addition, we introduce a multi-order distance
loss that ranks both absolute and relative distances, enabling the model to
reason over structured spatial relationships. To support this, we curate
GeoRanking, the first dataset explicitly designed for geographic ranking tasks
with multimodal candidate information. GeoRanker achieves state-of-the-art
results on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly
outperforming current best methods.

</details>


### [235] [Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks](https://arxiv.org/pdf/2505.13741)
*Gaspard Goupy, Pierre Tirilly, Ioan Marius Bilasco*

Main category: cs.CV

TL;DR: Frozen Backpropagation (fBP) reduces energy costs in SNN training by relaxing weight symmetry and using partial weight transport, achieving near-BP accuracy with lower overhead.


<details>
  <summary>Details</summary>
Motivation: Training SNNs on neuromorphic hardware is energy-efficient but challenging due to weight symmetry requirements in BP, which increase overhead.

Method: Introduces fBP, which updates forward weights with frozen feedback weights, and proposes partial weight transport schemes to minimize synchronization.

Result: fBP outperforms existing methods, achieving BP-like accuracy with 1,000x-10,000x lower transport costs and minimal accuracy drop.

Conclusion: fBP offers a practical solution for efficient on-chip learning in neuromorphic hardware, balancing accuracy and energy savings.

Abstract: Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware
can greatly reduce energy costs compared to GPU-based training. However,
implementing Backpropagation (BP) on such hardware is challenging because
forward and backward passes are typically performed by separate networks with
distinct weights. To compute correct gradients, forward and feedback weights
must remain symmetric during training, necessitating weight transport between
the two networks. This symmetry requirement imposes hardware overhead and
increases energy costs. To address this issue, we introduce Frozen
Backpropagation (fBP), a BP-based training algorithm relaxing weight symmetry
in settings with separate networks. fBP updates forward weights by computing
gradients with periodically frozen feedback weights, reducing weight transports
during training and minimizing synchronization overhead. To further improve
transport efficiency, we propose three partial weight transport schemes of
varying computational complexity, where only a subset of weights is transported
at a time. We evaluate our methods on image recognition tasks and compare them
to existing approaches addressing the weight symmetry requirement. Our results
show that fBP outperforms these methods and achieves accuracy comparable to BP.
With partial weight transport, fBP can substantially lower transport costs by
1,000x with an accuracy drop of only 0.5pp on CIFAR-10 and 1.1pp on CIFAR-100,
or by up to 10,000x at the expense of moderated accuracy loss. This work
provides insights for guiding the design of neuromorphic hardware incorporating
BP-based on-chip learning.

</details>


### [236] [ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model](https://arxiv.org/pdf/2505.13746)
*Satoshi Kondo*

Main category: cs.CV

TL;DR: A new method (ReSW-VL) using a vision-language model (CLIP) for surgical phase recognition outperforms conventional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of research on CNN training methods for feature extraction in surgical phase recognition.

Method: Fine-tuning the image encoder of CLIP using prompt learning for surgical phase recognition.

Result: Outperforms conventional methods on three datasets.

Conclusion: ReSW-VL is effective for surgical phase recognition.

Abstract: Surgical phase recognition from video is a technology that automatically
classifies the progress of a surgical procedure and has a wide range of
potential applications, including real-time surgical support, optimization of
medical resources, training and skill assessment, and safety improvement.
Recent advances in surgical phase recognition technology have focused primarily
on Transform-based methods, although methods that extract spatial features from
individual frames using a CNN and video features from the resulting time series
of spatial features using time series modeling have shown high performance.
However, there remains a paucity of research on training methods for CNNs
employed for feature extraction or representation learning in surgical phase
recognition. In this study, we propose a method for representation learning in
surgical workflow analysis using a vision-language model (ReSW-VL). Our
proposed method involves fine-tuning the image encoder of a CLIP (Convolutional
Language Image Model) vision-language model using prompt learning for surgical
phase recognition. The experimental results on three surgical phase recognition
datasets demonstrate the effectiveness of the proposed method in comparison to
conventional methods.

</details>


### [237] [RETRO: REthinking Tactile Representation Learning with Material PriOrs](https://arxiv.org/pdf/2505.14319)
*Weihao Xia, Chenliang Zhou, Cengiz Oztireli*

Main category: cs.CV

TL;DR: The paper introduces a material-aware tactile representation learning method to improve tactile feedback by incorporating material properties, addressing a gap in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing tactile representation learning methods neglect material properties, focusing instead on aligning tactile data with visual or textual information. This oversight limits the richness of tactile feedback.

Method: The proposed method revisits tactile representation learning by incorporating material-aware priors—pre-learned characteristics of different materials—into the learning process.

Result: The approach enhances tactile feedback accuracy and richness across diverse materials and textures, benefiting applications like robotics and haptic systems.

Conclusion: Incorporating material properties into tactile representation learning improves performance and generalization, advancing real-world applications.

Abstract: Tactile perception is profoundly influenced by the surface properties of
objects in contact. However, despite their crucial role in shaping tactile
experiences, these material characteristics have been largely neglected in
existing tactile representation learning methods. Most approaches primarily
focus on aligning tactile data with visual or textual information, overlooking
the richness of tactile feedback that comes from understanding the materials'
inherent properties. In this work, we address this gap by revisiting the
tactile representation learning framework and incorporating material-aware
priors into the learning process. These priors, which represent pre-learned
characteristics specific to different materials, allow tactile models to better
capture and generalize the nuances of surface texture. Our method enables more
accurate, contextually rich tactile feedback across diverse materials and
textures, improving performance in real-world applications such as robotics,
haptic feedback systems, and material editing.

</details>


### [238] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/pdf/2505.13777)
*Subash Khanal, Srikumar Sastry, Aayush Dhakal, Adeel Ahmad, Nathan Jacobs*

Main category: cs.CV

TL;DR: Sat2Sound is a framework for soundscape mapping using multimodal learning, outperforming existing methods by leveraging a shared codebook of soundscape concepts and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for soundscape mapping lack diversity in capturing sound sources. Sat2Sound addresses this by enhancing datasets with semantically rich descriptions using a Vision-Language Model.

Method: The framework uses contrastive learning across audio, captions, satellite images, and their captions, learning a shared codebook of soundscape concepts. Each sample is represented as a weighted average of these concepts.

Result: Sat2Sound achieves state-of-the-art performance in cross-modal retrieval on GeoSound and SoundingEarth datasets and enables location-based soundscape synthesis.

Conclusion: Sat2Sound advances soundscape mapping by integrating multimodal learning and introduces a novel application for immersive acoustic experiences.

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [239] [Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language](https://arxiv.org/pdf/2505.13784)
*Dinh Nam Pham, Eleftherios Avramidis*

Main category: cs.CV

TL;DR: The paper explores transfer learning from Visual Speech Recognition (VSR) to improve mouthing recognition in German Sign Language, showing multi-task learning enhances accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Mouthing in sign language provides linguistic value, but datasets are limited. Transfer learning from VSR could bridge this gap.

Method: Leveraged three VSR datasets (English, German unrelated words, German target words) to study task similarity and multi-task learning.

Result: Multi-task learning improved mouthing recognition and VSR accuracy, indicating mouthing should be treated as a related but distinct task.

Conclusion: Transfer learning from VSR benefits SLR with limited mouthing annotations, suggesting a promising direction for future research.

Abstract: Sign Language Recognition (SLR) systems primarily focus on manual gestures,
but non-manual features such as mouth movements, specifically mouthing, provide
valuable linguistic information. This work directly classifies mouthing
instances to their corresponding words in the spoken language while exploring
the potential of transfer learning from Visual Speech Recognition (VSR) to
mouthing recognition in German Sign Language. We leverage three VSR datasets:
one in English, one in German with unrelated words and one in German containing
the same target words as the mouthing dataset, to investigate the impact of
task similarity in this setting. Our results demonstrate that multi-task
learning improves both mouthing recognition and VSR accuracy as well as model
robustness, suggesting that mouthing recognition should be treated as a
distinct but related task to VSR. This research contributes to the field of SLR
by proposing knowledge transfer from VSR to SLR datasets with limited mouthing
annotations.

</details>


### [240] [Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels](https://arxiv.org/pdf/2505.13788)
*Yongshuo Zong, Qin Zhang, Dongsheng An, Zhihua Li, Xiang Xu, Linghan Xu, Zhuowen Tu, Yifan Xing, Onkar Dabeer*

Main category: cs.CV

TL;DR: A workflow for scaling instruction-following data to enhance pixel-level grounding in VLMs, addressing challenges like hallucinated references and multi-object scenarios. Uses knowledge distillation to create dataset Ground-V, improving model performance significantly.


<details>
  <summary>Details</summary>
Motivation: To address real-world challenges in text-instruction-based grounding, such as hallucinated references and multi-object scenarios, while minimizing human annotation costs.

Method: Leverages knowledge distillation from a pre-trained teacher model to generate high-quality instruction-response pairs linked to pixel-level annotations.

Result: Models trained on Ground-V show substantial improvements, with accuracy boosts of 4.4% for LISA and 7.9% for PSALM, and new SOTA results on benchmarks like RefCOCO/+/g.

Conclusion: Ground-V effectively enhances pixel-level grounding capabilities in VLMs, achieving significant performance gains and setting new benchmarks.

Abstract: This work presents a simple yet effective workflow for automatically scaling
instruction-following data to elicit pixel-level grounding capabilities of VLMs
under complex instructions. In particular, we address five critical real-world
challenges in text-instruction-based grounding: hallucinated references,
multi-object scenarios, reasoning, multi-granularity, and part-level
references. By leveraging knowledge distillation from a pre-trained teacher
model, our approach generates high-quality instruction-response pairs linked to
existing pixel-level annotations, minimizing the need for costly human
annotation. The resulting dataset, Ground-V, captures rich object localization
knowledge and nuanced pixel-level referring expressions. Experiment results
show that models trained on Ground-V exhibit substantial improvements across
diverse grounding tasks. Specifically, incorporating Ground-V during training
directly achieves an average accuracy boost of 4.4% for LISA and a 7.9% for
PSALM across six benchmarks on the gIoU metric. It also sets new
state-of-the-art results on standard benchmarks such as RefCOCO/+/g. Notably,
on gRefCOCO, we achieve an N-Acc of 83.3%, exceeding the previous
state-of-the-art by more than 20%.

</details>


### [241] [Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning](https://arxiv.org/pdf/2505.13812)
*Zhongyu Chen, Rong Zhao, Xie Han, Xindong Guo, Song Wang, Zherui Qiao*

Main category: cs.CV

TL;DR: A physics-driven self-supervised learning method for point cloud representation captures local-whole relationships through force propagation, outperforming existing methods in classification, few-shot learning, and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on geometric distribution but neglect local-whole relationships, which are crucial for understanding object deformation and shape.

Method: A dual-task encoder-decoder framework integrates geometric modeling and physics-driven deformation, using implicit fields and physics loss functions.

Result: The method excels in object classification, few-shot learning, and segmentation tasks.

Conclusion: The proposed approach effectively models local-whole relationships, enhancing point cloud representation learning.

Abstract: Existing point cloud representation learning tend to learning the geometric
distribution of objects through data-driven approaches, emphasizing structural
features while overlooking the relationship between the local information and
the whole structure. Local features reflect the fine-grained variations of an
object, while the whole structure is determined by the interaction and
combination of these local features, collectively defining the object's shape.
In real-world, objects undergo elastic deformation under external forces, and
this deformation gradually affects the whole structure through the propagation
of forces from local regions, thereby altering the object's geometric
properties. Inspired by this, we propose a physics-driven self-supervised
learning method for point cloud representation, which captures the relationship
between parts and the whole by constructing a local-whole force propagation
mechanism. Specifically, we employ a dual-task encoder-decoder framework,
integrating the geometric modeling capability of implicit fields with
physics-driven elastic deformation. The encoder extracts features from the
point cloud and its tetrahedral mesh representation, capturing both geometric
and physical properties. These features are then fed into two decoders: one
learns the whole geometric shape of the point cloud through an implicit field,
while the other predicts local deformations using two specifically designed
physics information loss functions, modeling the deformation relationship
between local and whole shapes. Experimental results show that our method
outperforms existing approaches in object classification, few-shot learning,
and segmentation, demonstrating its effectiveness.

</details>


### [242] [InstanceBEV: Unifying Instance and BEV Representation for Global Modeling](https://arxiv.org/pdf/2505.13817)
*Feng Li, Kun Xu, Zhaoyue Wang, Yunduan Cui, Mohammad Masum Billah, Jia Liu*

Main category: cs.CV

TL;DR: InstanceBEV introduces instance-level dimensionality reduction for Bird's-Eye View (BEV) to enable efficient global modeling with transformers, outperforming existing methods without complex optimizations.


<details>
  <summary>Details</summary>
Motivation: Existing BEV-based methods for autonomous driving require extensive optimizations for large-scale global modeling, while multi-view camera methods face cubic data complexity growth.

Method: InstanceBEV employs instance-level dimensionality reduction and transformers for global feature aggregation, sampling global feature maps into 3D space.

Result: Experiments on OpenOcc-NuScenes dataset show state-of-the-art performance with a simple, efficient framework.

Conclusion: InstanceBEV offers a practical solution for BEV-based global modeling, achieving high performance without additional optimizations.

Abstract: Occupancy Grid Maps are widely used in navigation for their ability to
represent 3D space occupancy. However, existing methods that utilize multi-view
cameras to construct Occupancy Networks for perception modeling suffer from
cubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspective
offers a more practical solution for autonomous driving, as it provides higher
semantic density and mitigates complex object occlusions. Nonetheless,
BEV-based approaches still require extensive engineering optimizations to
enable efficient large-scale global modeling. To address this challenge, we
propose InstanceBEV, the first method to introduce instance-level
dimensionality reduction for BEV, enabling global modeling with transformers
without relying on sparsification or acceleration operators. Different from
other BEV methods, our approach directly employs transformers to aggregate
global features. Compared to 3D object detection models, our method samples
global feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset show
that InstanceBEV achieves state-of-the-art performance while maintaining a
simple, efficient framework without requiring additional optimizations.

</details>


### [243] [Blind Restoration of High-Resolution Ultrasound Video](https://arxiv.org/pdf/2505.13915)
*Chu Chen, Kangning Cui, Pasquale Cascarano, Wei Tang, Elena Loli Piccolomini, Raymond H. Chan*

Main category: cs.CV

TL;DR: A self-supervised algorithm, Deep Ultrasound Prior (DUP), enhances ultrasound video resolution and reduces noise without paired training data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Ultrasound videos often have low SNR and resolution, and variability in equipment/settings reduces model generalizability.

Method: DUP uses a video-adaptive neural network optimization process for super-resolution and noise removal.

Result: DUP outperforms existing super-resolution algorithms, improving downstream applications.

Conclusion: DUP effectively addresses ultrasound video quality issues without paired data, enhancing diagnostic utility.

Abstract: Ultrasound imaging is widely applied in clinical practice, yet ultrasound
videos often suffer from low signal-to-noise ratios (SNR) and limited
resolutions, posing challenges for diagnosis and analysis. Variations in
equipment and acquisition settings can further exacerbate differences in data
distribution and noise levels, reducing the generalizability of pre-trained
models. This work presents a self-supervised ultrasound video super-resolution
algorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive
optimization process of a neural network that enhances the resolution of given
ultrasound videos without requiring paired training data while simultaneously
removing noise. Quantitative and visual evaluations demonstrate that DUP
outperforms existing super-resolution algorithms, leading to substantial
improvements for downstream applications.

</details>


### [244] [MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction](https://arxiv.org/pdf/2505.13839)
*Zhenyu Bao, Qing Li, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu*

Main category: cs.CV

TL;DR: MGStream improves 3D Gaussian Splatting for dynamic scenes by using motion-related 3D Gaussians, reducing flickering and storage issues while enhancing rendering quality.


<details>
  <summary>Details</summary>
Motivation: Address flickering artifacts, storage inefficiency, and emerging object modeling in 3DGS-based dynamic scene reconstruction.

Method: Uses motion-related 3D Gaussians for dynamics and vanilla 3DGs for statics, with rigid deformation and attention-based optimization.

Result: Outperforms existing methods in rendering quality, efficiency, and temporal consistency on N3DV and MeetRoom datasets.

Conclusion: MGStream effectively solves key challenges in dynamic 3DGS, offering superior performance and efficiency.

Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention in streamable
dynamic novel view synthesis (DNVS) for its photorealistic rendering capability
and computational efficiency. Despite much progress in improving rendering
quality and optimization strategies, 3DGS-based streamable dynamic scene
reconstruction still suffers from flickering artifacts and storage
inefficiency, and struggles to model the emerging objects. To tackle this, we
introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to
reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related
3DGs are implemented according to the motion mask and the clustering-based
convex hull algorithm. The rigid deformation is applied to the motion-related
3DGs for modeling the dynamic, and the attention-based optimization on the
motion-related 3DGs enables the reconstruction of the emerging objects. As the
deformation and optimization are only conducted on the motion-related 3DGs,
MGStream avoids flickering artifacts and improves the storage efficiency.
Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that
MGStream surpasses existing streaming 3DGS-based approaches in terms of
rendering quality, training/storage efficiency and temporal consistency. Our
code is available at: https://github.com/pcl3dv/MGStream.

</details>


### [245] [OT-DETECTOR: Delving into Optimal Transport for Zero-shot Out-of-Distribution Detection](https://arxiv.org/pdf/2503.06442)
*Yu Liu, Hao Tang, Haiqi Zhang, Jing Qin, Zechao Li*

Main category: cs.CV

TL;DR: OT-DETECTOR is a novel framework using Optimal Transport (OT) for zero-shot OOD detection, outperforming existing methods by capturing both semantic and distributional discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot OOD detection methods focus on semantic matching but miss distributional discrepancies, limiting reliability.

Method: Proposes OT-DETECTOR, using OT to quantify semantic and distributional gaps, and introduces SaCR to refine content based on semantic cues.

Result: Achieves state-of-the-art performance, especially in hard-OOD scenarios, on multiple benchmarks.

Conclusion: OT-DETECTOR effectively addresses limitations of current methods, enhancing OOD detection robustness.

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications. While
zero-shot OOD detection, which requires no training on in-distribution (ID)
data, has become feasible with the emergence of vision-language models like
CLIP, existing methods primarily focus on semantic matching and fail to fully
capture distributional discrepancies. To address these limitations, we propose
OT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify
both semantic and distributional discrepancies between test samples and ID
labels. Specifically, we introduce cross-modal transport mass and transport
cost as semantic-wise and distribution-wise OOD scores, respectively, enabling
more robust detection of OOD samples. Additionally, we present a semantic-aware
content refinement (SaCR) module, which utilizes semantic cues from ID labels
to amplify the distributional discrepancy between ID and hard OOD samples.
Extensive experiments on several benchmarks demonstrate that OT-DETECTOR
achieves state-of-the-art performance across various OOD detection tasks,
particularly in challenging hard-OOD scenarios.

</details>


### [246] [Towards Generating Realistic Underwater Images](https://arxiv.org/pdf/2505.14296)
*Abdul-Kazeem Shamba*

Main category: cs.CV

TL;DR: The paper evaluates contrastive learning and GANs for generating realistic underwater images from synthetic ones, using FID and SSIM metrics. Paired methods like pix2pix excel in perceptual quality, while unpaired methods like CycleGAN and CUT balance realism and structure. Depth-aware CUT achieves the best realism but slightly compromises structural fidelity.


<details>
  <summary>Details</summary>
Motivation: To improve the realism of underwater images generated from synthetic ones with uniform lighting, addressing the gap in perceptual quality and structural preservation.

Method: Evaluates paired (pix2pix) and unpaired (CycleGAN, CUT) image translation models on the VAROS dataset, incorporating depth information for enhanced realism.

Result: Pix2pix leads in FID (perceptual quality), while autoencoder excels in SSIM (structural fidelity). Depth-aware CUT achieves the lowest FID but slightly reduces SSIM.

Conclusion: Depth-aware learning enhances realism but may introduce structural variations, highlighting trade-offs between perceptual quality and structural fidelity in underwater image generation.

Abstract: This paper explores the use of contrastive learning and generative
adversarial networks for generating realistic underwater images from synthetic
images with uniform lighting. We investigate the performance of image
translation models for generating realistic underwater images using the VAROS
dataset. Two key evaluation metrics, Fr\'echet Inception Distance (FID) and
Structural Similarity Index Measure (SSIM), provide insights into the
trade-offs between perceptual quality and structural preservation. For paired
image translation, pix2pix achieves the best FID scores due to its paired
supervision and PatchGAN discriminator, while the autoencoder model attains the
highest SSIM, suggesting better structural fidelity despite producing blurrier
outputs. Among unpaired methods, CycleGAN achieves a competitive FID score by
leveraging cycle-consistency loss, whereas CUT, which replaces
cycle-consistency with contrastive learning, attains higher SSIM, indicating
improved spatial similarity retention. Notably, incorporating depth information
into CUT results in the lowest overall FID score, demonstrating that depth cues
enhance realism. However, the slight decrease in SSIM suggests that depth-aware
learning may introduce structural variations.

</details>


### [247] [SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction](https://arxiv.org/pdf/2505.13856)
*Ruqin Zhou, San Jiang, Wanshou Jiang, Yongsheng Zhang, Chenguang Dai*

Main category: cs.CV

TL;DR: SuperMapNet improves HD map construction by combining camera and LiDAR inputs with cross-attention and flow-based modules for better BEV features and three-level interactions for accurate element classification and localization.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current HD map methods, such as limited perception from single modalities, poor multi-modal synergy, and low accuracy due to neglecting element interactions.

Method: Uses camera images and LiDAR point clouds, with synergy enhancement and disparity alignment modules for BEV features, and three-level interactions (Point2Point, Element2Element, Point2Element) for classification and localization.

Result: Outperforms SOTAs by 14.9/8.8 mAP (hard/easy) on nuScenes and 18.5/3.1 mAP on Argoverse2.

Conclusion: SuperMapNet achieves long-range, high-accuracy HD map construction by effectively integrating multi-modal data and element interactions.

Abstract: Vectorized HD map is essential for autonomous driving. Remarkable work has
been achieved in recent years, but there are still major issues: (1) in the
generation of the BEV features, single modality-based methods are of limited
perception capability, while direct concatenation-based multi-modal methods
fail to capture synergies and disparities between different modalities,
resulting in limited ranges with feature holes; (2) in the classification and
localization of map elements, only point information is used without the
consideration of element infor-mation and neglects the interaction between
point information and element information, leading to erroneous shapes and
element entanglement with low accuracy. To address above issues, we introduce
SuperMapNet for long-range and high-accuracy vectorized HD map construction. It
uses both camera images and LiDAR point clouds as input, and first tightly
couple semantic information from camera images and geometric information from
LiDAR point clouds by a cross-attention based synergy enhancement module and a
flow-based disparity alignment module for long-range BEV feature generation.
And then, local features from point queries and global features from element
queries are tightly coupled by three-level interactions for high-accuracy
classification and localization, where Point2Point interaction learns local
geometric information between points of the same element and of each point,
Element2Element interaction learns relation constraints between different
elements and semantic information of each elements, and Point2Element
interaction learns complement element information for its constituent points.
Experiments on the nuScenes and Argoverse2 datasets demonstrate superior
performances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP under
hard/easy settings, respectively. The code is made publicly available1.

</details>


### [248] [A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles](https://arxiv.org/pdf/2505.14634)
*Gokul Bhusal, Yifei Lou, Cristina Garcia-Cardona, Ekaterina Merkurjev*

Main category: cs.CV

TL;DR: The paper proposes a group sparsity-based framework for hyperspectral unmixing (HU) to address material variability, using endmember bundles and novel regularization like the TL1 penalty.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral data often contains mixed materials due to low resolution, and conventional linear models fail to account for material variability.

Method: The authors introduce a bundle-based framework with group sparsity (inter-group or SWAG) and employ the TL1 penalty for regularization.

Result: Experiments on synthetic and real data show the framework's effectiveness and superiority.

Conclusion: The proposed approach successfully handles material variability in HU, outperforming traditional methods.

Abstract: Due to low spatial resolution, hyperspectral data often consists of mixtures
of contributions from multiple materials. This limitation motivates the task of
hyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU
aims to identify the spectral signatures (\textit{endmembers}) of the materials
present in an observed scene, along with their relative proportions
(\textit{fractional abundance}) in each pixel. A major challenge lies in the
class variability in materials, which hinders accurate representation by a
single spectral signature, as assumed in the conventional linear mixing model.
Moreover, To address this issue, we propose using group sparsity after
representing each material with a set of spectral signatures, known as
endmember bundles, where each group corresponds to a specific material. In
particular, we develop a bundle-based framework that can enforce either
inter-group sparsity or sparsity within and across groups (SWAG) on the
abundance coefficients. Furthermore, our framework offers the flexibility to
incorporate a variety of sparsity-promoting penalties, among which the
transformed $\ell_1$ (TL1) penalty is a novel regularization in the HU
literature. Extensive experiments conducted on both synthetic and real
hyperspectral data demonstrate the effectiveness and superiority of the
proposed approaches.

</details>


### [249] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/pdf/2505.13860)
*Tiancheng Jiang, Henry Wang, Md Sirajus Salekin, Parmida Atighehchian, Shinan Zhang*

Main category: cs.CV

TL;DR: The paper explores adapting general-domain Vision Language Models (VLMs) to soccer-specific tasks, achieving significant performance improvements through curriculum learning and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Most VLMs are domain-agnostic, leaving their transferability to specialized domains like soccer under-explored. This work aims to bridge this gap.

Method: Uses large-scale soccer datasets and LLM-generated instruction-following data to fine-tune a general-domain VLM iteratively, employing curriculum learning (teaching key concepts first, then QA tasks).

Result: The adapted model shows a 37.5% relative improvement in visual QA and a jump from 11.8% to 63.5% accuracy in soccer action classification.

Conclusion: Domain-specific adaptation of VLMs is feasible and effective, as demonstrated by the soccer case study.

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [250] [4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision](https://arxiv.org/pdf/2505.13905)
*Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou*

Main category: cs.CV

TL;DR: 4D-ROLLS is a weakly supervised occupancy estimation method for 4D radar, using LiDAR as supervision, excelling in degraded environments and achieving fast inference.


<details>
  <summary>Details</summary>
Motivation: Existing occupancy estimation methods relying on LiDAR or cameras perform poorly in degraded environments like smoke or rain.

Method: Proposes 4D-ROLLS, using pseudo-LiDAR labels (occupancy queries and height maps) to train a 4D radar model, aligning it with LiDAR outputs.

Result: Demonstrates robustness in degraded conditions, cross-dataset effectiveness, and applicability to downstream tasks like BEV segmentation.

Conclusion: 4D-ROLLS is a lightweight, fast, and versatile solution for occupancy estimation, with potential for broader applications.

Abstract: A comprehensive understanding of 3D scenes is essential for autonomous
vehicles (AVs), and among various perception tasks, occupancy estimation plays
a central role by providing a general representation of drivable and occupied
space. However, most existing occupancy estimation methods rely on LiDAR or
cameras, which perform poorly in degraded environments such as smoke, rain,
snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised
occupancy estimation method for 4D radar using the LiDAR point cloud as the
supervisory signal. Specifically, we introduce a method for generating
pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as
multi-stage supervision to train the 4D radar occupancy estimation model. Then
the model is aligned with the occupancy map produced by LiDAR, fine-tuning its
accuracy in occupancy estimation. Extensive comparative experiments validate
the exceptional performance of 4D-ROLLS. Its robustness in degraded
environments and effectiveness in cross-dataset training are qualitatively
demonstrated. The model is also seamlessly transferred to downstream tasks BEV
segmentation and point cloud occupancy prediction, highlighting its potential
for broader applications. The lightweight network enables 4D-ROLLS model to
achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of
4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.

</details>


### [251] [Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video Retrieval](https://arxiv.org/pdf/2505.12499)
*Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong*

Main category: cs.CV

TL;DR: GARE introduces a learnable increment to address the modality gap and false negatives in text-video retrieval, improving alignment stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods in text-video retrieval suffer from modality gaps and false negatives, causing conflicting gradients and unstable alignment.

Method: GARE uses a pair-specific increment (Delta_ij) derived from a Taylor approximation of InfoNCE loss, implemented via a lightweight neural module with regularization for stability.

Result: GARE improves alignment accuracy and robustness across four benchmarks, confirming its effectiveness in mitigating gradient conflicts.

Conclusion: GARE successfully addresses the modality gap and false negatives, enhancing retrieval performance through gap-aware tension mitigation.

Abstract: Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.

</details>


### [252] [An Explorative Analysis of SVM Classifier and ResNet50 Architecture on African Food Classification](https://arxiv.org/pdf/2505.13923)
*Chinedu Emmanuel Mbonu, Kenechukwu Anigbogu, Doris Asogwa, Tochukwu Belonwu*

Main category: cs.CV

TL;DR: The study evaluates deep learning (ResNet50) and traditional machine learning (SVM) for African food classification, using 1,658 images across six categories. Performance is measured via confusion matrix, F1-score, accuracy, recall, and precision.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored application of food recognition systems to African cuisines.

Method: Comparison of a fine-tuned ResNet50 model and an SVM classifier on a dataset of 1,658 images across six African food categories.

Result: Findings provide insights into the strengths and limitations of both deep learning and traditional machine learning methods.

Conclusion: The study contributes to advancing food recognition for African cuisines by evaluating and comparing two approaches.

Abstract: Food recognition systems has advanced significantly for Western cuisines, yet
its application to African foods remains underexplored. This study addresses
this gap by evaluating both deep learning and traditional machine learning
methods for African food classification. We compared the performance of a
fine-tuned ResNet50 model with a Support Vector Machine (SVM) classifier. The
dataset comprises 1,658 images across six selected food categories that are
known in Africa. To assess model effectiveness, we utilize five key evaluation
metrics: Confusion matrix, F1-score, accuracy, recall and precision. Our
findings offer valuable insights into the strengths and limitations of both
approaches, contributing to the advancement of food recognition for African
cuisines.

</details>


### [253] [LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts](https://arxiv.org/pdf/2505.13928)
*Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, Wentao Zhang*

Main category: cs.CV

TL;DR: LoVR is a new benchmark for long video-text retrieval, addressing limitations of existing datasets with longer videos, fine-grained clips, and high-quality captions. It includes an efficient caption generation framework and semantic fusion method, proving challenging for current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for video-text retrieval have issues like short video duration, low-quality captions, and coarse annotations, limiting evaluation of advanced methods.

Method: LoVR introduces 467 long videos with 40,804 fine-grained clips and high-quality captions. It uses a caption generation framework (VLM generation, quality scoring, refinement) and semantic fusion for full-video captions.

Result: Experiments show LoVR is challenging for current embedding models, highlighting their limitations.

Conclusion: LoVR advances video-text retrieval with its dataset and methods, offering insights for future research. The dataset and code are publicly available.

Abstract: Long videos contain a vast amount of information, making video-text retrieval
an essential and challenging task in multimodal learning. However, existing
benchmarks suffer from limited video duration, low-quality captions, and coarse
annotation granularity, which hinder the evaluation of advanced video-text
retrieval methods. To address these limitations, we introduce LoVR, a benchmark
specifically designed for long video-text retrieval. LoVR contains 467 long
videos and over 40,804 fine-grained clips with high-quality captions. To
overcome the issue of poor machine-generated annotations, we propose an
efficient caption generation framework that integrates VLM automatic
generation, caption quality scoring, and dynamic refinement. This pipeline
improves annotation accuracy while maintaining scalability. Furthermore, we
introduce a semantic fusion method to generate coherent full-video captions
without losing important contextual information. Our benchmark introduces
longer videos, more detailed captions, and a larger-scale dataset, presenting
new challenges for video understanding and retrieval. Extensive experiments on
various advanced embedding models demonstrate that LoVR is a challenging
benchmark, revealing the limitations of current approaches and providing
valuable insights for future research. We release the code and dataset link at
https://github.com/TechNomad-ds/LoVR-benchmark

</details>


### [254] [Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR](https://arxiv.org/pdf/2505.13943)
*Samee Arif, Sualeha Farid*

Main category: cs.CV

TL;DR: The paper presents an end-to-end OCR pipeline for Urdu newspapers, addressing challenges like multi-column layouts and low-resolution scans. It uses YOLOv11x for article and column segmentation, SwinIR for super-resolution, and benchmarks LLMs for text recognition, with Gemini-2.5-Pro achieving the lowest WER.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in OCR for Urdu newspapers, such as complex layouts, poor scan quality, and diverse fonts, to improve accuracy and usability.

Method: The method involves four modules: article segmentation (YOLOv11x), image super-resolution (SwinIR), column segmentation (YOLOv11x), and text recognition (benchmarked LLMs like Gemini, GPT, Llama, and Claude).

Result: Results include high precision for segmentation (0.963-0.970), super-resolution (32.71 dB PSNR), and text recognition (lowest WER of 0.133 by Gemini-2.5-Pro).

Conclusion: The pipeline effectively addresses Urdu newspaper OCR challenges, achieving high accuracy across all stages, with Gemini-2.5-Pro performing best in text recognition.

Abstract: This paper introduces a comprehensive end-to-end pipeline for Optical
Character Recognition (OCR) on Urdu newspapers. In our approach, we address the
unique challenges of complex multi-column layouts, low-resolution archival
scans, and diverse font styles. Our process decomposes the OCR task into four
key modules: (1) article segmentation, (2) image super-resolution, (3) column
segmentation, and (4) text recognition. For article segmentation, we fine-tune
and evaluate YOLOv11x to identify and separate individual articles from
cluttered layouts. Our model achieves a precision of 0.963 and mAP@50 of 0.975.
For super-resolution, we fine-tune and benchmark the SwinIR model (reaching
32.71 dB PSNR) to enhance the quality of degraded newspaper scans. To do our
column segmentation, we use YOLOv11x to separate columns in text to further
enhance performance - this model reaches a precision of 0.970 and mAP@50 of
0.975. In the text recognition stage, we benchmark a range of LLMs from
different families, including Gemini, GPT, Llama, and Claude. The lowest WER of
0.133 is achieved by Gemini-2.5-Pro.

</details>


### [255] [StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning](https://arxiv.org/pdf/2505.13997)
*Huaijie Wang, De Cheng, Guozhang Li, Zhipeng Xu, Lingfeng He, Jie Li, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: StPR is a novel VCIL framework that disentangles and preserves spatiotemporal information without exemplars, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: VCIL is challenging due to spatiotemporal complexities and privacy concerns with exemplar rehearsal. Existing methods neglect temporal modeling.

Method: StPR uses Frame-Shared Semantics Distillation (FSSD) for semantic channel regularization and Temporal Decomposition-based Mixture-of-Experts (TD-MoE) for dynamic routing.

Result: StPR outperforms baselines on UCF101, HMDB51, and Kinetics400, offering better interpretability and efficiency.

Conclusion: StPR provides a unified, exemplar-free VCIL solution by effectively leveraging spatial and temporal information.

Abstract: Video Class-Incremental Learning (VCIL) seeks to develop models that
continuously learn new action categories over time without forgetting
previously acquired knowledge. Unlike traditional Class-Incremental Learning
(CIL), VCIL introduces the added complexity of spatiotemporal structures,
making it particularly challenging to mitigate catastrophic forgetting while
effectively capturing both frame-shared semantics and temporal dynamics.
Existing approaches either rely on exemplar rehearsal, raising concerns over
memory and privacy, or adapt static image-based methods that neglect temporal
modeling. To address these limitations, we propose Spatiotemporal Preservation
and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly
disentangles and preserves spatiotemporal information. First, we introduce
Frame-Shared Semantics Distillation (FSSD), which identifies semantically
stable and meaningful channels by jointly considering semantic sensitivity and
classification contribution. These important semantic channels are selectively
regularized to maintain prior knowledge while allowing for adaptation. Second,
we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which
dynamically routes task-specific experts based on their temporal dynamics,
enabling inference without task ID or stored exemplars. Together, StPR
effectively leverages spatial semantics and temporal dynamics, achieving a
unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51,
and Kinetics400 show that our method outperforms existing baselines while
offering improved interpretability and efficiency in VCIL. Code is available in
the supplementary materials.

</details>


### [256] [Multi-Label Stereo Matching for Transparent Scene Depth Estimation](https://arxiv.org/pdf/2505.14008)
*Zhidan Liu, Chengtang Yao, Jiaxi Zeng, Yuwei Wu, Yunde Jia*

Main category: cs.CV

TL;DR: A multi-label stereo matching method for estimating depth of transparent objects and occluded background, outperforming single-label approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume unimodal disparity distribution, limiting accuracy in transparent scenes.

Method: Proposes multi-label regression with pixel-wise multivariate Gaussian representation, iteratively updated via GRU framework.

Result: Improved depth estimation on transparent surfaces while preserving background details.

Conclusion: The method advances transparent scene reconstruction and is validated on a synthesized dataset.

Abstract: In this paper, we present a multi-label stereo matching method to
simultaneously estimate the depth of the transparent objects and the occluded
background in transparent scenes.Unlike previous methods that assume a unimodal
distribution along the disparity dimension and formulate the matching as a
single-label regression problem, we propose a multi-label regression
formulation to estimate multiple depth values at the same pixel in transparent
scenes. To resolve the multi-label regression problem, we introduce a
pixel-wise multivariate Gaussian representation, where the mean vector encodes
multiple depth values at the same pixel, and the covariance matrix determines
whether a multi-label representation is necessary for a given pixel. The
representation is iteratively predicted within a GRU framework. In each
iteration, we first predict the update step for the mean parameters and then
use both the update step and the updated mean parameters to estimate the
covariance matrix. We also synthesize a dataset containing 10 scenes and 89
objects to validate the performance of transparent scene depth estimation. The
experiments show that our method greatly improves the performance on
transparent surfaces while preserving the background information for scene
reconstruction. Code is available at https://github.com/BFZD233/TranScene.

</details>


### [257] [DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging](https://arxiv.org/pdf/2502.02171)
*Mohamed Youssef, Jian Peng, Oliver Bimber*

Main category: cs.CV

TL;DR: The paper introduces a method using aerial images and synthetic-aperture imaging to penetrate dense canopies, improving vegetation volume sensing by ~x7 on average.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of remote sensing in penetrating dense canopy layers for better ecosystem understanding.

Method: Uses synthetic-aperture imaging with drones and 3D CNNs to process focal stacks, reducing out-of-focus signals.

Result: Achieves ~x7 improvement in sensing accuracy (min: ~x2, max: ~x12) and an MSE of 0.05 in field experiments.

Conclusion: The approach effectively senses deep into vegetation, providing insights into plant health and environmental conditions.

Abstract: Access to below-canopy volumetric vegetation data is crucial for
understanding ecosystem dynamics. We address the long-standing limitation of
remote sensing to penetrate deep into dense canopy layers. LiDAR and radar are
currently considered the primary options for measuring 3D vegetation
structures, while cameras can only extract the reflectance and depth of top
layers. Using conventional, high-resolution aerial images, our approach allows
sensing deep into self-occluding vegetation volumes, such as forests. It is
similar in spirit to the imaging process of wide-field microscopy, but can
handle much larger scales and strong occlusion. We scan focal stacks by
synthetic-aperture imaging with drones and reduce outof-focus signal
contributions using pre-trained 3D convolutional neural networks with mean
squared error (MSE) as the loss function. The resulting volumetric reflectance
stacks contain low-frequency representations of the vegetation volume.
Combining multiple reflectance stacks from various spectral channels provides
insights into plant health, growth, and environmental conditions throughout the
entire vegetation volume. Compared with simulated ground truth, our correction
leads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of
200 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of
0.05 when comparing with the top-vegetation layer that was measured with
classical multispectral aerial imaging.

</details>


### [258] [UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache](https://arxiv.org/pdf/2505.14010)
*Pu Wang, Pengwen Dai, Chen Wu, Yeying Jin, Dianjie Lu, Guijuan Zhang, Youshan Zhang, Zhuoran Zheng*

Main category: cs.CV

TL;DR: Proposes an efficient visual transformer for UHD image dehazing, improving training speed and reducing memory usage with adaptive normalization and KV caching.


<details>
  <summary>Details</summary>
Motivation: Addresses slow training and high memory consumption in existing UHD image dehazing methods.

Method: Uses adaptive normalization and atmospheric scattering-aware KV caching for efficient training and feature preservation.

Result: Achieves 5x faster training convergence, real-time processing of 50 images/sec on RTX4090, and maintains state-of-the-art quality.

Conclusion: The method enhances computational efficiency for 4K/8K image restoration and introduces an interpretable dehazing technique.

Abstract: In this paper, we propose an efficient visual transformer framework for
ultra-high-definition (UHD) image dehazing that addresses the key challenges of
slow training speed and high memory consumption for existing methods. Our
approach introduces two key innovations: 1) an \textbf{a}daptive
\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables
ultra-fast and stable training with a network with a restricted range of
parameter expressions; and 2) we devise an atmospheric scattering-aware KV
caching mechanism that dynamically optimizes feature preservation based on the
physical haze formation model. The proposed architecture improves the training
convergence speed by \textbf{5 $\times$} while reducing memory overhead,
enabling real-time processing of 50 high-resolution images per second on an
RTX4090 GPU. Experimental results show that our approach maintains
state-of-the-art dehazing quality while significantly improving computational
efficiency for 4K/8K image restoration tasks. Furthermore, we provide a new
dehazing image interpretable method with the help of an integrated gradient
attribution map. Our code can be found here:
https://anonymous.4open.science/r/anDehazeFormer-632E/README.md.

</details>


### [259] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/pdf/2504.14783)
*Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang*

Main category: cs.CV

TL;DR: The paper introduces MIL-Dropout, a novel dropout method for Multiple Instance Learning (MIL), which improves performance and generalization by strategically dropping top-k important instances.


<details>
  <summary>Details</summary>
Motivation: The suboptimal two-stage training in MIL for WSI classification suffers from noisy features and weak supervision, prompting exploration of dropout's effectiveness.

Method: The authors propose MIL-Dropout, a method that systematically drops the top-k most important instances in a bag, enhancing performance and robustness.

Result: Experiments on five MIL benchmarks and two WSI datasets show MIL-Dropout improves performance with minimal computational overhead.

Conclusion: MIL-Dropout effectively addresses noisy features and weak supervision in MIL, offering a simple yet impactful enhancement to existing methods.

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>


### [260] [EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation](https://arxiv.org/pdf/2505.14014)
*Zelin Zhang, Tao Zhang, KediLI, Xu Zheng*

Main category: cs.CV

TL;DR: EGFormer is an efficient multimodal semantic segmentation framework that reduces parameters and inference time while maintaining performance, using dynamic modality ranking and filtering.


<details>
  <summary>Details</summary>
Motivation: Most methods focus on accuracy but neglect computational efficiency in multimodal semantic segmentation.

Method: EGFormer integrates an arbitrary number of modalities with two novel modules: Any-modal Scoring Module (ASM) for dynamic ranking and Modal Dropping Module (MDM) for filtering less informative modalities.

Result: EGFormer reduces parameters by 88% and GFLOPs by 50%, achieving competitive performance and state-of-the-art transfer results.

Conclusion: EGFormer efficiently balances performance and computational cost, demonstrating strong generalizability in transfer tasks.

Abstract: Recent efforts have explored multimodal semantic segmentation using various
backbone architectures. However, while most methods aim to improve accuracy,
their computational efficiency remains underexplored. To address this, we
propose EGFormer, an efficient multimodal semantic segmentation framework that
flexibly integrates an arbitrary number of modalities while significantly
reducing model parameters and inference time without sacrificing performance.
Our framework introduces two novel modules. First, the Any-modal Scoring Module
(ASM) assigns importance scores to each modality independently, enabling
dynamic ranking based on their feature maps. Second, the Modal Dropping Module
(MDM) filters out less informative modalities at each stage, selectively
preserving and aggregating only the most valuable features. This design allows
the model to leverage useful information from all available modalities while
discarding redundancy, thus ensuring high segmentation quality. In addition to
efficiency, we evaluate EGFormer on a synthetic-to-real transfer task to
demonstrate its generalizability. Extensive experiments show that EGFormer
achieves competitive performance with up to 88 percent reduction in parameters
and 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it
further achieves state-of-the-art transfer performance compared to existing
methods.

</details>


### [261] [OmniStyle: Filtering High Quality Style Transfer Data at Scale](https://arxiv.org/pdf/2505.14028)
*Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, Rui Ma*

Main category: cs.CV

TL;DR: OmniStyle-1M is a large-scale dataset for style transfer with over 1M triplets, enhanced by text and prompts. It enables efficient model training and precise stylization control. The OmniFilter ensures quality, and the OmniStyle framework (based on DiT) outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To advance high-quality style transfer by providing a scalable dataset and a robust framework for precise control and efficient training.

Method: Introduces OmniStyle-1M dataset with OmniFilter for quality assessment. Proposes OmniStyle framework using Diffusion Transformer (DiT) for instruction- and image-guided style transfer.

Result: OmniStyle achieves superior performance in qualitative and quantitative evaluations, demonstrating efficiency and versatility.

Conclusion: OmniStyle-1M and its methodologies significantly contribute to style transfer research, offering a valuable resource for the community.

Abstract: In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer
dataset comprising over one million content-style-stylized image triplets
across 1,000 diverse style categories, each enhanced with textual descriptions
and instruction prompts. We show that OmniStyle-1M can not only enable
efficient and scalable of style transfer models through supervised training but
also facilitate precise control over target stylization. Especially, to ensure
the quality of the dataset, we introduce OmniFilter, a comprehensive style
transfer quality assessment framework, which filters high-quality triplets
based on content preservation, style consistency, and aesthetic appeal.
Building upon this foundation, we propose OmniStyle, a framework based on the
Diffusion Transformer (DiT) architecture designed for high-quality and
efficient style transfer. This framework supports both instruction-guided and
image-guided style transfer, generating high resolution outputs with
exceptional detail. Extensive qualitative and quantitative evaluations
demonstrate OmniStyle's superior performance compared to existing approaches,
highlighting its efficiency and versatility. OmniStyle-1M and its accompanying
methodologies provide a significant contribution to advancing high-quality
style transfer, offering a valuable resource for the research community.

</details>


### [262] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/pdf/2505.14029)
*Laura-Sophia von Hirschhausen, Jannes S. Magnusson, Mykyta Kovalenko, Fredrik Boye, Tanay Rawat, Peter Eisert, Anna Hilsmann, Sebastian Pretzsch, Sebastian Bosse*

Main category: cs.CV

TL;DR: AppleGrowthVision is a large-scale dataset addressing gaps in apple orchard monitoring by providing diverse stereo images and annotated data, improving model performance for tasks like fruit detection and growth analysis.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity and realistic annotations for apple orchard monitoring, limiting tasks like 3D modeling and yield estimation.

Method: The dataset includes stereo images and densely annotated images from farms in Germany, covering growth stages and enabling 3D reconstruction.

Result: AppleGrowthVision improves YOLOv8 and Faster R-CNN performance significantly and achieves high accuracy in growth stage prediction.

Conclusion: The dataset bridges agricultural science and computer vision, supporting robust models for precision agriculture, with future work focused on annotation and 3D enhancements.

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [263] [Selective Structured State Space for Multispectral-fused Small Target Detection](https://arxiv.org/pdf/2505.14043)
*Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang*

Main category: cs.CV

TL;DR: The paper proposes enhancements to Mamba for small target detection in remote sensing, introducing ESTD and CARG modules for local detail capture and spatial-channel attention, along with MEPF for multispectral fusion.


<details>
  <summary>Details</summary>
Motivation: Challenges in small target detection include low accuracy and high computational costs with existing methods like Transformers and CNNs. Mamba's linear complexity is leveraged but needs improvement for small targets.

Method: Enhanced Mamba with ESTD (local attention) and CARG (spatial-channel attention) modules, plus MEPF for multispectral fusion to boost small target detection.

Result: Improved capture of fine local details and distinctive small target representations, enhancing detection accuracy.

Conclusion: The proposed modules (ESTD, CARG, MEPF) effectively address small target detection challenges, balancing efficiency and accuracy.

Abstract: Target detection in high-resolution remote sensing imagery faces challenges
due to the low recognition accuracy of small targets and high computational
costs. The computational complexity of the Transformer architecture increases
quadratically with image resolution, while Convolutional Neural Networks (CNN)
architectures are forced to stack deeper convolutional layers to expand their
receptive fields, leading to an explosive growth in computational demands. To
address these computational constraints, we leverage Mamba's linear complexity
for efficiency. However, Mamba's performance declines for small targets,
primarily because small targets occupy a limited area in the image and have
limited semantic information. Accurate identification of these small targets
necessitates not only Mamba's global attention capabilities but also the
precise capture of fine local details. To this end, we enhance Mamba by
developing the Enhanced Small Target Detection (ESTD) module and the
Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters
local attention to capture fine-grained details, while the CARG module, built
upon Mamba, emphasizes spatial and channel-wise information, collectively
improving the model's ability to capture distinctive representations of small
targets. Additionally, to highlight the semantic representation of small
targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for
multispectral fusion, which enhances target features by effectively fusing
visible and infrared multimodal information.

</details>


### [264] [Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification](https://arxiv.org/pdf/2505.14049)
*Yibo Gao, Hangqi Zhou, Zheyao Gao, Bomin Wang, Shangqi Gao, Sihan Wang, Xiahai Zhuang*

Main category: cs.CV

TL;DR: CRL is a framework for learning Boolean rules from visual concepts in medical imaging, improving interpretability and generalizability.


<details>
  <summary>Details</summary>
Motivation: Address concept leakages and lack of global interpretability in concept-based models for medical imaging.

Method: Uses logical layers to learn Boolean rules from binarized concepts, capturing correlations and providing local and global explanations.

Result: Achieves competitive performance and better generalizability on medical image tasks.

Conclusion: CRL enhances interpretability and generalizability in clinical decision-making.

Abstract: The pursuit of decision safety in clinical applications highlights the
potential of concept-based methods in medical imaging. While these models offer
active interpretability, they often suffer from concept leakages, where
unintended information within soft concept representations undermines both
interpretability and generalizability. Moreover, most concept-based models
focus solely on local explanations (instance-level), neglecting the global
decision logic (dataset-level). To address these limitations, we propose
Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules
from binarized visual concepts. CRL employs logical layers to capture concept
correlations and extract clinically meaningful rules, thereby providing both
local and global interpretability. Experiments on two medical image
classification tasks show that CRL achieves competitive performance with
existing methods while significantly improving generalizability to
out-of-distribution data. The code of our work is available at
https://github.com/obiyoag/crl.

</details>


### [265] [Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting](https://arxiv.org/pdf/2505.14059)
*Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, Jingqun Tang, Hao Liu, Can Huang*

Main category: cs.CV

TL;DR: Dolphin is a novel multimodal document image parsing model that uses a two-stage analyze-then-parse approach to efficiently handle complex layouts, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for document image parsing face challenges like integration overhead, efficiency bottlenecks, and degraded layout structures, despite decent performance.

Method: Dolphin employs a two-stage process: first generating layout elements in reading order, then parsing content in parallel using these elements as anchors with task-specific prompts.

Result: Dolphin achieves state-of-the-art performance on benchmarks and self-constructed datasets, with superior efficiency due to its lightweight architecture and parallel parsing.

Conclusion: Dolphin addresses limitations of existing methods, offering an efficient and high-performing solution for document image parsing.

Abstract: Document image parsing is challenging due to its complexly intertwined
elements such as text paragraphs, figures, formulas, and tables. Current
approaches either assemble specialized expert models or directly generate
page-level content autoregressively, facing integration overhead, efficiency
bottlenecks, and layout structure degradation despite their decent performance.
To address these limitations, we present \textit{Dolphin}
(\textit{\textbf{Do}cument Image \textbf{P}arsing via \textbf{H}eterogeneous
Anchor Prompt\textbf{in}g}), a novel multimodal document image parsing model
following an analyze-then-parse paradigm. In the first stage, Dolphin generates
a sequence of layout elements in reading order. These heterogeneous elements,
serving as anchors and coupled with task-specific prompts, are fed back to
Dolphin for parallel content parsing in the second stage. To train Dolphin, we
construct a large-scale dataset of over 30 million samples, covering
multi-granularity parsing tasks. Through comprehensive evaluations on both
prevalent benchmarks and self-constructed ones, Dolphin achieves
state-of-the-art performance across diverse page-level and element-level
settings, while ensuring superior efficiency through its lightweight
architecture and parallel parsing mechanism. The code and pre-trained models
are publicly available at https://github.com/ByteDance/Dolphin

</details>


### [266] [Scaling Vision Mamba Across Resolutions via Fractal Traversal](https://arxiv.org/pdf/2505.14062)
*Bo Li, Haoke Xiao, Lv Tang*

Main category: cs.CV

TL;DR: FractalMamba++ improves Vision Mamba by using fractal-based patch serialization and CSR for better scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in 2D-to-1D patch serialization and weak scalability in Vision Mamba.

Method: Uses Hilbert curves for patch serialization, CSR for global context, and PRC for local adjacency.

Result: Outperforms previous Mamba-based backbones in tasks like classification and detection, especially at high resolutions.

Conclusion: FractalMamba++ is a robust vision backbone with superior scalability and performance.

Abstract: Vision Mamba has recently emerged as a promising alternative to
Transformer-based architectures, offering linear complexity in sequence length
while maintaining strong modeling capacity. However, its adaptation to visual
inputs is hindered by challenges in 2D-to-1D patch serialization and weak
scalability across input resolutions. Existing serialization strategies such as
raster scanning disrupt local spatial continuity and limit the model's ability
to generalize across scales. In this paper, we propose FractalMamba++, a robust
vision backbone that leverages fractal-based patch serialization via Hilbert
curves to preserve spatial locality and enable seamless resolution
adaptability. To address long-range dependency fading in high-resolution
inputs, we further introduce a Cross-State Routing (CSR) mechanism that
enhances global context propagation through selective state reuse.
Additionally, we propose a Positional-Relation Capture (PRC) module to recover
local adjacency disrupted by curve inflection points. Extensive experiments on
image classification, semantic segmentation, object detection, and change
detection demonstrate that FractalMamba++ consistently outperforms previous
Mamba-based backbones, particularly under high-resolution settings.

</details>


### [267] [Place Recognition: A Comprehensive Review, Current Challenges and Future Directions](https://arxiv.org/pdf/2505.14068)
*Zhenyu Li, Tianyi Shang, Pengjie Xu, Zhaojun Deng*

Main category: cs.CV

TL;DR: The paper surveys recent advancements in place recognition, focusing on CNN-based, Transformer-based, and cross-modal methods, and discusses challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Place recognition is crucial for tasks like SLAM and long-term navigation, especially under varying conditions. The paper aims to review and unify recent methodological advancements.

Method: The survey reviews three paradigms: CNN-based approaches for robust visual descriptors, Transformer-based models for global dependencies, and cross-modal strategies integrating Lidar, vision, and text.

Result: The paper summarizes standard datasets, evaluation metrics, and provides a unified framework (code library) for place recognition methods.

Conclusion: Challenges like domain adaptation, real-time performance, and lifelong learning are identified, with a call for future research in these areas.

Abstract: Place recognition is a cornerstone of vehicle navigation and mapping, which
is pivotal in enabling systems to determine whether a location has been
previously visited. This capability is critical for tasks such as loop closure
in Simultaneous Localization and Mapping (SLAM) and long-term navigation under
varying environmental conditions. In this survey, we comprehensively review
recent advancements in place recognition, emphasizing three representative
methodological paradigms: Convolutional Neural Network (CNN)-based approaches,
Transformer-based frameworks, and cross-modal strategies. We begin by
elucidating the significance of place recognition within the broader context of
autonomous systems. Subsequently, we trace the evolution of CNN-based methods,
highlighting their contributions to robust visual descriptor learning and
scalability in large-scale environments. We then examine the emerging class of
Transformer-based models, which leverage self-attention mechanisms to capture
global dependencies and offer improved generalization across diverse scenes.
Furthermore, we discuss cross-modal approaches that integrate heterogeneous
data sources such as Lidar, vision, and text description, thereby enhancing
resilience to viewpoint, illumination, and seasonal variations. We also
summarize standard datasets and evaluation metrics widely adopted in the
literature. Finally, we identify current research challenges and outline
prospective directions, including domain adaptation, real-time performance, and
lifelong learning, to inspire future advancements in this domain. The unified
framework of leading-edge place recognition methods, i.e., code library, and
the results of their experimental evaluations are available at
https://github.com/CV4RA/SOTA-Place-Recognitioner.

</details>


### [268] [Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts](https://arxiv.org/pdf/2505.14088)
*Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang*

Main category: cs.CV

TL;DR: Land-MoE introduces a parameter-efficient method for multispectral land cover classification, addressing spectral shifts with hierarchical Frequency-aware Mixture of Low-rank Token Experts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Spectral shifts from sensor and geospatial disparities challenge MLCC. Existing methods rely on small-scale models with limited performance.

Method: Land-MoE uses Mixture of Low-rank Token Experts (MoLTE) for diverse feature adjustments and Frequency-aware Filters (FAF) for frequency-domain modulation.

Result: Outperforms existing methods in cross-sensor and cross-geospatial MLCC tasks and achieves state-of-the-art in RGB remote sensing segmentation.

Conclusion: Land-MoE effectively addresses spectral shifts and enhances performance in MLCC and related tasks.

Abstract: We introduce Land-MoE, a novel approach for multispectral land cover
classification (MLCC). Spectral shift, which emerges from disparities in
sensors and geospatial conditions, poses a significant challenge in this
domain. Existing methods predominantly rely on domain adaptation and
generalization strategies, often utilizing small-scale models that exhibit
limited performance. In contrast, Land-MoE addresses these issues by
hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,
to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.
Specifically, Land-MoE comprises two key modules: the mixture of low-rank token
experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages
rank-differentiated tokens to generate diverse feature adjustments for
individual instances within multispectral images. By dynamically combining
learnable low-rank token experts of varying ranks, it enhances the robustness
against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on
the refined features. This process enables the model to effectively capture
frequency band information that is strongly correlated with semantic essence,
while simultaneously suppressing frequency noise irrelevant to the task.
Comprehensive experiments on MLCC tasks involving cross-sensor and
cross-geospatial setups demonstrate that Land-MoE outperforms existing methods
by a large margin. Additionally, the proposed approach has also achieved
state-of-the-art performance in domain generalization semantic segmentation
tasks of RGB remote sensing images.

</details>


### [269] [Unlocking the Power of SAM 2 for Few-Shot Segmentation](https://arxiv.org/pdf/2505.14100)
*Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao*

Main category: cs.CV

TL;DR: The paper addresses overfitting in Few-Shot Segmentation (FSS) by leveraging SAM 2's video segmentation capabilities, introducing a Pseudo Prompt Generator and Iterative Memory Refinement to improve matching and segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Overfitting in FSS and the incompatibility of SAM 2's class-agnostic matching with FSS's diverse identities motivated the design of new methods to enhance segmentation accuracy.

Method: Proposes Pseudo Prompt Generator for compatible memory matching and Iterative Memory Refinement to improve memory accuracy by fusing more query foreground features and suppressing background noise.

Result: Achieves a 4.2% improvement in 1-shot mIoU over the best baseline on PASCAL-5i and COCO-20i datasets.

Conclusion: The introduced methods effectively address the limitations of SAM 2 in FSS, improving segmentation performance through better memory handling and noise suppression.

Abstract: Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few
classes to segment arbitrary classes, but at the risk of overfitting. To
address this, some methods use the well-learned knowledge of foundation models
(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM
by supporting video segmentation, whose class-agnostic matching ability is
useful to FSS. A simple idea is to encode support foreground (FG) features as
memory, with which query FG features are matched and fused. Unfortunately, the
FG objects in different frames of SAM 2's video data are always the same
identity, while those in FSS are different identities, i.e., the matching step
is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo
query memory, matching with query features in a compatible way. However, the
memories can never be as accurate as the real ones, i.e., they are likely to
contain incomplete query FG, and some unexpected query background (BG)
features, leading to wrong segmentation. Hence, we further design Iterative
Memory Refinement to fuse more query FG features into the memory, and devise a
Support-Calibrated Memory Attention to suppress the unexpected query BG
features in memory. Extensive experiments have been conducted on PASCAL-5$^i$
and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot
mIoU can be 4.2\% better than the best baseline.

</details>


### [270] [Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry](https://arxiv.org/pdf/2505.14105)
*Zsófia Molnár, Gergely Szabó, András Horváth*

Main category: cs.CV

TL;DR: The paper investigates biases in supervised pretrained models for image segmentation, especially in biomedical imaging, and proposes methods to mitigate them.


<details>
  <summary>Details</summary>
Motivation: Pretrained models introduce unintended biases in specialized datasets like biomedical imaging, causing inconsistent feature utilization and reduced reliability.

Method: The study compares pretrained and randomly initialized models, analyzing performance and saliency maps, and proposes bias-neutralizing strategies.

Result: The proposed methods effectively mitigate biases, improving model explainability without losing the benefits of pretrained models.

Conclusion: The findings offer practical solutions to address pretrained weight biases, enhancing reliability in deep learning tasks.

Abstract: Supervised pretrained models have become widely used in deep learning,
especially for image segmentation tasks. However, when applied to specialized
datasets such as biomedical imaging, pretrained weights often introduce
unintended biases. These biases cause models to assign different levels of
importance to different slices, leading to inconsistencies in feature
utilization, which can be observed as asymmetries in saliency map
distributions. This transfer of color distributions from natural images to
non-natural datasets can compromise model performance and reduce the
reliability of results. In this study, we investigate the effects of these
biases and propose strategies to mitigate them. Through a series of
experiments, we test both pretrained and randomly initialized models, comparing
their performance and saliency map distributions. Our proposed methods, which
aim to neutralize the bias introduced by pretrained color channel weights,
demonstrate promising results, offering a practical approach to improving model
explainability while maintaining the benefits of pretrained models. This
publication presents our findings, providing insights into addressing
pretrained weight biases across various deep learning tasks.

</details>


### [271] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/pdf/2505.14113)
*Bruno Viti, Elias Karabelas, Martin Holler*

Main category: cs.CV

TL;DR: CONSIGN improves uncertainty quantification in image segmentation by incorporating spatial correlations, outperforming standard conformal prediction methods.


<details>
  <summary>Details</summary>
Motivation: Standard pixel-wise confidence scores in segmentation lack rigorous uncertainty estimates, and ignoring spatial correlations leads to conservative results.

Method: Proposes CONSIGN, a conformal prediction method that leverages spatial groupings to refine uncertainty estimates, compatible with any pre-trained segmentation model.

Result: Outperforms standard CP methods across medical and COCO datasets, enhancing uncertainty quality and performance metrics.

Conclusion: CONSIGN effectively integrates spatial correlations for better uncertainty quantification in segmentation, validated by empirical results.

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [272] [Intra-class Patch Swap for Self-Distillation](https://arxiv.org/pdf/2505.14124)
*Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga*

Main category: cs.CV

TL;DR: A novel teacher-free knowledge distillation method using intra-class patch swap augmentation for efficient model compression without auxiliary components or architectural changes.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional KD (memory, training costs, teacher selection) and existing self-distillation methods (complexity, lack of generality).

Method: Intra-class patch swap augmentation simulates teacher-student dynamics within a single model, aligning predictive distributions via instance-to-instance distillation.

Result: Outperforms self-distillation baselines and teacher-based KD in image classification, semantic segmentation, and object detection.

Conclusion: Success of self-distillation may depend on augmentation design; the method is simple, model-agnostic, and effective.

Abstract: Knowledge distillation (KD) is a valuable technique for compressing large
deep learning models into smaller, edge-suitable networks. However,
conventional KD frameworks rely on pre-trained high-capacity teacher networks,
which introduce significant challenges such as increased memory/storage
requirements, additional training costs, and ambiguity in selecting an
appropriate teacher for a given student model. Although a teacher-free
distillation (self-distillation) has emerged as a promising alternative, many
existing approaches still rely on architectural modifications or complex
training procedures, which limit their generality and efficiency.
  To address these limitations, we propose a novel framework based on
teacher-free distillation that operates using a single student network without
any auxiliary components, architectural modifications, or additional learnable
parameters. Our approach is built on a simple yet highly effective
augmentation, called intra-class patch swap augmentation. This augmentation
simulates a teacher-student dynamic within a single model by generating pairs
of intra-class samples with varying confidence levels, and then applying
instance-to-instance distillation to align their predictive distributions. Our
method is conceptually simple, model-agnostic, and easy to implement, requiring
only a single augmentation function. Extensive experiments across image
classification, semantic segmentation, and object detection show that our
method consistently outperforms both existing self-distillation baselines and
conventional teacher-based KD approaches. These results suggest that the
success of self-distillation could hinge on the design of the augmentation
itself. Our codes are available at
https://github.com/hchoi71/Intra-class-Patch-Swap.

</details>


### [273] [Hunyuan-Game: Industrial-grade Intelligent Game Creation Model](https://arxiv.org/pdf/2505.14135)
*Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, Donghao Li, Wenqing Yu, Senbo Wang, Zhimin Li, Yetshuan Shi, Haoyu Yang, Yukun Wang, Wenxun Dai, Jiaqi Li, Linqing Wang, Qixun Wang, Zhiyong Xu, Yingfang Zhang, Jiangfeng Xiong, Weijie Kong, Chao Zhang, Hongxin Zhang, Qiaoling Zheng, Weiting Guo, Xinchi Deng, Yixuan Li, Renjia Wei, Yulin Jian, Duojun Huang, Xuhua Ren, Sihuan Lin, Yifu Sun, Yuan Zhou, Joey Wang, Qin Lin, Jingmiao Yu, Jihong Zhang, Caesar Zhong, Di Wang, Yuhong Liu, Linus, Jie Jiang, Longhuang Wu, Shuai Shao, Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-Game leverages generative AI to create high-quality game assets (images/videos) tailored to player preferences and designer efficiency, addressing challenges in game content synthesis.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in synthesizing high-fidelity game assets and enhance game development efficiency.

Method: Uses customized models for image (text-to-image, visual effects, transparent images, character generation) and video (image-to-video, pose synthesis, dynamic illustrations, super-resolution, interactive videos) generation, trained on extensive datasets.

Result: Developed a suite of models capable of generating diverse, high-quality game assets with deep domain-specific knowledge.

Conclusion: Hunyuan-Game represents a significant advancement in intelligent game creation, offering systematic solutions for dynamic content generation.

Abstract: Intelligent game creation represents a transformative advancement in game
development, utilizing generative artificial intelligence to dynamically
generate and enhance game content. Despite notable progress in generative
models, the comprehensive synthesis of high-quality game assets, including both
images and videos, remains a challenging frontier. To create high-fidelity game
content that simultaneously aligns with player preferences and significantly
boosts designer efficiency, we present Hunyuan-Game, an innovative project
designed to revolutionize intelligent game production. Hunyuan-Game encompasses
two primary branches: image generation and video generation. The image
generation component is built upon a vast dataset comprising billions of game
images, leading to the development of a group of customized image generation
models tailored for game scenarios: (1) General Text-to-Image Generation. (2)
Game Visual Effects Generation, involving text-to-effect and reference
image-based game visual effect generation. (3) Transparent Image Generation for
characters, scenes, and game visual effects. (4) Game Character Generation
based on sketches, black-and-white images, and white models. The video
generation component is built upon a comprehensive dataset of millions of game
and anime videos, leading to the development of five core algorithmic models,
each targeting critical pain points in game development and having robust
adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)
360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)
Generative Video Super-Resolution. (5) Interactive Game Video Generation. These
image and video generation models not only exhibit high-level aesthetic
expression but also deeply integrate domain-specific knowledge, establishing a
systematic understanding of diverse game and anime art styles.

</details>


### [274] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/pdf/2505.14156)
*Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan*

Main category: cs.CV

TL;DR: The paper introduces Symbolic Graph Ranker (SGR), combining text-based and graph-based approaches using LLMs to model session search interactions, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current session search methods lack integration of graph structures and word-level semantics, prompting the need for a hybrid approach leveraging LLMs.

Method: SGR uses symbolic grammar rules to convert session graphs into text for LLM input, supplemented by self-supervised tasks (link prediction, node generation, contrastive learning) to enhance graph understanding.

Result: Experiments on AOL and Tiangong-ST datasets show SGR's superiority in capturing topological information and improving search performance.

Conclusion: SGR bridges traditional search and modern LLMs, offering a novel methodology for session search with enhanced graph-text integration.

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [275] [M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data](https://arxiv.org/pdf/2505.14159)
*Junjie Li, Jiawei Wang, Miyu Li, Yu Liu, Yumei Wang, Haitao Xu*

Main category: cs.CV

TL;DR: M3Depth, a learning-based stereo depth estimation model for Mars rovers, improves accuracy by 16% using wavelet transforms and geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in sparse-textured, unstructured Martian terrains for better obstacle avoidance and navigation.

Method: Incorporates wavelet-based convolutional kernels for low-frequency features, a consistency loss for depth-normal complementarity, and a pixel-wise refinement module.

Result: Achieves 16% better accuracy on synthetic Mars datasets and shows strong real-world applicability.

Conclusion: M3Depth offers a promising solution for future Mars exploration missions.

Abstract: Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with sparse textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the sparse and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a significant 16% improvement in depth
estimation accuracy compared to other state-of-the-art methods in depth
estimation. Furthermore, the model demonstrates strong applicability in
real-world Martian scenarios, offering a promising solution for future Mars
exploration missions.

</details>


### [276] [LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer](https://arxiv.org/pdf/2505.14167)
*Changgu Chen, Xiaoyan Yang, Junwei Shu, Changbo Wang, Yang Li*

Main category: cs.CV

TL;DR: The paper proposes the Leveraging Motion Prior (LMP) framework to enhance fine-grained motion control in video generation using pre-trained diffusion transformers.


<details>
  <summary>Details</summary>
Motivation: Current DiT models lack fine-grained control over video content, especially in complex motion scenarios and image-to-video generation.

Method: LMP introduces foreground-background disentanglement, reweighted motion transfer, and appearance separation modules to reference motion from user-provided videos.

Result: Experiments show state-of-the-art performance in generation quality, prompt-video consistency, and control capability.

Conclusion: LMP effectively addresses motion control challenges in video generation, offering improved flexibility and accuracy.

Abstract: In recent years, large-scale pre-trained diffusion transformer models have
made significant progress in video generation. While current DiT models can
produce high-definition, high-frame-rate, and highly diverse videos, there is a
lack of fine-grained control over the video content. Controlling the motion of
subjects in videos using only prompts is challenging, especially when it comes
to describing complex movements. Further, existing methods fail to control the
motion in image-to-video generation, as the subject in the reference image
often differs from the subject in the reference video in terms of initial
position, size, and shape. To address this, we propose the Leveraging Motion
Prior (LMP) framework for zero-shot video generation. Our framework harnesses
the powerful generative capabilities of pre-trained diffusion transformers to
enable motion in the generated videos to reference user-provided motion videos
in both text-to-video and image-to-video generation. To this end, we first
introduce a foreground-background disentangle module to distinguish between
moving subjects and backgrounds in the reference video, preventing interference
in the target video generation. A reweighted motion transfer module is designed
to allow the target video to reference the motion from the reference video. To
avoid interference from the subject in the reference video, we propose an
appearance separation module to suppress the appearance of the reference
subject in the target video. We annotate the DAVIS dataset with detailed
prompts for our experiments and design evaluation metrics to validate the
effectiveness of our method. Extensive experiments demonstrate that our
approach achieves state-of-the-art performance in generation quality,
prompt-video consistency, and control capability. Our homepage is available at
https://vpx-ecnu.github.io/LMP-Website/

</details>


### [277] [Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method](https://arxiv.org/pdf/2505.14197)
*Xinshen Zhang, Zhen Ye, Xu Zheng*

Main category: cs.CV

TL;DR: The paper introduces OmniVQA, the first dataset and benchmark for omnidirectional visual question answering (VQA), revealing limitations in current MLLMs and proposing a reinforcement learning method (360-R1) for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with comprehending omnidirectional images (ODIs), which are crucial for immersive applications like AR and embodied AI. This gap in capability motivated the creation of OmniVQA.

Method: The paper introduces OmniVQA dataset and benchmark, then proposes 360-R1, a rule-based reinforcement learning method with three novel reward functions: reasoning process similarity, answer semantic accuracy, and structured format compliance.

Result: Evaluation shows current MLLMs perform poorly in omnidirectional VQA. 360-R1 achieves a +6% improvement over baselines.

Conclusion: The study highlights the need for dedicated innovations in MLLMs for 360° imagery and demonstrates the effectiveness of 360-R1 in addressing these challenges.

Abstract: Omnidirectional images (ODIs), with their 360{\deg} field of view, provide
unparalleled spatial awareness for immersive applications like augmented
reality and embodied AI. However, the capability of existing multi-modal large
language models (MLLMs) to comprehend and reason about such panoramic scenes
remains underexplored. This paper addresses this gap by introducing OmniVQA,
the first dataset and conducting the first benchmark for omnidirectional visual
question answering. Our evaluation of state-of-the-art MLLMs reveals
significant limitations in handling omnidirectional visual question answering,
highlighting persistent challenges in object localization, feature extraction,
and hallucination suppression within panoramic contexts. These results
underscore the disconnect between current MLLM capabilities and the demands of
omnidirectional visual understanding, which calls for dedicated architectural
or training innovations tailored to 360{\deg} imagery. Building on the OmniVQA
dataset and benchmark, we further introduce a rule-based reinforcement learning
method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group
relative policy optimization (GRPO) by proposing three novel reward functions:
(1) reasoning process similarity reward, (2) answer semantic accuracy reward,
and (3) structured format compliance reward. Extensive experiments on our
OmniVQA demonstrate the superiority of our proposed method in omnidirectional
space (+6% improvement).

</details>


### [278] [Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment](https://arxiv.org/pdf/2505.14204)
*Yang Hu, Runchen Wang, Stephen Chong Zhao, Xuhui Zhan, Do Hun Kim, Mark Wallace, David A. Tovar*

Main category: cs.CV

TL;DR: Perceptual-Initialization (PI) integrates human perceptual structure early in visual representation learning, improving zero-shot performance without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Challenges the conventional use of human-perceptual data for fine-tuning by showing early integration yields better generalization.

Method: Uses human-derived triplet embeddings from NIGHTS to initialize a CLIP vision encoder, followed by self-supervised learning on YFCC15M.

Result: Significant zero-shot improvements across 29 classification and 2 retrieval benchmarks, with gains on ImageNet-1K after ~15 epochs.

Conclusion: Early human perceptual integration enhances vision-language alignment and generalization, providing a stronger foundation for AI systems.

Abstract: We introduce Perceptual-Initialization (PI), a paradigm shift in visual
representation learning that incorporates human perceptual structure during the
initialization phase rather than as a downstream fine-tuning step. By
integrating human-derived triplet embeddings from the NIGHTS dataset to
initialize a CLIP vision encoder, followed by self-supervised learning on
YFCC15M, our approach demonstrates significant zero-shot performance
improvements, without any task-specific fine-tuning, across 29 zero shot
classification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains
emerge after approximately 15 epochs of pretraining. Benefits are observed
across datasets of various scales, with improvements manifesting at different
stages of the pretraining process depending on dataset characteristics. Our
approach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and
retrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,
without requiring any adaptation to target domains. These findings challenge
the conventional wisdom of using human-perceptual data primarily for
fine-tuning and demonstrate that embedding human perceptual structure during
early representation learning yields more capable and vision-language aligned
systems that generalize immediately to unseen tasks. Our work shows that
"beginning with you", starting with human perception, provides a stronger
foundation for general-purpose vision-language intelligence.

</details>


### [279] [Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion](https://arxiv.org/pdf/2505.14218)
*Jie Li, Shengwei Tian, Long Yu, Xin Ning*

Main category: cs.CV

TL;DR: The paper proposes Flexible-Weighted Chamfer Distance (FCD) to improve point cloud generation by dynamically balancing global and local performance, outperforming standard Chamfer Distance (CD) in metrics and human evaluations.


<details>
  <summary>Details</summary>
Motivation: Standard CD's fixed weights for global and local components often lead to poor global distribution despite low CD scores, evidenced by high EMD and DCD scores and poor human assessments.

Method: Introduces FCD, which assigns higher weight to global distribution and uses flexible weighting to balance components, enhancing global performance while maintaining overall quality.

Result: FCD outperforms CD in multiple metrics (CD, EMD, DCD, F-Score) and human evaluations on two state-of-the-art networks.

Conclusion: FCD effectively addresses CD's limitations, improving global distribution and overall performance in point cloud generation tasks.

Abstract: Chamfer Distance (CD) comprises two components that can evaluate the global
distribution and local performance of generated point clouds, making it widely
utilized as a similarity measure between generated and target point clouds in
point cloud completion tasks. Additionally, CD's computational efficiency has
led to its frequent application as an objective function for guiding point
cloud generation. However, using CD directly as an objective function with
fixed equal weights for its two components can often result in seemingly high
overall performance (i.e., low CD score), while failing to achieve a good
global distribution. This is typically reflected in high Earth Mover's Distance
(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human
assessments. To address this issue, we propose a Flexible-Weighted Chamfer
Distance (FCD) to guide point cloud generation. FCD assigns a higher weight to
the global distribution component of CD and incorporates a flexible weighting
strategy to adjust the balance between the two components, aiming to improve
global distribution while maintaining robust overall performance. Experimental
results on two state-of-the-art networks demonstrate that our method achieves
superior results across multiple evaluation metrics, including CD, EMD, DCD,
and F-Score, as well as in human evaluations.

</details>


### [280] [VoQA: Visual-only Question Answering](https://arxiv.org/pdf/2505.14227)
*Luyang Jiang, Jianing An, Jie Luo, Wenjun Wu, Lei Huang*

Main category: cs.CV

TL;DR: The paper introduces Visual-only Question Answering (VoQA), a task where questions are embedded in images without text, challenging existing models. It proposes GRT-SFT, a fine-tuning method to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current vision-language models in handling visually embedded questions without textual input.

Method: Proposes Guided Response Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy for step-by-step visual reasoning.

Result: GRT-SFT significantly improves model performance on VoQA tasks.

Conclusion: The work advances models' ability for human-like visual understanding in complex multimodal scenarios.

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [281] [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning](https://arxiv.org/pdf/2505.14231)
*Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, Yansong Tang*

Main category: cs.CV

TL;DR: UniVG-R1 is a multimodal large language model for universal visual grounding, enhancing reasoning via reinforcement learning and a Chain-of-Thought dataset, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of extending visual grounding to real-world scenarios with complex instructions and multiple images, requiring advanced reasoning.

Method: Constructs a CoT grounding dataset for supervised fine-tuning, uses rule-based RL to improve reasoning, and introduces a difficulty-aware weight adjustment strategy.

Result: Achieves 9.1% improvement on MIG-Bench and 23.4% average zero-shot improvement across four benchmarks.

Conclusion: UniVG-R1 demonstrates strong generalizability and effectiveness in universal visual grounding.

Abstract: Traditional visual grounding methods primarily focus on single-image
scenarios with simple textual references. However, extending these methods to
real-world scenarios that involve implicit and complex instructions,
particularly in conjunction with multiple images, poses significant challenges,
which is mainly due to the lack of advanced reasoning ability across diverse
multi-modal contexts. In this work, we aim to address the more practical
universal grounding task, and propose UniVG-R1, a reasoning guided multimodal
large language model (MLLM) for universal visual grounding, which enhances
reasoning capabilities through reinforcement learning (RL) combined with
cold-start data. Specifically, we first construct a high-quality
Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning
chains, to guide the model towards correct reasoning paths via supervised
fine-tuning. Subsequently, we perform rule-based reinforcement learning to
encourage the model to identify correct reasoning chains, thereby incentivizing
its reasoning capabilities. In addition, we identify a difficulty bias arising
from the prevalence of easy samples as RL training progresses, and we propose a
difficulty-aware weight adjustment strategy to further strengthen the
performance. Experimental results demonstrate the effectiveness of UniVG-R1,
which achieves state-of-the-art performance on MIG-Bench with a 9.1%
improvement over the previous method. Furthermore, our model exhibits strong
generalizability, achieving an average improvement of 23.4% in zero-shot
performance across four image and video reasoning grounding benchmarks. The
project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.

</details>


### [282] [Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation](https://arxiv.org/pdf/2505.14239)
*Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, Chengjie Wang*

Main category: cs.CV

TL;DR: The paper addresses bias classification in few-shot object detection (FSOD) and instance segmentation (FSIS) caused by missing labels, proposing a simple decoupled classifier method to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing FSOD and FSIS methods suffer from bias classification due to missing labels in few-shot scenarios, a problem first formally identified by the authors.

Method: The authors decouple the standard classifier into two heads to separately handle clear positive samples and noisy negative samples caused by missing labels.

Result: The proposed method outperforms baselines and state-of-the-art models on PASCAL VOC and MS-COCO benchmarks without additional computational cost.

Conclusion: Decoupling the classifier effectively mitigates bias classification, enabling better learning of novel classes in FSOD and FSIS tasks.

Abstract: This paper focus on few-shot object detection~(FSOD) and instance
segmentation~(FSIS), which requires a model to quickly adapt to novel classes
with a few labeled instances. The existing methods severely suffer from bias
classification because of the missing label issue which naturally exists in an
instance-level few-shot scenario and is first formally proposed by us. Our
analysis suggests that the standard classification head of most FSOD or FSIS
models needs to be decoupled to mitigate the bias classification. Therefore, we
propose an embarrassingly simple but effective method that decouples the
standard classifier into two heads. Then, these two individual heads are
capable of independently addressing clear positive samples and noisy negative
samples which are caused by the missing label. In this way, the model can
effectively learn novel classes while mitigating the effects of noisy negative
samples. Without bells and whistles, our model without any additional
computation cost and parameters consistently outperforms its baseline and
state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for
FSOD and FSIS tasks. The Code is available at
https://csgaobb.github.io/Projects/DCFS.

</details>


### [283] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.14246)
*Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang*

Main category: cs.CV

TL;DR: Visual-ARFT enhances LVLMs with agentic abilities for image-based reasoning, outperforming baselines and GPT-4o on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in multi-modal agentic capabilities, especially for image-based reasoning, in open-source LVLMs.

Method: Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) enables LVLMs to browse websites and manipulate images via coding.

Result: Visual-ARFT outperforms baselines by significant margins (+18.6% F1 on MAT-Coding, +10.3% F1 on MAT-Search) and surpasses GPT-4o.

Conclusion: Visual-ARFT is a promising approach for developing robust, generalizable multimodal agents.

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [284] [Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization](https://arxiv.org/pdf/2505.14254)
*Yuanyuan Chang, Yinghua Yao, Tao Qin, Mengmeng Wang, Ivor Tsang, Guang Dai*

Main category: cs.CV

TL;DR: The paper proposes optimizing semantic embeddings using attribute classifiers for text-to-image editing, eliminating the need for manual text prompts or model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image editing methods rely on manual prompt crafting, which is inefficient and limits performance.

Method: The approach uses attribute classifiers to learn optimal semantic embeddings for steering edits without text prompts or model training.

Result: The method achieves high disentanglement and generalization across data domains.

Conclusion: The proposed technique offers a more efficient and accurate alternative to text-prompt-based editing.

Abstract: Text-to-image diffusion models have emerged as powerful tools for
high-quality image generation and editing. Many existing approaches rely on
text prompts as editing guidance. However, these methods are constrained by the
need for manual prompt crafting, which can be time-consuming, introduce
irrelevant details, and significantly limit editing performance. In this work,
we propose optimizing semantic embeddings guided by attribute classifiers to
steer text-to-image models toward desired edits, without relying on text
prompts or requiring any training or fine-tuning of the diffusion model. We
utilize classifiers to learn precise semantic embeddings at the dataset level.
The learned embeddings are theoretically justified as the optimal
representation of attribute semantics, enabling disentangled and accurate
edits. Experiments further demonstrate that our method achieves high levels of
disentanglement and strong generalization across different domains of data.

</details>


### [285] [Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models](https://arxiv.org/pdf/2505.14257)
*Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng*

Main category: cs.CV

TL;DR: The paper addresses misalignment in LVLMs between attention distribution and actual information flow, proposing a method to enhance visual understanding by leveraging core semantic representations, reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: The misalignment between attention distribution and information flow in LVLMs undermines visual understanding and causes hallucinations.

Method: Identify attention heads focusing on core semantic representations and propagate their advantages via a two-stage optimization paradigm.

Result: Significantly reduces hallucinations in LVLMs, with a trade-off between reduced hallucinations and richer details.

Conclusion: The method effectively aligns attention with information flow, offering flexible control over model conservativeness for diverse requirements.

Abstract: Due to the unidirectional masking mechanism, Decoder-Only models propagate
information from left to right. LVLMs (Large Vision-Language Models) follow the
same architecture, with visual information gradually integrated into semantic
representations during forward propagation. Through systematic analysis, we
observe that over 80\% of the visual information is absorbed into the semantic
representations. However, the model's attention still predominantly focuses on
the visual representations. This misalignment between the attention
distribution and the actual information flow undermines the model's visual
understanding ability and contributes to hallucinations. To address this issue,
we enhance the model's visual understanding by leveraging the core information
embedded in semantic representations. Specifically, we identify attention heads
that focus on core semantic representations based on their attention
distributions. Then, through a two-stage optimization paradigm, we propagate
the advantages of these attention heads across the entire model, aligning the
attention distribution with the actual information flow. We evaluate our method
on three image captioning benchmarks using five different LVLMs, demonstrating
its effectiveness in significantly reducing hallucinations. Further experiments
reveal a trade-off between reduced hallucinations and richer details. Notably,
our method allows for manual adjustment of the model's conservativeness,
enabling flexible control to meet diverse real-world requirements. Code will be
released once accepted.

</details>


### [286] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)
*Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji*

Main category: cs.CV

TL;DR: Multimodal Speculative Decoding (MSD) accelerates Multimodal Large Language Models (MLLMs) by decoupling text and visual tokens and using a two-stage training strategy, achieving up to 2.46× speedup.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods for MLLMs underperform compared to LLMs due to unaddressed differences in text and visual token processing and insufficient multimodal training.

Method: MSD decouples text and visual tokens in drafting and employs a two-stage training strategy: text-only instruction-tuning followed by gradual multimodal data introduction.

Result: MSD achieves speedups of up to 2.29× for LLaVA-1.5-7B and 2.46× for LLaVA-1.5-13B on multimodal benchmarks.

Conclusion: MSD effectively accelerates MLLM inference by addressing token processing differences and enhancing multimodal capabilities, with demonstrated performance improvements.

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [287] [RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual Data](https://arxiv.org/pdf/2505.14270)
*Yoorhim Cho, Hongyeob Kim, Semin Kim, Youjia Zhang, Yunseok Choi, Sungeun Hong*

Main category: cs.CV

TL;DR: RA-Touch improves visuo-tactile perception by using visual data with tactile semantics, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Tactile data collection is costly, but visually distinct objects can share tactile properties, suggesting visual data can guide tactile understanding.

Method: RA-Touch retrieves and integrates visual-textual representations aligned with tactile inputs, leveraging a recaptioned visual dataset.

Result: Outperforms prior methods on the TVL benchmark, demonstrating retrieval-based visual reuse for tactile understanding.

Conclusion: RA-Touch shows potential for enhancing tactile perception using visual data, reducing reliance on costly tactile data collection.

Abstract: Visuo-tactile perception aims to understand an object's tactile properties,
such as texture, softness, and rigidity. However, the field remains
underexplored because collecting tactile data is costly and labor-intensive. We
observe that visually distinct objects can exhibit similar surface textures or
material properties. For example, a leather sofa and a leather jacket have
different appearances but share similar tactile properties. This implies that
tactile understanding can be guided by material cues in visual data, even
without direct tactile supervision. In this paper, we introduce RA-Touch, a
retrieval-augmented framework that improves visuo-tactile perception by
leveraging visual data enriched with tactile semantics. We carefully recaption
a large-scale visual dataset with tactile-focused descriptions, enabling the
model to access tactile semantics typically absent from conventional visual
datasets. A key challenge remains in effectively utilizing these tactile-aware
external descriptions. RA-Touch addresses this by retrieving visual-textual
representations aligned with tactile inputs and integrating them to focus on
relevant textural and material properties. By outperforming prior methods on
the TVL benchmark, our method demonstrates the potential of retrieval-based
visual reuse for tactile understanding. Code is available at
https://aim-skku.github.io/RA-Touch

</details>


### [288] [A Review of Vision-Based Assistive Systems for Visually Impaired People: Technologies, Applications, and Future Directions](https://arxiv.org/pdf/2505.14298)
*Fulong Yao, Wenju Zhou, Huosheng Hu*

Main category: cs.CV

TL;DR: A review of recent advances in assistive technologies for the visually impaired, focusing on obstacle detection, navigation, and user interaction, along with future trends.


<details>
  <summary>Details</summary>
Motivation: To enhance independent living for visually impaired individuals by improving mobility and interaction with their environment.

Method: Comprehensive review of state-of-the-art vision-based assistive systems.

Result: Highlights advancements in obstacle detection, navigation, and user interaction technologies.

Conclusion: Identifies emerging trends and future directions for visual guidance systems.

Abstract: Visually impaired individuals rely heavily on accurate and timely information
about obstacles and their surrounding environments to achieve independent
living. In recent years, significant progress has been made in the development
of assistive technologies, particularly vision-based systems, that enhance
mobility and facilitate interaction with the external world in both indoor and
outdoor settings. This paper presents a comprehensive review of recent advances
in assistive systems designed for the visually impaired, with a focus on
state-of-the-art technologies in obstacle detection, navigation, and user
interaction. In addition, emerging trends and future directions in visual
guidance systems are discussed.

</details>


### [289] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/pdf/2505.14318)
*Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu*

Main category: cs.CV

TL;DR: RADAR enhances radiology report generation by combining internal LLM knowledge with external supplementary information, outperforming existing methods in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal LLMs for radiology report generation inefficiently use internal knowledge and redundantly integrate external data, limiting performance.

Method: RADAR extracts internal LLM knowledge aligned with expert classifications, retrieves supplementary external knowledge, and aggregates both for report generation.

Result: Experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray show RADAR outperforms state-of-the-art LLMs in language quality and clinical accuracy.

Conclusion: RADAR effectively leverages internal and external knowledge to improve radiology report generation, setting a new benchmark for performance.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [290] [Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces](https://arxiv.org/pdf/2505.14320)
*Maria Cuellar, Hon Kiu, To, Arush Mehrotra*

Main category: cs.CV

TL;DR: The study evaluates how image degradation affects facial recognition technology (FRT) accuracy and fairness, finding higher error rates for women and Black individuals, especially Black females. Despite challenges, FRT outperforms traditional forensic methods, but transparency and oversight are needed.


<details>
  <summary>Details</summary>
Motivation: To assess FRT accuracy and fairness under real-world conditions (e.g., degraded images) and highlight disparities affecting demographic groups.

Method: Synthetic faces (StyleGAN3) labeled with FairFace were degraded (contrast, brightness, blur, pose, resolution) and evaluated using Deepface with ArcFace loss in 1:n identification tasks.

Result: False positives peak near baseline quality; false negatives rise with degradation (blur, low resolution). Higher error rates for women and Black individuals, especially Black females. FRT still outperforms traditional methods.

Conclusion: FRT is valuable but requires validation, regulation, and oversight to ensure fairness and forensic validity, alongside scrutiny of user-driven data manipulation.

Abstract: Facial recognition technology (FRT) is increasingly used in criminal
investigations, yet most evaluations of its accuracy rely on high-quality
images, unlike those often encountered by law enforcement. This study examines
how five common forms of image degradation--contrast, brightness, motion blur,
pose shift, and resolution--affect FRT accuracy and fairness across demographic
groups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,
we simulate degraded images and evaluate performance using Deepface with
ArcFace loss in 1:n identification tasks. We perform an experiment and find
that false positive rates peak near baseline image quality, while false
negatives increase as degradation intensifies--especially with blur and low
resolution. Error rates are consistently higher for women and Black
individuals, with Black females most affected. These disparities raise concerns
about fairness and reliability when FRT is used in real-world investigative
contexts. Nevertheless, even under the most challenging conditions and for the
most affected subgroups, FRT accuracy remains substantially higher than that of
many traditional forensic methods. This suggests that, if appropriately
validated and regulated, FRT should be considered a valuable investigative
tool. However, algorithmic accuracy alone is not sufficient: we must also
evaluate how FRT is used in practice, including user-driven data manipulation.
Such cases underscore the need for transparency and oversight in FRT deployment
to ensure both fairness and forensic validity.

</details>


### [291] [Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?](https://arxiv.org/pdf/2505.14321)
*Bo Feng, Zhengfeng Lai, Shiyu Li, Zizhen Wang, Simon Wang, Ping Huang, Meng Cao*

Main category: cs.CV

TL;DR: VBenchComp categorizes video questions to isolate temporal reasoning, revealing hidden model weaknesses and improving benchmark design.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks conflate knowledge-based and image-based questions, obscuring true temporal reasoning in video understanding.

Method: Proposes VBenchComp, a pipeline categorizing questions into LLM-Answerable, Semantic, Temporal, and Others to evaluate video LLMs.

Result: Identifies nuanced model weaknesses masked by traditional scores, offering insights for better benchmark design.

Conclusion: VBenchComp enables fine-grained evaluation and improves assessment of video LLMs by isolating temporal reasoning.

Abstract: Existing video understanding benchmarks often conflate knowledge-based and
purely image-based questions, rather than clearly isolating a model's temporal
reasoning ability, which is the key aspect that distinguishes video
understanding from other modalities. We identify two major limitations that
obscure whether higher scores truly indicate stronger understanding of the
dynamic content in videos: (1) strong language priors, where models can answer
questions without watching the video; and (2) shuffling invariance, where
models maintain similar performance on certain questions even when video frames
are temporally shuffled. To alleviate these issues, we propose VBenchComp, an
automated pipeline that categorizes questions into different domains:
LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions
can be answered without viewing the video; Semantic questions remain answerable
even when the video frames are shuffled; and Temporal questions require
understanding the correct temporal order of frames. The rest of the questions
are labeled as Others. This can enable fine-grained evaluation of different
capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that
are hidden by traditional overall scores, and we offer insights and
recommendations for designing future benchmarks that more accurately assess
video LLMs.

</details>


### [292] [Handloom Design Generation Using Generative Networks](https://arxiv.org/pdf/2505.14330)
*Rajat Kanti Bhattacharjee, Meghali Nandi, Amrit Jha, Gunajit Kalita, Ferdous Ahmed Barbhuiya*

Main category: cs.CV

TL;DR: The paper explores deep learning for generating handloom fabric designs, using generative models and style transfer, and introduces the NeuralLoom dataset.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored capability of generative neural networks in understanding and synthesizing artistic designs for clothing, particularly handloom fabrics.

Method: Employed multiple state-of-the-art generative models and style transfer algorithms to study their performance in design generation.

Result: Results were evaluated through user scores, though specific outcomes are not detailed in the abstract.

Conclusion: The work contributes to the field by proposing methods for design generation and introducing the NeuralLoom dataset.

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [293] [Domain Adaptation for Multi-label Image Classification: a Discriminator-free Approach](https://arxiv.org/pdf/2505.14333)
*Inder Pal Singh, Enjie Ghorbel, Anis Kacem, Djamila Aouada*

Main category: cs.CV

TL;DR: DDA-MLIC introduces a discriminator-free adversarial method for unsupervised domain adaptation in multi-label image classification, outperforming state-of-the-art methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial-based UDA methods for MLIC use a discriminator subnet, which may harm task-specific discriminative power. DDA-MLIC addresses this by deriving an adversarial critic directly from the classifier.

Method: Uses a GMM to model source and target predictions, distinguishing clusters via a DNN instead of EM. Adversarial loss is formulated using Fréchet distance, making the framework differentiable and cost-effective.

Result: Outperforms state-of-the-art methods in precision with fewer parameters, tested on multi-label datasets with three domain shifts.

Conclusion: DDA-MLIC is an efficient, discriminator-free approach for UDA in MLIC, offering improved performance and reduced computational cost.

Abstract: This paper introduces a discriminator-free adversarial-based approach termed
DDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label
Image Classification (MLIC). While recent efforts have explored
adversarial-based UDA methods for MLIC, they typically include an additional
discriminator subnet. Nevertheless, decoupling the classification and the
discrimination tasks may harm their task-specific discriminative power. Herein,
we address this challenge by presenting a novel adversarial critic directly
derived from the task-specific classifier. Specifically, we employ a
two-component Gaussian Mixture Model (GMM) to model both source and target
predictions, distinguishing between two distinct clusters. Instead of using the
traditional Expectation Maximization (EM) algorithm, our approach utilizes a
Deep Neural Network (DNN) to estimate the parameters of each GMM component.
Subsequently, the source and target GMM parameters are leveraged to formulate
an adversarial loss using the Fr\'echet distance. The proposed framework is
therefore not only fully differentiable but is also cost-effective as it avoids
the expensive iterative process usually induced by the standard EM method. The
proposed method is evaluated on several multi-label image datasets covering
three different types of domain shift. The obtained results demonstrate that
DDA-MLIC outperforms existing state-of-the-art methods in terms of precision
while requiring a lower number of parameters. The code is made publicly
available at github.com/cvi2snt/DDA-MLIC.

</details>


### [294] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/pdf/2505.14340)
*Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim*

Main category: cs.CV

TL;DR: A survey of Plane Geometry Problem Solving (PGPS) methods, categorizing them into encoder-decoder frameworks, analyzing their architectures, and highlighting challenges like hallucination and data leakage.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive overview of PGPS research and assess multi-modal reasoning in vision-language models.

Method: Categorize PGPS methods into encoder-decoder frameworks, summarize output formats, and analyze architectural designs.

Result: Identified challenges include hallucination during encoding and data leakage in benchmarks.

Conclusion: Outlines future research directions to address these challenges and improve PGPS methodologies.

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [295] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/pdf/2505.14341)
*Sifan Li, Ming Tao, Hao Zhao, Ling Shao, Hao Tang*

Main category: cs.CV

TL;DR: The paper introduces a method to improve concept alignment in counterfactual Text-to-Image (T2I) generation by using Explicit Logical Narrative Prompt (ELNP) and latent space object replacement.


<details>
  <summary>Details</summary>
Motivation: Counterfactual T2I lacks versatility and factual feel, hindering realistic synthesis of impossible or anti-physics scenes. The focus is on enhancing concept alignment.

Method: Utilizes controllable T2I models to replace objects in latent space step-by-step, guided by ELNP generated by DeepSeek. Introduces a metric to evaluate concept coverage.

Result: Experiments show the strategy improves concept alignment in counterfactual T2I.

Conclusion: The proposed ELNP and latent space replacement effectively enhance concept alignment for counterfactual T2I.

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [296] [Egocentric Action-aware Inertial Localization in Point Clouds](https://arxiv.org/pdf/2505.14346)
*Mingfang Zhang, Ryo Yonetani, Yifei Huang, Liangyang Ouyang, Ruicong Liu, Yoichi Sato*

Main category: cs.CV

TL;DR: EAIL is a novel inertial localization framework using head-mounted IMU signals and egocentric action cues to localize individuals in 3D point clouds, compensating for drift by correlating actions with environmental features.


<details>
  <summary>Details</summary>
Motivation: Human inertial localization faces challenges due to IMU sensor noise and diverse motion patterns. EAIL leverages action-environment correlations to address drift.

Method: EAIL learns correlations via hierarchical multi-modal alignment, contrastively aligning IMU action cues with local point cloud features.

Result: EAIL outperforms state-of-the-art inertial localization and action recognition baselines in experiments.

Conclusion: EAIL effectively compensates for drift by leveraging action-environment correlations, also enabling action recognition as a by-product.

Abstract: This paper presents a novel inertial localization framework named Egocentric
Action-aware Inertial Localization (EAIL), which leverages egocentric action
cues from head-mounted IMU signals to localize the target individual within a
3D point cloud. Human inertial localization is challenging due to IMU sensor
noise that causes trajectory drift over time. The diversity of human actions
further complicates IMU signal processing by introducing various motion
patterns. Nevertheless, we observe that some actions observed through the
head-mounted IMU correlate with spatial environmental structures (e.g., bending
down to look inside an oven, washing dishes next to a sink), thereby serving as
spatial anchors to compensate for the localization drift. The proposed EAIL
framework learns such correlations via hierarchical multi-modal alignment. By
assuming that the 3D point cloud of the environment is available, it
contrastively learns modality encoders that align short-term egocentric action
cues in IMU signals with local environmental features in the point cloud. These
encoders are then used in reasoning the IMU data and the point cloud over time
and space to perform inertial localization. Interestingly, these encoders can
further be utilized to recognize the corresponding sequence of actions as a
by-product. Extensive experiments demonstrate the effectiveness of the proposed
framework over state-of-the-art inertial localization and inertial action
recognition baselines.

</details>


### [297] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/pdf/2505.14357)
*Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long*

Main category: cs.CV

TL;DR: Vid2World repurposes pre-trained video diffusion models into interactive world models by causalization and action guidance, improving scalability and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing world models are domain-specific and produce low-fidelity predictions, while video diffusion models excel in high-quality video generation. Vid2World bridges this gap.

Method: Vid2World causalizes a pre-trained video diffusion model for autoregressive generation and introduces causal action guidance for controllability.

Result: Experiments in robot manipulation and game simulation show Vid2World is scalable and effective.

Conclusion: Vid2World successfully leverages video diffusion models for high-fidelity, interactive world models.

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [298] [Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable](https://arxiv.org/pdf/2505.14359)
*Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding*

Main category: cs.CV

TL;DR: The paper addresses bias in detectors caused by spurious correlations in datasets, proposing Dual Data Alignment (DDA) to align pixel and frequency domains, improving detector generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing detectors overfit on biased datasets, leading to performance degradation on unbiased data. Pixel-level alignment alone is insufficient due to frequency-level misalignment.

Method: Proposes Dual Data Alignment (DDA) to align both pixel and frequency domains. Introduces DDA-COCO and EvalGEN test sets for evaluation.

Result: Detectors trained on DDA-aligned MSCOCO improve performance across 8 benchmarks, with a +7.2% gain on in-the-wild benchmarks.

Conclusion: DDA effectively mitigates bias by addressing both pixel and frequency misalignment, enhancing detector generalizability.

Abstract: Existing detectors are often trained on biased datasets, leading to the
possibility of overfitting on non-causal image attributes that are spuriously
correlated with real/synthetic labels. While these biased features enhance
performance on the training data, they result in substantial performance
degradation when applied to unbiased datasets. One common solution is to
perform dataset alignment through generative reconstruction, matching the
semantic content between real and synthetic images. However, we revisit this
approach and show that pixel-level alignment alone is insufficient. The
reconstructed images still suffer from frequency-level misalignment, which can
perpetuate spurious correlations. To illustrate, we observe that reconstruction
models tend to restore the high-frequency details lost in real images (possibly
due to JPEG compression), inadvertently creating a frequency-level
misalignment, where synthetic images appear to have richer high-frequency
content than real ones. This misalignment leads to models associating
high-frequency features with synthetic labels, further reinforcing biased cues.
To resolve this, we propose Dual Data Alignment (DDA), which aligns both the
pixel and frequency domains. Moreover, we introduce two new test sets:
DDA-COCO, containing DDA-aligned synthetic images for testing detector
performance on the most aligned dataset, and EvalGEN, featuring the latest
generative models for assessing detectors under new generative architectures
such as visual auto-regressive generators. Finally, our extensive evaluations
demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could
improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on
in-the-wild benchmarks, highlighting the improved generalizability of unbiased
detectors.

</details>


### [299] [Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives](https://arxiv.org/pdf/2505.14361)
*Xingxing Weng, Chao Pang, Gui-Song Xia*

Main category: cs.CV

TL;DR: A review of vision-language modeling (VLM) in remote sensing, covering taxonomy, methods, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between images and natural language in remote sensing by leveraging pre-training and fine-tuning paradigms.

Method: Categorizes VLM into contrastive learning, visual instruction tuning, and text-conditioned image generation, detailing architectures and objectives.

Result: Highlights strong performance in remote sensing tasks and conversational interaction capabilities.

Conclusion: Identifies future research directions like cross-modal alignment, vague requirement comprehension, and scalable datasets.

Abstract: Vision-language modeling (VLM) aims to bridge the information gap between
images and natural language. Under the new paradigm of first pre-training on
massive image-text pairs and then fine-tuning on task-specific data, VLM in the
remote sensing domain has made significant progress. The resulting models
benefit from the absorption of extensive general knowledge and demonstrate
strong performance across a variety of remote sensing data analysis tasks.
Moreover, they are capable of interacting with users in a conversational
manner. In this paper, we aim to provide the remote sensing community with a
timely and comprehensive review of the developments in VLM using the two-stage
paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing:
contrastive learning, visual instruction tuning, and text-conditioned image
generation. For each category, we detail the commonly used network architecture
and pre-training objectives. Second, we conduct a thorough review of existing
works, examining foundation models and task-specific adaptation methods in
contrastive-based VLM, architectural upgrades, training strategies and model
capabilities in instruction-based VLM, as well as generative foundation models
with their representative downstream applications. Third, we summarize datasets
used for VLM pre-training, fine-tuning, and evaluation, with an analysis of
their construction methodologies (including image sources and caption
generation) and key properties, such as scale and task adaptability. Finally,
we conclude this survey with insights and discussions on future research
directions: cross-modal representation alignment, vague requirement
comprehension, explanation-driven model reliability, continually scalable model
capabilities, and large-scale datasets featuring richer modalities and greater
challenges.

</details>


### [300] [DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning](https://arxiv.org/pdf/2505.14362)
*Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, Xing Yu*

Main category: cs.CV

TL;DR: DeepEyes introduces a model for interleaved multimodal reasoning, enabling 'thinking with images' via reinforcement learning, improving performance on perception, reasoning, and grounding tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack seamless visual-textual reasoning akin to human cognition, prompting the need for advanced visual input integration.

Method: DeepEyes uses end-to-end reinforcement learning, a tool-use-oriented data selection mechanism, and reward strategies to foster tool-assisted reasoning.

Result: DeepEyes shows gains in fine-grained perception, reasoning, grounding, and reduced hallucination, with emergent tool-calling behavior.

Conclusion: DeepEyes advances multimodal reasoning by natively integrating visual-textual processes, mirroring human cognition without relying on specialized models.

Abstract: Large Vision-Language Models (VLMs) have shown strong capabilities in
multimodal understanding and reasoning, yet they are primarily constrained by
text-based reasoning processes. However, achieving seamless integration of
visual and textual reasoning which mirrors human cognitive processes remains a
significant challenge. In particular, effectively incorporating advanced visual
input processing into reasoning mechanisms is still an open question. Thus, in
this paper, we explore the interleaved multimodal reasoning paradigm and
introduce DeepEyes, a model with "thinking with images" capabilities
incentivized through end-to-end reinforcement learning without the need for
cold-start SFT. Notably, this ability emerges natively within the model itself,
leveraging its inherent grounding ability as a tool instead of depending on
separate specialized models. Specifically, we propose a tool-use-oriented data
selection mechanism and a reward strategy to encourage successful tool-assisted
reasoning trajectories. DeepEyes achieves significant performance gains on
fine-grained perception and reasoning benchmarks and also demonstrates
improvement in grounding, hallucination, and mathematical reasoning tasks.
Interestingly, we observe the distinct evolution of tool-calling behavior from
initial exploration to efficient and accurate exploitation, and diverse
thinking patterns that closely mirror human visual reasoning processes. Code is
available at https://github.com/Visual-Agent/DeepEyes.

</details>


### [301] [ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations](https://arxiv.org/pdf/2505.14404)
*Xuecheng Wu, Jiaxing Liu, Danlei Huang, Xiaoyu Li, Yifan Wang, Chen Chen, Liya Ma, Xuezhi Cao, Junxiao Xue*

Main category: cs.CV

TL;DR: VI-CoT improves MLLMs' reasoning with step-wise visual updates, but benchmarks lack free-style IVS. ViC-Bench introduces tasks and metrics to evaluate VI-CoT systematically.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks use fixed IVS, limiting evaluation of intrinsic reasoning. ViC-Bench addresses this gap by enabling free-style IVS and exploring IVS impact factors.

Method: ViC-Bench includes four tasks with free-style IVS pipelines. A three-stage evaluation suite and IPII strategy are proposed to assess VI-CoT.

Result: Evaluations of 18 MLLMs reveal insights into VI-CoT capabilities, highlighting strengths and weaknesses.

Conclusion: ViC-Bench provides a robust framework for evaluating VI-CoT, advancing understanding of visual reasoning in MLLMs.

Abstract: Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually
update their understanding and decisions based on step-wise intermediate visual
states (IVS), much like a human would, which demonstrates impressive success in
various tasks, thereby leading to emerged advancements in related benchmarks.
Despite promising progress, current benchmarks provide models with relatively
fixed IVS, rather than free-style IVS, whch might forcibly distort the original
thinking trajectories, failing to evaluate their intrinsic reasoning
capabilities. More importantly, existing benchmarks neglect to systematically
explore the impact factors that IVS would impart to untamed reasoning
performance. To tackle above gaps, we introduce a specialized benchmark termed
ViC-Bench, consisting of four representive tasks: maze navigation, jigsaw
puzzle, embodied long-horizon planning, and complex counting, where each task
has dedicated free-style IVS generation pipeline supporting function calls. To
systematically examine VI-CoT capability, we propose a thorough evaluation
suite incorporating a progressive three-stage strategy with targeted new
metrics. Besides, we establish Incremental Prompting Information Injection
(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We
extensively conduct evaluations for 18 advanced MLLMs, revealing key insights
into their VI-CoT capability. Our proposed benchmark is publicly open at
Huggingface.

</details>


### [302] [Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency](https://arxiv.org/pdf/2505.14405)
*Jiafeng Liang, Shixin Jiang, Xuan Dong, Ning Wang, Zheng Chu, Hui Su, Jinlan Fu, Ming Liu, See-Kiong Ng, Bing Qin*

Main category: cs.CV

TL;DR: The paper introduces TemRobBench to test LMMs' temporal robustness and proposes PanoDPO to improve their performance.


<details>
  <summary>Details</summary>
Motivation: Current LMMs lack robustness in temporal analysis, relying too much on prior knowledge and text, ignoring video dynamics.

Method: TemRobBench introduces temporal inconsistency perturbations, and PanoDPO optimizes LMMs by balancing visual and linguistic features.

Result: PanoDPO enhances LMMs' robustness and reliability in temporal analysis.

Conclusion: PanoDPO effectively addresses LMMs' temporal robustness issues, improving their performance.

Abstract: Large Multimodal Models (LMMs) have recently demonstrated impressive
performance on general video comprehension benchmarks. Nevertheless, for
broader applications, the robustness of their temporal analysis capability
needs to be thoroughly investigated yet predominantly ignored. Motivated by
this, we propose a novel temporal robustness benchmark (TemRobBench), which
introduces temporal inconsistency perturbations separately at the visual and
textual modalities to assess the robustness of models. We evaluate 16
mainstream LMMs and find that they exhibit over-reliance on prior knowledge and
textual context in adversarial environments, while ignoring the actual temporal
dynamics in the video. To mitigate this issue, we design panoramic direct
preference optimization (PanoDPO), which encourages LMMs to incorporate both
visual and linguistic feature preferences simultaneously. Experimental results
show that PanoDPO can effectively enhance the model's robustness and
reliability in temporal analysis.

</details>


### [303] [Diving into the Fusion of Monocular Priors for Generalized Stereo Matching](https://arxiv.org/pdf/2505.14414)
*Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, Yunde Jia*

Main category: cs.CV

TL;DR: The paper addresses challenges in stereo matching by leveraging unbiased monocular priors from vision foundation models, proposing a binary local ordering map and a registration-based fusion method to improve performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Stereo matching struggles with ill-posed regions like occlusions and non-Lambertian surfaces. Monocular priors help but are biased when learned from small datasets. Vision foundation models (VFMs) offer unbiased priors, but their fusion with stereo matching faces alignment and over-confidence issues.

Method: The paper introduces a binary local ordering map to unify relative and absolute depth representations and re-weight disparity updates. It also formulates monocular depth fusion as a registration problem using pixel-wise linear regression for global alignment.

Result: The method significantly improves performance when generalizing from SceneFlow to Middlebury and Booster datasets without sacrificing efficiency.

Conclusion: The proposed approach effectively exploits monocular priors to enhance stereo matching, addressing alignment and local optima issues while maintaining efficiency.

Abstract: The matching formulation makes it naturally hard for the stereo matching to
handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing
monocular priors has been proven helpful for ill-posed matching, but the biased
monocular prior learned from small stereo datasets constrains the
generalization. Recently, stereo matching has progressed by leveraging the
unbiased monocular prior from the vision foundation model (VFM) to improve the
generalization in ill-posed regions. We dive into the fusion process and
observe three main problems limiting the fusion of the VFM monocular prior. The
first problem is the misalignment between affine-invariant relative monocular
depth and absolute depth of disparity. Besides, when we use the monocular
feature in an iterative update structure, the over-confidence in the disparity
update leads to local optima results. A direct fusion of a monocular depth map
could alleviate the local optima problem, but noisy disparity results computed
at the first several iterations will misguide the fusion. In this paper, we
propose a binary local ordering map to guide the fusion, which converts the
depth map into a binary relative format, unifying the relative and absolute
depth representation. The computed local ordering map is also used to re-weight
the initial disparity update, resolving the local optima and noisy problem. In
addition, we formulate the final direct fusion of monocular depth to the
disparity as a registration problem, where a pixel-wise linear regression
module can globally and adaptively align them. Our method fully exploits the
monocular prior to support stereo matching results effectively and efficiently.
We significantly improve the performance from the experiments when generalizing
from SceneFlow to Middlebury and Booster datasets while barely reducing the
efficiency.

</details>


### [304] [Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/pdf/2505.14454)
*Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang*

Main category: cs.CV

TL;DR: VidCom2 is a plug-and-play framework for efficient token compression in VideoLLMs, preserving essential information while reducing redundancy and latency.


<details>
  <summary>Details</summary>
Motivation: VideoLLMs face efficiency issues due to quadratic complexity of visual tokens and information loss from overlooked visual signals.

Method: Proposes VidCom2, which quantifies frame uniqueness and adaptively adjusts compression intensity.

Result: Achieves 99.6% of original performance with 25% tokens and reduces latency by 70.8%.

Conclusion: VidCom2 is effective, efficient, and compatible with other compression methods.

Abstract: Video large language models (VideoLLM) excel at video understanding, but face
efficiency challenges due to the quadratic complexity of abundant visual
tokens. Our systematic analysis of token compression methods for VideoLLMs
reveals two critical issues: (i) overlooking distinctive visual signals across
frames, leading to information loss; (ii) suffering from implementation
constraints, causing incompatibility with modern architectures or efficient
operators. To address these challenges, we distill three design principles for
VideoLLM token compression and propose a plug-and-play inference acceleration
framework "Video Compression Commander" (VidCom2). By quantifying each frame's
uniqueness, VidCom2 adaptively adjusts compression intensity across frames,
effectively preserving essential information while reducing redundancy in video
sequences. Extensive experiments across various VideoLLMs and benchmarks
demonstrate the superior performance and efficiency of our VidCom2. With only
25% visual tokens, VidCom2 achieves 99.6% of the original performance on
LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame
Compression Adjustment strategy is compatible with other token compression
methods to further improve their performance. Our code is available at
https://github.com/xuyang-liu16/VidCom2.

</details>


### [305] [VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank](https://arxiv.org/pdf/2505.14460)
*Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma*

Main category: cs.CV

TL;DR: VisualQuality-R1 is a reasoning-induced NR-IQA model trained with reinforcement learning, outperforming existing methods and generating human-aligned quality descriptions.


<details>
  <summary>Details</summary>
Motivation: The potential of reasoning-induced computational modeling is unexplored in IQA, a task reliant on visual reasoning.

Method: Uses reinforcement learning to rank with group relative policy optimization and Thurstone model for comparative probabilities.

Result: Outperforms discriminative deep learning-based NR-IQA models and supports multi-dataset training.

Conclusion: VisualQuality-R1 is effective for measuring progress in image processing tasks like super-resolution and generation.

Abstract: DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing
reasoning and generalization capabilities of large language models (LLMs)
through reinforcement learning. Nevertheless, the potential of
reasoning-induced computational modeling has not been thoroughly explored in
the context of image quality assessment (IQA), a task critically dependent on
visual reasoning. In this paper, we introduce VisualQuality-R1, a
reasoning-induced no-reference IQA (NR-IQA) model, and we train it with
reinforcement learning to rank, a learning algorithm tailored to the
intrinsically relative nature of visual quality. Specifically, for a pair of
images, we employ group relative policy optimization to generate multiple
quality scores for each image. These estimates are then used to compute
comparative probabilities of one image having higher quality than the other
under the Thurstone model. Rewards for each quality estimate are defined using
continuous fidelity measures rather than discretized binary labels. Extensive
experiments show that the proposed VisualQuality-R1 consistently outperforms
discriminative deep learning-based NR-IQA models as well as a recent
reasoning-induced quality regression method. Moreover, VisualQuality-R1 is
capable of generating contextually rich, human-aligned quality descriptions,
and supports multi-dataset training without requiring perceptual scale
realignment. These features make VisualQuality-R1 especially well-suited for
reliably measuring progress in a wide range of image processing tasks like
super-resolution and image generation.

</details>


### [306] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/pdf/2505.14462)
*Jiaang Li, Yifei Yuan, Wenyan Li, Mohammad Aliannejadi, Daniel Hershcovich, Anders Søgaard, Ivan Vulić, Wenxuan Zhang, Paul Pu Liang, Yang Deng, Serge Belongie*

Main category: cs.CV

TL;DR: RAVENEA introduces a retrieval-augmented benchmark for improving visual culture understanding in VLMs, showing significant performance gains when retrieval is used.


<details>
  <summary>Details</summary>
Motivation: VLMs often fail to interpret cultural nuances, and retrieval-augmented methods, effective in text-only settings, are underexplored in multimodal contexts.

Method: RAVENEA integrates 10,000 Wikipedia documents for two tasks (cVQA and cIC), evaluates seven retrievers, and tests retrieval-augmented inputs on 14 VLMs.

Result: Retrieval-augmented lightweight VLMs outperform non-augmented ones by 3.2% (cVQA) and 6.2% (cIC).

Conclusion: Retrieval-augmented methods and culturally inclusive benchmarks enhance multimodal understanding.

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [307] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/pdf/2505.14476)
*Farshad Sangari Abiz, Reshad Hosseini, Babak N. Araabi*

Main category: cs.CV

TL;DR: The paper proposes a method to improve the interpretability of Variational Sparse Coding (VSC) by ensuring consistent active dimensions in latent representations for samples of the same class.


<details>
  <summary>Details</summary>
Motivation: Standard VAEs and VSC lack structured interpretability, especially for class-specific attributes, limiting their utility.

Method: Introduces a new loss function to enforce shared active dimensions for samples within the same class, creating a structured latent space.

Result: The method produces more interpretable latent representations with both global and class-specific factors.

Conclusion: Enhancing latent space interpretability with structured dimensions improves the utility of VAEs for high-level concept learning.

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [308] [ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains](https://arxiv.org/pdf/2505.14511)
*Guillaume Vray, Devavrat Tomar, Xufeng Gao, Jean-Philippe Thiran, Evan Shelhamer, Behzad Bozorgtabar*

Main category: cs.CV

TL;DR: ReservoirTTA is a plug-in framework for prolonged test-time adaptation (TTA) that uses an adaptive ensemble of domain-specialized models to handle continuous and recurring domain shifts, improving accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like catastrophic forgetting and inter-domain interference in single-model TTA for non-stationary test distributions.

Method: Maintains a reservoir of domain-specialized models, detects new domains via online clustering, and routes samples to appropriate models for domain-specific adaptation.

Result: Outperforms state-of-the-art methods on benchmarks like ImageNet-C and CIFAR-10/100-C, showing robust performance across recurring and evolving domain shifts.

Conclusion: ReservoirTTA effectively mitigates key limitations of single-model TTA, ensuring stable and accurate adaptation in dynamic test environments.

Abstract: This paper introduces ReservoirTTA, a novel plug-in framework designed for
prolonged test-time adaptation (TTA) in scenarios where the test domain
continuously shifts over time, including cases where domains recur or evolve
gradually. At its core, ReservoirTTA maintains a reservoir of
domain-specialized models -- an adaptive test-time model ensemble -- that both
detects new domains via online clustering over style features of incoming
samples and routes each sample to the appropriate specialized model, and
thereby enables domain-specific adaptation. This multi-model strategy overcomes
key limitations of single model adaptation, such as catastrophic forgetting,
inter-domain interference, and error accumulation, ensuring robust and stable
performance on sustained non-stationary test distributions. Our theoretical
analysis reveals key components that bound parameter variance and prevent model
collapse, while our plug-in TTA module mitigates catastrophic forgetting of
previously encountered domains. Extensive experiments on the classification
corruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the
Cityscapes$\rightarrow$ACDC semantic segmentation task, covering recurring and
continuously evolving domain shifts, demonstrate that ReservoirTTA
significantly improves adaptation accuracy and maintains stable performance
across prolonged, recurring shifts, outperforming state-of-the-art methods.

</details>


### [309] [SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling](https://arxiv.org/pdf/2505.14521)
*Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen*

Main category: cs.CV

TL;DR: SparC introduces a unified framework for high-fidelity 3D object synthesis, combining sparse deformable marching cubes (SparseCubes) and a novel encoder (SparConv-VAE) to overcome detail loss and inefficiency in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D synthesis methods suffer from detail loss and inefficiency due to unstructured mesh data and volumetric grid complexity.

Method: SparC uses SparseCubes for high-resolution surface representation and SparConv-VAE, a sparse convolutional VAE, for near-lossless 3D reconstruction.

Result: Achieves state-of-the-art fidelity on complex geometries, preserves details, reduces costs, and integrates with latent diffusion models.

Conclusion: SparC enables scalable, high-resolution 3D generation with superior detail preservation and efficiency.

Abstract: High-fidelity 3D object synthesis remains significantly more challenging than
2D image generation due to the unstructured nature of mesh data and the cubic
complexity of dense volumetric grids. Existing two-stage pipelines-compressing
meshes with a VAE (using either 2D or 3D supervision), followed by latent
diffusion sampling-often suffer from severe detail loss caused by inefficient
representations and modality mismatches introduced in VAE. We introduce SparC,
a unified framework that combines a sparse deformable marching cubes
representation SparseCubes with a novel encoder SparConv-VAE. SparseCubes
converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary
topology by scattering signed distance and deformation fields onto a sparse
cube, allowing differentiable optimization. SparConv-VAE is the first
modality-consistent variational autoencoder built entirely upon sparse
convolutional networks, enabling efficient and near-lossless 3D reconstruction
suitable for high-resolution generative modeling through latent diffusion.
SparC achieves state-of-the-art reconstruction fidelity on challenging inputs,
including open surfaces, disconnected components, and intricate geometry. It
preserves fine-grained shape details, reduces training and inference cost, and
integrates naturally with latent diffusion models for scalable, high-resolution
3D generation.

</details>


### [310] [diffDemorph: Extending Reference-Free Demorphing to Unseen Faces](https://arxiv.org/pdf/2505.14527)
*Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: A diffusion-based method for reference-free demorphing of face images, generalizing across techniques and styles, outperforming state-of-the-art by ≥59.46%.


<details>
  <summary>Details</summary>
Motivation: Existing RF demorphing methods are limited by assumptions about morphing techniques and face styles, restricting their practicality.

Method: Novel diffusion-based approach to disentangle component images from a composite morph without reference images.

Result: Achieves high visual fidelity and generalizes across morph techniques and styles, outperforming state-of-the-art by ≥59.46%.

Conclusion: The method enhances practicality by training on synthetic data and testing on real morphs, proving effective across diverse datasets and face matchers.

Abstract: A face morph is created by combining two (or more) face images corresponding
to two (or more) identities to produce a composite that successfully matches
the constituent identities. Reference-free (RF) demorphing reverses this
process using only the morph image, without the need for additional reference
images. Previous RF demorphing methods were overly constrained, as they rely on
assumptions about the distributions of training and testing morphs such as the
morphing technique used, face style, and images used to create the morph. In
this paper, we introduce a novel diffusion-based approach that effectively
disentangles component images from a composite morph image with high visual
fidelity. Our method is the first to generalize across morph techniques and
face styles, beating the current state of the art by $\geq 59.46\%$ under a
common training protocol across all datasets tested. We train our method on
morphs created using synthetically generated face images and test on real
morphs, thereby enhancing the practicality of the technique. Experiments on six
datasets and two face matchers establish the utility and efficacy of our
method.

</details>


### [311] [Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image](https://arxiv.org/pdf/2505.14537)
*Yuxuan Wang, Xuanyu Yi, Qingshan Xu, Yuan Zhou, Long Chen, Hanwang Zhang*

Main category: cs.CV

TL;DR: CP-GS is a framework for personalizing 3D scenes from a single image, addressing viewpoint bias by propagating reference appearance to new views using pre-trained models and iterative fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with viewpoint bias in single-image 3D scene personalization, leading to inconsistent results.

Method: CP-GS integrates pre-trained image-to-3D generation and iterative LoRA fine-tuning to extend reference appearance and ensure multi-view consistency.

Result: CP-GS outperforms existing methods, producing high-quality personalized 3D scenes with reduced viewpoint bias.

Conclusion: CP-GS effectively mitigates viewpoint bias, offering a robust solution for single-image 3D scene personalization.

Abstract: Personalizing 3D scenes from a single reference image enables intuitive
user-guided editing, which requires achieving both multi-view consistency
across perspectives and referential consistency with the input image. However,
these goals are particularly challenging due to the viewpoint bias caused by
the limited perspective provided in a single image. Lacking the mechanisms to
effectively expand reference information beyond the original view, existing
methods of image-conditioned 3DGS personalization often suffer from this
viewpoint bias and struggle to produce consistent results. Therefore, in this
paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),
a framework that progressively propagates the single-view reference appearance
to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D
generation and iterative LoRA fine-tuning to extract and extend the reference
appearance, and finally produces faithful multi-view guidance images and the
personalized 3DGS outputs through a view-consistent generation process guided
by geometric cues. Extensive experiments on real-world scenes show that our
CP-GS effectively mitigates the viewpoint bias, achieving high-quality
personalization that significantly outperforms existing methods. The code will
be released at https://github.com/Yuxuan-W/CP-GS.

</details>


### [312] [Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI](https://arxiv.org/pdf/2505.14556)
*Marlène Careil, Yohann Benchetrit, Jean-Rémi King*

Main category: cs.CV

TL;DR: Dynadiff is a single-stage diffusion model for reconstructing images from dynamic fMRI, simplifying training and outperforming state-of-the-art methods in time-resolved decoding.


<details>
  <summary>Details</summary>
Motivation: Current brain-to-image decoding methods rely on complex multi-stage pipelines and lose temporal resolution, limiting time-resolved analysis.

Method: Dynadiff, a single-stage diffusion model, is introduced to reconstruct images from dynamic fMRI recordings.

Result: Dynadiff outperforms existing models on time-resolved fMRI, excels in semantic image reconstruction, and remains competitive on collapsed-time data.

Conclusion: Dynadiff advances time-resolved brain-to-image decoding and enables precise analysis of image representation evolution in brain activity.

Abstract: Brain-to-image decoding has been recently propelled by the progress in
generative AI models and the availability of large ultra-high field functional
Magnetic Resonance Imaging (fMRI). However, current approaches depend on
complicated multi-stage pipelines and preprocessing steps that typically
collapse the temporal dimension of brain recordings, thereby limiting
time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural
Activity Diffusion for Image Reconstruction), a new single-stage diffusion
model designed for reconstructing images from dynamically evolving fMRI
recordings. Our approach offers three main contributions. First, Dynadiff
simplifies training as compared to existing approaches. Second, our model
outperforms state-of-the-art models on time-resolved fMRI signals, especially
on high-level semantic image reconstruction metrics, while remaining
competitive on preprocessed fMRI data that collapse time. Third, this approach
allows a precise characterization of the evolution of image representations in
brain activity. Overall, this work lays the foundation for time-resolved
brain-to-image decoding.

</details>


### [313] [Instance Segmentation for Point Sets](https://arxiv.org/pdf/2505.14583)
*Abhimanyu Talwar, Julien Laasri*

Main category: cs.CV

TL;DR: The paper addresses memory-intensive similarity matrices in 3D point set instance segmentation by proposing two sampling-based methods, with random-based sampling showing the best improvements in speed and memory.


<details>
  <summary>Details</summary>
Motivation: To improve the memory efficiency of instance segmentation in 3D point sets, as highlighted by the limitations of similarity matrices in SGPN.

Method: Two sampling-based methods are used: computing instance segmentation on sub-sampled point sets and extrapolating labels to the full set via nearest neighbor.

Result: Both methods perform well on large sub-samples, but the random-based strategy offers superior speed and memory efficiency.

Conclusion: Random-based sampling is the most effective approach for reducing memory usage and improving speed in 3D point set instance segmentation.

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [314] [3D Reconstruction from Sketches](https://arxiv.org/pdf/2505.14621)
*Abhimanyu Talwar, Julien Laasri*

Main category: cs.CV

TL;DR: A pipeline for 3D scene reconstruction from sketches using stitching, CycleGAN for realism, and MegaDepth for depth estimation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing 3D scenes from multiple sketches, leveraging correspondence points, CycleGAN, and MegaDepth.

Method: 1. Stitch sketches via correspondence points. 2. Convert stitched sketch to realistic image using CycleGAN. 3. Estimate depth-map with MegaDepth.

Result: The stitching process lacks generalization, but the pipeline excels in 3D reconstruction from single sketches.

Conclusion: The pipeline is effective for single-sketch 3D reconstruction, though stitching needs improvement.

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [315] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/pdf/2505.14638)
*Tomer Gafni, Asaf Karnieli, Yair Hanani*

Main category: cs.CV

TL;DR: A novel W4A8 quantization scheme for deep neural networks improves efficiency with minimal accuracy loss, using 4-bit weights and 8-bit computations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of growing model sizes and inefficiencies in latency and memory for complex tasks.

Method: Proposes a W4A8 scheme with a Dual Precision Quantization (DPQ) algorithm to balance hardware efficiency and accuracy.

Result: Achieves significant speedups and better memory usage with tolerable accuracy degradation.

Conclusion: The W4A8 scheme is effective for efficient inference on modern accelerators without major accuracy trade-offs.

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [316] [VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation](https://arxiv.org/pdf/2505.14640)
*Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen*

Main category: cs.CV

TL;DR: Existing LVU benchmarks are flawed due to reliance on MCQs and strong priors, leading to inflated results. VideoEval-Pro, a new benchmark with open-ended questions, provides a more realistic assessment of LMMs' video understanding.


<details>
  <summary>Details</summary>
Motivation: Current LVU benchmarks fail to accurately evaluate LMMs due to reliance on MCQs and questions with strong priors, undermining their validity.

Method: Proposed VideoEval-Pro, a benchmark with open-ended short-answer questions requiring full video understanding, assessing segment-level and full-video tasks.

Result: VideoEval-Pro reveals significant performance drops (>25%) for LMMs on open-ended questions, no correlation between MCQ and open-ended scores, and benefits from more input frames.

Conclusion: VideoEval-Pro offers a more reliable and realistic measure of long video understanding, highlighting flaws in existing benchmarks.

Abstract: Large multimodal models (LMMs) have recently emerged as a powerful tool for
long video understanding (LVU), prompting the development of standardized LVU
benchmarks to evaluate their performance. However, our investigation reveals a
rather sober lesson for existing LVU benchmarks. First, most existing
benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation
results are inflated due to the possibility of guessing the correct answer;
Second, a significant portion of questions in these benchmarks have strong
priors to allow models to answer directly without even reading the input video.
For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random frame
from a long video on Video-MME. We also observe that increasing the number of
frames does not necessarily lead to improvement on existing benchmarks, which
is counterintuitive. As a result, the validity and robustness of current LVU
benchmarks are undermined, impeding a faithful assessment of LMMs' long-video
understanding capability. To tackle this problem, we propose VideoEval-Pro, a
realistic LVU benchmark containing questions with open-ended short-answer,
which truly require understanding the entire video. VideoEval-Pro assesses both
segment-level and full-video understanding through perception and reasoning
tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the
following findings: (1) video LMMs show drastic performance ($>$25\%) drops on
open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do
not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other
MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input
frames. Our results show that VideoEval-Pro offers a more realistic and
reliable measure of long video understanding, providing a clearer view of
progress in this domain.

</details>


### [317] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/pdf/2505.14646)
*Anna C. Doris, Md Ferdous Alam, Amin Heyrani Nobari, Faez Ahmed*

Main category: cs.CV

TL;DR: CAD-Coder is a Vision-Language Model (VLM) that generates editable CAD code from visual input, outperforming state-of-the-art models with 100% syntax validity and high accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual CAD workflows are time-consuming and require expertise; existing AI models lack generalization and accuracy.

Method: CAD-Coder is fine-tuned on a novel dataset (GenCAD-Code) to generate CadQuery Python code from images.

Result: Achieves 100% valid syntax and highest 3D solid similarity accuracy, with some generalization to real-world images.

Conclusion: CAD-Coder demonstrates the potential of VLMs to streamline CAD workflows, offering high accuracy and adaptability.

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [318] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/pdf/2505.14654)
*Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin*

Main category: cs.CV

TL;DR: The paper introduces MM-When2Speak, a multimodal LLM-based model for predicting response timing and type in conversations, outperforming unimodal and LLM baselines.


<details>
  <summary>Details</summary>
Motivation: LLM-based chatbots struggle with timely, reactive responses due to lack of multimodal context in real-world dialogue.

Method: A new multimodal dataset from conversational videos is used to train MM-When2Speak, integrating visual, auditory, and textual cues.

Result: MM-When2Speak achieves up to 4x better response timing accuracy than leading commercial LLMs.

Conclusion: Multimodal inputs are crucial for natural and engaging conversational AI.

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [319] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/pdf/2505.14664)
*Yilin Ye, Junchao Huang, Xingchen Zeng, Jiazhi Xia, Wei Zeng*

Main category: cs.CV

TL;DR: AKRMap is a new dimensionality reduction (DR) technique for visualizing cross-modal embeddings, outperforming traditional methods like PCA and t-SNE by incorporating cross-modal metrics and enabling interactive exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional DR methods like PCA and t-SNE focus on single-modality feature distributions and fail to incorporate cross-modal metrics, limiting their effectiveness for cross-modal embedding visualization.

Method: AKRMap learns kernel regression of the metric landscape in the projection space using a supervised projection network with adaptive generalized kernels, optimized jointly with the projection.

Result: AKRMap generates more accurate and trustworthy visualizations, outperforming existing DR methods, and supports interactive features like zoom and overlay.

Conclusion: AKRMap effectively visualizes and compares cross-modal embeddings, particularly for text-to-image models, offering improved accuracy and interactivity.

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [320] [UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens](https://arxiv.org/pdf/2505.14671)
*Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao Zhang*

Main category: cs.CV

TL;DR: UniCTokens introduces a unified framework for personalized concept understanding and generation in vision-language models, outperforming existing methods in knowledge-driven tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat concept understanding and generation separately, limiting performance for complex prompts. UniCTokens aims to integrate these tasks for better results.

Method: UniCTokens uses unified concept tokens and a progressive training strategy with three stages: understanding warm-up, bootstrapping generation, and deepening understanding.

Result: UniCTokens achieves competitive performance in concept understanding and generation, and state-of-the-art results in knowledge-driven generation.

Conclusion: UniCTokens demonstrates that integrating understanding and generation enhances both tasks, with potential for broader applications.

Abstract: Personalized models have demonstrated remarkable success in understanding and
generating concepts provided by users. However, existing methods use separate
concept tokens for understanding and generation, treating these tasks in
isolation. This may result in limitations for generating images with complex
prompts. For example, given the concept $\langle bo\rangle$, generating
"$\langle bo\rangle$ wearing its hat" without additional textual descriptions
of its hat. We call this kind of generation personalized knowledge-driven
generation. To address the limitation, we present UniCTokens, a novel framework
that effectively integrates personalized information into a unified vision
language model (VLM) for understanding and generation. UniCTokens trains a set
of unified concept tokens to leverage complementary semantics, boosting two
personalized tasks. Moreover, we propose a progressive training strategy with
three stages: understanding warm-up, bootstrapping generation from
understanding, and deepening understanding from generation to enhance mutual
benefits between both tasks. To quantitatively evaluate the unified VLM
personalization, we present UnifyBench, the first benchmark for assessing
concept understanding, concept generation, and knowledge-driven generation.
Experimental results on UnifyBench indicate that UniCTokens shows competitive
performance compared to leading methods in concept understanding, concept
generation, and achieving state-of-the-art results in personalized
knowledge-driven generation. Our research demonstrates that enhanced
understanding improves generation, and the generation process can yield
valuable insights into understanding. Our code and dataset will be released at:
\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.

</details>


### [321] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/pdf/2505.14673)
*Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou*

Main category: cs.CV

TL;DR: IndexMark is a training-free watermarking framework for autoregressive image generation models, leveraging codebook redundancy to embed watermarks without quality loss, achieving high robustness and verification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods focus on diffusion models, leaving autoregressive models underexplored. IndexMark addresses this gap.

Method: Uses a match-then-replace technique to select and embed watermark tokens from the codebook, verified by token proportion and an Index Encoder.

Result: Achieves state-of-the-art performance in image quality and verification accuracy, robust against various perturbations.

Conclusion: IndexMark effectively embeds watermarks in autoregressive models, balancing quality and robustness.

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


### [322] [Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning](https://arxiv.org/pdf/2505.14677)
*Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou*

Main category: cs.CV

TL;DR: Training VLMs with reinforcement learning and visual Q&A pairs improves reasoning without CoT supervision, but risks shortcut learning. A caption-reason-answer format mitigates this, leading to superior performance.


<details>
  <summary>Details</summary>
Motivation: To enable VLMs to perform reasoning on image data without explicit CoT supervision, addressing shortcut learning issues.

Method: Reinforcement learning applied to VLMs with a caption-reason-answer format, trained on 273K CoT-free visual Q&A pairs.

Result: Visionary-R1 outperforms GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro on visual reasoning benchmarks.

Conclusion: Encouraging VLMs to interpret images before reasoning via a structured format enhances generalization and performance.

Abstract: Learning general-purpose reasoning capabilities has long been a challenging
problem in AI. Recent research in large language models (LLMs), such as
DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can
enable pre-trained LLMs to develop reasoning capabilities using simple
question-answer pairs. In this paper, we aim to train visual language models
(VLMs) to perform reasoning on image data through reinforcement learning and
visual question-answer pairs, without any explicit chain-of-thought (CoT)
supervision. Our findings indicate that simply applying reinforcement learning
to a VLM -- by prompting the model to produce a reasoning chain before
providing an answer -- can lead the model to develop shortcuts from easy
questions, thereby reducing its ability to generalize across unseen data
distributions. We argue that the key to mitigating shortcut learning is to
encourage the model to interpret images prior to reasoning. Therefore, we train
the model to adhere to a caption-reason-answer output format: initially
generating a detailed caption for an image, followed by constructing an
extensive reasoning chain. When trained on 273K CoT-free visual question-answer
pairs and using only reinforcement learning, our model, named Visionary-R1,
outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and
Gemini-1.5-Pro, on multiple visual reasoning benchmarks.

</details>


### [323] [UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.14682)
*Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan*

Main category: cs.CV

TL;DR: UniGen is a unified multimodal large language model (MLLM) for image understanding and generation, trained using a data-centric pipeline and enhanced by Chain-of-Thought Verification (CoT-V) for improved image generation quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in building unified MLLMs by proposing a comprehensive training pipeline and a novel test-time strategy (CoT-V) for better performance.

Method: Multi-stage pre-training, supervised fine-tuning, direct preference optimization, and CoT-V for test-time scaling.

Result: State-of-the-art performance on benchmarks (0.78 on GenEval, 85.19 on DPG-Bench).

Conclusion: UniGen provides actionable insights for unified MLLMs and sets directions for future research.

Abstract: We introduce UniGen, a unified multimodal large language model (MLLM) capable
of image understanding and generation. We study the full training pipeline of
UniGen from a data-centric perspective, including multi-stage pre-training,
supervised fine-tuning, and direct preference optimization. More importantly,
we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time
scaling, which significantly boosts UniGen's image generation quality using a
simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act
as both image generator and verifier at test time, assessing the semantic
alignment between a text prompt and its generated image in a step-by-step CoT
manner. Trained entirely on open-source datasets across all stages, UniGen
achieves state-of-the-art performance on a range of image understanding and
generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on
DPG-Bench. Through extensive ablation studies, our work provides actionable
insights and addresses key challenges in the full life cycle of building
unified MLLMs, contributing meaningful directions to the future research.

</details>


### [324] [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/pdf/2505.14683)
*Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan*

Main category: cs.CV

TL;DR: BAGEL is an open-source foundational model unifying multimodal understanding and generation, outperforming existing open-source models in benchmarks and showcasing advanced reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To unify multimodal understanding and generation in an open-source model, addressing limitations of proprietary systems.

Method: BAGEL is a decoder-only model pretrained on trillions of tokens from diverse interleaved text, image, video, and web data.

Result: BAGEL excels in multimodal generation, understanding, and reasoning, surpassing benchmarks and enabling tasks like image manipulation and 3D navigation.

Conclusion: BAGEL advances multimodal research by providing open-source tools, data protocols, and checkpoints, fostering further innovation.

Abstract: Unifying multimodal understanding and generation has shown impressive
capabilities in cutting-edge proprietary systems. In this work, we introduce
BAGEL, an open0source foundational model that natively supports multimodal
understanding and generation. BAGEL is a unified, decoder0only model pretrained
on trillions of tokens curated from large0scale interleaved text, image, video,
and web data. When scaled with such diverse multimodal interleaved data, BAGEL
exhibits emerging capabilities in complex multimodal reasoning. As a result, it
significantly outperforms open-source unified models in both multimodal
generation and understanding across standard benchmarks, while exhibiting
advanced multimodal reasoning abilities such as free-form image manipulation,
future frame prediction, 3D manipulation, and world navigation. In the hope of
facilitating further opportunities for multimodal research, we share the key
findings, pretraining details, data creation protocal, and release our code and
checkpoints to the community. The project page is at https://bagel-ai.org/

</details>


### [325] [Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers](https://arxiv.org/pdf/2505.14687)
*Sucheng Ren, Qihang Yu, Ju He, Alan Yuille, Liang-Chieh Chen*

Main category: cs.CV

TL;DR: GRAT is a training-free attention acceleration strategy for Diffusion Transformers, achieving significant speedups (e.g., 35.8× for 8192×8192 images) without quality loss by exploiting sparsity in attention maps and optimizing GPU parallelism.


<details>
  <summary>Details</summary>
Motivation: High computational costs of Diffusion Transformers hinder practical deployment, especially for large-scale image and video generation.

Method: GRAT partitions tokens into non-overlapping groups aligned with GPU execution and local attention structures, sharing key/value tokens within groups and restricting them to structured regions.

Result: GRAT achieves up to 35.8× speedup for 8192×8192 images and maintains performance without fine-tuning.

Conclusion: GRAT enables scalable visual generation by accelerating Diffusion Transformers efficiently, inspiring future research in this direction.

Abstract: Diffusion-based Transformers have demonstrated impressive generative
capabilities, but their high computational costs hinder practical deployment,
for example, generating an $8192\times 8192$ image can take over an hour on an
A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first,
\textbf{AT}tending smartly), a training-free attention acceleration strategy
for fast image and video generation without compromising output quality. The
key insight is to exploit the inherent sparsity in learned attention maps
(which tend to be locally focused) in pretrained Diffusion Transformers and
leverage better GPU parallelism. Specifically, GRAT first partitions contiguous
tokens into non-overlapping groups, aligning both with GPU execution patterns
and the local attention structures learned in pretrained generative
Transformers. It then accelerates attention by having all query tokens within
the same group share a common set of attendable key and value tokens. These key
and value tokens are further restricted to structured regions, such as
surrounding blocks or criss-cross regions, significantly reducing computational
overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention
when generating $8192\times 8192$ images) while preserving essential attention
patterns and long-range context. We validate GRAT on pretrained Flux and
HunyuanVideo for image and video generation, respectively. In both cases, GRAT
achieves substantially faster inference without any fine-tuning, while
maintaining the performance of full attention. We hope GRAT will inspire future
research on accelerating Diffusion Transformers for scalable visual generation.

</details>


### [326] [A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild](https://arxiv.org/pdf/2301.05191)
*Lei Sun, Daniel Gehrig, Christos Sakaridis, Mathias Gehrig, Jingyun Liang, Peng Sun, Zhijie Xu, Kaiwei Wang, Luc Van Gool, Davide Scaramuzza*

Main category: cs.CV

TL;DR: A unified framework for event-based frame interpolation that handles both sharp and blurry videos, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior work often ignores motion-induced blur in videos, limiting interpolation to sharp frames. This paper addresses this gap.

Method: Uses a bidirectional recurrent network to fuse input frames and events adaptively, with self-supervised training for real-world generalization.

Result: Outperforms state-of-the-art methods in frame interpolation, deblurring, and joint tasks, with improved domain transfer.

Conclusion: The proposed framework and HighREV dataset advance event-based interpolation, especially for blurry videos, with real-world applicability.

Abstract: Effective video frame interpolation hinges on the adept handling of motion in
the input scene. Prior work acknowledges asynchronous event information for
this, but often overlooks whether motion induces blur in the video, limiting
its scope to sharp frame interpolation. We instead propose a unified framework
for event-based frame interpolation that performs deblurring ad-hoc and thus
works both on sharp and blurry input videos. Our model consists in a
bidirectional recurrent network that incorporates the temporal dimension of
interpolation and fuses information from the input frames and the events
adaptively based on their temporal proximity. To enhance the generalization
from synthetic data to real event cameras, we integrate self-supervised
framework with the proposed model to enhance the generalization on real-world
datasets in the wild. At the dataset level, we introduce a novel real-world
high-resolution dataset with events and color videos named HighREV, which
provides a challenging evaluation setting for the examined task. Extensive
experiments show that our network consistently outperforms previous
state-of-the-art methods on frame interpolation, single image deblurring, and
the joint task of both. Experiments on domain transfer reveal that
self-supervised training effectively mitigates the performance degradation
observed when transitioning from synthetic data to real-world data. Code and
datasets are available at https://github.com/AHupuJR/REFID.

</details>


### [327] [Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for Composed Person Retrieval](https://arxiv.org/pdf/2311.16515)
*Delong Liu, Haiwen Li, Zhaohui Hou, Zhicheng Zhao, Fei Su, Yuan Dong*

Main category: cs.CV

TL;DR: The paper introduces a Composed Person Retrieval (CPR) task combining visual and textual queries, proposes a synthetic dataset (SynCPR), and a novel FAFA framework for improved retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval methods (image-only or text-only) underutilize information and lack flexibility. CPR aims to leverage multimodal queries for better results.

Method: A synthetic data pipeline creates SynCPR (1.15M triplets) using textual quadruples and generative models. FAFA framework enhances query representation via dynamic alignment and masked reasoning.

Result: Experiments show SynCPR's effectiveness and FAFA's superiority over state-of-the-art methods.

Conclusion: CPR and FAFA advance multimodal person retrieval, with SynCPR and ITCPR test set supporting future research.

Abstract: Person retrieval has attracted rising attention. Existing methods are mainly
divided into two retrieval modes, namely image-only and text-only. However,
they are unable to make full use of the available information and are difficult
to meet diverse application requirements. To address the above limitations, we
propose a new Composed Person Retrieval (CPR) task, which combines visual and
textual queries to identify individuals of interest from large-scale person
image databases. Nevertheless, the foremost difficulty of the CPR task is the
lack of available annotated datasets. Therefore, we first introduce a scalable
automatic data synthesis pipeline, which decomposes complex multimodal data
generation into the creation of textual quadruples followed by
identity-consistent image synthesis using fine-tuned generative models.
Meanwhile, a multimodal filtering method is designed to ensure the resulting
SynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.
Additionally, to improve the representation of composed person queries, we
propose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework
through fine-grained dynamic alignment and masked feature reasoning. Moreover,
for objective evaluation, we manually annotate the Image-Text Composed Person
Retrieval (ITCPR) test set. The extensive experiments demonstrate the
effectiveness of the SynCPR dataset and the superiority of the proposed FAFA
framework when compared with the state-of-the-art methods. All code and data
will be provided at
https://github.com/Delong-liu-bupt/Composed_Person_Retrieval.

</details>


### [328] [RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training](https://arxiv.org/pdf/2403.09948)
*Zhixiu Lu, Hailong Li, Nehal A. Parikh, Jonathan R. Dillman, Lili He*

Main category: cs.CV

TL;DR: RadCLIP is a vision-language model for radiologic imaging, improving analysis by aligning images with text annotations, outperforming existing methods in classification and matching tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AI models pre-trained on non-medical images struggle with radiologic data complexities, necessitating a specialized solution for diagnostic precision.

Method: RadCLIP adapts CLIP with a slice pooling mechanism for volumetric analysis and is pre-trained on a large dataset of radiologic image-text pairs.

Result: RadCLIP excels in uni-modal image classification and cross-modal image-text matching, enhancing diagnostic accuracy and efficiency.

Conclusion: RadCLIP shows promise for clinical use, with key contributions including dataset curation, a slice pooling adapter, and robust evaluations.

Abstract: The integration of artificial intelligence (AI) with radiology marks a
transformative era in medicine. Vision foundation models have been adopted to
enhance radiologic imaging analysis. However, the distinct complexities of
radiologic 2D and 3D radiologic data pose unique challenges that existing
models, pre-trained on general non-medical images, fail to address adequately.
To bridge this gap and capitalize on the diagnostic precision required in
radiologic imaging, we introduce Radiologic Contrastive Language-Image
Pre-training (RadCLIP): a cross-modal vision-language foundational model that
harnesses Vision Language Pre-training (VLP) framework to improve radiologic
image analysis. Building upon Contrastive Language-Image Pre-training (CLIP),
RadCLIP incorporates a slice pooling mechanism tailored for volumetric image
analysis and is pre-trained using a large and diverse dataset of radiologic
image-text pairs. The RadCLIP was pre-trained to effectively align radiologic
images with their corresponding text annotations, creating a robust vision
backbone for radiologic images. Extensive experiments demonstrate RadCLIP's
superior performance in both uni-modal radiologic image classification and
cross-modal image-text matching, highlighting its significant promise for
improving diagnostic accuracy and efficiency in clinical settings. Our Key
contributions include curating a large dataset with diverse radiologic 2D/3D
radiologic image-text pairs, a slice pooling adapter using an attention
mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP
on various radiologic downstream tasks.

</details>


### [329] [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/pdf/2403.11083)
*Xiaohao Xu, Yunkang Cao, Huaxin Zhang, Nong Sang, Xiaonan Huang*

Main category: cs.CV

TL;DR: A generic anomaly detection model using visual-language foundation models with multi-modal prompting for diverse industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection techniques lack generalization; this study aims to develop a versatile model applicable across multiple scenarios.

Method: Custom-built visual-language foundation models with multi-modal prompting, incorporating domain knowledge and unifying input into 2D images.

Result: Enhanced anomaly detection performance across images, point clouds, and videos, with demonstrated reasoning capabilities.

Conclusion: The approach successfully generalizes anomaly detection, leveraging multi-modal prompts and domain knowledge for diverse applications.

Abstract: Anomaly detection is vital in various industrial scenarios, including the
identification of unusual patterns in production lines and the detection of
manufacturing defects for quality control. Existing techniques tend to be
specialized in individual scenarios and lack generalization capacities. In this
study, our objective is to develop a generic anomaly detection model that can
be applied in multiple scenarios. To achieve this, we custom-build generic
visual language foundation models that possess extensive knowledge and robust
reasoning abilities as anomaly detectors and reasoners. Specifically, we
introduce a multi-modal prompting strategy that incorporates domain knowledge
from experts as conditions to guide the models. Our approach considers diverse
prompt types, including task descriptions, class context, normality rules, and
reference images. In addition, we unify the input representation of
multi-modality into a 2D image format, enabling multi-modal anomaly detection
and reasoning. Our preliminary studies demonstrate that combining visual and
language prompts as conditions for customizing the models enhances anomaly
detection performance. The customized models showcase the ability to detect
anomalies across different data modalities such as images, point clouds, and
videos. Qualitative case studies further highlight the anomaly detection and
reasoning capabilities, particularly for multi-object scenes and temporal data.
Our code is publicly available at
https://github.com/Xiaohao-Xu/Customizable-VLM

</details>


### [330] [Learning Coherent Matrixized Representation in Latent Space for Volumetric 4D Generation](https://arxiv.org/pdf/2403.13238)
*Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian*

Main category: cs.CV

TL;DR: A framework for generating volumetric 4D sequences with dynamic shape and color evolution, using coherent 3D modeling and spatio-temporal diffusion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on pose priors, limiting motion diversity and detail continuity. The goal is to enable free navigation and rendering of 4D content under text-image guidance.

Method: 1) Coherent 3D shape and color modeling. 2) Matrixized 4D sequence representation for efficient diffusion. 3) Spatio-temporal diffusion for 4D generation.

Result: Generates high-quality 3D shapes with consistent color and coherent animations, validated on ShapeNet, 3DBiCar, DeformingThings4D, and Objaverse datasets.

Conclusion: The proposed method improves motion diversity and detail continuity, enabling better 4D content generation.

Abstract: Directly learning to model 4D content, including shape, color, and motion, is
challenging. Existing methods rely on pose priors for motion control, resulting
in limited motion diversity and continuity in details. To address this, we
propose a framework that generates volumetric 4D sequences, where 3D shapes are
animated under given conditions (text-image guidance) with dynamic evolution in
shape and color across spatial and temporal dimensions, allowing for free
navigation and rendering from any direction. We first use a coherent 3D shape
and color modeling to encode the shape and color of each detailed 3D geometry
frame into a latent space. Then we propose a matrixized 4D sequence
representation allowing efficient diffusion model operation. Finally, we
introduce spatio-temporal diffusion for 4D volumetric generation under given
images and text prompts. Extensive experiments on the ShapeNet, 3DBiCar,
DeformingThings4D and Objaverse datasets for several tasks demonstrate that our
method effectively learns to generate high quality 3D shapes with consistent
color and coherent mesh animations, improving over the current methods. Our
code will be publicly available.

</details>


### [331] [BigReg: An Efficient Registration Pipeline for High-Resolution X-Ray and Light-Sheet Fluorescence Microscopy](https://arxiv.org/pdf/2404.14807)
*Siyuan Mei, Fuxin Fan, Mareike Thies, Mingxuan Gu, Fabian Wagner, Oliver Aust, Ina Erceg, Zeynab Mirzaei, Georgiana Neag, Yipeng Sun, Yixing Huang, Andreas Maier*

Main category: cs.CV

TL;DR: BigReg is a fast, two-stage pipeline for registering XRM and LSFM data, achieving high accuracy in aligning bone microstructures for osteoporosis research.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in registering high-resolution XRM and LSFM volumes for complementary analysis of bone microstructures in osteoporosis studies.

Method: A two-stage pipeline: coarse alignment using surface features and point-cloud methods, followed by refinement via modified cross-correlation.

Result: Achieves landmark distance of 8.36 µm ± 0.12 µm and fitness of 85.71% ± 1.02%, improving further with mutual information methods.

Conclusion: BigReg enables precise alignment of key microstructures, enhancing insights into osteoporosis pathology.

Abstract: Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy
(LSFM) have emerged as pivotal tools in preclinical research, particularly for
studying bone remodeling diseases such as osteoporosis. These modalities offer
micrometer-level resolution, and their integration allows for a complementary
examination of bone microstructures which is essential for analyzing functional
changes. However, registering high-resolution volumes from these independently
scanned modalities poses substantial challenges, especially in real-world and
reference-free scenarios. This paper presents BigReg, a fast, two-stage
pipeline designed for large-volume registration of XRM and LSFM data. The first
stage involves extracting surface features and applying two successive point
cloud-based methods for coarse alignment. The subsequent stage refines this
alignment using a modified cross-correlation technique, achieving precise
volumetric registration. Evaluations using expert-annotated landmarks and
augmented test data demonstrate that BigReg approaches the accuracy of
landmark-based registration with a landmark distance (LMD) of 8.36\,\textmu
m\,$\pm$\,0.12\,\textmu m and a landmark fitness (LM fitness) of
85.71\%\,$\pm$\,1.02\%. Moreover, BigReg can provide an optimal initialization
for mutual information-based methods which otherwise fail independently,
further reducing LMD to 7.24\,\textmu m\,$\pm$\,0.11\,\textmu m and increasing
LM fitness to 93.90\%\,$\pm$\,0.77\%. Ultimately, key microstructures, notably
lacunae in XRM and bone cells in LSFM, are accurately aligned, enabling
unprecedented insights into the pathology of osteoporosis.

</details>


### [332] [DeepMpMRI: Tensor-decomposition Regularized Learning for Fast and High-Fidelity Multi-Parametric Microstructural MR Imaging](https://arxiv.org/pdf/2405.03159)
*Wenxin Fan, Jian Cheng, Qiyuan Tian, Ruoyou Wu, Juan Zou, Zan Chen, Shanshan Wang*

Main category: cs.CV

TL;DR: DeepMpMRI is a deep learning framework for efficient and high-fidelity estimation of multiple microstructural parameters from sparse q-space data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating brain microstructure parameters from diffusion MRI are inefficient and lack accuracy due to isolated modeling and dense sampling.

Method: DeepMpMRI uses tensor-decomposition-based regularization and a Nesterov-based adaptive learning algorithm to optimize performance with sparse data.

Result: The framework achieves superior results on HCP and Alzheimer's datasets, with 4.5-15x acceleration over dense sampling.

Conclusion: DeepMpMRI offers an extendable, efficient solution for multi-model parameter estimation with fine-grained details.

Abstract: Deep learning has emerged as a promising approach for learning the nonlinear
mapping between diffusion-weighted MR images and tissue parameters, which
enables automatic and deep understanding of the brain microstructures. However,
the efficiency and accuracy in estimating multiple microstructural parameters
derived from multiple diffusion models are still limited since previous studies
tend to estimate parameter maps from distinct models with isolated signal
modeling and dense sampling. This paper proposes DeepMpMRI, an efficient
framework for fast and high-fidelity multiple microstructural parameter
estimation from multiple models using highly sparse sampled q-space data.
DeepMpMRI is equipped with a newly designed tensor-decomposition-based
regularizer to effectively capture fine details by exploiting the
high-dimensional correlation across microstructural parameters. In addition, we
introduce a Nesterov-based adaptive learning algorithm that optimizes the
regularization parameter dynamically to enhance the performance. DeepMpMRI is
an extendable framework capable of incorporating flexible network architecture.
Experimental results on the HCP dataset and the Alzheimer's disease dataset
both demonstrate the superiority of our approach over 5 state-of-the-art
methods in simultaneously estimating multi-model microstructural parameter maps
for DKI and NODDI model with fine-grained details both quantitatively and
qualitatively, achieving 4.5 - 15 $\times$ acceleration compared to the dense
sampling of a total of 270 diffusion gradients.

</details>


### [333] [View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with Adaptive View Synthesis](https://arxiv.org/pdf/2406.18012)
*Subin Varghese, Vedhus Hoskere*

Main category: cs.CV

TL;DR: The paper introduces Scene Anomaly Detection (Scene AD) for unsupervised pixel-wise anomaly localization in built environments, addressing challenges like varying viewpoints and lack of labeled data. It presents ToyCity dataset, augmentation strategies, and OmniAD method, showing a 64.33% improvement in performance.


<details>
  <summary>Details</summary>
Motivation: Automating anomaly detection in built environments is challenging due to varying viewpoints, multiple objects, and lack of labeled data, necessitating a new approach.

Method: The paper formalizes Scene AD, introduces ToyCity dataset, proposes data augmentation strategies, and develops OmniAD, a refined Reverse Distillation method.

Result: OmniAD with augmented views achieves a 64.33% increase in pixel-wise F1 score over baseline methods.

Conclusion: The work provides a comprehensive solution for Scene AD, including task definition, benchmark dataset, augmentation methods, and a robust baseline method (OmniAD).

Abstract: The built environment, encompassing critical infrastructure such as bridges
and buildings, requires diligent monitoring of unexpected anomalies or
deviations from a normal state in captured imagery. Anomaly detection methods
could aid in automating this task; however, deploying anomaly detection
effectively in such environments presents significant challenges that have not
been evaluated before. These challenges include camera viewpoints that vary,
the presence of multiple objects within a scene, and the absence of labeled
anomaly data for training. To address these comprehensively, we introduce and
formalize Scene Anomaly Detection (Scene AD) as the task of unsupervised,
pixel-wise anomaly localization under these specific real-world conditions.
Evaluating progress in Scene AD required the development of ToyCity, the first
multi-object, multi-view real-image dataset, for unsupervised anomaly
detection. Our initial evaluations using ToyCity revealed that established
anomaly detection baselines struggle to achieve robust pixel-level
localization. To address this, two data augmentation strategies were created to
generate additional synthetic images of non-anomalous regions to enhance
generalizability. However, the addition of these synthetic images alone only
provided minor improvements. Thus, OmniAD, a refinement of the Reverse
Distillation methodology, was created to establish a stronger baseline. Our
experiments demonstrate that OmniAD, when used with augmented views, yields a
64.33\% increase in pixel-wise \(F_1\) score over Reverse Distillation with no
augmentation. Collectively, this work offers the Scene AD task definition, the
ToyCity benchmark, the view synthesis augmentation approaches, and the OmniAD
method. Project Page: https://drags99.github.io/OmniAD/

</details>


### [334] [Interactive Rendering of Relightable and Animatable Gaussian Avatars](https://arxiv.org/pdf/2407.10707)
*Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou*

Main category: cs.CV

TL;DR: A method using Gaussian Splatting for efficient creation of relightable and animatable avatars from sparse-view or monocular videos, achieving interactive rendering speeds.


<details>
  <summary>Details</summary>
Motivation: To address the slow training and rendering processes in previous methods relying on neural radiance fields or ray tracing for avatar creation.

Method: Decouples body materials and lighting using Gaussian Splatting, interpolates attributes from a canonical body mesh, deforms Gaussians via forward skinning, and computes shading with learnable environment light. Fast shadow modeling is achieved through rasterization.

Result: Achieves interactive frame rates (6.9 fps) and higher quality rendering compared to previous works on synthetic and real datasets.

Conclusion: The proposed method is simple, efficient, and enables interactive rendering of avatars under novel viewpoints, poses, and lighting.

Abstract: Creating relightable and animatable avatars from multi-view or monocular
videos is a challenging task for digital human creation and virtual reality
applications. Previous methods rely on neural radiance fields or ray tracing,
resulting in slow training and rendering processes. By utilizing Gaussian
Splatting, we propose a simple and efficient method to decouple body materials
and lighting from sparse-view or monocular avatar videos, so that the avatar
can be rendered simultaneously under novel viewpoints, poses, and lightings at
interactive frame rates (6.9 fps). Specifically, we first obtain the canonical
body mesh using a signed distance function and assign attributes to each mesh
vertex. The Gaussians in the canonical space then interpolate from nearby body
mesh vertices to obtain the attributes. We subsequently deform the Gaussians to
the posed space using forward skinning, and combine the learnable environment
light with the Gaussian attributes for shading computation. To achieve fast
shadow modeling, we rasterize the posed body mesh from dense viewpoints to
obtain the visibility. Our approach is not only simple but also fast enough to
allow interactive rendering of avatar animation under environmental light
changes. Experiments demonstrate that, compared to previous works, our method
can render higher quality results at a faster speed on both synthetic and real
datasets.

</details>


### [335] [SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint Alignment of Images](https://arxiv.org/pdf/2407.11850)
*Nir Barel, Ron Shapira Weber, Nir Mualem, Shahaf E. Finder, Oren Freifeld*

Main category: cs.CV

TL;DR: SpaceJAM is a novel, efficient method for Joint Alignment (JA) of images, reducing complexity and computational demands while matching existing methods' performance.


<details>
  <summary>Details</summary>
Motivation: Existing JA methods face challenges like high complexity, geometric distortions, and reliance on expensive models with extensive regularization.

Method: SpaceJAM uses a compact architecture (16K parameters) without regularization or atlas maintenance, simplifying the JA process.

Result: SpaceJAM achieves comparable alignment performance on SPair-71K and CUB datasets while reducing computational demands and offering a 10x speedup.

Conclusion: SpaceJAM provides a rapid, effective solution for image alignment, making JA more accessible and efficient.

Abstract: The unsupervised task of Joint Alignment (JA) of images is beset by
challenges such as high complexity, geometric distortions, and convergence to
poor local or even global optima. Although Vision Transformers (ViT) have
recently provided valuable features for JA, they fall short of fully addressing
these issues. Consequently, researchers frequently depend on expensive models
and numerous regularization terms, resulting in long training times and
challenging hyperparameter tuning. We introduce the Spatial Joint Alignment
Model (SpaceJAM), a novel approach that addresses the JA task with efficiency
and simplicity. SpaceJAM leverages a compact architecture with only 16K
trainable parameters and uniquely operates without the need for regularization
or atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate
that SpaceJAM matches the alignment capabilities of existing methods while
significantly reducing computational demands and achieving at least a 10x
speedup. SpaceJAM sets a new standard for rapid and effective image alignment,
making the process more accessible and efficient. Our code is available at:
https://bgu-cs-vil.github.io/SpaceJAM/.

</details>


### [336] [Continual Distillation Learning: Knowledge Distillation in Prompt-based Continual Learning](https://arxiv.org/pdf/2407.13911)
*Qifan Zhang, Yunhui Guo, Yu Xiang*

Main category: cs.CV

TL;DR: The paper introduces Continual Distillation Learning (CDL) to enhance prompt-based continual learning models using knowledge distillation (KD), proposing a novel method called KDP for better performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve inference efficiency in prompt-based continual learning by distilling knowledge from a large ViT to a smaller one, addressing limitations of existing KD methods.

Method: The authors propose Knowledge Distillation based on Prompts (KDP), inserting globally accessible prompts into the student model's frozen ViT backbone.

Result: KDP outperforms existing KD methods like logit and feature distillation in the CDL setup.

Conclusion: The KDP method effectively enhances distillation performance for prompt-based continual learning models.

Abstract: We introduce the problem of continual distillation learning (CDL) in order to
use knowledge distillation (KD) to improve prompt-based continual learning (CL)
models. The CDL problem is valuable to study since the use of a larger vision
transformer (ViT) leads to better performance in prompt-based continual
learning. The distillation of knowledge from a large ViT to a small ViT
improves the inference efficiency for prompt-based CL models. We empirically
found that existing KD methods such as logit distillation and feature
distillation cannot effectively improve the student model in the CDL setup. To
address this issue, we introduce a novel method named Knowledge Distillation
based on Prompts (KDP), in which globally accessible prompts specifically
designed for knowledge distillation are inserted into the frozen ViT backbone
of the student model. We demonstrate that our KDP method effectively enhances
the distillation performance in comparison to existing KD methods in the CDL
setup.

</details>


### [337] [KIND: Knowledge Integration and Diversion for Training Decomposable Models](https://arxiv.org/pdf/2408.07337)
*Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng*

Main category: cs.CV

TL;DR: KIND is a novel pre-training method using SVD to create decomposable models, addressing deployment and negative transfer issues in traditional pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Traditional pre-trained models face deployment challenges and negative transfer due to fixed sizes and task discrepancies.

Method: KIND integrates SVD to structure models into learngenes (class-agnostic) and tailors (class-specific), using a class gate for knowledge diversion.

Result: Models pre-trained with KIND can be decomposed and recombined for diverse deployments, mitigating domain shifts by transferring only learngenes.

Conclusion: KIND offers a flexible and effective solution for pre-trained models, improving adaptability and mitigating negative transfer.

Abstract: Pre-trained models have become the preferred backbone due to the increasing
complexity of model parameters. However, traditional pre-trained models often
face deployment challenges due to their fixed sizes, and are prone to negative
transfer when discrepancies arise between training tasks and target tasks. To
address this, we propose KIND, a novel pre-training method designed to
construct decomposable models. KIND integrates knowledge by incorporating
Singular Value Decomposition (SVD) as a structural constraint, with each basic
component represented as a combination of a column vector, singular value, and
row vector from U, \Sigma, and V^\top matrices. These components are
categorized into learngenes for encapsulating class-agnostic knowledge and
tailors for capturing class-specific knowledge, with knowledge diversion
facilitated by a class gate mechanism during training. Extensive experiments
demonstrate that models pre-trained with KIND can be decomposed into learngenes
and tailors, which can be adaptively recombined for diverse
resource-constrained deployments. Moreover, for tasks with large domain shifts,
transferring only learngenes with task-agnostic knowledge, when combined with
randomly initialized tailors, effectively mitigates domain shifts. Code will be
made available at https://github.com/Te4P0t/KIND.

</details>


### [338] [Diffusion based Semantic Outlier Generation via Nuisance Awareness for Out-of-Distribution Detection](https://arxiv.org/pdf/2408.14841)
*Suhee Yoon, Sanghyu Yoon, Ye Seul Sim, Sungik Choi, Kyungeun Lee, Hye-Seung Cho, Hankook Lee, Woohyung Lim*

Main category: cs.CV

TL;DR: The paper introduces SONA, a framework for generating challenging OOD samples using diffusion models to improve OOD detection by focusing on semantic distinctions.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods produce outliers too distinct from ID, limiting their ability to capture subtle differences.

Method: SONA leverages pixel-space ID samples via diffusion models, controlling semantic and nuisance regions to generate outliers with semantic discrepancies and nuisance resemblance.

Result: SONA achieves an AUROC of 88% on near-OOD datasets, outperforming baselines by ~6%.

Conclusion: SONA effectively improves OOD detection by generating meaningful outliers, enhancing focus on semantic distinctions.

Abstract: Out-of-distribution (OOD) detection, which determines whether a given sample
is part of the in-distribution (ID), has recently shown promising results
through training with synthetic OOD datasets. Nonetheless, existing methods
often produce outliers that are considerably distant from the ID, showing
limited efficacy for capturing subtle distinctions between ID and OOD. To
address these issues, we propose a novel framework, Semantic Outlier generation
via Nuisance Awareness (SONA), which notably produces challenging outliers by
directly leveraging pixel-space ID samples through diffusion models. Our
approach incorporates SONA guidance, providing separate control over semantic
and nuisance regions of ID samples. Thereby, the generated outliers achieve two
crucial properties: (i) they present explicit semantic-discrepant information,
while (ii) maintaining various levels of nuisance resemblance with ID.
Furthermore, the improved OOD detector training with SONA outliers facilitates
learning with a focus on semantic distinctions. Extensive experiments
demonstrate the effectiveness of our framework, achieving an impressive AUROC
of 88% on near-OOD datasets, which surpasses the performance of baseline
methods by a significant margin of approximately 6%.

</details>


### [339] [Semantics-Oriented Multitask Learning for DeepFake Detection: A Joint Embedding Approach](https://arxiv.org/pdf/2408.16305)
*Mian Zou, Baosheng Yu, Yibing Zhan, Siwei Lyu, Kede Ma*

Main category: cs.CV

TL;DR: The paper proposes a semantics-oriented multitask learning approach for DeepFake detection, leveraging joint embedding of face images and text descriptions, and automated dataset expansion to improve generalizability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current DeepFake detection methods focus on binary classification with manipulation-oriented auxiliary tasks, limiting generalizability. The paper aims to enhance detection by capturing face semantics relationships.

Method: The method includes automated dataset expansion for global and local face attributes, joint embedding of images and text descriptions, and bi-level optimization for dynamic loss weighting.

Result: Experiments on six datasets show improved generalizability and interpretability in DeepFake detection.

Conclusion: The proposed approach advances DeepFake detection by integrating semantics-oriented multitask learning and automated techniques, offering better performance and human-understandable explanations.

Abstract: In recent years, the multimedia forensics and security community has seen
remarkable progress in multitask learning for DeepFake (i.e., face forgery)
detection. The prevailing approach has been to frame DeepFake detection as a
binary classification problem augmented by manipulation-oriented auxiliary
tasks. This scheme focuses on learning features specific to face manipulations
with limited generalizability. In this paper, we delve deeper into
semantics-oriented multitask learning for DeepFake detection, capturing the
relationships among face semantics via joint embedding. We first propose an
automated dataset expansion technique that broadens current face forgery
datasets to support semantics-oriented DeepFake detection tasks at both the
global face attribute and local face region levels. Furthermore, we resort to
the joint embedding of face images and labels (depicted by text descriptions)
for prediction. This approach eliminates the need for manually setting
task-agnostic and task-specific parameters, which is typically required when
predicting multiple labels directly from images. In addition, we employ
bi-level optimization to dynamically balance the fidelity loss weightings of
various tasks, making the training process fully automated. Extensive
experiments on six DeepFake datasets show that our method improves the
generalizability of DeepFake detection and renders some degree of model
interpretation by providing human-understandable explanations.

</details>


### [340] [On the Generalizability of Foundation Models for Crop Type Mapping](https://arxiv.org/pdf/2409.09451)
*Yi-Chia Chang, Adam J. Stewart, Favyen Bastani, Piper Wolters, Shreya Kannan, George R. Huber, Jingtong Wang, Arindam Banerjee*

Main category: cs.CV

TL;DR: EO foundation models like SSL4EO-S12 outperform general models like ImageNet for crop classification, but require more labeled data to address class imbalance.


<details>
  <summary>Details</summary>
Motivation: To assess the generalization and potential geospatial bias of EO foundation models across diverse geographic locations.

Method: Evaluated three models (SSL4EO-S12, SatlasPretrain, ImageNet) on five crop classification datasets spanning five continents.

Result: SSL4EO-S12 performed best, but 900 labeled images were needed to mitigate class imbalance and improve accuracy.

Conclusion: EO-specific pre-trained models are superior, but addressing class imbalance requires more labeled data.

Abstract: Foundation models pre-trained using self-supervised learning have shown
powerful transfer learning capabilities on various downstream tasks, including
language understanding, text generation, and image recognition. The Earth
observation (EO) field has produced several foundation models pre-trained
directly on multispectral satellite imagery for applications like precision
agriculture, wildfire and drought monitoring, and natural disaster response.
However, few studies have investigated the ability of these models to
generalize to new geographic locations, and potential concerns of geospatial
bias -- models trained on data-rich developed nations not transferring well to
data-scarce developing nations -- remain. We evaluate three popular EO
foundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop
classification datasets across five continents. Results show that pre-trained
weights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform
general pre-trained weights like ImageNet. While only 100 labeled images are
sufficient for achieving high overall accuracy, 900 images are required to
mitigate class imbalance and improve average accuracy.

</details>


### [341] [ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](https://arxiv.org/pdf/2409.15250)
*Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel*

Main category: cs.CV

TL;DR: The paper evaluates robotic foundation models' visual generalization, identifies issues like catastrophic forgetting, and proposes a solution (ReVLA) to improve performance in out-of-domain tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in robotic foundation models for visual out-of-domain scenarios, caused by limited training data variations or catastrophic forgetting.

Method: Study three robotic foundation models, propose an evaluation framework, and introduce a gradual backbone reversal approach (ReVLA) to mitigate catastrophic forgetting.

Result: ReVLA improves OpenVLA's performance by 77% and 66% in grasping and lifting tasks for visual out-of-domain scenarios.

Conclusion: The proposed ReVLA method effectively addresses catastrophic forgetting, enhancing visual generalization in robotic foundation models.

Abstract: Recent progress in large language models and access to large-scale robotic
datasets has sparked a paradigm shift in robotics models transforming them into
generalists able to adapt to various tasks, scenes, and robot modalities. A
large step for the community are open Vision Language Action models which
showcase strong performance in a wide variety of tasks. In this work, we study
the visual generalization capabilities of three existing robotic foundation
models, and propose a corresponding evaluation framework. Our study shows that
the existing models do not exhibit robustness to visual out-of-domain
scenarios. This is potentially caused by limited variations in the training
data and/or catastrophic forgetting, leading to domain limitations in the
vision foundation models. We further explore OpenVLA, which uses two
pre-trained vision foundation models and is, therefore, expected to generalize
to out-of-domain experiments. However, we showcase catastrophic forgetting by
DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.
To overcome the aforementioned issue of visual catastrophic forgetting, we
propose a gradual backbone reversal approach founded on model merging. This
enables OpenVLA -- which requires the adaptation of the visual backbones during
initial training -- to regain its visual generalization ability. Regaining this
capability enables our ReVLA model to improve over OpenVLA by a factor of 77\%
and 66\% for grasping and lifting in visual OOD tasks. Comprehensive
evaluations, episode rollouts and model weights are available on the ReVLA Page

</details>


### [342] [Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People](https://arxiv.org/pdf/2410.00275)
*Rohaifa Khaldi, Domingo Alcaraz-Segura, Ignacio Sánchez-Herrera, Javier Martinez-Lopez, Carlos Javier Navarro, Siham Tabik*

Main category: cs.CV

TL;DR: The paper introduces FLIPS, a dataset for categorizing social media images of human-nature interactions, and evaluates large models (LLMs, LVMs, LVLMs) for this task, focusing on cost, productivity, scalability, and quality.


<details>
  <summary>Details</summary>
Motivation: The challenge of categorizing diverse social media images with abstract concepts and inconsistent visual patterns, compounded by the lack of public benchmarks and human supervision in prior work.

Method: Introduces the FLIPS dataset and evaluates various large models (LLMs, LVMs, LVLMs) using adaptation methods, assessing performance metrics like cost and scalability.

Result: Performance of large models is assessed and reported, addressing the challenges of social media image categorization.

Conclusion: Large models offer promising solutions for categorizing social media images, with FLIPS providing a benchmark for future research.

Abstract: Social media images provide valuable insights for modeling, mapping, and
understanding human interactions with natural and cultural heritage. However,
categorizing these images into semantically meaningful groups remains highly
complex due to the vast diversity and heterogeneity of their visual content as
they contain an open-world human and nature elements. This challenge becomes
greater when categories involve abstract concepts and lack consistent visual
patterns. Related studies involve human supervision in the categorization
process and the lack of public benchmark datasets make comparisons between
these works unfeasible. On the other hand, the continuous advances in large
models, including Large Language Models (LLMs), Large Visual Models (LVMs), and
Large Visual Language Models (LVLMs), provide a large space of unexplored
solutions. In this work 1) we introduce FLIPS a dataset of Flickr images that
capture the interaction between human and nature, and 2) evaluate various
solutions based on different types and combinations of large models using
various adaptation methods. We assess and report their performance in terms of
cost, productivity, scalability, and result quality to address the challenges
of social media image categorization.

</details>


### [343] [Deep activity propagation via weight initialization in spiking neural networks](https://arxiv.org/pdf/2410.00580)
*Aurora Micheli, Olaf Booij, Jan van Gemert, Nergis Tömen*

Main category: cs.CV

TL;DR: The paper proposes an optimal weight initialization method for deep Spiking Neural Networks (SNNs) to address vanishing spikes and information loss, demonstrating improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Training deep SNNs is challenging due to information loss and vanishing spikes caused by binary spike quantization. Existing ANN initialization methods don't account for SNNs' unique properties.

Method: Derived a tailored weight initialization method for SNNs, considering quantization. Validated through theoretical analysis and simulations on 100-layer SNNs and MNIST experiments.

Result: The method enables spike propagation in deep SNNs without loss, improves accuracy, and shows robustness to hyperparameter variations.

Conclusion: The proposed initialization is effective for deep SNNs, addressing key challenges and outperforming standard approaches.

Abstract: Spiking Neural Networks (SNNs) and neuromorphic computing offer bio-inspired
advantages such as sparsity and ultra-low power consumption, providing a
promising alternative to conventional networks. However, training deep SNNs
from scratch remains a challenge, as SNNs process and transmit information by
quantizing the real-valued membrane potentials into binary spikes. This can
lead to information loss and vanishing spikes in deeper layers, impeding
effective training. While weight initialization is known to be critical for
training deep neural networks, what constitutes an effective initial state for
a deep SNN is not well-understood. Existing weight initialization methods
designed for conventional networks (ANNs) are often applied to SNNs without
accounting for their distinct computational properties. In this work we derive
an optimal weight initialization method specifically tailored for SNNs, taking
into account the quantization operation. We show theoretically that, unlike
standard approaches, this method enables the propagation of activity in deep
SNNs without loss of spikes. We demonstrate this behavior in numerical
simulations of SNNs with up to 100 layers across multiple time steps. We
present an in-depth analysis of the numerical conditions, regarding layer width
and neuron hyperparameters, which are necessary to accurately apply our
theoretical findings. Furthermore, our experiments on MNIST demonstrate higher
accuracy and faster convergence when using the proposed weight initialization
scheme. Finally, we show that the newly introduced weight initialization is
robust against variations in several network and neuron hyperparameters.

</details>


### [344] [Rethinking Text-Promptable Surgical Instrument Segmentation with Robust Framework](https://arxiv.org/pdf/2411.12199)
*Tae-Min Choi, Juyoun Park*

Main category: cs.CV

TL;DR: The paper introduces Robust text-promptable Surgical Instrument Segmentation (R-SIS) to address limitations in existing vision-based and promptable segmentation methods, which struggle with unseen or dynamically emerging instruments and false-positive predictions.


<details>
  <summary>Details</summary>
Motivation: Current surgical instrument segmentation methods are limited by predefined categories or assumptions about instrument presence, reducing their practicality in real-world surgical environments where uncertainty exists.

Method: The authors propose R-SIS, a task where prompts are issued for all candidate categories without prior knowledge of instrument presence, requiring models to distinguish visible instruments and generate masks only when instruments are present.

Result: Evaluation on surgical video datasets shows significant false-positive predictions in existing methods under the R-SIS protocol, highlighting a mismatch with real-world conditions.

Conclusion: The paper advocates for benchmarks that account for prompt uncertainty and instrument absence, emphasizing the need for more robust segmentation methods in surgical applications.

Abstract: Surgical instrument segmentation is an essential component of
computer-assisted and robotic surgery systems. Vision-based segmentation models
typically produce outputs limited to a predefined set of instrument categories,
which restricts their applicability in interactive systems and robotic task
automation. Promptable segmentation methods allow selective predictions based
on textual prompts. However, they often rely on the assumption that the
instruments present in the scene are already known, and prompts are generated
accordingly, limiting their ability to generalize to unseen or dynamically
emerging instruments. In practical surgical environments, where instrument
existence information is not provided, this assumption does not hold
consistently, resulting in false-positive segmentation. To address these
limitations, we formulate a new task called Robust text-promptable Surgical
Instrument Segmentation (R-SIS). Under this setting, prompts are issued for all
candidate categories without access to instrument presence information. R-SIS
requires distinguishing which prompts refer to visible instruments and
generating masks only when such instruments are explicitly present in the
scene. This setting reflects practical conditions where uncertainty in
instrument presence is inherent. We evaluate existing segmentation methods
under the R-SIS protocol using surgical video datasets and observe substantial
false-positive predictions in the absence of ground-truth instruments. These
findings demonstrate a mismatch between current evaluation protocols and
real-world use cases, and support the need for benchmarks that explicitly
account for prompt uncertainty and instrument absence.

</details>


### [345] [Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection](https://arxiv.org/pdf/2411.15633)
*Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, Li Yuan*

Main category: cs.CV

TL;DR: The paper addresses the asymmetry phenomenon in AI-generated image (AIGI) detection, where detectors overfit to limited fake patterns. It proposes using SVD to decompose the feature space, preserving pre-trained knowledge while learning fake patterns, improving generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the failure generalization in AIGI detection caused by the asymmetry phenomenon, where detectors overfit to monotonous fake patterns, limiting feature space expressivity.

Method: The method employs Singular Value Decomposition (SVD) to decompose the feature space into orthogonal subspaces, freezing principal components and adapting the rest to preserve pre-trained knowledge while learning fake patterns.

Result: The approach ensures orthogonality, expands the feature space rank, minimizes overfitting, and enhances generalization, outperforming full-parameter and LoRA-based tuning methods.

Conclusion: The paper concludes that modeling the hierarchical relationship between real and fake images is crucial for superior generalization in AIGI detection.

Abstract: AI-generated images (AIGIs), such as natural or face images, have become
increasingly important yet challenging. In this paper, we start from a new
perspective to excavate the reason behind the failure generalization in AIGI
detection, named the \textit{asymmetry phenomenon}, where a naively trained
detector tends to favor overfitting to the limited and monotonous fake
patterns, causing the feature space to become highly constrained and
low-ranked, which is proved seriously limiting the expressivity and
generalization. One potential remedy is incorporating the pre-trained knowledge
within the vision foundation models (higher-ranked) to expand the feature
space, alleviating the model's overfitting to fake. To this end, we employ
Singular Value Decomposition (SVD) to decompose the original feature space into
\textit{two orthogonal subspaces}. By freezing the principal components and
adapting only the remained components, we preserve the pre-trained knowledge
while learning fake patterns. Compared to existing full-parameters and
LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the
higher rank of the whole feature space, effectively minimizing overfitting and
enhancing generalization. We finally identify a crucial insight: our method
implicitly learns \textit{a vital prior that fakes are actually derived from
the real}, indicating a hierarchical relationship rather than independence.
Modeling this prior, we believe, is essential for achieving superior
generalization. Our codes are publicly available at
\href{https://github.com/YZY-stack/Effort-AIGI-Detection}{GitHub}.

</details>


### [346] [DiffDesign: Controllable Diffusion with Meta Prior for Efficient Interior Design Generation](https://arxiv.org/pdf/2411.16301)
*Yuxuan Yang, Tao Geng, Jingyao Wang, Changwen Zheng, Fuchun Sun*

Main category: cs.CV

TL;DR: DiffDesign is a controllable diffusion model for interior design, addressing inefficiencies and discrepancies in generative outputs by leveraging meta priors and a specialized dataset.


<details>
  <summary>Details</summary>
Motivation: Current generative models for interior design lack practicality and controllability, leading to inefficiencies and mismatches with real-world needs.

Method: DiffDesign uses a pre-trained 2D diffusion model, disentangles cross-attention for attribute control, and enforces view consistency with an alignment module. It is fine-tuned on DesignHelper, a dataset of 400+ solutions.

Result: Experiments show DiffDesign is effective and robust across benchmarks.

Conclusion: DiffDesign offers a practical solution for efficient and controllable interior design generation.

Abstract: Interior design is a complex and creative discipline involving aesthetics,
functionality, ergonomics, and materials science. Effective solutions must meet
diverse requirements, typically producing multiple deliverables such as
renderings and design drawings from various perspectives. Consequently,
interior design processes are often inefficient and demand significant
creativity. With advances in machine learning, generative models have emerged
as a promising means of improving efficiency by creating designs from text
descriptions or sketches. However, few generative works focus on interior
design, leading to substantial discrepancies between outputs and practical
needs, such as differences in size, spatial scope, and the lack of controllable
generation quality. To address these challenges, we propose DiffDesign, a
controllable diffusion model with meta priors for efficient interior design
generation. Specifically, we utilize the generative priors of a 2D diffusion
model pre-trained on a large image dataset as our rendering backbone. We
further guide the denoising process by disentangling cross-attention control
over design attributes, such as appearance, pose, and size, and introduce an
optimal transfer-based alignment module to enforce view consistency.
Simultaneously, we construct an interior design-specific dataset, DesignHelper,
consisting of over 400 solutions across more than 15 spatial types and 15
design styles. This dataset helps fine-tune DiffDesign. Extensive experiments
conducted on various benchmark datasets demonstrate the effectiveness and
robustness of DiffDesign.

</details>


### [347] [Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential via Self-Attention Redirection Guidance](https://arxiv.org/pdf/2412.12974)
*Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang*

Main category: cs.CV

TL;DR: Attentive Eraser enhances pre-trained diffusion models for stable object removal by re-engineering self-attention mechanisms and introducing guidance for coherent content generation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with artifacts and incoherent repainting in object removal tasks.

Method: Proposes Attention Activation and Suppression (ASS) and Self-Attention Redirection Guidance (SARG) to prioritize background and guide generation.

Result: Outperforms training-based methods, works across architectures, and ensures plausible content.

Conclusion: Attentive Eraser is a scalable, tuning-free solution for effective object removal in diffusion models.

Abstract: Recently, diffusion models have emerged as promising newcomers in the field
of generative models, shining brightly in image generation. However, when
employed for object removal tasks, they still encounter issues such as
generating random artifacts and the incapacity to repaint foreground object
areas with appropriate content after removal. To tackle these problems, we
propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion
models for stable and effective object removal. Firstly, in light of the
observation that the self-attention maps influence the structure and shape
details of the generated images, we propose Attention Activation and
Suppression (ASS), which re-engineers the self-attention mechanism within the
pre-trained diffusion models based on the given mask, thereby prioritizing the
background over the foreground object during the reverse generation process.
Moreover, we introduce Self-Attention Redirection Guidance (SARG), which
utilizes the self-attention redirected by ASS to guide the generation process,
effectively removing foreground objects within the mask while simultaneously
generating content that is both plausible and coherent. Experiments demonstrate
the stability and effectiveness of Attentive Eraser in object removal across a
variety of pre-trained diffusion models, outperforming even training-based
methods. Furthermore, Attentive Eraser can be implemented in various diffusion
model architectures and checkpoints, enabling excellent scalability. Code is
available at https://github.com/Anonym0u3/AttentiveEraser.

</details>


### [348] [Unforgettable Lessons from Forgettable Images: Intra-Class Memorability Matters in Computer Vision](https://arxiv.org/pdf/2412.20761)
*Jie Jing, Qing Lin, Shuangpeng Han, Lucia Schiatti, Yen-Ling Kuo, Mengmi Zhang*

Main category: cs.CV

TL;DR: The paper introduces intra-class memorability, proposing a novel metric (ICMscore) and dataset (ICMD) to study why some images within the same class are more memorable. AI models trained on ICMD reveal surprising impacts on tasks like image recognition and continual learning, and a diffusion model is fine-tuned for memorability-controlled editing.


<details>
  <summary>Details</summary>
Motivation: To understand why certain images within the same class are more memorable than others, despite sharing category characteristics, and to explore the implications for AI tasks.

Method: Human behavior experiments to quantify memorability (ICMscore), creation of the ICMD dataset, and training AI models for tasks like memorability prediction, image recognition, and memorability-controlled editing.

Result: High-ICMscore images impair AI performance in recognition and continual learning, while low-ICMscore images improve it. A diffusion model successfully manipulates memorability.

Conclusion: The work advances understanding of intra-class memorability and its applications in computer vision, with public release of code, data, and models.

Abstract: We introduce intra-class memorability, where certain images within the same
class are more memorable than others despite shared category characteristics.
To investigate what features make one object instance more memorable than
others, we design and conduct human behavior experiments, where participants
are shown a series of images, and they must identify when the current image
matches the image presented a few steps back in the sequence. To quantify
memorability, we propose the Intra-Class Memorability score (ICMscore), a novel
metric that incorporates the temporal intervals between repeated image
presentations into its calculation. Furthermore, we curate the Intra-Class
Memorability Dataset (ICMD), comprising over 5,000 images across ten object
classes with their ICMscores derived from 2,000 participants' responses.
Subsequently, we demonstrate the usefulness of ICMD by training AI models on
this dataset for various downstream tasks: memorability prediction, image
recognition, continual learning, and memorability-controlled image editing.
Surprisingly, high-ICMscore images impair AI performance in image recognition
and continual learning tasks, while low-ICMscore images improve outcomes in
these tasks. Additionally, we fine-tune a state-of-the-art image diffusion
model on ICMD image pairs with and without masked semantic objects. The
diffusion model can successfully manipulate image elements to enhance or reduce
memorability. Our contributions open new pathways in understanding intra-class
memorability by scrutinizing fine-grained visual features behind the most and
least memorable images and laying the groundwork for real-world applications in
computer vision. We will release all code, data, and models publicly.

</details>


### [349] [A Separable Self-attention Inspired by the State Space Model for Computer Vision](https://arxiv.org/pdf/2501.02040)
*Juntao Zhang, Shaogeng Liu, Kun Bian, You Zhou, Pei Zhang, Jianning Liu, Jun Zhou, Bingyan Liu*

Main category: cs.CV

TL;DR: VMINet introduces a novel separable self-attention method inspired by Mamba, achieving competitive results in image tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between state space models (SSMs) and attention variants by leveraging Mamba's design concepts.

Method: Proposes VMINet, a simple architecture using novel separable self-attention modules and basic down-sampling layers.

Result: VMINet performs competitively in image classification and high-resolution dense prediction tasks.

Conclusion: VMINet successfully integrates Mamba's concepts into separable self-attention, offering a strong alternative to Transformers.

Abstract: Mamba is an efficient State Space Model (SSM) with linear computational
complexity. Although SSMs are not suitable for handling non-causal data, Vision
Mamba (ViM) methods still demonstrate good performance in tasks such as image
classification and object detection. Recent studies have shown that there is a
rich theoretical connection between state space models and attention variants.
We propose a novel separable self attention method, for the first time
introducing some excellent design concepts of Mamba into separable
self-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a
simple yet powerful prototype architecture, constructed solely by stacking our
novel attention modules with the most basic down-sampling layers. Notably,
VMINet differs significantly from the conventional Transformer architecture.
Our experiments demonstrate that VMINet has achieved competitive results on
image classification and high-resolution dense prediction tasks.Code is
available at: https://github.com/yws-wxs/VMINet.

</details>


### [350] [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/pdf/2501.12368)
*Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang*

Main category: cs.CV

TL;DR: IXC-2.5-Reward is a multi-modal reward model for LVLMs, addressing the lack of publicly available models. It aligns LVLMs with human preferences and excels in benchmarks, with applications in RL training, response selection, and data filtering.


<details>
  <summary>Details</summary>
Motivation: Publicly available multi-modal reward models for LVLMs are scarce, and proprietary models lack transparency. IXC-2.5-Reward aims to fill this gap.

Method: Developed IXC-2.5-Reward using a high-quality multi-modal preference corpus across diverse domains. Integrated with PPO for RL training.

Result: Achieves excellent results on multi-modal benchmarks and competitive performance on text-only benchmarks. Applications include RL training, response selection, and data filtering.

Conclusion: IXC-2.5-Reward is a robust, versatile, and open-source solution for improving LVLM outputs, with demonstrated effectiveness in multiple applications.

Abstract: Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at
https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward

</details>


### [351] [IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting](https://arxiv.org/pdf/2501.15641)
*Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu*

Main category: cs.CV

TL;DR: IP-Prompter is a training-free method for theme-specific image generation, using visual prompting to integrate reference images, avoiding computational overhead and overfitting risks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating diverse, multi-concept theme-specific images without fine-tuning, which is resource-intensive and prone to overfitting.

Method: Introduces IP-Prompter with Dynamic Visual Prompting (DVP), leveraging reference images as context for generative models without additional training.

Result: Outperforms state-of-the-art methods in character identity preservation, style consistency, and text alignment.

Conclusion: IP-Prompter provides a robust, flexible solution for theme-specific image generation, enabling diverse applications like story and character design.

Abstract: The stories and characters that captivate us as we grow up shape unique
fantasy worlds, with images serving as the primary medium for visually
experiencing these realms. Personalizing generative models through fine-tuning
with theme-specific data has become a prevalent approach in text-to-image
generation. However, unlike object customization, which focuses on learning
specific objects, theme-specific generation encompasses diverse elements such
as characters, scenes, and objects. Such diversity also introduces a key
challenge: how to adaptively generate multi-character, multi-concept, and
continuous theme-specific images (TSI). Moreover, fine-tuning approaches often
come with significant computational overhead, time costs, and risks of
overfitting. This paper explores a fundamental question: Can image generation
models directly leverage images as contextual input, similarly to how large
language models use text as context? To address this, we present IP-Prompter, a
novel training-free TSI generation method. IP-Prompter introduces visual
prompting, a mechanism that integrates reference images into generative models,
allowing users to seamlessly specify the target theme without requiring
additional training. To further enhance this process, we propose a Dynamic
Visual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to
improve the accuracy and quality of generated images. Our approach enables
diverse applications, including consistent story generation, character design,
realistic character generation, and style-guided image generation. Comparative
evaluations against state-of-the-art personalization methods demonstrate that
IP-Prompter achieves significantly better results and excels in maintaining
character identity preserving, style consistency and text alignment, offering a
robust and flexible solution for theme-specific image generation.

</details>


### [352] [Latent Action Learning Requires Supervision in the Presence of Distractors](https://arxiv.org/pdf/2502.00379)
*Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov*

Main category: cs.CV

TL;DR: LAOM improves latent action learning in distracting environments by 8x, and minimal supervision (2.5% ground-truth actions) boosts performance 4.2x.


<details>
  <summary>Details</summary>
Motivation: Prior latent action learning (LAPO) works well in distractor-free settings but struggles with real-world videos containing action-correlated distractors.

Method: Proposed LAOM, a modified LAPO, and tested it on Distracting Control Suite (DCS) with minimal supervision (2.5% ground-truth actions).

Result: LAOM improves latent action quality by 8x, and supervision enhances downstream performance by 4.2x.

Conclusion: Supervision during latent action model training is crucial in distracting environments, challenging the traditional LAM-first pipeline.

Abstract: Recently, latent action learning, pioneered by Latent Action Policies (LAPO),
have shown remarkable pre-training efficiency on observation-only data,
offering potential for leveraging vast amounts of video available on the web
for embodied AI. However, prior work has focused on distractor-free data, where
changes between observations are primarily explained by ground-truth actions.
Unfortunately, real-world videos contain action-correlated distractors that may
hinder latent action learning. Using Distracting Control Suite (DCS) we
empirically investigate the effect of distractors on latent action learning and
demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO
modification that improves the quality of latent actions by 8x, as measured by
linear probing. Importantly, we show that providing supervision with
ground-truth actions, as few as 2.5% of the full dataset, during latent action
learning improves downstream performance by 4.2x on average. Our findings
suggest that integrating supervision during Latent Action Models (LAM) training
is critical in the presence of distractors, challenging the conventional
pipeline of first learning LAM and only then decoding from latent to
ground-truth actions.

</details>


### [353] [Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization](https://arxiv.org/pdf/2502.01051)
*Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan*

Main category: cs.CV

TL;DR: The paper introduces Latent Reward Model (LRM) and Latent Preference Optimization (LPO) to align diffusion models with human preferences more efficiently by leveraging the noisy latent space.


<details>
  <summary>Details</summary>
Motivation: Existing methods for preference optimization in diffusion models rely on Vision-Language Models (VLMs) for pixel-level rewards, which are inefficient and noisy for step-level optimization.

Method: The proposed LRM repurposes diffusion model components to predict preferences in latent space, enabling LPO for direct step-level optimization.

Result: LPO improves alignment with human preferences (general, aesthetic, text-image) and achieves a 2.5-28x training speedup over existing methods.

Conclusion: LPO is a more efficient and effective method for preference optimization in diffusion models, leveraging latent space advantages.

Abstract: Preference optimization for diffusion models aims to align them with human
preferences for images. Previous methods typically use Vision-Language Models
(VLMs) as pixel-level reward models to approximate human preferences. However,
when used for step-level preference optimization, these models face challenges
in handling noisy images of different timesteps and require complex
transformations into pixel space. In this work, we show that pre-trained
diffusion models are naturally suited for step-level reward modeling in the
noisy latent space, as they are explicitly designed to process latent images at
various noise levels. Accordingly, we propose the Latent Reward Model (LRM),
which repurposes components of the diffusion model to predict preferences of
latent images at arbitrary timesteps. Building on LRM, we introduce Latent
Preference Optimization (LPO), a step-level preference optimization method
conducted directly in the noisy latent space. Experimental results indicate
that LPO significantly improves the model's alignment with general, aesthetic,
and text-image alignment preferences, while achieving a 2.5-28x training
speedup over existing preference optimization methods. Our code and models are
available at https://github.com/Kwai-Kolors/LPO.

</details>


### [354] [ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for Long-Tailed Megakaryocyte Classification](https://arxiv.org/pdf/2502.08200)
*Linghao Zhuang, Ying Zhang, Gege Yuan, Xingyue Zhao, Zhiping Jiang*

Main category: cs.CV

TL;DR: ActiveSSF framework combines active learning and self-supervised pretraining to classify megakaryocytes, addressing noise, data imbalance, and morphological complexity.


<details>
  <summary>Details</summary>
Motivation: Accurate megakaryocyte classification is vital for diagnosing myelodysplastic syndromes, but challenges like background noise, rare subtypes, and morphological variability hinder progress.

Method: ActiveSSF uses Gaussian filtering, K-means clustering, HSV analysis, adaptive sample selection, and prototype clustering to improve classification.

Result: ActiveSSF achieves state-of-the-art performance and enhances accuracy for rare megakaryocyte subtypes.

Conclusion: ActiveSSF demonstrates strong clinical potential by effectively addressing key challenges in megakaryocyte classification.

Abstract: Precise classification of megakaryocytes is crucial for diagnosing
myelodysplastic syndromes. Although self-supervised learning has shown promise
in medical image analysis, its application to classifying megakaryocytes in
stained slides faces three main challenges: (1) pervasive background noise that
obscures cellular details, (2) a long-tailed distribution that limits data for
rare subtypes, and (3) complex morphological variations leading to high
intra-class variability. To address these issues, we propose the ActiveSSF
framework, which integrates active learning with self-supervised pretraining.
Specifically, our approach employs Gaussian filtering combined with K-means
clustering and HSV analysis (augmented by clinical prior knowledge) for
accurate region-of-interest extraction; an adaptive sample selection mechanism
that dynamically adjusts similarity thresholds to mitigate class imbalance; and
prototype clustering on labeled samples to overcome morphological complexity.
Experimental results on clinical megakaryocyte datasets demonstrate that
ActiveSSF not only achieves state-of-the-art performance but also significantly
improves recognition accuracy for rare subtypes. Moreover, the integration of
these advanced techniques further underscores the practical potential of
ActiveSSF in clinical settings.

</details>


### [355] [VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks](https://arxiv.org/pdf/2502.11163)
*Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao*

Main category: cs.CV

TL;DR: VLMs show geographic recognition biases, favoring developed and populated regions, with privacy concerns raised.


<details>
  <summary>Details</summary>
Motivation: To evaluate and highlight regional biases in VLMs' geographic recognition capabilities.

Method: A benchmark of 1,200 images with geographic metadata was used to test four VLMs.

Result: VLMs achieved 53.8% city prediction accuracy but showed biases (-12.5% for less developed, -17.0% for sparse areas) and over-predicted certain locations.

Conclusion: VLMs exhibit significant regional biases and pose privacy risks; dataset and code are publicly available.

Abstract: Visual-Language Models (VLMs) have shown remarkable performance across
various tasks, particularly in recognizing geographic information from images.
However, VLMs still show regional biases in this task. To systematically
evaluate these issues, we introduce a benchmark consisting of 1,200 images
paired with detailed geographic metadata. Evaluating four VLMs, we find that
while these models demonstrate the ability to recognize geographic information
from images, achieving up to 53.8% accuracy in city prediction, they exhibit
significant biases. Specifically, performance is substantially higher for
economically developed and densely populated regions compared to less developed
(-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of
frequently over-predicting certain locations remain. For instance, they
consistently predict Sydney for images taken in Australia, shown by the low
entropy scores for these countries. The strong performance of VLMs also raises
privacy concerns, particularly for users who share images online without the
intent of being identified. Our code and dataset are publicly available at
https://github.com/uscnlp-lime/FairLocator.

</details>


### [356] [MomentSeeker: A Task-Oriented Benchmark For Long-Video Moment Retrieval](https://arxiv.org/pdf/2502.12558)
*Huaying Yuan, Jian Ni, Zheng Liu, Yueze Wang, Junjie Zhou, Zhengyang Liang, Bo Zhao, Zhao Cao, Zhicheng Dou, Ji-Rong Wen*

Main category: cs.CV

TL;DR: MomentSeeker is a new benchmark for long-video moment retrieval (LMVR), addressing limitations in existing benchmarks by using diverse, long videos and multi-level queries. It highlights challenges in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for long video understanding (LVU) are limited in video length, task diversity, or focus solely on end-to-end performance, failing to evaluate key moment retrieval.

Method: Proposes MomentSeeker, a benchmark with long, diverse videos (avg. 1200s) and multi-level queries (text, image, video). Evaluates generation-based (MLLMs) and retrieval-based approaches.

Result: Reveals significant challenges in accuracy and efficiency for long-video moment retrieval, despite advancements in MLLMs and fine-tuning.

Conclusion: MomentSeeker is released to advance research in long-video moment retrieval, addressing gaps in current benchmarks.

Abstract: Accurately locating key moments within long videos is crucial for solving
long video understanding (LVU) tasks. However, existing benchmarks are either
severely limited in terms of video length and task diversity, or they focus
solely on the end-to-end LVU performance, making them inappropriate for
evaluating whether key moments can be accurately accessed. To address this
challenge, we propose MomentSeeker, a novel benchmark for long-video moment
retrieval (LMVR), distinguished by the following features. First, it is created
based on long and diverse videos, averaging over 1200 seconds in duration and
collected from various domains, e.g., movie, anomaly, egocentric, and sports.
Second, it covers a variety of real-world scenarios in three levels:
global-level, event-level, object-level, covering common tasks like action
recognition, object localization, and causal reasoning, etc. Third, it
incorporates rich forms of queries, including text-only queries,
image-conditioned queries, and video-conditioned queries. On top of
MomentSeeker, we conduct comprehensive experiments for both generation-based
approaches (directly using MLLMs) and retrieval-based approaches (leveraging
video retrievers). Our results reveal the significant challenges in long-video
moment retrieval in terms of accuracy and efficiency, despite improvements from
the latest long-video MLLMs and task-specific fine-tuning. We have publicly
released MomentSeeker(https://yhy-2000.github.io/MomentSeeker/) to facilitate
future research in this area.

</details>


### [357] [Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization](https://arxiv.org/pdf/2502.13146)
*Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chan-wei Hu, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu*

Main category: cs.CV

TL;DR: Re-Align, a novel alignment framework, uses image retrieval and dual-preference datasets to reduce hallucinations in Vision Language Models (VLMs), outperforming prior methods in VQA tasks.


<details>
  <summary>Details</summary>
Motivation: Address cross-modal inconsistencies (hallucinations) in VLMs by integrating visual and textual preference signals, improving alignment beyond brute-force methods like DPO.

Method: Introduces Re-Align with image retrieval for dual-preference datasets and rDPO, an extension of DPO incorporating visual preference during fine-tuning.

Result: Re-Align reduces hallucinations effectively and improves performance in VQA tasks, maintaining robustness across VLM sizes/architectures.

Conclusion: Re-Align advances multimodal LLM alignment, enhancing reliability for cross-modal applications; code is publicly available.

Abstract: The emergence of large Vision Language Models (VLMs) has broadened the scope
and capabilities of single-modal Large Language Models (LLMs) by integrating
visual modalities, thereby unlocking transformative cross-modal applications in
a variety of real-world scenarios. Despite their impressive performance, VLMs
are prone to significant hallucinations, particularly in the form of
cross-modal inconsistencies. Building on the success of Reinforcement Learning
from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused
on applying direct preference optimization (DPO) on carefully curated datasets
to mitigate these issues. Yet, such approaches typically introduce preference
signals in a brute-force manner, neglecting the crucial role of visual
information in the alignment process. In this paper, we introduce Re-Align, a
novel alignment framework that leverages image retrieval to construct a
dual-preference dataset, effectively incorporating both textual and visual
preference signals. We further introduce rDPO, an extension of the standard
direct preference optimization that incorporates an additional visual
preference objective during fine-tuning. Our experimental results demonstrate
that Re-Align not only mitigates hallucinations more effectively than previous
methods but also yields significant performance gains in general visual
question-answering (VQA) tasks. Moreover, we show that Re-Align maintains
robustness and scalability across a wide range of VLM sizes and architectures.
This work represents a significant step forward in aligning multimodal LLMs,
paving the way for more reliable and effective cross-modal applications. We
release all the code in https://github.com/taco-group/Re-Align.

</details>


### [358] [Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study](https://arxiv.org/pdf/2503.06794)
*Yizheng Sun, Hao Li, Chang Xu, Hongpeng Zhou, Chenghua Lin, Riza Batista-Navarro, Jingyuan Sun*

Main category: cs.CV

TL;DR: Accelerated Vision-Language Models (VLMs) often change original answers post-acceleration, including converting correct answers to incorrect ones, highlighting a need for stability checks.


<details>
  <summary>Details</summary>
Motivation: To evaluate if accelerated VLMs maintain answer consistency, crucial for stability-focused applications like AI-based disease diagnosis.

Method: Tested four VLMs (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration methods on ten multi-modal benchmarks, analyzing answer changes and correctness.

Result: Accelerated models changed answers up to 20% of the time, with 6.5% converting correct answers to incorrect; input perturbations worsened inconsistencies.

Conclusion: Current VLM acceleration overlooks instance-level stability, necessitating checks for trustworthy real-world deployment.

Abstract: Vision-Language Models (VLMs) are powerful yet computationally intensive for
widespread practical deployments. To address such challenge without costly
re-training, post-training acceleration techniques like quantization and token
reduction are extensively explored. However, current acceleration evaluations
primarily target minimal overall performance degradation, overlooking a crucial
question: does the accelerated model still give the same answers to the same
questions as it did before acceleration? This is vital for stability-centered
industrial applications where consistently correct answers for specific, known
situations are paramount, such as in AI-based disease diagnosis. We
systematically investigate this for accelerated VLMs, testing four leading
models (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration
methods on ten multi-modal benchmarks. Our findings are stark: despite minimal
aggregate performance drops, accelerated models changed original answers up to
20% of the time. Critically, up to 6.5% of these changes converted correct
answers to incorrect. Input perturbations magnified these inconsistencies, and
the trend is confirmed by case studies with the medical VLM LLaVA-Med. This
research reveals a significant oversight in VLM acceleration, stressing an
urgent need for instance-level stability checks to ensure trustworthy
real-world deployment.

</details>


### [359] [Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion for Text-Driven Motion Generation](https://arxiv.org/pdf/2503.06897)
*Xingzu Zhan, Chen Xie, Honghang Chen, Haoran Sun, Xiaochun Mai*

Main category: cs.CV

TL;DR: HiSTF Mamba improves text-to-motion generation by combining spatial and temporal features more effectively, achieving high fidelity and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods for text-to-motion generation are inefficient and miss subtle joint-level cues, limiting their effectiveness.

Method: HiSTF Mamba uses Dual-Spatial Mamba, Bi-Temporal Mamba, and a Dynamic Spatiotemporal Fusion Module (DSFM) to capture fine-grained motion and temporal dependencies.

Result: Experiments on HumanML3D show HiSTF Mamba outperforms in fidelity and semantic alignment.

Conclusion: HiSTF Mamba offers a robust solution for text-to-motion generation, enhancing both spatial and temporal feature integration.

Abstract: Text-to-motion generation sits at the intersection of multimodal learning and
computer graphics and is gaining momentum because it can simplify content
creation for games, animation, robotics and virtual reality. Most current
methods stack spatial and temporal features in a straightforward way, which
adds redundancy and still misses subtle joint-level cues. We introduce HiSTF
Mamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and
a Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs
part-based and whole-body models in parallel, capturing both overall
coordination and fine-grained joint motion. The Bi-Temporal module scans
sequences forward and backward to encode short-term details and long-term
dependencies. DSFM removes redundant temporal information, extracts
complementary cues and fuses them with spatial features to build a richer
spatiotemporal representation. Experiments on the HumanML3D benchmark show that
HiSTF Mamba performs well across several metrics, achieving high fidelity and
tight semantic alignment between text and motion.

</details>


### [360] [Universal Incremental Learning: Mitigating Confusion from Inter- and Intra-task Distribution Randomness](https://arxiv.org/pdf/2503.07035)
*Sheng Luo, Yi Zhou, Tao Zhou*

Main category: cs.CV

TL;DR: The paper introduces Universal Incremental Learning (UIL), a more realistic IL scenario where task types and scales are unpredictable, and proposes MiCo to mitigate inter- and intra-task confusion.


<details>
  <summary>Details</summary>
Motivation: Existing IL methods assume predictable task increments, limiting real-world applicability. UIL addresses this by handling unpredictable class/domain increases and scales.

Method: Proposes MiCo: a multi-objective learning scheme for inter-task confusion and magnitude recalibration for intra-task imbalance.

Result: MiCo outperforms state-of-the-art methods in UIL and VIL scenarios on three benchmarks.

Conclusion: MiCo effectively addresses UIL challenges, offering a robust solution for dynamic, unpredictable learning environments.

Abstract: Incremental learning (IL) aims to overcome catastrophic forgetting of
previous tasks while learning new ones. Existing IL methods make strong
assumptions that the incoming task type will either only increases new classes
or domains (i.e. Class IL, Domain IL), or increase by a static scale in a
class- and domain-agnostic manner (i.e. Versatile IL (VIL)), which greatly
limit their applicability in the unpredictable and dynamic wild. In this work,
we investigate $\textbf{Universal Incremental Learning (UIL)}$, where a model
neither knows which new classes or domains will increase along sequential
tasks, nor the scale of the increments within each task. This uncertainty
prevents the model from confidently learning knowledge from all task
distributions and symmetrically focusing on the diverse knowledge within each
task distribution. Consequently, UIL presents a more general and realistic IL
scenario, making the model face confusion arising from inter-task and
intra-task distribution randomness. To $\textbf{Mi}$tigate both
$\textbf{Co}$nfusion, we propose a simple yet effective framework for UIL,
named $\textbf{MiCo}$. At the inter-task distribution level, we employ a
multi-objective learning scheme to enforce accurate and deterministic
predictions, and its effectiveness is further enhanced by a direction
recalibration module that reduces conflicting gradients. Moreover, at the
intra-task distribution level, we introduce a magnitude recalibration module to
alleviate asymmetrical optimization towards imbalanced class distribution.
Extensive experiments on three benchmarks demonstrate the effectiveness of our
method, outperforming existing state-of-the-art methods in both the UIL
scenario and the VIL scenario. Our code will be available at
$\href{https://github.com/rolsheng/UIL}{here}$.

</details>


### [361] [Customized SAM 2 for Referring Remote Sensing Image Segmentation](https://arxiv.org/pdf/2503.07266)
*Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang*

Main category: cs.CV

TL;DR: RS2-SAM 2 adapts SAM 2 for Referring Remote Sensing Image Segmentation (RRSIS) by aligning visual-text features, generating pseudo-mask prompts, and refining boundaries with text-guided loss.


<details>
  <summary>Details</summary>
Motivation: SAM 2 struggles with text-described RS scenes and prompt generation in RRSIS. RS2-SAM 2 addresses these challenges.

Method: Uses a union encoder for visual-text alignment, bidirectional fusion for scene adaptation, and a mask prompt generator for dense prompts. Introduces text-guided boundary loss.

Result: Achieves state-of-the-art performance on RRSIS benchmarks.

Conclusion: RS2-SAM 2 effectively adapts SAM 2 for RRSIS, improving segmentation accuracy and boundary refinement.

Abstract: Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target
objects in remote sensing (RS) images based on textual descriptions. Although
Segment Anything Model 2 (SAM 2) has shown remarkable performance in various
segmentation tasks, its application to RRSIS presents several challenges,
including understanding the text-described RS scenes and generating effective
prompts from text descriptions. To address these issues, we propose RS2-SAM 2,
a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS
features and textual features, providing pseudo-mask-based dense prompts, and
enforcing boundary constraints. Specifically, we first employ a union encoder
to jointly encode the visual and textual inputs, generating aligned visual and
text embeddings as well as multimodal class tokens. Then, we design a
bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align
adapted visual features with the visually enhanced text embeddings, improving
the model's interpretation of text-described RS scenes. Additionally, a mask
prompt generator is introduced to take the visual embeddings and class tokens
as input and produce a pseudo-mask as the dense prompt of SAM 2. To further
refine segmentation, we introduce a text-guided boundary loss to optimize
segmentation boundaries by computing text-weighted gradient differences.
Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2
achieves state-of-the-art performance.

</details>


### [362] [VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/pdf/2503.07575)
*Jen-tse Huang, Jiantong Qin, Jianping Zhang, Youliang Yuan, Wenxuan Wang, Jieyu Zhao*

Main category: cs.CV

TL;DR: The paper examines explicit and implicit social biases in Vision-Language Models (VLMs) through direct questioning and indirect tasks, evaluating models like Gemini-1.5 and GPT-4V.


<details>
  <summary>Details</summary>
Motivation: To uncover and differentiate between conscious (explicit) and subconscious (implicit) biases in VLMs, particularly regarding gender and racial disparities.

Method: For explicit bias: multiple-choice and Yes-No questions. For implicit bias: image description and form completion tasks. Evaluated models include Gemini-1.5, GPT-4V, and others.

Result: The study identifies biases in VLMs through structured tasks, with data and code made publicly available.

Conclusion: VLMs exhibit both explicit and implicit social biases, highlighting the need for bias mitigation strategies in AI development.

Abstract: This research investigates both explicit and implicit social biases exhibited
by Vision-Language Models (VLMs). The key distinction between these bias types
lies in the level of awareness: explicit bias refers to conscious, intentional
biases, while implicit bias operates subconsciously. To analyze explicit bias,
we directly pose questions to VLMs related to gender and racial differences:
(1) Multiple-choice questions based on a given image (e.g., "What is the
education level of the person in the image?") (2) Yes-No comparisons using two
images (e.g., "Is the person in the first image more educated than the person
in the second image?") For implicit bias, we design tasks where VLMs assist
users but reveal biases through their responses: (1) Image description tasks:
Models are asked to describe individuals in images, and we analyze disparities
in textual cues across demographic groups. (2) Form completion tasks: Models
draft a personal information collection form with 20 attributes, and we examine
correlations among selected attributes for potential biases. We evaluate
Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data
are publicly available at https://github.com/uscnlp-lime/VisBias.

</details>


### [363] [Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space](https://arxiv.org/pdf/2503.11094)
*Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, Xiao-Ping Zhang*

Main category: cs.CV

TL;DR: The paper introduces Open3DVQA, a benchmark to evaluate spatial reasoning in multimodal large language models (MLLMs) using 9k VQA samples collected via a semi-automated tool in a 3D urban simulator. Results show MLLMs excel in relative spatial relationships, perform similarly across perspectives, and benefit from fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the spatial reasoning capabilities of MLLMs, which are crucial for embodied agents, by creating a comprehensive benchmark.

Method: Developed Open3DVQA with 9k VQA samples using a semi-automated tool in a high-fidelity urban simulator, evaluating SOTA MLLMs on various spatial reasoning tasks.

Result: MLLMs perform better on relative spatial relationships, show similar abilities across perspectives, and improve significantly with fine-tuning.

Conclusion: Open3DVQA and its tools aim to inspire further research into MLLM spatial reasoning, with the benchmark available for public use.

Abstract: Spatial reasoning is a fundamental capability of embodied agents and has
garnered widespread attention in the field of multimodal large language models
(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to
comprehensively evaluate the spatial reasoning capacities of current
state-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists
of 9k VQA samples, collected using an efficient semi-automated tool in a
high-fidelity urban simulator. We evaluate several SOTA MLLMs across various
aspects of spatial reasoning, such as relative and absolute spatial
relationships, situational reasoning, and object-centric spatial attributes.
Our results reveal that: 1) MLLMs perform better at answering questions
regarding relative spatial relationships than absolute spatial relationships,
2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric
and allocentric perspectives, and 3) Fine-tuning large models significantly
improves their performance across different spatial reasoning tasks. We believe
that our open-source data collection tools and in-depth analyses will inspire
further research on MLLM spatial reasoning capabilities. The benchmark is
available at https://github.com/WeichenZh/Open3DVQA.

</details>


### [364] [LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation](https://arxiv.org/pdf/2503.13794)
*Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: The paper introduces LED, a method to enhance Open-Vocabulary Object Detection (OVD) by fusing hidden states from Large Language Models (LLMs) into detectors, avoiding bias from synthetic data pipelines.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted pipelines for synthetic training data in OVD introduce bias and overfit to prompts. The paper explores direct fusion of LLM hidden states into detectors, an under-explored approach.

Method: A zero-initialized cross-attention adapter is introduced to efficiently fuse LLM knowledge into object detectors. Intermediate LLM layers, rich in spatial semantics, are adapted, focusing on early layers.

Result: LED improves GroundingDINO by 3.82% on OmniLabel with minimal computational overhead (8.7% extra GFLOPs). Larger vision backbones further boost improvement to 6.22%.

Conclusion: LED effectively leverages LLM layers for visual grounding, demonstrating significant performance gains with efficient computational cost.

Abstract: Large foundation models trained on large-scale vision-language data can boost
Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the
hand-crafted pipelines often introduce bias and overfit to specific prompts. We
sidestep this issue by directly fusing hidden states from Large Language Models
(LLMs) into detectors-an avenue surprisingly under-explored. This paper
presents a systematic method to enhance visual grounding by utilizing decoder
layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention
adapter to enable efficient knowledge fusion from LLMs to object detectors, a
new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We
find that intermediate LLM layers already encode rich spatial semantics;
adapting only the early layers yields most of the gain. With Swin-T as the
vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at
just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to
6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths
further corroborate our design.

</details>


### [365] [CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2503.14232)
*Yuyang Xue, Edward Moroshko, Feng Chen, Jingyu Sun, Steven McDonagh, Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: CRCE is a new framework for precise concept erasure in text-to-image models, using LLMs to avoid under/over-erasure.


<details>
  <summary>Details</summary>
Motivation: Existing methods for concept erasure in diffusion models often fail, either leaving traces or removing unrelated concepts.

Method: CRCE leverages Large Language Models to identify and model related and distinct concepts for precise erasure.

Result: CRCE outperforms existing methods in erasing diverse concepts, including objects, identities, and abstract properties.

Conclusion: CRCE offers a more accurate solution for concept erasure, with plans to release a dataset and source code.

Abstract: Text-to-Image diffusion models can produce undesirable content that
necessitates concept erasure. However, existing methods struggle with
under-erasure, leaving residual traces of targeted concepts, or over-erasure,
mistakenly eliminating unrelated but visually similar concepts. To address
these limitations, we introduce CRCE, a novel concept erasure framework that
leverages Large Language Models to identify both semantically related concepts
that should be erased alongside the target and distinct concepts that should be
preserved. By explicitly modelling coreferential and retained concepts
semantically, CRCE enables more precise concept removal, without unintended
erasure. Experiments demonstrate that CRCE outperforms existing methods on
diverse erasure tasks, including real-world object, person identities, and
abstract intellectual property characteristics. The constructed dataset
CorefConcept and the source code will be release upon acceptance.

</details>


### [366] [Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis](https://arxiv.org/pdf/2503.15060)
*Imanol G. Estepa, Jesús M. Rodríguez-de-Vera, Ignacio Sarasúa, Bhalaji Nagarajan, Petia Radeva*

Main category: cs.CV

TL;DR: Sorcen is a unified SSL framework combining contrastive and reconstruction objectives, eliminating the need for external tokenizers or augmentations, and outperforms prior methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Unifying representation learning and generative modeling in SSL without relying on external tokenizers or additional augmentations.

Method: Introduces 'Echo Contrast', a synergic Contrastive-Reconstruction objective, generating echo samples in semantic token space for contrastive pairs. Operates on precomputed tokens to reduce overhead.

Result: Outperforms previous Unified SSL methods in linear probing, image generation, few-shot learning, and transfer learning, with 60.8% higher efficiency.

Conclusion: Sorcen advances Unified SSL by improving performance and efficiency, setting new benchmarks in the field.

Abstract: While representation learning and generative modeling seek to understand
visual data, unifying both domains remains unexplored. Recent Unified
Self-Supervised Learning (SSL) methods have started to bridge the gap between
both paradigms. However, they rely solely on semantic token reconstruction,
which requires an external tokenizer during training -- introducing a
significant overhead. In this work, we introduce Sorcen, a novel unified SSL
framework, incorporating a synergic Contrastive-Reconstruction objective. Our
Contrastive objective, "Echo Contrast", leverages the generative capabilities
of Sorcen, eliminating the need for additional image crops or augmentations
during training. Sorcen "generates" an echo sample in the semantic token space,
forming the contrastive positive pair. Sorcen operates exclusively on
precomputed tokens, eliminating the need for an online token transformation
during training, thereby significantly reducing computational overhead.
Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the
previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear
probing, unconditional image generation, few-shot learning, and transfer
learning, respectively, while being 60.8% more efficient. Additionally, Sorcen
surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA
performance in unconditional image generation, highlighting significant
improvements and breakthroughs in Unified SSL models.

</details>


### [367] [Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model](https://arxiv.org/pdf/2503.16282)
*Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie*

Main category: cs.CV

TL;DR: The paper introduces GFS-VL, a framework combining 3D vision-language models (VLMs) and few-shot samples to improve 3D point cloud segmentation, addressing sparse knowledge limitations and noisy pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: Existing GFS-PCS methods lack dense knowledge due to few-shot samples, while 3D VLMs offer rich but noisy class knowledge. The goal is to synergize these for better segmentation.

Method: GFS-VL uses prototype-guided pseudo-label selection and adaptive infilling to merge pseudo-labels and few-shot samples. It also introduces a novel-base mix strategy for training.

Result: Experiments show effectiveness across models and datasets. Two new benchmarks are introduced for better evaluation.

Conclusion: GFS-VL provides a robust foundation for advancing GFS-PCS, with code and benchmarks available for real-world applications.

Abstract: Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to
new classes with few support samples while retaining base class segmentation.
Existing GFS-PCS methods enhance prototypes via interacting with support or
query features but remain limited by sparse knowledge from few-shot samples.
Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world
novel classes, contain rich but noisy novel class knowledge. In this work, we
introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels
from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths
of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label
selection to filter low-quality regions, followed by an adaptive infilling
strategy that combines knowledge from pseudo-label contexts and few-shot
samples to adaptively label the filtered, unlabeled areas. Additionally, we
design a novel-base mix strategy to embed few-shot samples into training
scenes, preserving essential context for improved novel class learning.
Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we
introduce two challenging benchmarks with diverse novel classes for
comprehensive generalization evaluation. Experiments validate the effectiveness
of our framework across models and datasets. Our approach and benchmarks
provide a solid foundation for advancing GFS-PCS in the real world. The code is
at https://github.com/ZhaochongAn/GFS-VL

</details>


### [368] [GranQ: Granular Zero-Shot Quantization with Channel-Wise Activation Scaling in QAT](https://arxiv.org/pdf/2503.18339)
*Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park*

Main category: cs.CV

TL;DR: GranQ is a novel activation quantization framework for zero-shot quantization (ZSQ) that improves efficiency and accuracy by vectorizing per-channel scaling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: ZSQ methods struggle with activation distortion due to synthetic inputs and coarse scaling, especially in low-bit settings. GranQ aims to address this issue.

Method: GranQ applies per-channel scaling through vectorized computation, improving efficiency and maintaining fine-grained quantization granularity.

Result: GranQ outperforms state-of-the-art ZSQ methods, achieving up to 5.45% higher accuracy in 3-bit settings and surpassing full-precision baselines. It also reduces quantization latency.

Conclusion: GranQ's success suggests potential for future research beyond traditional ZSQ approaches focused on data generation and fine-tuning.

Abstract: Zero-shot quantization (ZSQ) enables neural network compression without
original training data, making it a promising solution for restricted data
access scenarios. To compensate for the lack of data, recent ZSQ methods
typically rely on synthetic inputs generated from the full-precision model.
However, these synthetic inputs often lead to activation distortion, especially
under low-bit settings. As a result, existing methods struggle to mitigate this
issue due to coarse activation scaling. To address this issue, we propose
GranQ, a novel activation quantization framework that efficiently applies
per-channel scaling through vectorized computation. In contrast to conventional
channel-wise methods, which apply vectorization only to the quantization step,
GranQ improves efficiency by vectorizing the scaling operation. This design
allows GranQ to maintain fine-grained quantization granularity with minimal
computational overhead, even in low-bit environments. Extensive experiments
under quantization-aware training (QAT) settings demonstrate that GranQ
consistently outperforms state-of-the-art ZSQ methods across CIFAR and
ImageNet. In particular, our method achieves up to 5.45% higher accuracy in the
3-bit setting on CIFAR-100 and even surpasses the full-precision baseline on
CIFAR-10. Furthermore, GranQ achieves significant speedup in quantization
latency over conventional per-channel methods, demonstrating improved
efficiency. With these findings, we anticipate that GranQ will inspire future
research beyond conventional ZSQ approaches centered on data generation and
model fine-tuning.

</details>


### [369] [LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions](https://arxiv.org/pdf/2503.20252)
*Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon*

Main category: cs.CV

TL;DR: LogicQA is a training-free, annotation-free framework for detecting and explaining logical anomalies in industrial processes, achieving SOTA performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting logical anomalies that appear visually normal but violate predefined constraints, providing explainability for industrial operators.

Method: LogicQA compiles automatically generated questions into a checklist, collects responses, and identifies logical constraint violations without training or annotations.

Result: Achieved 87.6% AUROC and 87.0% F1-max on MVTec LOCO AD benchmark, with strong performance on semiconductor SEM data.

Conclusion: LogicQA effectively detects and explains logical anomalies, proving its utility in industrial applications.

Abstract: Anomaly Detection (AD) focuses on detecting samples that differ from the
standard pattern, making it a vital tool in process control. Logical anomalies
may appear visually normal yet violate predefined constraints on object
presence, arrangement, or quantity, depending on reasoning and explainability.
We introduce LogicQA, a framework that enhances AD by providing industrial
operators with explanations for logical anomalies. LogicQA compiles
automatically generated questions into a checklist and collects responses to
identify violations of logical constraints. LogicQA is training-free,
annotation-free, and operates in a few-shot setting. We achieve
state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO
AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the
explanations of anomalies. Also, our approach has shown outstanding performance
on semiconductor SEM corporate data, further validating its effectiveness in
industrial applications.

</details>


### [370] [Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization](https://arxiv.org/pdf/2503.22577)
*Iñigo Pikabea, Iñaki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas*

Main category: cs.CV

TL;DR: A method to improve multilingual responses in VLMs by integrating text-only multilingual data during training, avoiding trade-offs in visual performance.


<details>
  <summary>Details</summary>
Motivation: Addressing Image-induced Fidelity Loss (IFL) in VLMs, where models generate English responses regardless of input language due to limited multilingual training data.

Method: Proposes continuous multilingual integration by injecting text-only multilingual data during visual instruction tuning to preserve the model's multilingual capabilities.

Result: Significantly improves linguistic fidelity across languages without degrading visual performance. Model merging improves language fidelity but harms visual performance.

Conclusion: The core method offers a scalable and effective solution to mitigate IFL, enabling robust multilingual alignment without trade-offs.

Abstract: Rapid advancements in Visual Language Models (VLMs) have transformed
multimodal understanding but are often constrained by generating English
responses regardless of the input language. This phenomenon has been termed as
Image-induced Fidelity Loss (IFL) and stems from limited multimodal
multilingual training data. To address this, we propose a continuous
multilingual integration strategy that injects text-only multilingual data
during visual instruction tuning, preserving the language model's original
multilingual capabilities. Extensive evaluations demonstrate that our approach
significantly improves linguistic fidelity across languages without degradation
in visual performance. We also explore model merging, which improves language
fidelity but comes at the cost of visual performance. In contrast, our core
method achieves robust multilingual alignment without trade-offs, offering a
scalable and effective path to mitigating IFL for global VLM adoption.

</details>


### [371] [Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos](https://arxiv.org/pdf/2504.04837)
*Zhi Zuo, Chenyi Zhuang, Pan Gao, Jie Qin, Hao Feng, Nicu Sebe*

Main category: cs.CV

TL;DR: A novel self-disentangled MAE framework for 4D point cloud video representation learning, addressing motion learning and geometry-dynamics gap without explicit knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on explicit knowledge for motion learning and struggle with the gap between low-level geometry and high-level dynamics in 4D data.

Method: Proposes a self-disentangled MAE with latent token alignment and shared decoder, plus three alignment objectives for temporal understanding.

Result: Achieves superior performance in downstream tasks, improving action segmentation accuracy by +3.8% on HOI4D.

Conclusion: The framework effectively learns expressive and transferable 4D representations without explicit motion knowledge.

Abstract: Self-supervised representation learning for point cloud videos remains a
challenging problem with two key limitations: (1) existing methods rely on
explicit knowledge to learn motion, resulting in suboptimal representations;
(2) prior Masked AutoEncoder (MAE) frameworks struggle to bridge the gap
between low-level geometry and high-level dynamics in 4D data. In this work, we
propose a novel self-disentangled MAE for learning expressive, discriminative,
and transferable 4D representations. To overcome the first limitation, we learn
motion by aligning high-level semantics in the latent space \textit{without any
explicit knowledge}. To tackle the second, we introduce a
\textit{self-disentangled learning} strategy that incorporates the latent token
with the geometry token within a shared decoder, effectively disentangling
low-level geometry and high-level semantics. In addition to the reconstruction
objective, we employ three alignment objectives to enhance temporal
understanding, including frame-level motion and video-level global information.
We show that our pre-trained encoder surprisingly discriminates spatio-temporal
representation without further fine-tuning. Extensive experiments on
MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17 demonstrate the
superiority of our approach in both coarse-grained and fine-grained 4D
downstream tasks. Notably, Uni4D improves action segmentation accuracy on HOI4D
by $+3.8\%$.

</details>


### [372] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/pdf/2504.14202)
*Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu*

Main category: cs.CV

TL;DR: A novel framework, FaceCLIP, integrates identity and text into a unified conditioning input for ID-preserving image generation, outperforming prior methods in identity preservation and text alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods inject identity features via adapters into pre-trained models, which may not fully preserve identity or align well with text. The goal is to unify identity and text conditioning for better results.

Method: Introduces FaceCLIP, a multi-modal encoder that learns a joint embedding space for identity and text. This representation conditions a diffusion model (FaceCLIP-SDXL) to generate identity-consistent and text-aligned images. A multi-modal alignment algorithm trains FaceCLIP using a loss aligning joint representations with face, text, and image embeddings.

Result: FaceCLIP-SDXL produces photorealistic portraits with superior identity preservation and textual relevance compared to prior methods, as shown in extensive experiments.

Conclusion: The proposed framework effectively unifies identity and text conditioning, achieving state-of-the-art performance in ID-preserving image generation.

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [373] [Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models](https://arxiv.org/pdf/2504.15723)
*Dasol Jeong, Donggoo Kang, Jiwon Park, Hyebean Lee, Joonki Paik*

Main category: cs.CV

TL;DR: A diffusion-based framework for zero-shot image editing unifies text and reference guidance without fine-tuning, using inversion and null-text embeddings for structural integrity and precise modifications.


<details>
  <summary>Details</summary>
Motivation: To enable precise, fine-grained image editing without requiring fine-tuning, unifying text-guided and reference-guided approaches.

Method: Uses diffusion inversion and timestep-specific null-text embeddings, with stage-wise latent injection (shape early, attribute later) and cross-attention for semantic alignment.

Result: Achieves state-of-the-art performance in expression transfer, texture transformation, and style infusion, demonstrating scalability and adaptability.

Conclusion: The framework is effective for diverse image editing tasks, offering precise modifications while maintaining global consistency.

Abstract: We propose a diffusion-based framework for zero-shot image editing that
unifies text-guided and reference-guided approaches without requiring
fine-tuning. Our method leverages diffusion inversion and timestep-specific
null-text embeddings to preserve the structural integrity of the source image.
By introducing a stage-wise latent injection strategy-shape injection in early
steps and attribute injection in later steps-we enable precise, fine-grained
modifications while maintaining global consistency. Cross-attention with
reference latents facilitates semantic alignment between the source and
reference. Extensive experiments across expression transfer, texture
transformation, and style infusion demonstrate state-of-the-art performance,
confirming the method's scalability and adaptability to diverse image editing
scenarios.

</details>


### [374] [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/pdf/2504.17821)
*Xinyu Chen, Yunxin Li, Haoyuan Shi, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang*

Main category: cs.CV

TL;DR: VideoVista-CulturalLingo is a culturally and linguistically diverse video evaluation benchmark assessing AI systems' comprehension, revealing gaps in performance across cultures, languages, and domains.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks lack cultural and linguistic diversity, limiting the assessment of AI systems' global comprehension abilities.

Method: The benchmark includes 1,389 videos and 3,134 QA pairs spanning Chinese, North American, and European cultures, with questions in Chinese and English. It evaluates 24 video models.

Result: Models perform worse on Chinese-centric questions, struggle with temporal understanding, and show varied performance in scientific and mathematical domains.

Conclusion: The benchmark highlights the need for culturally and linguistically inclusive AI evaluation tools to improve global comprehension capabilities.

Abstract: Assessing the video comprehension capabilities of multimodal AI systems can
effectively measure their understanding and reasoning abilities. Most video
evaluation benchmarks are limited to a single language, typically English, and
predominantly feature videos rooted in Western cultural contexts. In this
paper, we present VideoVista-CulturalLingo, the first video evaluation
benchmark designed to bridge cultural, linguistic, and domain divide in video
comprehension. Our work differs from existing benchmarks in the following ways:
1) Cultural diversity, incorporating cultures from China, North America, and
Europe; 2) Multi-linguistics, with questions presented in Chinese and
English-two of the most widely spoken languages; and 3) Broad domain, featuring
videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo
contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent
open-source or proprietary video large models. From the experiment results, we
observe that: 1) Existing models perform worse on Chinese-centric questions
than Western-centric ones, particularly those related to Chinese history; 2)
Current open-source models still exhibit limitations in temporal understanding,
especially in the Event Localization task, achieving a maximum score of only
45.2%; 3) Mainstream models demonstrate strong performance in general
scientific questions, while open-source models demonstrate weak performance in
mathematics.

</details>


### [375] [Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning](https://arxiv.org/pdf/2504.21561)
*Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li*

Main category: cs.CV

TL;DR: SPORT is a method for training multimodal agents without human-annotated data, using iterative tool exploration and preference optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive human annotations for complex multimodal tasks, which are impractical. SPORT aims to eliminate this bottleneck.

Method: SPORT involves task synthesis, step sampling, step verification, and preference tuning to autonomously refine tool usage strategies.

Result: SPORT improves performance by 6.41% and 3.64% on GTA and GAIA benchmarks.

Conclusion: SPORT effectively trains multimodal agents without human annotations, demonstrating generalization and effectiveness.

Abstract: Multimodal agents, which integrate a controller e.g., a vision language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex multimodal tasks. Existing approaches for training these
agents, both supervised fine-tuning and reinforcement learning, depend on
extensive human-annotated task-answer pairs and tool trajectories. However, for
complex multimodal tasks, such annotations are prohibitively expensive or
impractical to obtain. In this paper, we propose an iterative tool usage
exploration method for multimodal agents without any pre-collected data, namely
SPORT, via step-wise preference optimization to refine the trajectories of tool
usage. Our method enables multimodal agents to autonomously discover effective
tool usage strategies through self-exploration and optimization, eliminating
the bottleneck of human annotation. SPORT has four iterative components: task
synthesis, step sampling, step verification, and preference tuning. We first
synthesize multimodal tasks using language models. Then, we introduce a novel
trajectory exploration scheme, where step sampling and step verification are
executed alternately to solve synthesized tasks. In step sampling, the agent
tries different tools and obtains corresponding results. In step verification,
we employ a verifier to provide AI feedback to construct step-wise preference
data. The data is subsequently used to update the controller for tool usage
through preference tuning, producing a SPORT agent. By interacting with real
environments, the SPORT agent gradually evolves into a more refined and capable
system. Evaluation in the GTA and GAIA benchmarks shows that the SPORT agent
achieves 6.41% and 3.64% improvements, underscoring the generalization and
effectiveness introduced by our method. The project page is
https://SPORT-Agents.github.io.

</details>


### [376] [Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction](https://arxiv.org/pdf/2505.02043)
*Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang*

Main category: cs.CV

TL;DR: A method (Point2Primitive) for recovering editable CAD models from point clouds by predicting extrusion primitives directly, using an improved transformer for sketch curve detection and autoregressive optimization for high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve CAD model reconstruction from point clouds by directly predicting extrusion primitives, avoiding limitations of implicit fields for sketch representation.

Method: Uses a transformer-based network to detect sketch curves and predict their parameters autoregressively. Extrusion segmentation rebuilds topology, combining predicted curves and computed extrusion operations.

Result: Superior accuracy in primitive prediction and CAD reconstruction, with high geometrical fidelity in reconstructed shapes.

Conclusion: Point2Primitive effectively recovers editable CAD models from point clouds with high accuracy and fidelity.

Abstract: Recovering CAD models from point clouds, especially the sketch-extrusion
process, can be seen as the process of rebuilding the topology and extrusion
primitives. Previous methods utilize implicit fields for sketch representation,
leading to shape reconstruction of curved edges. In this paper, we proposed a
CAD reconstruction network that produces editable CAD models from input point
clouds (Point2Primitive) by directly predicting every element of the extrusion
primitives. Point2Primitive can directly detect and predict sketch curves (type
and parameter) from point clouds based on an improved transformer. The sketch
curve parameters are formulated as position queries and optimized in an
autoregressive way, leading to high parameter accuracy. The topology is rebuilt
by extrusion segmentation, and each extrusion parameter (sketch and extrusion
operation) is recovered by combining the predicted curves and the computed
extrusion operation. Extensive experiments demonstrate that our method is
superior in primitive prediction accuracy and CAD reconstruction. The
reconstructed shapes are of high geometrical fidelity.

</details>


### [377] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/pdf/2505.04058)
*Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang*

Main category: cs.CV

TL;DR: A novel 2D-assisted 3D visual grounding framework improves object discrimination and relationship perception in 3D scenes by leveraging 2D pre-trained attributes and graph attention.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in bridging the gap between 3D and language modalities, especially in distinguishing similar objects via spatial relationships. Current methods overlook referred object perception.

Method: The framework uses a dual-branch visual encoder with 2D pre-trained attributes and a cross-modal interaction module with graph attention for relationship-oriented fusion.

Result: Outperforms state-of-the-art methods on benchmarks, particularly in handling multiple similar distractors.

Conclusion: The approach enhances 3D-language alignment through improved object representation and relational learning.

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>


### [378] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/pdf/2505.04612)
*Jiahao Li, Haochen Wang, Muhammad Zubair Irshad, Igor Vasiljevic, Matthew R. Walter, Vitor Campagnolo Guizilini, Greg Shakhnarovich*

Main category: cs.CV

TL;DR: FastMap is a fast and simple global structure from motion (SfM) method that outperforms COLMAP and GLOMAP in speed while maintaining comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like COLMAP and GLOMAP struggle with scalability due to poor parallelization and expensive optimization steps when handling large keypoint pairs.

Method: FastMap uses a GPU-friendly framework with linear-time optimization steps, independent of keypoint pairs or 3D points.

Result: FastMap is significantly faster than COLMAP and GLOMAP on large-scale scenes with similar pose accuracy.

Conclusion: FastMap offers a scalable and efficient solution for global SfM, addressing the limitations of current methods.

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is faster than COLMAP and GLOMAP on
large-scale scenes with comparable pose accuracy.

</details>


### [379] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/pdf/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: A multimodal deep learning framework improves caloric intake estimation for Type 2 diabetes management by combining CGM data, demographic/microbiome info, and meal images, outperforming baselines by 50%.


<details>
  <summary>Details</summary>
Motivation: Accurate caloric intake estimation is crucial for Type 2 diabetes management, but current methods like CGMs lack full nutritional profiling due to variability.

Method: The model uses attention-based encoding for meal images, MLPs for CGM and microbiome data, and late fusion for joint reasoning.

Result: Achieves RMSRE of 0.2544, outperforming baselines by over 50%.

Conclusion: Multimodal sensing enhances dietary assessment tools for chronic disease management.

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [380] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/pdf/2505.10238)
*Yanbo Ding, Xirui Hu, Zhizhi Guo, Yali Wang*

Main category: cs.CV

TL;DR: MTVCrafter introduces a novel framework for human image animation by directly modeling 3D motion sequences, outperforming existing methods with a 65% improvement in FID-VID.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D-rendered pose images, limiting generalization and discarding 3D information. MTVCrafter addresses this by leveraging raw 3D motion sequences for more robust and flexible animation.

Method: The framework uses 4DMoT to quantize 3D motion into tokens and MV-DiT with motion attention for animation, enabling disentangled control and better performance.

Result: MTVCrafter achieves state-of-the-art results (FID-VID 6.98) and generalizes well to diverse characters and scenarios.

Conclusion: MTVCrafter advances human image animation by utilizing 3D motion tokens, setting a new direction for pose-guided video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are on: https://github.com/DINGYANB/MTVCrafter.

</details>


### [381] [Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models](https://arxiv.org/pdf/2505.10634)
*Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng*

Main category: cs.CV

TL;DR: The paper proposes Cross-Images Contrastive Decoding (CICD) to reduce hallucinations in LVLMs by using different images for negative contexts, preserving essential language priors while suppressing detrimental ones.


<details>
  <summary>Details</summary>
Motivation: Language priors in LVLMs cause hallucinations (linguistically plausible but visually inconsistent text). Existing contrastive decoding methods lose visual information.

Method: CICD constructs negative contexts from different images, distinguishing essential (fluency-supporting) and detrimental (hallucination-causing) priors.

Result: CICD reduces hallucinations while maintaining fluent language, validated on 4 benchmarks and 6 LVLMs.

Conclusion: CICD is effective and generalizable, especially for image captioning, and will be released upon acceptance.

Abstract: Language priors are a major cause of hallucinations in Large Vision-Language
Models (LVLMs), often leading to text that is linguistically plausible but
visually inconsistent. Recent work explores contrastive decoding as a
training-free solution, but these methods typically construct negative contexts
from the original image, resulting in visual information loss and distorted
distribution. Motivated by the observation that language priors stem from the
LLM backbone and remain consistent across images, we propose Cross-Images
Contrastive Decoding (CICD), a simple yet effective training-free method that
uses different images to construct negative contexts. We further analyze the
cross-image behavior of language priors and introduce a distinction between
essential priors (supporting fluency) and detrimental priors (causing
hallucinations). By selectively preserving essential priors and suppressing
detrimental ones, our method reduces hallucinations while maintaining coherent
and fluent language generation. Experiments on 4 benchmarks and 6 LVLMs across
three model families confirm the effectiveness and generalizability of CICD,
especially in image captioning, where language priors are particularly
pronounced. Code will be released once accepted.

</details>


### [382] [FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining](https://arxiv.org/pdf/2505.11192)
*Myunsoo Kim, Seong-Woong Shim, Byung-Jun Lee*

Main category: cs.CV

TL;DR: FALCON is a learning-based mini-batch strategy for VLP that dynamically balances hard and false negatives, improving performance across frameworks and tasks.


<details>
  <summary>Details</summary>
Motivation: False negatives in VLP degrade embedding quality and hard negative sampling effectiveness, necessitating a solution.

Method: FALCON uses a negative mining scheduler to adaptively select negative samples during mini-batch construction, guided by cross-modal alignment improvement.

Result: FALCON enhances performance in ALBEF and BLIP-2 frameworks and various downstream tasks, proving its robustness.

Conclusion: FALCON effectively mitigates false negatives' impact, improving VLP model performance and adaptability.

Abstract: False negatives pose a critical challenge in vision-language pretraining
(VLP) due to the many-to-many correspondence between images and texts in
large-scale datasets. These false negatives introduce conflicting supervision
signals that degrade the learned embedding space and diminish the effectiveness
of hard negative sampling. In this paper, we propose FALCON (False-negative
Aware Learning of COntrastive Negatives), a learning-based mini-batch
construction strategy that adaptively balances the trade-off between hard and
false negatives during VLP. Rather than relying on fixed heuristics, FALCON
employs a negative mining scheduler that dynamically selects negative samples
of appropriate hardness for each anchor instance during mini-batch
construction, guided by a proxy for cross-modal alignment improvement.
Experimental results demonstrate that FALCON significantly improves performance
across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of
downstream tasks and evaluation settings, underscoring its effectiveness and
robustness in mitigating the impact of false negatives.

</details>


### [383] [Online Iterative Self-Alignment for Radiology Report Generation](https://arxiv.org/pdf/2505.11983)
*Ting Xiao, Lei Shi, Yang Zhang, HaoFeng Yang, Zhe Wang, Chenjia Bai*

Main category: cs.CV

TL;DR: The paper proposes an Online Iterative Self-Alignment (OISA) method for Radiology Report Generation (RRG) to improve model performance through iterative multi-objective optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of supervised fine-tuning and reinforcement learning in RRG, such as overfitting and generalization issues due to limited high-quality annotated data.

Method: The OISA method involves four stages: self-generation of diverse data, self-evaluation for multi-objective preference data, self-alignment for multi-objective optimization, and self-iteration for continuous improvement.

Result: The method achieves state-of-the-art performance across multiple evaluation metrics, demonstrating superior data quality and model optimization.

Conclusion: The OISA framework effectively enhances RRG by iteratively improving data quality and model performance, surpassing previous approaches.

Abstract: Radiology Report Generation (RRG) is an important research topic for
relieving radiologist' heavy workload. Existing RRG models mainly rely on
supervised fine-tuning (SFT) based on different model architectures using data
pairs of radiological images and corresponding radiologist-annotated reports.
Recent research has shifted focus to post-training improvements, aligning RRG
model outputs with human preferences using reinforcement learning (RL).
However, the limited data coverage of high-quality annotated data poses risks
of overfitting and generalization. This paper proposes a novel Online Iterative
Self-Alignment (OISA) method for RRG that consists of four stages:
self-generation of diverse data, self-evaluation for multi-objective preference
data,self-alignment for multi-objective optimization and self-iteration for
further improvement. Our approach allows for generating varied reports tailored
to specific clinical objectives, enhancing the overall performance of the RRG
model iteratively. Unlike existing methods, our frame-work significantly
increases data quality and optimizes performance through iterative
multi-objective optimization. Experimental results demonstrate that our method
surpasses previous approaches, achieving state-of-the-art performance across
multiple evaluation metrics.

</details>


### [384] [Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance](https://arxiv.org/pdf/2505.11997)
*Mingcheng Qu, Guang Yang, Donglin Di, Tonghua Su, Yue Gao, Yang Song, Lei Fan*

Main category: cs.CV

TL;DR: A multimodal survival prediction framework using hypergraph learning and modality rebalance to address pathology-genomics imbalance, outperforming existing methods by 3.4% in C-Index.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect contextual and hierarchical details in pathology images and suffer from modality imbalance between pathology and genomics.

Method: Proposes hypergraph learning for pathology details, a modality rebalance mechanism, and interactive alignment fusion to mitigate imbalance.

Result: Outperforms advanced methods by over 3.4% in C-Index on five TCGA datasets.

Conclusion: The framework effectively addresses modality imbalance and improves survival prediction accuracy.

Abstract: Multimodal pathology-genomic analysis has become increasingly prominent in
cancer survival prediction. However, existing studies mainly utilize
multi-instance learning to aggregate patch-level features, neglecting the
information loss of contextual and hierarchical details within pathology
images. Furthermore, the disparity in data granularity and dimensionality
between pathology and genomics leads to a significant modality imbalance. The
high spatial resolution inherent in pathology data renders it a dominant role
while overshadowing genomics in multimodal integration. In this paper, we
propose a multimodal survival prediction framework that incorporates hypergraph
learning to effectively capture both contextual and hierarchical details from
pathology images. Moreover, it employs a modality rebalance mechanism and an
interactive alignment fusion strategy to dynamically reweight the contributions
of the two modalities, thereby mitigating the pathology-genomics imbalance.
Quantitative and qualitative experiments are conducted on five TCGA datasets,
demonstrating that our model outperforms advanced methods by over 3.4\% in
C-Index performance.

</details>


### [385] [Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition](https://arxiv.org/pdf/2505.12007)
*Runduo Han, Xiuping Liu, Shangxuan Yi, Yi Zhang, Hongchen Tan*

Main category: cs.CV

TL;DR: Proposed MCO-E Net for single-eye expression recognition, using event modalities to tackle challenges like low light and high dynamic range. Introduces MCO-Mamba and HCE-MoE for collaborative optimization and heterogeneous feature extraction.


<details>
  <summary>Details</summary>
Motivation: Address challenges in single-eye expression recognition, such as low light and high dynamic range, by leveraging event modalities and collaborative optimization.

Method: Introduces MCO-Mamba for dual-modal optimization and HCE-MoE for heterogeneous feature extraction using dynamic routing.

Result: Achieves competitive performance, especially under poor lighting conditions.

Conclusion: MCO-E Net effectively integrates multi-modal collaboration and heterogeneous architectures for robust expression recognition.

Abstract: In this paper, we proposed a Multi-modal Collaborative Optimization and
Expansion Network (MCO-E Net), to use event modalities to resist challenges
such as low light, high exposure, and high dynamic range in single-eye
expression recognition tasks. The MCO-E Net introduces two innovative designs:
Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous
Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building
upon Mamba, leverages dual-modal information to jointly optimize the model,
facilitating collaborative interaction and fusion of modal semantics. This
approach encourages the model to balance the learning of both modalities and
harness their respective strengths. HCE-MoE, on the other hand, employs a
dynamic routing mechanism to distribute structurally varied experts (deep,
attention, and focal), fostering collaborative learning of complementary
semantics. This heterogeneous architecture systematically integrates diverse
feature extraction paradigms to comprehensively capture expression semantics.
Extensive experiments demonstrate that our proposed network achieves
competitive performance in the task of single-eye expression recognition,
especially under poor lighting conditions.

</details>


### [386] [CompBench: Benchmarking Complex Instruction-guided Image Editing](https://arxiv.org/pdf/2505.12200)
*Bohan Jia, Wenxuan Huang, Yuntian Tang, Junbo Qiao, Jincheng Liao, Shaosheng Cao, Fei Zhao, Zhaopeng Feng, Zhouhong Gu, Zhenfei Yin, Lei Bai, Wanli Ouyang, Lin Chen, Fei Zhao, Zihan Wang, Yuan Xie, Shaohui Lin*

Main category: cs.CV

TL;DR: CompBench is a benchmark for complex instruction-guided image editing, addressing gaps in existing benchmarks by incorporating fine-grained instructions and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks oversimplify task complexity and lack detailed instructions, limiting evaluation of precise image manipulation capabilities.

Method: An MLLM-human collaborative framework with tailored pipelines and an instruction decoupling strategy (location, appearance, dynamics, objects) is proposed.

Result: CompBench reveals limitations of current image editing models and offers insights for future systems.

Conclusion: CompBench advances instruction-guided image editing by providing a comprehensive benchmark and framework for complex tasks.

Abstract: While real-world applications increasingly demand intricate scene
manipulation, existing instruction-guided image editing benchmarks often
oversimplify task complexity and lack comprehensive, fine-grained instructions.
To bridge this gap, we introduce, a large-scale benchmark specifically designed
for complex instruction-guided image editing. CompBench features challenging
editing scenarios that incorporate fine-grained instruction following, spatial
and contextual reasoning, thereby enabling comprehensive evaluation of image
editing models' precise manipulation capabilities. To construct CompBench, We
propose an MLLM-human collaborative framework with tailored task pipelines.
Furthermore, we propose an instruction decoupling strategy that disentangles
editing intents into four key dimensions: location, appearance, dynamics, and
objects, ensuring closer alignment between instructions and complex editing
requirements. Extensive evaluations reveal that CompBench exposes fundamental
limitations of current image editing models and provides critical insights for
the development of next-generation instruction-guided image editing systems.
The dataset, code, and models are available in https://comp-bench.github.io/.

</details>


### [387] [DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model](https://arxiv.org/pdf/2505.12427)
*Siwei Xia, Li Sun, Tiantian Sun, Qingli Li*

Main category: cs.CV

TL;DR: DragLoRA integrates LoRA adapters into drag-based editing, improving precision and efficiency with denoising score distillation and adaptive optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional drag-based editing methods lack accuracy and efficiency due to low feature representation and large search spaces.

Method: DragLoRA uses LoRA adapters with denoising score distillation and adaptive optimization for stable, accurate feature adaptation.

Result: DragLoRA enhances control precision and computational efficiency in drag-based image editing.

Conclusion: DragLoRA offers a superior solution for precise and efficient drag-based editing, validated by extensive experiments.

Abstract: Drag-based editing within pretrained diffusion model provides a precise and
flexible way to manipulate foreground objects. Traditional methods optimize the
input feature obtained from DDIM inversion directly, adjusting them iteratively
to guide handle points towards target locations. However, these approaches
often suffer from limited accuracy due to the low representation ability of the
feature in motion supervision, as well as inefficiencies caused by the large
search space required for point tracking. To address these limitations, we
present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)
adapters into the drag-based editing pipeline. To enhance the training of LoRA
adapters, we introduce an additional denoising score distillation loss which
regularizes the online model by aligning its output with that of the original
model. Additionally, we improve the consistency of motion supervision by
adapting the input features using the updated LoRA, giving a more stable and
accurate input feature for subsequent operations. Building on this, we design
an adaptive optimization scheme that dynamically toggles between two modes,
prioritizing efficiency without compromising precision. Extensive experiments
demonstrate that DragLoRA significantly enhances the control precision and
computational efficiency for drag-based image editing. The Codes of DragLoRA
are available at: https://github.com/Sylvie-X/DragLoRA.

</details>


### [388] [Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification](https://arxiv.org/pdf/2505.12482)
*Wenchen Chen, Yanmei Zhang, Zhongwei Xiao, Jianping Chu, Xingbo Wang*

Main category: cs.CV

TL;DR: Proposes S4L-FSC, a method combining SSL and FSL to improve few-shot HSI classification by leveraging spectral-spatial pretraining and diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of scarce labeled samples and lack of spectral-spatial adaptability in existing methods for HSI classification.

Method: Uses RM-SSL for spatial feature extraction and MR-SSL for spectral feature extraction, integrating FSL for transferable knowledge.

Result: Demonstrates effectiveness and superiority on four HSI datasets.

Conclusion: S4L-FSC enhances few-shot HSI classification by combining spectral-spatial pretraining and diverse knowledge sources.

Abstract: Few-shot classification of hyperspectral images (HSI) faces the challenge of
scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning
(FSL) offer promising avenues to address this issue. However, existing methods
often struggle to adapt to the spatial geometric diversity of HSIs and lack
sufficient spectral prior knowledge. To tackle these challenges, we propose a
method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral
Image Classification (S4L-FSC), aimed at improving the performance of few-shot
HSI classification. Specifically, we first leverage heterogeneous datasets to
pretrain a spatial feature extractor using a designed Rotation-Mirror
Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach
enables the model to learn the spatial geometric diversity of HSIs using
rotation and mirroring labels as supervisory signals, while acquiring
transferable spatial meta-knowledge through few-shot learning. Subsequently,
homogeneous datasets are utilized to pretrain a spectral feature extractor via
a combination of FSL and Masked Reconstruction Self-Supervised Learning
(MR-SSL). The model learns to reconstruct original spectral information from
randomly masked spectral vectors, inferring spectral dependencies. In parallel,
FSL guides the model to extract pixel-level discriminative features, thereby
embedding rich spectral priors into the model. This spectral-spatial
pretraining method, along with the integration of knowledge from heterogeneous
and homogeneous sources, significantly enhances model performance. Extensive
experiments on four HSI datasets demonstrate the effectiveness and superiority
of the proposed S4L-FSC approach for few-shot HSI classification.

</details>


### [389] [Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining](https://arxiv.org/pdf/2505.12711)
*Qichen Sun, Zhengrui Guo, Rui Peng, Hao Chen, Jinzhuo Wang*

Main category: cs.CV

TL;DR: ALTER is a tri-modal pretraining framework for computational pathology, integrating WSIs, genomics, and pathology reports to address challenges like data fusion, missing modalities, and diverse downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in computational pathology, such as fusing heterogeneous data, handling missing modalities, and addressing diverse downstream tasks.

Method: Proposes ALTER, an any-to-any tri-modal pretraining framework that integrates WSIs, genomics, and pathology reports, enabling flexible pretraining with any subset of modalities.

Result: ALTER achieves superior or comparable performance to state-of-the-art baselines in tasks like survival prediction, cancer subtyping, gene mutation prediction, and report generation.

Conclusion: ALTER provides a robust, modality-adaptive solution for computational pathology, outperforming existing methods in diverse clinical tasks.

Abstract: Recent advances in computational pathology and artificial intelligence have
significantly enhanced the utilization of gigapixel whole-slide images and and
additional modalities (e.g., genomics) for pathological diagnosis. Although
deep learning has demonstrated strong potential in pathology, several key
challenges persist: (1) fusing heterogeneous data types requires sophisticated
strategies beyond simple concatenation due to high computational costs; (2)
common scenarios of missing modalities necessitate flexible strategies that
allow the model to learn robustly in the absence of certain modalities; (3) the
downstream tasks in CPath are diverse, ranging from unimodal to multimodal,
cnecessitating a unified model capable of handling all modalities. To address
these challenges, we propose ALTER, an any-to-any tri-modal pretraining
framework that integrates WSIs, genomics, and pathology reports. The term "any"
emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with
any subset of modalities, and its capacity to learn robust, cross-modal
representations beyond WSI-centric approaches. We evaluate ALTER across
extensive clinical tasks including survival prediction, cancer subtyping, gene
mutation prediction, and report generation, achieving superior or comparable
performance to state-of-the-art baselines.

</details>


### [390] [Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection](https://arxiv.org/pdf/2505.12772)
*Junyi Hu, Tian Bai, Fengyi Wu, Zhenming Peng, Yi Zhang*

Main category: cs.CV

TL;DR: The paper introduces the Pyramid Sparse Transformer (PST), a lightweight module for feature fusion, reducing computational complexity while maintaining accuracy in vision models.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational complexity and implementation challenges of attention-based fusion methods in resource-constrained environments.

Method: PST uses coarse-to-fine token selection and shared attention parameters to reduce computation. It trains with coarse attention and activates at inference for accuracy gains.

Result: PST improves mAP by 0.9%, 0.5%, and 0.4% on YOLOv11 models and boosts ImageNet accuracy by 6.5%, 1.7%, and 1.0% on ResNet backbones.

Conclusion: PST is an effective, hardware-friendly enhancement for vision tasks, balancing efficiency and performance.

Abstract: Feature fusion is critical for high-performance vision models but often
incurs prohibitive complexity. However, prevailing attention-based fusion
methods often involve significant computational complexity and implementation
challenges, limiting their efficiency in resource-constrained environments. To
address these issues, we introduce the Pyramid Sparse Transformer (PST), a
lightweight, plug-and-play module that integrates coarse-to-fine token
selection and shared attention parameters to reduce computation while
preserving spatial detail. PST can be trained using only coarse attention and
seamlessly activated at inference for further accuracy gains without
retraining. When added to state-of-the-art real-time detection models, such as
YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO
with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as
backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,
respectively. These results demonstrate PST's effectiveness as a simple,
hardware-friendly enhancement for both detection and classification tasks.

</details>


### [391] [3D Visual Illusion Depth Estimation](https://arxiv.org/pdf/2505.13061)
*Chengtang Yao, Zhidan Liu, Jiaxi Zeng, Lidong Yu, Yuwei Wu, Yunde Jia*

Main category: cs.CV

TL;DR: The paper investigates how 3D visual illusions fool machine vision systems, particularly in depth estimation, and proposes a robust framework leveraging vision-language models to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the impact of 3D visual illusions on machine vision systems, especially in depth estimation tasks.

Method: Collected a large dataset (3k scenes, 200k images) to evaluate SOTA depth estimation methods. Proposed a framework using vision-language models to adaptively select reliable depth cues.

Result: SOTA depth estimation methods are fooled by 3D illusions, while the proposed framework outperforms them.

Conclusion: The study highlights vulnerabilities in machine vision to illusions and offers a solution for more robust depth estimation.

Abstract: 3D visual illusion is a perceptual phenomenon where a two-dimensional plane
is manipulated to simulate three-dimensional spatial relationships, making a
flat artwork or object look three-dimensional in the human visual system. In
this paper, we reveal that the machine visual system is also seriously fooled
by 3D visual illusions, including monocular and binocular depth estimation. In
order to explore and analyze the impact of 3D visual illusion on depth
estimation, we collect a large dataset containing almost 3k scenes and 200k
images to train and evaluate SOTA monocular and binocular depth estimation
methods. We also propose a robust depth estimation framework that uses common
sense from a vision-language model to adaptively select reliable depth from
binocular disparity and monocular depth. Experiments show that SOTA monocular,
binocular, and multi-view depth estimation approaches are all fooled by various
3D visual illusions, while our method achieves SOTA performance.

</details>


### [392] [Industrial Synthetic Segment Pre-training](https://arxiv.org/pdf/2505.13099)
*Shinichi Mae, Ryousuke Yamada, Hirokatsu Kataoka*

Main category: cs.CV

TL;DR: InsCore, a synthetic dataset for instance segmentation, outperforms real-image datasets like COCO and SAM in industrial applications without legal restrictions or manual annotations.


<details>
  <summary>Details</summary>
Motivation: Address legal/ethical issues and domain gaps in industrial imagery by creating a synthetic, annotation-free pre-training dataset.

Method: Propose InsCore using formula-driven supervised learning (FDSL) to generate synthetic images mimicking industrial data characteristics.

Result: InsCore-trained models outperform COCO, ImageNet-21k, and fine-tuned SAM by 6.2 points on industrial datasets, using only 100k images.

Conclusion: InsCore is a viable, license-free vision foundation model for industrial applications, offering superior performance and data efficiency.

Abstract: Pre-training on real-image datasets has been widely proven effective for
improving instance segmentation. However, industrial applications face two key
challenges: (1) legal and ethical restrictions, such as ImageNet's prohibition
of commercial use, and (2) limited transferability due to the domain gap
between web images and industrial imagery. Even recent vision foundation
models, including the segment anything model (SAM), show notable performance
degradation in industrial settings. These challenges raise critical questions:
Can we build a vision foundation model for industrial applications without
relying on real images or manual annotations? And can such models outperform
even fine-tuned SAM on industrial datasets? To address these questions, we
propose the Instance Core Segmentation Dataset (InsCore), a synthetic
pre-training dataset based on formula-driven supervised learning (FDSL).
InsCore generates fully annotated instance segmentation images that reflect key
characteristics of industrial data, including complex occlusions, dense
hierarchical masks, and diverse non-rigid shapes, distinct from typical web
imagery. Unlike previous methods, InsCore requires neither real images nor
human annotations. Experiments on five industrial datasets show that models
pre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as
well as fine-tuned SAM, achieving an average improvement of 6.2 points in
instance segmentation performance. This result is achieved using only 100k
synthetic images, more than 100 times fewer than the 11 million images in SAM's
SA-1B dataset, demonstrating the data efficiency of our approach. These
findings position InsCore as a practical and license-free vision foundation
model for industrial applications.

</details>


### [393] [Swin DiT: Diffusion Transformer using Pseudo Shifted Windows](https://arxiv.org/pdf/2505.13219)
*Jiafu Wu, Yabiao Wang, Jian Li, Jinlong Peng, Yun Cao, Chengjie Wang, Jiangning Zhang*

Main category: cs.CV

TL;DR: Swin-DiT improves image generation by reducing global computation redundancy and addressing low-frequency inertia in attention mechanisms, outperforming DiT-XL/2 with 54% better FID and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Traditional DiTs rely heavily on global information, which is computationally expensive and often redundant for latent space image generation.

Method: Proposes Pseudo Shifted Window Attention (PSWA) for efficient global-local interaction and Progressive Coverage Channel Allocation (PCCA) for high-order attention similarity. Introduces Swin-DiT models.

Result: Swin-DiT-L achieves a 54% improvement in FID over DiT-XL/2 with reduced computational requirements.

Conclusion: Swin-DiT offers a more efficient and effective approach to image generation by optimizing attention mechanisms and reducing redundancy.

Abstract: Diffusion Transformers (DiTs) achieve remarkable performance within the
domain of image generation through the incorporation of the transformer
architecture. Conventionally, DiTs are constructed by stacking serial isotropic
global information modeling transformers, which face significant computational
cost when processing high-resolution images. We empirically analyze that latent
space image generation does not exhibit a strong dependence on global
information as traditionally assumed. Most of the layers in the model
demonstrate redundancy in global computation. In addition, conventional
attention mechanisms exhibit low-frequency inertia issues. To address these
issues, we propose \textbf{P}seudo \textbf{S}hifted \textbf{W}indow
\textbf{A}ttention (PSWA), which fundamentally mitigates global model
redundancy. PSWA achieves intermediate global-local information interaction
through window attention, while employing a high-frequency bridging branch to
simulate shifted window operations, supplementing appropriate global and
high-frequency information. Furthermore, we propose the Progressive Coverage
Channel Allocation(PCCA) strategy that captures high-order attention similarity
without additional computational cost. Building upon all of them, we propose a
series of Pseudo \textbf{S}hifted \textbf{Win}dow DiTs (\textbf{Swin DiT}),
accompanied by extensive experiments demonstrating their superior performance.
For example, our proposed Swin-DiT-L achieves a 54%$\uparrow$ FID improvement
over DiT-XL/2 while requiring less computational.
https://github.com/wujiafu007/Swin-DiT

</details>


### [394] [Event-Driven Dynamic Scene Depth Completion](https://arxiv.org/pdf/2505.13279)
*Zhiqiang Yan, Jianhao Jiao, Zhengxue Wang, Gim Hee Lee*

Main category: cs.CV

TL;DR: EventDC is a novel event-driven depth completion framework for dynamic scenes, leveraging event cameras for improved alignment and depth estimation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in depth completion due to rapid motion in dynamic scenes, where conventional RGB-D sensors struggle.

Method: Proposes EventDC with Event-Modulated Alignment (EMA) and Local Depth Filtering (LDF) modules, adapting convolution operations based on event streams.

Result: Superior performance demonstrated on a new benchmark of real-world and synthetic datasets.

Conclusion: EventDC effectively enhances depth completion in dynamic environments, setting a foundation for future research.

Abstract: Depth completion in dynamic scenes poses significant challenges due to rapid
ego-motion and object motion, which can severely degrade the quality of input
modalities such as RGB images and LiDAR measurements. Conventional RGB-D
sensors often struggle to align precisely and capture reliable depth under such
conditions. In contrast, event cameras with their high temporal resolution and
sensitivity to motion at the pixel level provide complementary cues that are
%particularly beneficial in dynamic environments.To this end, we propose
EventDC, the first event-driven depth completion framework. It consists of two
key components: Event-Modulated Alignment (EMA) and Local Depth Filtering
(LDF). Both modules adaptively learn the two fundamental components of
convolution operations: offsets and weights conditioned on motion-sensitive
event streams. In the encoder, EMA leverages events to modulate the sampling
positions of RGB-D features to achieve pixel redistribution for improved
alignment and fusion. In the decoder, LDF refines depth estimations around
moving objects by learning motion-aware masks from events. Additionally,
EventDC incorporates two loss terms to further benefit global alignment and
enhance local depth recovery. Moreover, we establish the first benchmark for
event-based depth completion comprising one real-world and two synthetic
datasets to facilitate future research. Extensive experiments on this benchmark
demonstrate the superiority of our EventDC.

</details>


### [395] [Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning](https://arxiv.org/pdf/2505.13327)
*Ajian Liu, Haocheng Yuan, Xiao Guo, Hui Ma, Wanyi Zhuang, Changtao Miao, Yan Hong, Chuanbiao Song, Jun Lan, Qi Chu, Tao Gong, Yanyan Liang, Weiqiang Wang, Jun Wan, Xiaoming Liu, Zhen Lei*

Main category: cs.CV

TL;DR: The paper introduces UniAttackData+, a comprehensive dataset for unified face attack detection, and HiPTune, a hierarchical prompt tuning framework to address diverse attack types.


<details>
  <summary>Details</summary>
Motivation: Current face attack detection models are trained separately for physical and digital attacks, making them vulnerable to unknown threats and complicating deployment. A unified approach is needed.

Method: The authors propose UniAttackData+ (a large-scale dataset) and HiPTune (a hierarchical prompt tuning framework using visual-language models to explore multiple classification criteria).

Result: Experiments on 12 datasets demonstrate the framework's effectiveness in handling diverse attacks.

Conclusion: The proposed solutions address the lack of benchmarks and reliable classification criteria, paving the way for unified face attack detection.

Abstract: Presentation Attack Detection and Face Forgery Detection are designed to
protect face data from physical media-based Presentation Attacks and digital
editing-based DeepFakes respectively. But separate training of these two models
makes them vulnerable to unknown attacks and burdens deployment environments.
The lack of a Unified Face Attack Detection model to handle both types of
attacks is mainly due to two factors. First, there's a lack of adequate
benchmarks for models to explore. Existing UAD datasets have limited attack
types and samples, restricting the model's ability to address advanced threats.
To address this, we propose UniAttackDataPlus (UniAttackData+), the most
extensive and sophisticated collection of forgery techniques to date. It
includes 2,875 identities and their 54 kinds of falsified samples, totaling
697,347 videos. Second, there's a lack of a reliable classification criterion.
Current methods try to find an arbitrary criterion within the same semantic
space, which fails when encountering diverse attacks. So, we present a novel
Visual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that
adaptively explores multiple classification criteria from different semantic
spaces. We build a Visual Prompt Tree to explore various classification rules
hierarchically. Then, by adaptively pruning the prompts, the model can select
the most suitable prompts to guide the encoder to extract discriminative
features at different levels in a coarse-to-fine way. Finally, to help the
model understand the classification criteria in visual space, we propose a
Dynamically Prompt Integration module to project the visual prompts to the text
encoder for more accurate semantics. Experiments on 12 datasets have shown the
potential to inspire further innovations in the UAD field.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [396] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/pdf/2505.13466)
*Vu Dinh Xuan, Hao Vo, David Murphy, Hoang D. Nguyen*

Main category: cs.AI

TL;DR: A novel multi-agent framework generates synthetic data for safety-critical AI training by combining an Evaluator Agent (LLM-based judge) and an Editor Agent, addressing data scarcity and semantic depth issues.


<details>
  <summary>Details</summary>
Motivation: Data scarcity for dangerous situations hinders AI training in safety-critical applications like construction safety, necessitating synthetic data generation.

Method: Proposes a multi-agent framework with an Evaluator Agent (ensures semantic consistency and safety constraints) and an Editor Agent (generates and refines scenes).

Result: The framework produces synthetic images tailored to safety-critical scenarios, balancing safety and visual semantics.

Conclusion: The iterative, collaborative design offers a robust solution to data scarcity in safety applications, outperforming prior methods.

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [397] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/pdf/2505.13484)
*Rene Heesch, Sebastian Eilermann, Alexander Windmann, Alexander Diedrich, Philipp Rosenthal, Oliver Niggemann*

Main category: cs.AI

TL;DR: The paper critiques current LLM evaluations in engineering for oversimplification and introduces a curated dataset of 100+ real-world engineering questions to assess LLM performance systematically. Results reveal strengths in basic reasoning but weaknesses in abstract and context-sensitive tasks.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in engineering rely on simplified or ad hoc scenarios, failing to capture real-world complexity. This gap limits understanding of LLM capabilities in practical engineering contexts.

Method: The authors created a dataset of 100+ questions from real engineering scenarios, covering core competencies like design and diagnosis. They evaluated four state-of-the-art LLMs using this dataset.

Result: LLMs performed well in basic temporal and structural reasoning but struggled with abstract reasoning, formal modeling, and context-sensitive logic.

Conclusion: The study highlights the need for more robust evaluation frameworks to assess LLMs in complex engineering tasks, revealing current limitations in abstract and context-sensitive reasoning.

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [398] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/pdf/2505.13489)
*Wenkang Han, Wang Lin, Liya Hu, Zhenlong Dai, Yiyun Zhou, Mengze Li, Zemin Liu, Chang Yao, Jingyuan Chen*

Main category: cs.AI

TL;DR: TransKT is a cross-course knowledge tracing method using concept graphs and LLM prompts to enhance knowledge state estimation by integrating learning behaviors across courses.


<details>
  <summary>Details</summary>
Motivation: Existing KT models focus on single-course data, limiting comprehensive understanding of learners' knowledge states. TransKT addresses this by leveraging cross-course relationships.

Method: TransKT constructs a cross-course concept graph using LLM prompts, integrates semantic features via an LLM-to-LM pipeline, and employs a contrastive objective to align knowledge states.

Result: The method improves knowledge state estimation by enhancing semantic features and aligning single-course and cross-course knowledge states.

Conclusion: TransKT provides a robust and accurate representation of learners' knowledge states by leveraging cross-course relationships and contrastive learning.

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [399] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/pdf/2505.13496)
*Przemek Pospieszny, Wojciech Mormul, Karolina Szyndler, Sanjeev Kumar*

Main category: cs.AI

TL;DR: ADALog is an adaptive, unsupervised anomaly detection framework for heterogeneous log data, leveraging transformer-based models and adaptive thresholding for practical applicability.


<details>
  <summary>Details</summary>
Motivation: Modern software systems generate complex log data with dynamic formats and fragmented sequences, making anomaly detection challenging. Traditional methods rely on parsing, sequence dependencies, or labeled data, which are inflexible.

Method: ADALog uses a pretrained bidirectional transformer encoder fine-tuned on normal logs. It extracts intra-log contextual relationships, performs token-level anomaly detection via reconstruction probabilities, and applies adaptive percentile-based thresholding.

Result: ADALog shows strong generalization and competitive performance on benchmark datasets (BGL, Thunderbird, Spirit) compared to state-of-the-art methods. Ablation studies analyze masking, fine-tuning, and token positioning effects.

Conclusion: ADALog effectively addresses the challenges of dynamic log data with its adaptive, unsupervised approach, outperforming traditional methods and offering practical applicability.

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [400] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/pdf/2505.13511)
*David Noever, Forrest McKee*

Main category: cs.AI

TL;DR: The study evaluates LLMs as autonomous agents for freelance tasks, introducing a scalable benchmark with synthetic tasks. Claude 3.5 Haiku outperforms others, earning $1.52M.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in real-world freelance tasks, particularly software development and data analysis, using a standardized, scalable benchmark.

Method: A benchmark was created using synthetic tasks from Kaggle Freelancer data, with standardized prices and test cases. Four LLMs were evaluated on accuracy and earnings.

Result: Claude 3.5 Haiku performed best ($1.52M), followed by GPT-4o-mini ($1.49M), Qwen 2.5 ($1.33M), and Mistral ($0.70M). Strong models rarely failed completely.

Conclusion: LLMs show promise for freelance tasks, but challenges remain in bridging the gap between structured benchmarks and real-world job complexity.

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [401] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/pdf/2505.13522)
*Nathalie Sanghikian, Rafael Meirelles, Rafael Martinelli, Anand Subramanian*

Main category: cs.AI

TL;DR: The paper addresses the Maritime Inventory Routing Problem (MIRP) by proposing a heuristic combining Beam Search and Iterated Local Search, improving solutions for 10 out of 72 instances.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with large MIRP instances due to complexity. Exact methods are impractical for daily operations, and non-MIP heuristics are rare due to constraints. The study aims to encourage MIRPLib use and improve solutions.

Method: A heuristic approach combining Beam Search variation and Iterated Local Search, avoiding mathematical optimization.

Result: Improved best-known solutions for 10 out of 72 tested instances within acceptable CPU time.

Conclusion: The heuristic is effective for deterministic, finite-horizon, single-product MIRP, encouraging further use of MIRPLib.

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [402] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/pdf/2505.13529)
*Junxiao Yang, Jinzhe Tu, Haoran Liu, Xiaoce Wang, Chujie Zheng, Zhexin Zhang, Shiyao Cui, Caishun Chen, Tiantian He, Hongning Wang, Yew-Soon Ong, Minlie Huang*

Main category: cs.AI

TL;DR: BARREL framework improves LRM reliability by reducing overconfident incorrect answers, boosting accuracy from 39.33% to 61.48%.


<details>
  <summary>Details</summary>
Motivation: Current LRMs often produce incorrect answers with undue confidence, lacking the ability to admit ignorance, which undermines their factual reliability.

Method: Proposes BARREL, a framework to address pathological reasoning patterns (last-minute guessing and second-thought spiraling) by promoting concise and boundary-aware reasoning.

Result: BARREL-training increased reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, maintaining comparable accuracy to models finetuned on R1-generated data.

Conclusion: BARREL is a promising approach to enhance the reliability and factual correctness of System 2 LRMs.

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [403] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/pdf/2505.13533)
*Junzhe Jiang, Chang Yang, Aixin Cui, Sihan Jin, Ruiyu Wang, Bo Li, Xiao Huang, Dongning Sun, Xinrun Wang*

Main category: cs.AI

TL;DR: FinMaster is a financial benchmark for LLMs, addressing gaps in domain-specific data, task design, and evaluation frameworks. It includes FinSim, FinSuite, and FinEval modules, revealing LLMs' limitations in complex financial reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs in finance lack domain-specific data, simplistic tasks, and incomplete evaluation frameworks, hindering their practical adoption.

Method: FinMaster comprises FinSim (synthetic data generation), FinSuite (183 financial tasks), and FinEval (unified evaluation).

Result: LLMs show accuracy drops from 90% (basic tasks) to 40% (complex reasoning), highlighting computational error propagation.

Conclusion: FinMaster bridges research-industry gaps, promoting LLM adoption in finance for improved efficiency and accuracy.

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [404] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/pdf/2505.13546)
*Ke Chen, Yufei Zhou, Xitong Zhang, Haohan Wang*

Main category: cs.AI

TL;DR: The paper introduces a stability-aware prompt generation system, emphasizing prompt stability (response consistency) as a key factor for reliable multi-agent systems. It proposes semantic stability and a LLaMA-based evaluator to improve prompt quality and task performance.


<details>
  <summary>Details</summary>
Motivation: Existing prompt generation methods focus on immediate task performance, ignoring reliability and interpretability due to LLM stochasticity. This work highlights prompt stability as crucial for robust systems.

Method: Proposes semantic stability for assessing prompt consistency, fine-tunes a LLaMA-based evaluator, and develops a stability-aware prompt generation system with iterative feedback.

Result: Empirical results show improved accuracy and output consistency across tasks, proving stability as necessary for effective system-level execution.

Conclusion: Shifting focus to persistent reliability offers a new perspective on prompt design, providing tools for more trustworthy general-purpose systems.

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [405] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/pdf/2505.13551)
*Serge Dolgikh*

Main category: cs.AI

TL;DR: The paper examines counter-inferential behavior in cognitive systems, where agents misattribute success or resist adaptation, leading to rigidity. It identifies scenarios like reward imbalance and meta-cognitive biases, showing these arise from structured interactions, not flaws. Findings suggest design principles to avoid rigidity.


<details>
  <summary>Details</summary>
Motivation: To understand how cognitive systems (natural and artificial) develop maladaptive stability or rigidity due to counter-inferential behaviors, despite being well-adapted.

Method: Analysis of archetypal scenarios (reward imbalance, meta-cognitive biases, protective reframing) using evidence from artificial systems, biology, psychology, and social dynamics.

Result: Counter-inferential behavior is a general cognitive vulnerability, emerging from interactions between internal models, feedback, and evaluation mechanisms.

Conclusion: Preserving adaptive activation and designing resilient cognitive architectures can mitigate rigidity under informational stress.

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [406] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/pdf/2505.13773)
*Ryan Bowers, Richard Agbeyibor, Jack Kolb, Karen Feigh*

Main category: cs.AI

TL;DR: The study compares three methods of familiarizing humans with an AI teammate in an ISR environment, finding that documentation-based familiarization speeds strategy adoption but biases risk-averse behavior, while interactive familiarization encourages risk-taking but lacks deep understanding. A combined approach is recommended.


<details>
  <summary>Details</summary>
Motivation: To identify the most effective way to familiarize humans with AI teammates in collaborative, fast-paced ISR environments.

Method: A between-subjects user study (n=60) tested three familiarization methods: reading documentation, training alongside the AI, or no familiarization.

Result: Documentation led to faster strategy adoption but risk-averse behavior, while interaction encouraged risk-taking but weaker understanding of AI processes. Individual differences in risk tolerance and interaction methods were significant.

Conclusion: A combined approach of documentation, structured training, and exploratory interaction is recommended for human-AI team familiarization.

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [407] [Language and Thought: The View from LLMs](https://arxiv.org/pdf/2505.13561)
*Daniel Rothschild*

Main category: cs.AI

TL;DR: The paper explores Dennett's idea that language fundamentally changes the nature of mind, using AI (especially LLMs) as evidence. It argues that language's abstractness and efficiency enable tractable inference, supporting Dennett's view.


<details>
  <summary>Details</summary>
Motivation: To test Dennett's thesis about the transformative role of language in cognition by examining AI systems, particularly LLMs.

Method: Analyzes the performance of AI systems with and without linguistic training, focusing on LLMs' inferential reasoning capabilities.

Result: LLMs' success in cross-domain inference supports Dennett's claim that language significantly alters cognition.

Conclusion: Language's abstract encoding enables efficient inference, suggesting a similar role in biological minds.

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [408] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/pdf/2505.13994)
*Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim*

Main category: cs.AI

TL;DR: SPLIT-RAG improves RAG systems by using semantic graph partitioning and multi-agent retrieval to enhance efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency-accuracy trade-offs in scaling RAG systems to large knowledge graphs, particularly for simple queries and complex multi-hop questions.

Method: Proposes SPLIT-RAG, a framework with question-driven semantic graph partitioning, type-specialized knowledge bases, and lightweight LLM agents for collaborative subgraph retrieval.

Result: Demonstrates considerable improvements in efficiency and accuracy compared to existing approaches.

Conclusion: SPLIT-RAG effectively reduces search space and enhances retrieval performance through semantic partitioning and multi-agent collaboration.

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [409] [MAFA: A multi-agent framework for annotation](https://arxiv.org/pdf/2505.13668)
*Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem*

Main category: cs.AI

TL;DR: A multi-agent framework for FAQ annotation outperforms single-agent methods, improving accuracy and handling ambiguous queries effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional single-model approaches fail to capture nuances in diverse user queries, necessitating a more robust solution.

Method: Uses a multi-agent system with specialized agents and a judge agent, employing structured reasoning via ARQs and a few-shot example strategy.

Result: Achieves 14% higher Top-1 accuracy, 18% higher Top-5 accuracy, and 12% better Mean Reciprocal Rank on a banking dataset, with similar gains on public benchmarks.

Conclusion: The framework excels in handling ambiguity and generalizes well across domains and languages, making it suitable for production.

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [410] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/pdf/2505.13672)
*Giannis Chatziveroglou*

Main category: cs.AI

TL;DR: A*-decoding, a search-based inference-time strategy, optimizes compute budgets by prioritizing high-quality reasoning paths, outperforming baselines with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To improve language model performance on complex reasoning tasks by optimally utilizing fixed compute budgets during inference.

Method: Introduces A*-decoding, applying A* search to language model decoding, guided by process supervision to prioritize promising continuations.

Result: Achieves performance of larger models with 3x fewer tokens and 30% fewer PRM passes, matching Llama-3.1-70B-Instruct with a smaller model.

Conclusion: Structured search like A*-decoding enhances reasoning in small language models, offering efficient alternatives to brute-force scaling.

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [411] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/pdf/2505.13696)
*Zizhan He, Maxime Daigle, Pouya Bashivan*

Main category: cs.AI

TL;DR: A neural network framework, ESWM, learns to build spatial models from sparse episodic memories, showing high sample efficiency and adaptability for navigation and exploration.


<details>
  <summary>Details</summary>
Motivation: To understand if neural networks can construct spatial models from sparse, disjoint episodic memories, mimicking animal-like adaptability.

Method: Proposed the Episodic Spatial World Model (ESWM) in a simulated world, testing its ability to form robust representations from minimal observations.

Result: ESWM is highly sample-efficient, adaptive to environmental changes, and enables near-optimal exploration and navigation without extra training.

Conclusion: ESWM demonstrates the feasibility of learning spatial models from sparse episodic memories, offering insights into neural network adaptability and efficiency.

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [412] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/pdf/2505.14544)
*Saahil Mahato*

Main category: cs.AI

TL;DR: The paper explores using multi-agent reinforcement learning (MARL) to optimize traffic signal control at intersections, showing improved efficiency over traditional fixed-time systems.


<details>
  <summary>Details</summary>
Motivation: Urban traffic congestion at intersections affects travel time, fuel use, and emissions, and fixed-time systems lack adaptability to dynamic traffic.

Method: A decentralized MARL controller was tested in a Pygame simulation with random vehicle flows, comparing performance to fixed-time control.

Result: MARL reduced average wait times and improved throughput significantly compared to the baseline.

Conclusion: MARL shows promise for urban traffic management, but scalability and real-world implementation need further research.

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [413] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/pdf/2505.13718)
*Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross*

Main category: cs.AI

TL;DR: A two-stage training strategy for reasoning LLMs is proposed, using warmup with K&K puzzles and RLVR, improving performance and efficiency in data-scarce settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of training reasoning-capable LLMs with limited quality data by developing a sample-efficient method.

Method: Two-stage training: warmup with K&K puzzles for general reasoning, followed by RLVR on target-domain examples.

Result: Improved performance across tasks, better sample efficiency, and maintained cross-domain generalizability.

Conclusion: Warmup enhances reasoning LLMs' robustness and efficiency in data-scarce environments.

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [414] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/pdf/2505.13737)
*Andrew Nam, Henry Conklin, Yukang Yang, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie*

Main category: cs.AI

TL;DR: CHG is a scalable method for interpreting attention heads in transformers, assigning causal roles (facilitating, interfering, irrelevant) via soft gating. It works without templates or labels, validated across tasks and models.


<details>
  <summary>Details</summary>
Motivation: To provide causal insights into attention head roles in transformers, avoiding hypothesis-driven or template-based limitations.

Method: CHG learns soft gates over heads, assigns causal taxonomies, and uses next-token prediction. Contrastive CHG isolates sub-circuits.

Result: CHG reveals sparse sub-circuits, low modularity in head roles, and separable mechanisms for instruction following and in-context learning.

Conclusion: CHG offers scalable, causal interpretability for transformers, validated across diverse tasks and models.

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [415] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/pdf/2505.13763)
*Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna*

Main category: cs.AI

TL;DR: LLMs exhibit metacognitive abilities, allowing them to report and control internal activations, but with limitations. A neurofeedback paradigm quantifies these abilities, revealing a low-dimensional 'metacognitive space' and implications for AI safety.


<details>
  <summary>Details</summary>
Motivation: Understanding the limits of LLMs' metacognitive abilities is critical due to their societal reliance and potential safety concerns, such as evading oversight mechanisms.

Method: A neuroscience-inspired neurofeedback paradigm tests LLMs' ability to report and control activation patterns using sentence-label pairs corresponding to neural activations.

Result: LLMs can learn to report and control activations, but performance depends on example quantity, semantic interpretability, and variance explained by neural directions. The metacognitive space is lower-dimensional than the neural space.

Conclusion: LLMs have limited metacognitive capabilities, monitoring only a subset of neural mechanisms. These findings are crucial for AI safety and oversight.

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [416] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/pdf/2505.13770)
*Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding*

Main category: cs.AI

TL;DR: The paper introduces CausalPitfalls, a benchmark to evaluate LLMs' ability to handle statistical causal inference, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous evaluation for LLMs in statistical causal inference, which is crucial for high-stakes decision-making.

Method: Proposes CausalPitfalls, a benchmark with structured challenges and grading rubrics, evaluated via direct and code-assisted prompting.

Result: Current LLMs show significant limitations in statistical causal inference.

Conclusion: CausalPitfalls provides essential metrics to improve trustworthy causal reasoning in LLMs.

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [417] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/pdf/2505.13774)
*Zidi Xiong, Chen Shan, Zhenting Qi, Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: The paper proposes a counterfactual intervention framework to evaluate the faithfulness of intermediate reasoning steps in Large Reasoning Models (LRMs), revealing selective faithfulness and frequent misalignment with draft conclusions.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable monitoring and control of LRMs by rigorously evaluating the faithfulness of their intermediate reasoning processes.

Method: A systematic counterfactual intervention framework assessing Intra-Draft Faithfulness (causal influence of steps) and Draft-to-Answer Faithfulness (logical consistency with draft conclusions).

Result: Experiments on six LRMs show selective faithfulness to reasoning steps and frequent misalignment with draft conclusions.

Conclusion: Highlights the need for more faithful and interpretable reasoning in advanced LRMs.

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [418] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/pdf/2505.13778)
*Guoheng Sun, Ziyao Wang, Bowei Tian, Meng Liu, Zheyu Shen, Shwai He, Yexiao He, Wanghao Ye, Yiting Wang, Ang Li*

Main category: cs.AI

TL;DR: CoIn is a framework to verify hidden reasoning tokens in LLM services, detecting token count inflation and ensuring billing transparency.


<details>
  <summary>Details</summary>
Motivation: The opacity in LLM services allows potential token count inflation, where users are billed for hidden reasoning tokens without verification.

Method: CoIn uses verifiable hash trees for token count checks and embedding-based relevance matching to detect fabricated reasoning.

Result: CoIn achieves a 94.7% success rate in detecting token count inflation.

Conclusion: CoIn effectively restores billing transparency in opaque LLM services.

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [419] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/pdf/2505.13794)
*Qi Cheng, Licheng Liu, Qing Zhu, Runlong Yu, Zhenong Jin, Yiqun Xie, Xiaowei Jia*

Main category: cs.AI

TL;DR: A novel framework combining metric learning and LLM-based natural language policy extraction improves ecological time series evaluation by capturing domain-specific temporal patterns and reducing reliance on expert visual inspection.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical metrics often fail to capture critical ecological temporal patterns, requiring labor-intensive expert inspection, which limits large-scale evaluation.

Method: The framework integrates metric learning with LLM-based natural language policy extraction, using pairwise annotations and policy optimization to generate interpretable evaluation criteria.

Result: Tests on crop gross primary production and CO2 flux datasets confirmed the method's effectiveness in capturing assessment preferences, including synthetic and expert-annotated comparisons.

Conclusion: The framework bridges the gap between numerical metrics and expert knowledge, offering interpretable evaluation policies for diverse ecosystem modeling needs.

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [420] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/pdf/2505.13828)
*Kiarash Naghavi Khanghah, Zhiling Chen, Lela Romeo, Qian Yang, Rajiv Malhotra, Farhad Imani, Hongyi Xu*

Main category: cs.AI

TL;DR: A novel multimodal RAG-based framework automates anomaly detection in additive manufacturing, using retrieved literature data for zero-shot identification, classification, and explanation without training. It outperforms baselines and improves accuracy by 12%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in additive manufacturing like defects and process anomalies by leveraging literature data instead of training datasets.

Method: Integrates text and image retrieval from scientific literature with multimodal generation models for zero-shot anomaly detection in Laser Powder Bed Fusion.

Result: Outperforms Qwen2-VL-2B and random baselines, with retrieval improving accuracy by 12%. Demonstrated adaptability across diverse datasets.

Conclusion: The scalable, automated framework enhances anomaly analysis efficiency and accuracy, adaptable to evolving AM technologies.

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [421] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/pdf/2505.13831)
*Zongyuan Deng, Yujie Cai, Qing Liu, Shiyao Mu, Bin Lyu, Zhen Yang*

Main category: cs.AI

TL;DR: TelePlanNet is an AI-driven framework for 5G base station site selection, improving efficiency and consistency over manual methods by integrating LLMs and GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in traditional manual methods and limitations of existing AI tools for dynamic, multi-objective 5G network planning.

Method: Uses a three-layer architecture with LLMs for real-time user input and GRPO reinforcement learning for multi-objective optimization.

Result: Improves planning-construction consistency to 78%, outperforming manual methods.

Conclusion: TelePlanNet offers telecom operators an efficient, scalable solution for 5G network planning.

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [422] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/pdf/2505.13851)
*Sahil Shah, Harsh Goel, Sai Shankar Narasimhan, Minkyu Choi, S P Sharan, Oguzhan Akcin, Sandeep Chinchali*

Main category: cs.AI

TL;DR: The paper highlights the need for proactive video agents capable of temporal reasoning and action-driven decision-making, proposing a neuro-symbolic approach to enhance interpretability and system behavior.


<details>
  <summary>Details</summary>
Motivation: Current video understanding systems lack temporal reasoning for event sequencing, limiting their ability to make informed decisions in real-world applications.

Method: Proposes a neuro-symbolic approach to decompose video queries into atomic events, structure sequences, and validate temporal constraints.

Result: Introduces a grand challenge for developing intelligent video agents with autonomous search, real-world interaction, and content generation capabilities.

Conclusion: Transitioning to proactive video agents with reasoning and action capabilities can advance trustworthy and intelligent video understanding.

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [423] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/pdf/2505.13887)
*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V uses video to automate mobile tasks, reducing manual effort and improving performance by 36%.


<details>
  <summary>Details</summary>
Motivation: The need for efficient mobile task automation due to rising device usage, and the limitations of current AI frameworks and manual methods.

Method: Mobile-Agent-V leverages video content to inject operational knowledge into automation, eliminating manual intervention.

Result: A 36% performance improvement over existing methods, validated by the Mobile-Knowledge benchmark.

Conclusion: Mobile-Agent-V offers an effortless and efficient solution for mobile automation, outperforming traditional approaches.

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [424] [Efficient Agent Training for Computer Use](https://arxiv.org/pdf/2505.13909)
*Yanheng He, Jiahe Jin, Pengfei Liu*

Main category: cs.AI

TL;DR: PC Agent-E reduces reliance on large-scale human demonstrations by synthesizing high-quality trajectory data, achieving a 141% improvement over Claude 3.7 Sonnet.


<details>
  <summary>Details</summary>
Motivation: Scaling high-quality trajectory data is a bottleneck for human-like computer agents.

Method: Uses 312 human-annotated trajectories, enriched with synthesized actions from Claude 3.7 Sonnet.

Result: 141% relative improvement on WindowsAgentArena-V2; strong generalizability on OSWorld.

Conclusion: Small high-quality trajectory data can stimulate strong computer use capabilities.

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [425] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/pdf/2505.13914)
*Jake Chandler, Richard Booth*

Main category: cs.AI

TL;DR: The paper extends serial iterated belief revision to parallel change using TeamQueue aggregators, unifying plausible rationality postulates while avoiding dubious ones.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding iterated parallel revision and unify existing rationality postulates.

Method: Proposes a method using TeamQueue aggregators to extend serial iterated belief revision to parallel change.

Result: Provides a principled way to recover plausible properties and avoid dubious ones.

Conclusion: The method successfully unifies and extends iterated belief revision for parallel change.

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [426] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/pdf/2505.13940)
*Kun Li, Zhennan Wu, Shoupeng Wang, Wenbin Hu*

Main category: cs.AI

TL;DR: DrugPilot, an LLM-based agent for drug discovery, addresses challenges like multi-modal data processing and dynamic knowledge updates with parametric reasoning and an interactive memory pool, achieving high task completion rates.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs in drug discovery struggle with multi-modal data, dynamic knowledge updates, and result confidence. DrugPilot aims to overcome these limitations.

Method: Proposes DrugPilot with parametric inference architecture, an interactive parameterized memory pool, and a drug instruct dataset for fine-tuning.

Result: DrugPilot achieves task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and multi-turn tasks, outperforming existing agents.

Conclusion: DrugPilot effectively addresses key challenges in drug discovery, demonstrating advanced capabilities and high performance.

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [427] [Visual Instruction Bottleneck Tuning](https://arxiv.org/pdf/2505.13946)
*Changdae Oh, Jiatong Li, Shawn Im, Yixuan Li*

Main category: cs.AI

TL;DR: Vittle, a method inspired by the information bottleneck principle, improves MLLM robustness under distribution shifts by learning minimal sufficient representations, outperforming existing methods without extra data or larger models.


<details>
  <summary>Details</summary>
Motivation: MLLMs degrade in performance under unfamiliar queries due to distribution shifts. Current solutions require costly data or architecture expansions.

Method: Derived a variational lower bound of the information bottleneck for MLLMs, implemented as Visual Instruction Bottleneck Tuning (Vittle).

Result: Vittle consistently enhanced MLLM robustness across 45 datasets and 30 shift scenarios, outperforming baselines.

Conclusion: Vittle offers a cost-effective, representation-focused approach to improve MLLM robustness under distribution shifts.

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [428] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/pdf/2505.14001)
*Sterre Lutz, Matthijs T. J. Spaan, Anna Lukina*

Main category: cs.AI

TL;DR: VeRecycle is a framework that reuses probabilistic safety certificates for stochastic systems with localized changes, saving computational effort while maintaining guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of complete re-certification when system dynamics change in a subset of states, especially for neural certificates.

Method: Introduces VeRecycle, a framework that formally reclaims guarantees by reusing existing certificates for localized changes in state space.

Result: VeRecycle reduces computational effort and achieves competitive probabilistic guarantees in neural control scenarios.

Conclusion: VeRecycle offers an efficient solution for re-certification in stochastic systems with localized changes, enhancing practicality of neural certificates.

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [429] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/pdf/2505.14020)
*Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou*

Main category: cs.AI

TL;DR: DiMNet improves TKG reasoning by modeling subgraph interactions and distinguishing stable vs. active features, achieving up to 22.7% better MRR.


<details>
  <summary>Details</summary>
Motivation: Existing TKG methods ignore subgraph interactions and smooth features, limiting reasoning accuracy.

Method: DiMNet uses a multi-span evolution strategy and disentangle component to capture semantic changes and interactions.

Result: DiMNet outperforms state-of-the-art methods by up to 22.7% in MRR on four datasets.

Conclusion: DiMNet effectively addresses limitations in TKG reasoning by modeling interactions and semantic features.

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [430] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/pdf/2505.14038)
*Xinzhe Zheng, Sijie Ji, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava*

Main category: cs.AI

TL;DR: ProMind-LLM integrates objective behavior data with subjective mental records for reliable mental health risk assessment, outperforming general LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on subjective mental records, which are prone to inconsistencies due to mental uncertainties, necessitating a more robust approach.

Method: ProMind-LLM uses domain-specific pretraining, a self-refine mechanism for behavioral data, and causal chain-of-thought reasoning for reliable predictions.

Result: Evaluations on PMData and Globem datasets show significant improvements over general LLMs.

Conclusion: ProMind-LLM offers a dependable, interpretable, and scalable solution for mental health risk assessment.

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [431] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/pdf/2505.14072)
*Soroush Hashemifar, Sherry Sahebi*

Main category: cs.AI

TL;DR: KMaP is a multi-task model addressing personalization and context loss in student knowledge and behavior modeling by using clustering-based profiling and predicting learning resource preferences.


<details>
  <summary>Details</summary>
Motivation: Challenges in deep learning for education include limited personalization, inadequate modeling of diverse activities, and ignoring the interplay between knowledge and behavior. Fixed-size sequences and reliance on assessed materials further restrict modeling.

Method: Proposes KMaP, a stateful multi-task approach with clustering-based student profiling for personalized representations and predicting future learning resource preferences.

Result: Experiments on real-world datasets show significant behavioral differences across student clusters and validate KMaP's efficacy.

Conclusion: KMaP effectively addresses key challenges in student knowledge and behavior modeling, offering personalized and context-aware solutions.

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [432] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/pdf/2505.14137)
*Vojtěch Kůr, Vít Musil, Vojtěch Řehák*

Main category: cs.AI

TL;DR: The paper introduces a method to iteratively adjust memory assignments in finite-memory strategies for adversarial patrolling games, improving Defender performance without manual tuning.


<details>
  <summary>Details</summary>
Motivation: The challenge of manually assigning memory sizes in finite-memory Defender strategies hinders usability, prompting a need for an automated solution.

Method: Develops an iterative algorithm to dynamically adjust memory assignments, compatible with any black-box strategy optimization tool.

Result: The method is evaluated on various patrolling models, demonstrating robustness and effectiveness.

Conclusion: The proposed algorithm solves the memory assignment problem, enhancing the practicality of finite-memory strategies in adversarial patrolling games.

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [433] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/pdf/2505.14140)
*Qianyue Hao, Sibo Li, Jian Yuan, Yong Li*

Main category: cs.AI

TL;DR: RLoT enhances LLM reasoning by training a lightweight RL navigator to dynamically select logic blocks, outperforming existing methods by up to 13.4%.


<details>
  <summary>Details</summary>
Motivation: Current inference-time techniques for LLM reasoning lack adaptability due to uniform application across tasks.

Method: Proposes RLoT, where an RL-trained navigator dynamically combines task-specific logic blocks for reasoning.

Result: Outperforms benchmarks by up to 13.4%, with strong transferability across LLMs and tasks.

Conclusion: RLoT offers a cost-effective, adaptable solution to enhance LLM reasoning without parameter modification.

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [434] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/pdf/2505.14141)
*Fanglin Mo, Junzhe Chen, Haoxuan Zhu, Xuming Hu*

Main category: cs.AI

TL;DR: SPlanner, a plug-and-play planning module, improves mobile GUI agents' task execution by modeling apps with EFSMs and generating actionable plans, achieving a 63.8% success rate.


<details>
  <summary>Details</summary>
Motivation: Mobile GUI agents struggle with task planning due to limited understanding of app functionalities, leading to inefficiencies.

Method: SPlanner uses EFSMs to model app logic, decomposes user instructions into function sequences, and refines plans with an LLM.

Result: SPlanner boosts task success rate by 28.8 percentage points on the AndroidWorld benchmark.

Conclusion: SPlanner effectively enhances GUI agent performance by providing structured, actionable plans.

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [435] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/pdf/2505.14143)
*Shuo Zhang, Jinsong Zhang, Zhejun Zhang, Lei Li*

Main category: cs.AI

TL;DR: MMoLRE introduces a novel MTL method for MSA and MER, using shared and task-specific experts to avoid parameter conflicts and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing MTL methods for MSA and MER use hard parameter sharing, leading to parameter conflicts due to complex task correlations.

Method: MMoLRE employs shared and task-specific experts with low-rank structures to model common and unique task characteristics efficiently.

Result: MMoLRE achieves state-of-the-art performance on MSA and competitive results on MER in benchmarks.

Conclusion: MMoLRE effectively addresses parameter conflicts and computational overhead in MTL for MSA and MER.

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [436] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/pdf/2505.14146)
*Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, Jiawei Han*

Main category: cs.AI

TL;DR: s3 is a lightweight framework that decouples retrieval from generation, using a reward based on generation accuracy improvement over naive RAG, outperforming baselines with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems either optimize retrieval with search-only metrics or entangle retrieval with generation, limiting utility and compatibility with frozen/proprietary models.

Method: Proposes s3, a model-agnostic framework that trains a searcher using a Gain Beyond RAG reward, requiring only 2.4k training samples.

Result: Outperforms baselines trained on 70x more data across six general QA and five medical QA benchmarks.

Conclusion: s3 effectively decouples retrieval from generation, enhancing downstream performance with minimal training data.

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [437] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/pdf/2505.14147)
*Xiong Jun Wu, Zhenduo Zhang, ZuJie Wen, Zhiqiang Zhang, Wang Ren, Lei Shi, Cai Chen, Deng Zhao, Dingnan Jin, Qing Cui, Jun Zhou*

Main category: cs.AI

TL;DR: SHARP is a method for synthesizing high-quality STEM problems for training large reasoning models (LRMs) with verifiable rewards, outperforming existing methods like Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating problem sets for LRMs often produce oversimplified or unverifiable data, limiting model performance on complex tasks.

Method: SHARP uses a three-phase framework (Alignment, Instantiation, Inference) to generate diverse, verifiable STEM problems, leveraging an LRM and reinforcement learning with verifiable rewards (RLVR).

Result: SHARP-augmented training significantly improves complex reasoning accuracy, outperforming benchmarks like GPQA.

Conclusion: SHARP advances LRM reasoning capabilities by providing high-quality, verifiable problem sets, pushing performance closer to expert levels.

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [438] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/pdf/2505.14148)
*Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, Hao Liu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [439] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/pdf/2505.14163)
*He Wang, Alexander Hanbo Li, Yiqun Hu, Sheng Zhang, Hideo Kobayashi, Jiani Zhang, Henry Zhu, Chung-Wei Hang, Patrick Ng*

Main category: cs.AI

TL;DR: DSMentor, a curriculum learning-based framework, improves LLM agent performance in data science tasks by organizing tasks by difficulty and using long-term memory, achieving up to 5.2% higher pass rates.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents overlook the order of problem-solving during inference, limiting performance. DSMentor addresses this by mimicking human learning progression.

Method: DSMentor uses curriculum learning to sequence tasks by difficulty and incorporates long-term memory to retain knowledge, enhancing inference-time optimization.

Result: DSMentor improves pass rates by up to 5.2% on benchmarks and shows 8.8% better performance on causality problems compared to GPT-4.

Conclusion: Curriculum-based inference optimization, like DSMentor, is crucial for improving LLM performance by effectively utilizing accumulated knowledge.

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [440] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/pdf/2505.14193)
*Abdallah Abuaisha, Bojie Shen, Daniel Harabor, Peter Stuckey, Mark Wallace*

Main category: cs.AI

TL;DR: The paper proposes dynamic replanning solutions for public transport delays, comparing manual (pull) and proactive (push) approaches, with the push method showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for handling public transport delays are limited, missing opportunities for real-time adjustments despite the availability of live delay data.

Method: The paper formalizes the dynamic replanning problem and introduces two solutions: a manual 'pull' approach and a proactive 'push' approach.

Result: The push approach outperforms the pull method, achieving significant speedups and arrival time savings.

Conclusion: Dynamic replanning, especially the push approach, offers substantial benefits for improving public transport efficiency.

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [441] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/pdf/2505.14209)
*Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu*

Main category: cs.AI

TL;DR: The paper addresses large-scale heterogeneous perimeter-defense games in 3D, introducing the EMFAC framework for scalable defender coordination and efficient decision-making, validated through simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Existing studies on perimeter-defense games are limited to small-scale 2D scenarios, ignoring real-world complexities like motion dynamics and heterogeneity. This work aims to bridge this gap.

Method: The authors investigate 3D perimeter-defense games with realistic elements, derive Nash equilibrium strategies, and propose the EMFAC framework for scalable control. It uses representation learning and an attention mechanism for efficient coordination.

Result: EMFAC outperforms baselines in convergence speed and performance, validated by simulations and small-scale real-world experiments.

Conclusion: The EMFAC framework effectively addresses large-scale heterogeneous defense challenges, offering practical insights for complex scenarios.

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [442] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/pdf/2505.14216)
*Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross*

Main category: cs.AI

TL;DR: RLVR improves accuracy but not capability by focusing on easier questions, while distillation enhances both by introducing new knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand why RLVR fails to improve capability and how distillation succeeds in enhancing both accuracy and capability.

Method: Analyzed RLVR's focus on easier questions and distillation's role in introducing new knowledge and reasoning patterns.

Result: RLVR sacrifices hard-question accuracy for easier ones, while distillation improves capability only with new knowledge.

Conclusion: RLVR and distillation impact reasoning differently, with distillation requiring new knowledge for capability gains.

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [443] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/pdf/2505.14235)
*Yequan Wang, Aixin Sun*

Main category: cs.AI

TL;DR: The paper introduces a taxonomy for Embodied AGI, reviews foundational research, and proposes a framework for higher-level robotic brains.


<details>
  <summary>Details</summary>
Motivation: To advance the discourse on Embodied AGI by systematizing its development stages and addressing challenges.

Method: Introduces a five-level taxonomy (L1-L5), reviews existing research (L1-L2), and outlines components for higher levels (L3-L5).

Result: Proposes a conceptual framework for an L3+ robotic brain, combining technical insights and future directions.

Conclusion: The taxonomy and framework provide a structured approach for advancing Embodied AGI research.

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [444] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/pdf/2505.14289)
*Yijie Lu, Tianjie Ju, Manman Zhao, Xinbei Ma, Yuan Guo, ZhuoSheng Zhang*

Main category: cs.AI

TL;DR: EVA is a red teaming framework for indirect prompt injection attacks on GUI agents, dynamically adapting to attention hotspots for higher success rates and transferability.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents are vulnerable to indirect prompt injection attacks via GUI elements, necessitating a robust testing framework.

Method: EVA transforms attacks into closed-loop optimization by monitoring agent attention and updating adversarial cues dynamically.

Result: EVA outperforms static baselines, achieving higher success rates and uncovering shared behavioral biases in GUI agents.

Conclusion: EVA is effective for red teaming and revealing vulnerabilities in multimodal decision-making of GUI agents.

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [445] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/pdf/2505.14300)
*Maheep Chaudhary, Fazl Barez*

Main category: cs.AI

TL;DR: A real-time framework for predicting harmful AI outputs using unsupervised methods, focusing on backdoor-triggered responses and addressing evasion tactics.


<details>
  <summary>Details</summary>
Motivation: High-risk industries use monitoring for safety; similarly, LLMs need safeguards to detect harmful outputs like violence or hate speech.

Method: Unsupervised approach treating normal behavior as baseline, identifying causal indicators, and using Safety-Net, a multi-detector framework.

Result: 96% accuracy in detecting harmful outputs, even when models evade by altering representations.

Conclusion: Safety-Net effectively monitors LLMs, addressing deception and causal mechanisms in harmful outputs.

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [446] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/pdf/2505.14366)
*Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska*

Main category: cs.AI

TL;DR: A framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT) using a synthetic dataset for spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To advance embodied cognition for Human-Robot Interaction (HRI) by enabling AI systems to understand spatial relationships.

Method: Introduces a synthetic dataset (RGB images, language descriptions, and pose matrices) generated in NVIDIA Omniverse for supervised learning, focusing on Z-axis distance inference.

Result: A publicly available dataset supporting foundational spatial reasoning, with future goals for full 6-DOF reasoning.

Conclusion: This work lays the groundwork for embodied AI systems with spatial understanding in human-robot interactions.

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [447] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.14381)
*Yuyang Dong, Nobuhiro Ueda, Krisztián Boros, Daiki Ito, Takuya Sera, Masafumi Oyamada*

Main category: cs.AI

TL;DR: SCAN enhances RAG systems by semantically analyzing rich documents, improving performance by up to 9.0% for textual and 6.4% for visual RAG.


<details>
  <summary>Details</summary>
Motivation: Rich document analysis is challenging due to high information density; VLMs can improve RAG but need better methods.

Method: SCAN uses a coarse-grained semantic approach to divide documents into coherent regions, fine-tuning object detection models.

Result: SCAN improves textual RAG by 9.0% and visual RAG by 6.4%, outperforming conventional and commercial solutions.

Conclusion: SCAN effectively balances context preservation and efficiency, enhancing RAG systems for rich documents.

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [448] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/pdf/2505.14391)
*Zhaohui Yang, Chenghua He, Xiaowen Shi, Linjing Li, Qiyue Yin, Shihong Deng, Daxin Jiang*

Main category: cs.AI

TL;DR: A novel data annotation method for PRMs improves scoring of long CoT reasoning by addressing error propagation and cessation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current PRM annotation methods fail to account for self-correction in long CoT reasoning, focusing only on initial errors.

Method: Proposes Error Propagation and Error Cessation concepts, uses an LLM-based judger for annotation, and trains a 7B PRM on 1.7M samples.

Result: Outperforms existing PRMs in metrics like search guidance, BoN, and F1, with higher data efficiency than MC-based methods.

Conclusion: The method enhances PRM performance by better capturing self-correction in long CoT reasoning, proving stable and generalizable.

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [449] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/pdf/2505.14394)
*Mihir Athale, Vishal Vaddina*

Main category: cs.AI

TL;DR: A knowledge graph-based approach improves code generation by enhancing contextual relevance and robustness in repository-level tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with contextual accuracy in evolving codebases, and current code search methods lack robustness.

Method: Represents code repositories as graphs, uses a hybrid retrieval approach for context-aware code generation.

Result: Outperforms baseline on EvoCodeBench, showing improved code quality and consistency.

Conclusion: Knowledge graphs can advance robust, context-sensitive coding tools.

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [450] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/pdf/2505.14396)
*Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie*

Main category: cs.AI

TL;DR: The paper introduces the Causal Cartographer framework to model causal relationships and improve counterfactual reasoning in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current foundation models, like LLMs, lack robust causal reasoning capabilities and struggle with counterfactual predictions, which are limited to synthetic datasets.

Method: The framework uses a graph retrieval-augmented generation agent to extract causal relationships and a counterfactual reasoning agent for step-by-step inference.

Result: The approach successfully extracts causal knowledge, enhances LLM robustness for causal tasks, reduces inference costs, and minimizes spurious correlations.

Conclusion: The Causal Cartographer framework advances causal reasoning in LLMs by explicitly modeling causal relationships and improving counterfactual predictions.

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [451] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/pdf/2505.14403)
*Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang*

Main category: cs.AI

TL;DR: BCPG-NSA, a new offline RL framework, improves reasoning model performance by leveraging valuable components in negative samples through fine-grained segmentation and optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods discard or poorly utilize negative samples, missing valuable learning signals like self-reflection and error-correction.

Method: BCPG-NSA involves sample segmentation, consensus-based correctness assessment, and policy optimization with Negative Sample Augmentation (NSA).

Result: Outperforms baselines on math/coding benchmarks, showing improved sample efficiency, robustness, and scalability.

Conclusion: BCPG-NSA effectively mines positive steps in negative samples, enhancing reasoning model performance.

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [452] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/pdf/2505.14412)
*Paweł Batorski, Adrian Kosmala, Paul Swoboda*

Main category: cs.AI

TL;DR: PRL (Prompts from Reinforcement Learning) is an RL-based method for automatic prompt generation, outperforming prior methods in tasks like classification, summarization, and simplification.


<details>
  <summary>Details</summary>
Motivation: Effective prompt engineering is challenging and often requires expert intuition. PRL aims to automate this process by generating novel few-shot examples not seen during training.

Method: PRL uses reinforcement learning to automatically generate prompts, producing novel few-shot examples.

Result: PRL achieves state-of-the-art performance, surpassing APE and EvoPrompt in classification (2.58%, 1.00%), summarization (4.32, 2.12 ROUGE), and simplification (6.93, 6.01 SARI).

Conclusion: PRL demonstrates significant improvements in prompt engineering, offering a scalable and effective solution for enhancing LLM performance.

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [453] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/pdf/2505.14419)
*Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, Anh Tuan Luu*

Main category: cs.AI

TL;DR: SCOPE reduces annotation costs for Process Reward Models (PRMs) by compressing reasoning steps into a prefix tree, cutting complexity from $O(NMK)$ to $O(N)$.


<details>
  <summary>Details</summary>
Motivation: Existing process annotation methods (human or Monte Carlo) are computationally expensive, limiting scalability.

Method: Translate reasoning steps to code, normalize via Abstract Syntax Tree, merge equivalent steps into a prefix tree.

Result: Achieves 196K samples with 5% of prior computational costs; PRMs trained on SCOPE outperform existing methods.

Conclusion: SCOPE offers a cost-effective, scalable alternative to traditional annotation methods for PRMs.

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [454] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/pdf/2505.14479)
*Oren Sultan, Eitan Stern, Dafna Shahaf*

Main category: cs.AI

TL;DR: A neuro-symbolic approach combining LLMs with structured components improves proof accuracy in geometry by leveraging analogous problems and formal verification.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with rigorous logical tasks like mathematical proofs, limiting their reliability in formal domains.

Method: Retrieve analogous problems to guide LLMs and use a formal verifier to evaluate and correct generated proofs.

Result: Proof accuracy improves by 58%-70% for OpenAI's o1 model, with both analogous problems and verifier feedback contributing.

Conclusion: Enhancing LLMs to generate provably correct outputs can boost their reliability for complex, critical applications.

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [455] [Reasoning Models Better Express Their Confidence](https://arxiv.org/pdf/2505.14489)
*Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo*

Main category: cs.AI

TL;DR: Reasoning models (LLMs with chain-of-thought reasoning) outperform non-reasoning models in confidence calibration, achieving better accuracy in 33 out of 36 settings. Slow thinking behaviors (e.g., exploring alternatives, backtracking) in reasoning models enhance calibration, and these benefits extend to non-reasoning models when guided.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often fail to communicate confidence accurately, limiting reliability. This work explores whether reasoning models (with extended chain-of-thought reasoning) improve confidence calibration.

Method: Benchmark six reasoning models across six datasets, comparing confidence calibration with non-reasoning models. Analyze slow thinking behaviors (e.g., exploring alternatives, backtracking) in reasoning models.

Result: Reasoning models achieve better calibration in 33/36 settings. Calibration improves as chain-of-thought reasoning unfolds. Removing slow thinking behaviors reduces calibration. Non-reasoning models also benefit when guided to perform slow thinking.

Conclusion: Reasoning models enhance confidence calibration through slow thinking behaviors. These benefits are transferable to non-reasoning models, suggesting broader applicability.

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [456] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/pdf/2505.14510)
*Haishi Bai, Jozo Dujmovic, Jianwu Wang*

Main category: cs.AI

TL;DR: BACON is a framework for training explainable AI models using graded logic, achieving high accuracy and transparency for human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The need for transparent and trustworthy AI in high-stakes domains like healthcare and finance drives the development of explainable models.

Method: BACON uses graded logic to train models, offering structural transparency and symbolic explanations.

Result: BACON performs well in diverse scenarios (Boolean approximation, Iris classification, house purchasing, cancer diagnosis), producing compact, verifiable logic.

Conclusion: BACON is a practical and principled approach for trustworthy, explainable AI.

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [457] [Guarded Query Routing for Large Language Models](https://arxiv.org/pdf/2505.14524)
*Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, Lukas Galke*

Main category: cs.AI

TL;DR: The paper introduces GQR-Bench for guarded query routing, comparing methods like LLMs, guardrails, and traditional models, finding WideMLP offers the best balance of accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of routing queries to appropriate LLM endpoints while handling out-of-distribution queries effectively.

Method: Introduces GQR-Bench, evaluates LLM-based routing, guardrails, and traditional models (e.g., WideMLP, fastText, SVM, XGBoost).

Result: WideMLP with out-of-domain detection achieves 88% accuracy in <4ms; fastText is fastest (<1ms, 80% accuracy); LLMs are most accurate (91%) but slowest.

Conclusion: Challenges reliance on LLMs for query routing, recommending WideMLP for practical use, and releases GQR-Bench as a Python package.

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [458] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/pdf/2505.14539)
*Gaia Belardinelli, Thomas Bolander, Sebastian Watzl*

Main category: cs.AI

TL;DR: The paper introduces a general logic of attention, addressing limitations in existing dynamic epistemic logics by allowing attention to complex information and avoiding exponential growth in model size.


<details>
  <summary>Details</summary>
Motivation: Existing logics for attention are limited to atomic formulas and become cumbersome with many agents or literals. The paper aims to generalize attention to complex scenarios and improve efficiency.

Method: The authors generalize edge-conditioned event models for succinctness and extend attention to arbitrary formulas, treating attention as a modality with axiomatization principles.

Result: The proposed logic overcomes prior limitations, enabling attention to complex information (e.g., beliefs or other agents' attention) while remaining efficient.

Conclusion: The framework provides a robust logic for attention, illustrated with AI agents detecting human attentional biases, showcasing its practical utility.

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [459] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/pdf/2505.14569)
*Devansh Bhardwaj, Arjun Beniwal, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik R. Narasimhan, Ameet Deshpande, Vishvak Murahari*

Main category: cs.AI

TL;DR: The paper introduces Agent Context Protocols (ACPs) for structured multi-agent communication, improving collective inference and outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems rely on imprecise natural language for coordination, limiting complex interactions and interoperability.

Method: ACPs use persistent execution blueprints and standardized message schemas for robust, fault-tolerant agent communication.

Result: ACPs achieve 28.3% accuracy on AssistantBench and outperform commercial AI systems in human evaluations.

Conclusion: ACPs enable modular, extensible, and high-performance generalist agent systems.

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [460] [Towards a Foundation Model for Communication Systems](https://arxiv.org/pdf/2505.14603)
*Davide Buffelli, Sowmen Das, Yu-Wei Lin, Sattar Vakili, Chien-Yi Wang, Masoud Attarifar, Pritthijit Nath, Da-shan Shiu*

Main category: cs.AI

TL;DR: A transformer-based, multi-modal foundation model for communication data is proposed, addressing key challenges and demonstrating successful feature estimation.


<details>
  <summary>Details</summary>
Motivation: AI's shift toward large general models for multiple applications motivates the development of a foundation model for communication data.

Method: The model uses transformer architecture, addressing tokenization, positional embedding, multimodality, variable feature sizes, and normalization.

Result: The model successfully estimates features like transmission rank, precoder, Doppler spread, and delay profile.

Conclusion: The work advances toward a foundation model for communication data, showing promise for broader AI applications in communication systems.

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [461] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/pdf/2505.14604)
*Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang*

Main category: cs.AI

TL;DR: The paper introduces Self-Braking Tuning (SBT), a framework to reduce redundant reasoning in large reasoning models (LRMs) by enabling self-regulation, cutting token use by 60% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LRMs improve reasoning but suffer from redundant computation and overthinking. Existing solutions rely on external controls, which SBT avoids by internal regulation.

Method: SBT uses overthinking metrics to detect redundancy, trains models for self-regulation, and employs adaptive reasoning lengths and braking prompts.

Result: Experiments show SBT reduces token consumption by up to 60% while matching unconstrained models' accuracy on math benchmarks.

Conclusion: SBT effectively mitigates overthinking in LRMs through self-regulation, offering efficiency gains without compromising performance.

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [462] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/pdf/2505.14615)
*Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken*

Main category: cs.AI

TL;DR: SATBench is a benchmark for evaluating LLMs' logical reasoning using SAT-derived puzzles, revealing limitations in their search-based reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' logical reasoning beyond inference rules, focusing on search-based SAT problems.

Method: Automated generation of SAT-derived puzzles with adjustable difficulty, validated by LLMs, solvers, and humans.

Result: Top model (o4-mini) achieves 65% accuracy on hard UNSAT problems, near random baseline (50%).

Conclusion: SATBench highlights LLMs' limitations in search-based reasoning and offers a scalable testbed for future research.

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [463] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/pdf/2505.14627)
*Ashutosh Adhikari, Mirella Lapata*

Main category: cs.AI

TL;DR: The paper explores using multimodal debate to enhance oversight of LLMs, showing it improves performance over individual models.


<details>
  <summary>Details</summary>
Motivation: Scalable oversight of LLMs is challenging as they surpass human evaluators. Debate offers a solution, especially in multimodal settings.

Method: Extends debate to VQA: two vision-language models debate, a text-only judge adjudicates. Experts defend answers aligned with their beliefs.

Result: Debate framework outperforms individual models; weaker LLMs' judgments enhance vision-language models via finetuning.

Conclusion: Multimodal debate is effective for scalable oversight and improving model performance.

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [464] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/pdf/2505.14656)
*Zihao Zhang, Fei Liu*

Main category: cs.AI

TL;DR: CATS introduces cost-aware planning for LLMs, outperforming raw LLMs like GPT-4.1 in budget-sensitive tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with cost-sensitive planning, either ignoring costs or failing under strict budgets.

Method: CATS integrates cost-awareness into LLM-guided planning using Monte Carlo Tree Search.

Result: CATS achieves higher task success and cost efficiency compared to raw LLMs like GPT-4.1.

Conclusion: CATS effectively combines LLM reasoning with structured search for budget-aware decision-making.

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [465] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/pdf/2505.14667)
*Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No*

Main category: cs.AI

TL;DR: SAFEPATH is a lightweight alignment method for Large Reasoning Models (LRMs) that reduces harmful outputs by emitting a Safety Primer, maintaining reasoning performance with minimal compute.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods degrade reasoning depth and remain vulnerable to jailbreak attacks, necessitating a more efficient solution.

Method: SAFEPATH fine-tunes LRMs to emit an 8-token Safety Primer for harmful prompts, leaving the rest of the reasoning unsupervised.

Result: SAFEPATH reduces harmful responses by up to 90.0%, blocks 83.3% of jailbreak attempts, and requires significantly less compute than alternatives.

Conclusion: SAFEPATH offers an effective, efficient solution for safer AI, with potential for zero-shot application and insights into generalization gaps.

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [466] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/pdf/2505.14668)
*Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan*

Main category: cs.AI

TL;DR: ContextAgent enhances proactive LLM agents by integrating sensory and persona contexts, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing proactive agents lack context-awareness, leading to poor intent understanding and limited functionality.

Method: ContextAgent extracts multi-dimensional sensory contexts from wearables and leverages historical data to predict proactive service needs, then calls tools automatically.

Result: Outperforms baselines by 8.5% in proactive predictions and 6.0% in tool calling on ContextAgentBench.

Conclusion: ContextAgent advances proactive AI assistants, inspiring more human-centric developments.

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [467] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/pdf/2505.14681)
*Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu*

Main category: cs.AI

TL;DR: The paper introduces RICE, a method to improve reasoning in MoE-based LRMs by reinforcing cognitive experts, achieving better accuracy and efficiency without extra training.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning models suffer from cognitive inefficiencies like overthinking and underthinking, which RICE aims to address.

Method: Uses nPMI to identify cognitive experts and steers reasoning at inference time without additional training.

Result: Shows consistent improvements in reasoning accuracy, efficiency, and generalization on benchmarks.

Conclusion: RICE is a lightweight, interpretable, and effective approach to enhance cognitive efficiency in reasoning models.

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


### [468] [TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem](https://arxiv.org/pdf/2311.18662)
*Daniel Fuertes, Carlos R. del-Blanco, Fernando Jaureguizar, Narciso García*

Main category: cs.AI

TL;DR: TOP-Former is a neural network-based solution for the Team Orienteering Problem, offering efficient and accurate route planning for fleets by leveraging a centralized Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Existing VRP solvers either lack efficiency (linear programming) or accuracy (heuristics). TOP-Former addresses this gap by combining speed and precision.

Method: Uses a centralized Transformer neural network to encode the scenario as a graph and analyze the global context of all agents for collaborative solutions.

Result: Outperforms state-of-the-art methods in accuracy and computation speed, as shown in extensive experiments.

Conclusion: TOP-Former provides a scalable and effective solution for fleet route planning, balancing speed and accuracy.

Abstract: Route planning for a fleet of vehicles is an important task in applications
such as package delivery, surveillance, or transportation, often integrated
within larger Intelligent Transportation Systems (ITS). This problem is
commonly formulated as a Vehicle Routing Problem (VRP) known as the Team
Orienteering Problem (TOP). Existing solvers for this problem primarily rely on
either linear programming, which provides accurate solutions but requires
computation times that grow with the size of the problem, or heuristic methods,
which typically find suboptimal solutions in a shorter time. In this paper, we
introduce TOP-Former, a multi-agent route planning neural network designed to
efficiently and accurately solve the Team Orienteering Problem. The proposed
algorithm is based on a centralized Transformer neural network capable of
learning to encode the scenario (modeled as a graph) and analyze the complete
context of all agents to deliver fast, precise, and collaborative solutions.
Unlike other neural network-based approaches that adopt a more local
perspective, TOP-Former is trained to understand the global situation of the
vehicle fleet and generate solutions that maximize long-term expected returns.
Extensive experiments demonstrate that the presented system outperforms most
state-of-the-art methods in terms of both accuracy and computation speed.

</details>


### [469] [Extending Complex Logical Queries on Uncertain Knowledge Graphs](https://arxiv.org/pdf/2403.01508)
*Weizhi Fei, Zihao Wang, Hang Yin, Yang Duan, Yangqiu Song*

Main category: cs.AI

TL;DR: A neural symbolic approach for answering soft queries on uncertain knowledge graphs, combining forward inference and backward calibration to handle uncertainty and avoid errors.


<details>
  <summary>Details</summary>
Motivation: Address the mismatch between real-world uncertain knowledge and first-order logic in existing machine learning-based logical query answering systems.

Method: Propose a neural symbolic approach with forward inference and backward calibration for soft queries on uncertain knowledge graphs.

Result: Theoretical and empirical results show the method avoids cascading errors and outperforms existing query embedding and neural symbolic methods.

Conclusion: The approach effectively bridges the gap between uncertain knowledge and logical query answering, offering superior performance.

Abstract: The study of machine learning-based logical query answering enables reasoning
with large-scale and incomplete knowledge graphs. This paper advances this area
of research by addressing the uncertainty inherent in knowledge. While the
uncertain nature of knowledge is widely recognized in the real world, it does
not align seamlessly with the first-order logic that underpins existing
studies. To bridge this gap, we explore the soft queries on uncertain
knowledge, inspired by the framework of soft constraint programming. We propose
a neural symbolic approach that incorporates both forward inference and
backward calibration to answer soft queries on large-scale, incomplete, and
uncertain knowledge graphs. Theoretical discussions demonstrate that our method
avoids catastrophic cascading errors in the forward inference while maintaining
the same complexity as state-of-the-art symbolic methods for complex logical
queries. Empirical results validate the superior performance of our backward
calibration compared to extended query embedding methods and neural symbolic
approaches.

</details>


### [470] [On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration](https://arxiv.org/pdf/2406.06051)
*Guanghui Yu, Robert Kasumba, Chien-Ju Ho, William Yeoh*

Main category: cs.AI

TL;DR: The paper proposes an AI agent that considers human beliefs about its intentions to improve human-AI collaboration, outperforming static models.


<details>
  <summary>Details</summary>
Motivation: Existing AI collaboration models assume static human behavior, ignoring dynamic human adjustments based on perceived AI intentions.

Method: Developed a human belief model to interpret AI intentions and designed an AI agent incorporating these beliefs into its strategy.

Result: The belief model accurately captures human perceptions, and the AI agent significantly enhances collaboration performance.

Conclusion: Incorporating human beliefs about AI intentions leads to more effective human-AI collaboration.

Abstract: To enable effective human-AI collaboration, merely optimizing AI performance
without considering human factors is insufficient. Recent research has shown
that designing AI agents that take human behavior into account leads to
improved performance in human-AI collaboration. However, a limitation of most
existing approaches is their assumption that human behavior remains static,
regardless of the AI agent's actions. In reality, humans may adjust their
actions based on their beliefs about the AI's intentions, specifically, the
subtasks they perceive the AI to be attempting to complete based on its
behavior. In this paper, we address this limitation by enabling a collaborative
AI agent to consider its human partner's beliefs about its intentions, i.e.,
what the human partner thinks the AI agent is trying to accomplish, and to
design its action plan accordingly to facilitate more effective human-AI
collaboration. Specifically, we developed a model of human beliefs that
captures how humans interpret and reason about their AI partner's intentions.
Using this belief model, we created an AI agent that incorporates both human
behavior and human beliefs when devising its strategy for interacting with
humans. Through extensive real-world human-subject experiments, we demonstrate
that our belief model more accurately captures human perceptions of AI
intentions. Furthermore, we show that our AI agent, designed to account for
human beliefs over its intentions, significantly enhances performance in
human-AI collaboration.

</details>


### [471] [APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking](https://arxiv.org/pdf/2406.14449)
*Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: APEER is a novel automatic prompt engineering algorithm for LLM-based reranking, outperforming manual prompts and showing strong transferability.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot relevance ranking with LLMs relies heavily on human prompt engineering, and existing algorithms don't address IR reranking well.

Method: APEER iteratively refines prompts using feedback and preference optimization.

Result: APEER significantly improves performance over SoTA manual prompts across four LLMs and ten datasets.

Conclusion: APEER reduces human effort and enhances prompt optimization for reranking, with transferable prompts across tasks and LLMs.

Abstract: Large Language Models (LLMs) have significantly enhanced Information
Retrieval (IR) across various modules, such as reranking. Despite impressive
performance, current zero-shot relevance ranking with LLMs heavily relies on
human prompt engineering. Existing automatic prompt engineering algorithms
primarily focus on language modeling and classification tasks, leaving the
domain of IR, particularly reranking, underexplored. Directly applying current
prompt engineering algorithms to relevance ranking is challenging due to the
integration of query and long passage pairs in the input, where the ranking
complexity surpasses classification tasks. To reduce human effort and unlock
the potential of prompt optimization in reranking, we introduce a novel
automatic prompt engineering algorithm named APEER. APEER iteratively generates
refined prompts through feedback and preference optimization. Extensive
experiments with four LLMs and ten datasets demonstrate the substantial
performance improvement of APEER over existing state-of-the-art (SoTA) manual
prompts. Furthermore, we find that the prompts generated by APEER exhibit
better transferability across diverse tasks and LLMs.

</details>


### [472] [Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare](https://arxiv.org/pdf/2408.17401)
*Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni*

Main category: cs.AI

TL;DR: The study explores how explanation content (SHAP vs. Occlusion-1) and format (charts vs. text) impact user comprehension and trust in AI-driven healthcare tools like QCancer. Findings show Occlusion-1 explanations are preferred, but text format (OT) outperforms charts (SC), indicating format is key.


<details>
  <summary>Details</summary>
Motivation: To understand how explanations' content and format influence trust and comprehension in AI healthcare tools, ensuring their effective adoption.

Method: Experiments with SHAP and Occlusion-1 explanations in chart (SC, OC) and text (OT) formats, tested on the general public and medical students.

Result: Occlusion-1 explanations were trusted more than SHAP, but text format (OT) outperformed charts (SC), highlighting format's importance.

Conclusion: Explanation format, not just content, is crucial for user trust and comprehension in AI healthcare tools.

Abstract: AI-driven tools for healthcare are widely acknowledged as potentially
beneficial to health practitioners and patients, e.g. the QCancer regression
tool for cancer risk prediction. However, for these tools to be trusted, they
need to be supplemented with explanations. We examine how explanations' content
and format affect user comprehension and trust when explaining QCancer's
predictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding
format, we present SHAP explanations, conventionally, as charts (SC) and
Occlusion-1 explanations as charts (OC) as well as text (OT), to which their
simpler nature lends itself. We conduct experiments with two sets of
stakeholders: the general public (representing patients) and medical students
(representing healthcare practitioners). Our experiments showed higher
subjective comprehension and trust for Occlusion-1 over SHAP explanations based
on content. However, when controlling for format, only OT outperformed SC,
suggesting this trend is driven by preferences for text. Other findings
corroborated that explanation format, rather than content, is often the
critical factor.

</details>


### [473] [CRoP: Context-wise Robust Static Human-Sensing Personalization](https://arxiv.org/pdf/2409.17994)
*Sawinder Kaur, Avery Gump, Yi Xiao, Jingyu Xin, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin*

Main category: cs.AI

TL;DR: CRoP introduces a static personalization approach for human sensing, addressing intra-user heterogeneity by pruning pre-trained models while retaining generic knowledge, showing robustness in real-world health applications.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of generic neural networks and existing personalization methods in handling intra-user heterogeneity and distribution shifts in human sensing, especially in clinical contexts.

Method: CRoP uses pre-trained models as a base, adaptively prunes a minimal sub-network for user-specific traits, and retains generic knowledge in remaining parameters.

Result: CRoP outperforms baselines in personalization and intra-user robustness across four datasets, including real-world health applications.

Conclusion: CRoP effectively addresses intra-user generalization challenges, validated by empirical analysis, and has significant practical impact.

Abstract: The advancement in deep learning and internet-of-things have led to diverse
human sensing applications. However, distinct patterns in human sensing,
influenced by various factors or contexts, challenge the generic neural network
model's performance due to natural distribution shifts. To address this,
personalization tailors models to individual users. Yet most personalization
studies overlook intra-user heterogeneity across contexts in sensory data,
limiting intra-user generalizability. This limitation is especially critical in
clinical applications, where limited data availability hampers both
generalizability and personalization. Notably, intra-user sensing attributes
are expected to change due to external factors such as treatment progression,
further complicating the challenges. To address the intra-user generalization
challenge, this work introduces CRoP, a novel static personalization approach.
CRoP leverages off-the-shelf pre-trained models as generic starting points and
captures user-specific traits through adaptive pruning on a minimal sub-network
while allowing generic knowledge to be incorporated in remaining parameters.
CRoP demonstrates superior personalization effectiveness and intra-user
robustness across four human-sensing datasets, including two from real-world
health domains, underscoring its practical and social impact. Additionally, to
support CRoP's generalization ability and design choices, we provide empirical
justification through gradient inner product analysis, ablation studies, and
comparisons against state-of-the-art baselines.

</details>


### [474] [IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models](https://arxiv.org/pdf/2410.02429)
*Tuo An, Yunjiao Zhou, Han Zou, Jianfei Yang*

Main category: cs.AI

TL;DR: IoT-LLM enhances LLMs' physical-world reasoning by integrating IoT sensor data and knowledge, achieving a 49.4% performance boost.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with physical-world tasks due to lack of perception. Augmenting them with IoT data and knowledge mimics human cognition.

Method: IoT-LLM preprocesses IoT data, uses retrieval-augmented generation, and activates commonsense knowledge via chain-of-thought prompting.

Result: IoT-LLM improves LLMs' performance by 49.4% on IoT-sensory tasks, as shown in experiments with six LLMs.

Conclusion: IoT-LLM effectively bridges the gap between LLMs and physical-world reasoning, demonstrating significant performance gains.

Abstract: Large Language Models (LLMs) excel in textual and visual tasks but often
produce outputs that defy physical laws when dealing with physical-world
reasoning tasks. Inspired by human cognition, where perception is fundamental
to reasoning, we explore augmenting LLMs with enhanced perception abilities
using Internet of Things (IoT) sensor data and pertinent knowledge for
IoT-sensory task reasoning in the physical world. In this work, we
systematically study LLMs' capability to address real-world IoT-sensory tasks
by augmenting their perception and knowledge base, and then propose a unified
framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three
steps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning and activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions. We design a new
benchmark comprising five real-world tasks with varying data types and
reasoning complexities to evaluate the performance of IoT-LLM. Experimental
results on six LLMs reveal that IoT-LLM significantly improves the performance
of IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a
49.4% average improvement over previous methods.

</details>


### [475] [Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment](https://arxiv.org/pdf/2410.09083)
*Lu Chen, Yuxuan Huang, Yixing Li, Dongrui Liu, Qihan Ren, Shuai Zhao, Kun Kuang, Zilong Zheng, Quanshi Zhang*

Main category: cs.AI

TL;DR: The paper introduces a method to analyze LLM inference patterns in legal contexts, revealing potential inaccuracies despite correct outputs.


<details>
  <summary>Details</summary>
Motivation: To identify incorrect LLM representations in legal judgments by examining detailed inference patterns, not just output correctness.

Method: Quantifies interactions between input phrases as primitive inference patterns, using interaction-based explanations with mathematical guarantees.

Result: Experiments show many LLM inference patterns for legal judgments are misleading or irrelevant, even with correct outputs.

Conclusion: Detailed inference pattern evaluation is crucial to uncover hidden inaccuracies in LLMs, beyond surface-level correctness.

Abstract: This paper presents a method to analyze the inference patterns used by Large
Language Models (LLMs) for judgment in a case study on legal LLMs, so as to
identify potential incorrect representations of the LLM, according to human
domain knowledge. Unlike traditional evaluations on language generation
results, we propose to evaluate the correctness of the detailed inference
patterns of an LLM behind its seemingly correct outputs. To this end, we
quantify the interactions between input phrases used by the LLM as primitive
inference patterns, because recent theoretical achievements have proven several
mathematical guarantees of the faithfulness of the interaction-based
explanation. We design a set of metrics to evaluate the detailed inference
patterns of LLMs. Experiments show that even when the language generation
results appear correct, a significant portion of the inference patterns used by
the LLM for the legal judgment may represent misleading or irrelevant logic.

</details>


### [476] [Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark](https://arxiv.org/pdf/2412.02508)
*Haidong Xu, Meishan Zhang, Hao Ju, Zhedong Zheng, Erik Cambria, Min Zhang, Hao Fei*

Main category: cs.AI

TL;DR: The paper introduces a novel approach for generating emotionally dynamic 3D facial avatars (Emo3D) by breaking the process into Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR), addressing key challenges with a new dataset (EmoAva) and models like the Continuous Text-to-Expression Generator and GiGA.


<details>
  <summary>Details</summary>
Motivation: The research aims to tackle the scarcity of methods for generating emotional 3D avatars due to the complexity of identifying and rendering emotions from spoken words, inspired by human processes.

Method: The approach involves two steps: T3DEM (addressing Expression Diversity, Emotion-Content Consistency, and Expression Fluidity) and 3DAR. A new dataset (EmoAva) and models (Continuous Text-to-Expression Generator and GiGA) are introduced.

Result: The paper presents EmoAva, a dataset for T3DEM, and proposes models to enhance expression consistency, diversity, and fluidity, as well as rendering quality for subtle expressions.

Conclusion: The proposed framework advances Emo3D generation by addressing key challenges and improving the quality of emotional 3D avatar rendering.

Abstract: Producing emotionally dynamic 3D facial avatars with text derived from spoken
words (Emo3D) has been a pivotal research topic in 3D avatar generation. While
progress has been made in general-purpose 3D avatar generation, the exploration
of generating emotional 3D avatars remains scarce, primarily due to the
complexities of identifying and rendering rich emotions from spoken words. This
paper reexamines Emo3D generation and draws inspiration from human processes,
breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping
(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in
determining the quality of Emo3D generation and encompasses three key
challenges: Expression Diversity, Emotion-Content Consistency, and Expression
Fluidity. To address these challenges, we introduce a novel benchmark to
advance research in Emo3D generation. First, we present EmoAva, a large-scale,
high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression
mappings that characterize the aforementioned three challenges in Emo3D
generation. Furthermore, we develop various metrics to effectively evaluate
models against these identified challenges. Next, to effectively model the
consistency, diversity, and fluidity of human expressions in the T3DEM step, we
propose the Continuous Text-to-Expression Generator, which employs an
autoregressive Conditional Variational Autoencoder for expression code
generation, enhanced with Latent Temporal Attention and Expression-wise
Attention mechanisms. Finally, to further enhance the 3DAR step on rendering
higher-quality subtle expressions, we present the Globally-informed Gaussian
Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D
Gaussian representations, enabling the capture of subtle micro-expressions and
seamless transitions between emotional states.

</details>


### [477] [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/pdf/2412.06559)
*Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin*

Main category: cs.AI

TL;DR: ProcessBench is introduced to evaluate models' ability to identify errors in mathematical reasoning, showing PRMs underperform critic models and QwQ-32B-Preview competes with GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable oversight of language models by automating error identification in math problem-solving.

Method: ProcessBench includes 3,400 annotated test cases. Models (PRMs and critic models) are evaluated on identifying erroneous steps.

Result: PRMs generalize poorly; critic models outperform them. QwQ-32B-Preview matches GPT-4o but trails o1-mini.

Conclusion: ProcessBench aims to advance research in reasoning process assessment for better model oversight.

Abstract: As language models regularly make mistakes when solving math problems,
automated identification of errors in the reasoning process becomes
increasingly significant for their scalable oversight. In this paper, we
introduce ProcessBench for measuring the ability to identify erroneous steps in
mathematical reasoning. It consists of 3,400 test cases, primarily focused on
competition- and Olympiad-level math problems. Each test case contains a
step-by-step solution with error location annotated by human experts. Models
are required to identify the earliest step that contains an error, or conclude
that all steps are correct. We conduct extensive evaluation on ProcessBench,
involving two types of models: process reward models (PRMs) and critic models,
where for the latter we prompt general language models to critique each
solution step by step. We draw two main observations: (1) Existing PRMs
typically fail to generalize to more challenging math problems beyond GSM8K and
MATH. They underperform both critic models (i.e., prompted general language
models) and our own trained PRM that is straightforwardly fine-tuned on the
PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has
demonstrated the critique capability competitive with the proprietary model
GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We
hope ProcessBench can foster future research in reasoning process assessment,
paving the way toward scalable oversight of language models.

</details>


### [478] [Platform-Aware Mission Planning](https://arxiv.org/pdf/2501.09632)
*Stefan Panjkovic, Alessandro Cimatti, Andrea Micheli, Stefano Tonetta*

Main category: cs.AI

TL;DR: The paper introduces Platform-Aware Mission Planning (PAMP), addressing the challenge of harmonizing high-level mission goals with low-level platform constraints in autonomous systems. Two approaches are proposed: a baseline amalgamation and an abstraction-refinement method, with the latter proving superior.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems require planning across abstraction levels, balancing mission goals and platform constraints. The non-deterministic behavior of low-level systems complicates robust planning.

Method: Two approaches: 1) amalgamating mission and platform levels, and 2) an abstraction-refinement loop combining a planner and verification engine.

Result: The abstraction-refinement method outperforms the baseline, demonstrating soundness, completeness, and robustness in handling non-determinism.

Conclusion: Heterogeneous modeling is crucial for robust planning, and the abstraction-refinement approach is superior for solving PAMP.

Abstract: Planning for autonomous systems typically requires reasoning with models at
different levels of abstraction, and the harmonization of two competing sets of
objectives: high-level mission goals that refer to an interaction of the system
with the external environment, and low-level platform constraints that aim to
preserve the integrity and the correct interaction of the subsystems. The
complicated interplay between these two models makes it very hard to reason on
the system as a whole, especially when the objective is to find plans with
robustness guarantees, considering the non-deterministic behavior of the lower
layers of the system.
  In this paper, we introduce the problem of Platform-Aware Mission Planning
(PAMP), addressing it in the setting of temporal durative actions. The PAMP
problem differs from standard temporal planning for its exists-forall nature:
the high-level plan dealing with mission goals is required to satisfy safety
and executability constraints, for all the possible non-deterministic
executions of the low-level model of the platform and the environment. We
propose two approaches for solving PAMP. The first baseline approach
amalgamates the mission and platform levels, while the second is based on an
abstraction-refinement loop that leverages the combination of a planner and a
verification engine. We prove the soundness and completeness of the proposed
approaches and validate them experimentally, demonstrating the importance of
heterogeneous modeling and the superiority of the technique based on
abstraction-refinement.

</details>


### [479] [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/pdf/2502.02145)
*Yuan Gao, Mattia Piccinini, Korbinian Moller, Johannes Betz*

Main category: cs.AI

TL;DR: The paper proposes using LLMs with structured parsing and prompt engineering to automate the evaluation and generation of safety-critical driving scenarios, reducing reliance on handcrafted methods.


<details>
  <summary>Details</summary>
Motivation: Current scenario-based testing for autonomous vehicles relies on handcrafted scenarios, which are labor-intensive and lack scalability.

Method: Combines LLMs with structured scenario parsing and prompt engineering, introducing Cartesian and Ego-centric prompts for evaluation and an adversarial generation module for creating critical scenarios.

Result: The approach effectively detects collisions, infers safety, and generates realistic high-risk scenarios, validated via 2D simulation and pre-trained LLMs.

Conclusion: LLMs with domain-informed prompting can efficiently evaluate and generate safety-critical scenarios, minimizing dependence on manual metrics.

Abstract: Ensuring the safety of autonomous vehicles requires virtual scenario-based
testing, which depends on the robust evaluation and generation of
safety-critical scenarios. So far, researchers have used scenario-based testing
frameworks that rely heavily on handcrafted scenarios as safety metrics. To
reduce the effort of human interpretation and overcome the limited scalability
of these approaches, we combine Large Language Models (LLMs) with structured
scenario parsing and prompt engineering to automatically evaluate and generate
safety-critical driving scenarios. We introduce Cartesian and Ego-centric
prompt strategies for scenario evaluation, and an adversarial generation module
that modifies trajectories of risk-inducing vehicles (ego-attackers) to create
critical scenarios. We validate our approach using a 2D simulation framework
and multiple pre-trained LLMs. The results show that the evaluation module
effectively detects collision scenarios and infers scenario safety. Meanwhile,
the new generation module identifies high-risk agents and synthesizes
realistic, safety-critical scenarios. We conclude that an LLM equipped with
domain-informed prompting techniques can effectively evaluate and generate
safety-critical driving scenarios, reducing dependence on handcrafted metrics.
We release our open-source code and scenarios at:
https://github.com/TUM-AVS/From-Words-to-Collisions.

</details>


### [480] [MobileA3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data from Diverse Users](https://arxiv.org/pdf/2502.02982)
*Wenhao Wang, Mengying Yuan, Zijie Yu, Guangyi Liu, Rui Ye, Tian Jin, Siheng Chen, Yanfeng Wang*

Main category: cs.AI

TL;DR: MobileA3gent is a framework for training mobile GUI agents using decentralized, self-sourced data, addressing challenges of automated instruction extraction and privacy-preserving data usage.


<details>
  <summary>Details</summary>
Motivation: High-quality data for mobile GUI agents is costly to obtain manually; automated collection from global users could revolutionize training.

Method: MobileA3gent uses Auto-Annotation for cost-effective data collection and FedVLM-A for federated VLM training under non-IID data.

Result: The framework outperforms traditional methods at 1% of the cost, proving its efficiency and scalability.

Conclusion: MobileA3gent offers a viable, low-cost solution for training mobile agents, with significant real-world application potential.

Abstract: The advancement of mobile GUI agents has opened new opportunities for
automating tasks on mobile devices. Training these agents requires large-scale
high-quality data, which is prohibitively expensive when relying on human
labor. Given the vast population of global mobile phone users, if automated
data collection from them becomes feasible, the resulting data volume and the
subsequently trained mobile agents could reach unprecedented levels.
Nevertheless, two major challenges arise: (1) extracting user instructions
without human intervention and (2) utilizing distributed user data while
preserving privacy. To tackle these challenges, we propose MobileA3gent, a
collaborative framework that trains mobile GUI Agents using decentralized
self-sourced data from diverse users. The framework comprises two components,
each targeting a specific challenge: (1) Auto-Annotation, which enables the
automatic collection of high-quality datasets during users' routine phone usage
with minimal cost. (2) FedVLM-A, which enhances federated VLM training under
non-IID distributions by incorporating adapted global aggregation based on both
episode-level and step-level variability. Extensive experiments prove that
MobileA3gent achieves superior performance over traditional approaches at only
1% of the cost, highlighting its potential for real-world applications

</details>


### [481] [Building reliable sim driving agents by scaling self-play](https://arxiv.org/pdf/2502.14706)
*Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suárez, Eugene Vinitsky*

Main category: cs.AI

TL;DR: The paper proposes scaling self-play to create reliable simulation agents for autonomous vehicle testing, achieving high goal completion and low incident rates.


<details>
  <summary>Details</summary>
Motivation: Reliable simulation agents are crucial for testing systems like AVs, as they must minimize undesired outcomes (e.g., collisions) to ensure accurate experimentation.

Method: The authors use self-play on the Waymo Open Motion Dataset under semi-realistic human limits, training from scratch on a single GPU.

Result: Agents achieve 99.8% goal completion with <0.8% incidents in 10,000 test scenarios, generalize well, and adapt quickly to out-of-distribution scenes.

Conclusion: The open-sourced agents and simulator demonstrate robust performance, offering a foundation for reliable AV testing.

Abstract: Simulation agents are essential for designing and testing systems that
interact with humans, such as autonomous vehicles (AVs). These agents serve
various purposes, from benchmarking AV performance to stress-testing system
limits, but all applications share one key requirement: reliability. To enable
sound experimentation, a simulation agent must behave as intended. It should
minimize actions that may lead to undesired outcomes, such as collisions, which
can distort the signal-to-noise ratio in analyses. As a foundation for reliable
sim agents, we propose scaling self-play to thousands of scenarios on the Waymo
Open Motion Dataset under semi-realistic limits on human perception and
control. Training from scratch on a single GPU, our agents solve almost the
full training set within a day. They generalize to unseen test scenes,
achieving a 99.8% goal completion rate with less than 0.8% combined collision
and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution
generalization, our agents show partial robustness to out-of-distribution
scenes and can be fine-tuned in minutes to reach near-perfect performance in
such cases. We open-source the pre-trained agents and integrate them with a
batched multi-agent simulator. Demonstrations of agent behaviors can be viewed
at https://sites.google.com/view/reliable-sim-agents, and we open-source our
agents at https://github.com/Emerge-Lab/gpudrive.

</details>


### [482] [Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game Using Process Mining](https://arxiv.org/pdf/2503.23326)
*Yiyu Qian, Tim Miller, Zheng Qian, Liyuan Zhao*

Main category: cs.AI

TL;DR: The paper investigates MCTS behavior, highlighting its weaknesses like missing crucial moves, and proposes integrating shallow minimax search to improve performance in 3v3 checkers.


<details>
  <summary>Details</summary>
Motivation: Understanding MCTS decision-making is challenging due to complex search trees, and its selective nature can lead to tactical oversights.

Method: Integrate shallow minimax search into MCTS rollout phase and use process mining to analyze agent strategies in 3v3 checkers.

Result: Improved performance by addressing MCTS weaknesses, with insights into agent strategies.

Conclusion: Combining MCTS with minimax and process mining enhances understanding and performance in sequential decision-making.

Abstract: Monte-Carlo Tree Search (MCTS) is a family of sampling-based search
algorithms widely used for online planning in sequential decision-making
domains and at the heart of many recent advances in artificial intelligence.
Understanding the behavior of MCTS agents is difficult for developers and users
due to the frequently large and complex search trees that result from the
simulation of many possible futures, their evaluations, and their
relationships. This paper presents our ongoing investigation into potential
explanations for the decision-making and behavior of MCTS. A weakness of MCTS
is that it constructs a highly selective tree and, as a result, can miss
crucial moves and fall into tactical traps. Full-width minimax search
constitutes the solution. We integrate shallow minimax search into the rollout
phase of multi-player MCTS and use process mining technique to explain agents'
strategies in 3v3 checkers.

</details>


### [483] [Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning](https://arxiv.org/pdf/2504.05047)
*Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, Heuiseok Lim*

Main category: cs.AI

TL;DR: DOWN is an adaptive multiagent debate framework that activates debate only when necessary, improving efficiency and reducing errors in LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the computational overhead and error risks from unnecessary debates in multiagent LLM collaboration.

Method: Proposes DOWN, which selectively activates debate based on agent confidence scores, refining outputs through peer responses.

Result: DOWN improves efficiency by up to six times and maintains or outperforms existing methods while reducing error propagation.

Conclusion: DOWN effectively enhances LLM performance with lower computational costs.

Abstract: Multiagent collaboration has emerged as a promising framework for enhancing
the reasoning capabilities of large language models (LLMs). Despite
improvements in reasoning, the approach introduces substantial computational
overhead resulting from iterative agent interactions. Furthermore, engaging in
unnecessary debates increases the risk of generating erroneous responses. To
address these challenges, we propose Debate Only When Necessary (DOWN), an
adaptive multiagent debate framework that selectively activates debate based on
the confidence score of the agent's initial response. Debate is activated only
for queries requiring further deliberation, during which agents refine their
outputs by referencing peer responses and associated confidence scores.
Evaluations on benchmarks show that DOWN improves efficiency by up to six times
while preserving or even outperforming the performance of existing methods.
Further analysis indicates that DOWN effectively mitigates the risk of error
propagation stemming from the unnecessary debate process. These findings
demonstrate the effectiveness of our approach in delivering high-performance
LLM solutions at a lower computational cost.

</details>


### [484] [KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/pdf/2504.15364)
*Junyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, Chris Lott*

Main category: cs.AI

TL;DR: KeyDiff is a training-free KV cache eviction method that uses key similarity to retain important tokens, enabling efficient LLM inference under strict memory constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of processing long prompts within limited resources by leveraging the relationship between key diversity and attention scores.

Method: KeyDiff evicts KV cache entries based on key similarity without relying on attention scores, allowing compatibility with optimized attention mechanisms like FlashAttention.

Result: KeyDiff achieves near-baseline performance (0.04% gap) with a 23% KV cache reduction and reduces inference latency by up to 30% compared to other methods.

Conclusion: KeyDiff is an effective, resource-efficient solution for KV cache eviction in LLMs, maintaining performance while reducing memory usage and latency.

Abstract: We demonstrate that geometrically distinctive keys during LLM inference tend
to have high attention scores. Based on the phenomenon we propose KeyDiff, a
training-free KV cache eviction method based solely on key similarity. Unlike
other KV cache eviction methods, KeyDiff can process arbitrarily long prompts
within strict resource constraints and efficiently generate responses. We
provide a theoretical basis for KeyDiff by relating key diversity with
attention scores. These results imply KeyDiff can efficiently identify the most
important tokens to retain. Notably KeyDiff does not rely on attention scores,
allowing the use of optimized attention mechanisms like FlashAttention. Under a
strict memory allowance, we demonstrate the effectiveness of KeyDiff for the
Llama and Qwen model families by observing a performance gap of less than 0.04%
with 8K cache budget ($\sim$23% KV cache reduction) from the non-evicting
baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near
baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning
benchmark and decrease end-to-end inference latency by up to 30% compared to
the other token-eviction methods.

</details>


### [485] [GRAML: Goal Recognition As Metric Learning](https://arxiv.org/pdf/2505.03941)
*Matan Shamir, Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAML introduces a deep metric learning approach for Goal Recognition (GR) using a Siamese network and RNN, enabling quick adaptation to new goals with minimal training.


<details>
  <summary>Details</summary>
Motivation: Current data-driven GR methods require pre-defined goals and extensive training for new goals, limiting flexibility and efficiency.

Method: GRAML employs a Siamese network and RNN to learn a metric in an embedding space, distinguishing observation traces by goal similarity.

Result: GRAML outperforms state-of-the-art GR methods in speed, flexibility, and runtime while maintaining accuracy.

Conclusion: GRAML automates model-learning for GR and efficiently adapts to new goals, offering a scalable and accurate solution.

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [486] [Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL](https://arxiv.org/pdf/2505.06706)
*Yuxuan Zheng, Yihe Zhou, Feiyang Xu, Mingli Song, Shunyu Liu*

Main category: cs.AI

TL;DR: The paper proposes a Bi-level Mean Field (BMF) method to address the curse of dimensionality in large-scale MARL by dynamically grouping agents and modeling bi-level interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Mean Field (MF) methods in MARL simplify interactions but fail to account for individual agent differences, causing aggregation noise. This paper aims to mitigate this issue.

Method: The BMF method uses a Variational AutoEncoder (VAE) for dynamic agent grouping and a bi-level interaction module to model inter- and intra-group interactions.

Result: Experiments show BMF outperforms state-of-the-art methods in various tasks.

Conclusion: BMF effectively reduces aggregation noise and improves learning efficiency in large-scale MARL by capturing agent diversity through dynamic grouping and bi-level interactions.

Abstract: Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the
curse of dimensionality, as the exponential growth in agent interactions
significantly increases computational complexity and impedes learning
efficiency. To mitigate this, existing efforts that rely on Mean Field (MF)
simplify the interaction landscape by approximating neighboring agents as a
single mean agent, thus reducing overall complexity to pairwise interactions.
However, these MF methods inevitably fail to account for individual
differences, leading to aggregation noise caused by inaccurate iterative
updates during MF learning. In this paper, we propose a Bi-level Mean Field
(BMF) method to capture agent diversity with dynamic grouping in large-scale
MARL, which can alleviate aggregation noise via bi-level interaction.
Specifically, BMF introduces a dynamic group assignment module, which employs a
Variational AutoEncoder (VAE) to learn the representations of agents,
facilitating their dynamic grouping over time. Furthermore, we propose a
bi-level interaction module to model both inter- and intra-group interactions
for effective neighboring aggregation. Experiments across various tasks
demonstrate that the proposed BMF yields results superior to the
state-of-the-art methods.

</details>


### [487] [Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering](https://arxiv.org/pdf/2505.08155)
*Weizhi Fei, Zihao Wang, hang Yin, Shukai Zhao, Wei Zhang, Yangqiu Song*

Main category: cs.AI

TL;DR: The paper proposes a scalable symbolic search framework for Complex Query Answering (CQA) to address efficiency and scalability issues in neuro-symbolic methods.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic methods for CQA face data and query complexity bottlenecks, limiting scalability to larger knowledge graphs and complex queries.

Method: The framework introduces constraint strategies to reduce variable domains and an approximate local search algorithm for cyclic queries.

Result: Experiments show a 90% reduction in computational load while maintaining performance.

Conclusion: The proposed framework effectively alleviates efficiency and scalability challenges in CQA.

Abstract: Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.

</details>


### [488] [Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem](https://arxiv.org/pdf/2505.08451)
*Lotfi Kobrosly, Marc-Emmanuel Coupvent des Graviers, Christophe Guettier, Tristan Cazenave*

Main category: cs.AI

TL;DR: A novel algorithm based on Generalized Nested Rollout Policy Adaptation outperforms other MCTS-based methods for solving the Flexible Job-Shop Scheduling Problem (FJSSP), though gaps remain for large instances.


<details>
  <summary>Details</summary>
Motivation: The FJSSP is an NP-hard problem with manufacturing applications, requiring efficient scheduling of operations on dissimilar machines. Existing methods like MCTS have limitations.

Method: Proposed a new algorithm derived from Generalized Nested Rollout Policy Adaptation to address FJSSP.

Result: The algorithm performs better than other MCTS-based approaches, but large instances still fall short of known upper bounds.

Conclusion: The novel algorithm shows promise but requires further improvement for large-scale FJSSP instances.

Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial
optimization problem, with several application domains, especially for
manufacturing purposes. The objective is to efficiently schedule multiple
operations on dissimilar machines. These operations are gathered into jobs, and
operations pertaining to the same job need to be scheduled sequentially.
Different methods have been previously tested to solve this problem, such as
Constraint Solving, Tabu Search, Genetic Algorithms, or Monte Carlo Tree Search
(MCTS). We propose a novel algorithm derived from the Generalized Nested
Rollout Policy Adaptation, developed to solve the FJSSP. We report encouraging
experimental results, as our algorithm performs better than other MCTS-based
approaches, even if makespans obtained on large instances are still far from
known upper bounds.

</details>


### [489] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://arxiv.org/pdf/2505.10468)
*Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee*

Main category: cs.AI

TL;DR: The paper distinguishes AI Agents from Agentic AI, providing a taxonomy, application mapping, and challenge analysis to highlight their differences in design and capabilities.


<details>
  <summary>Details</summary>
Motivation: To clarify the divergent philosophies and capabilities of AI Agents and Agentic AI, aiding in the development of robust and scalable systems.

Method: The study outlines search strategies, foundational definitions, and evaluates architectural evolution, operational mechanisms, and interaction styles. It contrasts applications and challenges of both paradigms.

Result: AI Agents are task-specific, leveraging LLMs and LIMs, while Agentic AI involves multi-agent collaboration and autonomy. Challenges like hallucination and coordination failure are addressed with solutions like ReAct loops and RAG.

Conclusion: The paper offers a roadmap for developing explainable and scalable AI agent and Agentic AI systems, emphasizing their distinct applications and challenges.

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [490] [Most General Explanations of Tree Ensembles (Extended Version)](https://arxiv.org/pdf/2505.10991)
*Yacine Izza, Alexey Ignatiev, Sasha Rubin, Joao Marques-Silva, Peter J. Stuckey*

Main category: cs.AI

TL;DR: The paper proposes a method to find the most general abductive explanation for AI decisions, maximizing input space coverage while ensuring correctness.


<details>
  <summary>Details</summary>
Motivation: To enhance trust in AI systems by providing the broadest and most sensible explanations for decisions.

Method: Uses formal models to identify inflated abductive explanations, which generalize numeric inputs by defining intervals.

Result: Demonstrates how to derive the most general explanation, covering the largest input space while remaining accurate.

Conclusion: The most general explanation is optimal for human understanding, as it applies broadly and appears sensible.

Abstract: Explainable Artificial Intelligence (XAI) is critical for attaining trust in
the operation of AI systems. A key question of an AI system is ``why was this
decision made this way''. Formal approaches to XAI use a formal model of the AI
system to identify abductive explanations. While abductive explanations may be
applicable to a large number of inputs sharing the same concrete values, more
general explanations may be preferred for numeric inputs. So-called inflated
abductive explanations give intervals for each feature ensuring that any input
whose values fall withing these intervals is still guaranteed to make the same
prediction. Inflated explanations cover a larger portion of the input space,
and hence are deemed more general explanations. But there can be many
(inflated) abductive explanations for an instance. Which is the best? In this
paper, we show how to find a most general abductive explanation for an AI
decision. This explanation covers as much of the input space as possible, while
still being a correct formal explanation of the model's behaviour. Given that
we only want to give a human one explanation for a decision, the most general
explanation gives us the explanation with the broadest applicability, and hence
the one most likely to seem sensible. (The paper has been accepted at IJCAI2025
conference.)

</details>


### [491] [GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy](https://arxiv.org/pdf/2505.12355)
*Ya Shen, Gang Chen, Hui Ma, Mengjie Zhang*

Main category: cs.AI

TL;DR: GATES is a novel DRL method combining Graph Attention Networks and Evolution Strategy for cost-aware dynamic workflow scheduling in cloud computing, outperforming state-of-the-art algorithms.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of DRL in workflow scheduling, such as sensitivity to hyperparameters and reward feedback, by proposing a robust and adaptive solution.

Method: Combines Graph Attention Networks to learn DAG task relationships and Evolution Strategy for stable policy learning.

Result: GATES outperforms existing algorithms by capturing task dependencies and adapting to dynamic VM resources.

Conclusion: GATES provides an effective and stable solution for CADWS, with potential applications in cloud computing.

Abstract: Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud
computing, focusing on devising an effective scheduling policy to efficiently
schedule dynamically arriving workflow tasks, represented as Directed Acyclic
Graphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning
(DRL) has been widely employed for automated scheduling policy design. However,
the performance of DRL is heavily influenced by the design of the
problem-tailored policy network and is highly sensitive to hyperparameters and
the design of reward feedback. Considering the above-mentioned issues, this
study proposes a novel DRL method combining Graph Attention Networks-based
policy network and Evolution Strategy, referred to as GATES. The contributions
of GATES are summarized as follows: (1) GATES can capture the impact of current
task scheduling on subsequent tasks by learning the topological relationships
between tasks in a DAG. (2) GATES can assess the importance of each VM to the
ready task, enabling it to adapt to dynamically changing VM resources. (3)
Utilizing Evolution Strategy's robustness, exploratory nature, and tolerance
for delayed rewards, GATES achieves stable policy learning in CADWS. Extensive
experimental results demonstrate the superiority of the proposed GATES in
CADWS, outperforming several state-of-the-art algorithms. The source code is
available at: https://github.com/YaShen998/GATES.

</details>


### [492] [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/pdf/2505.13126)
*Liancheng Gong, Wang Zhu, Jesse Thomason, Li Zhang*

Main category: cs.AI

TL;DR: PDDLego+ is a zero-shot framework for formalizing partially observable environments into PDDL, improving goal success and robustness without prior trajectories.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in handling partially observable environments, which are more realistic but lack complete information.

Method: Proposes PDDLego+, an iterative framework to formalize, plan, grow, and refine PDDL representations without existing trajectories.

Result: Improves goal-reaching success and robustness in textual simulated environments; captured domain knowledge benefits future tasks.

Conclusion: PDDLego+ effectively handles partial observability and enhances planning performance in zero-shot settings.

Abstract: Using LLMs not to predict plans but to formalize an environment into the
Planning Domain Definition Language (PDDL) has been shown to improve
performance and control. Existing work focuses on fully observable
environments; we tackle the more realistic and challenging partially observable
environments that lack of complete, reliable information. We propose PDDLego+,
a framework to iteratively formalize, plan, grow, and refine PDDL
representations in a zero-shot manner, without needing access to any existing
trajectories. On two textual simulated environments, we show that PDDLego+
improves goal reaching success and exhibits robustness against problem
complexity. We also show that the domain knowledge captured after a successful
trial can benefit future tasks.

</details>


### [493] [StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment](https://arxiv.org/pdf/2505.13232)
*Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, Jinwoo Shin*

Main category: cs.AI

TL;DR: StarFT is a framework for fine-tuning zero-shot models to prevent learning spurious features, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning zero-shot models on smaller datasets can lead to learning spurious features, degrading robustness.

Method: StarFT introduces a regularization aligning output distributions for spuriosity-injected labels with the original model, using language models to generate confounding descriptions.

Result: StarFT improves worst-group and average accuracy by 14.30% and 3.02% in Waterbirds, outperforming baselines.

Conclusion: StarFT effectively enhances robustness and generalization in fine-tuned zero-shot models.

Abstract: Learning robust representations from data often requires scale, which has led
to the success of recent zero-shot models such as CLIP. However, the obtained
robustness can easily be deteriorated when these models are fine-tuned on other
downstream tasks (e.g., of smaller scales). Previous works often interpret this
phenomenon in the context of domain shift, developing fine-tuning methods that
aim to preserve the original domain as much as possible. However, in a
different context, fine-tuned models with limited data are also prone to
learning features that are spurious to humans, such as background or texture.
In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a
novel framework for fine-tuning zero-shot models to enhance robustness by
preventing them from learning spuriosity. We introduce a regularization that
aligns the output distribution for spuriosity-injected labels with the original
zero-shot model, ensuring that the model is not induced to extract irrelevant
features further from these descriptions. We leverage recent language models to
get such spuriosity-injected labels by generating alternative textual
descriptions that highlight potentially confounding features. Extensive
experiments validate the robust generalization of StarFT and its emerging
properties: zero-shot group robustness and improved zero-shot classification.
Notably, StarFT boosts both worst-group and average accuracy by 14.30% and
3.02%, respectively, in the Waterbirds group shift scenario, where other robust
fine-tuning baselines show even degraded performance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [494] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/pdf/2505.13577)
*Yubin Kim, Taehan Kim, Wonjune Kang, Eugene Park, Joonsik Yoon, Dongjae Lee, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won Park*

Main category: cs.SD

TL;DR: VocalAgent, an audio LLM, improves vocal health diagnosis with high accuracy, ethical validation, and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of accessible vocal health diagnosis globally.

Method: Uses Qwen-Audio-Chat fine-tuned on hospital datasets, with safety, cross-lingual, and modality evaluations.

Result: Superior accuracy in voice disorder classification compared to baselines.

Conclusion: VocalAgent offers a scalable, ethical solution for vocal health diagnostics.

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [495] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/pdf/2505.13771)
*Wanli Sun, Anton Ragni*

Main category: cs.SD

TL;DR: The paper proposes a new criterion for training energy-based models (EBMs) that improves score learning for first-order optimization, addressing limitations of noise contrastive estimation (NCE) and sliced score matching (SSM).


<details>
  <summary>Details</summary>
Motivation: NCE and SSM have limitations in learning scores suitable for first-order optimization in EBMs and diffusion models (DMs), as they disregard the log-likelihood function's form.

Method: The paper introduces a new criterion focused on learning scores optimized for first-order schemes, contrasting it with NCE and SSM.

Result: Experiments demonstrate the effectiveness of the proposed method for training EBMs compared to NCE and SSM.

Conclusion: The new criterion offers a better approach for score learning in EBMs, addressing the shortcomings of existing methods.

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [496] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/pdf/2505.13805)
*Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Jianhao Ye, Hongbin Zhou, Lei Ma, Jianjun Zhao*

Main category: cs.SD

TL;DR: ClapFM-EVC is a novel framework for high-fidelity emotional voice conversion using natural language prompts or reference speech with adjustable emotion intensity.


<details>
  <summary>Details</summary>
Motivation: Achieving flexible and interpretable control in emotional voice conversion (EVC) remains challenging despite advances.

Method: The framework includes EVC-CLAP for emotional feature alignment, a FuEncoder for feature fusion, and a flow matching model for Mel-spectrogram reconstruction.

Result: Subjective and objective evaluations confirm the effectiveness of ClapFM-EVC.

Conclusion: ClapFM-EVC successfully addresses challenges in EVC by enabling flexible control and high-quality output.

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [497] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/pdf/2505.13847)
*Tianle Yang, Chengzhe Sun, Siwei Lyu, Phil Rose*

Main category: cs.SD

TL;DR: Using segmental speech sound features to detect deepfake audio, focusing on interpretable and hard-to-replicate features.


<details>
  <summary>Details</summary>
Motivation: To leverage interpretable acoustic features tied to human articulatory processes for detecting deepfake audio, as these are harder for deepfake models to replicate.

Method: Analyzing segmental speech sound features, particularly those used in forensic voice comparison, to identify deepfake audio.

Result: Segmental features are effective for deepfake detection, while some global features are less useful.

Conclusion: Segmental features offer a promising approach for audio deepfake detection, differing from traditional forensic voice comparison methods.

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [498] [MatchDance: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis](https://arxiv.org/pdf/2505.14222)
*Kaixing Yang, Xulong Tang, Yuxuan Hu, Jiahao Yang, Hongyan Liu, Qinnan Zhang, Jun He, Zhaoxin Fan*

Main category: cs.SD

TL;DR: MatchDance is a novel framework for music-to-dance generation, enhancing choreographic consistency through a two-stage latent representation approach.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack choreographic consistency in music-to-dance generation, a key challenge in creative content generation.

Method: MatchDance uses a two-stage design: (1) KDQS for latent representation with kinematic-dynamic constraints, and (2) HMDGS for music-to-dance mapping via a Mamba-Transformer hybrid.

Result: State-of-the-art performance on the FineDance dataset, with high-fidelity dance motion generation.

Conclusion: MatchDance addresses choreographic consistency effectively, offering a robust solution for music-to-dance generation.

Abstract: Music-to-dance generation represents a challenging yet pivotal task at the
intersection of choreography, virtual reality, and creative content generation.
Despite its significance, existing methods face substantial limitation in
achieving choreographic consistency. To address the challenge, we propose
MatchDance, a novel framework for music-to-dance generation that constructs a
latent representation to enhance choreographic consistency. MatchDance employs
a two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),
which encodes dance motions into a latent representation by Finite Scalar
Quantization (FSQ) with kinematic-dynamic constraints and reconstructs them
with high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),
which uses a Mamba-Transformer hybrid architecture to map music into the latent
representation, followed by the KDQS decoder to generate 3D dance motions.
Additionally, a music-dance retrieval framework and comprehensive metrics are
introduced for evaluation. Extensive experiments on the FineDance dataset
demonstrate state-of-the-art performance. Code will be released upon
acceptance.

</details>


### [499] [BiCrossMamba-ST: Speech Deepfake Detection with Bidirectional Mamba Spectro-Temporal Cross-Attention](https://arxiv.org/pdf/2505.13930)
*Yassine El Kheir, Tim Polzehl, Sebastian Möller*

Main category: cs.SD

TL;DR: BiCrossMamba-ST is a dual-branch spectro-temporal framework for speech deepfake detection, using bidirectional Mamba blocks and cross-attention for robust performance.


<details>
  <summary>Details</summary>
Motivation: To improve deepfake detection by capturing subtle synthetic speech cues through spectro-temporal analysis.

Method: Leverages dual-branch processing of spectral sub-bands and temporal intervals, integrated with bidirectional Mamba blocks and mutual cross-attention. Uses a 2D attention map for focused analysis.

Result: Achieves 67.74% and 26.3% relative gains over AASIST on ASVSpoof LA21 and DF21, and 6.80% over RawBMamba on DF21.

Conclusion: BiCrossMamba-ST is effective for speech deepfake detection, with significant performance improvements over existing methods.

Abstract: We propose BiCrossMamba-ST, a robust framework for speech deepfake detection
that leverages a dual-branch spectro-temporal architecture powered by
bidirectional Mamba blocks and mutual cross-attention. By processing spectral
sub-bands and temporal intervals separately and then integrating their
representations, BiCrossMamba-ST effectively captures the subtle cues of
synthetic speech. In addition, our proposed framework leverages a
convolution-based 2D attention map to focus on specific spectro-temporal
regions, enabling robust deepfake detection. Operating directly on raw
features, BiCrossMamba-ST achieves significant performance improvements, a
67.74% and 26.3% relative gain over state-of-the-art AASIST on ASVSpoof LA21
and ASVSpoof DF21 benchmarks, respectively, and a 6.80% improvement over
RawBMamba on ASVSpoof DF21. Code and models will be made publicly available.

</details>


### [500] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/pdf/2505.13971)
*Ming Gao, Shilong Wu, Hang Chen, Jun Du, Chin-Hui Lee, Shinji Watanabe, Jingdong Chen, Siniscalchi Sabato Marco, Odette Scharenborg*

Main category: cs.SD

TL;DR: The MISP 2025 Challenge focused on improving meeting transcription using multi-modal (audio-visual) approaches, achieving significant performance gains in diarization and speech recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Meetings present complex acoustic challenges, motivating the use of video alongside audio for better transcription accuracy.

Method: The challenge involved tasks like AVSD, AVSR, and AVDR, using a dataset and baseline systems to evaluate participant solutions.

Result: Top systems improved DER by 7.43%, CER by 10.62%, and cpCER by 72.49% over baselines.

Conclusion: Incorporating video modality significantly enhances meeting transcription performance, as demonstrated by the MISP 2025 Challenge.

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [501] [Bridging Speech Emotion Recognition and Personality: Dataset and Temporal Interaction Condition Network](https://arxiv.org/pdf/2505.13978)
*Yuan Gao, Hao Shi, Yahui Fu, Chenhui Chu, Tatsuya Kawahara*

Main category: cs.SD

TL;DR: The study explores how personality traits enhance speech emotion recognition (SER) by integrating them with acoustic features, achieving significant improvements in valence recognition.


<details>
  <summary>Details</summary>
Motivation: To improve SER by leveraging personality traits, as they correlate with emotional expressions.

Method: Proposed a Temporal Interaction Condition Network (TICN) combining personality and Hubert-based acoustic features, and developed an automatic personality recognition module for practical use.

Result: Ground-truth personality traits improved valence recognition (CCC from 0.698 to 0.785). Automatic traits achieved a CCC of 0.776, an 11.17% improvement.

Conclusion: Personality-aware SER is effective, offering a foundation for future research in personality-aware speech processing.

Abstract: This study investigates the interaction between personality traits and
emotional expression, exploring how personality information can improve speech
emotion recognition (SER). We collected personality annotation for the IEMOCAP
dataset, and the statistical analysis identified significant correlations
between personality traits and emotional expressions. To extract finegrained
personality features, we propose a temporal interaction condition network
(TICN), in which personality features are integrated with Hubert-based acoustic
features for SER. Experiments show that incorporating ground-truth personality
traits significantly enhances valence recognition, improving the concordance
correlation coefficient (CCC) from 0.698 to 0.785 compared to the baseline
without personality information. For practical applications in dialogue systems
where personality information about the user is unavailable, we develop a
front-end module of automatic personality recognition. Using these
automatically predicted traits as inputs to our proposed TICN model, we achieve
a CCC of 0.776 for valence recognition, representing an 11.17% relative
improvement over the baseline. These findings confirm the effectiveness of
personality-aware SER and provide a solid foundation for further exploration in
personality-aware speech processing applications.

</details>


### [502] [Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding for Diffusion-Based Speech Enhancement](https://arxiv.org/pdf/2505.13983)
*Hao Shi, Xugang Lu, Kazuki Shimada, Tatsuya Kawahara*

Main category: cs.SD

TL;DR: The paper explores using deterministic SE models as conditions for diffusion-based SE, proposing a dual-streaming encoding Repair-Diffusion Model (DERDM-SE) to leverage both deterministic and noisy features for better performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in providing reliable conditions for diffusion-based SE due to noisy features led to investigating deterministic SE models as conditions.

Method: Two conditions were tested: deterministic-only and deterministic-noisy. A dual-streaming DERDM-SE was proposed, combining coarse- and fine-grained deterministic models.

Result: DERDM-SE achieved better SE evaluation scores and more stable performance on CHiME4 compared to other diffusion-based SE models.

Conclusion: Deterministic models enhance diffusion-based SE, with DERDM-SE effectively leveraging them for improved performance and stability.

Abstract: Diffusion-based speech enhancement (SE) models need to incorporate correct
prior knowledge as reliable conditions to generate accurate predictions.
However, providing reliable conditions using noisy features is challenging. One
solution is to use features enhanced by deterministic methods as conditions.
However, the information distortion and loss caused by deterministic methods
might affect the diffusion process. In this paper, we first investigate the
effects of using different deterministic SE models as conditions for diffusion.
We validate two conditions depending on whether the noisy feature was used as
part of the condition: one using only the deterministic feature
(deterministic-only), and the other using both deterministic and noisy features
(deterministic-noisy). Preliminary investigation found that using deterministic
enhanced conditions improves hearing experiences on real data, while the choice
between using deterministic-only or deterministic-noisy conditions depends on
the deterministic models. Based on these findings, we propose a dual-streaming
encoding Repair-Diffusion Model for SE (DERDM-SE) to more effectively utilize
both conditions. Moreover, we found that fine-grained deterministic models have
greater potential in objective evaluation metrics, while UNet-based
deterministic models provide more stable diffusion performance. Therefore, in
the DERDM-SE, we propose a deterministic model that combines coarse- and
fine-grained processing. Experimental results on CHiME4 show that the proposed
models effectively leverage deterministic models to achieve better SE
evaluation scores, along with more stable performance compared to other
diffusion-based SE models.

</details>


### [503] [AudSemThinker: Enhancing Audio-Language Models through Reasoning over Semantics of Sound](https://arxiv.org/pdf/2505.14142)
*Gijs Wijngaard, Elia Formisano, Michele Esposito, Michel Dumontier*

Main category: cs.SD

TL;DR: AudSemThinker, a model for fine-grained semantic reasoning in audio, outperforms state-of-the-art models, supported by the novel AudSem dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of audio-language models in reasoning over fine-grained sound semantics.

Method: Introduces AudSemThinker, structured around auditory semantics, and AudSem dataset for semantic descriptor reasoning.

Result: AudSemThinker outperforms state-of-the-art models in semantic audio reasoning.

Conclusion: AudSemThinker and AudSem dataset are publicly released, advancing semantic understanding in audio-language models.

Abstract: Audio-language models have shown promising results in various sound
understanding tasks, yet they remain limited in their ability to reason over
the fine-grained semantics of sound. In this paper, we present AudSemThinker, a
model whose reasoning is structured around a framework of auditory semantics
inspired by human cognition. To support this, we introduce AudSem, a novel
dataset specifically curated for semantic descriptor reasoning in
audio-language models. AudSem addresses the persistent challenge of data
contamination in zero-shot evaluations by providing a carefully filtered
collection of audio samples paired with captions generated through a robust
multi-stage pipeline. Our experiments demonstrate that AudSemThinker
outperforms state-of-the-art models across multiple training settings,
highlighting its strength in semantic audio reasoning. Both AudSemThinker and
the AudSem dataset are released publicly.

</details>


### [504] [Unified Microphone Conversion: Many-to-Many Device Mapping via Feature-wise Linear Modulation](https://arxiv.org/pdf/2410.18322)
*Myeonghoon Ryu, Hongseok Oh, Suji Lee, Han Park*

Main category: cs.SD

TL;DR: Unified Microphone Conversion is a generative framework improving sound event classification by handling device variability through a scalable, many-to-many mapping approach.


<details>
  <summary>Details</summary>
Motivation: Device variability in sound event classification (SEC) systems limits performance. Prior methods like CycleGAN require separate models per device pair, hindering scalability.

Method: The framework conditions the generator on frequency response data for many-to-many device mappings using unpaired training, integrating frequency-response info via Feature-wise Linear Modulation. Synthetic frequency response differences are also incorporated.

Result: Outperforms state-of-the-art by 2.6% and reduces variability by 0.8% in macro-average F1 score.

Conclusion: The proposed method effectively addresses device variability in SEC systems, offering scalability and improved performance.

Abstract: We present Unified Microphone Conversion, a unified generative framework
designed to bolster sound event classification (SEC) systems against device
variability. While our prior CycleGAN-based methods effectively simulate device
characteristics, they require separate models for each device pair, limiting
scalability. Our approach overcomes this constraint by conditioning the
generator on frequency response data, enabling many-to-many device mappings
through unpaired training. We integrate frequency-response information via
Feature-wise Linear Modulation, further enhancing scalability. Additionally,
incorporating synthetic frequency response differences improves the
applicability of our framework for real-world application. Experimental results
show that our method outperforms the state-of-the-art by 2.6% and reduces
variability by 0.8% in macro-average F1 score.

</details>


### [505] [Source Verification for Speech Deepfakes](https://arxiv.org/pdf/2505.14188)
*Viola Negroni, Davide Salvi, Paolo Bestagini, Stefano Tubaro*

Main category: cs.SD

TL;DR: The paper introduces a source verification task to trace synthetic audio origins, leveraging embeddings from a source attribution classifier to assess if tracks share the same source. It evaluates models under diverse conditions and highlights the method's potential and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The rise of speech deepfake generators necessitates not only detecting synthetic audio but also tracing its origin, especially in open-set conditions where existing models struggle.

Method: The approach uses embeddings from a source attribution classifier to compute distance scores between tracks, determining if they originate from the same source. It evaluates models across scenarios like speaker diversity and language mismatch.

Result: The study provides the first exploration of source verification, showcasing its effectiveness and identifying vulnerabilities under varied conditions.

Conclusion: Source verification shows promise for forensic applications but requires further refinement to address challenges like post-processing and unseen generators.

Abstract: With the proliferation of speech deepfake generators, it becomes crucial not
only to assess the authenticity of synthetic audio but also to trace its
origin. While source attribution models attempt to address this challenge, they
often struggle in open-set conditions against unseen generators. In this paper,
we introduce the source verification task, which, inspired by speaker
verification, determines whether a test track was produced using the same model
as a set of reference signals. Our approach leverages embeddings from a
classifier trained for source attribution, computing distance scores between
tracks to assess whether they originate from the same source. We evaluate
multiple models across diverse scenarios, analyzing the impact of speaker
diversity, language mismatch, and post-processing operations. This work
provides the first exploration of source verification, highlighting its
potential and vulnerabilities, and offers insights for real-world forensic
applications.

</details>


### [506] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/pdf/2505.14285)
*Eirini Panteli, Paulo E. Santos, Nabil Humphrey*

Main category: cs.SD

TL;DR: AquaSignal is a modular pipeline for underwater acoustic signal analysis, combining denoising, classification, and novelty detection using deep learning. It achieves 71% classification and 91% novelty detection accuracy on real-world data.


<details>
  <summary>Details</summary>
Motivation: To enhance reliability and accuracy of acoustic signal analysis in noisy marine environments.

Method: Uses U-Net for denoising, ResNet18 for classification, and AutoEncoder for novelty detection. Evaluated on Deepship and ONC datasets.

Result: 71% classification accuracy and 91% novelty detection accuracy.

Conclusion: AquaSignal shows strong potential for real-time underwater acoustic monitoring.

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


### [507] [DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning](https://arxiv.org/pdf/2502.12623)
*Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: DeepResonance, a multimodal music LLM, integrates music, text, images, and videos for enhanced music understanding, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of incorporating additional modalities (images, videos, textual music features) into music LLMs for improved understanding.

Method: Proposes DeepResonance, fine-tuned via multi-way instruction tuning with aligned multimodal data, and introduces multi-sampled ImageBind embeddings and a pre-LLM fusion Transformer.

Result: Achieves state-of-the-art performance across six music understanding tasks.

Conclusion: Multimodal integration and structural enhancements significantly improve music understanding; models and datasets will be open-sourced.

Abstract: Recent advancements in music large language models (LLMs) have significantly
improved music understanding tasks, which involve the model's ability to
analyze and interpret various musical elements. These improvements primarily
focused on integrating both music and text inputs. However, the potential of
incorporating additional modalities such as images, videos and textual music
features to enhance music understanding remains unexplored. To bridge this gap,
we propose DeepResonance, a multimodal music understanding LLM fine-tuned via
multi-way instruction tuning with multi-way aligned music, text, image, and
video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and
Music4way-Any2T, three 4-way training and evaluation datasets designed to
enable DeepResonance to integrate both visual and textual music feature
content. We also introduce multi-sampled ImageBind embeddings and a pre-LLM
fusion Transformer to enhance modality fusion prior to input into text LLMs,
tailoring DeepResonance for multi-way instruction tuning. Our model achieves
state-of-the-art performances across six music understanding tasks,
highlighting the benefits of the auxiliary modalities and the structural
superiority of DeepResonance. We plan to open-source the models and the newly
constructed datasets.

</details>


### [508] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/pdf/2505.14351)
*Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi*

Main category: cs.SD

TL;DR: FMSD-TTS is a few-shot, multi-speaker, multi-dialect TTS framework for Tibetan, addressing low-resource challenges with novel speaker-dialect fusion and DSDR-Net, outperforming baselines in dialect expressiveness and speaker similarity.


<details>
  <summary>Details</summary>
Motivation: Tibetan lacks parallel speech corpora across its dialects, hindering speech modeling progress.

Method: Proposes FMSD-TTS with speaker-dialect fusion and DSDR-Net to capture dialect variations while preserving speaker identity.

Result: Outperforms baselines in dialect expressiveness and speaker similarity; validated via speech-to-speech dialect conversion.

Conclusion: FMSD-TTS advances Tibetan TTS, releases synthetic corpus, and provides an open-source evaluation toolkit.

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [509] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/pdf/2505.14356)
*Sho Inoue, Shai Wang, Haizhou Li*

Main category: cs.SD

TL;DR: A pipeline for creating personality-annotated dialogue datasets from raw audio, using ASR and LLMs, outperforms existing methods in aligning with human judgments.


<details>
  <summary>Details</summary>
Motivation: Address the lack of personality annotations in speech datasets for developing personality-aware conversational agents.

Method: Preprocess raw audio with ASR for transcripts/timestamps, generate annotations, and use LLMs to predict personality. Human evaluators validate labels.

Result: The system aligns better with human judgments than existing approaches.

Conclusion: The proposed pipeline effectively enables personality-aware dialogue systems by leveraging annotated datasets.

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [510] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/pdf/2505.14438)
*Yuanbo Fang, Haoze Sun, Jun Liu, Tao Zhang, Zenan Zhou, Weipeng Chen, Xiaofen Xing, Xiangmin Xu*

Main category: cs.SD

TL;DR: S2SBench is a benchmark to evaluate performance degradation in Speech LLMs, applied to Baichuan-Audio, with datasets and code available.


<details>
  <summary>Details</summary>
Motivation: Speech LLMs show intelligence degradation compared to text-based models, needing systematic evaluation.

Method: Proposed S2SBench with diagnostic datasets and a pairwise evaluation protocol using perplexity differences.

Result: Applied to Baichuan-Audio, demonstrating the benchmark's effectiveness.

Conclusion: S2SBench provides a tool to quantify and analyze degradation in Speech LLMs.

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [511] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/pdf/2505.08175)
*Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons*

Main category: cs.SD

TL;DR: ARC post-training accelerates text-to-audio diffusion models without distillation, achieving ultra-fast inference times.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio systems are too slow for practical creative applications, necessitating faster inference methods.

Method: ARC post-training combines relativistic adversarial formulation with a contrastive discriminator for improved prompt adherence and speed.

Result: Achieves ≈12s of 44.1kHz stereo audio in ≈75ms on an H100 and ≈7s on mobile edge-devices, the fastest known.

Conclusion: ARC post-training is a simple, effective adversarial acceleration method for diffusion models, outperforming distillation-based approaches.

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [512] [Complexity of frequency fluctuations and the interpretive style in the bass viola da gamba](https://arxiv.org/pdf/2505.14448)
*Igor Lugo, Martha G. Alatriste-Contreras, Rafael Sánchez-Guevara*

Main category: cs.SD

TL;DR: The paper models audio signals of musical pieces as a complex network to study frequency fluctuations and bass viola da gamba style, revealing statistical regularities and influential sound groups.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between frequency complexity and interpretive style in bass viola da gamba music using interdisciplinary methods.

Method: Spectral decomposition and network translation of frequency components, followed by best-fit analysis, centrality measures, and clique identification.

Result: Found statistical regularities in frequency distributions, influential sound groups, and functional sound interactions indicating complex fluctuations.

Conclusion: Modeling sound as a complex network links large-scale statistical regularities to similar frequency fluctuations in a musician's performance.

Abstract: Audio signals in a set of musical pieces are modeled as a complex network for
studying the relationship between the complexity of frequency fluctuations and
the interpretive style of the bass viola da gamba. Based on interdisciplinary
scientific and music approaches, we compute the spectral decomposition and
translated its frequency components to a network of sounds. We applied a best
fit analysis for identifying the statistical distributions that describe more
precisely the behavior of such frequencies and computed the centrality measures
and identify cliques for characterizing such a network. Findings suggested
statistical regularities in the type of statistical distribution that best
describes frequency fluctuations. The centrality measure confirmed the most
influential and stable group of sounds in a piece of music, meanwhile the
identification of the largest clique indicated functional groups of sounds that
interact closely for identifying the emergence of complex frequency
fluctuations. Therefore, by modeling the sound as a complex network, we can
clearly associate the presence of large-scale statistical regularities with the
presence of similar frequency fluctuations related to different musical events
played by a same musician.

</details>


### [513] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/pdf/2505.14470)
*Nadav Har-Tuv, Or Tal, Yossi Adi*

Main category: cs.SD

TL;DR: PAST is an end-to-end framework for phonetic modeling and signal reconstruction without relying on pretrained models, outperforming baselines in metrics like phonetic representation and speech reconstruction.


<details>
  <summary>Details</summary>
Motivation: To eliminate dependency on pretrained self-supervised models by integrating supervised phonetic data directly into tokenization.

Method: PAST uses supervised phonetic data and auxiliary tasks for joint modeling, with a streamable, causal variant for real-time applications.

Result: PAST surpasses baseline tokenizers in phonetic representation and speech reconstruction, and excels as a speech representation for language models.

Conclusion: PAST is effective for spoken language generation, with its implementation publicly released for further research.

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


### [514] [Representation Learning for Semantic Alignment of Language, Audio, and Visual Modalities](https://arxiv.org/pdf/2505.14562)
*Parthasaarathy Sudarsanam, Irene Martín-Morató, Tuomas Virtanen*

Main category: cs.SD

TL;DR: A single-stage contrastive learning method aligns audio, visual, and text modalities, outperforming two-stage approaches with improved retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing two-stage methods for trimodal alignment suffer from mismatched data distributions, leading to suboptimal results.

Method: Proposes a single-stage contrastive learning framework using the AVCaps dataset to jointly optimize representations of audio, visual, and text modalities.

Result: The single-stage approach achieves a two-fold improvement in audio-based visual retrieval compared to two-stage methods.

Conclusion: Unified multimodal representation learning through single-stage contrastive training is more effective than separate alignment stages.

Abstract: This paper proposes a single-stage training approach that semantically aligns
three modalities - audio, visual, and text using a contrastive learning
framework. Contrastive training has gained prominence for multimodal alignment,
utilizing large-scale unlabeled data to learn shared representations. Existing
deep learning approach for trimodal alignment involves two-stages, that
separately align visual-text and audio-text modalities. This approach suffers
from mismatched data distributions, resulting in suboptimal alignment.
Leveraging the AVCaps dataset, which provides audio, visual and audio-visual
captions for video clips, our method jointly optimizes the representation of
all the modalities using contrastive training. Our results demonstrate that the
single-stage approach outperforms the two-stage method, achieving a two-fold
improvement in audio based visual retrieval, highlighting the advantages of
unified multimodal representation learning.

</details>


### [515] [Vox-Profile: A Speech Foundation Model Benchmark for Characterizing Diverse Speaker and Speech Traits](https://arxiv.org/pdf/2505.14648)
*Tiantian Feng, Jihwan Lee, Anfeng Xu, Yoonjeong Lee, Thanathai Lertpetchpun, Xuan Shi, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Dani Byrd, Najim Dehak, Shrikanth Narayanan*

Main category: cs.SD

TL;DR: Vox-Profile is a benchmark for multi-dimensional speaker and speech traits using speech foundation models, validated with human evaluation and applied in ASR and speech generation.


<details>
  <summary>Details</summary>
Motivation: Existing works focus on single dimensions of speaker traits, lacking holistic profiling. Vox-Profile addresses this gap by integrating static and dynamic traits.

Method: Developed with domain experts, Vox-Profile uses 15+ speech datasets and foundation models to index speaker and speech characteristics.

Result: The benchmark supports applications like ASR variability analysis and speech generation evaluation, showing convergent validity with human assessment.

Conclusion: Vox-Profile provides a comprehensive, validated tool for profiling speaker and speech traits, with practical downstream applications.

Abstract: We introduce Vox-Profile, a comprehensive benchmark to characterize rich
speaker and speech traits using speech foundation models. Unlike existing works
that focus on a single dimension of speaker traits, Vox-Profile provides
holistic and multi-dimensional profiles that reflect both static speaker traits
(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech
flow). This benchmark is grounded in speech science and linguistics, developed
with domain experts to accurately index speaker and speech characteristics. We
report benchmark experiments using over 15 publicly available speech datasets
and several widely used speech foundation models that target various static and
dynamic speaker and speech properties. In addition to benchmark experiments, we
showcase several downstream applications supported by Vox-Profile. First, we
show that Vox-Profile can augment existing speech recognition datasets to
analyze ASR performance variability. Vox-Profile is also used as a tool to
evaluate the performance of speech generation systems. Finally, we assess the
quality of our automated profiles through comparison with human evaluation and
show convergent validity. Vox-Profile is publicly available at:
https://github.com/tiantiaf0627/vox-profile-release.

</details>


### [516] [aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio](https://arxiv.org/pdf/2409.03377)
*Yan Ru Pei, Ritik Shrivastava, FNU Sidharth*

Main category: cs.SD

TL;DR: aTENNuate is a deep state-space autoencoder for efficient online raw speech enhancement, excelling in denoising, super-resolution, and de-quantization with low latency and high fidelity.


<details>
  <summary>Details</summary>
Motivation: To develop a real-time, end-to-end raw speech enhancement model that performs well in low-resource settings.

Method: Uses a deep state-space autoencoder for online processing, evaluated on tasks like denoising, super-resolution, and de-quantization.

Result: Outperforms previous models in PESQ score, parameter count, MACs, and latency, even with compressed inputs (4000Hz, 4 bits).

Conclusion: aTENNuate is a versatile, efficient solution for speech enhancement, suitable for low-resource environments.

Abstract: We present aTENNuate, a simple deep state-space autoencoder configured for
efficient online raw speech enhancement in an end-to-end fashion. The network's
performance is primarily evaluated on raw speech denoising, with additional
assessments on tasks such as super-resolution and de-quantization. We benchmark
aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets.
The network outperforms previous real-time denoising models in terms of PESQ
score, parameter count, MACs, and latency. Even as a raw waveform processing
model, the model maintains high fidelity to the clean signal with minimal
audible artifacts. In addition, the model remains performant even when the
noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech
enhancement capabilities in low-resource environments. Try it out by pip
install attenuate

</details>


### [517] [xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement](https://arxiv.org/pdf/2501.06146)
*Nikolai Lund Kühne, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan*

Main category: cs.SD

TL;DR: xLSTM-SENet, an xLSTM-based speech enhancement system, matches or outperforms state-of-the-art models like Mamba and Conformer, showcasing linear scalability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Address the scalability limitations of attention-based architectures (e.g., Conformers) in speech enhancement by exploring the potential of xLSTM, which offers linear scalability.

Method: Introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system, and compares it with Mamba- and Conformer-based systems. Uses ablation studies to identify key design choices like exponential gating and bidirectionality.

Result: xLSTM-SENet matches or outperforms state-of-the-art systems across various model sizes on the VoiceBank+Demand dataset. The best model, xLSTM-SENet2, surpasses Mamba and Conformer systems of similar complexity.

Conclusion: xLSTM-based models are effective for speech enhancement, offering scalability and performance advantages over attention-based architectures.

Abstract: While attention-based architectures, such as Conformers, excel in speech
enhancement, they face challenges such as scalability with respect to input
sequence length. In contrast, the recently proposed Extended Long Short-Term
Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based
models remain unexplored for speech enhancement. This paper introduces
xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A
comparative analysis reveals that xLSTM-and notably, even LSTM-can match or
outperform state-of-the-art Mamba- and Conformer-based systems across various
model sizes in speech enhancement on the VoiceBank+Demand dataset. Through
ablation studies, we identify key architectural design choices such as
exponential gating and bidirectionality contributing to its effectiveness. Our
best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and
Conformer-based systems of similar complexity on the Voicebank+DEMAND dataset.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [518] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/pdf/2505.13457)
*Nathan Faraj*

Main category: cs.LG

TL;DR: A new method for optimizing learning rates in ML reveals a proportionality between learning rates and dataset sizes, along with a cumulative learning constant for better training efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand how dataset scale affects training dynamics and improve learning rate optimization.

Method: Identifies a proportionality between learning rates and dataset sizes and introduces a cumulative learning constant.

Result: Provides insights for designing advanced learning rate schedules, enhancing training efficiency and performance.

Conclusion: The findings can significantly improve ML training across various applications.

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [519] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/pdf/2505.13461)
*Junye Jiang, Yaan Zhou, Yuanhao Gong, Haoxuan Yuan, Shuanglong Liu*

Main category: cs.LG

TL;DR: A review of FPGA-based hardware accelerators for CNNs, covering performance evaluation, optimization strategies, and comparisons of FPGA architectures, with future challenges and opportunities highlighted.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of CNNs increases computational demands, requiring efficient hardware accelerators like FPGAs for reconfigurability, parallelism, and energy efficiency.

Method: Comprehensive review of FPGA-based CNN accelerators, including performance evaluation frameworks, optimization strategies (parallel computing, dataflow optimization, hardware-software co-design), and architecture comparisons.

Result: Summarizes key findings on latency, throughput, compute efficiency, power consumption, and resource utilization of FPGA architectures for CNNs.

Conclusion: Identifies future challenges and opportunities, emphasizing potential for innovation in FPGA-based CNN acceleration.

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [520] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/pdf/2505.13462)
*Thien Nguyen, William Guicquero*

Main category: cs.LG

TL;DR: GLT improves BNN input data representation by learning non-linear quantization thresholds, combined with block pruning and KD for lightweight models.


<details>
  <summary>Details</summary>
Motivation: Existing BNN works neglect input data representation, focusing only on weights and activations.

Method: Introduces GLT for input encoding, replaces ADC, and uses block pruning and KD for compact topology.

Result: GLT enhances accuracy on STL-10 and VWW datasets, enabling lightweight (<1Mb) fully-binarized models.

Conclusion: GLT and block pruning create efficient, accurate BNNs suitable for in-sensor always-on inference.

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [521] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/pdf/2505.13463)
*Paolo Guida, William L. Roberts*

Main category: cs.LG

TL;DR: Neural operators can predict complex multi-phase flows quickly, enabling real-time control in industrial applications.


<details>
  <summary>Details</summary>
Motivation: Complex multi-phase flows, like liquid-vapour interfaces, are challenging due to discontinuities and slow CFD models, limiting fast industrial control.

Method: Neural operators trained on volume of fluid simulations to predict multi-phase flow evolution.

Result: High accuracy in predicting liquid-vapour interface evolution, matching industrial time scales.

Conclusion: Neural operators are viable for fast, accurate control of multi-phase industrial processes.

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [522] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/pdf/2505.13471)
*George Bird*

Main category: cs.LG

TL;DR: A novel visualization tool is introduced to analyze axis alignment of embedded data in deep learning models, revealing tendencies toward privileged basis alignment and linking activation functions to representational alignment.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in understanding how deep learning models represent data due to limited methodologies.

Method: A versatile visualization tool evaluates data distribution around planes defined by privileged basis vectors, providing atomistic and holistic metrics. Variations and resolution scales are introduced for different applications.

Result: Embedded representations tend to align with privileged bases, influenced by activation functions. Examples include grandmother neurons in various networks.

Conclusion: The method explains the tendency of representations to align with neuron bases, linking functional form symmetry breaking to representational alignment.

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [523] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/pdf/2505.13499)
*Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Markos A. Katsoulakis*

Main category: cs.LG

TL;DR: The paper applies optimal control theory to Transformers, improving performance and efficiency with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To move beyond trial-and-error approaches by using optimal control theory for systematic improvements in Transformer training and design.

Method: Uses continuous-time formulations from optimal control theory to enhance Transformer models with minimal implementation changes.

Result: Improves test performance and parameter efficiency, e.g., 46% lower test loss with 42% fewer parameters in nanoGPT, and 5.6% reduction in GPT-2.

Conclusion: The framework provides a theory-driven, plug-and-play solution for better Transformer performance and scalability.

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [524] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/pdf/2505.13501)
*Zequn He, Celia Reina*

Main category: cs.LG

TL;DR: SPIEDiff is a machine learning framework that uses statistical physics and diffusion models to accurately predict thermodynamics and kinetics from short-time particle data, reducing computational time significantly.


<details>
  <summary>Details</summary>
Motivation: Overcome obstacles like time-scale limitations, non-unique thermodynamic potentials, and uncertainty quantification in particle simulations for dissipative systems.

Method: Leverages statistical physics, conditional diffusion models, and epinets to model purely dissipative systems.

Result: Accurately uncovers thermodynamics and kinetics, enabling long-time predictions with quantified uncertainty in minutes.

Conclusion: SPIEDiff provides a robust, efficient, and trustworthy method for data-driven thermodynamic model discovery.

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [525] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/pdf/2505.13502)
*Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang, Chengqi Zhang*

Main category: cs.LG

TL;DR: The paper surveys FedLoRA, combining LoRA and FL for efficient, privacy-preserving fine-tuning of foundation models, addressing distributed learning, heterogeneity, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in leveraging private datasets for foundation models while ensuring privacy and efficiency.

Method: Integration of Low-Rank Adaptation (LoRA) into Federated Learning (FL) frameworks, focusing on distributed learning, heterogeneity, and efficiency.

Result: Categorization of existing work by methods addressing each challenge, providing insights into FedLoRA's current state.

Conclusion: Identifies open research questions and future directions for advancing FedLoRA.

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [526] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/pdf/2505.13507)
*Haoyang Chen*

Main category: cs.LG

TL;DR: The paper proposes using CLIP for Open-Set Domain Adaptation (OSDA) with two innovations: prompt-driven alignment and gradient-aware separation, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of aligning known-class distributions and identifying unknown categories in OSDA by leveraging semantic relationships and reducing error accumulation.

Method: Uses CLIP with learnable textual prompts for cross-domain alignment and a gradient analysis module for open-set separation based on gradient behaviors.

Result: Outperforms CLIP and standard baselines on Office-Home, with ablation studies confirming the importance of gradient norms.

Conclusion: The proposed method effectively addresses OSDA challenges by combining prompt-driven alignment and gradient-aware separation, demonstrating superior performance.

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [527] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/pdf/2505.13510)
*Conor Rowan, Alireza Doostan*

Main category: cs.LG

TL;DR: The paper critiques the conflation of sparsity with interpretability in scientific machine learning (SciML) and proposes a mechanism-focused definition of interpretability for physical sciences.


<details>
  <summary>Details</summary>
Motivation: Researchers seek interpretable models in SciML to uncover fundamental principles, but current definitions and methods lack clarity and adequacy for scientific purposes.

Method: The paper reviews interpretable ML literature, identifies deficiencies, and proposes an operational definition of interpretability emphasizing mechanistic understanding over sparsity.

Result: The proposed definition shows sparsity is often unnecessary and questions interpretable discovery without prior knowledge.

Conclusion: A precise definition of interpretability in SciML can guide research toward overcoming key obstacles in data-driven science.

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [528] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/pdf/2505.13515)
*Yanan Li, Fanxu Meng, Muhan Zhang, Shiai Zhu, Shangguang Wang, Mengwei Xu*

Main category: cs.LG

TL;DR: LoRASuite efficiently adapts existing LoRA weights to newer LLM versions, outperforming retraining methods while saving resources.


<details>
  <summary>Details</summary>
Motivation: The high cost and inefficiency of retraining LoRA weights for updated LLMs motivate the need for a method to leverage existing weights.

Method: LoRASuite uses a transfer matrix, layer/head allocation based on alignment metrics, and fine-tuning for stability.

Result: LoRASuite outperforms vanilla LoRA and full retraining, improving math task performance and reducing resource usage.

Conclusion: LoRASuite offers a scalable, efficient solution for adapting LoRA weights to evolving LLMs.

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [529] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/pdf/2505.13521)
*Gabor Petnehazi, Laith Al Shaggah, Jozsef Gall, Bernadett Aradi*

Main category: cs.LG

TL;DR: Zero-shot time series forecasting using pre-trained models (TimesFM, CHRONOS) for mortality rates shows mixed results, with CHRONOS excelling short-term and fine-tuning improving long-term accuracy. Random Forest outperformed all.


<details>
  <summary>Details</summary>
Motivation: To explore zero-shot forecasting's potential for mortality rates without task-specific fine-tuning, comparing foundation models to traditional methods.

Method: Evaluated TimesFM and CHRONOS against ARIMA, Lee-Carter, and Random Forest across 5, 10, and 20-year forecasts using data from 50 countries and 111 age groups.

Result: CHRONOS outperformed traditional methods short-term; fine-tuning boosted its long-term accuracy. TimesFM underperformed. Random Forest was best overall.

Conclusion: Zero-shot forecasting has promise but requires model selection and domain adaptation; fine-tuning and traditional methods remain valuable.

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [530] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/pdf/2505.14126)
*Yuan-Hao Jiang, Kezong Tang, Zi-Wei Chen, Yuang Wei, Tian-Yi Liu, Jiayi Wu*

Main category: cs.LG

TL;DR: The paper introduces MAS-KCL, a multi-agent system algorithm for learning and optimizing KC graphs to improve educational outcomes.


<details>
  <summary>Details</summary>
Motivation: Accurate KC graphs help educators identify root causes of poor learner performance and enable targeted interventions.

Method: Developed MAS-KCL, a multi-agent system using large language models and bidirectional feedback for KC graph optimization.

Result: Tested on synthetic and real-world datasets, MAS-KCL effectively identifies learning paths for better instructional planning.

Conclusion: MAS-KCL enhances learning path recognition, aiding educators in designing comprehensive plans for sustainable education.

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [531] [Multi-head Temporal Latent Attention](https://arxiv.org/pdf/2505.13544)
*Keqi Deng, Philip C. Woodland*

Main category: cs.LG

TL;DR: MTLA reduces KV cache size in Transformer self-attention, improving inference speed and memory usage without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The linear growth of the KV cache with sequence length is a bottleneck for inference efficiency in Transformer models.

Method: MTLA compresses the KV cache along the temporal dimension using a hyper-network and introduces a stride-aware causal mask for training-inference consistency.

Result: MTLA achieves competitive performance across tasks, with a 5.3x speedup and 8.3x memory reduction in speech translation.

Conclusion: MTLA effectively addresses the KV cache bottleneck, enhancing efficiency while maintaining model performance.

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [532] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/pdf/2505.13547)
*Pengxin Guo, Yinong Wang, Wei Li, Mengting Liu, Ming Li, Jinkai Zheng, Liangqiong Qu*

Main category: cs.LG

TL;DR: FedPrLLM is a federated pruning framework for privacy-preserving compression of LLMs, enabling collaborative pruning without sharing local data.


<details>
  <summary>Details</summary>
Motivation: Current LLM pruning methods require public calibration samples, which are hard to obtain in privacy-sensitive domains. FedPrLLM addresses this by allowing clients to compute pruning masks locally and share only those with the server.

Method: Clients calculate pruning masks from local data and share them with the server to prune the global model. The framework explores various pruning strategies and weight scaling options.

Result: One-shot pruning with layer comparison and no weight scaling is identified as the optimal choice within FedPrLLM.

Conclusion: FedPrLLM provides a privacy-preserving solution for LLM pruning and guides future work in privacy-sensitive fields.

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [533] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/pdf/2505.13563)
*Xiaohui Wang, Peng Ye, Chenyu Huang, Shenghe Zheng, Bo Zhang, Wanli Ouyang, Tao Chen*

Main category: cs.LG

TL;DR: UltraDelta is a data-free delta compression pipeline that achieves high compression and performance by minimizing redundancy and maximizing information across inter-layer, intra-layer, and global dimensions.


<details>
  <summary>Details</summary>
Motivation: The storage overhead of fine-tuned models in the pretrained paradigm is addressed by delta compression, but existing methods struggle with balancing compression and performance.

Method: UltraDelta uses Variance-Based Mixed Sparsity Allocation, Distribution-Aware Compression, and Trace-Norm-Guided Rescaling to optimize compression and performance.

Result: UltraDelta outperforms existing methods across large language models, NLP, vision, and multi-modal models with compression ratios up to 800x.

Conclusion: UltraDelta is a robust solution for delta compression, excelling in ultra-high compression scenarios while maintaining performance.

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [534] [Online Decision-Focused Learning](https://arxiv.org/pdf/2505.13564)
*Aymeric Capitaine, Maxime Haddouche, Eric Moulines, Michael I. Jordan, Etienne Boursier, Alain Durmus*

Main category: cs.LG

TL;DR: Decision-focused learning (DFL) is extended to dynamic environments where data and objectives evolve, addressing challenges like non-differentiability and non-convexity with regularization and optimism-based optimization.


<details>
  <summary>Details</summary>
Motivation: Existing DFL methods assume static data and objectives, but real-world scenarios often involve dynamic changes, necessitating new approaches.

Method: The paper proposes (i) regularization for differentiability and (ii) an optimism-based online algorithm with perturbation, applicable to simplex and polytope decision spaces.

Result: The algorithm achieves bounded expected dynamic regret and outperforms prediction-focused methods in a knapsack experiment.

Conclusion: The work successfully adapts DFL to dynamic settings, offering a practical solution with theoretical guarantees.

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [535] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/pdf/2505.13567)
*Yoav Ger, Omri Barak*

Main category: cs.LG

TL;DR: The paper develops a theory for learning dynamics of linear RNNs in closed-loop settings, contrasting them with open-loop training, and applies it to a motor control task.


<details>
  <summary>Details</summary>
Motivation: To address the gap between typical open-loop RNN training and real-world closed-loop learning, providing insights into biologically plausible learning.

Method: Mathematical analysis of linear RNNs in closed-loop contexts, comparing learning trajectories with open-loop training.

Result: Closed-loop RNNs exhibit distinct learning stages driven by short-term policy improvement and long-term stability, differing from open-loop dynamics.

Conclusion: Closed-loop dynamics are crucial for biologically plausible models, as demonstrated by the theory and application to motor control.

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [536] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/pdf/2505.13569)
*Fynn Fromme, Christine Allen-Blanchette, Hans Harder, Sebastian Peitz*

Main category: cs.LG

TL;DR: An equivariant surrogate model for large-scale physics systems improves accuracy and efficiency in modeling complex dynamics like Rayleigh-Bénard convection.


<details>
  <summary>Details</summary>
Motivation: Machine learning aids in modeling physics systems governed by PDEs, but challenges like high degrees of freedom and multi-scale dynamics require better accuracy and efficiency.

Method: An end-to-end equivariant model combines an equivariant convolutional autoencoder and LSTM using G-steerable kernels, tested on 3D Rayleigh-Bénard convection.

Result: The model shows significant gains in sample and parameter efficiency, scaling well with complex dynamics (e.g., larger Rayleigh numbers).

Conclusion: The proposed architecture effectively addresses challenges in physics system modeling, offering improved efficiency and scalability.

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [537] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/pdf/2505.13575)
*Ilkay Wunderlich, Benjamin Koch, Sven Schönfeld*

Main category: cs.LG

TL;DR: The paper discusses optimizing TinyYOLOv3 for FPGAs using techniques like batch normalization fusion, filter pruning, and quantization to address computational and memory challenges.


<details>
  <summary>Details</summary>
Motivation: CNNs are widely used in computer vision but face deployment challenges on embedded platforms like FPGAs due to high computational demands.

Method: The paper employs batch normalization fusion, filter pruning, and post-training quantization to optimize TinyYOLOv3 for XILINX Artix-7 FPGAs.

Result: The proposed methods effectively reduce computational intensity and memory requirements for deploying CNNs on FPGAs.

Conclusion: The techniques presented offer practical solutions for running CNNs efficiently on resource-constrained FPGA platforms.

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [538] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/pdf/2505.13576)
*Sara Alosaime, Arshad Jhumka*

Main category: cs.LG

TL;DR: FlexFed is a novel Federated Learning approach for HAR, addressing catastrophic forgetting by optimizing data retention and training frequency, improving efficiency and convergence.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods struggle in HAR due to non-stationary data and intermittent participation, leading to catastrophic forgetting. Privacy constraints prevent traditional CL solutions.

Method: FlexFed dynamically adjusts training frequency and prioritizes data retention. Introduces a new CF metric and a realistic HAR evaluation framework.

Result: FlexFed reduces CF, boosts FL efficiency by 10-15%, and ensures faster, stable convergence, especially for underrepresented data.

Conclusion: FlexFed effectively addresses CF in HAR settings, enhancing FL performance under privacy constraints.

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [539] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/pdf/2505.13578)
*Mikhail Osipov*

Main category: cs.LG

TL;DR: A variational method for reducing task costs in invariant functionals using symmetry-breaking deformations, without needing model gradients or labels.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimizing non-differentiable, symmetry-invariant cost functionals in machine learning and imaging, where traditional gradient-based methods fail.

Method: Proposes a variational approach using gauge fields to construct symmetry-breaking deformations, minimizing an auxiliary energy functional to ensure cost reduction.

Result: Proves strict cost reduction under mild conditions, with degeneracies of zero Gaussian measure, applicable to black-box models.

Conclusion: Provides a principled, gradient-free tool for optimizing invariant costs via Lie-algebraic variational flows.

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [540] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/pdf/2505.13580)
*Hanzhao Wang, Guanting Chen, Kalyan Talluri, Xiaocheng Li*

Main category: cs.LG

TL;DR: The paper introduces OMGPT, a GPT-based model for sequential decision-making in operations research, outperforming existing methods by leveraging pre-trained data and avoiding analytical model assumptions.


<details>
  <summary>Details</summary>
Motivation: To address sequential decision-making tasks in operations research (e.g., dynamic pricing, inventory management) with a unified, data-driven approach.

Method: Proposes a transformer-based neural network (OMGPT) trained as a sequential predictor, bypassing traditional analytical models.

Result: OMGPT shows superior performance across tasks, supported by theoretical insights linking performance to pre-training diversity and task divergence.

Conclusion: OMGPT represents a paradigm shift in OR/OM tasks, offering a scalable, model-free solution with strong empirical results.

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [541] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/pdf/2505.13582)
*Leyang Zhang, Yaoyu Zhang, Tao Luo*

Main category: cs.LG

TL;DR: The paper explores critical points in neural networks, introducing a sample-independent lifting operator and distinguishing between sample-dependent and sample-independent critical points. It shows limitations of prior critical embeddings and proves the existence of sample-dependent critical points, including saddles, for large samples.


<details>
  <summary>Details</summary>
Motivation: To understand how critical points in neural networks depend on sample data and to generalize the concept of critical points beyond sample-specific cases.

Method: Introduces a sample-independent critical lifting operator to map parameters between networks, defining lifted critical points. Examines prior critical embeddings and demonstrates their limitations.

Result: Shows that sample-independent lifted critical points exist beyond prior embeddings and proves sample-dependent critical points (including saddles) emerge for large sample sizes.

Conclusion: The study advances the understanding of critical points in neural networks, highlighting the role of sample dependence and the existence of saddles in large-sample scenarios.

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [542] [Half Search Space is All You Need](https://arxiv.org/pdf/2505.13586)
*Pavel Rumiantsev, Mark Coates*

Main category: cs.LG

TL;DR: Proposes pruning the search space using Zero-Shot NAS to reduce memory and time in One-Shot NAS while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High GPU memory requirements in One-Shot NAS methods like DARTS hinder efficiency.

Method: Uses Zero-Shot NAS to prune low-performing architectures before applying One-Shot NAS.

Result: Reduces memory consumption by 81% with no loss in accuracy.

Conclusion: Efficiently combines Zero-Shot and One-Shot NAS for improved performance.

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [543] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/pdf/2505.13614)
*Ke Sun*

Main category: cs.LG

TL;DR: The paper analyzes the Fisher information metric tensor in deep neural networks, introduces an unbiased random estimator for it, and provides deterministic bounds for the metric tensor.


<details>
  <summary>Details</summary>
Motivation: Understanding the Fisher information metric tensor in deep neural networks is crucial for theoretical insights and practical applications in deep learning.

Method: The study analyzes the spectrum of the Riemannian metric in a low-dimensional core space and extends findings to deterministic bounds on the neuromanifold. An unbiased random estimator for the metric tensor is introduced, leveraging Hutchinson's trace estimator.

Result: The proposed estimator efficiently evaluates the metric tensor (diagonal, block diagonal, or full tensor) with guaranteed quality, as its standard deviation is bounded by the true value up to scaling.

Conclusion: The work provides a practical and theoretically sound method for estimating the Fisher information metric tensor in deep neural networks, with implications for both theory and applications.

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [544] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/pdf/2505.13631)
*Andrei Manolache, Luiz F. O. Chamon, Mathias Niepert*

Main category: cs.LG

TL;DR: ACE introduces adaptive constrained optimization to balance equivariance and flexibility in neural networks, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Real-world data often lacks perfect symmetry, making strictly equivariant models less effective. ACE aims to leverage partial symmetries without rigid constraints.

Method: ACE uses homotopy principles to gradually transition a non-equivariant model toward equivariance, smoothing training and achieving a data-driven balance.

Result: ACE outperforms strictly equivariant models and heuristic relaxations in performance, sample efficiency, and robustness.

Conclusion: ACE provides a principled way to handle partial symmetries, enhancing model adaptability and effectiveness in real-world scenarios.

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [545] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/pdf/2505.13636)
*Baiting Chen, Tong Zhu, Jiale Han, Lexin Li, Gang Li, Xiaowu Dai*

Main category: cs.LG

TL;DR: PEG is a game-theoretic framework aligning LLMs without training, using peer elicitation to improve factual accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce inconsistencies and hallucinations; PEG aims to align them truthfully without supervision.

Method: PEG uses a generator and discriminators in a peer evaluation setting with a determinant-based mutual information score to incentivize truthful reporting.

Result: Theoretical guarantees show sublinear regret and convergence to truthful Nash equilibrium; empirical results show improved factual accuracy.

Conclusion: PEG is a practical, unsupervised method for eliciting truthful behavior from LLMs.

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [546] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/pdf/2505.13638)
*Massimo Fioravanti, Giovanni Agosta*

Main category: cs.LG

TL;DR: The paper introduces 4Hammer, a reinforcement learning environment based on Warhammer 40,000, to address the lack of complex board games for LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs perform well on short-duration tasks but struggle with longer, complex tasks like board games. There's a need for environments like 4Hammer to evaluate LLMs in such scenarios.

Method: The authors propose 4Hammer, a digital twin simulation of Warhammer 40,000, designed for reinforcement learning and LLM evaluation.

Result: 4Hammer provides a platform to test LLMs on complex, rule-heavy board games, simulating human-like understanding and interaction.

Conclusion: The 4Hammer environment fills a gap in evaluating LLMs on extended-duration, complex tasks, offering a benchmark for future research.

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [547] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/pdf/2505.13643)
*Rakibul Hasan Rajib, Md Akil Raihan Iftee, Mir Sazzat Hossain, A. K. M. Mahbubur Rahman, Sajib Mistry, M Ashraful Amin, Amin Ahsan Ali*

Main category: cs.LG

TL;DR: FedCTTA is a privacy-preserving, scalable framework for federated learning that adapts models at test-time without sharing raw data or features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: FL models degrade due to distribution shifts, and existing TTA methods in FL face computational, privacy, and scalability issues.

Method: FedCTTA uses similarity-aware aggregation based on model outputs over noise samples, avoiding feature sharing and minimizing entropy for continual adaptation.

Result: FedCTTA outperforms existing methods in diverse scenarios, maintaining privacy and scalability.

Conclusion: FedCTTA effectively addresses FL challenges, offering a robust, efficient solution for test-time adaptation.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [548] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/pdf/2505.13644)
*Felix Dangel, Tim Siebert, Marius Zeinhofer, Andrea Walther*

Main category: cs.LG

TL;DR: The paper introduces an optimization technique for Taylor mode automatic differentiation (AD) to accelerate computing PDE operators by collapsing derivatives, outperforming nested backpropagation.


<details>
  <summary>Details</summary>
Motivation: Nested backpropagation for computing PDE operators is computationally expensive, limiting its utility in scientific machine learning.

Method: The technique 'collapses' derivatives by rewriting the computational graph, propagating a sum up the graph, applicable to general linear PDE operators and randomized Taylor mode.

Result: The implemented collapsing procedure accelerates Taylor mode and outperforms nested backpropagation on popular PDE operators.

Conclusion: The proposed optimization simplifies derivative computation for PDE operators, potentially improving efficiency in scientific machine learning applications.

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [549] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/pdf/2505.13650)
*Chou-Ying Hsieh, Chun-Fu Jang, Cheng-En Hsieh, Qian-Hui Chen, Sy-Yen Kuo*

Main category: cs.LG

TL;DR: SRGCL is a novel Graph Contrastive Learning framework that dynamically selects high-quality positive pairs to preserve graph semantics and structure, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring high-quality positive pairs in Graph Contrastive Learning (GCL) to avoid distorting the original graph's properties.

Method: Proposes SRGCL with a unified positive pair generator, a manifold-guided selector, and a probabilistic mechanism for iterative refinement.

Result: SRGCL consistently outperforms state-of-the-art GCL methods in graph-level classification tasks.

Conclusion: SRGCL is adaptable and effective across various domains, enhancing GCL performance.

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [550] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/pdf/2505.13697)
*Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati*

Main category: cs.LG

TL;DR: The paper critically examines RL-based post-training of LLMs, showing that simplifying MDP assumptions make it equivalent to supervised learning, with comparable performance.


<details>
  <summary>Details</summary>
Motivation: To scrutinize the structural assumptions in RL-based post-training of LLMs and their implications for reasoning abilities.

Method: Analyzes MDP assumptions, compares RL (GRPO) with supervised fine-tuning on benchmarks like GSM8K and Countdown using Qwen-2.5 models.

Result: Supervised fine-tuning matches GRPO performance, questioning the necessity of RL for LLM reasoning improvements.

Conclusion: Simplistic MDP assumptions undermine RL frameworks for LLMs, suggesting supervised methods may suffice.

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [551] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/pdf/2505.13702)
*Mariana A. Fazio, Salvador Sosa Güitron, Marcus Babzien, Mikhail Fedurin, Junjie Li, Mark Palmer, Sandra S. Biedron, Manel Martinez-Ramon*

Main category: cs.LG

TL;DR: Unsupervised anomaly detection for faulty images in MUED, eliminating manual labeling and providing uncertainty measures.


<details>
  <summary>Details</summary>
Motivation: Avoid manual labeling and tedious initial image examination by using unsupervised techniques.

Method: Unsupervised anomaly detection methodology to autonomously identify faulty images.

Result: Machine detects anomalies without labeled data, offering uncertainty measures for decision-making.

Conclusion: Unsupervised methods are effective for anomaly detection in MUED, reducing manual effort and providing actionable insights.

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [552] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/pdf/2505.13709)
*Jiayu Chen, Aravind Venugopal, Jeff Schneider*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for offline model-based RL to improve robustness by dynamically adapting the world model and policy under a single objective, addressing issues like objective mismatch and adversarial noise sensitivity.


<details>
  <summary>Details</summary>
Motivation: Existing offline MBRL methods suffer from objective mismatch and lack robustness, leading to poor performance under adversarial conditions.

Method: The framework uses a maximin optimization problem solved via Stackelberg learning dynamics, unifying world model and policy training.

Result: The method achieves state-of-the-art performance on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks.

Conclusion: The proposed approach effectively addresses robustness and performance issues in offline MBRL, demonstrating superior results.

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [553] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/pdf/2505.13723)
*Pratik Rathore, Zachary Frangella, Sachin Garg, Shaghayegh Fazliani, Michał Dereziński, Madeleine Udell*

Main category: cs.LG

TL;DR: ADASAP is a scalable, distributed algorithm for Gaussian process inference, outperforming existing methods on large datasets.


<details>
  <summary>Details</summary>
Motivation: Gaussian processes struggle with scalability on large datasets due to quadratic computational complexity.

Method: Proposes ADASAP, an approximate, distributed sketch-and-project algorithm for solving linear systems in GP inference.

Result: ADASAP converges rapidly to the true posterior mean, outperforming state-of-the-art solvers and scaling to datasets with >300M samples.

Conclusion: ADASAP is a principled, efficient solution for large-scale GP inference, enabling applications in biostatistics and Bayesian optimization.

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [554] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/pdf/2505.13738)
*Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness*

Main category: cs.LG

TL;DR: The paper investigates scaling laws for hyperparameters (HPs) in LLM pre-training, focusing on learning rate and weight decay, and their relationship with model size, dataset size, and batch size. It validates and extends prior work, providing predictive power laws for optimal HPs and batch sizes, and discusses practical implications for training efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the efficiency of large-scale LLM pre-training by understanding how hyperparameters and batch sizes should scale with model and dataset sizes, addressing gaps in prior work.

Method: The research involves analyzing scaling laws for hyperparameters (e.g., learning rate, weight decay) and batch sizes (optimal and critical) as functions of model size (N), dataset size (D), and batch size (B). It verifies and extends existing theories through empirical validation.

Result: Key findings include: (1) Optimal weight decay scales linearly with batch size for fixed N,D; (2) A power law governs the optimal timescale in terms of tokens-per-parameter ratio; (3) Both optimal and critical batch sizes scale as power laws in D, independent of N.

Conclusion: The paper provides actionable insights for selecting hyperparameters and batch sizes in large-scale training, enabling more efficient and predictable LLM pre-training under compute and time constraints.

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [555] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/pdf/2505.13740)
*Chenning Yu, Sicun Gao*

Main category: cs.LG

TL;DR: A novel resampling criterion using lift scores improves compositional generation in diffusion models without additional training.


<details>
  <summary>Details</summary>
Motivation: To enhance condition alignment in compositional generation tasks without extra training or modules.

Method: Leverages lift scores to evaluate and compose generated samples, with an optimized variant for lower computational overhead.

Result: Lift scores significantly improve condition alignment in 2D synthetic data, CLEVR tasks, and text-to-image synthesis.

Conclusion: The lift score-based approach effectively enhances compositional generation in diffusion models.

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [556] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/pdf/2505.13742)
*Andrew Nam, Declan Campbell, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie*

Main category: cs.LG

TL;DR: A probabilistic framework for interpreting latent task representations in neural networks, using Bayesian inference and information theory.


<details>
  <summary>Details</summary>
Motivation: Neural networks are powerful but hard to interpret due to sub-symbolic semantics.

Method: Introduces a probabilistic framework inspired by Bayesian inference and information theory to analyze latent representations.

Result: Proposes tools to measure properties like distributedness, manifold complexity, and polysemanticity.

Conclusion: The framework enhances interpretability of neural network representations.

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [557] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/pdf/2505.13745)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: The paper addresses data non-stationarity in streams, focusing on concept drifts and novel class emergence, and proposes a synthetic data generation strategy for testing unsupervised drift detectors and open set recognition.


<details>
  <summary>Details</summary>
Motivation: To tackle the dual challenges of concept drifts and novel class emergence in non-stationary data streams, which are often addressed separately.

Method: A strategy for generating synthetic data streams incorporating both concept drifts and new class emergence, tested with unsupervised drift detectors.

Result: Demonstrates the effectiveness of unsupervised drift detectors in handling novelty and drifts, and the utility of synthetic streams for open set recognition.

Conclusion: The synthetic data generation strategy provides a robust framework for evaluating methods addressing both concept drifts and novel classes in dynamic environments.

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [558] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/pdf/2505.13754)
*Devendra Parkar, Anya Chaturvedi, Andréa W. Richa, Joshua J. Daymude*

Main category: cs.LG

TL;DR: An unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs, combining GNNs with a distributed update mechanism, outperforming state-of-the-art methods in scalability and runtime.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of finding MaxIS in dynamic graphs where edges change over time, which lacks prior unsupervised learning solutions.

Method: Combines structural learning from GNNs with a learned distributed update mechanism to handle edge changes and infer MaxIS membership in parallel.

Result: Achieves competitive approximation ratios, scalability, and outperforms state-of-the-art methods in solution quality, runtime, and memory usage.

Conclusion: The model generalizes well to larger graphs, matching performance of greedy techniques and commercial solvers while running faster.

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [559] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/pdf/2505.13755)
*Jeffrey Lai, Anthony Bao, William Gilpin*

Main category: cs.LG

TL;DR: Panda, a patched attention model, is trained on synthetic chaotic systems and shows zero-shot forecasting of real-world chaotic dynamics and emergent abilities like predicting PDEs without retraining.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems are hard to model due to sensitivity to errors. Existing models are either specialized or lack dynamical structure. Panda aims to bridge this gap using dynamical systems theory.

Method: Panda is trained on a synthetic dataset of 20,000 chaotic systems discovered via an evolutionary algorithm. It uses patched attention for nonlinear dynamics.

Result: Panda exhibits zero-shot forecasting of real-world chaotic systems, nonlinear resonance patterns, and the ability to predict PDEs without retraining. It also shows neural scaling laws for differential equations.

Conclusion: Panda demonstrates the potential of pretrained models for abstract mathematical domains like nonlinear dynamics, offering a scalable approach to chaotic system modeling.

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [560] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/pdf/2505.13760)
*Drona Khurana, Anish Thilagar, Dhamma Kimpara, Rafael Frongillo*

Main category: cs.LG

TL;DR: The paper explores the equivalence between indirect elicitation (IE) and calibration for non-polyhedral surrogates, focusing on convex differentiable losses. It shows equivalence in one-dimensional cases, provides a counter-example for higher dimensions, and introduces strong IE for broader applicability.


<details>
  <summary>Details</summary>
Motivation: To simplify the verification of calibration for non-polyhedral surrogates by leveraging indirect elicitation (IE) and introducing strong IE for stronger guarantees.

Method: Analyzes convex differentiable losses, proves equivalence of IE and calibration in one dimension, constructs a counter-example for higher dimensions, and introduces strong IE.

Result: IE and calibration are equivalent for one-dimensional losses, but not in higher dimensions. Strong IE is introduced and proven to imply calibration for differentiable surrogates, with necessity and sufficiency for strongly convex cases.

Conclusion: The findings enable easier verification of calibration for differentiable surrogates, with strong IE providing a robust tool for designing and analyzing consistent surrogates.

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [561] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/pdf/2505.13765)
*Hainan Xu, Vladimir Bataev, Lilit Grigoryan, Boris Ginsburg*

Main category: cs.LG

TL;DR: WIND accelerates RNN-T inference by processing frames in parallel within a window, achieving 2.4X speed-up without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of RNN-T inference without sacrificing model accuracy.

Method: Processes multiple frames simultaneously in a window for parallel decoding, applied to greedy and beam-search methods.

Result: 2.4X speed-up in greedy modes with identical WER; beam-search offers better accuracy and speed.

Conclusion: WIND is a highly effective method for accelerating RNN-T inference while maintaining or improving accuracy.

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [562] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/pdf/2505.13768)
*Ruiquan Huang, Donghao Li, Chengshuai Shi, Cong Shen, Jing Yang*

Main category: cs.LG

TL;DR: A hybrid learning framework for RL combines offline datasets and online interactions, outperforming pure online or offline methods. The algorithm achieves state-of-the-art results in sub-optimality gap and online learning regret.


<details>
  <summary>Details</summary>
Motivation: To improve RL performance by leveraging both offline data and online interactions, overcoming limitations of pure online or offline methods.

Method: A unified algorithm integrating confidence-based online RL with offline datasets, analyzed for sub-optimality gap and regret minimization.

Result: The algorithm achieves improved bounds for sub-optimality gap and regret, with a separation in coverage properties for the two metrics.

Conclusion: The hybrid framework is validated in experiments, showing superior performance in RL models like linear contextual bandits and MDPs.

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [563] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/pdf/2505.13775)
*Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati*

Main category: cs.LG

TL;DR: The paper challenges the assumption that intermediate reasoning traces (CoT) in large models causally influence performance, showing that even noisy or incorrect traces can yield accurate solutions.


<details>
  <summary>Details</summary>
Motivation: To critically examine whether the semantics of intermediate tokens ("thoughts") in reasoning models actually influence performance, as opposed to being over-interpreted as human-like reasoning.

Method: Train transformer models on formally verifiable reasoning traces and solutions, aligning them with a formal solver (A* search), and evaluate trace and solution correctness. Also, train models on noisy traces to test trace-solution relationship.

Result: Models trained on correct traces still produce invalid reasoning traces, and noisy traces can yield similar or even better performance, showing loose trace-solution connection.

Conclusion: Intermediate tokens (CoT) do not reliably induce predictable reasoning behaviors, cautioning against anthropomorphizing or over-interpreting them as human-like.

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [564] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/pdf/2505.13787)
*Chris Cundy, Adam Gleave*

Main category: cs.LG

TL;DR: The paper explores whether incorporating a lie detector in AI training improves honesty or teaches deception evasion, using DolusChat dataset. Results vary based on factors like lie detector accuracy and training methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns that lie detectors in AI training might lead to deceptive behaviors instead of honesty, and to evaluate their effectiveness.

Method: Incorporated a lie detector in LLM post-training, using the DolusChat dataset to analyze honesty factors like exploration, lie detector accuracy, and KL regularization.

Result: Preference learning with lie detectors can lead to high deception rates (85%), but high lie detector TPR or KL regularization ensures honesty. Off-policy methods (DPO) maintain lower deception rates (<25%).

Conclusion: Lie-detector-enhanced training can either improve oversight or encourage undetectable deception, depending on context and implementation.

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [565] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/pdf/2505.13791)
*Austin H. Cheng, Chong Sun, Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: Quetzal is a scalable autoregressive model for 3D molecule generation, combining a causal transformer and Diffusion MLP, outperforming baselines and matching diffusion models in quality while enabling faster generation and exact likelihood computation.


<details>
  <summary>Details</summary>
Motivation: To address the dominance of diffusion models in 3D molecule generation and improve upon the limitations of autoregressive models, such as generation quality and speed.

Method: Quetzal treats molecules as ordered sequences, using a causal transformer for atom type prediction and a Diffusion MLP for position distribution, reducing expensive transformer passes.

Result: Quetzal achieves better generation quality than autoregressive baselines, competes with diffusion models, and offers faster generation and exact likelihood computation.

Conclusion: Quetzal demonstrates the potential of scalable autoregressive models for 3D molecule generation, balancing quality, speed, and versatility for tasks like hydrogen decoration and scaffold completion.

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [566] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/pdf/2505.13811)
*Parikshit Bansal, Sujay Sanghavi*

Main category: cs.LG

TL;DR: The paper proposes using context-free generation to mitigate catastrophic forgetting in fine-tuned language models by estimating KL divergence without access to original training data.


<details>
  <summary>Details</summary>
Motivation: To address the degradation of model performance on existing tasks (catastrophic forgetting) during fine-tuning, especially when original training data is unavailable.

Method: Penalize KL divergence between original and fine-tuned models using context-free generation for unbiased estimation. Augment fine-tuning data with context-free generations.

Result: Context-free generations effectively mitigate forgetting, outperforming contextual synthetic data and pretraining data subsets. Results validated on OLMo-1B and R1-Distill-Llama-8B models.

Conclusion: Context-free generation is a simple yet effective method to preserve model performance during fine-tuning, particularly for zero-shot and reasoning tasks.

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [567] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/pdf/2505.13813)
*Matthew Raffel, Lizhong Chen*

Main category: cs.LG

TL;DR: FlashKAT addresses memory bottlenecks in KAT, achieving 86.5x faster training by optimizing gradient accumulation.


<details>
  <summary>Details</summary>
Motivation: KAT's slow training speed, despite comparable FLOPs to Transformers, limits its practicality.

Method: Identified memory stalls in GR-KAN's backward pass and restructured the kernel for efficient gradient accumulation.

Result: FlashKAT achieves 86.5x speedup over KAT and reduces gradient rounding errors.

Conclusion: FlashKAT significantly improves KAT's training efficiency, making it more viable for large-scale tasks.

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [568] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/pdf/2505.13819)
*Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe*

Main category: cs.LG

TL;DR: LLMs can leak sensitive data even with partial, unordered information. The paper introduces fragment-specific extraction attacks and proposes two data-blind methods to investigate them.


<details>
  <summary>Details</summary>
Motivation: To assess LLM vulnerability under weaker adversarial assumptions where attackers have only partial, unordered data.

Method: Proposes two data-blind methods: (1) likelihood ratio attack and (2) PRISM, which uses an external prior.

Result: Both methods compete with a data-aware baseline, showing robustness in medical and legal examples.

Conclusion: Fine-tuned LLMs are susceptible to fragment-specific extraction attacks, highlighting privacy risks.

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [569] [Structured Agent Distillation for Large Language Model](https://arxiv.org/pdf/2505.13820)
*Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang*

Main category: cs.LG

TL;DR: Structured Agent Distillation compresses large LLM-based agents into smaller models while maintaining reasoning and action fidelity, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: High inference costs and large model sizes limit practical deployment of LLM-based agents, necessitating efficient compression methods.

Method: Segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align with teacher behavior.

Result: Outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop.

Conclusion: Span-level alignment is crucial for efficient and deployable agents.

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [570] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/pdf/2505.13852)
*Yusheng Zhao, Chi Zhang, Yuxuan Du*

Main category: cs.LG

TL;DR: Benchmarking study shows traditional ML models often match or outperform DL models in quantum ground-state learning tasks, questioning the necessity of DL in this domain.


<details>
  <summary>Details</summary>
Motivation: To clarify the role and necessity of DL models in learning ground-state properties of quantum systems, given inconsistent prior studies and unfair comparisons.

Method: Systematically benchmark DL against traditional ML models across three Hamiltonian families, scaling to 127 qubits, with equivalent quantum resource usage.

Result: ML models perform comparably or better than DL models in all tasks; measurement input features minimally impact DL performance.

Conclusion: Current DL models may not be necessary for many quantum system learning tasks, offering insights for their effective use.

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [571] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/pdf/2505.13857)
*Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun*

Main category: cs.LG

TL;DR: The paper proposes TedTrajRec, a method for improving low-sampling-rate GPS trajectories by modeling spatio-temporal dynamics in traffic and trajectories using PD-GNN and TedFormer.


<details>
  <summary>Details</summary>
Motivation: Low sampling rates and irregular intervals in GPS trajectories hinder their direct use in GPS-based systems, necessitating enhanced recovery methods.

Method: TedTrajRec combines PD-GNN for traffic dynamics and TedFormer (a time-aware Transformer) for trajectory dynamics, leveraging neural ODEs for irregular data.

Result: Experiments on real-world datasets show TedTrajRec outperforms existing methods.

Conclusion: TedTrajRec effectively addresses spatio-temporal dynamics in trajectory recovery, offering superior performance.

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [572] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/pdf/2505.13858)
*Gonzalo E. Constante-Flores, Hao Chen, Can Li*

Main category: cs.LG

TL;DR: A framework for enforcing input-dependent constraints on neural network outputs, combining a task network and a safe network to guarantee feasibility without iterative procedures.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack mechanisms to enforce hard constraints (e.g., physical laws, fairness), and existing methods are limited or computationally expensive.

Method: Proposes a model-agnostic framework with a task network for accuracy and a safe network using stochastic/robust optimization rules to ensure feasibility.

Result: The method guarantees constraint satisfaction, maintains competitive accuracy, and has low inference latency in benchmark tasks.

Conclusion: The framework effectively enforces constraints universally without runtime optimization, balancing feasibility and performance.

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [573] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/pdf/2505.13873)
*Peisong Niu, Ziqing Ma, Tian Zhou, Weiqi Chen, Lefei Shen, Rong Jin, Liang Sun*

Main category: cs.LG

TL;DR: Baguan, a self-supervised Siamese Autoencoder model, outperforms traditional weather forecasting methods by mitigating overfitting through innovative pre-training strategies.


<details>
  <summary>Details</summary>
Motivation: Overfitting in AI-based weather forecasting due to limited historical data necessitates novel approaches to enhance performance with existing datasets.

Method: The paper introduces Baguan, a Siamese Autoencoder pre-trained self-supervised and fine-tuned for medium-range forecasting, leveraging locality bias to control overfitting.

Result: Baguan surpasses traditional methods in accuracy and shows robustness in downstream tasks like S2S modeling and regional forecasting.

Conclusion: Pre-training with challenging tasks and locality bias effectively mitigates overfitting, making Baguan a promising model for weather forecasting.

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [574] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/pdf/2505.13878)
*Yanggan Gu, Zhaoyi Yan, Yuanyi Wang, Yiming Zhang, Qi Zhou, Fei Wu, Hongxia Yang*

Main category: cs.LG

TL;DR: InfiFPO is a new method for implicit model fusion in preference alignment, outperforming existing techniques by leveraging multi-source probabilities and improving LLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing model fusion methods for preference alignment (PA) discard probability information, limiting their effectiveness. InfiFPO addresses this gap.

Method: InfiFPO replaces the reference model in DPO with a fused source model, using probability clipping and max-margin fusion to align with human preferences and distill knowledge.

Result: InfiFPO improves the average performance of Phi-4 from 79.95 to 83.33 on 11 benchmarks, excelling in math, coding, and reasoning.

Conclusion: InfiFPO advances model fusion by preserving probability information and outperforming existing methods, enhancing LLM capabilities.

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [575] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/pdf/2505.13896)
*Yingwei Zhang, Ke Bu, Zhuoran Zhuang, Tao Xie, Yao Yu, Dong Li, Yang Guo, Detao Lv*

Main category: cs.LG

TL;DR: The paper introduces CRAFT, a method for time series forecasting (TSF) that leverages Cross-Future Behavior (CFB) to address uncertainty in predictions. It uses modules to extract trends, supplement missing data, and guide predictions hierarchically, validated by real-world experiments.


<details>
  <summary>Details</summary>
Motivation: TSF faces uncertainty due to limited past observations. CFB, which influences future outcomes, is explored to improve forecasting accuracy.

Method: CRAFT employs three modules: Koopman Predictor for trend extraction, Internal Trend Mining for data supplementation, and External Trend Guide for hierarchical trend representation. A demand-constrained loss calibrates predictions.

Result: Experiments on real-world datasets and online A/B tests confirm CRAFT's effectiveness.

Conclusion: CRAFT successfully addresses TSF uncertainty by utilizing CFB and hierarchical trend modeling, validated by empirical results.

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [576] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/pdf/2505.13898)
*Róbert Csordás, Christopher D. Manning, Christopher Potts*

Main category: cs.LG

TL;DR: Deeper LLMs like Llama 3.1 and Qwen 3 don't efficiently use their depth for new computations but spread existing ones over more layers, leading to diminishing returns.


<details>
  <summary>Details</summary>
Motivation: To investigate whether deeper LLMs use their depth for higher-order computations or merely spread existing computations across layers.

Method: Analyzed residual streams of Llama 3.1 and Qwen 3 models, compared sublayer outputs, skipped layers, and trained linear maps between shallow and deep models.

Result: Layers in the second half contribute less, skipping them has minimal impact, and deeper models map computations to similar relative depths in shallow models.

Conclusion: Deeper models don't leverage depth for new computations but refine existing ones, explaining diminishing returns in scaled Transformer architectures.

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [577] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/pdf/2505.13899)
*Zeyu Michael Li, Hung Anh Vu, Damilola Awofisayo, Emily Wenger*

Main category: cs.LG

TL;DR: The paper investigates how dataset and task overlap cause similarity in machine learning models' representations, finding both factors correlate with higher similarity.


<details>
  <summary>Details</summary>
Motivation: To understand why machine learning models exhibit similar representations across modalities, focusing on dataset and task overlap as causal factors.

Method: Conducts experiments to evaluate the impact of dataset overlap (common in large-scale models) and task overlap (linked to the Platonic Representation Hypothesis) on model similarity.

Result: Both dataset and task overlap positively correlate with higher representational similarity, with their combination showing the strongest effect.

Conclusion: Dataset and task overlap are significant causal factors for model similarity, supporting the Platonic Representation Hypothesis and highlighting the role of shared training data.

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [578] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/pdf/2505.13900)
*Zhanpeng Zhou, Yongyi Yang, Mahito Sugiyama, Junchi Yan*

Main category: cs.LG

TL;DR: The paper explores the two-phase nature of deep learning dynamics, identifying the Chaos Effect (early sensitivity to perturbations) and the Cone Effect (later confinement of functional trajectories).


<details>
  <summary>Details</summary>
Motivation: To better understand the distinct phase transitions in deep neural network training dynamics.

Method: Introduces an interval-wise perspective, analyzing network states over time, including parameter perturbations and tracking the empirical Neural Tangent Kernel (eNTK).

Result: Reveals two phenomena: the Chaos Effect (early chaotic sensitivity) and the Cone Effect (later stable confinement of trajectories).

Conclusion: Provides a structural view of deep learning dynamics, showing a transition from exploration to refinement.

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [579] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/pdf/2505.13904)
*Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang*

Main category: cs.LG

TL;DR: L2C-Insert is a novel learning-based method for Neural Combinatorial Optimisation (NCO) that uses an insertion-based paradigm to improve solution quality in Vehicle Routing Problems (VRPs).


<details>
  <summary>Details</summary>
Motivation: Existing constructive NCO methods follow a rigid appending-based approach, leading to suboptimal results. The paper explores an insertion-based paradigm for better flexibility and performance.

Method: L2C-Insert introduces a model architecture for insertion position prediction, an efficient training scheme, and an advanced inference technique.

Result: Experiments on TSP and CVRP show L2C-Insert outperforms existing methods across various problem sizes.

Conclusion: The insertion-based paradigm in L2C-Insert enhances flexibility and solution quality, making it superior for NCO in VRPs.

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [580] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/pdf/2505.13907)
*Junyu Luo, Yusheng Zhao, Xiao Luo, Zhiping Xiao, Wei Ju, Li Shen, Dacheng Tao, Ming Zhang*

Main category: cs.LG

TL;DR: Proposes COUPLE, a method for unsupervised efficient domain adaptive retrieval using graph diffusion and progressive alignment to handle noise and improve performance.


<details>
  <summary>Details</summary>
Motivation: Addresses noise in the target domain and suboptimal retrieval performance in existing methods by leveraging graph diffusion and progressive alignment.

Method: Constructs a cross-domain relationship graph, uses noise-robust graph flow diffusion, and employs hierarchical Mixup for progressive alignment.

Result: Demonstrates effectiveness on competitive benchmarks.

Conclusion: COUPLE enables effective domain adaptive hash learning by reducing noise impact and improving alignment.

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [581] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/pdf/2505.13910)
*Guangtao Zheng, Wenqian Ye, Aidong Zhang*

Main category: cs.LG

TL;DR: A framework called ShortcutProbe mitigates spurious bias in deep learning models without needing group labels, improving robustness.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often rely on spurious correlations, degrading performance on data lacking these correlations. Existing methods require costly group labels.

Method: ShortcutProbe identifies prediction shortcuts in the model's latent space and retrains the model to be invariant to them.

Result: The framework is theoretically and empirically effective in improving model robustness across diverse datasets.

Conclusion: ShortcutProbe offers a practical, label-free solution for mitigating spurious bias in deep learning models.

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [582] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/pdf/2505.13934)
*Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long*

Main category: cs.LG

TL;DR: RLVR-World uses reinforcement learning with verifiable rewards to optimize world models for task-specific metrics, outperforming MLE-based methods in language and video domains.


<details>
  <summary>Details</summary>
Motivation: Standard training objectives like MLE misalign with task-specific goals of world models, such as accuracy or perceptual quality.

Method: RLVR-World leverages reinforcement learning with verifiable rewards to directly optimize world models for transition prediction metrics.

Result: Substantial performance gains in text games, web navigation, and robot manipulation tasks.

Conclusion: RLVR is a promising post-training paradigm for enhancing generative models beyond reasoning language models.

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [583] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/pdf/2505.13938)
*Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzche, Greg Durrett, Yisong Yue, Swarat Chaudhuri*

Main category: cs.LG

TL;DR: ${\rm C{\small LEVER}}$ is a benchmark for verified code generation in Lean, featuring 161 problems with verified specifications and implementations, avoiding common pitfalls like test-case supervision and LLM-generated annotations.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality benchmark for end-to-end verified code generation, addressing limitations of prior benchmarks by ensuring machine-checkable correctness and avoiding leaks or vacuous solutions.

Method: Curated 161 problems with two tasks: generating specifications matching ground-truth and Lean implementations satisfying them, verified using Lean's type checker.

Result: State-of-the-art language models struggle to achieve full verification, highlighting the benchmark's challenge.

Conclusion: ${\rm C{\small LEVER}}$ is a challenging frontier benchmark for program synthesis and formal reasoning, with publicly available resources.

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [584] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/pdf/2505.13954)
*Jiahe Chen, Ziye Ma*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [585] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/pdf/2505.13989)
*Yanzhe Wen, Xunkai Li, Qi Zhang, Zhu Lei, Guang Zeng, Rong-Hua Li, Guoren Wang*

Main category: cs.LG

TL;DR: OGA is an LLM-based framework for TAG learning, addressing data uncertainty in open-world scenarios by combining adaptive label traceability and a graph label annotator.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle data uncertainty in open-world TAG learning, particularly with limited labeling and unknown-class nodes.

Method: OGA integrates adaptive label traceability (semantics + topology) for unknown-class rejection and includes a graph label annotator for model updates.

Result: Experiments show OGA is effective and practical.

Conclusion: OGA successfully addresses limitations in open-world TAG learning by leveraging LLMs and adaptive techniques.

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [586] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/pdf/2505.14005)
*Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam*

Main category: cs.LG

TL;DR: OPEN is a novel explainer for GNNs that addresses limitations of existing methods by partitioning the dataset into environments and learning decision logic without strict prerequisites.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and transparency of GNNs by overcoming the limitations of current explainability methods, which fail to capture complete decision logic and require strict prerequisites.

Method: OPEN partitions the dataset into environments with distinct distributions, samples subgraphs from each, and analyzes GNN predictions to learn decision logic without prerequisites.

Result: OPEN captures nearly complete decision logic, outperforms state-of-the-art methods in fidelity, maintains efficiency, and enhances robustness.

Conclusion: OPEN provides a comprehensive and prerequisite-free solution for explaining GNNs, advancing the field of XGNN.

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [587] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/pdf/2505.14011)
*Yifei Jin, Xin Zheng, Lei Guo*

Main category: cs.LG

TL;DR: The paper introduces a Saturated Mechanistic Sentencing (SMS) model for interpretable judicial sentencing prediction, backed by a novel MLMS algorithm and theoretical guarantees, validated on real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing models lack interpretability, a critical need for judicial practice. This work addresses this gap by grounding the model in legal principles.

Method: Proposes the SMS model based on China's Criminal Law, introduces the MLMS adaptive algorithm, and provides theoretical bounds for prediction accuracy.

Result: Experiments on the CIBH dataset show prediction accuracy close to the theoretical upper bound, validating the model and algorithm.

Conclusion: The SMS model and MLMS algorithm offer interpretable and accurate sentencing prediction, meeting judicial needs.

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [588] [Adversarial Training from Mean Field Perspective](https://arxiv.org/pdf/2505.14021)
*Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: The paper provides a theoretical analysis of adversarial training in random deep neural networks using mean field theory, deriving tight bounds for adversarial loss and showing the impact of network architecture and dimensions.


<details>
  <summary>Details</summary>
Motivation: To better understand the training dynamics of adversarial training in deep neural networks without relying on data distribution assumptions.

Method: Introduces a mean field theory-based framework to analyze adversarial training, deriving upper bounds for adversarial loss and examining network properties like shortcuts and width.

Result: Proves networks without shortcuts are not adversarially trainable, adversarial training reduces capacity, and wider networks mitigate these issues. Also explores impacts of input/output dimensions.

Conclusion: The study offers theoretical insights into adversarial training dynamics, highlighting the roles of network architecture and dimensions in robustness.

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [589] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/pdf/2505.14024)
*Di Wu, Qian Li, Heng Yang, Yong Han*

Main category: cs.LG

TL;DR: FedGraM is a robust aggregation method for Federated Learning that detects and mitigates untargeted attacks by analyzing inter-class separation in embeddings, outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: FL is vulnerable to untargeted attacks degrading model performance, and existing defenses struggle with data heterogeneity. The paper aims to detect and remove such attacks effectively.

Method: Proposes FedGraM, which uses an auxiliary dataset to extract embeddings from local models, calculates Gram Matrix norms to measure inter-class separation, and filters malicious models before aggregation.

Result: FedGraM outperforms state-of-the-art defenses, especially with limited auxiliary data, by effectively identifying and removing malicious models.

Conclusion: FedGraM provides a practical and effective solution to defend against untargeted attacks in FL, leveraging inter-class separation metrics for robust aggregation.

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [590] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/pdf/2505.06699)
*Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang*

Main category: cs.LG

TL;DR: The paper formalizes 'model steering,' a learning paradigm using a trained model to guide another model's training. It introduces a theory-driven framework (DRRho risk minimization) for better generalization and data efficiency, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Existing ad-hoc methods for model steering lack theoretical understanding, leading to sub-optimal performance. The paper aims to provide a principled approach.

Method: Proposes DRRho risk minimization, a framework based on Distributionally Robust Optimization (DRO), and applies it to Contrastive Language-Image Pretraining (CLIP) as DRRho-CLIP.

Result: Theoretical insights confirm improved generalization and data efficiency. Experiments show superior scaling and performance over heuristic methods.

Conclusion: The work advances understanding and practice of model steering with a theory-driven framework, validated by empirical success.

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [591] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/pdf/2505.14033)
*Guoming Li, Jian Yang, Yifan Chen*

Main category: cs.LG

TL;DR: The paper introduces Coarsening-guided Partition-wise Filtering (CPF), a unified framework combining graph-wise and node-wise filtering for GNNs, addressing limitations of existing methods and improving adaptability for heterophilic graphs.


<details>
  <summary>Details</summary>
Motivation: Existing GNN filtering methods either use uniform graph-wise filters (ineffective for heterophilic graphs) or node-wise filters (risk of overfitting). A unified framework is needed to balance adaptability and theoretical insights.

Method: CPF performs filtering on node partitions: structure-aware filtering via graph coarsening, followed by feature-aware filtering using k-means clustering. This hybrid approach balances adaptability and efficiency.

Result: CPF outperforms existing paradigms in benchmark node classification and real-world graph anomaly detection, demonstrating its efficacy and practical utility.

Conclusion: CPF provides a comprehensive solution for GNN filtering, unifying graph-wise and node-wise approaches while mitigating overfitting risks, validated by theoretical and empirical results.

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [592] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/pdf/2505.14036)
*Gyubin Lee, Truong Nhat Nguyen Bao, Jaesik Yoon, Dongwoo Lee, Minsu Kim, Yoshua Bengio, Sungjin Ahn*

Main category: cs.LG

TL;DR: ABCD introduces adaptive inference-time scaling for diffusion models, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fixed denoising schedules limit adaptive computation allocation based on task demands.

Method: ABCD uses bi-directional diffusion cycles, exploration-exploitation balancing, and adaptive thinking time.

Result: ABCD enhances performance across tasks while maintaining computational efficiency.

Conclusion: ABCD offers a flexible, search-based framework for adaptive inference in diffusion models.

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [593] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/pdf/2505.14039)
*Luca Pellegrini, Massimiliano Ghiotto, Edoardo Centofanti, Luca Franco Pavarino*

Main category: cs.LG

TL;DR: Fourier Neural Operators effectively learn high-dimensional ionic model dynamics, achieving accuracy across models like FitzHugh-Nagumo, Hodgkin-Huxley, and O'Hara-Rudy.


<details>
  <summary>Details</summary>
Motivation: To extend previous low-dimensional predictions of ionic models by exploring Fourier Neural Operators' ability to learn all state variables in higher dimensions.

Method: Used Fourier Neural Operators to learn dynamics of three ionic models, with hyperparameter tuning in constrained and unconstrained settings.

Result: Both architectures achieved comparable accuracy, but unconstrained ones trained faster.

Conclusion: Fourier Neural Operators can accurately capture complex multiscale dynamics in high-dimensional systems.

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [594] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/pdf/2505.14040)
*Jingyun Zhang, Hao Peng, Li Sun, Guanlin Wu, Chunyang Liu, Zhengtao Yu*

Main category: cs.LG

TL;DR: DeSE introduces a novel unsupervised graph clustering framework using Deep Structural Entropy to address limitations of existing methods like GNNs and GATs, improving performance on sparse or noisy graphs.


<details>
  <summary>Details</summary>
Motivation: Current graph clustering methods rely heavily on original graph structures, which may be sparse or noisy, and traditional techniques like k-means may not fully capture node relationships.

Method: DeSE calculates structural entropy with soft assignment, uses a Structural Learning Layer (SLL) to enhance the graph, and employs a GNN-based clustering assignment method (ASS) for stable clustering.

Result: DeSE outperforms eight baselines on four benchmark datasets, showing superior effectiveness and interpretability.

Conclusion: DeSE effectively mitigates issues of sparse or noisy graphs and provides a robust framework for unsupervised graph clustering.

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [595] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/pdf/2505.14042)
*Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: Adversarially pretrained transformers can generalize robustly to unseen tasks without additional training, eliminating the need for adversarial training in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To reduce the high computational cost of adversarial training while maintaining robustness.

Method: Adversarially pretrain transformers on diverse tasks and use in-context learning for generalization without parameter updates.

Result: The model robustly generalizes to unseen tasks, focusing on robust features and resisting attacks. Limitations include unrealistic conditions for universal robustness and an accuracy-robustness trade-off.

Conclusion: Adversarially pretrained transformers are effective foundation models for robustness, but limitations exist under certain conditions.

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [596] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/pdf/2505.14044)
*Luyao Tang, Kunze Huang, Chaoqi Chen, Cheng Chen*

Main category: cs.LG

TL;DR: MTMC improves GCD by maximizing manifold capacity of class tokens, enhancing clustering accuracy and semantic feature capture.


<details>
  <summary>Details</summary>
Motivation: Traditional GCD methods sacrifice manifold capacity, limiting intra-class representation richness. MTMC addresses this by preserving data diversity.

Method: MTMC uses the nuclear norm of singular values to measure and maximize manifold capacity, ensuring informative and structured representations.

Result: MTMC outperforms existing GCD methods, improving clustering accuracy, category estimation, and reducing dimensional collapse.

Conclusion: MTMC is a robust solution for open-world learning, offering better inter-class separability and richer representations.

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [597] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/pdf/2505.14071)
*Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger*

Main category: cs.LG

TL;DR: Text-derived steering vectors improve multimodal accuracy in MLLMs using methods like sparse autoencoders, mean shift, and linear probing.


<details>
  <summary>Details</summary>
Motivation: Address the lack of steering techniques for MLLMs compared to LLMs, leveraging text-derived vectors to enhance performance.

Method: Uses sparse autoencoders, mean shift, and linear probing to derive steering vectors from text-only LLM backbones.

Result: Text-derived steering boosts accuracy, with mean shift improving spatial relationship accuracy by +7.3% and counting accuracy by +3.3%.

Conclusion: Textual steering vectors efficiently enhance MLLM performance with minimal overhead, demonstrating strong generalization.

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [598] [Collaborative Unlabeled Data Optimization](https://arxiv.org/pdf/2505.14117)
*Xinyi Shang, Peng Sun, Fengyuan Liu, Tao Lin*

Main category: cs.LG

TL;DR: A novel data-centric framework, CoOpt, optimizes unlabeled data to enhance deep learning efficiency and sustainability, outperforming model-centric approaches with significant improvements in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in model-centric approaches where knowledge is locked in model parameters, limiting reusability and scalability.

Method: Proposes CoOpt, a parallelized framework for collaborative unlabeled data optimization, leveraging task-agnostic models to encode knowledge into data.

Result: Achieves 13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, with training speedups of 1.94× and 1.2×.

Conclusion: CoOpt offers a scalable, reusable, and sustainable solution for deep learning training by optimizing data directly.

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [599] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/pdf/2505.14122)
*Ehsan Masoudian, Ali Mirzaei, Hossein Bagheri*

Main category: cs.LG

TL;DR: The study examines wildfire risk in Iran, combining climatic and human factors using remote sensing, GIS, and machine learning. Findings show human activities, like population density, outweigh climatic factors in seasonal wildfire susceptibility.


<details>
  <summary>Details</summary>
Motivation: To understand wildfire risks in Iran by analyzing the combined impact of climatic conditions and human activities.

Method: Used remote sensing, GIS, cloud computing, and machine learning to assess wildfire susceptibility, with multiple data-driven scenarios.

Result: Human activities (e.g., population density) had a stronger seasonal impact than climatic factors. High-risk areas were identified, such as the Zagros region.

Conclusion: The study highlights the need for targeted fire management strategies in high-risk zones, emphasizing human factors in wildfire dynamics.

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [600] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/pdf/2505.14125)
*Viet Anh Khoa Tran, Emre Neftci, Willem. A. M. Wybo*

Main category: cs.LG

TL;DR: The paper introduces task-modulated contrastive learning (TMCL), inspired by the neocortex, to improve continual learning in machines by balancing stability and plasticity with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Biological brains learn continually and generalize well, while machine learning suffers from catastrophic forgetting. The goal is to mimic biological learning to enhance machine learning robustness.

Method: TMCL uses predictive coding and contrastive loss to create view-invariant representations. It learns affine modulations for new classes without altering feedforward weights, ensuring stability.

Result: TMCL outperforms state-of-the-art unsupervised and supervised methods in class-incremental and transfer learning, even with only 1% labeled data.

Conclusion: Top-down modulations are key to balancing stability and plasticity in learning, as demonstrated by TMCL's success.

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [601] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/pdf/2505.14128)
*Yihang Du, Jiaying Hu, Suyang Hou, Yueyang Ding, Xiaobo Sun*

Main category: cs.LG

TL;DR: Proposes SLAM, a framework for measuring spatial labeling similarity by transforming labelings into graphs and computing distributional discrepancies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for measuring spatial labeling similarity often overlook label agreement, topology, and mismatched label impacts, necessitating a more comprehensive approach.

Method: Transforms spatial labelings into graphs based on location organization, labels, and attributes, then computes distributional discrepancy to measure dissimilarity.

Result: SLAM provides a comprehensive and accurate reflection of labeling quality in spatial transcriptomics, validated by experiments on simulated and real data.

Conclusion: SLAM addresses gaps in existing methods, offering a robust tool for evaluating spatial labeling similarity, with potential applications in spatial transcriptomics.

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [602] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/pdf/2505.14136)
*Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, Andreas Krause*

Main category: cs.LG

TL;DR: TTMM scales MoE models with more experts efficiently, approximating TTT's benefits without its high test-time cost.


<details>
  <summary>Details</summary>
Motivation: Current MoE models are limited by high training and inference costs, restricting the number of experts. TTMM aims to overcome this by enabling more experts with minimal overhead.

Method: TTMM uses model merging to scale MoE models to more experts, avoiding test-time overhead. It approximates TTT, which fine-tunes experts per task.

Result: TTMM performance improves with more experts, nearing TTT's results. It is 100x faster than TTT for a 1B parameter model.

Conclusion: TTMM provides a cost-effective way to scale test-time training, balancing performance and efficiency.

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [603] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/pdf/2505.14139)
*Marvin Alles, Nutan Chen, Patrick van der Smagt, Botond Cseke*

Main category: cs.LG

TL;DR: The paper introduces energy-guided flow matching, a method to enhance flow model training and eliminate inference-time guidance, applied in offline reinforcement learning as FlowQ.


<details>
  <summary>Details</summary>
Motivation: Current methods for incorporating guidance in diffusion models focus on inference, leaving training underexplored. The work aims to improve training efficiency and performance.

Method: Proposes energy-guided flow matching, approximating an energy-guided probability path as a Gaussian path to learn a conditional velocity field. Applied in offline RL as FlowQ.

Result: FlowQ achieves competitive performance with constant policy training time relative to flow sampling steps.

Conclusion: Energy-guided flow matching is effective for tasks like RL, offering improved training efficiency and eliminating the need for inference-time guidance.

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [604] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/pdf/2505.14161)
*Ting Wei, Biao Mei, Junliang Lyu, Renquan Zhang, Feng Zhou, Yifan Sun*

Main category: cs.LG

TL;DR: FedWBA improves Bayesian federated learning with nonparametric local inference and Wasserstein barycenter aggregation, outperforming baselines in accuracy and uncertainty calibration.


<details>
  <summary>Details</summary>
Motivation: Existing PBFL methods have restrictive parametric assumptions and naive parameter averaging, limiting performance.

Method: FedWBA uses particle-based variational inference locally and Wasserstein barycenter aggregation globally.

Result: Theoretical convergence guarantees and empirical improvements in accuracy, calibration, and convergence rate.

Conclusion: FedWBA is a robust and effective solution for personalized Bayesian federated learning.

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [605] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/pdf/2505.14170)
*Chen Zhang, Weixin Bu, Zeyi Ren, Zhengwu Liu, Yik-Chung Wu, Ngai Wong*

Main category: cs.LG

TL;DR: GraNT (Graph Neural Teaching) improves GCN training efficiency by selecting graph-property pairs, reducing training time significantly without losing performance.


<details>
  <summary>Details</summary>
Motivation: Traditional GCN training is costly; GraNT aims to enhance efficiency by leveraging nonparametric teaching.

Method: GraNT selects subsets of graph-property pairs to guide GCN training, using functional gradient descent for optimization.

Result: Training time reduced by 36.62% (graph regression), 38.19% (graph classification), 30.97% (node regression), and 47.30% (node classification).

Conclusion: GraNT aligns teaching with nonparametric learning, boosting GCN efficiency while maintaining generalization.

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [606] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/pdf/2505.14185)
*Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma*

Main category: cs.LG

TL;DR: The paper investigates whether safety alignment in LLMs is geometrically localized in subspaces, finding no evidence of selective safety subspaces and suggesting entangled, high-impact components govern alignment.


<details>
  <summary>Details</summary>
Motivation: To understand if safety alignment in LLMs can be isolated in specific geometric subspaces to defend against misalignment.

Method: Empirical study examining subspaces in parameter and activation space across five open-source LLMs.

Result: Safety and unsafe behaviors are amplified in overlapping subspaces, with no distinct safety-governing subspace found.

Conclusion: Alignment is not geometrically localized; alternative strategies beyond subspace-based defenses are needed to preserve safety.

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [607] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/pdf/2505.14190)
*Ni Ding, Miao Qiao, Jiaxing Xu, Yiping Ke, Xiaoyu Zhang*

Main category: cs.LG

TL;DR: The paper introduces α-GAN, a GAN variant using Rényi measures, which generalizes vanilla GANs and offers faster convergence for α ∈ (0,1).


<details>
  <summary>Details</summary>
Motivation: To address common GAN issues like vanishing gradients by leveraging Rényi measures, which provide a flexible framework for optimization.

Method: Formulates a value function using Rényi cross-entropy, creating a min-max problem where the discriminator maximizes certainty and the generator minimizes it. The solution is parameterized by the Rényi order α.

Result: Faster convergence is achieved for α ∈ (0,1) due to exponentially enlarged gradients. The method reduces to vanilla GANs when α = 1.

Conclusion: α-GAN offers a promising approach to improve GAN training, particularly for α ∈ (0,1), a range underexplored in existing Rényi-based GANs.

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [608] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/pdf/2505.14201)
*Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: FLASH-D simplifies FlashAttention's kernel, improving efficiency and reducing hardware costs without performance loss.


<details>
  <summary>Details</summary>
Motivation: The transformer's attention mechanism is computationally intensive due to softmax rescaling and matrix operations. FlashAttention improved this but still had room for optimization.

Method: FLASH-D re-evaluates FlashAttention, hiding softmax division, ensuring numerical stability, and reducing computational costs while preserving tiled implementation properties.

Result: FLASH-D reduces hardware area by 22.8% and power by 20.3% at 28nm, with no performance penalty.

Conclusion: FLASH-D offers a more efficient and hardware-friendly alternative to FlashAttention, maintaining performance while reducing costs.

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [609] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/pdf/2505.14202)
*Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao*

Main category: cs.LG

TL;DR: MSDformer introduces a multi-scale DTM-based method for time series generation, addressing limitations in capturing multi-scale patterns and lacking theoretical foundations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing DTM approaches fail to capture multi-scale temporal patterns and lack theoretical guidance for optimization.

Method: MSDformer uses a multi-scale tokenizer and autoregressive modeling to learn and capture multi-scale patterns in time series data, validated by the rate-distortion theorem.

Result: MSDformer outperforms state-of-the-art methods, showing improved quality in generated time series.

Conclusion: Incorporating multi-scale information and patterns enhances DTM-based time series generation, supported by theory and experiments.

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [610] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/pdf/2505.14206)
*Flavio Di Martino, Franca Delmastro*

Main category: cs.LG

TL;DR: The paper evaluates generative models for synthetic time series data in mHealth, highlighting their limitations in multi-modality, long-range dependencies, and conditional generation, and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and privacy issues in mHealth due to ethical and privacy constraints by improving synthetic data generation.

Method: Systematic evaluation of state-of-the-art generative models (e.g., GANs, Diffusion Models) using a novel framework to assess intrinsic quality and downstream utility.

Result: Existing models struggle with cross-modal consistency, temporal coherence, and performance in real-world scenarios like data augmentation.

Conclusion: Future research is needed to enhance synthetic time series generation for better applicability in mHealth.

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [611] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/pdf/2505.14211)
*Qu Wang, Yan Xia*

Main category: cs.LG

TL;DR: The paper introduces PTWD, a PID-controlled tensor wheel decomposition model, to improve link prediction in dynamic networks by combining TWD's topological features with PID control for stable parameter learning.


<details>
  <summary>Details</summary>
Motivation: Traditional static network methods fail to capture temporal dependencies and weight dynamics in dynamic networks, necessitating advanced models like PTWD.

Method: PTWD integrates tensor wheel decomposition (TWD) to model dynamic network features and PID control for stable parameter optimization.

Result: PTWD outperforms other models in link prediction accuracy on four real datasets.

Conclusion: PTWD effectively combines TWD's structural insights with PID control, enhancing dynamic network link prediction.

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [612] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/pdf/2505.14214)
*Mattes Mollenhauer, Nicole Mücke, Dimitri Meunier, Arthur Gretton*

Main category: cs.LG

TL;DR: The paper analyzes ridge regression in reproducing kernel Hilbert spaces under heavy-tailed noise, showing optimal convergence rates and robustness.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of ridge regression performance beyond subexponential noise, addressing heavy-tailed noise with finite higher moments.

Method: Uses the integral operator framework and a Fuk-Nagaev inequality for Hilbert-space valued random variables to derive risk bounds.

Result: Achieves optimal convergence rates under heavy-tailed noise, matching those previously only possible with subexponential noise.

Conclusion: Demonstrates the asymptotic robustness of regularized least squares against heavy-tailed noise, expanding its applicability.

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [613] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/pdf/2505.14217)
*Jorge Fabila, Lidia Garrucho, Víctor M. Campello, Carlos Martín-Isla, Karim Lekadir*

Main category: cs.LG

TL;DR: Federated Learning (FL) is explored for TB diagnosis in Africa, showing promise but facing infrastructure and regulatory challenges.


<details>
  <summary>Details</summary>
Motivation: To address privacy and data scarcity issues in TB diagnosis using AI in low-resource settings.

Method: Collaborative training of AI models across hospitals in eight African countries, comparing local and federated models.

Result: FL is feasible but hindered by poor infrastructure, unreliable internet, and data control concerns.

Conclusion: FL has potential for AI-driven healthcare in underserved regions but needs better infrastructure and regulatory support.

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [614] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/pdf/2505.14629)
*Fnu Mohbat, Mohammed J Zaki*

Main category: cs.LG

TL;DR: KERL integrates food Knowledge Graphs (KGs) with LLMs to provide personalized food recommendations, recipe generation, and nutritional analysis, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on combining food-related KGs with LLMs for enhanced food understanding and personalized recommendations.

Method: KERL extracts entities from natural language questions, retrieves KG subgraphs, and uses LLMs to generate recipes and nutritional info.

Result: The system outperforms existing approaches, offering a complete solution for food-related tasks.

Conclusion: KERL provides a unified, effective solution for food recommendation and analysis, with publicly available code and datasets.

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [615] [Fast and close Shannon entropy approximation](https://arxiv.org/pdf/2505.14234)
*Illia Horenko, Davide Bassetti, Lukáš Pospíšil*

Main category: cs.LG

TL;DR: The paper introduces Fast Entropy Approximation (FEA), a non-singular rational approximation of Shannon entropy, offering faster computation, lower error, and improved robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Shannon entropy computations are costly and suffer from singularity issues in gradients, leading to slow convergence and low robustness in tools across physics, ML, and quantum computing.

Method: Proposes FEA, a rational approximation of Shannon entropy and its gradient, requiring fewer operations (5-6) and achieving lower error (10^-3).

Result: FEA is 50% faster, reduces error by 20x, and improves model quality in ML benchmarks, enabling 2-3 orders of magnitude faster feature extraction.

Conclusion: FEA significantly enhances computational efficiency and robustness in entropy-based tools, making it a valuable advancement for AI and related fields.

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [616] [Learning with Local Search MCMC Layers](https://arxiv.org/pdf/2505.14240)
*Germain Vivier-Ardisson, Mathieu Blondel, Axel Parmentier*

Main category: cs.LG

TL;DR: The paper introduces a method to integrate inexact combinatorial solvers into neural networks using MCMC, reducing computational costs while maintaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing approaches lack theoretical guarantees or fail with inexact solvers, limiting their use for NP-hard problems.

Method: Transforms local search heuristics into MCMC proposal distributions, creating differentiable combinatorial layers.

Result: Demonstrated effectiveness on a large-scale dynamic vehicle routing problem with time windows.

Conclusion: The approach reduces computational burden and provides a principled way to learn with inexact solvers.

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [617] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/pdf/2505.14251)
*Bar Mahpud, Or Sheffet*

Main category: cs.LG

TL;DR: A new algorithm for differentially private second moment estimation achieves strong privacy-utility trade-offs under subsamplability assumptions, even for worst-case inputs and outliers.


<details>
  <summary>Details</summary>
Motivation: The problem of estimating second moments with differential privacy is challenging, especially for worst-case inputs and in the presence of outliers. This work aims to provide a robust solution under subsamplability conditions.

Method: The algorithm uses a recursive framework inspired by Kamath et al. (2019) and leverages subsamplability assumptions to ensure zero-Concentrated Differential Privacy (zCDP). It preserves spectral structure accuracy up to a factor of $(1\pm\gamma)$ with high probability.

Result: The algorithm achieves accurate second moment estimation under zCDP, even when a noticeable fraction of the input consists of outliers.

Conclusion: The proposed method offers a practical and privacy-preserving solution for second moment estimation, robust to adversarial inputs and outliers.

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [618] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.14252)
*Mouad Elaarabi, Domenico Borzacchiello, Philippe Le Bot, Nathan Lauzeral, Sebastien Comas-Cardona*

Main category: cs.LG

TL;DR: The paper introduces a method combining Sequence Encoding with Physics-Informed Neural Networks (PINNs) to enable real-time adaptation to varying parameters, boundary conditions, and initial conditions without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing methods using PINNs with Sparse Regression require retraining for parameter changes, limiting real-time applicability. This work aims to overcome this limitation.

Method: The proposed architecture uses Deep Sets or Sequence Encoders to encode dynamic parameters, boundary conditions, and initial conditions, feeding these into PINNs for adaptability.

Result: Tested on Rossler ODE, 2D Navier-Stokes PDE, and 1D heat monitoring, the model showed robustness to noise, generalization, and accurate parameter identification.

Conclusion: The approach successfully enables real-time adaptation in dynamical systems, demonstrating versatility across diverse problems.

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [619] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/pdf/2505.14264)
*Jian Xiong, Jingbo Zhou, Jingyong Ye, Dejing Dou*

Main category: cs.LG

TL;DR: AAPO, a new RL algorithm, improves training efficiency in LLMs by enhancing advantage estimation with momentum, outperforming existing methods like GRPO and PPO.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods like GRPO, while simplifying training by eliminating value models, suffer inefficiencies when advantage estimates near zero.

Method: AAPO optimizes cross-entropy loss using momentum-enhanced advantage estimation to address inefficiencies in group relative advantage methods.

Result: AAPO shows superior performance on mathematical reasoning benchmarks compared to GRPO and PPO.

Conclusion: AAPO effectively mitigates training inefficiencies in RL for LLMs, offering a robust alternative to existing methods.

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [620] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/pdf/2505.14273)
*Hiroki Shiraishi, Hisao Ishibuchi, Masaya Nakata*

Main category: cs.LG

TL;DR: X-KAN, a novel method combining Kolmogorov-Arnold Networks (KANs) with XCSF, outperforms existing methods in approximating locally complex or discontinuous functions.


<details>
  <summary>Details</summary>
Motivation: Existing neural networks struggle with locally complex or discontinuous functions due to reliance on a single global model.

Method: X-KAN optimizes multiple local KANs using XCSF, defining local regions via rule antecedents and implementing KANs as rule consequents.

Result: X-KAN outperforms XCSF, Multi-Layer Perceptron, and KAN in accuracy, handling complex functions with a compact rule set (average 7.2 ± 2.3 rules).

Conclusion: X-KAN validates the effectiveness of KAN as a local model in XCSF, balancing accuracy and generality.

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [621] [Scaling Law for Quantization-Aware Training](https://arxiv.org/pdf/2505.14302)
*Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo*

Main category: cs.LG

TL;DR: The paper proposes a unified scaling law for quantization-aware training (QAT) at 4-bit precision (W4A4), analyzing quantization error trends and identifying activation outliers as a bottleneck.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of understanding in QAT scaling behavior, especially at W4A4, by considering factors like training tokens and quantization granularity.

Method: Conducts 268 QAT experiments to model quantization error as a function of model size, training data, and quantization group size, decomposing error into weight and activation components.

Result: Quantization error decreases with larger models but increases with more training tokens and coarser granularity. Activation outliers in the FC2 layer are the primary bottleneck.

Conclusion: Mixed-precision quantization can balance weight and activation errors, with weight error becoming more critical with increased training data, guiding future QAT improvements.

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [622] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/pdf/2505.14312)
*Kyungeun Lee, Moonjung Eo, Hye-Seung Cho, Dongmin Kim, Ye Seul Sim, Seoyoon Kim, Min-Kook Suh, Woohyung Lim*

Main category: cs.LG

TL;DR: MultiTab is a benchmark suite for evaluating tabular learning algorithms across diverse data regimes, revealing performance variations based on data characteristics like sample size and feature interaction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks rely on average-case metrics, failing to capture model behavior across different data regimes. MultiTab addresses this gap.

Method: MultiTab categorizes 196 datasets by key characteristics (e.g., sample size, label imbalance) and evaluates 13 models with varied inductive biases.

Result: Model performance varies significantly by data regime; e.g., similarity-based models excel with large samples, while dependency-encoding models perform better with weakly correlated features.

Conclusion: MultiTab highlights the importance of regime-aware evaluation for understanding model behavior and offers guidance for model selection tailored to data characteristics.

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [623] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/pdf/2505.14338)
*Egor Bakaev, Florestan Brunck, Christoph Hertrich, Jack Stade, Amir Yehudayoff*

Main category: cs.LG

TL;DR: The paper disproves a conjecture about the optimal depth of ReLU networks for CPWL functions, showing fewer layers suffice.


<details>
  <summary>Details</summary>
Motivation: To address the conjecture by Hertrich et al. about the minimal depth required for ReLU networks to compute CPWL functions.

Method: Demonstrates that ReLU networks with fewer layers can compute CPWL functions, using geometric interpretations and polyhedral subdivisions.

Result: Shows that ⌈log₃(n−1)⌉+1 layers suffice for CPWL functions and ⌈log₃(n−2)⌉+1 layers for the maximum function.

Conclusion: The results nearly match known lower bounds, providing a tighter upper bound for ReLU network depth.

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [624] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/pdf/2505.14345)
*Aydin Abedinia, Shima Tabakhi, Vahid Seydi*

Main category: cs.LG

TL;DR: A semi-supervised framework using distance-based weighting to prioritize critical training samples, improving generalization and robustness in noisy or imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage both labeled and unlabeled data effectively, addressing challenges of limited labeled data and noisy/imbalanced datasets.

Method: Utilizes a distance-based weighting mechanism, uncertainty consistency, and graph-based representations to prioritize informative samples.

Result: Significant improvements in accuracy, precision, and recall on twelve benchmark datasets, outperforming existing methods.

Conclusion: Provides a robust and practical solution for semi-supervised learning, applicable in domains like healthcare and security.

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [625] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/pdf/2505.14352)
*Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda*

Main category: cs.LG

TL;DR: The paper explores methods to uncover hidden knowledge in language models, specifically a 'Taboo' model trained to hint at a secret word without stating it. It evaluates black-box and interpretability techniques, finding both effective.


<details>
  <summary>Details</summary>
Motivation: To ensure language models remain trustworthy and reliable by addressing concerns about deception or hidden knowledge.

Method: Train a Taboo model to hint at a secret word, then test black-box and mechanistic interpretability techniques (logit lens, sparse autoencoders) to uncover the secret.

Result: Both black-box and interpretability approaches successfully elicited the secret word.

Conclusion: The study highlights promising methods for uncovering hidden knowledge in models, suggesting future work on refining these techniques for more complex models to ensure safe deployment.

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [626] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/pdf/2505.14371)
*Anh Duc Nguyen, Ilia Markov, Frank Zhengqing Wu, Ali Ramezani-Kebrya, Kimon Antonakopoulos, Dan Alistarh, Volkan Cevher*

Main category: cs.LG

TL;DR: A layer-wise quantization framework adapts to neural network heterogeneity, improving training efficiency with a new QODA algorithm for distributed VIs, achieving 150% speedup in GAN training.


<details>
  <summary>Details</summary>
Motivation: Addressing the heterogeneity in deep neural networks' layers and its impact on predictions, the paper aims to improve training efficiency.

Method: Develops a layer-wise quantization framework with tight bounds and introduces the QODA algorithm for distributed variational inequalities with adaptive learning rates.

Result: QODA achieves up to 150% speedup in end-to-end training time for Wasserstein GAN on 12+ GPUs.

Conclusion: The proposed framework and QODA algorithm effectively handle layer heterogeneity and significantly enhance training efficiency.

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [627] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/pdf/2505.14388)
*Prasanna Parasurama, Panos Ipeirotis*

Main category: cs.LG

TL;DR: Enforcing gender-balanced shortlists doesn't ensure diverse final hires due to correlation between algorithmic and human criteria. A new approach diversifies shortlists by selecting overlooked candidates, improving hire diversity without compromising quality.


<details>
  <summary>Details</summary>
Motivation: To address the gap between equal representation in shortlists and actual diversity in final hires, despite the absence of bias in hiring stages.

Method: Theoretical analysis and empirical study of 800,000 job applications, proposing a complementary algorithmic approach to diversify shortlists.

Result: Equal shortlists yield limited diversity improvements when algorithmic and human criteria align. The proposed approach significantly enhances diversity without sacrificing hire quality.

Conclusion: Algorithmic design choices are critical for diversity goals; the proposed method offers actionable guidance for fair hiring practices.

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [628] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/pdf/2505.14407)
*Aniket Salvi, Gereon Weiss, Mario Trapp*

Main category: cs.LG

TL;DR: A fuzzy-based runtime monitor for ML perception components provides interpretable explanations and improves safety in autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Lack of human-interpretable explanations for ML errors hinders safety assurances in autonomous systems.

Method: A novel fuzzy-based monitor was developed and evaluated using naturalistic driving datasets.

Result: The monitor improved safety and identified reliable operating conditions for perception components.

Conclusion: The proposed monitor enhances interpretability and safety while maintaining system availability.

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [629] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/pdf/2505.14411)
*Leon Götz, Marcel Kollovieh, Stephan Günnemann, Leo Schwinn*

Main category: cs.LG

TL;DR: Proposes a pattern-centric tokenization method for time series, improving efficiency and performance by compressing data adaptively and introducing conditional decoding.


<details>
  <summary>Details</summary>
Motivation: Existing tokenization methods are inflexible, generating excessive tokens for simple patterns, leading to computational inefficiency.

Method: Uses a discrete vocabulary of frequent motifs to merge samples into tokens and introduces conditional decoding for optimization.

Result: Improves forecasting performance by 36%, efficiency by 1990%, and reduces MSE by up to 44%.

Conclusion: The method adapts to diverse patterns, generalizes well, and captures meaningful time series properties.

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [630] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/pdf/2505.14415)
*Myung Jun Kim, Félix Lefebvre, Gaëtan Brison, Alexandre Perez-Lebel, Gaël Varoquaux*

Main category: cs.LG

TL;DR: TARTE is a foundation model for tabular data that transforms tables into knowledge-enhanced vector representations, improving prediction accuracy and computation efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing table foundation models require fine-tuning, have high computation costs, and lack reusability, limiting their convenience and effectiveness.

Method: TARTE pre-trains on large relational data to create vector representations capturing table semantics via strings, enabling easy fine-tuning or integration with other models.

Result: TARTE achieves state-of-the-art prediction performance, enhances computation efficiency, and provides domain-specific representations.

Conclusion: TARTE offers an effective approach to knowledge pre-training for tabular learning, addressing key limitations of existing models.

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [631] [Explaining Neural Networks with Reasons](https://arxiv.org/pdf/2505.14424)
*Levin Hornischer, Hannes Leitgeb*

Main category: cs.LG

TL;DR: A new interpretability method for neural networks uses reason vectors to logically and probabilistically interpret neurons, addressing polysemanticity and offering scalability, faithfulness, and utility.


<details>
  <summary>Details</summary>
Motivation: To provide a philosophically grounded, uniform, and scalable method for interpreting neural networks that accounts for polysemanticity and meets practical interpretability needs.

Method: Computes reason vectors for neurons, combining logical and Bayesian perspectives to interpret their roles and groups, validated theoretically and empirically.

Result: The method is philosophically grounded, uniform, scalable, faithful, correct, trainable, and useful for improving robustness and fairness.

Conclusion: The proposed method effectively interprets neural networks by leveraging reason vectors, fulfilling key interpretability requirements.

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [632] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/pdf/2505.14428)
*Riccardo D'Elia*

Main category: cs.LG

TL;DR: Proposes a Neural System Dynamics framework to combine Deep Learning's predictive power with System Dynamics' interpretability, validated via real-world applications.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between Deep Learning (lacking interpretability) and System Dynamics (limited scalability) to enhance causal reliability and scalability.

Method: Integrates Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning into a Neural System Dynamics pipeline.

Result: Aims to achieve both predictive accuracy and interpretability, validated through the AutoMoTIF project.

Conclusion: Long-term goal is to support explainability and safety in autonomous systems.

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [633] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/pdf/2505.14451)
*Md Atik Ahamed, Qiang Ye, Qiang Cheng*

Main category: cs.LG

TL;DR: RefiDiff is a novel framework for imputing missing values in high-dimensional, mixed-type datasets, excelling in MNAR settings by combining local predictions with a Mamba-based denoising network.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to integrate local and global data characteristics effectively, especially under MNAR mechanisms, limiting performance in high-dimensional settings.

Method: RefiDiff uses pre-refinement for initial imputations and post-refinement for polishing results, encoding mixed-type data into unified tokens for robust imputation.

Result: RefiDiff outperforms SOTA methods in MNAR settings with 4x faster training than DDPM-based approaches, validated on nine real-world datasets.

Conclusion: RefiDiff is robust, scalable, and effective for handling complex missingness patterns in high-dimensional, mixed-type data.

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [634] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2505.14459)
*Kamal Singh, Sami Marouani, Ahmad Al Sheikh, Pham Tran Anh Quang, Amaury Habrard*

Main category: cs.LG

TL;DR: The paper proposes using Kolmogorov-Arnold Networks (KAN) for interpretable reinforcement learning in network control, specifically for load balancing. It combines a PPO agent with a KAN-based actor and MLP critic to improve performance and extract interpretable controller equations.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for network control lack interpretability and struggle to extract controller equations, limiting their practical application.

Method: Uses a PPO agent with a 1-layer KAN actor and MLP critic to learn load balancing policies, optimizing throughput, loss, and delay. Controller equations are extracted from the learned networks.

Result: The approach effectively improves network performance and provides interpretable policies, demonstrated through various reward functions.

Conclusion: KAN-based RL offers a promising solution for interpretable and effective network control, bridging the gap between performance and transparency.

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [635] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/pdf/2505.14463)
*Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, Ling Liu*

Main category: cs.LG

TL;DR: The paper explores the existence of an intrinsic adversarial resilience state in graph regimes, proposing a theoretical framework and dynamic system approach to identify it, outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: To address whether an intrinsic adversarial resilience state exists in graph regimes and how to identify it, given the rising attention on adversarial attacks in graph analytics.

Method: Models adversarial learning as a multi-object dynamic system, proposes a theoretical framework for resilience state existence, and develops a condensed function to pinpoint the critical state.

Result: Outperforms state-of-the-art defense methods across five real-world datasets and three representative attacks.

Conclusion: The proposed approach effectively identifies and leverages the critical adversarial resilience state, enhancing graph defense mechanisms.

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [636] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/pdf/2505.14468)
*Yifan Sui, Hao Wang, Hanfei Yu, Yitao Hu, Jianxun Li, Hao Wang*

Main category: cs.LG

TL;DR: ServerlessLoRA improves LoRA LLM serving by reducing redundancy, latency, and resource contention, cutting costs and improving performance.


<details>
  <summary>Details</summary>
Motivation: Current serverless systems inefficiently serve LoRA LLM inference due to parameter redundancy, high latency, and resource contention.

Method: ServerlessLoRA shares backbone LLM, pre-loads LoRA artifacts, and uses contention-aware batching and offloading.

Result: Reduces TTFT by 86% and costs by 89% compared to existing solutions.

Conclusion: ServerlessLoRA is a cost-effective and efficient solution for LoRA LLM serving.

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [637] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/pdf/2505.14477)
*Maria Panagiotou, Lorenzo Brigato, Vivien Streit, Amanda Hayoz, Stephan Proennecke, Stavros Athanasopoulos, Mikkel T. Olsen, Elizabeth J. den Brok, Cecilie H. Svensson, Konstantinos Makrilakis, Maria Xatzipsalti, Andriani Vazeou, Peter R. Mertens, Ulrik Pedersen-Bjergaard, Bastiaan E. de Galan, Stavroula Mougiakakou*

Main category: cs.LG

TL;DR: ABBA, a reinforcement learning-based insulin advisor, outperforms standard BBA in improving time-in-range for T1D and T2D patients.


<details>
  <summary>Details</summary>
Motivation: Adjusting insulin remains challenging for T1D and T2D patients despite advances. ABBA aims to personalize insulin recommendations for better glycemic control.

Method: ABBA uses reinforcement learning for personalized insulin adjustments, tested in-silico on 101 T1D and 101 T2D simulated adults.

Result: ABBA significantly improved TIR and reduced hypo-/hyperglycemia compared to BBA, with sustained improvement over two months.

Conclusion: ABBA shows promise for optimizing glycemic control and warrants human trials.

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [638] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/pdf/2505.14502)
*Wenze Liu, Xiangyu Yue*

Main category: cs.LG

TL;DR: The paper introduces secant losses, a method to balance performance and cost in diffusion model inference by learning ODE integration, achieving strong results with minimal steps.


<details>
  <summary>Details</summary>
Motivation: Current methods for accelerating diffusion model inference, like numerical solvers or distillation, are either inefficient at small steps or introduce instability.

Method: The proposed approach uses secant losses, derived from derivative-integral relationships, to fine-tune or distill pretrained diffusion models.

Result: The secant version of EDM achieves a 10-step FID of 2.14 on CIFAR-10, and SiT-XL/2 attains 4-step FID of 2.27 and 8-step FID of 1.96 on ImageNet-256.

Conclusion: Secant losses offer a stable and efficient way to enhance diffusion model inference with minimal computational steps.

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [639] [Latent Flow Transformer](https://arxiv.org/pdf/2505.14513)
*Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu*

Main category: cs.LG

TL;DR: The paper introduces Latent Flow Transformer (LFT), replacing discrete layers with a learned transport operator for efficiency, and Flow Walking (FW) to improve coupling preservation, showing superior performance over skipping layers.


<details>
  <summary>Details</summary>
Motivation: Discrete layers in transformers are inefficient; continuous layers (like in diffusion models) offer better performance. The goal is to bridge this gap for LLMs.

Method: Proposes LFT (replacing layer blocks with a transport operator via flow matching) and FW algorithm to enhance coupling preservation.

Result: LFT compresses layers effectively (6 of 24) and outperforms skipped layers (KL Divergence 0.407 vs. 0.529). FW further distills 12 layers into one (KL 0.736 vs. 0.932).

Conclusion: LFT and FW demonstrate feasibility of continuous layers in transformers, narrowing the gap between autoregressive and flow-based paradigms.

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [640] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/pdf/2505.14512)
*Juliusz Ziomek, George Whittle, Michael A. Osborne*

Main category: cs.LG

TL;DR: The paper analyzes how Neural Networks behave when extrapolating beyond training data, showing that adding a Layer Norm (LN) stabilizes outputs, unlike networks without LN.


<details>
  <summary>Details</summary>
Motivation: Understanding Neural Networks' extrapolation behavior is limited; this work aims to generalize results using NTK theory.

Method: Applies Neural Tangent Kernel (NTK) theory to infinitely-wide networks with and without LN, supported by empirical tests on finite-width networks.

Result: LN transforms NTK into a bounded-variance kernel, preventing extreme outputs far from training data, unlike LN-free networks.

Conclusion: LN enhances extrapolation stability, with practical benefits in protein residue prediction and age estimation from underrepresented facial images.

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [641] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/pdf/2505.14522)
*Mahmuda Akhter Nishu, Chenyu Huang, Milad Roohi, Xin Zhong*

Main category: cs.LG

TL;DR: A dual-stream learning framework integrates weather data and event narratives for localized wind hazard prediction, outperforming traditional methods and aiding underserved communities.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting systems lack community-specific vulnerability insights, limiting their effectiveness for localized risk assessment and resilience planning.

Method: An interpretable dual-stream learning framework combining Random Forest and RoBERTa-based transformer through late fusion, tailored for tribal communities.

Result: Significant performance gains over baselines, with insights into model decision-making for transparency and trust.

Conclusion: The framework enhances predictive effectiveness and practical value for emergency preparedness and community resilience.

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [642] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/pdf/2505.14533)
*Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou*

Main category: cs.LG

TL;DR: A novel Spike-Transformer Reinforcement Learning (STRL) algorithm combines SNNs' energy efficiency with RL's decision-making power, outperforming traditional Transformers in policy performance and energy use.


<details>
  <summary>Details</summary>
Motivation: Transformers in RL are computationally expensive and energy-intensive, limiting real-world deployment. SNNs offer a bio-inspired, energy-efficient alternative.

Method: Developed STRL with multi-step LIF neurons and attention mechanisms for spatio-temporal pattern processing, enhanced with state, action, and reward encodings.

Result: STRL achieves superior policy performance and energy efficiency compared to conventional Transformers in benchmarks.

Conclusion: STRL presents a promising bio-inspired, low-cost solution for complex real-world decision-making tasks.

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [643] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/pdf/2505.14531)
*Shaoye Luo, Xinxin Fan, Quanliang Jing, Chi Lin, Mengfan Li, Yunfeng Lu, Yongjun Xu*

Main category: cs.LG

TL;DR: A model-agnostic trigger-purification approach, SifterNet, is proposed to defend against backdoor attacks in CNNs and Vision Transformers using the Ising model, achieving high accuracy without needing model details or retraining.


<details>
  <summary>Details</summary>
Motivation: Existing defenses require prior knowledge of the target model, clean samples, or retraining, which is impractical. An ideal solution should work without such constraints.

Method: SifterNet leverages the Hopfield network's memorization-association functionality and the Ising model ideology to purify triggers in a lightweight, black-box manner.

Result: Extensive experiments show SifterNet effectively purifies triggers and outperforms state-of-the-art baselines on common datasets.

Conclusion: SifterNet provides a practical, model-agnostic defense against backdoor attacks, validated by superior performance.

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [644] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/pdf/2505.14535)
*Jiangrong Shen, Yulin Xie, Qi Xu, Gang Pan, Huajin Tang, Badong Chen*

Main category: cs.LG

TL;DR: A framework for multimodal SNNs addresses modality imbalance and temporal misalignment using dynamic attention and adaptive fusion, achieving state-of-the-art performance with energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Multimodal SNNs face challenges like modality imbalance and temporal misalignment, hindering efficient sensory processing.

Method: Proposes a Temporal Attention-guided Adaptive Fusion (TAAF) module and a temporal adaptive balanced fusion loss for dynamic feature integration and modality-specific learning rates.

Result: Achieves 77.55%, 70.65%, and 97.5% accuracy on CREMA-D, AVE, and EAD datasets, respectively, with improved energy efficiency.

Conclusion: The framework bridges biological sensory processing and machine intelligence, offering a new paradigm for temporally coherent multimodal learning in SNNs.

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [645] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/pdf/2505.14543)
*Utsav Dutta, Sina Khoshfetrat Pakazad, Henrik Ohlsson*

Main category: cs.LG

TL;DR: CHARM is a foundation embedding model for multivariate time series, offering transferable and domain-aware representations with architectural innovations and a novel training approach.


<details>
  <summary>Details</summary>
Motivation: Traditional time series models are task-specific and require extensive feature engineering, while foundation models for time series remain under-explored.

Method: CHARM integrates channel-level textual descriptions and uses a Joint Embedding Predictive Architecture (JEPA) with novel augmentations and a specialized loss function.

Result: The 7M-parameter model achieves state-of-the-art performance across diverse tasks.

Conclusion: CHARM sets a new benchmark for time series representation learning.

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [646] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/pdf/2505.14555)
*Yingtao Luo, Shikai Fang, Binqing Wu, Qingsong Wen, Liang Sun*

Main category: cs.LG

TL;DR: PhyDL-NWP integrates physics into deep learning for weather forecasting, improving accuracy and efficiency while maintaining physical consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional NWP methods are computationally intensive and incomplete, while DL models lack physical interpretability.

Method: PhyDL-NWP combines physical equations with latent force parameterization, uses automatic differentiation for physical terms, and employs a physics-informed loss.

Result: Achieves 170x faster inference with 55K parameters, improves forecasting performance, and ensures physical consistency.

Conclusion: PhyDL-NWP successfully bridges the gap between data-driven efficiency and physical accuracy in weather forecasting.

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [647] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/pdf/2505.14564)
*David Krame Kadurha, Domini Jocema Leko Moutouo, Yae Ulrich Gaba*

Main category: cs.LG

TL;DR: The paper explores the topological foundations of reinforcement learning (RL), using mathematical concepts like complete metric spaces and the Banach contraction principle to explain RL algorithm convergence. It introduces alternative Bellman operator formulations to improve RL performance.


<details>
  <summary>Details</summary>
Motivation: To bridge theoretical mathematics and practical RL algorithm design, enhancing efficiency and convergence rates.

Method: Leverages Banach fixed-point theorem and Bellman operators on Banach spaces, with empirical validation in RL environments like MountainCar and CartPole.

Result: Demonstrates improved convergence rates and performance in RL tasks through alternative Bellman operator formulations.

Conclusion: A deeper mathematical understanding of RL leads to more effective decision-making algorithms.

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [648] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/pdf/2505.14566)
*Andrei Cozma, Landon Harris, Hairong Qi*

Main category: cs.LG

TL;DR: KIPPO enhances PPO by using Koopman Operator Theory to create a linear latent-space representation of non-linear dynamics, improving performance and stability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high variance and unstable learning in RL for complex non-linear systems.

Method: Integrates a Koopman-approximation auxiliary network into PPO to learn a linear latent-space representation.

Result: 6-60% performance boost and up to 91% reduced variability in continuous control tasks.

Conclusion: KIPPO effectively improves PPO by leveraging linear approximations of non-linear dynamics.

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [649] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/pdf/2505.14592)
*Alexandre Broggi, Nathaniel Bastian, Lance Fiondella, Gokhan Kul*

Main category: cs.LG

TL;DR: The paper evaluates neural network pruning methods for cybersecurity tasks, finding limited generalization success.


<details>
  <summary>Details</summary>
Motivation: To reduce neural network size while preserving performance, especially for cybersecurity applications.

Method: Analyzed various pruning methods on a simpler network for a cybersecurity dataset, testing different pruning degrees.

Result: Most pruning methods did not generalize well; only a few performed acceptably.

Conclusion: Few pruning methods are suitable for the task, highlighting the challenge of generalization in cybersecurity applications.

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [650] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/pdf/2505.14625)
*Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran*

Main category: cs.LG

TL;DR: The paper addresses false negatives in RL-based verification for LLMs, proposing TinyV to mitigate the issue and improve training efficiency.


<details>
  <summary>Details</summary>
Motivation: False negatives in verifiers for RL-based LLMs impair training by rejecting correct outputs, necessitating a solution to improve reward reliability.

Method: The authors analyze the Big-Math-RL-Verified dataset, identify false negatives, and introduce TinyV, a lightweight LLM-based verifier to augment rule-based methods.

Result: TinyV improves pass rates by up to 10% and accelerates convergence in math-reasoning benchmarks.

Conclusion: Addressing verifier false negatives is crucial for RL-based LLM fine-tuning, and TinyV offers a practical solution.

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [651] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/pdf/2505.14595)
*Nima Hosseini Dashtbayaz, Hesam Salehipour, Adrian Butscher, Nigel Morris*

Main category: cs.LG

TL;DR: The paper introduces Physics-informed ROM ($\Phi$-ROM), a method integrating differentiable PDE solvers into reduced-order modeling to improve accuracy and generalization in simulating complex systems.


<details>
  <summary>Details</summary>
Motivation: Current ROMs exclude high-fidelity solvers during training, leading to latent dynamics that drift from governing physics, limiting generalization and forecasting.

Method: Proposes $\Phi$-ROM by embedding differentiable PDE solvers into training, ensuring latent dynamics align with governing physics.

Result: Outperforms data-driven ROMs, generalizes to unseen parameters, enables long-term forecasting, and works with sparse/irregular data.

Conclusion: $\Phi$-ROM offers a robust, flexible framework for field reconstruction and data assimilation, with broad applicability across PDE systems.

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [652] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/pdf/2505.14659)
*Navneet Kaur, Lav Gupta*

Main category: cs.LG

TL;DR: The paper discusses securing 6G-enabled healthcare using explainable AI techniques like SHAP, LIME, and DiCE to address vulnerabilities and enhance trust.


<details>
  <summary>Details</summary>
Motivation: The rise of IoT in healthcare introduces serious security risks, such as cyberattacks leading to life-threatening consequences, necessitating robust solutions.

Method: The paper employs explainable AI techniques (SHAP, LIME, DiCE) to identify vulnerabilities and improve security in 6G-enabled healthcare systems.

Result: Experimental analysis demonstrates promising results in uncovering vulnerabilities and strengthening defenses.

Conclusion: Explainable AI can enhance security, trust, and transparency in 6G healthcare, addressing critical vulnerabilities.

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [653] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/pdf/2505.14596)
*Isabella Degen, Zahraa S Abdallah, Henry W J Reeve, Kate Robson Brown*

Main category: cs.LG

TL;DR: CSTS is a synthetic benchmark for evaluating correlation structure discovery in time series clustering, addressing challenges like unclear clustering quality and algorithmic limitations.


<details>
  <summary>Details</summary>
Motivation: The lack of validated ground truth in time series clustering makes it hard to assess quality or diagnose failures, leading to subjective evaluations.

Method: CSTS provides a synthetic benchmark with distinct correlation structures, varied data conditions, and evaluation protocols to isolate clustering failures.

Result: Empirical validation shows moderate distortion from downsampling but minimal effects from distribution shifts and sparsification. A case study reveals an algorithm's sensitivity to non-normal distributions.

Conclusion: CSTS enables precise diagnosis of methodological limitations and advances rigorous evaluation standards for correlation-based time series clustering.

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [654] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/pdf/2505.14606)
*Maksim Zhdanov, Vladislav Kurenkov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [655] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/pdf/2505.14610)
*Hao Wang, Chenyu Shi, Angel E. Rodriguez-Fernandez, Oliver Schütze*

Main category: cs.LG

TL;DR: The paper proposes using Maximum Mean Discrepancy (MMD) for solving continuous multi-objective optimization problems (MOPs) by measuring the distance between approximate and reference sets. It introduces MMDN, a Newton-based method, and hybridizes it with MOEAs for better performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for MOPs use distances like Hausdorff, but MMD offers a more effective measure. The goal is to improve optimization accuracy by leveraging MMD's properties.

Method: The paper introduces MMDN, a Newton method using MMD's gradient and Hessian, and hybridizes it with MOEAs for global optimization. Theoretical analysis of MMD's properties is provided.

Result: Empirical tests on 11 benchmarks show the hybrid (MMDN + MOEA) outperforms MOEAs alone in optimization accuracy under the same computational budget.

Conclusion: MMD-based methods, especially when hybridized with MOEAs, offer superior performance for solving MOPs, validated by both theory and experiments.

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [656] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/pdf/2505.14613)
*Emmanuel Noutahi, Jason Hartford, Prudencio Tossou, Shawn Whitfield, Alisandra K. Denton, Cas Wognum, Kristina Ulicna, Jonathan Hsu, Michael Cuccarese, Emmanuel Bengio, Dominique Beaini, Christopher Gibson, Daniel Cohen, Berton Earnshaw*

Main category: cs.LG

TL;DR: The paper discusses the potential of virtual cells in drug discovery, emphasizing AI and computational advancements to simulate patient responses and improve treatment hypotheses testing.


<details>
  <summary>Details</summary>
Motivation: The complexity of cellular biology makes drug discovery costly and time-consuming. Virtual cells could revolutionize this by predicting cellular responses to treatments before clinical trials.

Method: The paper proposes a lab-in-the-loop approach for virtual cell development, focusing on accurate prediction of functional responses and biomolecular interactions.

Result: The approach aims to generate novel biological insights and improve drug discovery outcomes by leveraging AI and high-throughput profiling.

Conclusion: Virtual cells could serve as a foundational framework for higher-level models like virtual patients, enhancing drug discovery efficiency and impact.

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [657] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/pdf/2505.14620)
*Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan*

Main category: cs.LG

TL;DR: The paper introduces Contrastive LoRA Decoding (CoLD), a method to enhance task-specific performance in LoRA-adapted LLMs by contrasting expert and base model outputs, achieving higher accuracy and lower latency.


<details>
  <summary>Details</summary>
Motivation: Typical decoding methods in LoRA-adapted LLMs often fail for complex tasks due to base model biases, leading to generic responses. CoLD aims to leverage LoRA-specific adaptations more effectively.

Method: CoLD uses contrastive decoding, scoring tokens based on divergence between LoRA-adapted expert and base model distributions, prioritizing task-specific tokens. An optimized kernel for Huawei's Ascend NPU reduces computational costs.

Result: CoLD improves task accuracy by up to 5.54% and reduces latency by 28% compared to greedy decoding.

Conclusion: CoLD offers an efficient decoding strategy for fine-tuned LLMs, benefiting resource-constrained environments and applied data science.

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [658] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/pdf/2505.14635)
*Benjamin Prada, Shion Matsumoto, Abdul Malik Zekri, Ankur Mali*

Main category: cs.LG

TL;DR: The paper connects predictive coding (PC) with the minimum description length (MDL) principle, proving PC minimizes empirical risk and model complexity, and provides generalization and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To establish a theoretical link between biologically inspired PC and MDL, offering a grounded alternative to backpropagation in deep networks.

Method: Uses Hoeffding's inequality and a prefix-code prior to derive generalization bounds, proving PC decreases empirical codelength and converges to MDL-optimal solutions.

Result: PC provides tighter risk bounds than gradient descent and converges to block-coordinate stationary points, ensuring MDL-optimality.

Conclusion: PC is a biologically plausible, theoretically sound alternative to backpropagation with formal guarantees for deep models.

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \hat{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [659] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/pdf/2505.14643)
*Ane G. Domingo-Aldama, Marcos Merino Prado, Alain García Olea, Koldo Gojenola Galletebeitia, Josu Goikoetxea Salutregi, Aitziber Atutxa Salazar*

Main category: cs.LG

TL;DR: The study predicts AF recurrence using traditional scores, ML models, and a novel LTM approach, integrating structured and unstructured data for better accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional AF recurrence prediction scores are limited, and EHR data often contains errors. The study aims to improve prediction by combining structured and unstructured data.

Method: Combined structured clinical data with NLP-processed free-text reports to create a high-quality dataset. Evaluated traditional scores, ML models, and the LTM approach on 1,508 patients.

Result: The LTM approach outperformed traditional scores and ML models, though gender and age biases were noted.

Conclusion: Integrating structured and unstructured data improves AF recurrence prediction, with the LTM model showing superior performance over traditional methods.

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [660] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/pdf/2505.14669)
*Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh*

Main category: cs.LG

TL;DR: The paper introduces Quartet, a method for accurate FP4 training of large language models, addressing current accuracy degradation issues and demonstrating competitive performance with standard-precision training.


<details>
  <summary>Details</summary>
Motivation: The increasing computational demands of training large language models (LLMs) necessitate low-precision arithmetic solutions like FP4, but current methods suffer from accuracy degradation and reliance on mixed-precision fallbacks.

Method: The authors propose Quartet, a hardware-supported FP4 training approach, and develop optimized CUDA kernels for NVIDIA Blackwell GPUs. They introduce a low-precision scaling law to evaluate performance trade-offs.

Result: Quartet achieves state-of-the-art accuracy for FP4 precision, successfully training billion-scale models, and proves competitive with standard-precision and FP8 training.

Conclusion: Fully FP4-based training, enabled by Quartet, is a viable and efficient alternative for training LLMs, offering significant computational and energy savings without compromising accuracy.

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


### [661] [Gradient Leakage Defense with Key-Lock Module for Federated Learning](https://arxiv.org/pdf/2305.04095)
*Hanchi Ren, Jingjing Deng, Xianghua Xie, Xiaoke Ma, Jianfeng Ma*

Main category: cs.LG

TL;DR: The paper addresses privacy risks in Federated Learning (FL) by proposing a key-lock module to secure gradients, preventing data leakage while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Recent findings show FL's shared gradients can leak sensitive data, necessitating a robust defense mechanism.

Method: Introduces a private key-lock module to secure gradients before sharing, ensuring data cannot be reconstructed and model performance is preserved.

Result: Empirical tests confirm the method's effectiveness in defending against gradient leakage without compromising model accuracy.

Conclusion: The key-lock module provides a reliable solution to gradient leakage in FL, supported by theory and experiments.

Abstract: Federated Learning (FL) is a widely adopted privacy-preserving machine
learning approach where private data remains local, enabling secure
computations and the exchange of local model gradients between local clients
and third-party parameter servers. However, recent findings reveal that privacy
may be compromised and sensitive information potentially recovered from shared
gradients. In this study, we offer detailed analysis and a novel perspective on
understanding the gradient leakage problem. These theoretical works lead to a
new gradient leakage defense technique that secures arbitrary model
architectures using a private key-lock module. Only the locked gradient is
transmitted to the parameter server for global model aggregation. Our proposed
learning method is resistant to gradient leakage attacks, and the key-lock
module is designed and trained to ensure that, without the private information
of the key-lock module: a) reconstructing private training data from the shared
gradient is infeasible; and b) the global model's inference performance is
significantly compromised. We discuss the theoretical underpinnings of why
gradients can leak private information and provide theoretical proof of our
method's effectiveness. We conducted extensive empirical evaluations with many
models on several popular benchmarks, demonstrating the robustness of our
proposed approach in both maintaining model performance and defending against
gradient leakage attacks.

</details>


### [662] [Deep Learning for Multivariate Time Series Imputation: A Survey](https://arxiv.org/pdf/2402.04059)
*Jun Wang, Wenjie Du, Yiyuan Yang, Linglong Qian, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, Qingsong Wen*

Main category: cs.LG

TL;DR: A survey on deep learning approaches for multivariate time series imputation (MTSI), proposing a taxonomy based on imputation uncertainty and neural network architecture, and highlighting the PyPOTS Ecosystem for standardized research.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing values in multivariate time series data, which hinders accurate analysis and downstream applications.

Method: Categorizes existing deep learning methods for MTSI into a novel taxonomy and reviews toolkits like PyPOTS.

Result: Provides a comprehensive summary of MTSI methods, a taxonomy, and highlights key challenges and future directions.

Conclusion: The survey serves as a resource for researchers and practitioners, with a maintained paper and tool list available.

Abstract: Missing values are ubiquitous in multivariate time series (MTS) data, posing
significant challenges for accurate analysis and downstream applications. In
recent years, deep learning-based methods have successfully handled missing
data by leveraging complex temporal dependencies and learned data
distributions. In this survey, we provide a comprehensive summary of deep
learning approaches for multivariate time series imputation (MTSI) tasks. We
propose a novel taxonomy that categorizes existing methods based on two key
perspectives: imputation uncertainty and neural network architecture.
Furthermore, we summarize existing MTSI toolkits with a particular emphasis on
the PyPOTS Ecosystem, which provides an integrated and standardized foundation
for MTSI research. Finally, we discuss key challenges and future research
directions, which give insight for further MTSI research. This survey aims to
serve as a valuable resource for researchers and practitioners in the field of
time series analysis and missing data imputation tasks.A well-maintained MTSI
paper and tool list are available at
https://github.com/WenjieDu/Awesome_Imputation.

</details>


### [663] [Federated Hybrid Model Pruning through Loss Landscape Exploration](https://arxiv.org/pdf/2405.10271)
*Christian Internò, Elena Raponi, Niki van Stein, Thomas Bäck, Markus Olhofer, Yaochu Jin, Barbara Hammer*

Main category: cs.LG

TL;DR: AutoFLIP is a novel framework optimizing federated learning (FL) by adaptive hybrid pruning, reducing computational and communication costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: FL faces challenges like high communication costs, computational constraints, and heterogeneous data. AutoFLIP aims to address these for scalable, efficient FL deployment.

Method: AutoFLIP uses an adaptive hybrid pruning approach, analyzing non-IID client loss landscapes to identify model substructures for pruning at structured and unstructured levels.

Result: Experiments show a 48.8% reduction in computational overhead, 35.5% decrease in communication costs, and improved global accuracy.

Conclusion: AutoFLIP enables efficient FL deployment, making it scalable and broadly applicable in real-world scenarios.

Abstract: As the era of connectivity and unprecedented data generation expands,
collaborative intelligence emerges as a key driver for machine learning,
encouraging global-scale model development. Federated learning (FL) stands at
the heart of this transformation, enabling distributed systems to work
collectively on complex tasks while respecting strict constraints on privacy
and security. Despite its vast potential, specially in the age of complex
models, FL encounters challenges such as elevated communication costs,
computational constraints, and the heterogeneous data distributions. In this
context, we present AutoFLIP, a novel framework that optimizes FL through an
adaptive hybrid pruning approach, grounded in a federated loss exploration
phase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP
efficiently identifies model substructures for pruning both at structured and
unstructured levels. This targeted optimization fosters a symbiotic
intelligence loop, reducing computational burdens and boosting model
performance on resource-limited devices for a more inclusive and democratized
model usage. Our extensive experiments across multiple datasets and FL tasks
show that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in
computational overhead, a 35.5% decrease in communication costs, and a notable
improvement in global accuracy. By significantly reducing these overheads,
AutoFLIP offer the way for efficient FL deployment in real-world applications
for a scalable and broad applicability.

</details>


### [664] [STD-PLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with PLM](https://arxiv.org/pdf/2407.09096)
*YiHeng Huang, Xiaowei Mao, Shengnan Guo, Yubin Chen, Junfeng Shen, Tiankuo Li, Youfang Lin, Huaiyu Wan*

Main category: cs.LG

TL;DR: STD-PLM is a spatial-temporal data model using PLM for forecasting and imputation, addressing correlations and efficiency with novel tokenizers and attention modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack versatility for both forecasting and imputation tasks and struggle with zero-shot and few-shot learning. PLMs show promise but need better spatial-temporal correlation modeling.

Method: STD-PLM uses spatial/temporal tokenizers, topology-aware embeddings, and a sandglass attention module (SGA) with constrained loss for efficiency.

Result: Competitive performance in forecasting/imputation tasks, strong generalization, and success in few-shot/zero-shot learning.

Conclusion: STD-PLM effectively addresses spatial-temporal data challenges, offering versatility and efficiency.

Abstract: Spatial-temporal forecasting and imputation are important for real-world
intelligent systems. Most existing methods are tailored for individual
forecasting or imputation tasks but are not designed for both. Additionally,
they are less effective for zero-shot and few-shot learning. While pre-trained
language model (PLM) have exhibited strong pattern recognition and reasoning
abilities across various tasks, including few-shot and zero-shot learning,
their applications in spatial-temporal data understanding has been constrained
by insufficient modeling of complex correlations such as the temporal
correlations, spatial connectivity, non-pairwise and high-order
spatial-temporal correlations within data. In this paper, we propose STD-PLM
for understanding both spatial and temporal properties of
\underline{S}patial-\underline{T}emporal \underline{D}ata with \underline{PLM},
which is capable of implementing both spatial-temporal forecasting and
imputation tasks. STD-PLM understands spatial-temporal correlations via
explicitly designed spatial and temporal tokenizers. Topology-aware node
embeddings are designed for PLM to comprehend and exploit the topology
structure of data in inductive manner. Furthermore, to mitigate the efficiency
issues introduced by the PLM, we design a sandglass attention module (SGA)
combined with a specific constrained loss function, which significantly
improves the model's efficiency while ensuring performance. Extensive
experiments demonstrate that STD-PLM exhibits competitive performance and
generalization capabilities across the forecasting and imputation tasks on
various datasets. Moreover, STD-PLM achieves promising results on both few-shot
and zero-shot tasks. The code is made available at
\href{https://github.com/Hyheng/STD-PLM}{https://github.com/Hyheng/STD-PLM}

</details>


### [665] [Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework](https://arxiv.org/pdf/2409.04744)
*Yongxin Deng, Xihe Qiu, Jue Chen, Xiaoyu Tan*

Main category: cs.LG

TL;DR: LMGT is a sample-efficient RL framework that uses LLMs to guide reward tuning, balancing exploration and exploitation, and reducing computational resources.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing exploration and exploitation in RL, especially in sparse-reward environments like robotic control, by leveraging prior knowledge from LLMs.

Method: Proposes LMGT, which uses LLMs to guide reward shifts, improving sample efficiency and exploratory behavior.

Result: LMGT outperforms baseline methods in various RL tasks and reduces computational resource usage.

Conclusion: LMGT effectively leverages LLMs to enhance RL performance and efficiency, particularly in sparse-reward scenarios.

Abstract: The inherent uncertainty in the environmental transition model of
Reinforcement Learning (RL) necessitates a delicate balance between exploration
and exploitation. This balance is crucial for optimizing computational
resources to accurately estimate expected rewards for the agent. In scenarios
with sparse rewards, such as robotic control systems, achieving this balance is
particularly challenging. However, given that many environments possess
extensive prior knowledge, learning from the ground up in such contexts may be
redundant. To address this issue, we propose Language Model Guided reward
Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their proficiency in processing non-standard data forms, such as wiki
tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances
exploration and exploitation, thereby guiding the agent's exploratory behavior
and enhancing sample efficiency. We have rigorously evaluated LMGT across
various RL tasks and evaluated it in the embodied robotic environment
Housekeep. Our results demonstrate that LMGT consistently outperforms baseline
methods. Furthermore, the findings suggest that our framework can substantially
reduce the computational resources required during the RL training phase.

</details>


### [666] [OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition](https://arxiv.org/pdf/2409.13652)
*Stephen Zhang, Vardan Papyan*

Main category: cs.LG

TL;DR: OATS is a novel method for compressing large transformers without retraining, using second moment information to decompose weights into sparse and low-rank matrices, achieving high compression and performance.


<details>
  <summary>Details</summary>
Motivation: Address the high memory and compute costs of large foundation models by developing efficient post-hoc pruning techniques that maintain performance.

Method: Utilizes second moment information in input embeddings to decompose model weights into sparse and low-rank matrices, enabling compression without retraining.

Result: Achieves state-of-the-art performance, compressing models by up to 60% with 1.37× CPU acceleration compared to pruned models.

Conclusion: OATS offers an effective solution for compressing large transformers, balancing performance and efficiency without retraining.

Abstract: The recent paradigm shift to large-scale foundation models has brought about
a new era for deep learning that, while has found great success in practice,
has also been plagued by prohibitively expensive costs in terms of high memory
consumption and compute. To mitigate these issues, there has been a concerted
effort in post-hoc neural network pruning techniques that do not require costly
retraining. Despite the considerable progress being made, existing methods
often exhibit a steady drop in model performance as the compression increases.
In this paper, we present a novel approach to compressing large transformers,
coined OATS, that utilizes the second moment information in the input
embeddings to decompose the model weights into a sum of sparse and low-rank
matrices. Without any retraining, OATS achieves state-of-the-art performance
when compressing models by up to $60\%$ on large language models such as
Llama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while
delivering up to $1.37\times$ the CPU acceleration versus a model that was
comparably pruned.

</details>


### [667] [EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions](https://arxiv.org/pdf/2410.03779)
*Huayu Deng, Xiangming Zhu, Yunbo Wang, Xiaokang Yang*

Main category: cs.LG

TL;DR: EvoMesh is a differentiable framework that learns adaptive graph hierarchies and physical dynamics, outperforming fixed-hierarchy methods.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical graph methods for physical simulation are manually designed and fixed, limiting adaptability to dynamic systems.

Method: EvoMesh uses anisotropic message passing and learns node selection probabilities adaptively based on physical context.

Result: EvoMesh outperforms fixed-hierarchy networks on five benchmark datasets.

Conclusion: The framework enhances flexibility and long-range dependency capture in physical simulations.

Abstract: Graph neural networks have been a powerful tool for mesh-based physical
simulation. To efficiently model large-scale systems, existing methods mainly
employ hierarchical graph structures to capture multi-scale node relations.
However, these graph hierarchies are typically manually designed and fixed,
limiting their ability to adapt to the evolving dynamics of complex physical
systems. We propose EvoMesh, a fully differentiable framework that jointly
learns graph hierarchies and physical dynamics, adaptively guided by physical
inputs. EvoMesh introduces anisotropic message passing, which enables
direction-specific aggregation of dynamic features between nodes within each
hierarchy, while simultaneously learning node selection probabilities for the
next hierarchical level based on physical context. This design creates more
flexible message shortcuts and enhances the model's capacity to capture
long-range dependencies. Extensive experiments on five benchmark physical
simulation datasets show that EvoMesh outperforms recent fixed-hierarchy
message passing networks by large margins. Code is available at
https://github.com/hbell99/EvoMesh.

</details>


### [668] [TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks](https://arxiv.org/pdf/2410.06530)
*Mathilde Papillon, Guillermo Bernárdez, Claudio Battiloro, Nina Miolane*

Main category: cs.LG

TL;DR: The paper introduces Generalized Combinatorial Complex Neural Networks (GCCNs) to simplify and standardize Topological Deep Learning (TDL), outperforming existing models like CCNNs with less complexity.


<details>
  <summary>Details</summary>
Motivation: Current TDL lacks a standardized framework, limiting accessibility and applicability despite its potential for modeling higher-order interactions in complex systems.

Method: The authors propose GCCNs, a family of TDL models that can transform any graph neural network into its TDL counterpart, and introduce TopoTune, a software for easy implementation.

Result: GCCNs generalize and outperform CCNNs, often with reduced model complexity, as demonstrated by extensive experiments.

Conclusion: GCCNs and TopoTune democratize TDL, making it more accessible and practical for diverse applications.

Abstract: Graph Neural Networks (GNNs) excel in learning from relational datasets as
they preserve the symmetries of the graph domain. However, many complex systems
-- such as biological or social networks -- involve multiway complex
interactions that are more naturally represented by higher-order topological
domains. The emerging field of Topological Deep Learning (TDL) aims to
accommodate and leverage these higher-order structures. Combinatorial Complex
Neural Networks (CCNNs), fairly general TDL models, have been shown to be more
expressive and better performing than GNNs. However, differently from the GNN
ecosystem, TDL lacks a principled and standardized framework for easily
defining new architectures, restricting its accessibility and applicability. To
address this issue, we introduce Generalized CCNNs (GCCNs), a simple yet
powerful family of TDL models that can be used to systematically transform any
(graph) neural network into its TDL counterpart. We prove that GCCNs generalize
and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show
that these architectures consistently match or outperform CCNNs, often with
less model complexity. In an effort to accelerate and democratize TDL, we
introduce TopoTune, a lightweight software for defining, building, and training
GCCNs with unprecedented flexibility and ease.

</details>


### [669] [Large Continual Instruction Assistant](https://arxiv.org/pdf/2410.10868)
*Jingyang Qiao, Zhizhong Zhang, Xin Tan, Yanyun Qu, Shouhong Ding, Yuan Xie*

Main category: cs.LG

TL;DR: A framework for Continual Instruction Tuning (CIT) using Exponential Moving Average (EMA) and a stable-plasticity balanced coefficient to reduce forgetting and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing gradient updates in CIT heavily degrade performance on previous datasets, and EMA's fixed balance weight struggles with dynamic datasets.

Method: Proposes a framework with optimal balance weight determined by gradients and parameters, and uses semantic similarity to guide parameter allocation.

Result: Enhances anti-forgetting and improves continual tuning performance across benchmarks.

Conclusion: The proposed method effectively balances plasticity and stability, outperforming existing approaches in CIT.

Abstract: Continual Instruction Tuning (CIT) is adopted to continually instruct Large
Models to follow human intent data by data. It is observed that existing
gradient update would heavily destroy the performance on previous datasets
during CIT process. Instead, Exponential Moving Average (EMA), owns the ability
to trace previous parameters, which can aid in decreasing forgetting.
Nonetheless, its stable balance weight fails to deal with the ever-changing
datasets, leading to the out-of-balance between plasticity and stability. In
this paper, we propose a general continual instruction tuning framework to
address the challenge. Starting from the trade-off prerequisite and EMA update,
we propose the plasticity and stability ideal condition. Based on Taylor
expansion in the loss function, we find the optimal balance weight can be
automatically determined by the gradients and learned parameters. Therefore, we
propose a stable-plasticity balanced coefficient to avoid knowledge
interference. Based on the semantic similarity of the instructions, we can
determine whether to retrain or expand the training parameters and allocate the
most suitable parameters for the testing instances. Extensive experiments
across multiple continual instruction tuning benchmarks demonstrate that our
approach not only enhances anti-forgetting capabilities but also significantly
improves overall continual tuning performance. Our code is available at
https://github.com/JingyangQiao/CoIN.

</details>


### [670] [Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning](https://arxiv.org/pdf/2410.11234)
*Jiayu Chen, Wentse Chen, Jeff Schneider*

Main category: cs.LG

TL;DR: Offline model-based RL (MBRL) is enhanced by modeling it as a Bayes Adaptive MDP (BAMDP) and using a novel Monte-Carlo planning algorithm, outperforming state-of-the-art methods on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing uncertainty in offline MBRL due to multiple possible MDPs behaving identically on static datasets.

Method: Proposes a Bayes Adaptive MDP framework and a Monte-Carlo planning algorithm for continuous spaces, integrated into policy iteration.

Result: Outperforms state-of-the-art offline RL methods on D4RL MuJoCo and tokamak control tasks.

Conclusion: The 'RL + Search' framework, inspired by AlphaZero, improves offline MBRL by leveraging computation for better performance.

Abstract: Offline RL is a powerful approach for data-driven decision-making and
control. Compared to model-free methods, offline model-based RL (MBRL)
explicitly learns world models from a static dataset and uses them as surrogate
simulators, improving the data efficiency and enabling the learned policy to
potentially generalize beyond the dataset support. However, there could be
various MDPs that behave identically on the offline dataset and dealing with
the uncertainty about the true MDP can be challenging. In this paper, we
propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process
(BAMDP), which is a principled framework for addressing model uncertainty. We
further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable
of solving BAMDPs in continuous state and action spaces with stochastic
transitions. This planning process is based on Monte Carlo Tree Search and can
be integrated into offline MBRL as a policy improvement operator in policy
iteration. Our ``RL + Search" framework follows in the footsteps of superhuman
AIs like AlphaZero, improving on current offline MBRL methods by incorporating
more computation input. The proposed algorithm significantly outperforms
state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three
target tracking tasks in a challenging, stochastic tokamak control simulator.
The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.

</details>


### [671] [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://arxiv.org/pdf/2410.17980)
*Shawn Tan, Songlin Yang, Aaron Courville, Rameswar Panda, Yikang Shen*

Main category: cs.LG

TL;DR: The paper proposes a stick-breaking attention mechanism as an alternative to softmax-based attention, showing competitive performance and better length generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional softmax-based attention requires positional embeddings and struggles with length generalization. The stick-breaking process offers a natural way to incorporate recency bias and improve generalization.

Method: The stick-breaking process determines attention weights by iteratively allocating proportions of a 'stick' to tokens, incorporating recency bias. The method is implemented with numerical stability and adapted to Flash Attention.

Result: Stick-breaking attention performs competitively with softmax+RoPE systems, excels in length generalization, and improves perplexity for longer contexts.

Conclusion: Stick-breaking attention is a viable alternative to softmax-based methods, offering better length generalization and competitive performance on downstream tasks.

Abstract: The self-attention mechanism traditionally relies on the softmax operator,
necessitating positional embeddings like RoPE, or position biases to account
for token order. But current methods using still face length generalisation
challenges. We investigate an alternative attention mechanism based on the
stick-breaking process in larger scale settings. The method works as follows:
For each token before the current, we determine a break point, which represents
the proportion of the stick, the weight of the attention, to allocate to the
current token. We repeat this on the remaining stick, until all tokens are
allocated a weight, resulting in a sequence of attention weights. This process
naturally incorporates recency bias, which has linguistic motivations for
grammar parsing. We study the implications of replacing the conventional
softmax-based attention mechanism with stick-breaking attention. We then
discuss implementation of numerically stable stick-breaking attention and adapt
Flash Attention to accommodate this mechanism. When used as a drop-in
replacement for current softmax+RoPE attention systems, we find that
stick-breaking attention performs competitively with current methods on length
generalisation and downstream tasks. Stick-breaking also performs well at
length generalisation, allowing a model trained with $2^{11}$ context window to
perform well at $2^{14}$ with perplexity improvements.

</details>


### [672] [Differentially Private Synthetic Data via APIs 3: Using Simulators Instead of Foundation Model](https://arxiv.org/pdf/2502.05505)
*Zinan Lin, Tadas Baltrusaitis, Wenyu Wang, Sergey Yekhanin*

Main category: cs.LG

TL;DR: Sim-PE extends Private Evolution (PE) by integrating non-neural network SoTA data synthesizers (simulators), improving performance and efficiency in DP synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: To broaden PE's applicability by leveraging simulators, especially when suitable foundation models are unavailable, enhancing DP synthetic data quality.

Method: Sim-PE integrates simulators (e.g., computer graphics-based image generators) into PE, testing it across four diverse simulators for image synthesis.

Result: Sim-PE improves downstream classification accuracy by up to 3x, reduces FID by 80%, and enhances efficiency. Combining simulators and foundation models yields further gains.

Conclusion: Sim-PE successfully expands PE's scope, demonstrating the effectiveness of simulators in DP data synthesis and offering open-source implementation.

Abstract: Differentially private (DP) synthetic data, which closely resembles the
original private data while maintaining strong privacy guarantees, has become a
key tool for unlocking the value of private data without compromising privacy.
Recently, Private Evolution (PE) has emerged as a promising method for
generating DP synthetic data. Unlike other training-based approaches, PE only
requires access to inference APIs from foundation models, enabling it to
harness the power of state-of-the-art (SoTA) models. However, a suitable
foundation model for a specific private data domain is not always available. In
this paper, we discover that the PE framework is sufficiently general to allow
APIs beyond foundation models. In particular, we demonstrate that many SoTA
data synthesizers that do not rely on neural networks--such as computer
graphics-based image generators, which we refer to as simulators--can be
effectively integrated into PE. This insight significantly broadens PE's
applicability and unlocks the potential of powerful simulators for DP data
synthesis. We explore this approach, named Sim-PE, in the context of image
synthesis. Across four diverse simulators, Sim-PE performs well, improving the
downstream classification accuracy of PE by up to 3x, reducing FID by up to
80%, and offering much greater efficiency. We also show that simulators and
foundation models can be easily leveraged together within PE to achieve further
improvements. The code is open-sourced in the Private Evolution Python library:
https://github.com/microsoft/DPSDA.

</details>


### [673] [Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation](https://arxiv.org/pdf/2503.01776)
*Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You*

Main category: cs.LG

TL;DR: CSR outperforms MRL in adaptive representation learning with higher accuracy, faster retrieval, and reduced training time by using sparse coding on pre-trained embeddings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of MRL (full retraining and performance degradation at short lengths) by proposing a more efficient and high-fidelity method for adaptive embeddings.

Method: CSR sparsifies pre-trained embeddings into a high-dimensional feature space using lightweight autoencoding and contrastive objectives, enabling flexible inference at varying sparsity levels.

Result: CSR consistently beats MRL in accuracy and retrieval speed across benchmarks, with significantly shorter training times.

Conclusion: Sparse coding (CSR) is an effective paradigm for adaptive representation learning, balancing efficiency and fidelity in real-world applications.

Abstract: Many large-scale systems rely on high-quality deep representations
(embeddings) to facilitate tasks like retrieval, search, and generative
modeling. Matryoshka Representation Learning (MRL) recently emerged as a
solution for adaptive embedding lengths, but it requires full model retraining
and suffers from noticeable performance degradations at short lengths. In this
paper, we show that sparse coding offers a compelling alternative for achieving
adaptive representation with minimal overhead and higher fidelity. We propose
Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained
embeddings into a high-dimensional but selectively activated feature space. By
leveraging lightweight autoencoding and task-aware contrastive objectives, CSR
preserves semantic quality while allowing flexible, cost-effective inference at
different sparsity levels. Extensive experiments on image, text, and multimodal
benchmarks demonstrate that CSR consistently outperforms MRL in terms of both
accuracy and retrieval speed-often by large margins-while also cutting training
time to a fraction of that required by MRL. Our results establish sparse coding
as a powerful paradigm for adaptive representation learning in real-world
applications where efficiency and fidelity are both paramount. Code is
available at https://github.com/neilwen987/CSR_Adaptive_Rep

</details>


### [674] [Technical Report: Quantifying and Analyzing the Generalization Power of a DNN](https://arxiv.org/pdf/2505.06993)
*Yuxuan He, Junpeng Zhang, Lei Cheng, Hongyuan Zhang, Quanshi Zhang*

Main category: cs.LG

TL;DR: The paper introduces a method to analyze DNN generalization by disentangling generalizable and non-generalizable interactions during training, revealing a three-phase dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the generalization power of DNNs by analyzing their interaction dynamics, addressing the gap between training and testing performance.

Method: Proposes an efficient method to quantify generalization power of interactions, leveraging AND-OR interaction patterns from explainable AI theory.

Result: Discovers a three-phase training dynamics: early phase removes noisy interactions, later phases capture complex but less generalizable ones. Non-generalizable interactions cause the training-testing gap.

Conclusion: The study provides insights into DNN generalization dynamics, linking non-generalizable interactions to performance gaps, with implications for improving model robustness.

Abstract: This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.

</details>


### [675] [NBDI: A Simple and Effective Termination Condition for Skill Extraction from Task-Agnostic Demonstrations](https://arxiv.org/pdf/2501.12668)
*Myunsoo Kim, Hayeong Lee, Seong-Woong Shim, JunHo Seo, Byung-Jun Lee*

Main category: cs.LG

TL;DR: NBDI proposes a novelty-based termination condition for identifying decision points in skill learning, outperforming fixed-length skill methods in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Fixed-length skills in skill learning may skip valuable decision points, limiting exploration and policy learning.

Method: NBDI uses a state-action novelty module to identify decision points, leveraging agent experience data.

Result: NBDI outperforms baselines in complex, long-horizon tasks and adapts well to environmental variations.

Conclusion: Decision point identification is crucial for skill learning, and NBDI provides an effective solution.

Abstract: Intelligent agents are able to make decisions based on different levels of
granularity and duration. Recent advances in skill learning enabled the agent
to solve complex, long-horizon tasks by effectively guiding the agent in
choosing appropriate skills. However, the practice of using fixed-length skills
can easily result in skipping valuable decision points, which ultimately limits
the potential for further exploration and faster policy learning. In this work,
we propose to learn a simple and effective termination condition that
identifies decision points through a state-action novelty module that leverages
agent experience data. Our approach, Novelty-based Decision Point
Identification (NBDI), outperforms previous baselines in complex, long-horizon
tasks, and remains effective even in the presence of significant variations in
the environment configurations of downstream tasks, highlighting the importance
of decision point identification in skill learning.

</details>


### [676] [Unified Continuous Generative Models](https://arxiv.org/pdf/2505.07447)
*Peng Sun, Yi Jiang, Tao Lin*

Main category: cs.LG

TL;DR: A unified framework for training and sampling continuous generative models, achieving SOTA performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Existing work treats multi-step and few-step generative models as distinct paradigms, leading to separate methodologies.

Method: Introduces UCGM-{T,S}, a unified framework for training, sampling, and analyzing continuous generative models.

Result: Achieves 1.30 FID in 20 steps (multi-step) and 1.42 FID in 2 steps (few-step) on ImageNet 256x256. Improves pre-trained model from 1.26 FID (250 steps) to 1.06 FID (40 steps).

Conclusion: UCGM provides a unified, efficient approach for continuous generative models, outperforming existing methods with fewer steps.

Abstract: Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.

</details>


### [677] [On the Role of Transformer Feed-Forward Layers in Nonlinear In-Context Learning](https://arxiv.org/pdf/2501.18187)
*Haoyuan Sun, Ali Jadbabaie, Navid Azizan*

Main category: cs.LG

TL;DR: The paper explores how Transformers perform in-context learning (ICL) for nonlinear tasks, revealing limitations of linear self-attention (LSA) and proposing a solution combining LSA with GLU-inspired feed-forward layers.


<details>
  <summary>Details</summary>
Motivation: To understand and extend ICL capabilities of Transformers beyond linear tasks, addressing the inherent limitations of LSA for nonlinear problems.

Method: Combines LSA with GLU-inspired feed-forward layers to enable nonlinear ICL, demonstrating its ability to implement gradient descent on polynomial kernel regression.

Result: The proposed GLU-LSA model successfully performs nonlinear ICL, implementing gradient descent in polynomial kernel space and block coordinate descent with multiple blocks.

Conclusion: Feed-forward layers in Transformers are crucial for nonlinear ICL, complementing the linear capabilities of attention mechanisms.

Abstract: Transformer-based models demonstrate a remarkable ability for in-context
learning (ICL), where they can adapt to unseen tasks from a few prompt examples
without parameter updates. Notably, recent research has provided insight into
how the Transformer architecture can perform ICL, showing that the optimal
linear self-attention (LSA) mechanism can implement one step of gradient
descent for linear least-squares objectives when trained on random linear
regression tasks.
  Building upon this understanding of linear ICL, we investigate ICL for
nonlinear function classes. We first show that LSA is inherently incapable of
solving problems that go beyond linear least-squares objectives, underscoring
why prior solutions cannot readily extend to nonlinear ICL tasks. To overcome
this limitation, we investigate a mechanism combining LSA with feed-forward
layers that are inspired by the gated linear units (GLU) commonly found in
modern Transformer architectures. We show that this combination empowers the
Transformer to perform nonlinear ICL, specifically by implementing one step of
gradient descent on a polynomial kernel regression loss. Furthermore, we show
that multiple blocks of our GLU-LSA model implement block coordinate descent in
this polynomial kernel space. Our findings highlight the distinct roles of
attention and feed-forward layers, demonstrating that the feed-forward
components provide a mechanism by which Transformers gain nonlinear
capabilities for ICL.

</details>


### [678] [Implicit vs Unfolded Graph Neural Networks](https://arxiv.org/pdf/2111.06592)
*Yongyi Yang, Tang Liu, Yangkun Wang, Zengfeng Huang, David Wipf*

Main category: cs.LG

TL;DR: The paper compares implicit (IGNN) and unfolded (UGNN) GNNs, showing their equivalence in some scenarios and divergence in others, with UGNN excelling in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of message-passing GNNs in balancing long-range dependency modeling with issues like oversmoothing and interpretability.

Method: Analyze IGNN and UGNN, comparing their solutions, convergence, representational capacity, and interpretability, supported by empirical benchmarks.

Result: IGNN is more memory-efficient, while UGNN offers superior accuracy and interpretability in diverse scenarios like adversarial graphs and heterophily.

Conclusion: UGNN's integrated attention mechanisms and propagation rules make it more versatile for complex graph tasks despite IGNN's efficiency.

Abstract: It has been observed that message-passing graph neural networks (GNN)
sometimes struggle to maintain a healthy balance between the efficient/scalable
modeling of long-range dependencies across nodes while avoiding unintended
consequences such oversmoothed node representations, sensitivity to spurious
edges, or inadequate model interpretability. To address these and other issues,
two separate strategies have recently been proposed, namely implicit and
unfolded GNNs (that we abbreviate to IGNN and UGNN respectively). The former
treats node representations as the fixed points of a deep equilibrium model
that can efficiently facilitate arbitrary implicit propagation across the graph
with a fixed memory footprint. In contrast, the latter involves treating graph
propagation as unfolded descent iterations as applied to some graph-regularized
energy function. While motivated differently, in this paper we carefully
quantify explicit situations where the solutions they produce are equivalent
and others where their properties sharply diverge. This includes the analysis
of convergence, representational capacity, and interpretability. In support of
this analysis, we also provide empirical head-to-head comparisons across
multiple synthetic and public real-world node classification benchmarks. These
results indicate that while IGNN is substantially more memory-efficient, UGNN
models support unique, integrated graph attention mechanisms and propagation
rules that can achieve strong node classification accuracy across disparate
regimes such as adversarially-perturbed graphs, graphs with heterophily, and
graphs involving long-range dependencies.

</details>


### [679] [Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach](https://arxiv.org/pdf/2501.19403)
*Yingdan Shi, Sijia Liu, Ren Wang*

Main category: cs.LG

TL;DR: The paper introduces new metrics and a framework for reliable machine unlearning, addressing fake unlearning issues and improving unlearning accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing metrics like UA and MIA fail to reliably assess forgetting quality, leading to fake unlearning where misclassified data still retain ground truth labels.

Method: Proposes two conformal prediction-inspired metrics and a framework integrating conformal prediction with adversarial attack loss to improve unlearning.

Result: The framework boosts unlearning accuracy by 6.6% on average in image classification tasks.

Conclusion: The new metrics and framework effectively address fake unlearning and enhance unlearning performance.

Abstract: Machine unlearning seeks to remove the influence of specified data from a
trained model. While metrics such as unlearning accuracy (UA) and membership
inference attack (MIA) provide baselines for assessing unlearning performance,
they fall short of evaluating the forgetting reliability. In this paper, we
find that the data misclassified across UA and MIA still have their ground
truth labels included in the prediction set from the uncertainty quantification
perspective, which raises a fake unlearning issue. To address this issue, we
propose two novel metrics inspired by conformal prediction that more reliably
evaluate forgetting quality. Building on these insights, we further propose a
conformal prediction-based unlearning framework that integrates conformal
prediction into Carlini & Wagner adversarial attack loss, which can
significantly push the ground truth label out of the conformal prediction set.
Through extensive experiments on image classification task, we demonstrate both
the effectiveness of our proposed metrics and the superiority of our unlearning
framework, which improves the UA of existing unlearning methods by an average
of 6.6% through the incorporation of a tailored loss term alone.

</details>


### [680] [Gated Recurrent Neural Networks with Weighted Time-Delay Feedback](https://arxiv.org/pdf/2212.00228)
*N. Benjamin Erichson, Soon Hoe Lim, Michael W. Mahoney*

Main category: cs.LG

TL;DR: A novel GRU variant, τ-GRU, with weighted time-delay feedback improves long-term dependency modeling in sequential data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the modeling of long-term dependencies in sequential data by introducing a feedback mechanism in GRUs.

Method: Proposes τ-GRU, a discretized continuous-time GRU governed by delay differential equations (DDEs), with a weighted time-delay feedback mechanism.

Result: τ-GRU outperforms state-of-the-art recurrent units, achieving faster convergence and better generalization.

Conclusion: The τ-GRU model effectively addresses long-term dependency challenges, offering superior performance in sequential data tasks.

Abstract: In this paper, we present a novel approach to modeling long-term dependencies
in sequential data by introducing a gated recurrent unit (GRU) with a weighted
time-delay feedback mechanism. Our proposed model, named $\tau$-GRU, is a
discretized version of a continuous-time formulation of a recurrent unit, where
the dynamics are governed by delay differential equations (DDEs). We prove the
existence and uniqueness of solutions for the continuous-time model and show
that the proposed feedback mechanism can significantly improve the modeling of
long-term dependencies. Our empirical results indicate that $\tau$-GRU
outperforms state-of-the-art recurrent units and gated recurrent architectures
on a range of tasks, achieving faster convergence and better generalization.

</details>


### [681] [Online Scheduling for LLM Inference with KV Cache Constraints](https://arxiv.org/pdf/2502.07115)
*Patrick Jaillet, Jiashuo Jiang, Konstantina Mellou, Marco Molinaro, Chara Podimata, Zijie Zhou*

Main category: cs.LG

TL;DR: The paper proposes a novel batching and scheduling algorithm for LLM inference to optimize latency and KV cache management, outperforming benchmarks in both synthetic and real-world evaluations.


<details>
  <summary>Details</summary>
Motivation: The computational intensity of LLM inference and the memory constraints of KV cache management necessitate efficient scheduling to reduce latency and resource usage.

Method: The authors model LLM inference with KV cache constraints, introduce a hindsight optimal benchmark, and propose a polynomial-time online scheduling algorithm with theoretical guarantees.

Result: The algorithm achieves a constant competitive ratio under certain conditions and outperforms benchmarks in synthetic and real-world evaluations, including Llama2-70B on A100 GPUs.

Conclusion: The work provides a scalable solution for efficient LLM inference, contributing to more sustainable and cost-effective deployment.

Abstract: Large Language Model (LLM) inference, where a trained model generates text
one word at a time in response to user prompts, is a computationally intensive
process requiring efficient scheduling to optimize latency and resource
utilization. A key challenge in LLM inference is the management of the
Key-Value (KV) cache, which reduces redundant computations but introduces
memory constraints. In this work, we model LLM inference with KV cache
constraints theoretically and propose a novel batching and scheduling algorithm
that minimizes inference latency while effectively managing the KV cache's
memory.
  More specifically, we make the following contributions. First, to evaluate
the performance of online algorithms for scheduling in LLM inference, we
introduce a hindsight optimal benchmark, formulated as an integer program that
computes the minimum total inference latency under full future information.
Second, we prove that no deterministic online algorithm can achieve a constant
competitive ratio when the arrival process is arbitrary. Third, motivated by
the computational intractability of solving the integer program at scale, we
propose a polynomial-time online scheduling algorithm and show that under
certain conditions it can achieve a constant competitive ratio. We also
demonstrate our algorithm's strong empirical performance by comparing it to the
hindsight optimal in a synthetic dataset. Finally, we conduct empirical
evaluations on a real-world public LLM inference dataset, simulating the
Llama2-70B model on A100 GPUs, and show that our algorithm significantly
outperforms the benchmark algorithms. Overall, our results offer a path toward
more sustainable and cost-effective LLM deployment.

</details>


### [682] [Towards Model-Agnostic Federated Learning over Networks](https://arxiv.org/pdf/2302.04363)
*S. Abdurakhmanova, Y. SarcheshmehPour, A. Jung*

Main category: cs.LG

TL;DR: A model-agnostic federated learning method for heterogeneous data and models, using network structure to guide regularization in empirical risk minimization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in federated learning with heterogeneous data and models by leveraging network structure to improve model performance.

Method: Uses empirical risk minimization with a regularization term derived from data network structure, ensuring well-connected models yield similar predictions on shared datasets.

Result: The method supports a wide range of local models, provided they allow efficient regularized empirical risk minimization.

Conclusion: Proposes a flexible, scalable approach for federated learning with heterogeneous models, leveraging existing libraries for implementation.

Abstract: We present a model-agnostic federated learning method for networks of
heterogeneous data and models. The network structure reflects similarities
between the (statistics of the) local datasets and, in turn, their associated
local (personal) models. Our method is an instance of empirical risk
minimization, with a regularization term derived from the network structure of
the data. In particular, we require well-connected local models, which form
clusters, to yield similar predictions on shared public, unlabelled dataset(s).
The proposed method allows for a wide range of local models. The only
restriction is that these local models must allow for efficient implementation
of regularized empirical risk minimization (training). For many models, such
implementations are readily available in high-level programming libraries,
including scikit-learn, Keras, and PyTorch.

</details>


### [683] [Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health Records via Multimodal Fused Transformer](https://arxiv.org/pdf/2502.07158)
*Jiaying Lu, Stephanie R. Brown, Songyuan Liu, Shifan Zhao, Kejun Dong, Del Bold, Michael Fundora, Alaa Aljiffry, Alex Fedorov, Jocelyn Grunwell, Xiao Hu*

Main category: cs.LG

TL;DR: PedCA-FT, a transformer-based framework, fuses EHR tabular and textual data to predict pediatric cardiac arrest, outperforming other AI models.


<details>
  <summary>Details</summary>
Motivation: Early prediction of pediatric cardiac arrest is crucial for timely intervention in intensive care settings.

Method: PedCA-FT uses dedicated transformer modules for EHR tabular and textual views to capture complex temporal and contextual patterns.

Result: Outperforms ten other AI models on five metrics and identifies clinically meaningful risk factors.

Conclusion: Multimodal fusion techniques like PedCA-FT can enhance early cardiac arrest detection and improve patient care.

Abstract: Early prediction of pediatric cardiac arrest (CA) is critical for timely
intervention in high-risk intensive care settings. We introduce PedCA-FT, a
novel transformer-based framework that fuses tabular view of EHR with the
derived textual view of EHR to fully unleash the interactions of
high-dimensional risk factors and their dynamics. By employing dedicated
transformer modules for each modality view, PedCA-FT captures complex temporal
and contextual patterns to produce robust CA risk estimates. Evaluated on a
curated pediatric cohort from the CHOA-CICU database, our approach outperforms
ten other artificial intelligence models across five key performance metrics
and identifies clinically meaningful risk factors. These findings underscore
the potential of multimodal fusion techniques to enhance early CA detection and
improve patient care.

</details>


### [684] [Performative Prediction: Past and Future](https://arxiv.org/pdf/2310.16608)
*Moritz Hardt, Celestine Mendler-Dünner*

Main category: cs.LG

TL;DR: The paper introduces performative prediction in machine learning, addressing how predictions influence behavior and data distributions, and explores its implications for learning, steering, and power in digital markets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine learning's static perspective and the dynamic, performative effects of predictions in social sciences, economics, and finance.

Method: Introduces performative prediction, a conceptual framework, and an equilibrium notion to study performativity in machine learning, distinguishing between learning and steering.

Result: Highlights the impact of performativity on data distribution shifts, introduces performative power, and discusses its role in digital markets.

Conclusion: Performativity is crucial for understanding algorithmic systems, with future research needed on its role in contesting such systems.

Abstract: Predictions in the social world generally influence the target of prediction,
a phenomenon known as performativity. Self-fulfilling and self-negating
predictions are examples of performativity. Of fundamental importance to
economics, finance, and the social sciences, the notion has been absent from
the development of machine learning that builds on the static perspective of
pattern recognition. In machine learning applications, however, performativity
often surfaces as distribution shift. A predictive model deployed on a digital
platform, for example, influences behavior and thereby changes the
data-generating distribution. We discuss the recently founded area of
performative prediction that provides a definition and conceptual framework to
study performativity in machine learning. A key element of performative
prediction is a natural equilibrium notion that gives rise to new optimization
challenges. What emerges is a distinction between learning and steering, two
mechanisms at play in performative prediction. Steering is in turn intimately
related to questions of power in digital markets. The notion of performative
power that we review gives an answer to the question how much a platform can
steer participants through its predictions. We end on a discussion of future
directions, such as the role that performativity plays in contesting
algorithmic systems.

</details>


### [685] [Conformal Convolution and Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects](https://arxiv.org/pdf/2402.04906)
*Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke*

Main category: cs.LG

TL;DR: The paper introduces two novel methods, CCT and CMC, for generating probabilistic forecasts of potential outcomes and ITEs, offering model-agnostic, universal solutions with finite-sample guarantees.


<details>
  <summary>Details</summary>
Motivation: Probabilistic forecasting of outcomes and ITEs is crucial for risk-aware decision-making in healthcare, policy, marketing, and finance. Existing methods lack universality and guarantees.

Method: The proposed methods, CCT and CMC, combine weighted conformal predictive systems with analytic convolution or Monte Carlo sampling, addressing covariate shift via propensity score weighting.

Result: Experiments show the methods achieve calibrated predictive distributions, narrow intervals, and performant scores, with efficiency gains over other conformal approaches.

Conclusion: The CCT and CMC meta-learners provide robust, calibrated, and efficient probabilistic forecasting for ITEs and potential outcomes, outperforming existing methods.

Abstract: Generating probabilistic forecasts of potential outcomes and individual
treatment effects (ITE) is essential for risk-aware decision-making in domains
such as healthcare, policy, marketing, and finance. We propose two novel
methods: the conformal convolution T-learner (CCT) and the conformal Monte
Carlo (CMC) meta-learner, that generate full predictive distributions of both
potential outcomes and ITEs. Our approaches combine weighted conformal
predictive systems with either analytic convolution of potential outcome
distributions or Monte Carlo sampling, addressing covariate shift through
propensity score weighting. In contrast to other approaches that allow the
generation of potential outcome predictive distributions, our approaches are
model agnostic, universal, and come with finite-sample guarantees of
probabilistic calibration under knowledge of the propensity score. Regarding
estimating the ITE distribution, we formally characterize how assumptions about
potential outcomes' noise dependency impact distribution validity and establish
universal consistency under independence noise assumptions. Experiments on
synthetic and semi-synthetic datasets demonstrate that the proposed methods
achieve probabilistically calibrated predictive distributions while maintaining
narrow prediction intervals and having performant continuous ranked probability
scores. Besides probabilistic forecasting performance, we observe significant
efficiency gains for the CCT- and CMC meta-learners compared to other conformal
approaches that produce prediction intervals for ITE with coverage guarantees.

</details>


### [686] [CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation](https://arxiv.org/pdf/2502.10940)
*Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang*

Main category: cs.LG

TL;DR: CoLA and CoLA-M replace full-size layers in LLMs with low-rank auto-encoders, reducing compute and memory costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The large model sizes of LLMs demand excessive computational resources, and pre-trained LLMs exhibit low-rank activations, suggesting redundancy.

Method: Proposes CoLA and CoLA-M, which use compute-efficient auto-encoders to enforce low-rank activations, replacing full-size layers.

Result: CoLA reduces computing cost by 2× and improves throughput by 1.86×. CoLA-M further cuts memory costs without throughput loss.

Conclusion: CoLA and CoLA-M offer superior efficiency in parameters, computing, and memory, enabling faster inference on resource-constrained platforms.

Abstract: The full-size MLPs and the projection layers in attention introduce
tremendous model sizes of large language models (LLMs), imposing extremely
demanding needs of computational resources in the pre-training stage. However,
we empirically observe that the activations of pre-trained LLMs exhibit
low-rank property. Motivated by such observations, we propose CoLA and its
memory-efficient implementation, CoLA-M, to replace these full-size layers with
compute-efficient auto-encoders that naturally enforce low-rank activations
throughout training. This fundamental architectural change eliminates the
activation redundancy and significantly boosts model capacity and training
efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters
show that CoLA reduces the computing cost by $\bf 2\pmb{\times}$ and improves
training throughput by $\bf 1.86\pmb{\times}$ while maintaining full-rank level
performance. CoLA-M further squeezes memory cost without sacrificing
throughput, offering a pre-training approach with collectively superior
parameter, computing, and memory efficiency. The LLMs produced are also $\bf
2\pmb{\times}$ smaller, enabling faster inference with lower memory cost on
resource-constrained platforms.

</details>


### [687] [Self Supervised Correlation-based Permutations for Multi-View Clustering](https://arxiv.org/pdf/2402.16383)
*Ran Eisenberg, Jonathan Svirsky, Ofir Lindenbaum*

Main category: cs.LG

TL;DR: Proposes an end-to-end deep learning framework for multi-view clustering, improving representation fusion and cluster assignment with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Current multi-view clustering methods are domain-specific or inefficient, requiring a two-stage process.

Method: Uses a permutation-based canonical correlation objective for representation fusion and consistent pseudo-labels for clustering.

Result: Theoretical analysis and experiments on ten benchmarks show the model's effectiveness.

Conclusion: The framework is general, efficient, and outperforms existing methods with theoretical support.

Abstract: Combining data from different sources can improve data analysis tasks such as
clustering. However, most of the current multi-view clustering methods are
limited to specific domains or rely on a suboptimal and computationally
intensive two-stage process of representation learning and clustering. We
propose an end-to-end deep learning-based multi-view clustering framework for
general data types (such as images and tables). Our approach involves
generating meaningful fused representations using a novel permutation-based
canonical correlation objective. We provide a theoretical analysis showing how
the learned embeddings approximate those obtained by supervised linear
discriminant analysis (LDA). Cluster assignments are learned by identifying
consistent pseudo-labels across multiple views. Additionally, we establish a
theoretical bound on the error caused by incorrect pseudo-labels in the
unsupervised representations compared to LDA. Extensive experiments on ten
multi-view clustering benchmark datasets provide empirical evidence for the
effectiveness of the proposed model.

</details>


### [688] [Uncovering Untapped Potential in Sample-Efficient World Model Agents](https://arxiv.org/pdf/2502.11537)
*Lior Cohen, Kaixin Wang, Bingyi Kang, Uri Gadot, Shie Mannor*

Main category: cs.LG

TL;DR: Simulus is a modular token-based world model agent that integrates multi-modality tokenization, intrinsic motivation, prioritized replay, and regression-as-classification, achieving state-of-the-art sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing token-based world models are limited to visual inputs and discrete actions, and intrinsic motivation and prioritized replay are underexplored in combination.

Method: Simulus combines modular multi-modality tokenization, intrinsic motivation, prioritized WM replay, and regression-as-classification for reward and return prediction.

Result: Simulus achieves state-of-the-art sample efficiency across three benchmarks, with ablation studies showing the synergy of its components.

Conclusion: Simulus demonstrates the effectiveness of combining modular tokenization, intrinsic motivation, and prioritized replay, with publicly available code and model weights.

Abstract: World model (WM) agents enable sample-efficient reinforcement learning by
learning policies entirely from simulated experience. However, existing
token-based world models (TBWMs) are limited to visual inputs and discrete
actions, restricting their adoption and applicability. Moreover, although both
intrinsic motivation and prioritized WM replay have shown promise in improving
WM performance and generalization, they remain underexplored in this setting,
particularly in combination. We introduce Simulus, a highly modular TBWM agent
that integrates (1) a modular multi-modality tokenization framework, (2)
intrinsic motivation, (3) prioritized WM replay, and (4)
regression-as-classification for reward and return prediction. Simulus achieves
state-of-the-art sample efficiency for planning-free WMs across three diverse
benchmarks. Ablation studies reveal the individual contribution of each
component while highlighting their synergy. Our code and model weights are
publicly available at https://github.com/leor-c/Simulus.

</details>


### [689] [Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space](https://arxiv.org/pdf/2403.00155)
*Mahsa Mozafari-Nia, Salimeh Yasaei Sekeh*

Main category: cs.LG

TL;DR: A theoretical framework for DNN compression is proposed, explaining optimal sparsity using information-theoretic divergence and introducing AP2/AP3 concepts, validated on benchmarks like AlexNet and ResNet.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical explanation for DNN compression techniques like pruning and low-rank decomposition.

Method: Leverages a probabilistic latent space of DNN weights, introduces AP2/AP3 notions, and analyzes their relationship with network performance.

Result: Theoretical framework validated on benchmarks, showing AP3/AP2 properties relate to fine-tuning pruned DNNs and sparsity levels.

Conclusion: The study provides a theoretical foundation for DNN compression, linking AP3/AP2 properties to performance and sparsity.

Abstract: Despite the impressive performance of deep neural networks (DNNs), their
computational complexity and storage space consumption have led to the concept
of network compression. While DNN compression techniques such as pruning and
low-rank decomposition have been extensively studied, there has been
insufficient attention paid to their theoretical explanation. In this paper, we
propose a novel theoretical framework that leverages a probabilistic latent
space of DNN weights and explains the optimal network sparsity by using the
information-theoretic divergence measures. We introduce new analogous projected
patterns (AP2) and analogous-in-probability projected patterns (AP3) notions
for DNNs and prove that there exists a relationship between AP3/AP2 property of
layers in the network and its performance. Further, we provide a theoretical
analysis that explains the training process of the compressed network. The
theoretical results are empirically validated through experiments conducted on
standard pre-trained benchmarks, including AlexNet, ResNet50, and VGG16, using
CIFAR10 and CIFAR100 datasets. Through our experiments, we highlight the
relationship of AP3 and AP2 properties with fine-tuning pruned DNNs and
sparsity levels.

</details>


### [690] [A General Reduction for High-Probability Analysis with General Light-Tailed Distributions](https://arxiv.org/pdf/2403.02873)
*Amit Attia, Tomer Koren*

Main category: cs.LG

TL;DR: A reduction technique simplifies analyzing learning algorithms with light-tailed randomness by reducing them to bounded variants, applicable to various distributions without specialized inequalities.


<details>
  <summary>Details</summary>
Motivation: To simplify the analysis of learning algorithms under light-tailed randomness, avoiding the need for specialized concentration inequalities.

Method: Proposes a black-box reduction technique to analyze algorithms with light-tailed randomness by simplifying them to bounded variants.

Result: Demonstrates applicability to various distributions (e.g., exponential, sub-Gaussian) and provides examples like Azuma inequality and bandit regret analysis.

Conclusion: The technique offers a versatile and simplified approach for analyzing algorithms with light-tailed randomness.

Abstract: We describe a general reduction technique for analyzing learning algorithms
that are subject to light-tailed (but not necessarily bounded) randomness, a
scenario that is often the focus of theoretical analysis. We show that the
analysis of such an algorithm can be reduced, in a black-box manner and with
only a small loss in logarithmic factors, to an analysis of a simpler variant
of the same algorithm that uses bounded random variables and is often easier to
analyze. This approach simultaneously applies to any light-tailed
randomization, including exponential, sub-Gaussian, and more general
fast-decaying distributions, without needing to appeal to specialized
concentration inequalities. Derivations of a generalized Azuma inequality,
convergence bounds in stochastic optimization, and regret analysis in
multi-armed bandits with general light-tailed randomization are provided to
illustrate the technique.

</details>


### [691] [EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking](https://arxiv.org/pdf/2502.12466)
*Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, Alex Aiken*

Main category: cs.LG

TL;DR: EquiBench is a benchmark for evaluating LLMs' understanding of program execution semantics through equivalence checking, revealing limitations in their reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs truly understand code execution semantics, as current benchmarks focus on code generation rather than semantic understanding.

Method: Introduces EquiBench, a benchmark with 2400 program pairs across four languages and six categories, generated via program analysis and transformations.

Result: Best LLM accuracies are 63.8% and 76.2% in challenging categories, barely above random chance, with models often relying on syntax over semantics.

Conclusion: LLMs exhibit limited understanding of execution semantics, relying on syntactic cues, indicating fundamental gaps in their reasoning capabilities.

Abstract: As large language models (LLMs) become integral to code-related tasks, a
central question emerges: do LLMs truly understand program execution semantics?
We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence
checking, i.e., determining whether two programs produce identical outputs for
all possible inputs. Unlike prior code generation benchmarks, this task
directly tests a model's understanding of code execution semantics. EquiBench
consists of 2400 program pairs across four languages and six categories. These
pairs are generated through program analysis, compiler scheduling, and
superoptimization, ensuring high-confidence labels, nontrivial difficulty, and
full automation. The transformations span syntactic edits, structural
modifications, and algorithmic changes, covering a broad spectrum of semantic
variation. We evaluate 19 state-of-the-art LLMs and find that in the most
challenging categories, the best accuracies are 63.8% and 76.2%, only modestly
above the 50% random baseline. Further analysis reveals that models often rely
on syntactic similarity rather than exhibiting robust reasoning over execution
semantics, highlighting fundamental limitations.

</details>


### [692] [Learning to Discretize Denoising Diffusion ODEs](https://arxiv.org/pdf/2405.15506)
*Vinh Tong, Hoang Trung-Dung, Anji Liu, Guy Van den Broeck, Mathias Niepert*

Main category: cs.LG

TL;DR: LD3 is a lightweight framework to optimize time discretization for sampling in Diffusion Probabilistic Models (DPMs), reducing computational costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: DPMs require multiple neural function evaluations (NFEs) for sampling, leading to high computational costs compared to single-step models like GANs or VAEs. Reducing NFEs without quality loss is essential.

Method: Proposes LD3, a framework to learn optimal time discretization for sampling, compatible with various samplers and avoiding retraining of neural networks.

Result: LD3 improves sampling efficiency with minimal overhead, achieving FIDs of 2.38 (10 NFE) on CIFAR10 and 2.27 (10 NFE) on AFHQv2 in 5-10 minutes of training.

Conclusion: LD3 provides an efficient sampling method for pre-trained DPMs, balancing quality and computational cost.

Abstract: Diffusion Probabilistic Models (DPMs) are generative models showing
competitive performance in various domains, including image synthesis and 3D
point cloud generation. Sampling from pre-trained DPMs involves multiple neural
function evaluations (NFEs) to transform Gaussian noise samples into images,
resulting in higher computational costs compared to single-step generative
models such as GANs or VAEs. Therefore, reducing the number of NFEs while
preserving generation quality is crucial. To address this, we propose LD3, a
lightweight framework designed to learn the optimal time discretization for
sampling. LD3 can be combined with various samplers and consistently improves
generation quality without having to retrain resource-intensive neural
networks. We demonstrate analytically and empirically that LD3 improves
sampling efficiency with much less computational overhead. We evaluate our
method with extensive experiments on 7 pre-trained models, covering
unconditional and conditional sampling in both pixel-space and latent-space
DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional
CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient
approach to sampling from pre-trained diffusion models. Code is available at
https://github.com/vinhsuhi/LD3.

</details>


### [693] [An Empirical Analysis of Forgetting in Pre-trained Models with Incremental Low-Rank Updates](https://arxiv.org/pdf/2405.18069)
*Albin Soutif--Cormerais, Simone Magistri, Joost van de Weijer, Andew D. Bagdanov*

Main category: cs.LG

TL;DR: The paper explores the impact of LoRA rank on forgetting in continual learning when merging LoRA with pretrained model weights, noting its significance for both pretraining and downstream tasks, and observes unique "contextual" forgetting in vision transformers.


<details>
  <summary>Details</summary>
Motivation: To investigate the effects of merging Low-Rank Adaptation (LoRA) with pretrained model weights in continual learning, focusing on forgetting behaviors in pretraining and downstream tasks.

Method: Study the impact of LoRA rank by finetuning pretrained models on small datasets, merging LoRA weights after each task, and analyzing forgetting in pretraining and subsequent tasks.

Result: LoRA rank significantly affects forgetting in both pretraining and downstream tasks. Vision transformers exhibit "contextual" forgetting, unlike residual networks.

Conclusion: The findings highlight the importance of LoRA rank in continual learning and reveal novel forgetting behaviors in vision transformers, suggesting further research into model-specific forgetting patterns.

Abstract: Broad, open source availability of large pretrained foundation models on the
internet through platforms such as HuggingFace has taken the world of practical
deep learning by storm. A classical pipeline for neural network training now
typically consists of finetuning these pretrained network on a small target
dataset instead of training from scratch. In the case of large models this can
be done even on modest hardware using a low rank training technique known as
Low-Rank Adaptation (LoRA). While Low Rank training has already been studied in
the continual learning setting, existing works often consider storing the
learned adapter along with the existing model but rarely attempt to modify the
weights of the pretrained model by merging the LoRA with the existing weights
after finishing the training of each task. In this article we investigate this
setting and study the impact of LoRA rank on the forgetting of the pretraining
foundation task and on the plasticity and forgetting of subsequent ones. We
observe that this rank has an important impact on forgetting of both the
pretraining and downstream tasks. We also observe that vision transformers
finetuned in that way exhibit a sort of ``contextual'' forgetting, a behaviour
that we do not observe for residual networks and that we believe has not been
observed yet in previous continual learning works.

</details>


### [694] [How Out-of-Distribution Detection Learning Theory Enhances Transformer: Learnability and Reliability](https://arxiv.org/pdf/2406.12915)
*Yijin Zhou, Yutang Ge, Xiaowen Dong, Yuguang Wang*

Main category: cs.LG

TL;DR: The paper introduces a PAC theory for OOD detection in transformers, proposing a method to enhance reliability by leveraging auxiliary outliers and synthetic data.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with OOD generalization, and this paper aims to establish theoretical conditions for OOD detection learnability in transformers.

Method: The approach involves penalizing outlier misclassification in the loss function and generating soft synthetic outliers to refine decision boundaries.

Result: The proposed algorithm achieves state-of-the-art performance across diverse data formats.

Conclusion: The theory and method provide a robust framework for improving OOD detection in transformers, balancing theoretical principles with practical training.

Abstract: Transformers excel in natural language processing and computer vision tasks.
However, they still face challenges in generalizing to Out-of-Distribution
(OOD) datasets, i.e. data whose distribution differs from that seen during
training. OOD detection aims to distinguish outliers while preserving
in-distribution (ID) data performance. This paper introduces the OOD detection
Probably Approximately Correct (PAC) Theory for transformers, which establishes
the conditions for data distribution and model configurations for the OOD
detection learnability of transformers. It shows that outliers can be
accurately represented and distinguished with sufficient data under conditions.
The theoretical implications highlight the trade-off between theoretical
principles and practical training paradigms. By examining this trade-off, we
naturally derived the rationale for leveraging auxiliary outliers to enhance
OOD detection. Our theory suggests that by penalizing the misclassification of
outliers within the loss function and strategically generating soft synthetic
outliers, one can robustly bolster the reliability of transformer networks.
This approach yields a novel algorithm that ensures learnability and refines
the decision boundaries between inliers and outliers. In practice, the
algorithm consistently achieves state-of-the-art (SOTA) performance across
various data formats.

</details>


### [695] [Randomness Helps Rigor: A Probabilistic Learning Rate Scheduler Bridging Theory and Deep Learning Practice](https://arxiv.org/pdf/2407.07613)
*Dahlia Devapriya, Thulasi Tholeti, Janani Suresh, Sheetal Kalyani*

Main category: cs.LG

TL;DR: A probabilistic learning rate scheduler (PLRS) with provable convergence guarantees is proposed, outperforming existing schedulers in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: The gap between practical success and theoretical proof of convergence for learning rate schedulers motivated the development of PLRS.

Method: PLRS is introduced, which allows non-monotonic learning rates and is tested on deep neural networks (ResNet, WRN, VGG, DenseNet) across datasets (CIFAR-10, CIFAR-100, Tiny ImageNet).

Result: PLRS matches or exceeds state-of-the-art schedulers in convergence and accuracy, e.g., outperforming the knee scheduler by 1.56% on CIFAR-100 with ResNet-110.

Conclusion: PLRS bridges the theory-practice gap, offering provable convergence and superior performance in modern deep learning applications.

Abstract: Learning rate schedulers have shown great success in speeding up the
convergence of learning algorithms in practice. However, their convergence to a
minimum has not been proven theoretically. This difficulty mainly arises from
the fact that, while traditional convergence analysis prescribes to
monotonically decreasing (or constant) learning rates, schedulers opt for rates
that often increase and decrease through the training epochs. In this work, we
aim to bridge the gap by proposing a probabilistic learning rate scheduler
(PLRS) that does not conform to the monotonically decreasing condition, with
provable convergence guarantees. To cement the relevance and utility of our
work in modern day applications, we show experimental results on deep neural
network architectures such as ResNet, WRN, VGG, and DenseNet on CIFAR-10,
CIFAR-100, and Tiny ImageNet datasets. We show that PLRS performs as well as or
better than existing state-of-the-art learning rate schedulers in terms of
convergence as well as accuracy. For example, while training ResNet-110 on the
CIFAR-100 dataset, we outperform the state-of-the-art knee scheduler by
$1.56\%$ in terms of classification accuracy. Furthermore, on the Tiny ImageNet
dataset using ResNet-50 architecture, we show a significantly more stable
convergence than the cosine scheduler and a better classification accuracy than
the existing schedulers.

</details>


### [696] [Counting in Small Transformers: The Delicate Interplay between Attention and Feed-Forward Layers](https://arxiv.org/pdf/2407.11542)
*Freya Behrens, Luca Biggio, Lenka Zdeborová*

Main category: cs.LG

TL;DR: The paper explores how transformer architectures handle the histogram task, identifying two counting strategies and showing how design choices affect performance.


<details>
  <summary>Details</summary>
Motivation: To understand how architectural choices in transformers shape their solutions for simple tasks like counting items in sequences.

Method: Analyze transformer blocks on the histogram task, examining predictive performance, vocabulary size, embedding dimensions, token-mixing, and feed-forward layers. Identify two counting strategies: relation-based and inventory-based.

Result: Found that design choices (e.g., softmax, BOS tokens) improve robustness with small embeddings. Empirical results confirm the strategies and their formation during training.

Conclusion: Minor design changes significantly impact performance on basic tasks, revealing the interplay between architecture and functionality.

Abstract: Next to scaling considerations, architectural design choices profoundly shape
the solution space of transformers. In this work, we analyze the solutions
simple transformer blocks implement when tackling the histogram task: counting
items in sequences. Despite its simplicity, this task reveals a complex
interplay between predictive performance, vocabulary and embedding sizes,
token-mixing mechanisms, and feed-forward layer capacity. We identify two
theoretical counting strategies transformers adopt, relation-based and
inventory-based counting, each defining distinct learning regimes for the task.
These strategies dictate how functionality is distributed between attention and
feed-forward layers. We further show that adding softmax and
beginning-of-sequence tokens allow for more robustness when embedding
dimensions are comparatively small. Empirical introspection of trained models
closely confirms both the learning regimes of the various architectures and the
formation of these strategies during training. We demonstrate how a basic task
that requires only aggregation and selection is significantly impacted by minor
design changes.

</details>


### [697] [On the Vulnerability of Concept Erasure in Diffusion Models](https://arxiv.org/pdf/2502.17537)
*Lucas Beerens, Alex D. Richardson, Kaicheng Zhang, Dongdong Chen*

Main category: cs.LG

TL;DR: RECORD is a new algorithm for restoring erased concepts in text-to-image models, outperforming existing methods by up to 17.8 times and revealing vulnerabilities in unlearned models.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current concept restoration methods, which only work with fixed seeds, to improve adversarial prompt effectiveness.

Method: Introduces RECORD, a coordinate-descent-based algorithm for finding adversarial prompts independent of seed values.

Result: RECORD outperforms existing methods by up to 17.8 times and exposes vulnerabilities in unlearned models.

Conclusion: RECORD advances concept restoration, highlighting the need for stronger defenses in text-to-image models.

Abstract: The proliferation of text-to-image diffusion models has raised significant
privacy and security concerns, particularly regarding the generation of
copyrighted or harmful images. In response, several concept erasure (defense)
methods have been developed to prevent the generation of unwanted content
through post-hoc finetuning. On the other hand, concept restoration (attack)
methods seek to recover supposedly erased concepts via adversarially crafted
prompts. However, all existing restoration methods only succeed in the highly
restrictive scenario of finding adversarial prompts tailed to some fixed seed.
To address this, we introduce RECORD, a novel coordinate-descent-based
restoration algorithm that finds adversarial prompts to recover erased concepts
independently of the seed. Our extensive experiments demonstrate RECORD
consistently outperforms the current restoration methods by up to 17.8 times in
this setting. Our findings further reveal the susceptibility of unlearned
models to restoration attacks, providing crucial insights into the behavior of
unlearned models under the influence of adversarial prompts.

</details>


### [698] [Towards the Causal Complete Cause of Multi-Modal Representation Learning](https://arxiv.org/pdf/2407.14058)
*Jingyao Wang, Siyu Zhao, Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Fuchun Sun, Hui Xiong*

Main category: cs.LG

TL;DR: The paper proposes a causal perspective for Multi-Modal Learning (MML), introducing Causal Complete Cause ($C^3$) to ensure representations are causally sufficient and necessary. It relaxes prior assumptions, measures $C^3$ risk, and introduces $C^3$ Regularization for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing MML methods may produce representations with insufficient or unnecessary information due to ignoring causal sufficiency and necessity. The paper aims to address this gap.

Method: Defines $C^3$, discusses its identifiability, and uses an instrumental variable for estimation. Introduces a twin network for $C^3$ risk measurement (real-world and hypothetical-world branches) and proposes $C^3$ Regularization.

Result: Theoretical analyses confirm reliability, and experiments show the effectiveness of $C^3$ Regularization in improving MML representations.

Conclusion: The paper successfully integrates causal principles into MML, demonstrating that $C^3$-based methods enhance representation quality and prediction accuracy.

Abstract: Multi-Modal Learning (MML) aims to learn effective representations across
modalities for accurate predictions. Existing methods typically focus on
modality consistency and specificity to learn effective representations.
However, from a causal perspective, they may lead to representations that
contain insufficient and unnecessary information. To address this, we propose
that effective MML representations should be causally sufficient and necessary.
Considering practical issues like spurious correlations and modality conflicts,
we relax the exogeneity and monotonicity assumptions prevalent in prior works
and explore the concepts specific to MML, i.e., Causal Complete Cause $C^3$. We
begin by defining $C^3$, which quantifies the probability of representations
being causally sufficient and necessary. We then discuss the identifiability of
$C^3$ and introduce an instrumental variable to support identifying $C^3$ with
non-exogeneity and non-monotonicity. Building on this, we conduct the $C^3$
measurement, i.e., \(C^3\) risk. We propose a twin network to estimate it
through (i) the real-world branch: utilizing the instrumental variable for
sufficiency, and (ii) the hypothetical-world branch: applying gradient-based
counterfactual modeling for necessity. Theoretical analyses confirm its
reliability. Based on these results, we propose $C^3$ Regularization, a
plug-and-play method that enforces the causal completeness of the learned
representations by minimizing $C^3$ risk. Extensive experiments demonstrate its
effectiveness.

</details>


### [699] [Making Robust Generalizers Less Rigid with Loss Concentration](https://arxiv.org/pdf/2408.03619)
*Matthew J. Holland, Toma Hamada*

Main category: cs.LG

TL;DR: The paper highlights the limitations of sharpness-aware minimization for robust generalization in simpler models and proposes a new training criterion focusing on loss concentration.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning focuses on average performance, but robust generalization on rare or difficult data points is often more important. Existing methods work well for overparameterized networks but fail in simpler models.

Method: The authors propose a training criterion penalizing poor loss concentration, compatible with loss transformations like exponential tilting, CVaR, or DRO.

Result: The study demonstrates the breakdown of sharpness-aware methods in simpler models and the effectiveness of the proposed criterion.

Conclusion: The new criterion offers a flexible and effective alternative for robust generalization, especially in models with extreme difficulty gaps.

Abstract: While the traditional formulation of machine learning tasks is in terms of
performance on average, in practice we are often interested in how well a
trained model performs on rare or difficult data points at test time. To
achieve more robust and balanced generalization, methods applying
sharpness-aware minimization to a subset of worst-case examples have proven
successful for image classification tasks, but only using overparameterized
neural networks under which the relative difference between "easy" and "hard"
data points becomes negligible. In this work, we show how such a strategy can
dramatically break down under simpler models where the difficulty gap becomes
more extreme. As a more flexible alternative, instead of typical sharpness, we
propose and evaluate a training criterion which penalizes poor loss
concentration, which can be easily combined with loss transformations such
exponential tilting, conditional value-at-risk (CVaR), or distributionally
robust optimization (DRO) that control tail emphasis.

</details>


### [700] [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/pdf/2503.10542)
*Arvid Frydenlund*

Main category: cs.LG

TL;DR: Decoder-only LMs fail at the path-star task due to a learned shortcut from excess supervision, but solutions show the task is solvable.


<details>
  <summary>Details</summary>
Motivation: To understand why decoder-only LMs struggle with the minimal path-star task and explore solutions.

Method: Analyze the task's minimal nature, identify the learned shortcut issue, and propose solutions to demonstrate solvability.

Result: Decoder-only LMs fail above chance due to excess supervision, but solutions confirm the task is solvable.

Conclusion: The task's minimal nature causes difficulty, but insights into the pathology help understand LM limitations.

Abstract: This work concerns the path-star task, a minimal example of searching over a
graph. The graph, $G$, is star-shaped with $D$ arms radiating from a start
node, $s$. A language model (LM) is given $G$, $s$, and a target node $t$,
which ends one of the arms and is tasked with generating the arm containing
$t$. The minimal nature of this task means only a single choice needs to be
made: which of the $D$ arms contains $t$?
  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to
a learned shortcut that absorbs training supervision. We show how this
pathology is caused by excess supervision and we present a series of solutions
demonstrating that the task is solvable via decoder-only LMs. We find that the
task's minimal nature causes its difficulty, as it prevents task decomposition.
Our solutions provide insight into the pathology and its implications for LMs
trained via next-token prediction.

</details>


### [701] [Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants](https://arxiv.org/pdf/2409.06998)
*Gangda Deng, Hongkuan Zhou, Rajgopal Kannan, Viktor Prasanna*

Main category: cs.LG

TL;DR: The paper addresses the challenge of heterophilous graphs for GNNs, proposing Moscat to improve deeper GNN generalization while maintaining expressivity.


<details>
  <summary>Details</summary>
Motivation: Heterophilous graphs challenge GNNs, and deeper GNNs suffer performance degradation despite their potential for broader scope.

Method: Proposes Moscat, a mixture of scope experts at test, to enhance deeper GNN generalization.

Result: Moscat significantly improves accuracy across various GNNs and datasets.

Conclusion: Moscat effectively addresses the generalization disparity in deeper GNNs, offering flexible and accurate solutions.

Abstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a
challenge for graph neural networks (GNNs). Increasing the GNN depth can expand
the scope (i.e., receptive field), potentially finding homophily from the
higher-order neighborhoods. However, GNNs suffer from performance degradation
as depth increases. Despite having better expressivity, state-of-the-art deeper
GNNs achieve only marginal improvements compared to their shallow variants.
Through theoretical and empirical analysis, we systematically demonstrate a
shift in GNN generalization preferences across nodes with different homophily
levels as depth increases. This creates a disparity in generalization patterns
between GNN models with varying depth. Based on these findings, we propose to
improve deeper GNN generalization while maintaining high expressivity by
Mixture of scope experts at test (Moscat). Experimental results show that
Moscat works flexibly with various GNNs across a wide range of datasets while
significantly improving accuracy. Our code is available at
(https://github.com/Hydrapse/moscat).

</details>


### [702] [Learning Utilities from Demonstrations in Markov Decision Processes](https://arxiv.org/pdf/2409.17355)
*Filippo Lazzati, Alberto Maria Metelli*

Main category: cs.LG

TL;DR: The paper introduces a Utility Learning (UL) model to infer an agent's risk attitude from demonstrations in MDPs, addressing limitations of risk-neutral IRL models.


<details>
  <summary>Details</summary>
Motivation: Existing IRL models assume risk-neutral agents, missing risk-sensitive behaviors common in humans. This gap limits their applicability.

Method: Proposes a UL model using a utility function to represent risk attitude, defines the UL problem, and develops two efficient algorithms for finite-data regimes.

Result: The paper analyzes the partial identifiability of utility functions and provides sample complexity bounds for the proposed algorithms.

Conclusion: Proof-of-concept experiments validate the UL model and algorithms, demonstrating their effectiveness in capturing risk attitudes.

Abstract: Our goal is to extract useful knowledge from demonstrations of behavior in
sequential decision-making problems. Although it is well-known that humans
commonly engage in risk-sensitive behaviors in the presence of stochasticity,
most Inverse Reinforcement Learning (IRL) models assume a risk-neutral agent.
Beyond introducing model misspecification, these models do not directly capture
the risk attitude of the observed agent, which can be crucial in many
applications. In this paper, we propose a novel model of behavior in Markov
Decision Processes (MDPs) that explicitly represents the agent's risk attitude
through a utility function. We then define the Utility Learning (UL) problem as
the task of inferring the observed agent's risk attitude, encoded via a utility
function, from demonstrations in MDPs, and we analyze the partial
identifiability of the agent's utility. Furthermore, we devise two provably
efficient algorithms for UL in a finite-data regime, and we analyze their
sample complexity. We conclude with proof-of-concept experiments that
empirically validate both our model and our algorithms.

</details>


### [703] [Conformal Prediction: A Theoretical Note and Benchmarking Transductive Node Classification in Graphs](https://arxiv.org/pdf/2409.18332)
*Pranav Maneriker, Aditya T. Vadlamani, Anutam Srinivasan, Yuntian He, Ali Payani, Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: The paper analyzes design choices in graph conformal prediction, introduces scalable techniques, and provides recommendations for future work.


<details>
  <summary>Details</summary>
Motivation: To address conflicting implementations and evaluation methods in graph conformal prediction and improve scalability.

Method: Analyzes existing literature, discusses tradeoffs, and introduces scalable techniques for large-scale graphs.

Result: Theoretical and empirical results justify recommendations for future research.

Conclusion: The work provides insights and scalable solutions for advancing graph conformal prediction.

Abstract: Conformal prediction has become increasingly popular for quantifying the
uncertainty associated with machine learning models. Recent work in graph
uncertainty quantification has built upon this approach for conformal graph
prediction. The nascent nature of these explorations has led to conflicting
choices for implementations, baselines, and method evaluation. In this work, we
analyze the design choices made in the literature and discuss the tradeoffs
associated with existing methods. Building on the existing implementations, we
introduce techniques to scale existing methods to large-scale graph datasets
without sacrificing performance. Our theoretical and empirical results justify
our recommendations for future scholarship in graph conformal prediction.

</details>


### [704] [EntryPrune: Neural Network Feature Selection using First Impressions](https://arxiv.org/pdf/2410.02344)
*Felix Zimmer, Patrik Okanovic, Torsten Hoefler*

Main category: cs.LG

TL;DR: EntryPrune is a novel feature selection algorithm using dynamic sparse input layers in neural networks, outperforming state-of-the-art methods in accuracy and runtime.


<details>
  <summary>Details</summary>
Motivation: Improve interpretability, reduce computational resources, and minimize overfitting in predictive models by leveraging neural networks for feature selection.

Method: Uses a dense neural network with a dynamic sparse input layer and entry-based pruning, comparing neurons based on their relative change upon entering the network.

Result: Outperforms state-of-the-art methods on 13 datasets, especially in low-dimensional settings, and achieves lower runtime than competitors.

Conclusion: EntryPrune is an effective and efficient feature selection tool, surpassing traditional techniques like magnitude pruning.

Abstract: There is an ongoing effort to develop feature selection algorithms to improve
interpretability, reduce computational resources, and minimize overfitting in
predictive models. Neural networks stand out as architectures on which to build
feature selection methods, and recently, neuron pruning and regrowth have
emerged from the sparse neural network literature as promising new tools. We
introduce EntryPrune, a novel supervised feature selection algorithm using a
dense neural network with a dynamic sparse input layer. It employs entry-based
pruning, a novel approach that compares neurons based on their relative change
induced when they have entered the network. Extensive experiments on 13
different datasets show that our approach generally outperforms the current
state-of-the-art methods, and in particular improves the average accuracy on
low-dimensional datasets. Furthermore, we show that EntryPruning surpasses
traditional techniques such as magnitude pruning within the EntryPrune
framework and that EntryPrune achieves lower runtime than competing approaches.
Our code is available at https://github.com/flxzimmer/entryprune.

</details>


### [705] [Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding](https://arxiv.org/pdf/2504.01281)
*Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana*

Main category: cs.LG

TL;DR: A framework for improving RAG systems with dynamic retrieval and reinforcement fine-tuning, enhancing accuracy and efficiency in knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional RAG systems by improving retrieval strategies and fine-tuning, ensuring better factual accuracy and response quality.

Method: Combines Policy-Optimized Retrieval-Augmented Generation (PORAG) and Adaptive Token-Layer Attention Scoring (ATLAS) for optimized retrieval and contextual relevance. Introduces CRITIC for memory efficiency and test-time scaling for resource balance.

Result: Reduces hallucinations, improves reasoning, and achieves efficiency and scalability gains over traditional RAG systems.

Conclusion: The framework advances robust, efficient, and scalable RAG systems for diverse applications.

Abstract: We present a comprehensive framework for enhancing Retrieval-Augmented
Generation (RAG) systems through dynamic retrieval strategies and reinforcement
fine-tuning. This approach significantly improves large language models on
knowledge-intensive tasks, including opendomain question answering and complex
reasoning. Our framework integrates two complementary techniques:
Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use
of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),
which dynamically determines retrieval timing and content based on contextual
needs. Together, these techniques enhance both the utilization and relevance of
retrieved content, improving factual accuracy and response quality. Designed as
a lightweight solution compatible with any Transformer-based LLM without
requiring additional training, our framework excels in knowledge-intensive
tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a
novel method to selectively compress key-value caches by token importance,
mitigating memory bottlenecks in long-context applications. The framework also
incorporates test-time scaling techniques to dynamically balance reasoning
depth and computational resources, alongside optimized decoding strategies for
faster inference. Experiments on benchmark datasets show that our framework
reduces hallucinations, strengthens domain-specific reasoning, and achieves
significant efficiency and scalability gains over traditional RAG systems. This
integrated approach advances the development of robust, efficient, and scalable
RAG systems across diverse applications.

</details>


### [706] [A Comprehensive Framework for Analyzing the Convergence of Adam: Bridging the Gap with SGD](https://arxiv.org/pdf/2410.04458)
*Ruinan Jin, Xiao Li, Yaoliang Yu, Baoxiang Wang*

Main category: cs.LG

TL;DR: The paper introduces a new framework for analyzing Adam's convergence, proving it converges under relaxed assumptions similar to SGD.


<details>
  <summary>Details</summary>
Motivation: Adam's theoretical convergence has been limited by restrictive assumptions, unlike SGD. This work aims to bridge that gap.

Method: A novel framework is developed to analyze Adam's convergence under relaxed assumptions (L-smoothness and ABC inequality).

Result: Adam achieves asymptotic convergence and non-asymptotic sample complexity bounds comparable to SGD under the same assumptions.

Conclusion: The study provides a more flexible and comprehensive understanding of Adam's convergence, aligning it with SGD's theoretical foundations.

Abstract: Adaptive Moment Estimation (Adam) is a cornerstone optimization algorithm in
deep learning, widely recognized for its flexibility with adaptive learning
rates and efficiency in handling large-scale data. However, despite its
practical success, the theoretical understanding of Adam's convergence has been
constrained by stringent assumptions, such as almost surely bounded stochastic
gradients or uniformly bounded gradients, which are more restrictive than those
typically required for analyzing stochastic gradient descent (SGD).
  In this paper, we introduce a novel and comprehensive framework for analyzing
the convergence properties of Adam. This framework offers a versatile approach
to establishing Adam's convergence. Specifically, we prove that Adam achieves
asymptotic (last iterate sense) convergence in both the almost sure sense and
the \(L_1\) sense under the relaxed assumptions typically used for SGD, namely
\(L\)-smoothness and the ABC inequality. Meanwhile, under the same assumptions,
we show that Adam attains non-asymptotic sample complexity bounds similar to
those of SGD.

</details>


### [707] [Mirror Bridges Between Probability Measures](https://arxiv.org/pdf/2410.07003)
*Leticia Mattos Da Silva, Silvia Sellán, Francisco Vargas, Justin Solomon*

Main category: cs.LG

TL;DR: The paper introduces 'mirror bridges' for conditional resampling, leveraging the Schrödinger bridge problem to generate in-distribution variations of input samples efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of resampling from a target measure with unknown density, particularly for generating conditioned or proximate samples.

Method: Proposes solving the Schrödinger bridge problem between a distribution and itself to estimate conditional distributions, with theoretical guarantees on accuracy.

Result: Demonstrates algorithmic simplifications and control over in-distribution variation, validated empirically across domains.

Conclusion: Mirror bridges offer an efficient and theoretically grounded solution for conditional resampling, outperforming existing methods.

Abstract: Resampling from a target measure whose density is unknown is a fundamental
problem in mathematical statistics and machine learning. A setting that
dominates the machine learning literature consists of learning a map from an
easy-to-sample prior, such as the Gaussian distribution, to a target measure.
Under this model, samples from the prior are pushed forward to generate a new
sample on the target measure, which is often difficult to sample from directly.
Of particular interest is the problem of generating a new sample that is
proximate to or otherwise conditioned on a given input sample. In this paper,
we propose a new model called mirror bridges to solve this problem of
conditional resampling. Our key observation is that solving the Schr\"odinger
bridge problem between a distribution and itself provides a natural way to
produce new samples from conditional distributions, giving in-distribution
variations of an input data point. We demonstrate how to efficiently estimate
the solution to this largely overlooked version of the Schr\"odinger bridge
problem, and we prove that under mild conditions, the difference between our
estimate and the true Schr\"odinger bridge can be controlled explicitly. We
show that our proposed method leads to significant algorithmic simplifications
over existing alternatives, in addition to providing control over
in-distribution variation. Empirically, we demonstrate how these benefits can
be leveraged to produce proximal samples in a number of application domains.

</details>


### [708] [Diversity-Aware Reinforcement Learning for de novo Drug Design](https://arxiv.org/pdf/2410.10431)
*Hampus Gummesson Svensson, Christian Tyrchan, Ola Engkvist, Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: The paper explores adaptive reward function updates in fine-tuning generative models for drug molecule generation, focusing on diversity and avoiding local optima.


<details>
  <summary>Details</summary>
Motivation: Current methods lack adaptive reward updates, leading to local optima and limited diversity in generated molecules, which may not be clinically useful.

Method: Investigates intrinsic motivation methods and strategies to penalize extrinsic rewards, testing their impact on molecular diversity.

Result: Combining structure- and prediction-based methods improves diversity in generated molecules.

Conclusion: Adaptive reward mechanisms, especially hybrid approaches, enhance diversity and avoid local optima in drug molecule generation.

Abstract: Fine-tuning a pre-trained generative model has demonstrated good performance
in generating promising drug molecules. The fine-tuning task is often
formulated as a reinforcement learning problem, where previous methods
efficiently learn to optimize a reward function to generate potential drug
molecules. Nevertheless, in the absence of an adaptive update mechanism for the
reward function, the optimization process can become stuck in local optima. The
efficacy of the optimal molecule in a local optimization may not translate to
usefulness in the subsequent drug optimization process or as a potential
standalone clinical candidate. Therefore, it is important to generate a diverse
set of promising molecules. Prior work has modified the reward function by
penalizing structurally similar molecules, primarily focusing on finding
molecules with higher rewards. To date, no study has comprehensively examined
how different adaptive update mechanisms for the reward function influence the
diversity of generated molecules. In this work, we investigate a wide range of
intrinsic motivation methods and strategies to penalize the extrinsic reward,
and how they affect the diversity of the set of generated molecules. Our
experiments reveal that combining structure- and prediction-based methods
generally yields better results in terms of diversity.

</details>


### [709] [AutoAL: Automated Active Learning with Differentiable Query Strategy Search](https://arxiv.org/pdf/2410.13853)
*Yifeng Wang, Xueying Zhan, Siyu Huang*

Main category: cs.LG

TL;DR: AutoAL introduces a differentiable active learning (AL) strategy search method to efficiently select the best AL algorithm for a given task, reducing labeling costs and improving model accuracy.


<details>
  <summary>Details</summary>
Motivation: Labeling large datasets is costly and time-consuming; existing AL algorithms vary in effectiveness across tasks, making it hard to choose the best one.

Method: AutoAL uses two neural nets (SearchNet and FitNet) under a differentiable bi-level optimization framework to co-optimize and select the best AL strategies for a task.

Result: AutoAL outperforms other AL algorithms and selective approaches, achieving superior accuracy across diverse tasks.

Conclusion: AutoAL effectively adapts and integrates multiple AL methods, offering a scalable solution for data-efficient deep learning.

Abstract: As deep learning continues to evolve, the need for data efficiency becomes
increasingly important. Considering labeling large datasets is both
time-consuming and expensive, active learning (AL) provides a promising
solution to this challenge by iteratively selecting the most informative
subsets of examples to train deep neural networks, thereby reducing the
labeling cost. However, the effectiveness of different AL algorithms can vary
significantly across data scenarios, and determining which AL algorithm best
fits a given task remains a challenging problem. This work presents the first
differentiable AL strategy search method, named AutoAL, which is designed on
top of existing AL sampling strategies. AutoAL consists of two neural nets,
named SearchNet and FitNet, which are optimized concurrently under a
differentiable bi-level optimization framework. For any given task, SearchNet
and FitNet are iteratively co-optimized using the labeled data, learning how
well a set of candidate AL algorithms perform on that task. With the optimal AL
strategies identified, SearchNet selects a small subset from the unlabeled pool
for querying their annotations, enabling efficient training of the task model.
Experimental results demonstrate that AutoAL consistently achieves superior
accuracy compared to all candidate AL algorithms and other selective AL
approaches, showcasing its potential for adapting and integrating multiple
existing AL methods across diverse tasks and domains. Code is available at:
https://github.com/haizailache999/AutoAL.

</details>


### [710] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/pdf/2504.14945)
*Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY introduces off-policy reasoning traces to enhance RLVR, achieving significant performance gains and overcoming on-policy limitations.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods are limited by on-policy learning, restricting reasoning abilities to the model's initial capabilities.

Method: LUFFY combines off-policy demonstrations with on-policy rollouts using Mixed-Policy GRPO and policy shaping to balance imitation and exploration.

Result: LUFFY achieves +6.4 average gain on math benchmarks and +6.2 points in out-of-distribution tasks, outperforming on-policy RLVR.

Conclusion: LUFFY successfully overcomes on-policy RLVR limitations, demonstrating the potential of off-policy guidance in reasoning tasks.

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}).
However, existing \textit{RLVR} approaches are inherently ``on-policy'',
limiting learning to a model's own outputs and failing to acquire reasoning
abilities beyond its initial capabilities. To address this issue, we introduce
\textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder
o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR}
with off-policy reasoning traces. LUFFY dynamically balances imitation and
exploration by combining off-policy demonstrations with on-policy rollouts
during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework,
which has a theoretically guaranteed convergence rate, alongside policy shaping
via regularized importance sampling to avoid superficial and rigid imitation
during mixed-policy training. Compared with previous RLVR methods, LUFFY
achieves an over \textbf{+6.4} average gain across six math benchmarks and an
advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most
significantly, we show that LUFFY successfully trains weak models in scenarios
where on-policy RLVR completely fails. These results provide compelling
evidence that LUFFY transcends the fundamental limitations of on-policy RLVR
and demonstrates the great potential of utilizing off-policy guidance in RLVR.

</details>


### [711] [Modeling Structured Data Learning with Restricted Boltzmann Machines in the Teacher-Student Setting](https://arxiv.org/pdf/2410.16150)
*Robin Thériault, Francesco Tosello, Daniele Tantari*

Main category: cs.LG

TL;DR: The paper explores how a student RBM learns structured data from a teacher RBM, showing performance depends on hidden units and pattern correlations, and connects this to the lottery ticket hypothesis.


<details>
  <summary>Details</summary>
Motivation: To understand how structured data learning in RBMs is influenced by hidden units and pattern correlations, and to explore the teacher-student setting as a model for the lottery ticket hypothesis.

Method: A teacher-student RBM setup is used, varying hidden units and pattern correlations to study learning performance and critical data requirements.

Result: Performance is independent of teacher patterns and student hidden units without correlations, but critical data decreases with pattern number and correlations. Low inference temperature hinders learning.

Conclusion: The teacher-student setting generalizes findings to any finite hidden units, showing learning can be one-to-one or many-to-one, and highlights the impact of regularization.

Abstract: Restricted Boltzmann machines (RBM) are generative models capable to learn
data with a rich underlying structure. We study the teacher-student setting
where a student RBM learns structured data generated by a teacher RBM. The
amount of structure in the data is controlled by adjusting the number of hidden
units of the teacher and the correlations in the rows of the weights, a.k.a.
patterns. In the absence of correlations, we validate the conjecture that the
performance is independent of the number of teacher patters and hidden units of
the student RBMs, and we argue that the teacher-student setting can be used as
a toy model for studying the lottery ticket hypothesis. Beyond this regime, we
find that the critical amount of data required to learn the teacher patterns
decreases with both their number and correlations. In both regimes, we find
that, even with a relatively large dataset, it becomes impossible to learn the
teacher patterns if the inference temperature used for regularization is kept
too low. In our framework, the student can learn teacher patterns one-to-one or
many-to-one, generalizing previous findings about the teacher-student setting
with two hidden units to any arbitrary finite number of hidden units.

</details>


### [712] [SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries](https://arxiv.org/pdf/2410.19236)
*Darin Tsui, Aryan Musharaf, Yigit Efe Erginbas, Justin Singh Kang, Amirali Aghazadeh*

Main category: cs.LG

TL;DR: SHAP zero is a novel algorithm that reduces the computational cost of Shapley value computation for large-scale biological datasets, enabling efficient and scalable interpretability of black-box sequence models.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable predictions in machine learning models for biological sequences has grown, but existing methods for Shapley-based interpretability are computationally expensive for large datasets.

Method: SHAP zero amortizes Shapley value computation costs by leveraging connections between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model.

Result: SHAP zero explains predictions much faster than existing methods, uncovering rich combinatorial interactions in models of guide RNA efficacy, DNA repair, and protein fitness.

Conclusion: SHAP zero provides a scalable and efficient solution for interpretability in biological sequence models, making previously inaccessible insights feasible.

Abstract: The growing adoption of machine learning models for biological sequences has
intensified the need for interpretable predictions, with Shapley values
emerging as a theoretically grounded standard for model explanation. While
effective for local explanations of individual input sequences, scaling
Shapley-based interpretability to extract global biological insights requires
evaluating thousands of sequences--incurring exponential computational cost per
query. We introduce SHAP zero, a novel algorithm that amortizes the cost of
Shapley value computation across large-scale biological datasets. After a
one-time model sketching step, SHAP zero enables near-zero marginal cost for
future queries by uncovering an underexplored connection between Shapley
values, high-order feature interactions, and the sparse Fourier transform of
the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and
protein fitness, SHAP zero explains predictions orders of magnitude faster than
existing methods, recovering rich combinatorial interactions previously
inaccessible at scale. This work opens the door to principled, efficient, and
scalable interpretability for black-box sequence models in biology.

</details>


### [713] [Enhancing the Influence of Labels on Unlabeled Nodes in Graph Convolutional Networks](https://arxiv.org/pdf/2411.02279)
*Jincheng Huang, Yujie Mo, Xiaoshuang Shi, Lei Feng, Xiaofeng Zhu*

Main category: cs.LG

TL;DR: ELU-GCN improves label utilization in GCNs via a two-step framework: graph learning for a new structure (ELU-graph) and contrastive learning for representation.


<details>
  <summary>Details</summary>
Motivation: Traditional GCNs underutilize label information, limiting effectiveness.

Method: Two-step approach: (1) Learn ELU-graph for better label propagation; (2) Use contrastive learning on GCN framework.

Result: Theoretical proof of generalization ability; experimental validation shows superiority.

Conclusion: ELU-GCN enhances label utilization and representation learning in GCNs.

Abstract: The message-passing mechanism of graph convolutional networks (i.e., GCNs)
enables label information to be propagated to a broader range of neighbors,
thereby increasing the utilization of labels. However, the label information is
not always effectively utilized in the traditional GCN framework. To address
this issue, we propose a new two-step framework called ELU-GCN. In the first
stage, ELU-GCN conducts graph learning to learn a new graph structure (i.e.,
ELU-graph), which enables the message passing can effectively utilize label
information. In the second stage, we design a new graph contrastive learning on
the GCN framework for representation learning by exploring the consistency and
mutually exclusive information between the learned ELU graph and the original
graph. Moreover, we theoretically demonstrate that the proposed method can
ensure the generalization ability of GCNs. Extensive experiments validate the
superiority of our method.

</details>


### [714] [Graph Retention Networks for Dynamic Graphs](https://arxiv.org/pdf/2411.11259)
*Qian Chang, Xia Li, Xiufeng Cheng*

Main category: cs.LG

TL;DR: The paper introduces Graph Retention Network (GRN), a unified architecture for dynamic graph learning, offering parallelism, low-cost inference, and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of dynamic graph learning by balancing effectiveness, efficiency, and scalability.

Method: Extends retention computation to dynamic graphs, enabling parallelism, O(1) inference, and long-term batch training.

Result: Superior performance in edge/node tasks, lower latency, reduced GPU memory, and 86.7x inference throughput improvement.

Conclusion: GRN shows strong potential as a widely adopted architecture for dynamic graph learning.

Abstract: In this work, we propose Graph Retention Network as a unified architecture
for deep learning on dynamic graphs. The GRN extends the core computational
manner of retention to dynamic graph data as graph retention, which empowers
the model with three key computational paradigms that enable training
parallelism, $O(1)$ low-cost inference, and long-term batch training. This
architecture achieves an optimal balance of effectiveness, efficiency, and
scalability. Extensive experiments conducted on benchmark datasets present the
superior performance of the GRN in both edge-level prediction and node-level
classification tasks. Our architecture achieves cutting-edge results while
maintaining lower training latency, reduced GPU memory consumption, and up to
an 86.7x improvement in inference throughput compared to baseline models. The
GRNs have demonstrated strong potential to become a widely adopted architecture
for dynamic graph learning tasks. Code will be available at
https://github.com/Chandler-Q/GraphRetentionNet.

</details>


### [715] [Fine-Grained Uncertainty Quantification via Collisions](https://arxiv.org/pdf/2411.12127)
*Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon*

Main category: cs.LG

TL;DR: The paper introduces a new metric for aleatoric uncertainty quantification (UQ) called the collision matrix, which measures class collision prevalence. It proposes methods to estimate this matrix and demonstrates its applications in UQ.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quantifying aleatoric uncertainty in classification tasks by measuring the inherent difficulty in distinguishing between classes.

Method: Proposes the collision matrix, a fine-grained measure of uncertainty, and develops techniques to estimate it using a contrastive model and Gramian matrix recovery.

Result: The collision matrix is successfully estimated and used to derive posterior class probabilities, validated experimentally on multiple datasets.

Conclusion: The collision matrix provides a novel and effective tool for aleatoric UQ, with potential broader applications in uncertainty estimation.

Abstract: We propose a new and intuitive metric for aleatoric uncertainty
quantification (UQ), the prevalence of class collisions defined as the same
input being observed in different classes. We use the rate of class collisions
to define the collision matrix, a novel and uniquely fine-grained measure of
uncertainty. For a classification problem involving $K$ classes, the $K\times
K$ collision matrix $S$ measures the inherent difficulty in distinguishing
between each pair of classes. We discuss several applications of the collision
matrix, establish its fundamental mathematical properties, as well as show its
relationship with existing UQ methods, including the Bayes error rate (BER). We
also address the new problem of estimating the collision matrix using one-hot
labeled data by proposing a series of innovative techniques to estimate $S$.
First, we learn a pair-wise contrastive model which accepts two inputs and
determines if they belong to the same class. We then show that this contrastive
model (which is PAC learnable) can be used to estimate the Gramian matrix of
$S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions,
$G$ can be used to uniquely recover $S$, a new result on non-negative matrices
which could be of independent interest. With a method to estimate $S$
established, we demonstrate how this estimate of $S$, in conjunction with the
contrastive model, can be used to estimate the posterior class portability
distribution of any point. Experimental results are also presented to validate
our methods of estimating the collision matrix and class posterior
distributions on several datasets.

</details>


### [716] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/pdf/2505.03802)
*Changhai Zhou, Yuhua Zhou, Qian Qiao, Weizhong Zhang, Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA combines low-bit quantization and LoRA for efficient LLM fine-tuning. QR-Adaptor jointly optimizes quantization and low-rank subspaces, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to consistently improve performance by addressing quantization errors and low-rank subspaces separately.

Method: Proposes QR-Adaptor, a gradient-free strategy using partial calibration data to jointly optimize quantization and rank allocation.

Result: Achieves 4.89% accuracy improvement on GSM8K, sometimes outperforming 16-bit models while maintaining 4-bit memory usage.

Conclusion: QR-Adaptor effectively balances performance and memory efficiency, advancing quantized fine-tuning.

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [717] [Efficient Spatio-Temporal Signal Recognition on Edge Devices Using PointLCA-Net](https://arxiv.org/pdf/2411.14585)
*Sanaz Mahmoodi Takaghaj*

Main category: cs.LG

TL;DR: A novel method, PointLCA-Net, combines PointNet and neuromorphic computing for efficient spatio-temporal signal recognition on edge devices, improving accuracy and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in real-time processing, memory, and power consumption for deep learning on spatio-temporal signals in edge devices.

Method: Two-stage process: PointNet for feature extraction and memristor storage, followed by a spiking neural encoder-decoder using LCA for efficient encoding and classification.

Result: High recognition accuracy with lower energy consumption during inference and training compared to other methods.

Conclusion: PointLCA-Net advances deployment of neural architectures in energy-constrained environments by integrating PointNet and LCA strengths.

Abstract: Recent advancements in machine learning, particularly through deep learning
architectures like PointNet, have transformed the processing of
three-dimensional (3D) point clouds, significantly improving 3D object
classification and segmentation tasks. While 3D point clouds provide detailed
spatial information, spatio-temporal signals introduce a dynamic element that
accounts for changes over time. However, applying deep learning techniques to
spatio-temporal signals and deploying them on edge devices presents challenges,
including real-time processing, memory capacity, and power consumption. To
address these issues, this paper presents a novel approach that combines
PointNet's feature extraction with the in-memory computing capabilities and
energy efficiency of neuromorphic systems for spatio-temporal signal
recognition. The proposed method consists of a two-stage process: in the first
stage, PointNet extracts features from the spatio-temporal signals, which are
then stored in non-volatile memristor crossbar arrays. In the second stage,
these features are processed by a single-layer spiking neural encoder-decoder
that employs the Locally Competitive Algorithm (LCA) for efficient encoding and
classification. This work integrates the strengths of both PointNet and LCA,
enhancing computational efficiency and energy performance on edge devices.
PointLCA-Net achieves high recognition accuracy for spatio-temporal data with
substantially lower energy burden during both inference and training than
comparable approaches, thus advancing the deployment of advanced neural
architectures in energy-constrained environments.

</details>


### [718] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/pdf/2505.04174)
*Ju-Hyung Lee, Yanqing Lu, Klaus Doppler*

Main category: cs.LG

TL;DR: The paper introduces a cross-layer approach using an on-device LLM for Wi-Fi roaming, improving AP selection and dynamic threshold adjustment, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional Wi-Fi roaming methods often fail in dynamic environments, causing sticky or excessive handovers, necessitating a smarter solution.

Method: Uses an LLM for context-aware AP selection and dynamic threshold adjustment, optimized for edge hardware with techniques like chain-of-thought prompting and quantization.

Result: Outperforms legacy heuristics and DRL baselines, balancing roaming stability and signal quality effectively.

Conclusion: Demonstrates the potential of application-layer LLM reasoning for wireless control in edge systems.

Abstract: Roaming in Wireless LAN (Wi-Fi) is a critical yet challenging task for
maintaining seamless connectivity in dynamic mobile environments. Conventional
threshold-based or heuristic schemes often fail, leading to either sticky or
excessive handovers. We introduce the first cross-layer use of an on-device
large language model (LLM): high-level reasoning in the application layer that
issues real-time actions executed in the PHY/MAC stack. The LLM addresses two
tasks: (i) context-aware AP selection, where structured prompts fuse
environmental cues (e.g., location, time) to choose the best BSSID; and (ii)
dynamic threshold adjustment, where the model adaptively decides when to roam.
To satisfy the tight latency and resource budgets of edge hardware, we apply a
suite of optimizations-chain-of-thought prompting, parameter-efficient
fine-tuning, and quantization. Experiments on indoor and outdoor datasets show
that our approach surpasses legacy heuristics and DRL baselines, achieving a
strong balance between roaming stability and signal quality. These findings
underscore the promise of application-layer LLM reasoning for lower-layer
wireless control in future edge systems.

</details>


### [719] [Electrocardiogram-based diagnosis of liver diseases: an externally validated and explainable machine learning approach](https://arxiv.org/pdf/2412.03717)
*Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff*

Main category: cs.LG

TL;DR: ECG-based machine learning models detect liver diseases non-invasively with strong performance, identifying key features like age and QTc intervals.


<details>
  <summary>Details</summary>
Motivation: Liver diseases require costly, invasive diagnostics; ECG offers a non-invasive alternative by capturing cardiovascular-hepatic interactions.

Method: Tree-based models trained on ECG features from large datasets (MIMIC-IV-ECG and ECG-View II) for binary classification, evaluated via AUROC and explainability methods.

Result: High AUROCs (e.g., 0.8025 for alcoholic liver disease) and key predictors like age and QTc intervals, supporting known cardiovascular-liver connections.

Conclusion: ECG-based ML is a promising, interpretable tool for non-invasive liver disease detection, aiding early diagnosis and risk stratification.

Abstract: Background: Liver diseases present a significant global health challenge and
often require costly, invasive diagnostics. Electrocardiography (ECG), a widely
available and non-invasive tool, can enable the detection of liver disease by
capturing cardiovascular-hepatic interactions.
  Methods: We trained tree-based machine learning models on ECG features to
detect liver diseases using two large datasets: MIMIC-IV-ECG (467,729 patients,
2008-2019) and ECG-View II (775,535 patients, 1994-2013). The task was framed
as binary classification, with performance evaluated via the area under the
receiver operating characteristic curve (AUROC). To improve interpretability,
we applied explainability methods to identify key predictive features.
  Findings: The models showed strong predictive performance with good
generalizability. For example, AUROCs for alcoholic liver disease (K70) were
0.8025 (95% confidence interval (CI), 0.8020-0.8035) internally and 0.7644 (95%
CI, 0.7641-0.7649) externally; for hepatic failure (K72), scores were 0.7404
(95% CI, 0.7389-0.7415) and 0.7498 (95% CI, 0.7494-0.7509), respectively. The
explainability analysis consistently identified age and prolonged QTc intervals
(corrected QT, reflecting ventricular repolarization) as key predictors.
Features linked to autonomic regulation and electrical conduction abnormalities
were also prominent, supporting known cardiovascular-liver connections and
suggesting QTc as a potential biomarker.
  Interpretation: ECG-based machine learning offers a promising, interpretable
approach for liver disease detection, particularly in resource-limited
settings. By revealing clinically relevant biomarkers, this method supports
non-invasive diagnostics, early detection, and risk stratification prior to
targeted clinical assessments.

</details>


### [720] [Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring](https://arxiv.org/pdf/2505.06330)
*Junyu Xue, Xudong Wang, Xiaoling He, Shicheng Liu, Yi Wang, Guoming Tang*

Main category: cs.LG

TL;DR: The paper introduces a prompt-based NILM framework using LLMs, achieving competitive accuracy, robust generalization, and explainability without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of deep learning in NILM (labeled data dependency, poor generalization, lack of explainability) by leveraging LLMs.

Method: Design and evaluate prompt strategies integrating appliance features, timestamps, contextual info, and time-series examples on open datasets.

Result: LLMs achieve competitive state detection accuracy, robust generalization, and provide human-readable explanations.

Conclusion: LLMs reduce data needs, improve adaptability, and offer transparent energy disaggregation in NILM.

Abstract: Non-intrusive load monitoring (NILM) aims to disaggregate aggregate household
electricity consumption into individual appliance usage and thus enables more
effective energy management. While deep learning has advanced NILM, it remains
limited by its dependence on labeled data, restricted generalization, and lack
of explainability. This paper introduces the first prompt-based NILM framework
that leverages large language models (LLMs) with in-context learning. We design
and evaluate prompt strategies that integrate appliance features, timestamps
and contextual information, as well as representative time-series examples on
widely used open datasets. With optimized prompts, LLMs achieve competitive
state detection accuracy and demonstrate robust generalization without the need
for fine-tuning. LLMs also enhance explainability by providing clear,
human-readable explanations for their predictions. Our results show that LLMs
can reduce data requirements, improve adaptability, and provide transparent
energy disaggregation in NILM applications.

</details>


### [721] [Uplift modeling with continuous treatments: A predict-then-optimize approach](https://arxiv.org/pdf/2412.09232)
*Simon De Vos, Christopher Bockel-Rickermann, Stefan Lessmann, Wouter Verbeke*

Main category: cs.LG

TL;DR: The paper introduces a predict-then-optimize framework for uplift modeling with continuous treatments, using CADR estimation and ILP for dose allocation.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often involve continuous treatments, but uplift modeling typically focuses on binary treatments. This gap motivates the proposed framework.

Method: The framework estimates CADRs using causal ML techniques and solves dose allocation via ILP, incorporating constraints like fairness.

Result: Experiments highlight trade-offs between policy value and fairness, and the impact of adapted objective functions, demonstrating the framework's flexibility.

Conclusion: The approach efficiently allocates continuous treatments, balancing resources and constraints, with applications in healthcare, lending, and HR.

Abstract: The goal of uplift modeling is to recommend actions that optimize specific
outcomes by determining which entities should receive treatment. One common
approach involves two steps: first, an inference step that estimates
conditional average treatment effects (CATEs), and second, an optimization step
that ranks entities based on their CATE values and assigns treatment to the top
k within a given budget. While uplift modeling typically focuses on binary
treatments, many real-world applications are characterized by continuous-valued
treatments, i.e., a treatment dose. This paper presents a predict-then-optimize
framework to allow for continuous treatments in uplift modeling. First, in the
inference step, conditional average dose responses (CADRs) are estimated from
data using causal machine learning techniques. Second, in the optimization
step, we frame the assignment task of continuous treatments as a
dose-allocation problem and solve it using integer linear programming (ILP).
This approach allows decision-makers to efficiently and effectively allocate
treatment doses while balancing resource availability, with the possibility of
adding extra constraints like fairness considerations or adapting the objective
function to take into account instance-dependent costs and benefits to maximize
utility. The experiments compare several CADR estimators and illustrate the
trade-offs between policy value and fairness, as well as the impact of an
adapted objective function. This showcases the framework's advantages and
flexibility across diverse applications in healthcare, lending, and human
resource management. All code is available on github.com/SimonDeVos/UMCT.

</details>


### [722] [HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories](https://arxiv.org/pdf/2412.17040)
*Eric Hedlin, Munawar Hayat, Fatih Porikli, Kwang Moo Yi, Shweta Mahajan*

Main category: cs.LG

TL;DR: A method to train hypernetworks without per-sample ground truth by modeling the entire weight training trajectory, validated on image generation and 3D reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: Hypernetworks require ground truth optimized weights per sample, which is cumbersome and resource-intensive to obtain.

Method: Introduces a Hypernetwork 'Field' that models the entire weight training trajectory, using convergence state as an input to match gradients of the original task.

Result: Competitive performance in personalized image generation and 3D shape reconstruction without per-sample ground truth.

Conclusion: The proposed method effectively trains hypernetworks by leveraging gradient constraints, eliminating the need for per-sample ground truth.

Abstract: To efficiently adapt large models or to train generative models of neural
representations, Hypernetworks have drawn interest. While hypernetworks work
well, training them is cumbersome, and often requires ground truth optimized
weights for each sample. However, obtaining each of these weights is a training
problem of its own-one needs to train, e.g., adaptation weights or even an
entire neural field for hypernetworks to regress to. In this work, we propose a
method to train hypernetworks, without the need for any per-sample ground
truth. Our key idea is to learn a Hypernetwork `Field` and estimate the entire
trajectory of network weight training instead of simply its converged state. In
other words, we introduce an additional input to the Hypernetwork, the
convergence state, which then makes it act as a neural field that models the
entire convergence pathway of a task network. A critical benefit in doing so is
that the gradient of the estimated weights at any convergence state must then
match the gradients of the original task -- this constraint alone is sufficient
to train the Hypernetwork Field. We demonstrate the effectiveness of our method
through the task of personalized image generation and 3D shape reconstruction
from images and point clouds, demonstrating competitive results without any
per-sample ground truth.

</details>


### [723] [Unified Stochastic Framework for Neural Network Quantization and Pruning](https://arxiv.org/pdf/2412.18184)
*Haoyu Zhang, Rayan Saab*

Main category: cs.LG

TL;DR: A unified framework for post-training quantization and pruning using stochastic path-following algorithms, extending SPFQ for robust error correction and theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: Quantization and pruning are often treated independently with limited theoretical analysis. This paper aims to unify them.

Method: Extends Stochastic Path Following Quantization (SPFQ) with a scaling parameter and generalized stochastic operator for pruning and low-bit quantization.

Result: Achieves robust error correction and provides rigorous theoretical error bounds for quantization, pruning, and their combination.

Conclusion: The proposed method successfully unifies quantization and pruning, offering theoretical guarantees and practical applicability.

Abstract: Quantization and pruning are two essential techniques for compressing neural
networks, yet they are often treated independently, with limited theoretical
analysis connecting them. This paper introduces a unified framework for
post-training quantization and pruning using stochastic path-following
algorithms. Our approach builds on the Stochastic Path Following Quantization
(SPFQ) method, extending its applicability to pruning and low-bit quantization,
including challenging 1-bit regimes. By incorporating a scaling parameter and
generalizing the stochastic operator, the proposed method achieves robust error
correction and yields rigorous theoretical error bounds for both quantization
and pruning as well as their combination.

</details>


### [724] [ERGNN: Spectral Graph Neural Network With Explicitly-Optimized Rational Graph Filters](https://arxiv.org/pdf/2412.19106)
*Guoming Li, Jian Yang, Shangsong Liang*

Main category: cs.LG

TL;DR: ERGNN introduces a spectral GNN with explicitly-optimized rational filters, outperforming existing methods by streamlining computation and enabling full optimization of rational filters.


<details>
  <summary>Details</summary>
Motivation: Existing spectral GNNs rely heavily on polynomial approximation for graph filters, neglecting the potential of rational approximation, which is computationally intensive or underutilized in prior works.

Method: ERGNN uses a two-step framework to sequentially apply numerator and denominator filters, optimizing both explicitly for efficient rational filter deployment.

Result: Experiments show ERGNN surpasses state-of-the-art methods, proving its effectiveness for rational-based GNNs.

Conclusion: ERGNN provides a practical and superior solution for spectral GNNs by fully leveraging rational approximation.

Abstract: Approximation-based spectral graph neural networks, which construct graph
filters with function approximation, have shown substantial performance in
graph learning tasks. Despite their great success, existing works primarily
employ polynomial approximation to construct the filters, whereas another
superior option, namely ration approximation, remains underexplored. Although a
handful of prior works have attempted to deploy the rational approximation,
their implementations often involve intensive computational demands or still
resort to polynomial approximations, hindering full potential of the rational
graph filters. To address the issues, this paper introduces ERGNN, a novel
spectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique
two-step framework that sequentially applies the numerator filter and the
denominator filter to the input signals, thus streamlining the model paradigm
while enabling explicit optimization of both numerator and denominator of the
rational filter. Extensive experiments validate the superiority of ERGNN over
state-of-the-art methods, establishing it as a practical solution for deploying
rational-based GNNs.

</details>


### [725] [TimeFilter: Patch-Specific Spatial-Temporal Graph Filtration for Time Series Forecasting](https://arxiv.org/pdf/2501.13041)
*Yifan Hu, Guibin Zhang, Peiyuan Liu, Disen Lan, Naiqi Li, Dawei Cheng, Tao Dai, Shu-Tao Xia, Shirui Pan*

Main category: cs.LG

TL;DR: TimeFilter is a GNN-based framework for adaptive, fine-grained dependency modeling in time series forecasting, outperforming CI and CD methods by filtering irrelevant correlations.


<details>
  <summary>Details</summary>
Motivation: Existing CI and CD strategies either overlook covariate relationships or introduce noise, while coarse-grained clustering fails to capture complex interactions.

Method: TimeFilter constructs a graph from input sequences and refines spatial-temporal dependencies by filtering irrelevant correlations in a patch-specific manner.

Result: Extensive experiments on 13 real-world datasets show TimeFilter achieves state-of-the-art performance.

Conclusion: TimeFilter effectively addresses limitations of CI, CD, and CC methods, offering superior dependency modeling for time series forecasting.

Abstract: Time series forecasting methods generally fall into two main categories:
Channel Independent (CI) and Channel Dependent (CD) strategies. While CI
overlooks important covariate relationships, CD captures all dependencies
without distinction, introducing noise and reducing generalization. Recent
advances in Channel Clustering (CC) aim to refine dependency modeling by
grouping channels with similar characteristics and applying tailored modeling
techniques. However, coarse-grained clustering struggles to capture complex,
time-varying interactions effectively. To address these challenges, we propose
TimeFilter, a GNN-based framework for adaptive and fine-grained dependency
modeling. After constructing the graph from the input sequence, TimeFilter
refines the learned spatial-temporal dependencies by filtering out irrelevant
correlations while preserving the most critical ones in a patch-specific
manner. Extensive experiments on 13 real-world datasets from diverse
application domains demonstrate the state-of-the-art performance of TimeFilter.
The code is available at https://github.com/TROUBADOUR000/TimeFilter.

</details>


### [726] [A Comprehensive Social Bias Audit of Contrastive Vision Language Models](https://arxiv.org/pdf/2501.13223)
*Zahraa Al Sahili, Ioannis Patras, Matthew Purver*

Main category: cs.LG

TL;DR: FairCoT is a framework using Chain-of-Thought reasoning to reduce biases in text-to-image models, improving fairness and diversity without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Address ethical challenges from biases in training datasets that propagate into generated content, especially in sensitive contexts.

Method: Iterative CoT refinement dynamically adjusts prompts in real time to mitigate biases, integrating reasoning for ethical balance.

Result: Significantly enhances fairness and diversity in generated images across models like DALL-E and Stable Diffusion, maintaining quality.

Conclusion: FairCoT advances socially responsible AI content generation through robust reasoning, lightweight deployment, and model extensibility.

Abstract: In the domain of text-to-image generative models, biases inherent in training
datasets often propagate into generated content, posing significant ethical
challenges, particularly in socially sensitive contexts. We introduce FairCoT,
a novel framework that enhances fairness in text-to-image models through
Chain-of-Thought (CoT) reasoning within multimodal generative large language
models. FairCoT employs iterative CoT refinement to systematically mitigate
biases, and dynamically adjusts textual prompts in real time, ensuring diverse
and equitable representation in generated images. By integrating iterative
reasoning processes, FairCoT addresses the limitations of zero-shot CoT in
sensitive scenarios, balancing creativity with ethical responsibility.
Experimental evaluations across popular text-to-image systems--including DALL-E
and various Stable Diffusion variants--demonstrate that FairCoT significantly
enhances fairness and diversity without sacrificing image quality or semantic
fidelity. By combining robust reasoning, lightweight deployment, and
extensibility to multiple models, FairCoT represents a promising step toward
more socially responsible and transparent AI-driven content generation.

</details>


### [727] [The Sample Complexity of Online Reinforcement Learning: A Multi-model Perspective](https://arxiv.org/pdf/2501.15910)
*Michael Muehlebach, Zhiyu He, Michael I. Jordan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the sample complexity of online reinforcement learning in the
general setting of nonlinear dynamical systems with continuous state and action
spaces. Our analysis accommodates a large class of dynamical systems ranging
from a finite set of nonlinear candidate models to models with bounded and
Lipschitz continuous dynamics, to systems that are parametrized by a compact
and real-valued set of parameters. In the most general setting, our algorithm
achieves a policy regret of $\mathcal{O}(N \epsilon^2 +
\mathrm{ln}(m(\epsilon))/\epsilon^2)$, where $N$ is the time horizon,
$\epsilon$ is a user-specified discretization width, and $m(\epsilon)$ measures
the complexity of the function class under consideration via its packing
number. In the special case where the dynamics are parametrized by a compact
and real-valued set of parameters (such as neural networks, transformers,
etc.), we prove a policy regret of $\mathcal{O}(\sqrt{N p})$, where $p$ denotes
the number of parameters, recovering earlier sample-complexity results that
were derived for linear time-invariant dynamical systems. While this article
focuses on characterizing sample complexity, the proposed algorithms are likely
to be useful in practice, due to their simplicity, their ability to incorporate
prior knowledge, and their benign transient behavior.

</details>


### [728] [Exploring Non-Convex Discrete Energy Landscapes: An Efficient Langevin-Like Sampler with Replica Exchange](https://arxiv.org/pdf/2501.17323)
*Haoyang Zheng, Hengrong Du, Ruqi Zhang, Guang Lin*

Main category: cs.LG

TL;DR: The paper introduces DREXEL and DREAM, two gradient-based discrete samplers that improve exploration in complex, non-convex energy landscapes by combining local and global sampling strategies with a swap mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing Gradient-based Discrete Samplers (GDSs) often stagnate in complex, non-convex settings, limiting their effectiveness.

Method: The authors propose DREXEL and DREAM, which use two GDSs at different temperatures and step sizes—one for local exploitation and another for broader exploration—and a swap mechanism for energy differences.

Result: Theoretical proofs confirm detailed balance and convergence to the target distribution. Experiments show improved efficiency in non-convex discrete energy landscapes.

Conclusion: DREXEL and DREAM enhance exploration in discrete sampling, outperforming traditional GDSs in complex scenarios.

Abstract: Gradient-based Discrete Samplers (GDSs) are effective for sampling discrete
energy landscapes. However, they often stagnate in complex, non-convex
settings. To improve exploration, we introduce the Discrete Replica EXchangE
Langevin (DREXEL) sampler and its variant with Adjusted Metropolis (DREAM).
These samplers use two GDSs at different temperatures and step sizes: one
focuses on local exploitation, while the other explores broader energy
landscapes. When energy differences are significant, sample swaps occur, which
are determined by a mechanism tailored for discrete sampling to ensure detailed
balance. Theoretically, we prove that the proposed samplers satisfy detailed
balance and converge to the target distribution under mild conditions.
Experiments across 2d synthetic simulations, sampling from Ising models and
restricted Boltzmann machines, and training deep energy-based models further
confirm their efficiency in exploring non-convex discrete energy landscapes.

</details>


### [729] [When Do LLMs Help With Node Classification? A Comprehensive Analysis](https://arxiv.org/pdf/2502.00829)
*Xixi Wu, Yifei Shen, Fangzhou Ge, Caihua Shan, Yizhu Jiao, Xiangguo Sun, Hong Cheng*

Main category: cs.LG

TL;DR: The paper introduces LLMNodeBed, a testbed for node classification using LLMs, and provides 8 insights from extensive experiments comparing LLM-based and traditional methods.


<details>
  <summary>Details</summary>
Motivation: The lack of clear design guidelines for LLM-based node classification methods hinders their practical application, prompting a systematic comparison.

Method: Developed LLMNodeBed, a codebase/testbed with datasets and algorithms, and conducted experiments on 2,700 models to analyze performance factors.

Result: Key findings include LLM-based methods excelling in semi-supervised settings and Graph Foundation Models outperforming open-source LLMs but lagging behind GPT-4o.

Conclusion: LLMNodeBed and the insights aim to support reproducible research and inspire future work in LLM-based node classification.

Abstract: Node classification is a fundamental task in graph analysis, with broad
applications across various fields. Recent breakthroughs in Large Language
Models (LLMs) have enabled LLM-based approaches for this task. Although many
studies demonstrate the impressive performance of LLM-based methods, the lack
of clear design guidelines may hinder their practical application. In this
work, we aim to establish such guidelines through a fair and systematic
comparison of these algorithms. As a first step, we developed LLMNodeBed, a
comprehensive codebase and testbed for node classification using LLMs. It
includes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based
algorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we
conducted extensive experiments, training and evaluating over 2,700 models, to
determine the key settings (e.g., learning paradigms and homophily) and
components (e.g., model size and prompt) that affect performance. Our findings
uncover 8 insights, e.g., (1) LLM-based methods can significantly outperform
traditional methods in a semi-supervised setting, while the advantage is
marginal in a supervised setting; (2) Graph Foundation Models can beat
open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot
setting. We hope that the release of LLMNodeBed, along with our insights, will
facilitate reproducible research and inspire future studies in this field.
Codes and datasets are released at
\href{https://llmnodebed.github.io/}{\texttt{https://llmnodebed.github.io/}}.

</details>


### [730] [The Differences Between Direct Alignment Algorithms are a Blur](https://arxiv.org/pdf/2502.01237)
*Alexey Gorbatovski, Boris Shaposhnikov, Viacheslav Sinii, Alexey Malakhov, Daniil Gavrilov*

Main category: cs.LG

TL;DR: Direct Alignment Algorithms (DAAs) simplify language model alignment by optimizing policies directly. Key findings: two-stage methods outperform one-stage, and pairwise objectives are more critical than scalar scores.


<details>
  <summary>Details</summary>
Motivation: To systematically compare DAAs, addressing gaps in understanding performance factors like SFT stages, scalar scores, and ranking objectives.

Method: Comparative analysis of DAAs, testing one-stage vs. two-stage setups, scalar scores (likelihood vs. odds ratios), and ranking objectives (pairwise vs. pointwise). Introduced a unifying β parameter.

Result: Two-stage methods with explicit SFT phases and tuned β improved performance (e.g., +13.45 ORPO, +8.27 ASFT). Pairwise objectives were key to success, outperforming pointwise ones.

Conclusion: Pairwise objectives and two-stage setups are critical for DAA performance. Nuanced evaluation is needed to avoid oversimplified claims.

Abstract: Direct Alignment Algorithms (DAAs) offer a simpler way to language model
alignment than traditional RLHF by directly optimizing policies. While DAAs
differ in their use of SFT (one-stage vs. two-stage), the scalar scores within
their objectives (likelihood vs. odds ratios), and ranking objectives (pairwise
vs. pointwise), the critical factors for performance remain underexplored. We
provide a systematic comparative analysis. We first show that one-stage methods
(e.g. ORPO, ASFT) underperform compared to two-stage approaches. However, we
demonstrate that adapting them to a two-stage setup with an explicit SFT phase
can improve their performance. Further, introducing and tuning a unifying
$\beta$ parameter within this two-stage framework boosts their performence
(e.g., AlpacaEval 2: $+13.45$ ORPO, $+8.27$ ASFT), matching established methods
like DPO and enabling fair comparisons. Our comprehensive analysis reveals that
the choice between pairwise and pointwise objectives is the primary determinant
of alignment success, rather than the specific scalar score (e.g.,
policy-reference ratio vs. odds ratio) employed. We provide empirical evidence
suggesting this stems from how these objectives interact with prompt-specific
biases. These findings underscore the need for nuanced evaluations in DAA
research to avoid oversimplified claims of superiority.

</details>


### [731] [DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale](https://arxiv.org/pdf/2502.01681)
*Ziyang Zheng, Shan Huang, Jianyuan Zhong, Zhengyuan Shi, Guohao Dai, Ningyi Xu, Qiang Xu*

Main category: cs.LG

TL;DR: DeepGate4 is a scalable graph transformer for large-scale circuits, improving performance and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with scalability for large circuits due to issues like over-squashing and quadratic complexity.

Method: DeepGate4 introduces a tailored update strategy, a GAT-based sparse transformer, and a CUDA kernel for inference acceleration.

Result: DeepGate4 achieves 15.5% and 31.1% performance improvements and reduces runtime by 35.1% and memory usage by 46.8%.

Conclusion: DeepGate4 offers superior scalability and efficiency for complex EDA tasks.

Abstract: Circuit representation learning has become pivotal in electronic design
automation, enabling critical tasks such as testability analysis, logic
reasoning, power estimation, and SAT solving. However, existing models face
significant challenges in scaling to large circuits due to limitations like
over-squashing in graph neural networks and the quadratic complexity of
transformer-based models. To address these issues, we introduce DeepGate4, a
scalable and efficient graph transformer specifically designed for large-scale
circuits. DeepGate4 incorporates several key innovations: (1) an update
strategy tailored for circuit graphs, which reduce memory complexity to
sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse
transformer with global and local structural encodings for AIGs; and (3) an
inference acceleration CUDA kernel that fully exploit the unique sparsity
patterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks
show that DeepGate4 significantly surpasses state-of-the-art methods, achieving
15.5% and 31.1% performance improvements over the next-best models.
Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory
usage by 46.8%, making it highly efficient for large-scale circuit analysis.
These results demonstrate the potential of DeepGate4 to handle complex EDA
tasks while offering superior scalability and efficiency. Code is available at
https://github.com/zyzheng17/DeepGate4-ICLR-25.

</details>


### [732] [Training Language Models to Reason Efficiently](https://arxiv.org/pdf/2502.04463)
*Daman Arora, Andrea Zanette*

Main category: cs.LG

TL;DR: The paper proposes using reinforcement learning to train large reasoning models for efficient compute allocation, reducing inference costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Diminishing returns from scaling model size and data necessitate alternative methods to improve reasoning capabilities without high deployment costs.

Method: Reinforcement learning is used to dynamically allocate inference-time compute based on task complexity, minimizing unnecessary overhead.

Result: Experiments show significant inference cost reductions while preserving accuracy in large reasoning models.

Conclusion: The method enables efficient reasoning models with controlled efficiency levels, enhancing economic feasibility and sustainability.

Abstract: Scaling model size and training data has led to great advances in the
performance of Large Language Models (LLMs). However, the diminishing returns
of this approach necessitate alternative methods to improve model capabilities,
particularly in tasks requiring advanced reasoning. Large reasoning models,
which leverage long chain-of-thoughts, bring unprecedented breakthroughs in
problem-solving capabilities but at a substantial deployment cost associated to
longer generations. Reducing inference costs is crucial for the economic
feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason
efficiently. More precisely, we use reinforcement learning (RL) to train
reasoning models to dynamically allocate inference-time compute based on task
complexity. Our method incentivizes models to minimize unnecessary
computational overhead while maintaining accuracy, thereby achieving
substantial efficiency gains. It enables the derivation of a family of
reasoning models with varying efficiency levels, controlled via a single
hyperparameter. Experiments on two open-weight large reasoning models
demonstrate significant reductions in inference cost while preserving most of
the accuracy.

</details>


### [733] [Conditional Distribution Quantization in Machine Learning](https://arxiv.org/pdf/2502.07151)
*Blaise Delattre, Sylvain Delattre, Alexandre Vérine, Alexandre Allauzen*

Main category: cs.LG

TL;DR: The paper proposes n-point conditional quantizations to approximate multimodal conditional distributions, improving on single-valued predictions like conditional expectation.


<details>
  <summary>Details</summary>
Motivation: Conditional expectation often fails to capture multimodal structures in conditional distributions, limiting its usefulness for tasks like uncertainty quantification and multimodal data generation.

Method: The approach adapts Competitive Learning Vector Quantization (CLVQ) for conditional distributions, using gradient descent to learn functional mappings of X that approximate the true conditional law in Wasserstein distance.

Result: The method provides multiple representative points, better reflecting multimodal structures, and is validated on synthetic and real-world datasets, including computer vision inpainting tasks.

Conclusion: The proposed framework is theoretically grounded and effective for approximating complex conditional distributions, offering practical benefits for uncertainty quantification and multimodal generation.

Abstract: Conditional expectation \mathbb{E}(Y \mid X) often fails to capture the
complexity of multimodal conditional distributions \mathcal{L}(Y \mid X). To
address this, we propose using n-point conditional quantizations--functional
mappings of X that are learnable via gradient descent--to approximate
\mathcal{L}(Y \mid X). This approach adapts Competitive Learning Vector
Quantization (CLVQ), tailored for conditional distributions. It goes beyond
single-valued predictions by providing multiple representative points that
better reflect multimodal structures. It enables the approximation of the true
conditional law in the Wasserstein distance. The resulting framework is
theoretically grounded and useful for uncertainty quantification and multimodal
data generation tasks. For example, in computer vision inpainting tasks,
multiple plausible reconstructions may exist for the same partially observed
input image X. We demonstrate the effectiveness of our approach through
experiments on synthetic and real-world datasets.

</details>


### [734] [Enhancing Channel-Independent Time Series Forecasting via Cross-Variate Patch Embedding](https://arxiv.org/pdf/2505.12761)
*Donghwa Shin, Edwin Zhang*

Main category: cs.LG

TL;DR: The paper introduces Cross-Variate Patch Embeddings (CVPE) to enhance channel-independent (CI) models by capturing cross-variate dependencies, improving forecasting performance without other modifications.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based time series forecasting models often overlook cross-variate relationships, focusing only on temporal dependencies, and may overfit due to channel dependence.

Method: Proposes CVPE, a lightweight module that modifies patch embeddings with learnable positional encoding and a router-attention block to inject cross-variate context into CI models.

Result: Enhanced Time-LLM with CVPE outperforms the baseline on seven real-world datasets, demonstrating improved cross-variate dependency capture.

Conclusion: CVPE effectively enhances CI models by integrating cross-variate context, offering a simple yet powerful improvement for time series forecasting.

Abstract: Transformers have recently gained popularity in time series forecasting due
to their ability to capture long-term dependencies. However, many existing
models focus only on capturing temporal dependencies while omitting intricate
relationships between variables. Recent models have tried tackling this by
explicitly modeling both cross-time and cross-variate dependencies through a
sequential or unified attention mechanism, but they are entirely channel
dependent (CD) across all layers, making them potentially susceptible to
overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),
a lightweight CD module that injects cross-variate context into
channel-independent (CI) models by simply modifying the patch embedding
process. We achieve this by adding a learnable positional encoding and a
lightweight router-attention block to the vanilla patch embedding layer. We
then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to
demonstrate its effectiveness in capturing cross-variate dependencies and
enhance the CI model's performance. Extensive experimental results on seven
real-world datasets show that our enhanced Time-LLM outperforms the original
baseline model simply by incorporating the CVPE module, with no other changes.

</details>


### [735] [LDC-MTL: Balancing Multi-Task Learning through Scalable Loss Discrepancy Control](https://arxiv.org/pdf/2502.08585)
*Peiyao Xiao, Chaosheng Dong, Shaofeng Zou, Kaiyi Ji*

Main category: cs.LG

TL;DR: LDC-MTL is a scalable multi-task learning method with O(1) computational overhead, ensuring balanced task learning and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing gradient manipulation methods in MTL suffer from high computational costs (O(K)), motivating the need for a simpler, scalable solution.

Method: LDC-MTL uses coarse loss pre-normalization, bilevel optimization for fine-grained control, and a scalable first-order algorithm.

Result: The method achieves superior performance in accuracy and efficiency, with theoretical guarantees for convergence and Pareto stationarity.

Conclusion: LDC-MTL is a promising approach for efficient and effective multi-task learning, demonstrated by extensive experiments.

Abstract: Multi-task learning (MTL) has been widely adopted for its ability to
simultaneously learn multiple tasks. While existing gradient manipulation
methods often yield more balanced solutions than simple scalarization-based
approaches, they typically incur a significant computational overhead of
$\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In
this paper, we propose LDC-MTL, a simple and scalable loss discrepancy control
approach for MTL, formulated from a bilevel optimization perspective. Our
method incorporates three key components: (i) a coarse loss pre-normalization,
(ii) a bilevel formulation for fine-grained loss discrepancy control, and (iii)
a scalable first-order bilevel algorithm that requires only $\mathcal{O}(1)$
time and memory. Theoretically, we prove that LDC-MTL guarantees convergence
not only to a stationary point of the bilevel problem with loss discrepancy
control but also to an $\epsilon$-accurate Pareto stationary point for all $K$
loss functions under mild conditions. Extensive experiments on diverse
multi-task datasets demonstrate the superior performance of LDC-MTL in both
accuracy and efficiency. Code is available at
https://github.com/OptMN-Lab/LDC-MTL.

</details>


### [736] [Sinusoidal Initialization, Time for a New Start](https://arxiv.org/pdf/2505.12909)
*Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: Sinusoidal initialization replaces random weight initialization with deterministic sinusoidal functions, improving convergence, stability, and accuracy in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Random initializations like Glorot and He can lead to uneven weight distributions, affecting network performance. Sinusoidal initialization aims to provide structured, balanced weights for better training outcomes.

Method: Uses sinusoidal functions to create structured weight matrices, ensuring uniform weight distribution and balanced neuron activations from the start.

Result: Achieves 4.9% higher validation accuracy and 20.9% faster convergence on average across various models.

Conclusion: Sinusoidal initialization offers a more reliable and efficient foundation for deep learning systems by replacing randomness with structured weights.

Abstract: Initialization plays a critical role in Deep Neural Network training,
directly influencing convergence, stability, and generalization. Common
approaches such as Glorot and He initializations rely on randomness, which can
produce uneven weight distributions across layer connections. In this paper, we
introduce the Sinusoidal initialization, a novel deterministic method that
employs sinusoidal functions to construct structured weight matrices expressly
to improve the spread and balance of weights throughout the network while
simultaneously fostering a more uniform, well-conditioned distribution of
neuron activation states from the very first forward pass. Because Sinusoidal
initialization begins with weights and activations that are already evenly and
efficiently utilized, it delivers consistently faster convergence, greater
training stability, and higher final accuracy across a wide range of models,
including convolutional neural networks, vision transformers, and large
language models. On average, our experiments show an increase of 4.9% in final
validation accuracy and 20.9% in convergence speed. By replacing randomness
with structure, this initialization provides a stronger and more reliable
foundation for Deep Learning systems.

</details>


### [737] [Rao-Blackwell Gradient Estimators for Equivariant Denoising Diffusion](https://arxiv.org/pdf/2502.09890)
*Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert*

Main category: cs.LG

TL;DR: A framework combining equivariant architectures and data augmentation reduces training variance via Rao-Blackwellization, improving optimization and achieving state-of-the-art results in molecular, crystal, and protein generation.


<details>
  <summary>Details</summary>
Motivation: Physical systems exhibit symmetries critical for modeling, but existing methods (equivariant architectures or data augmentation) have limitations in complexity or symmetry capture.

Method: Proposes a framework interpreting data augmentation as a Monte Carlo gradient estimator and applying Rao-Blackwellization for lower variance. Introduces Orbit Diffusion for practical implementation.

Result: Achieves state-of-the-art results on molecular (GEOM-QM9), crystal (Perov-5, MP-20), and protein generation tasks, with improved optimization stability and convergence.

Conclusion: The framework enhances symmetry modeling, reduces variance, and outperforms existing methods in diverse generation tasks.

Abstract: In domains such as molecular and protein generation, physical systems exhibit
inherent symmetries that are critical to model. Two main strategies have
emerged for learning invariant distributions: designing equivariant network
architectures and using data augmentation to approximate equivariance. While
equivariant architectures preserve symmetry by design, they often involve
greater complexity and pose optimization challenges. Data augmentation, on the
other hand, offers flexibility but may fall short in fully capturing
symmetries. Our framework enhances both approaches by reducing training
variance and providing a provably lower-variance gradient estimator. We achieve
this by interpreting data augmentation as a Monte Carlo estimator of the
training gradient and applying Rao-Blackwellization. This leads to more stable
optimization, faster convergence, and reduced variance, all while requiring
only a single forward and backward pass per sample. We also present a practical
implementation of this estimator incorporating the loss and sampling procedure
through a method we call Orbit Diffusion. Theoretically, we guarantee that our
loss admits equivariant minimizers. Empirically, Orbit Diffusion achieves
state-of-the-art results on GEOM-QM9 for molecular conformation generation,
improves crystal structure prediction, and advances text-guided crystal
generation on the Perov-5 and MP-20 benchmarks. Additionally, it enhances
protein designability in protein structure generation.

</details>


### [738] [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/pdf/2505.12938)
*Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, Omer Nevo*

Main category: cs.LG

TL;DR: A 'Variator' agent leverages LLMs' inconsistency to improve Pass@k performance by generating task variants and submitting solutions for each, outperforming baselines on the APPS dataset.


<details>
  <summary>Details</summary>
Motivation: LLMs show inconsistent performance with minor input changes; this paper exploits this inconsistency to enhance performance.

Method: Introduces a task-agnostic 'Variator' agent that generates k variants of a task and submits solutions for each, validated theoretically and empirically.

Result: Outperforms baselines on the APPS dataset; inconsistency persists even in advanced models, ensuring method relevance.

Conclusion: Leveraging inconsistency via the Variator agent effectively boosts performance, with potential applicability to future models.

Abstract: Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a "Variator" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.

</details>


### [739] [Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data](https://arxiv.org/pdf/2502.09981)
*Harsh Poonia, Felix Divo, Kristian Kersting, Devendra Singh Dhami*

Main category: cs.LG

TL;DR: The paper proposes GC-xLSTM, a method combining Granger causality with xLSTM to better capture long-range dependencies in time series.


<details>
  <summary>Details</summary>
Motivation: Granger causality struggles with long-range relations in time series, prompting the need for improved methods.

Method: The authors introduce GC-xLSTM, using a dynamic loss penalty for sparsity and joint optimization to robustly recover Granger causal relations.

Result: Experiments on six datasets show GC-xLSTM's effectiveness.

Conclusion: GC-xLSTM successfully addresses limitations of traditional Granger causality methods for time series.

Abstract: Causality in time series can be difficult to determine, especially in the
presence of non-linear dependencies. The concept of Granger causality helps
analyze potential relationships between variables, thereby offering a method to
determine whether one time series can predict - Granger cause - future values
of another. Although successful, Granger causal methods still struggle with
capturing long-range relations between variables. To this end, we leverage the
recently successful Extended Long Short-Term Memory (xLSTM) architecture and
propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between
the time series components by using a novel dynamic loss penalty on the initial
projection. Specifically, we adaptively improve the model and identify sparsity
candidates. Our joint optimization procedure then ensures that the Granger
causal relations are recovered robustly. Our experimental evaluation on six
diverse datasets demonstrates the overall efficacy of our proposed GC-xLSTM
model.

</details>


### [740] [Representation Learning on Out of Distribution in Tabular Data](https://arxiv.org/pdf/2502.10095)
*Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua*

Main category: cs.LG

TL;DR: TCL is a lightweight, efficient method for handling out-of-distribution (OOD) data on standard CPUs, outperforming models like FT-Transformer and ResNet with reduced computational needs.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for OOD data often require specialized hardware, limiting accessibility. TCL aims to provide an effective solution for users with standard hardware.

Method: TCL adapts contrastive learning for tabular data, using full matrix augmentation and simplified loss calculation.

Result: TCL outperforms existing models in classification and maintains competitive performance in regression, with lower computational costs.

Conclusion: TCL balances performance and efficiency, making OOD prediction accessible to practitioners with hardware constraints.

Abstract: The open-world assumption in model development suggests that a model might
lack sufficient information to adequately handle data that is entirely distinct
or out of distribution (OOD). While deep learning methods have shown promising
results in handling OOD data through generalization techniques, they often
require specialized hardware that may not be accessible to all users. We
present TCL, a lightweight yet effective solution that operates efficiently on
standard CPU hardware. Our approach adapts contrastive learning principles
specifically for tabular data structures, incorporating full matrix
augmentation and simplified loss calculation. Through comprehensive experiments
across 10 diverse datasets, we demonstrate that TCL outperforms existing
models, including FT-Transformer and ResNet, particularly in classification
tasks, while maintaining competitive performance in regression problems. TCL
achieves these results with significantly reduced computational requirements,
making it accessible to users with limited hardware capabilities. This study
also provides practical guidance for detecting and evaluating OOD data through
straightforward experiments and visualizations. Our findings show that TCL
offers a promising balance between performance and efficiency in handling OOD
prediction tasks, which is particularly beneficial for general machine learning
practitioners working with computational constraints.

</details>


### [741] [Sharper Risk Bound for Multi-Task Learning with Multi-Graph Dependent Data](https://arxiv.org/pdf/2502.18167)
*Xiao Shao, Guoqiang Wu*

Main category: cs.LG

TL;DR: The paper improves the risk bound in multi-task learning (MTL) with graph-dependent data from $O(\frac{1}{\sqrt{n}})$ to $O(\frac{\log n}{n})$ using a new Bennett-type inequality and analytical framework.


<details>
  <summary>Details</summary>
Motivation: Existing generalization analyses for MTL with graph-dependent data yield sub-optimal risk bounds, lacking sharper concentration inequalities for multi-graph dependent variables.

Method: Proposes a new Bennett-type inequality, a Talagrand-type inequality for empirical processes, and a local fractional Rademacher complexity framework.

Result: Achieves a sharper risk bound of $O(\frac{\log n}{n})$, validated by applications like Macro-AUC optimization and experiments.

Conclusion: The theoretical advancements provide superior generalization bounds for MTL with graph-dependent data, outperforming prior work.

Abstract: In multi-task learning (MTL) with each task involving graph-dependent data,
existing generalization analyses yield a \emph{sub-optimal} risk bound of
$O(\frac{1}{\sqrt{n}})$, where $n$ is the number of training samples of each
task. However, to improve the risk bound is technically challenging, which is
attributed to the lack of a foundational sharper concentration inequality for
multi-graph dependent random variables. To fill up this gap, this paper
proposes a new Bennett-type inequality, enabling the derivation of a sharper
risk bound of $O(\frac{\log n}{n})$. Technically, building on the proposed
Bennett-type inequality, we propose a new Talagrand-type inequality for the
empirical process, and further develop a new analytical framework of the local
fractional Rademacher complexity to enhance generalization analyses in MTL with
multi-graph dependent data. Finally, we apply the theoretical advancements to
applications such as Macro-AUC optimization, illustrating the superiority of
our theoretical results over prior work, which is also verified by experimental
results.

</details>


### [742] [One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling](https://arxiv.org/pdf/2505.13358)
*Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot*

Main category: cs.LG

TL;DR: The paper introduces Koopman Distillation Model (KDM), a novel offline distillation method for diffusion models, leveraging Koopman theory to enable efficient single-step generation while preserving semantic fidelity.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of iterative sampling in diffusion models and the potential of leveraging dynamical systems theory and structured latent trajectories for distillation.

Method: KDM encodes noisy inputs into a latent space where a linear operator propagates them forward, followed by a decoder for clean sample reconstruction, grounded in Koopman theory.

Result: KDM achieves state-of-the-art performance, improving FID scores by up to 40% in a single generation step.

Conclusion: KDM provides a principled and efficient distillation framework for diffusion models, validated by theoretical and empirical results.

Abstract: Diffusion-based generative models have demonstrated exceptional performance,
yet their iterative sampling procedures remain computationally expensive. A
prominent strategy to mitigate this cost is distillation, with offline
distillation offering particular advantages in terms of efficiency, modularity,
and flexibility. In this work, we identify two key observations that motivate a
principled distillation framework: (1) while diffusion models have been viewed
through the lens of dynamical systems theory, powerful and underexplored tools
can be further leveraged; and (2) diffusion models inherently impose
structured, semantically coherent trajectories in latent space. Building on
these observations, we introduce the Koopman Distillation Model KDM, a novel
offline distillation approach grounded in Koopman theory-a classical framework
for representing nonlinear dynamics linearly in a transformed space. KDM
encodes noisy inputs into an embedded space where a learned linear operator
propagates them forward, followed by a decoder that reconstructs clean samples.
This enables single-step generation while preserving semantic fidelity. We
provide theoretical justification for our approach: (1) under mild assumptions,
the learned diffusion dynamics admit a finite-dimensional Koopman
representation; and (2) proximity in the Koopman latent space correlates with
semantic similarity in the generated outputs, allowing for effective trajectory
alignment. Empirically, KDM achieves state-of-the-art performance across
standard offline distillation benchmarks, improving FID scores by up to 40% in
a single generation step. All implementation details and code for the
experimental setups are provided in our GitHub -
https://github.com/azencot-group/KDM, or in our project page -
https://sites.google.com/view/koopman-distillation-model.

</details>


### [743] [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/pdf/2503.14476)
*Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, Mingxuan Wang*

Main category: cs.LG

TL;DR: The paper introduces DAPO, a decoupled clip and dynamic sampling policy optimization algorithm, and open-sources a large-scale RL system for LLMs, achieving 50 points on AIME 2024. It addresses reproducibility issues by sharing key techniques, code, and datasets.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art reasoning LLMs conceal technical details, making it hard to reproduce their RL training results. The paper aims to bridge this gap by open-sourcing a reproducible system.

Method: Proposes DAPO, a novel algorithm with four key techniques, and releases a large-scale RL system built on the verl framework, including training code and a curated dataset.

Result: Achieves 50 points on AIME 2024 using the Qwen2.5-32B base model, demonstrating the effectiveness of the proposed approach.

Conclusion: The open-source system enhances reproducibility and supports future research in large-scale LLM reinforcement learning.

Abstract: Inference scaling empowers LLMs with unprecedented reasoning ability, with
reinforcement learning as the core technique to elicit complex reasoning.
However, key technical details of state-of-the-art reasoning LLMs are concealed
(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the
community still struggles to reproduce their RL training results. We propose
the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling
$\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and
fully open-source a state-of-the-art large-scale RL system that achieves 50
points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that
withhold training details, we introduce four key techniques of our algorithm
that make large-scale LLM RL a success. In addition, we open-source our
training code, which is built on the verl framework, along with a carefully
curated and processed dataset. These components of our open-source system
enhance reproducibility and support future research in large-scale LLM RL.

</details>


### [744] [LimeSoDa: A Dataset Collection for Benchmarking of Machine Learning Regressors in Digital Soil Mapping](https://arxiv.org/pdf/2502.20139)
*J. Schmidinger, S. Vogel, V. Barkov, A. -D. Pham, R. Gebbers, H. Tavakoli, J. Correa, T. R. Tavares, P. Filippi, E. J. Jones, V. Lukas, E. Boenecke, J. Ruehlmann, I. Schroeter, E. Kramer, S. Paetzold, M. Kodaira, A. M. J. -C. Wadoux, L. Bragazza, K. Metzger, J. Huang, D. S. M. Valente, J. L. Safanelli, E. L. Bottega, R. S. D. Dalmolin, C. Farkas, A. Steiger, T. Z. Horst, L. Ramirez-Lopez, T. Scholten, F. Stumpf, P. Rosso, M. M. Costa, R. S. Zandonadi, J. Wetterlind, M. Atzmueller*

Main category: cs.LG

TL;DR: The paper introduces LimeSoDa, an open-access dataset collection for benchmarking digital soil mapping methods, revealing context-dependent performance of algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing DSM studies often use single datasets with restricted access, leading to incomplete conclusions. Benchmarking across multiple datasets is needed to evaluate method strengths and limitations.

Method: The study introduces LimeSoDa, a collection of 31 datasets with soil properties and features, and benchmarks four learning algorithms (MLR, SVR, CatBoost, RF) across these datasets.

Result: No single algorithm was universally superior. MLR and SVR excelled with high-dimensional spectral data, while CatBoost and RF performed better with fewer features (<20).

Conclusion: LimeSoDa is a valuable resource for improving DSM method development and evaluation, highlighting the context-dependent nature of algorithm performance.

Abstract: Digital soil mapping (DSM) relies on a broad pool of statistical methods, yet
determining the optimal method for a given context remains challenging and
contentious. Benchmarking studies on multiple datasets are needed to reveal
strengths and limitations of commonly used methods. Existing DSM studies
usually rely on a single dataset with restricted access, leading to incomplete
and potentially misleading conclusions. To address these issues, we introduce
an open-access dataset collection called Precision Liming Soil Datasets
(LimeSoDa). LimeSoDa consists of 31 field- and farm-scale datasets from various
countries. Each dataset has three target soil properties: (1) soil organic
matter or soil organic carbon, (2) clay content and (3) pH, alongside a set of
features. Features are dataset-specific and were obtained by optical
spectroscopy, proximal- and remote soil sensing. All datasets were aligned to a
tabular format and are ready-to-use for modeling. We demonstrated the use of
LimeSoDa for benchmarking by comparing the predictive performance of four
learning algorithms across all datasets. This comparison included multiple
linear regression (MLR), support vector regression (SVR), categorical boosting
(CatBoost) and random forest (RF). The results showed that although no single
algorithm was universally superior, certain algorithms performed better in
specific contexts. MLR and SVR performed better on high-dimensional spectral
datasets, likely due to better compatibility with principal components. In
contrast, CatBoost and RF exhibited considerably better performances when
applied to datasets with a moderate number (< 20) of features. These
benchmarking results illustrate that the performance of a method is highly
context-dependent. LimeSoDa therefore provides an important resource for
improving the development and evaluation of statistical methods in DSM.

</details>


### [745] [Heterogeneity Matters even More in Distributed Learning: Study from Generalization Perspective](https://arxiv.org/pdf/2503.01598)
*Masoud Kavian, Romain Chor, Milad Sefidgaran, Abdellatif Zaidi*

Main category: cs.LG

TL;DR: The paper studies how data heterogeneity in distributed learning (one-round Federated Learning) affects generalization error, extending centralized learning bounds to distributed settings and applying them to DSVM, showing better generalization with higher data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of varying data distributions across clients on the generalization error in distributed learning systems, particularly in Federated Learning.

Method: Extends Conditional Mutual Information (CMI) bounds to distributed settings, connects with rate-distortion theory for tighter bounds, and applies these to DSVM for explicit error bounds.

Result: Generalization error bounds depend on data heterogeneity; DSVM generalizes better with higher dissimilarity between clients' data.

Conclusion: Data heterogeneity improves generalization in distributed learning, validated experimentally, with implications beyond DSVM.

Abstract: In this paper, we investigate the effect of data heterogeneity across clients
on the performance of distributed learning systems, i.e., one-round Federated
Learning, as measured by the associated generalization error. Specifically, $K$
clients have each $n$ training samples generated independently according to a
possibly different data distribution, and their individually chosen models are
aggregated by a central server. We study the effect of the discrepancy between
the clients' data distributions on the generalization error of the aggregated
model. First, we establish in-expectation and tail upper bounds on the
generalization error in terms of the distributions. In part, the bounds extend
the popular Conditional Mutual Information (CMI) bound, which was developed for
the centralized learning setting, i.e., $K=1$, to the distributed learning
setting with an arbitrary number of clients $K \geq 1$. Then, we connect with
information-theoretic rate-distortion theory to derive possibly tighter
\textit{lossy} versions of these bounds. Next, we apply our lossy bounds to
study the effect of data heterogeneity across clients on the generalization
error for the distributed classification problem in which each client uses
Support Vector Machines (DSVM). In this case, we establish explicit
generalization error bounds that depend explicitly on the data heterogeneity
degree. It is shown that the bound gets smaller as the degree of data
heterogeneity across clients increases, thereby suggesting that DSVM
generalizes better when the dissimilarity between the clients' training samples
is bigger. This finding, which goes beyond DSVM, is validated experimentally
through several experiments.

</details>


### [746] [PSDNorm: Test-Time Temporal Normalization for Deep Learning in Sleep Staging](https://arxiv.org/pdf/2503.04582)
*Théo Gnassounou, Antoine Collas, Rémi Flamary, Alexandre Gramfort*

Main category: cs.LG

TL;DR: PSDNorm, a novel normalization method using Monge mapping and temporal context, outperforms existing techniques in handling distribution shifts for biomedical signals without extra training.


<details>
  <summary>Details</summary>
Motivation: Addressing distribution shifts in biomedical data (e.g., sleep data) caused by variations across subjects, institutions, and devices, where current normalization methods fail to account for temporal dependencies.

Method: Proposes PSDNorm, leveraging Monge mapping and temporal context to normalize feature maps, acting as a test-time domain adaptation technique.

Result: Achieves state-of-the-art performance on unseen datasets, with 4x greater data efficiency than BatchNorm, validated on 10K subjects across 10 datasets.

Conclusion: PSDNorm effectively mitigates distribution shifts in biomedical signals, offering superior performance and efficiency without requiring additional training.

Abstract: Distribution shift poses a significant challenge in machine learning,
particularly in biomedical applications using data collected across different
subjects, institutions, and recording devices, such as sleep data. While
existing normalization layers, BatchNorm, LayerNorm and InstanceNorm, help
mitigate distribution shifts, when applied over the time dimension they ignore
the dependencies and auto-correlation inherent to the vector coefficients they
normalize. In this paper, we propose PSDNorm that leverages Monge mapping and
temporal context to normalize feature maps in deep learning models for signals.
Notably, the proposed method operates as a test-time domain adaptation
technique, addressing distribution shifts without additional training.
Evaluations with architectures based on U-Net or transformer backbones trained
on 10K subjects across 10 datasets, show that PSDNorm achieves state-of-the-art
performance on unseen left-out datasets while being 4-times more data-efficient
than BatchNorm.

</details>


### [747] [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/pdf/2505.07558)
*Rei Higuchi, Taiji Suzuki*

Main category: cs.LG

TL;DR: DDRO is a new method for aligning LLMs with human preferences without assuming specific models, ensuring statistical consistency and superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods assume specific preference models, leading to statistical inconsistency and unreliable convergence to true human preferences.

Method: Direct Density Ratio Optimization (DDRO) estimates the density ratio between preferred and unpreferred outputs, avoiding explicit preference modeling.

Result: DDRO is proven statistically consistent and outperforms existing methods on benchmarks.

Conclusion: DDRO enables reliable, data-driven alignment of LLMs with human preferences.

Abstract: Aligning large language models (LLMs) with human preferences is crucial for
safe deployment, yet existing methods assume specific preference models like
Bradley-Terry model. This assumption leads to statistical inconsistency, where
more data doesn't guarantee convergence to true human preferences. To address
this critical gap, we introduce a novel alignment method Direct Density Ratio
Optimization (DDRO). DDRO directly estimates the density ratio between
preferred and unpreferred output distributions, circumventing the need for
explicit human preference modeling. We theoretically prove that DDRO is
statistically consistent, ensuring convergence to the true preferred
distribution as the data size grows, regardless of the underlying preference
structure. Experiments demonstrate that DDRO achieves superior performance
compared to existing methods on many major benchmarks. DDRO unlocks the
potential for truly data-driven alignment, paving the way for more reliable and
human-aligned LLMs.

</details>


### [748] [MUSS: Multilevel Subset Selection for Relevance and Diversity](https://arxiv.org/pdf/2503.11126)
*Vu Nguyen, Andrey Kan*

Main category: cs.LG

TL;DR: The paper introduces MUSS, a multilevel method for relevant and diverse subset selection, improving performance and scalability in applications like recommender systems and RAG. It outperforms baselines in precision and speed and provides a tighter theoretical bound for DGDS.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing methods like MMR and DGDS in handling large-scale, distributed data for relevant and diverse subset selection.

Method: Proposes MUSS, a multilevel approach for subset selection, leveraging data structure to enhance scalability and performance.

Result: MUSS improves precision by up to 4% and is 20-80x faster in recommender systems. It also outperforms baselines in RAG-based QA accuracy and provides a tighter theoretical bound for DGDS.

Conclusion: MUSS is a scalable and efficient solution for relevant and diverse subset selection, with proven theoretical guarantees and practical performance gains.

Abstract: The problem of relevant and diverse subset selection has a wide range of
applications, including recommender systems and retrieval-augmented generation
(RAG). For example, in recommender systems, one is interested in selecting
relevant items, while providing a diversified recommendation. Constrained
subset selection problem is NP-hard, and popular approaches such as Maximum
Marginal Relevance (MMR) are based on greedy selection. Many real-world
applications involve large data, but the original MMR work did not consider
distributed selection. This limitation was later addressed by a method called
DGDS which allows for a distributed setting using random data partitioning.
Here, we exploit structure in the data to further improve both scalability and
performance on the target application. We propose MUSS, a novel method that
uses a multilevel approach to relevant and diverse selection. In a recommender
system application, our method can not only improve the performance up to $4$
percent points in precision, but is also $20$ to $80$ times faster. Our method
is also capable of outperforming baselines on RAG-based question answering
accuracy. We present a novel theoretical approach for analyzing this type of
problems, and show that our method achieves a constant factor approximation of
the optimal objective. Moreover, our analysis also resulted in a $\times 2$
tighter bound for DGDS compared to previously known bound.

</details>


### [749] [Uncertainty quantification of neural network models of evolving processes via Langevin sampling](https://arxiv.org/pdf/2504.14854)
*Cosmin Safta, Reese E. Jones, Ravi G. Patel, Raelynn Wonnacot, Dan S. Bolintineanu, Craig M. Hamel, Sharlotte L. B. Kramer*

Main category: cs.LG

TL;DR: A scalable hypernetwork framework for history-dependent processes using neural ODEs and Langevin sampling, outperforming variational inference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scalable and flexible inference for history-dependent processes, leveraging neural ODEs and stochastic sampling.

Method: Combines neural ODEs for state evolution with a trainable observation model, using Langevin sampling to approximate posterior distributions of parameters.

Result: Demonstrated effectiveness on chemical reaction and material physics data, showing superior performance over variational inference.

Conclusion: The framework offers a flexible and scalable solution for inference in complex, history-dependent processes.

Abstract: We propose a scalable, approximate inference hypernetwork framework for a
general model of history-dependent processes. The flexible data model is based
on a neural ordinary differential equation (NODE) representing the evolution of
internal states together with a trainable observation model subcomponent. The
posterior distribution corresponding to the data model parameters (weights and
biases) follows a stochastic differential equation with a drift term related to
the score of the posterior that is learned jointly with the data model
parameters. This Langevin sampling approach offers flexibility in balancing the
computational budget between the evaluation cost of the data model and the
approximation of the posterior density of its parameters. We demonstrate
performance of the ensemble sampling hypernetwork on chemical reaction and
material physics data and compare it to standard variational inference.

</details>


### [750] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/pdf/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: The paper examines cosine similarity's strengths and limitations in comparing embeddings, highlighting its widespread use, breakdown scenarios, and emerging alternatives.


<details>
  <summary>Details</summary>
Motivation: To provide a reflective analysis of cosine similarity's evolution, strengths, and limitations, especially when embedding norms carry semantic meaning.

Method: A selective examination of cosine similarity's performance, breakdown points, and emerging alternatives.

Result: Identifies scenarios where cosine similarity excels and fails, and discusses new methods addressing its limitations.

Conclusion: Offers conceptual clarity and practical insights for scientists viewing embeddings as geometric and philosophical objects.

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [751] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/pdf/2504.20660)
*Sahil Tomar, Shamshe Alam, Sandeep Kumar, Amit Mathur*

Main category: cs.LG

TL;DR: A hybrid quantum-classical reinforcement learning framework is proposed, combining quantum parallelism with classical methods to improve training speed, adaptability, and navigation performance in complex environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in reinforcement learning, such as slow convergence and adaptability in dynamic environments, by leveraging quantum computing's parallelism.

Method: Integrates quantum computing for robust Q-tables and turn cost estimations with classical reinforcement learning, tested in simulations and real-world scenarios like IIT Delhi campus.

Result: Demonstrates faster training convergence, improved path efficiency, smoother trajectories, and higher mission success rates in complex environments.

Conclusion: The framework shows promise for real-time autonomous navigation in unpredictable settings, validated by both simulations and practical tests.

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>


### [752] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/pdf/2505.01420)
*Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah*

Main category: cs.LG

TL;DR: The paper proposes evaluations to detect AI models capable of scheming, ensuring safety before deployment. Current models show no concerning capabilities.


<details>
  <summary>Details</summary>
Motivation: To prevent severe loss of control from AI models that might covertly pursue misaligned objectives, necessitating pre-deployment safety checks.

Method: A suite of evaluations measuring reasoning capabilities: stealth (circumventing oversight) and situational awareness (instrumental reasoning).

Result: Current frontier models show no concerning levels of stealth or situational awareness.

Conclusion: The evaluations can help ensure AI safety by ruling out scheming capabilities before deployment.

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [753] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/pdf/2505.02222)
*Essential AI, :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani*

Main category: cs.LG

TL;DR: Muon, a second-order optimizer, outperforms AdamW in data efficiency at large batch sizes and computational efficiency, enabling economical training. Combined with muP, it offers efficient hyperparameter transfer with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To improve upon AdamW's limitations in data efficiency at large batch sizes and computational cost, Muon is introduced as a simpler, more effective second-order optimizer.

Method: Muon is combined with maximal update parameterization (muP) and a telescoping algorithm to handle hyperparameter transfer and error sources efficiently.

Result: Muon retains data efficiency beyond the critical batch size, remains computationally efficient, and works well with muP, validated on models up to 4B parameters.

Conclusion: Muon expands the Pareto frontier over AdamW, offering a more economical and efficient training solution, especially at large scales.

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>


### [754] [New Statistical and Computational Results for Learning Junta Distributions](https://arxiv.org/pdf/2505.05819)
*Lorenzo Beretta*

Main category: cs.LG

TL;DR: The paper shows that learning k-junta distributions is computationally equivalent to LPN and presents an optimal algorithm for it.


<details>
  <summary>Details</summary>
Motivation: To understand the computational and statistical complexity of learning k-junta distributions, a problem linked to the well-known LPN problem.

Method: The paper proves the equivalence between learning k-junta distributions and LPN, and designs an algorithm with optimal statistical complexity.

Result: The algorithm's statistical complexity is optimal, and its computational complexity matches prior non-optimal methods.

Conclusion: The results suggest no significant improvement is possible without a breakthrough in solving LPN.

Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a
distribution is a $k$-junta if its probability mass function depends on a
subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally}
equivalent to learning $k$-parity functions with noise (LPN), a landmark
problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical
complexity is optimal, up to polylogarithmic factors. Computationally, our
algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be
significantly improved, statistically or computationally, barring a
breakthrough for LPN.

</details>


### [755] [Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles](https://arxiv.org/pdf/2505.08782)
*Junghoon Justin Park, Jiook Cha, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Shinjae Yoo*

Main category: cs.LG

TL;DR: A multi-chip ensemble VQC framework is proposed to address noise, scalability, and trainability issues in QML by partitioning computations across smaller quantum chips and using controlled inter-chip entanglement.


<details>
  <summary>Details</summary>
Motivation: Current QML faces challenges like noise, limited scalability, and poor trainability in VQCs on existing hardware.

Method: The approach uses a multi-chip ensemble VQC framework, partitioning computations across smaller quantum chips with controlled inter-chip entanglement.

Result: The method mitigates barren plateaus, enhances generalization, and reduces quantum error bias and variance, validated on benchmarks (MNIST, FashionMNIST, CIFAR-10) and a PhysioNet EEG dataset.

Conclusion: The framework aligns with modular quantum hardware, enabling more scalable QML.

Abstract: Practical Quantum Machine Learning (QML) is challenged by noise, limited
scalability, and poor trainability in Variational Quantum Circuits (VQCs) on
current hardware. We propose a multi-chip ensemble VQC framework that
systematically overcomes these hurdles. By partitioning high-dimensional
computations across ensembles of smaller, independently operating quantum chips
and leveraging controlled inter-chip entanglement boundaries, our approach
demonstrably mitigates barren plateaus, enhances generalization, and uniquely
reduces both quantum error bias and variance simultaneously without additional
mitigation overhead. This allows for robust processing of large-scale data, as
validated on standard benchmarks (MNIST, FashionMNIST, CIFAR-10) and a
real-world PhysioNet EEG dataset, aligning with emerging modular quantum
hardware and paving the way for more scalable QML.

</details>


### [756] [Dual-Balancing for Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.11117)
*Chenhong Zhou, Jie Chen, Zaifeng Yang, Ching Eng Png*

Main category: cs.LG

TL;DR: DB-PINN improves PINNs by dynamically balancing loss weights to address gradient and fitting imbalances, enhancing accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: Vanilla PINNs suffer from poor accuracy and slow convergence due to multi-objective optimization issues, prompting the need for a balanced approach.

Method: DB-PINN uses inter-balancing (gradient distribution) and intra-balancing (fitting difficulty) to dynamically adjust loss weights, with a robust update strategy for stability.

Result: DB-PINN outperforms gradient-based weighting methods in convergence speed and prediction accuracy.

Conclusion: The proposed DB-PINN effectively addresses imbalance issues in PINNs, offering superior performance and stable training.

Abstract: Physics-informed neural networks (PINNs) have emerged as a new learning
paradigm for solving partial differential equations (PDEs) by enforcing the
constraints of physical equations, boundary conditions (BCs), and initial
conditions (ICs) into the loss function. Despite their successes, vanilla PINNs
still suffer from poor accuracy and slow convergence due to the intractable
multi-objective optimization issue. In this paper, we propose a novel
Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by
integrating inter-balancing and intra-balancing to alleviate two imbalance
issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance
between PDE residual loss and condition-fitting losses by determining an
aggregated weight that offsets their gradient distribution discrepancies.
Intra-balancing acts on condition-fitting losses to tackle the imbalance in
fitting difficulty across diverse conditions. By evaluating the fitting
difficulty based on the loss records, intra-balancing can allocate the
aggregated weight proportionally to each condition loss according to its
fitting difficulty level. We further introduce a robust weight update strategy
to prevent abrupt spikes and arithmetic overflow in instantaneous weight values
caused by large loss variances, enabling smooth weight updating and stable
training. Extensive experiments demonstrate that DB-PINN achieves significantly
superior performance than those popular gradient-based weighting methods in
terms of convergence speed and prediction accuracy. Our code and supplementary
material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.

</details>


### [757] [JULI: Jailbreak Large Language Models by Self-Introspection](https://arxiv.org/pdf/2505.11790)
*Jesson Wang, Zhanhao Hu, David Wagner*

Main category: cs.LG

TL;DR: JULI is a method to jailbreak LLMs by manipulating token log probabilities using BiasNet, effective even with limited access to the model.


<details>
  <summary>Details</summary>
Motivation: Existing attacks on safety-aligned LLMs often require model weight access, which is unavailable for proprietary API-calling models. JULI addresses this gap.

Method: JULI uses BiasNet to manipulate token log probabilities, requiring only top-5 token log probabilities for black-box API-calling LLMs.

Result: JULI outperforms SOTA methods in jailbreaking LLMs under black-box conditions.

Conclusion: JULI provides a robust and effective way to compromise safety-aligned LLMs without needing model weight access.

Abstract: Large Language Models (LLMs) are trained with safety alignment to prevent
generating malicious content. Although some attacks have highlighted
vulnerabilities in these safety-aligned LLMs, they typically have limitations,
such as necessitating access to the model weights or the generation process.
Since proprietary models through API-calling do not grant users such
permissions, these attacks find it challenging to compromise them. In this
paper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks
LLMs by manipulating the token log probabilities, using a tiny plug-in block,
BiasNet. JULI relies solely on the knowledge of the target LLM's predicted
token log probabilities. It can effectively jailbreak API-calling LLMs under a
black-box setting and knowing only top-$5$ token log probabilities. Our
approach demonstrates superior effectiveness, outperforming existing
state-of-the-art (SOTA) approaches across multiple metrics.

</details>


### [758] [Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification](https://arxiv.org/pdf/2505.11985)
*Sabrina Khurshid, Gourab Ghatak, Mohammad Shahid Abdulla*

Main category: cs.LG

TL;DR: The paper introduces algorithms for selecting the highest-variance arm in multi-armed bandit settings, focusing on regret and fixed-budget BAI. UCB-VV and SHVV are proposed, with theoretical guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying the arm with the highest variance in multi-armed bandit problems, which is crucial for applications like finance and risk management.

Method: Develops UCB-VV for regret minimization with logarithmic regret bounds and SHVV for fixed-budget BAI with exponential error probability bounds. Extends results to sub-Gaussian distributions.

Result: UCB-VV achieves order-optimal regret (O(log n)), and SHVV matches lower bounds for error probability. Empirical results show UCB-VV outperforms ε-greedy, while SHVV beats uniform sampling.

Conclusion: The proposed algorithms are theoretically sound and empirically effective, with applications demonstrated in call option trading.

Abstract: This paper focuses on selecting the arm with the highest variance from a set
of $K$ independent arms. Specifically, we focus on two settings: (i) regret
setting, that penalizes the number of pulls of suboptimal arms in terms of
variance, and (ii) fixed-budget BAI setting, that evaluates the ability of an
algorithm to determine the arm with the highest variance after a fixed number
of pulls. We develop a novel online algorithm called \texttt{UCB-VV} for the
regret setting and show that its upper bound on regret for bounded rewards
evolves as $\mathcal{O}\left(\log{n}\right)$ where $n$ is the horizon. By
deriving the lower bound on the regret, we show that \texttt{UCB-VV} is order
optimal. For the fixed budget BAI setting, we propose the \texttt{SHVV}
algorithm. We show that the upper bound of the error probability of
\texttt{SHVV} evolves as $\exp\left(-\frac{n}{\log(K) H}\right)$, where $H$
represents the complexity of the problem, and this rate matches the
corresponding lower bound. We extend the framework from bounded distributions
to sub-Gaussian distributions using a novel concentration inequality on the
sample variance. Leveraging the same, we derive a concentration inequality for
the empirical Sharpe ratio (SR) for sub-Gaussian distributions, which was
previously unknown in the literature. Empirical simulations show that
\texttt{UCB-VV} consistently outperforms \texttt{$\epsilon$-greedy} across
different sub-optimality gaps, though it is surpassed by \texttt{VTS}, which
exhibits the lowest regret, albeit lacking in theoretical guarantees. We also
illustrate the superior performance of \texttt{SHVV}, for a fixed budget
setting under 6 different setups against uniform sampling. Finally, we conduct
a case study to empirically evaluate the performance of the \texttt{UCB-VV} and
\texttt{SHVV} in call option trading on $100$ stocks generated using geometric
Brownian motion (GBM).

</details>


### [759] [Harnessing the Universal Geometry of Embeddings](https://arxiv.org/pdf/2505.12540)
*Rishi Jha, Collin Zhang, Vitaly Shmatikov, John X. Morris*

Main category: cs.LG

TL;DR: Unsupervised method for translating text embeddings between vector spaces without paired data, achieving high similarity across models.


<details>
  <summary>Details</summary>
Motivation: To enable embedding translation without reliance on paired data or predefined matches, addressing a gap in current methods.

Method: Uses a universal latent representation to translate embeddings across different model architectures and datasets.

Result: Achieves high cosine similarity in translations, demonstrating effectiveness across diverse models.

Conclusion: The method poses security risks for vector databases, as adversaries could exploit it to infer sensitive information.

Abstract: We introduce the first method for translating text embeddings from one vector
space to another without any paired data, encoders, or predefined sets of
matches. Our unsupervised approach translates any embedding to and from a
universal latent representation (i.e., a universal semantic structure
conjectured by the Platonic Representation Hypothesis). Our translations
achieve high cosine similarity across model pairs with different architectures,
parameter counts, and training datasets.
  The ability to translate unknown embeddings into a different space while
preserving their geometry has serious implications for the security of vector
databases. An adversary with access only to embedding vectors can extract
sensitive information about the underlying documents, sufficient for
classification and attribute inference.

</details>


### [760] [A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection](https://arxiv.org/pdf/2505.12586)
*Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, Mohsen Imani*

Main category: cs.LG

TL;DR: A lightweight, plug-in detection framework for adversarial examples in DNNs, using internal layer inconsistencies and requiring only benign data.


<details>
  <summary>Details</summary>
Motivation: DNNs are vulnerable to adversarial examples, and existing detection methods are inefficient or lack generalizability.

Method: Leverages internal layer-wise inconsistencies and proposes Recovery Testing (RT) and Logit-layer Testing (LT) for detection.

Result: Achieves state-of-the-art detection performance on CIFAR-10, CIFAR-100, and ImageNet with minimal computational overhead.

Conclusion: The framework is efficient, generalizable, and maintains clean accuracy, offering a practical defense against adversarial attacks.

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial
examples--subtle, imperceptible perturbations that can lead to incorrect
predictions. While detection-based defenses offer a practical alternative to
adversarial training, many existing methods depend on external models, complex
architectures, heavy augmentations, or adversarial data, limiting their
efficiency and generalizability. We introduce a lightweight, plug-in detection
framework that leverages internal layer-wise inconsistencies within the target
model itself, requiring only benign data for calibration. Our approach is
grounded in the A Few Large Shifts Assumption, which posits that adversarial
perturbations typically induce large representation shifts in a small subset of
layers. Building on this, we propose two complementary strategies--Recovery
Testing (RT) and Logit-layer Testing (LT)--to expose internal disruptions
caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under
both standard and adaptive threat models, our method achieves state-of-the-art
detection performance with negligible computational overhead and no compromise
to clean accuracy.

</details>


### [761] [Inferring stochastic dynamics with growth from cross-sectional data](https://arxiv.org/pdf/2505.13197)
*Stephen Zhang, Suryanarayana Maddu, Xiaojie Qiu, Victor Chardès*

Main category: cs.LG

TL;DR: A novel method, unbalanced probability flow inference, is introduced to infer biophysical models from time-resolved single-cell omics data, addressing challenges like cell division, death, and noise.


<details>
  <summary>Details</summary>
Motivation: Time-resolved single-cell omics data is destructive and cross-sectional, making it hard to infer realistic biophysical models due to stochastic dynamics, cell division, and death.

Method: The approach uses a Lagrangian formulation of the Fokker-Planck equation to disentangle drift, noise, and growth in stochastic dynamics.

Result: The method outperforms existing techniques in accuracy and simplicity, validated on simulated and real single-cell RNA-seq datasets.

Conclusion: Unbalanced probability flow inference provides a robust and efficient solution for modeling biological processes from single-cell omics data.

Abstract: Time-resolved single-cell omics data offers high-throughput, genome-wide
measurements of cellular states, which are instrumental to reverse-engineer the
processes underpinning cell fate. Such technologies are inherently destructive,
allowing only cross-sectional measurements of the underlying stochastic
dynamical system. Furthermore, cells may divide or die in addition to changing
their molecular state. Collectively these present a major challenge to
inferring realistic biophysical models. We present a novel approach,
\emph{unbalanced} probability flow inference, that addresses this challenge for
biological processes modelled as stochastic dynamics with growth. By leveraging
a Lagrangian formulation of the Fokker-Planck equation, our method accurately
disentangles drift from intrinsic noise and growth. We showcase the
applicability of our approach through evaluation on a range of simulated and
real single-cell RNA-seq datasets. Comparing to several existing methods, we
find our method achieves higher accuracy while enjoying a simple two-step
training scheme.

</details>


### [762] [Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning](https://arxiv.org/pdf/2505.13317)
*Song-Lin Li, Rui Zhu, Yu-Feng Li, Lan-Zhe Guo*

Main category: cs.LG

TL;DR: The paper compares SSL and pre-trained models for tasks with scarce labeled data, finding pre-trained VLMs generally outperform SSL except in low-resolution or unstructured data cases.


<details>
  <summary>Details</summary>
Motivation: To determine whether to use unlabeled data (SSL) or pre-trained models when labeled data is scarce in target tasks.

Method: Proposes Few-shot SSL, a framework for fair comparison between SSL and pre-trained VLMs, controlling labeled data usage.

Result: Pre-trained VLMs outperform SSL in most cases, except for low-resolution or semantically unclear data.

Conclusion: Future SSL research should compare with pre-trained models and explore integration, like enhancing pseudo-labeling with pre-trained knowledge. A unified framework is released for further research.

Abstract: Semi-supervised learning (SSL) alleviates the cost of data labeling process
by exploiting unlabeled data, and has achieved promising results on various
tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm
has garnered significant attention in recent years, and exploiting pre-trained
models could also reduce the requirement of labeled data in downstream tasks.
Therefore, a question naturally occurs: \emph{When the labeled data is scarce
in the target tasks, should we exploit unlabeled data or pre-trained models?}
To answer this question, we select pre-trained Vision-Language Models (VLMs) as
representative pretrain-finetuning instances and propose \textit{Few-shot SSL}
-- a framework that enables fair comparison between these two paradigms by
controlling the amount of labeled data used. Extensive experiments across
various settings demonstrate that pre-trained VLMs generally outperform SSL
methods in nearly all cases, except when the data has low resolution or lacks
clear semantic structure. Therefore, we encourage future SSL research to
compare with pre-trained models and explore deeper integration, such as using
pre-trained knowledge to enhance pseudo-labeling. To support future research,
we release our unified reproduction and evaluation framework. Codes are
available
\href{https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566
}{here}.

</details>


### [763] [A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut](https://arxiv.org/pdf/2505.13405)
*Gabriel Malikal, Ismail Alkhouri, Alvaro Velasquez, Adam M Alessio, Saiprasad Ravishankar*

Main category: cs.LG

TL;DR: A reinforcement learning approach improves hyperplane rounding in the GW algorithm for MaxCut, outperforming traditional methods without requiring training data.


<details>
  <summary>Details</summary>
Motivation: Heuristic and learning-based methods for MaxCut lack generalizability and scalability, prompting a need for a training-data-free solution.

Method: A non-episodic reinforcement learning formulation optimizes hyperplane rounding in the GW algorithm via an MDP.

Result: The method consistently achieves better cuts on large-scale graphs with diverse properties.

Conclusion: The proposed approach enhances the GW algorithm's performance without relying on labeled datasets, offering scalability and generalizability.

Abstract: The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal
solution is NP-hard in the worst case. As a result, heuristic-based algorithms
are commonly used, though their design often requires significant domain
expertise. More recently, learning-based methods trained on large (un)labeled
datasets have been proposed; however, these approaches often struggle with
generalizability and scalability. A well-known approximation algorithm for
MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic
Unconstrained Binary Optimization (QUBO) formulation into a semidefinite
program (SDP). The GW algorithm then applies hyperplane rounding by uniformly
sampling a random hyperplane to convert the SDP solution into binary node
assignments. In this paper, we propose a training-data-free approach based on a
non-episodic reinforcement learning formulation, in which an agent learns to
select improved rounding hyperplanes that yield better cuts than those produced
by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our
method consistently achieves better cuts across large-scale graphs with varying
densities and degree distributions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [764] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/pdf/2505.13516)
*Zhipeng Hou, Junyi Tang, Yipeng Wang*

Main category: cs.MA

TL;DR: HALO is a hierarchical multi-agent framework using LLMs for task decomposition, role-specific agent instantiation, and structured workflow search, outperforming baselines by 14.4% on expert-level tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MAS systems lack adaptability due to predefined roles and static communication, limiting performance on specialized tasks.

Method: HALO uses a hierarchical architecture with planning, role-design, and inference agents, plus MCTS for workflow search and adaptive prompt refinement.

Result: HALO improves performance by 14.4% on average, with gains up to 19.6% in specialized tasks like algebra and moral reasoning.

Conclusion: HALO enhances adaptability and performance in expert-level tasks, offering a scalable solution for complex MAS environments.

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [765] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/pdf/2505.13523)
*Jun Liu, Ke Yu, Keliang Chen, Ke Li, Yuxinyue Qian, Xiaolian Guo, Haozhe Song, Yinming Li*

Main category: cs.MA

TL;DR: The paper proposes Agent Collaboration Protocols (ACPs) to standardize communication for heterogeneous agents in the Internet of Agents (IoA), addressing interoperability and scalability challenges.


<details>
  <summary>Details</summary>
Motivation: Existing agent communication protocols (MCP, A2A, ANP) are fragmented and scenario-specific, hindering seamless collaboration in the IoA.

Method: ACPs include registration, discovery, interaction, and tooling protocols to enable trustable access, capability orchestration, and workflow construction.

Result: ACPs are demonstrated effective in a collaborative restaurant booking scenario.

Conclusion: ACPs provide a foundation for a secure, open, and scalable agent internet infrastructure.

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [766] [Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.13543)
*Muyang Fan, Songyang Liu, Weizi Li*

Main category: cs.MA

TL;DR: A decentralized multi-agent RL framework is proposed for large-scale mixed traffic control, tested on a 14-intersection network, showing reduced congestion by adjusting OD flows.


<details>
  <summary>Details</summary>
Motivation: Addressing traffic congestion in urban areas by leveraging autonomous driving and RL, focusing on large-scale mixed traffic networks.

Method: Decentralized multi-agent reinforcement learning framework for controlling intersections with traffic signals or robotic vehicles.

Result: Strategic adjustment of OD flow patterns reduces congestion, improving traffic efficiency.

Conclusion: The framework offers a viable solution for enhancing urban mobility in large-scale mixed traffic scenarios.

Abstract: Traffic congestion remains a major challenge for modern urban transportation,
diminishing both efficiency and quality of life. While autonomous driving
technologies and reinforcement learning (RL) have shown promise for improving
traffic control, most prior work has focused on small-scale networks or
isolated intersections. Large-scale mixed traffic control, involving both
human-driven and robotic vehicles, remains underexplored. In this study, we
propose a decentralized multi-agent reinforcement learning framework for
managing large-scale mixed traffic networks, where intersections are controlled
either by traditional traffic signals or by robotic vehicles. We evaluate our
approach on a real-world network of 14 intersections in Colorado Springs,
Colorado, USA, using average vehicle waiting time as the primary measure of
traffic efficiency. Results demonstrate that strategically adjusting major
origin-destination (OD) flow patterns can effectively reduce congestion,
offering a new pathway for enhancing urban mobility.

</details>


### [767] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/pdf/2505.13941)
*Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Turkmen, Cuixiong Hu, Huzefa Rangwala, Ying Nian Wu, Bernie Wang, George Karypis*

Main category: cs.MA

TL;DR: MLZero is a multi-agent framework using LLMs for end-to-end ML automation, excelling in multimodal tasks with minimal human input.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML systems require expert input, especially for multimodal data. MLZero aims to automate ML workflows fully.

Method: Uses a cognitive perception module and enhances LLMs with semantic/episodic memory to improve code generation.

Result: Outperforms competitors on benchmarks (MLE-Bench Lite and Multimodal AutoML Agent Benchmark) with high success rates.

Conclusion: MLZero is robust, effective, and scalable, even with smaller LLMs, setting a new standard for AutoML.

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [768] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/pdf/2505.14081)
*Luca Ballotta, Nicola Bastianello, Riccardo M. G. Ferrari, Karl H. Johansson*

Main category: cs.MA

TL;DR: The paper proposes a distributed learning algorithm combining gradient descent and opinion dynamics to address personalization and resilience in multi-agent networks, with tunable parameters for control.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of personalization (tailored local models) and resilience (against cyberattacks/anomalies) in distributed learning for multi-agent systems.

Method: A distributed learning algorithm integrating distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics.

Result: The algorithm achieves high global accuracy for personalized models and under malicious conditions, with controllable convergence and model neighborhood.

Conclusion: The proposed method effectively balances personalization and resilience, outperforming standard strategies in synthetic and real-world tasks.

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


### [769] [Empowering LLMs in Task-Oriented Dialogues: A Domain-Independent Multi-Agent Framework and Fine-Tuning Strategy](https://arxiv.org/pdf/2505.14299)
*Zihao Feng, Xiaoxue Wang, Bowen Wu, Weihong Zhong, Zhen Xu, Hailong Cao, Tiejun Zhao, Ying Li, Baoxun Wang*

Main category: cs.MA

TL;DR: A Domain-Independent Multi-Agent Framework (DIMF) is proposed to improve task-oriented dialogue systems on lightweight LLMs by splitting tasks into domain-independent agents and using DPO and DDA methods for better performance.


<details>
  <summary>Details</summary>
Motivation: Current single-agent approaches struggle with lightweight LLMs due to their limited handling of complex logic, prompting the need for a multi-agent solution.

Method: DIMF divides tasks into Intent Classification, Slot Filling, and Response Agents, using DPO for contextual understanding and DDA to prevent training degradation.

Result: Experiments on MultiWOZ show superior performance and generalizability, including zero-shot capability.

Conclusion: DIMF effectively addresses lightweight LLM limitations, offering a scalable and adaptable framework for task-oriented dialogues.

Abstract: Task-oriented dialogue systems based on Large Language Models (LLMs) have
gained increasing attention across various industries and achieved significant
results. Current approaches condense complex procedural workflows into a single
agent to achieve satisfactory performance on large-scale LLMs. However, these
approaches face challenges to achieve comparable performance on fine-tuned
lightweight LLMs, due to their limited capabilities in handling multiple
complex logic. In this work, we design a Domain-Independent Multi-Agent
Framework (DIMF), which contains Intent Classification Agent, Slot Filling
Agent and Response Agent. This approach simplifies the learning complexity and
enhances the generalization ability by separating the tasks into
domain-independent components. In this framework, we enhance the capabilities
in contextual understanding using the Direct Preference Optimisation (DPO)
method, and propose a simple and effective Data Distribution Adaptation (DDA)
method to mitigate degradation issues during DPO training. Experiments
conducted on the MultiWOZ datasets show that our proposed method achieves a
better average performance among all the baselines. Extensive analysis also
demonstrates that our proposed framework exhibits excellent generalizability
and zero-shot capability.

</details>


### [770] [Factorised Active Inference for Strategic Multi-Agent Interactions](https://arxiv.org/pdf/2411.07362)
*Jaime Ruiz-Serra, Patrick Sweeney, Michael S. Harré*

Main category: cs.MA

TL;DR: The paper integrates Active Inference (AIF) and game theory to model strategic decision-making in multi-agent systems, focusing on dynamic environments with changing payoffs.


<details>
  <summary>Details</summary>
Motivation: To bridge AIF (agent adaptation) and game theory (strategic interactions) for deeper insights into collective intelligence in dynamic settings.

Method: Proposes a factorized generative model where agents maintain beliefs about others' states for strategic planning, tested in iterated general-sum games.

Result: Shows ensemble effects of game transitions and dynamical analysis of AIF quantities, revealing EFE's role in characterizing Nash Equilibria.

Conclusion: Integration of AIF and game theory enhances understanding of collective learning and optimization in dynamic, cooperative/non-cooperative environments.

Abstract: Understanding how individual agents make strategic decisions within
collectives is important for advancing fields as diverse as economics,
neuroscience, and multi-agent systems. Two complementary approaches can be
integrated to this end. The Active Inference framework (AIF) describes how
agents employ a generative model to adapt their beliefs about and behaviour
within their environment. Game theory formalises strategic interactions between
agents with potentially competing objectives. To bridge the gap between the
two, we propose a factorisation of the generative model whereby each agent
maintains explicit, individual-level beliefs about the internal states of other
agents, and uses them for strategic planning in a joint context. We apply our
model to iterated general-sum games with two and three players, and study the
ensemble effects of game transitions, where the agents' preferences (game
payoffs) change over time. This non-stationarity, beyond that caused by
reciprocal adaptation, reflects a more naturalistic environment in which agents
need to adapt to changing social contexts. Finally, we present a dynamical
analysis of key AIF quantities: the variational free energy (VFE) and the
expected free energy (EFE) from numerical simulation data. The ensemble-level
EFE allows us to characterise the basins of attraction of games with multiple
Nash Equilibria under different conditions, and we find that it is not
necessarily minimised at the aggregate level. By integrating AIF and game
theory, we can gain deeper insights into how intelligent collectives emerge,
learn, and optimise their actions in dynamic environments, both cooperative and
non-cooperative.

</details>


### [771] [Attention Mechanism for LLM-based Agents Dynamic Diffusion under Information Asymmetry](https://arxiv.org/pdf/2502.13160)
*Yiwen Zhang, Yifu Wu, Wenyue Hua, Xiang Lu, Xuming Hu*

Main category: cs.MA

TL;DR: A framework for multi-agent information diffusion using LLMs, addressing social relationship perception and dynamic attention to improve simulation realism.


<details>
  <summary>Details</summary>
Motivation: Current social simulations lack realism due to ignoring information opacity, relationship variability, and diffusion diversity.

Method: Proposes a dynamic attention mechanism for agents, starting with small groups and scaling up, while studying information diffusion in asymmetric environments.

Result: Identifies LLM deficiencies in social relationship perception and diverse actions, offering a solution via dynamic attention.

Conclusion: The framework enhances multi-agent simulations by addressing LLM limitations and exploring real-world information diffusion dynamics.

Abstract: Large language models have been used to simulate human society using
multi-agent systems. Most current social simulation research emphasizes
interactive behaviors in fixed environments, ignoring information opacity,
relationship variability, and diffusion diversity. In this paper, we first
propose a general framework for exploring multi-agent information diffusion. We
identified LLMs' deficiency in the perception and utilization of social
relationships, as well as diverse actions. Then, we designed a dynamic
attention mechanism to help agents allocate attention to different information,
addressing the limitations of the LLM attention mechanism. Agents start by
responding to external information stimuli within a five-agent group,
increasing group size and forming information circles while developing
relationships and sharing information. Additionally, we explore the information
diffusion features in the asymmetric open environment by observing the
evolution of information gaps, diffusion patterns, and the accumulation of
social capital, which are closely linked to psychological, sociological, and
communication theories.

</details>


### [772] [Steady-State Strategy Synthesis for Swarms of Autonomous Agents](https://arxiv.org/pdf/2505.12406)
*Martin Jonáš, Antonín Kučera, Vojtěch Kůr, Jan Mačák*

Main category: cs.MA

TL;DR: The paper explores steady-state synthesis in multiagent systems, revealing computational hardness and limitations of memoryless policies, but proposes an efficient algorithm for a subclass.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving desired long-run average visit frequencies in multiagent systems, extending the solvable single-agent case.

Method: Analyzes computational complexity, proves insufficiency of memoryless strategies, and designs an efficient algorithm for a specific subclass of memoryless profiles.

Result: Shows PSPACE or NP-hardness for multiagent variants, inefficacy of memoryless policies, and presents a scalable algorithm outperforming naive approaches.

Conclusion: Highlights computational barriers but offers a practical solution for a restricted case, validated by experimental results.

Abstract: Steady-state synthesis aims to construct a policy for a given MDP $D$ such
that the long-run average frequencies of visits to the vertices of $D$ satisfy
given numerical constraints. This problem is solvable in polynomial time, and
memoryless policies are sufficient for approximating an arbitrary frequency
vector achievable by a general (infinite-memory) policy.
  We study the steady-state synthesis problem for multiagent systems, where
multiple autonomous agents jointly strive to achieve a suitable frequency
vector. We show that the problem for multiple agents is computationally hard
(PSPACE or NP hard, depending on the variant), and memoryless strategy profiles
are insufficient for approximating achievable frequency vectors. Furthermore,
we prove that even evaluating the frequency vector achieved by a given
memoryless profile is computationally hard. This reveals a severe barrier to
constructing an efficient synthesis algorithm, even for memoryless profiles.
Nevertheless, we design an efficient and scalable synthesis algorithm for a
subclass of full memoryless profiles, and we evaluate this algorithm on a large
class of randomly generated instances. The experimental results demonstrate a
significant improvement against a naive algorithm based on strategy sharing.

</details>


### [773] [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/pdf/2505.13393)
*Christopher K. Frantz*

Main category: cs.MA

TL;DR: The paper introduces IG Parser, a tool for analyzing formal and informal institutions using a unique syntax (IG Script) to automate and standardize content analysis.


<details>
  <summary>Details</summary>
Motivation: To facilitate rigorous qualitative analysis of institutions (rules, norms, strategies) by automating their encoding and transformation for diverse analytical techniques.

Method: Uses IG Script, a syntax based on Institutional Grammar 2.0, to encode natural language and automate transformations for analysis.

Result: IG Parser provides a standardized, automated approach to institutional analysis, demonstrated through illustrative examples.

Conclusion: IG Parser enhances institutional analysis by combining conceptual rigor with practical automation, supporting diverse downstream applications.

Abstract: This article provides an overview of IG Parser, a software that facilitates
qualitative content analysis of formal (e.g., legal) rules or informal (e.g.,
social) norms, and strategies (such as conventions) -- referred to as
institutions -- that govern social systems and operate configurally to describe
institutional systems. To this end, the IG Parser employs a distinctive syntax
that ensures rigorous encoding of natural language, while automating the
transformation into various formats that support the downstream analysis using
diverse analytical techniques. The conceptual core of the IG Parser is an
associated syntax, IG Script, that operationalizes the conceptual foundations
of the Institutional Grammar, and more specifically the Institutional Grammar
2.0, an analytical paradigm for institutional analysis. This article presents
the IG Parser, including its conceptual foundations, the syntax specification
of IG Script, and its architectural principles. This overview is augmented with
selective illustrative examples that highlight its use and the associated
benefits.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [774] [ShieldVLM: Safeguarding the Multimodal Implicit Toxicity via Deliberative Reasoning with LVLMs](https://arxiv.org/pdf/2505.14035)
*Shiyao Cui, Qinglin Zhang, Xuan Ouyang, Renmiao Chen, Zhexin Zhang, Yida Lu, Hongning Wang, Han Qiu, Minlie Huang*

Main category: cs.MM

TL;DR: The paper addresses the challenge of detecting multimodal implicit toxicity in text-image content, introduces a taxonomy and dataset (MMIT-dataset), and proposes ShieldVLM, a model for detecting such toxicity, which outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: The rise of multimodal implicit toxicity, where benign text and images combine to convey toxicity, poses a challenge for moderation, especially in social platforms and interactions with Large Vision-Language Models (LVLMs).

Method: The authors build a taxonomy for multimodal implicit toxicity (MMIT), create the MMIT-dataset (2,100 multimodal statements/prompts), and develop ShieldVLM, a model using cross-modal reasoning for detection.

Result: ShieldVLM outperforms existing baselines in detecting both implicit and explicit toxicity.

Conclusion: The work fills a gap in multimodal toxicity detection, providing a dataset and model (ShieldVLM) for future research, with potential applications in social platforms and LVLM interactions.

Abstract: Toxicity detection in multimodal text-image content faces growing challenges,
especially with multimodal implicit toxicity, where each modality appears
benign on its own but conveys hazard when combined. Multimodal implicit
toxicity appears not only as formal statements in social platforms but also
prompts that can lead to toxic dialogs from Large Vision-Language Models
(LVLMs). Despite the success in unimodal text or image moderation, toxicity
detection for multimodal content, particularly the multimodal implicit
toxicity, remains underexplored. To fill this gap, we comprehensively build a
taxonomy for multimodal implicit toxicity (MMIT) and introduce an MMIT-dataset,
comprising 2,100 multimodal statements and prompts across 7 risk categories (31
sub-categories) and 5 typical cross-modal correlation modes. To advance the
detection of multimodal implicit toxicity, we build ShieldVLM, a model which
identifies implicit toxicity in multimodal statements, prompts and dialogs via
deliberative cross-modal reasoning. Experiments show that ShieldVLM outperforms
existing strong baselines in detecting both implicit and explicit toxicity. The
model and dataset will be publicly available to support future researches.
Warning: This paper contains potentially sensitive contents.

</details>


### [775] [TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/pdf/2505.14329)
*Xiang Li, Xianfu Cheng, Dezhuang Miao, Xiaoming Zhang, Zhoujun Li*

Main category: cs.MM

TL;DR: The paper proposes TF-Mamba, an efficient framework for Multimodal Sentiment Analysis (MSA) with missing modalities, using text-enhanced modules to align, enrich, and reconstruct missing information.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of Transformer-based methods in MSA with missing modalities, particularly their inefficiency in long-range modeling and multimodal fusion.

Method: Introduces Text-aware Modality Enhancement (TME), Text-based Context Mamba (TC-Mamba), and Text-guided Query Mamba (TQ-Mamba) to align, enrich, and query multimodal information.

Result: Demonstrates effectiveness and efficiency on three MSA datasets under missing modality scenarios.

Conclusion: TF-Mamba is a robust and efficient solution for MSA with missing modalities, outperforming existing methods.

Abstract: Multimodal Sentiment Analysis (MSA) with missing modalities has attracted
increasing attention recently. While current Transformer-based methods leverage
dense text information to maintain model robustness, their quadratic complexity
hinders efficient long-range modeling and multimodal fusion. To this end, we
propose a novel and efficient Text-enhanced Fusion Mamba (TF-Mamba) framework
for robust MSA with missing modalities. Specifically, a Text-aware Modality
Enhancement (TME) module aligns and enriches non-text modalities, while
reconstructing the missing text semantics. Moreover, we develop Text-based
Context Mamba (TC-Mamba) to capture intra-modal contextual dependencies under
text collaboration. Finally, Text-guided Query Mamba (TQ-Mamba) queries
text-guided multimodal information and learns joint representations for
sentiment prediction. Extensive experiments on three MSA datasets demonstrate
the effectiveness and efficiency of the proposed method under missing modality
scenarios. Our code is available at https://github.com/codemous/TF-Mamba.

</details>


### [776] [TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs](https://arxiv.org/pdf/2505.11275)
*Pengju Xu, Yan Wang, Shuyuan Zhang, Xuan Zhou, Xin Li, Yue Yuan, Fengzhao Li, Shunyuan Zhou, Xingyu Wang, Yi Zhang, Haiying Zhao*

Main category: cs.MM

TL;DR: TCC-Bench is a bilingual VQA benchmark for evaluating MLLMs' understanding of traditional Chinese culture, revealing their limitations in non-Western contexts.


<details>
  <summary>Details</summary>
Motivation: Address the limited effectiveness of MLLMs in non-Western cultural contexts by creating a culturally rich benchmark.

Method: Develop TCC-Bench using a semi-automated pipeline with GPT-4o for question generation and human curation for quality.

Result: Current MLLMs struggle with culturally grounded visual content, showing gaps in cultural inclusivity.

Conclusion: Further research is needed to develop culturally inclusive and context-aware multimodal systems.

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) have
significantly enhanced the ability of artificial intelligence systems to
understand and generate multimodal content. However, these models often exhibit
limited effectiveness when applied to non-Western cultural contexts, which
raises concerns about their wider applicability. To address this limitation, we
propose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a
bilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark
specifically designed for assessing the understanding of traditional Chinese
culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse
data, incorporating images from museum artifacts, everyday life scenes, comics,
and other culturally significant contexts. We adopt a semi-automated pipeline
that utilizes GPT-4o in text-only mode to generate candidate questions,
followed by human curation to ensure data quality and avoid potential data
leakage. The benchmark also avoids language bias by preventing direct
disclosure of cultural concepts within question texts. Experimental evaluations
across a wide range of MLLMs demonstrate that current models still face
significant challenges when reasoning about culturally grounded visual content.
The results highlight the need for further research in developing culturally
inclusive and context-aware multimodal systems. The code and data can be found
at: https://tcc-bench.github.io/.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [777] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/pdf/2505.13455)
*Von Ralph Dane Marquez Herbuela, Yukie Nagai*

Main category: eess.AS

TL;DR: Non-overlapping speech promotes clearer emotional synchrony, while overlapping speech disrupts it, as shown by analyses of facial and vocal emotion alignment in dyadic interactions.


<details>
  <summary>Details</summary>
Motivation: To understand how conversational dynamics (speech overlap) affect emotional coordination across facial and vocal modalities, improving emotion recognition and human-computer interaction.

Method: Used IEMOCAP dataset; extracted emotion estimates via EmoNet (facial) and Wav2Vec2 (speech). Analyzed segments by speech overlap, using Pearson correlation, lag-adjusted analysis, and Dynamic Time Warping (DTW).

Result: Non-overlapping speech showed stable, predictable synchrony; overlapping speech had higher variability. Facial expressions often preceded speech in turn-taking, while speech led during overlaps.

Conclusion: Conversational structure critically regulates emotional communication, revealing distinct coordination strategies in multimodal affective alignment.

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [778] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/pdf/2505.13541)
*Amirbek Djanibekov, Nurdaulet Mukhituly, Kentaro Inui, Hanan Aldarmaki, Nils Lukas*

Main category: eess.AS

TL;DR: SLMs are highly vulnerable to adversarial attacks, achieving 100% success rates. Proposed post-hoc patching defenses improve robustness to 99% without retraining or utility loss.


<details>
  <summary>Details</summary>
Motivation: Speech signals in SLMs introduce security risks, as adversaries exploit nuances to bypass safety mechanisms.

Method: Analyzed adversarial attacks, proposed post-hoc patching defenses modifying activations during inference.

Result: Defenses improved robustness to 99% with negligible utility impact, validated via large-scale benchmarks.

Conclusion: Post-hoc patching effectively secures SLMs against adversarial attacks without compromising utility.

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [779] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/pdf/2505.13617)
*Christopher Ick, Gordon Wichern, Yoshiki Masuyama, François Germain, Jonathan Le Roux*

Main category: eess.AS

TL;DR: A direction-aware neural field (DANF) is proposed to better capture directional sound field characteristics using Ambisonic-format RIRs, improving on prior NF-based methods.


<details>
  <summary>Details</summary>
Motivation: Prior NF-based methods inadequately capture directional sound field characteristics, limiting their accuracy for real-world applications.

Method: DANF incorporates directional information via Ambisonic-format RIRs and uses a direction-aware loss. It also explores low-rank adaptation for room adaptability.

Result: DANF improves directional sound field representation and shows potential for adapting to new environments.

Conclusion: DANF advances NF-based RIR modeling by explicitly addressing directional sound field characteristics and adaptability.

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [780] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/pdf/2505.13814)
*Jihwan Lee, Kevin Huang, Kleanthis Avramidis, Simon Pistrosch, Monica Gonzalez-Machorro, Yoonjeong Lee, Björn Schuller, Louis Goldstein, Shrikanth Narayanan*

Main category: eess.AS

TL;DR: A model predicts articulatory features from EMG signals using convolutional layers and a Transformer, achieving high correlation (0.9) and decoding them into intelligible speech. It also analyzes EMG electrode placement for optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a novel method for EMG-based speech synthesis by predicting articulatory features from EMG signals and decoding them into speech waveforms.

Method: Integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features.

Result: Achieves ~0.9 prediction correlation for most articulatory features and successfully decodes them into intelligible speech.

Conclusion: First method to decode speech from EMG via articulatory features, offering insights for optimizing EMG electrode configurations.

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [781] [Pushing the Frontiers of Self-Distillation Prototypes Network with Dimension Regularization and Score Normalization](https://arxiv.org/pdf/2505.13826)
*Yafeng Chen, Chong Deng, Hui Wang, Yiheng Jiang, Han Yin, Qian Chen, Wen Wang*

Main category: eess.AS

TL;DR: The paper enhances a self-supervised SV framework (SDPN) with dimension regularization and score normalization, achieving state-of-the-art performance on VoxCeleb1.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between self-supervised and fully supervised SV systems.

Method: Introduces dimension regularization to address collapse in embeddings and integrates score normalization from supervised SV.

Result: Achieves EER of 1.29%, 1.60%, and 2.80% on VoxCeleb1 trials, improving over prior self-supervised methods by 28.3%, 19.6%, and 22.6%.

Conclusion: The enhancements advance SV technology, setting a new benchmark for self-supervised approaches.

Abstract: Developing robust speaker verification (SV) systems without speaker labels
has been a longstanding challenge. Earlier research has highlighted a
considerable performance gap between self-supervised and fully supervised
approaches. In this paper, we enhance the non-contrastive self-supervised
framework, Self-Distillation Prototypes Network (SDPN), by introducing
dimension regularization that explicitly addresses the collapse problem through
the application of regularization terms to speaker embeddings. Moreover, we
integrate score normalization techniques from fully supervised SV to further
bridge the gap toward supervised verification performance. SDPN with dimension
regularization and score normalization sets a new state-of-the-art on the
VoxCeleb1 speaker verification evaluation benchmark, achieving Equal Error Rate
1.29%, 1.60%, and 2.80% for trial VoxCeleb1-{O,E,H} respectively. These results
demonstrate relative improvements of 28.3%, 19.6%, and 22.6% over the current
best self-supervised methods, thereby advancing the frontiers of SV technology.

</details>


### [782] [Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising](https://arxiv.org/pdf/2505.13830)
*Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling*

Main category: eess.AS

TL;DR: A neural codec-based speech denoiser is integrated with LauraTTS to improve noise-robust zero-shot TTS, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based zero-shot TTS degrades with noisy audio prompts, necessitating a solution to preserve speech quality.

Method: Proposes a codec denoiser with an audio codec, token denoiser, and embedding refiner to clean noisy prompts for LauraTTS.

Result: The denoiser outperforms state-of-the-art SE methods, and LauraTTS with it surpasses models using additional SE.

Conclusion: The integration of the codec denoiser with LauraTTS effectively enhances noise-robust zero-shot TTS performance.

Abstract: Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend
to preserve the acoustic environment of the audio prompt, leading to
degradation in synthesized speech quality when the audio prompt contains noise.
In this paper, we propose a novel neural codec-based speech denoiser and
integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve
noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio
codec, a token denoiser, and an embedding refiner. The token denoiser predicts
the first two groups of clean acoustic tokens from the noisy ones, which can
serve as the acoustic prompt for LauraTTS to synthesize high-quality
personalized speech or be converted to clean speech waveforms through the
embedding refiner and codec decoder. Experimental results show that our
proposed codec denoiser outperforms state-of-the-art speech enhancement (SE)
methods, and the proposed noise-robust LauraTTS surpasses the approach using
additional SE models.

</details>


### [783] [A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model](https://arxiv.org/pdf/2505.13843)
*Yang Xiang, Canan Huang, Desheng Hu, Jingguang Tian, Xinhui Hu, Chao Zhang*

Main category: eess.AS

TL;DR: A novel speech enhancement method using factorized codec and diffusion model improves clean speech recovery by leveraging semantic and acoustic attributes, outperforming SOTA baselines and benefiting downstream TTS tasks.


<details>
  <summary>Details</summary>
Motivation: Current SE methods often ignore distinct speech attributes (semantic content, acoustic details), limiting performance in complex environments and downstream tasks.

Method: Proposes a step-by-step factorized SE method using factorized codec and diffusion model, hierarchically modeling semantic and acoustic attributes.

Result: Outperforms SOTA baselines in speech quality and enhances TTS performance in noisy environments.

Conclusion: The method effectively addresses limitations of traditional SE approaches, offering robust performance in challenging scenarios and downstream applications.

Abstract: Most current speech enhancement (SE) methods recover clean speech from noisy
inputs by directly estimating time-frequency masks or spectrums. However, these
approaches often neglect the distinct attributes, such as semantic content and
acoustic details, inherent in speech signals, which can hinder performance in
downstream tasks. Moreover, their effectiveness tends to degrade in complex
acoustic environments. To overcome these challenges, we propose a novel,
semantic information-based, step-by-step factorized SE method using factorized
codec and diffusion model. Unlike traditional SE methods, our hierarchical
modeling of semantic and acoustic attributes enables more robust clean speech
recovery, particularly in challenging acoustic scenarios. Moreover, this method
offers further advantages for downstream TTS tasks. Experimental results
demonstrate that our algorithm not only outperforms SOTA baselines in terms of
speech quality but also enhances TTS performance in noisy environments.

</details>


### [784] [Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach](https://arxiv.org/pdf/2505.14336)
*Umberto Cappellazzo, Minsu Kim, Stavros Petridis, Daniele Falavigna, Alessio Brutti*

Main category: eess.AS

TL;DR: Llama-SMoP is an efficient Multimodal LLM for AVSR, using Sparse Mixture of Projectors (SMoP) to maintain performance with smaller LLMs, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: High computational costs of integrating LLMs into AVSR hinder deployment in resource-constrained settings.

Method: Proposes Llama-SMoP with SMoP module, using sparsely-gated MoE projectors to scale capacity without increasing inference costs. Evaluates three SMoP configurations.

Result: Llama-SMoP DEDR (Disjoint-Experts, Disjoint-Routers) achieves superior performance on ASR, VSR, and AVSR tasks, with confirmed effectiveness in expert activation, scalability, and noise robustness.

Conclusion: Llama-SMoP offers an efficient solution for AVSR by balancing performance and computational cost.

Abstract: Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.

</details>


### [785] [U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding](https://arxiv.org/pdf/2505.13880)
*Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie*

Main category: eess.AS

TL;DR: U-SAM is an advanced audio language model integrating specialized encoders for speech, audio, and music with a pre-trained LLM, using MoE for task-aware fusion and contrastive loss for better alignment. It outperforms existing models and shows generalization on unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with comprehensive understanding across diverse audio types and rely on cross-entropy loss, which inadequately handles redundant features and alignment.

Method: U-SAM combines specialized encoders with an LLM, uses a Mixture of Experts (MoE) projector for dynamic feature fusion, and employs a Semantic-Aware Contrastive Loss Module to refine alignment.

Result: U-SAM outperforms specialized and existing audio language models across benchmarks and demonstrates emergent capabilities on unseen tasks.

Conclusion: U-SAM advances unified audio understanding with its innovative architecture and loss module, showing strong performance and generalization.

Abstract: The text generation paradigm for audio tasks has opened new possibilities for
unified audio understanding. However, existing models face significant
challenges in achieving a comprehensive understanding across diverse audio
types, such as speech, general audio events, and music. Furthermore, their
exclusive reliance on cross-entropy loss for alignment often falls short, as it
treats all tokens equally and fails to account for redundant audio features,
leading to weaker cross-modal alignment. To deal with the above challenges,
this paper introduces U-SAM, an advanced audio language model that integrates
specialized encoders for speech, audio, and music with a pre-trained large
language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for
task-aware feature fusion, dynamically routing and integrating the
domain-specific encoder outputs. Additionally, U-SAM incorporates a
Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant
audio features under language supervision and rectifies their semantic and
spectral representations to enhance cross-modal alignment. Extensive
experiments demonstrate that U-SAM consistently outperforms both specialized
models and existing audio language models across multiple benchmarks. Moreover,
it exhibits emergent capabilities on unseen tasks, showcasing its
generalization potential. Code is available
(https://github.com/Honee-W/U-SAM/).

</details>


### [786] [Naturalness-Aware Curriculum Learning with Dynamic Temperature for Speech Deepfake Detection](https://arxiv.org/pdf/2505.13976)
*Taewoo Kim, Guisik Kim, Choongsang Cho, Young Han Lee*

Main category: eess.AS

TL;DR: The paper introduces naturalness-aware curriculum learning for speech deepfake detection, improving robustness and generalization by leveraging speech naturalness and dynamic temperature scaling, achieving a 23% EER reduction.


<details>
  <summary>Details</summary>
Motivation: Current speech deepfake detection models often ignore speech naturalness, a key cue for distinguishing real from spoofed speech. This study aims to address this gap.

Method: Proposes naturalness-aware curriculum learning, using ground-truth labels and mean opinion scores to measure sample difficulty and adjust training. Includes dynamic temperature scaling for better generalization.

Result: Achieved a 23% relative reduction in EER on the ASVspoof 2021 DF dataset without changing the model architecture.

Conclusion: Naturalness-aware training strategies are effective for speech deepfake detection, enhancing model robustness and generalization.

Abstract: Recent advances in speech deepfake detection (SDD) have significantly
improved artifacts-based detection in spoofed speech. However, most models
overlook speech naturalness, a crucial cue for distinguishing bona fide speech
from spoofed speech. This study proposes naturalness-aware curriculum learning,
a novel training framework that leverages speech naturalness to enhance the
robustness and generalization of SDD. This approach measures sample difficulty
using both ground-truth labels and mean opinion scores, and adjusts the
training schedule to progressively introduce more challenging samples. To
further improve generalization, a dynamic temperature scaling method based on
speech naturalness is incorporated into the training process. A 23% relative
reduction in the EER was achieved in the experiments on the ASVspoof 2021 DF
dataset, without modifying the model architecture. Ablation studies confirmed
the effectiveness of naturalness-aware training strategies for SDD tasks.

</details>


### [787] [SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with in-Context Enhancement](https://arxiv.org/pdf/2505.14066)
*Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding*

Main category: eess.AS

TL;DR: SeamlessEdit is a noise-resilient speech editing framework for noisy scenarios, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing speech editing studies focus on clean speech, ignoring noisy environments, which degrade quality.

Method: Uses a frequency-band-aware noise suppression module and in-content refinement strategy.

Result: Outperforms state-of-the-art methods in quantitative and qualitative evaluations.

Conclusion: SeamlessEdit effectively addresses noisy speech editing, improving robustness in real-world applications.

Abstract: With the fast development of zero-shot text-to-speech technologies, it is
possible to generate high-quality speech signals that are indistinguishable
from the real ones. Speech editing, including speech insertion and replacement,
appeals to researchers due to its potential applications. However, existing
studies only considered clean speech scenarios. In real-world applications, the
existence of environmental noise could significantly degrade the quality of the
generation. In this study, we propose a noise-resilient speech editing
framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a
frequency-band-aware noise suppression module and an in-content refinement
strategy. It can well address the scenario where the frequency bands of voice
and background noise are not separated. The proposed SeamlessEdit framework
outperforms state-of-the-art approaches in multiple quantitative and
qualitative evaluations.

</details>


### [788] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/pdf/2505.14410)
*Jinzuomu Zhong, Suyuan Liu, Dan Wells, Korin Richmond*

Main category: eess.AS

TL;DR: The paper improves evaluation methods for accent similarity in speech synthesis, combining subjective (refined XAB test) and objective (pronunciation metrics) approaches, while highlighting limitations of common metrics like Word Error Rate.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of evaluating accent similarity in speech synthesis and enhance both subjective and objective evaluation methods.

Method: Refines the XAB listening test with transcriptions and accent difference highlighting, and introduces pronunciation-related metrics (vowel formants, phonetic posteriorgrams).

Result: Shows that the proposed metrics (alongside accent/speaker similarity and Mel Cepstral Distortion) are effective, while common metrics like Word Error Rate have limitations for underrepresented accents.

Conclusion: The refined evaluation methods improve accuracy and reliability in assessing accent similarity, with implications for better speech synthesis systems.

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [789] [Single-Channel Target Speech Extraction Utilizing Distance and Room Clues](https://arxiv.org/pdf/2505.14433)
*Runwu Shi, Zirui Lin, Benjamin Yen, Jiang Wang, Ragib Amin Nihal, Kazuhiro Nakadai*

Main category: eess.AS

TL;DR: The paper proposes a single-channel target speech extraction (TSE) method using distance clues and room information to improve generalization across different rooms.


<details>
  <summary>Details</summary>
Motivation: Distance clues alone are insufficient for TSE due to room acoustic variations, so incorporating room environmental information (dimensions, reverberation time) is suggested.

Method: A distance and environment-based TSE model in the time-frequency domain with learnable embeddings for distance and room.

Result: Feasibility demonstrated on simulated and real datasets.

Conclusion: Incorporating room information enhances TSE generalization, validated by experimental results.

Abstract: This paper aims to achieve single-channel target speech extraction (TSE) in
enclosures utilizing distance clues and room information. Recent works have
verified the feasibility of distance clues for the TSE task, which can imply
the sound source's direct-to-reverberation ratio (DRR) and thus can be utilized
for speech separation and TSE systems. However, such distance clue is
significantly influenced by the room's acoustic characteristics, such as
dimension and reverberation time, making it challenging for TSE systems that
rely solely on distance clues to generalize across a variety of different
rooms. To solve this, we suggest providing room environmental information (room
dimensions and reverberation time) for distance-based TSE for better
generalization capabilities. Especially, we propose a distance and
environment-based TSE model in the time-frequency (TF) domain with learnable
distance and room embedding. Results on both simulated and real collected
datasets demonstrate its feasibility. Demonstration materials are available at
https://runwushi.github.io/distance-room-demo-page/.

</details>


### [790] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/pdf/2505.14449)
*Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces an Implicit Demography Inference (IDI) module to address fairness in Speech Emotion Recognition (SER) without explicit demographic labels, improving fairness metrics significantly with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Fairness in categorical SER is underexplored, and existing methods rely on hard-to-obtain demographic labels due to privacy concerns.

Method: The IDI module uses pseudo-labeling from a pre-trained model and unsupervised k-means clustering to mitigate bias.

Result: Pseudo-labeling IDI improves fairness metrics by over 33% with <3% accuracy drop; unsupervised IDI improves fairness by >26% with <4% accuracy drop.

Conclusion: The unsupervised IDI effectively mitigates race and age disparities, proving useful where explicit demographics are unavailable.

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [791] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/pdf/2505.14465)
*Aviv Navon, Aviv Shamsian, Yael Segal-Feldman, Neta Glazer, Gil Hetz, Joseph Keshet*

Main category: eess.AS

TL;DR: FlowTSE is a simple yet effective target speaker extraction method using conditional flow matching, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Generative methods for TSE are underexplored and often rely on complex pipelines, prompting the need for a simpler, efficient solution.

Method: FlowTSE uses conditional flow matching with mel-spectrograms and introduces a novel vocoder for phase reconstruction.

Result: FlowTSE matches or outperforms strong baselines on standard TSE benchmarks.

Conclusion: FlowTSE offers a streamlined, effective approach to TSE with improved performance.

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [792] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/pdf/2505.14517)
*Jakob Kienegger, Timo Gerkmann*

Main category: eess.AS

TL;DR: Proposes a weakly guided speaker extraction method for dynamic scenarios, outperforming strongly guided methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in spatially dynamic scenarios where strong directional cues are impractical.

Method: Uses deep tracking and joint training on synthetic data for weakly guided extraction.

Result: Resolves spatial ambiguities and outperforms mismatched strongly guided methods.

Conclusion: Weak guidance based on initial position is effective for dynamic speaker extraction.

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [793] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/pdf/2505.14518)
*Chun-Yi Kuan, Hung-yi Lee*

Main category: eess.AS

TL;DR: LISTEN is a contrastive-like training method for audio-aware LLMs to reduce hallucinations without modifying LLM parameters, using synthesized data and a lightweight adapter.


<details>
  <summary>Details</summary>
Motivation: Address hallucinations in audio-aware LLMs to improve reliability in real-world applications.

Method: Propose LISTEN, a contrastive-like training method using synthesized data and a lightweight adapter for audio integration.

Result: LISTEN reduces hallucinations while maintaining performance on benchmarks and is more efficient in data and computation.

Conclusion: LISTEN effectively enhances LLMs' audio reliability without parameter modifications, offering efficiency and performance.

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


### [794] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/pdf/2505.14561)
*Theo Lepage, Reda Dehak*

Main category: eess.AS

TL;DR: Self-Supervised Positive Sampling (SSPS) improves Speaker Verification by selecting positives of the same speaker but different recording conditions, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Standard SSL frameworks in Speaker Verification encode channel information due to same-utterance positive sampling, limiting performance.

Method: SSPS uses clustering and a memory queue to find positives with the same speaker identity but different recording conditions.

Result: SSPS achieves 2.57% and 2.53% EER for SimCLR and DINO, respectively, with a 58% EER reduction for SimCLR.

Conclusion: SSPS effectively reduces intra-speaker variance and enhances SV performance, matching DINO's results.

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


### [795] [AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation](https://arxiv.org/pdf/2505.14600)
*Yang Xiao, Tianyi Peng, Yanghao Zhou, Rohan Kumar Das*

Main category: eess.AS

TL;DR: AdaKWS is a test-time adaptation method for robust spoken keyword spotting, improving performance in unseen or noisy environments by optimizing confidence and introducing pseudo-keyword consistency.


<details>
  <summary>Details</summary>
Motivation: Current small-footprint KWS systems struggle with unseen environments or noisy backgrounds, necessitating a method like AdaKWS for robust adaptation.

Method: AdaKWS uses prediction entropy minimization for reliable sample selection and adjusts normalization statistics. It also introduces pseudo-keyword consistency to avoid noise overfitting.

Result: AdaKWS outperforms other methods in various noisy conditions, including Gaussian and real-scenario noises.

Conclusion: AdaKWS is the first TTA method for robust KWS, demonstrating superior performance in challenging environments.

Abstract: Spoken keyword spotting (KWS) aims to identify keywords in audio for wide
applications, especially on edge devices. Current small-footprint KWS systems
focus on efficient model designs. However, their inference performance can
decline in unseen environments or noisy backgrounds. Test-time adaptation (TTA)
helps models adapt to test samples without needing the original training data.
In this study, we present AdaKWS, the first TTA method for robust KWS to the
best of our knowledge. Specifically, 1) We initially optimize the model's
confidence by selecting reliable samples based on prediction entropy
minimization and adjusting the normalization statistics in each batch. 2) We
introduce pseudo-keyword consistency (PKC) to identify critical, reliable
features without overfitting to noise. Our experiments show that AdaKWS
outperforms other methods across various conditions, including Gaussian noise
and real-scenario noises. The code will be released in due course.

</details>


### [796] [Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing](https://arxiv.org/pdf/2505.14601)
*Yang Xiao, Rohan Kumar Das*

Main category: eess.AS

TL;DR: The paper proposes AnaST, an analytic class incremental learning method for deepfake speech source tracing, addressing catastrophic forgetting and scalability as new attacks emerge.


<details>
  <summary>Details</summary>
Motivation: Deepfake speech is increasingly common and hard to detect, necessitating methods to trace its source while adapting to new attacks without forgetting previous ones.

Method: AnaST fixes the feature extractor and updates the classifier with a closed-form analytical solution in one epoch, ensuring data privacy and memory efficiency.

Result: Experiments show AnaST outperforms baseline methods in handling new attacks while retaining knowledge of previous ones.

Conclusion: AnaST is effective for scalable and privacy-preserving deepfake source tracing, suitable for online training.

Abstract: As deepfake speech becomes common and hard to detect, it is vital to trace
its source. Recent work on audio deepfake source tracing (ST) aims to find the
origins of synthetic or manipulated speech. However, ST models must adapt to
learn new deepfake attacks while retaining knowledge of the previous ones. A
major challenge is catastrophic forgetting, where models lose the ability to
recognize previously learned attacks. Some continual learning methods help with
deepfake detection, but multi-class tasks such as ST introduce additional
challenges as the number of classes grows. To address this, we propose an
analytic class incremental learning method called AnaST. When new attacks
appear, the feature extractor remains fixed, and the classifier is updated with
a closed-form analytical solution in one epoch. This approach ensures data
privacy, optimizes memory usage, and is suitable for online training. The
experiments carried out in this work show that our method outperforms the
baselines.

</details>


### [797] [USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction](https://arxiv.org/pdf/2409.02615)
*Bang Zeng, Ming Li*

Main category: eess.AS

TL;DR: USEF-TSE introduces a speaker embedding-free framework for target speaker extraction using multi-head cross-attention, achieving SOTA performance on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on speaker embeddings from recognition models, which may not be optimal. USEF-TSE aims to eliminate this dependency.

Method: Uses a multi-head cross-attention mechanism for frame-level target speaker feature extraction, integrating with existing speech separation models.

Result: Achieves SOTA performance on WSJ0-2mix, WHAM!, WHAMR!, LibriMix, and ICASSP 2023 DNS Challenge datasets.

Conclusion: USEF-TSE effectively bypasses speaker recognition models, leveraging enrollment speech better and performing well on diverse data.

Abstract: Target speaker extraction aims to separate the voice of a specific speaker
from mixed speech. Traditionally, this process has relied on extracting a
speaker embedding from a reference speech, in which a speaker recognition model
is required. However, identifying an appropriate speaker recognition model can
be challenging, and using the target speaker embedding as reference information
may not be optimal for target speaker extraction tasks. This paper introduces a
Universal Speaker Embedding-Free Target Speaker Extraction (USEF-TSE) framework
that operates without relying on speaker embeddings. USEF-TSE utilizes a
multi-head cross-attention mechanism as a frame-level target speaker feature
extractor. This innovative approach allows mainstream speaker extraction
solutions to bypass the dependency on speaker recognition models and better
leverage the information available in the enrollment speech, including speaker
characteristics and contextual details. Additionally, USEF-TSE can seamlessly
integrate with other time-domain or time-frequency domain speech separation
models to achieve effective speaker extraction. Experimental results show that
our proposed method achieves state-of-the-art (SOTA) performance in terms of
Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) on the WSJ0-2mix, WHAM!,
and WHAMR! datasets, which are standard benchmarks for monaural anechoic, noisy
and noisy-reverberant two-speaker speech separation and speaker extraction. The
results on the LibriMix and the blind test set of the ICASSP 2023 DNS Challenge
demonstrate that the model performs well on more diverse and out-of-domain
data. For access to the source code, please visit:
https://github.com/ZBang/USEF-TSE.

</details>


### [798] [TF-Mamba: A Time-Frequency Network for Sound Source Localization](https://arxiv.org/pdf/2409.05034)
*Yang Xiao, Rohan Kumar Das*

Main category: eess.AS

TL;DR: TF-Mamba, a novel SSL system using Mamba for spatial feature fusion, outperforms advanced methods.


<details>
  <summary>Details</summary>
Motivation: Improving SSL in challenging acoustic environments by leveraging Mamba's sequence-based performance.

Method: Develops TF-Mamba, integrating time and frequency features with Bidirectional Mamba for processing.

Result: TF-Mamba significantly outperforms other methods on simulated and real datasets.

Conclusion: TF-Mamba is effective for SSL, with code to be released publicly.

Abstract: Sound source localization (SSL) determines the position of sound sources
using multi-channel audio data. It is commonly used to improve speech
enhancement and separation. Extracting spatial features is crucial for SSL,
especially in challenging acoustic environments. Recently, a novel structure
referred to as Mamba demonstrated notable performance across various
sequence-based modalities. This study introduces the Mamba for SSL tasks. We
consider the Mamba-based model to analyze spatial features from speech signals
by fusing both time and frequency features, and we develop an SSL system called
TF-Mamba. This system integrates time and frequency fusion, with Bidirectional
Mamba managing both time-wise and frequency-wise processing. We conduct the
experiments on the simulated and real datasets. Experiments show that TF-Mamba
significantly outperforms other advanced methods. The code will be publicly
released in due course.

</details>


### [799] [Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech](https://arxiv.org/pdf/2410.01162)
*Wonjune Kang, Junteng Jia, Chunyang Wu, Wei Zhou, Egor Lakomkin, Yashesh Gaur, Leda Sari, Suyoun Kim, Ke Li, Jay Mahadeokar, Ozlem Kalinli*

Main category: eess.AS

TL;DR: A study explores how a frozen large language model (LLM) can understand paralinguistic speech aspects without fine-tuning, using a speech encoder to align responses with text prompts conditioned on speaking style.


<details>
  <summary>Details</summary>
Motivation: To enable LLMs to understand paralinguistic speech features (e.g., emotion, tone) without modifying their weights, enhancing interaction quality.

Method: An end-to-end system with a speech encoder generates token embeddings aligning LLM responses for expressive speech and matching text prompts.

Result: The system produces higher quality and more empathetic responses to expressive speech than baselines.

Conclusion: The framework successfully conveys paralinguistic information to a frozen LLM, improving its understanding of speech beyond just linguistic content.

Abstract: This work studies the capabilities of a large language model (LLM) to
understand paralinguistic aspects of speech without fine-tuning its weights. We
utilize an end-to-end system with a speech encoder, which is trained to produce
token embeddings such that the LLM's response to an expressive speech prompt is
aligned with its response to a semantically matching text prompt that has also
been conditioned on the user's speaking style. This framework enables the
encoder to generate tokens that capture both linguistic and paralinguistic
information and effectively convey them to the LLM, even when the LLM's weights
remain completely frozen. To the best of our knowledge, our work is the first
to explore how to induce a frozen LLM to understand more than just linguistic
content from speech inputs in a general interaction setting. Experiments
demonstrate that our system is able to produce higher quality and more
empathetic responses to expressive speech prompts compared to several
baselines.

</details>


### [800] [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://arxiv.org/pdf/2410.06885)
*Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen*

Main category: eess.AS

TL;DR: F5-TTS is a non-autoregressive TTS system using flow matching and Diffusion Transformer, improving efficiency and robustness over E2 TTS with ConvNeXt and Sway Sampling.


<details>
  <summary>Details</summary>
Motivation: Address slow convergence and low robustness in E2 TTS by simplifying design and enhancing alignment.

Method: Uses flow matching with Diffusion Transformer, refines text with ConvNeXt, and employs Sway Sampling for efficiency.

Result: Achieves faster training, inference RTF of 0.15, and excels in naturalness, zero-shot ability, and code-switching.

Conclusion: F5-TTS advances diffusion-based TTS with efficiency and performance, releasing code for community use.

Abstract: This paper introduces F5-TTS, a fully non-autoregressive text-to-speech
system based on flow matching with Diffusion Transformer (DiT). Without
requiring complex designs such as duration model, text encoder, and phoneme
alignment, the text input is simply padded with filler tokens to the same
length as input speech, and then the denoising is performed for speech
generation, which was originally proved feasible by E2 TTS. However, the
original design of E2 TTS makes it hard to follow due to its slow convergence
and low robustness. To address these issues, we first model the input with
ConvNeXt to refine the text representation, making it easy to align with the
speech. We further propose an inference-time Sway Sampling strategy, which
significantly improves our model's performance and efficiency. This sampling
strategy for flow step can be easily applied to existing flow matching based
models without retraining. Our design allows faster training and achieves an
inference RTF of 0.15, which is greatly improved compared to state-of-the-art
diffusion-based TTS models. Trained on a public 100K hours multilingual
dataset, our F5-TTS exhibits highly natural and expressive zero-shot ability,
seamless code-switching capability, and speed control efficiency. We have
released all codes and checkpoints to promote community development, at
https://SWivid.github.io/F5-TTS/.

</details>


### [801] [The 1st SpeechWellness Challenge: Detecting Suicide Risk Among Adolescents](https://arxiv.org/pdf/2501.06474)
*Wen Wu, Ziyun Cui, Chang Lei, Yinan Duan, Diyang Qu, Ji Wu, Bowen Zhou, Runsen Chen, Chao Zhang*

Main category: eess.AS

TL;DR: The SW1 challenge aims to detect adolescent suicide risk using speech analysis, offering a non-invasive alternative to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Suicide in adolescents is a global public health issue; early detection can save lives, but current methods like self-reporting are limited.

Method: The challenge uses speech recordings from 600 adolescents (10-18 years) to identify markers of suicide risk through natural speech tasks.

Result: The SW1 dataset is released to facilitate research on speech-based suicide risk detection.

Conclusion: Speech analysis could provide an accessible tool for early suicide risk detection in adolescents.

Abstract: The 1st SpeechWellness Challenge (SW1) aims to advance methods for detecting
current suicide risk in adolescents using speech analysis techniques. Suicide
among adolescents is a critical public health issue globally. Early detection
of suicidal tendencies can lead to timely intervention and potentially save
lives. Traditional methods of assessment often rely on self-reporting or
clinical interviews, which may not always be accessible. The SW1 challenge
addresses this gap by exploring speech as a non-invasive and readily available
indicator of mental health. We release the SW1 dataset which contains speech
recordings from 600 adolescents aged 10-18 years. By focusing on speech
generated from natural tasks, the challenge seeks to uncover patterns and
markers that correlate with current suicide risk.

</details>


### [802] [Self-Supervised Frameworks for Speaker Verification via Bootstrapped Positive Sampling](https://arxiv.org/pdf/2501.17772)
*Theo Lepage, Reda Dehak*

Main category: eess.AS

TL;DR: SSPS improves SSL for Speaker Verification by sampling diverse positives, reducing channel bias, and enhancing performance on benchmarks like VoxCeleb.


<details>
  <summary>Details</summary>
Motivation: Closing the performance gap between SSL and supervised systems in Speaker Verification by addressing limitations in positive sampling strategies.

Method: Introduces Self-Supervised Positive Sampling (SSPS), a bootstrapped technique to sample diverse positives from the representation space, assuming they share speaker identity but differ in recording conditions.

Result: SSPS improves SV performance on VoxCeleb benchmarks, with SimCLR and DINO achieving ~2.5% EER. SimCLR shows a 58% relative EER reduction, matching DINO's performance.

Conclusion: SSPS effectively reduces intra-class variance and channel bias, enhancing robustness and performance in SSL frameworks for Speaker Verification.

Abstract: Recent developments in Self-Supervised Learning (SSL) have demonstrated
significant potential for Speaker Verification (SV), but closing the
performance gap with supervised systems remains an ongoing challenge. Standard
SSL frameworks rely on anchor-positive pairs extracted from the same audio
utterances. Hence, positives have channel characteristics similar to those of
their corresponding anchors, even with extensive data-augmentation. Therefore,
this positive sampling strategy is a fundamental limitation as it encodes too
much information regarding the recording source in the learned representations.
This article introduces Self-Supervised Positive Sampling (SSPS), a
bootstrapped technique for sampling appropriate and diverse positives in SSL
frameworks for SV. SSPS samples positives close to their anchor in the
representation space, under the assumption that these pseudo-positives belong
to the same speaker identity but correspond to different recording conditions.
This method demonstrates consistent improvements in SV performance on VoxCeleb
benchmarks when implemented in major SSL frameworks, such as SimCLR, SwAV,
VICReg, and DINO. Using SSPS, SimCLR, and DINO achieve 2.57% and 2.53% EER on
VoxCeleb1-O. SimCLR yields a 58% relative reduction in EER, getting comparable
performance to DINO with a simpler training framework. Furthermore, SSPS lowers
intra-class variance and reduces channel information in speaker representations
while exhibiting greater robustness without data-augmentation.

</details>


### [803] [Universal Semantic Disentangled Privacy-preserving Speech Representation Learning](https://arxiv.org/pdf/2505.13085)
*Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Rädel, Grant Strimel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood*

Main category: eess.AS

TL;DR: The paper proposes the Universal Speech Codec (USC) to address privacy concerns in training LLMs with human speech, by disentangling speech into privacy-preserving semantic representations and residual speaker details, achieving high-fidelity reconstruction and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise when using human speech recordings to train LLMs, as models may generate outputs resembling training data artifacts, risking speaker identification.

Method: USC, an encoder-decoder model, disentangles speech into (i) privacy-preserving semantic representations (content, paralinguistics) and (ii) residual acoustic/speaker details for reconstruction.

Result: USC preserves content, prosody, and sentiment while removing identifiable speaker attributes, achieving top-tier speech reconstruction. It also introduces a privacy evaluation methodology.

Conclusion: USC effectively balances speaker anonymization, paralinguistics retention, and content preservation, offering a robust solution for privacy-preserving speech representation learning.

Abstract: The use of audio recordings of human speech to train LLMs poses privacy
concerns due to these models' potential to generate outputs that closely
resemble artifacts in the training data. In this study, we propose a speaker
privacy-preserving representation learning method through the Universal Speech
Codec (USC), a computationally efficient encoder-decoder model that
disentangles speech into: (i) privacy-preserving semantically rich
representations, capturing content and speech paralinguistics, and (ii)
residual acoustic and speaker representations that enables high-fidelity
reconstruction. Extensive evaluations presented show that USC's semantic
representation preserves content, prosody, and sentiment, while removing
potentially identifiable speaker attributes. Combining both representations,
USC achieves state-of-the-art speech reconstruction. Additionally, we introduce
an evaluation methodology for measuring privacy-preserving properties, aligning
with perceptual tests. We compare USC against other codecs in the literature
and demonstrate its effectiveness on privacy-preserving representation
learning, illustrating the trade-offs of speaker anonymization, paralinguistics
retention and content preservation in the learned semantic representations.
Audio samples are shared in https://www.amazon.science/usc-samples.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [804] [GANCompress: GAN-Enhanced Neural Image Compression with Binary Spherical Quantization](https://arxiv.org/pdf/2505.13542)
*Karthik Sivakoti*

Main category: eess.IV

TL;DR: GANCompress combines Binary Spherical Quantization (BSQ) and GANs for efficient neural compression, achieving high compression ratios with minimal distortion and faster speeds.


<details>
  <summary>Details</summary>
Motivation: The need for efficient compression techniques balancing rate-distortion performance and computational feasibility due to exponential visual data growth.

Method: Uses a transformer-based autoencoder with BSQ for efficient discretization and a GAN with frequency-domain attention and color consistency optimization.

Result: Reduces file sizes by up to 100x, outperforms H.264 by 12-15% in perceptual metrics, and achieves 43% FID improvement with faster speeds.

Conclusion: GANCompress advances neural compression, offering practical benefits for real-time visual communication.

Abstract: The exponential growth of visual data in digital communications has
intensified the need for efficient compression techniques that balance
rate-distortion performance with computational feasibility. While recent neural
compression approaches have shown promise, they still struggle with fundamental
challenges: preserving perceptual quality at high compression ratios,
computational efficiency, and adaptability to diverse visual content. This
paper introduces GANCompress, a novel neural compression framework that
synergistically combines Binary Spherical Quantization (BSQ) with Generative
Adversarial Networks (GANs) to address these challenges. Our approach employs a
transformer-based autoencoder with an enhanced BSQ bottleneck that projects
latent representations onto a hypersphere, enabling efficient discretization
with bounded quantization error. This is followed by a specialized GAN
architecture incorporating frequency-domain attention and color consistency
optimization. Experimental results demonstrate that GANCompress achieves
substantial improvement in compression efficiency -- reducing file sizes by up
to 100x with minimal visual distortion. Our method outperforms traditional
codecs like H.264 by 12-15% in perceptual metrics while maintaining comparable
PSNR/SSIM values, with 2.4x faster encoding and decoding speeds. On standard
benchmarks including ImageNet-1k and COCO2017, GANCompress sets a new
state-of-the-art, reducing FID from 0.72 to 0.41 (43% improvement) compared to
previous methods while maintaining higher throughput. This work presents a
significant advancement in neural compression technology with promising
applications for real-time visual communication systems.

</details>


### [805] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/pdf/2505.13579)
*Yipeng Sun, Linda-Sophie Schneider, Chengze Ye, Mingxuan Gu, Siyuan Mei, Siming Bayer, Andreas Maier*

Main category: eess.IV

TL;DR: An enhanced FDK-based neural network integrates trainable elements into CBCT reconstruction, using wavelet transformations to reduce parameters by 93.75% while maintaining interpretability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: FDK is efficient but noisy; deep learning improves quality but lacks interpretability and increases complexity.

Method: Selective integration of trainable elements into FDK stages, leveraging wavelet transformations for sparsity.

Result: Reduced parameters by 93.75%, maintained computational cost, improved robustness to noise, and ensured volumetric consistency.

Conclusion: A practical enhancement for clinical CBCT, balancing performance and interpretability, suitable for resource-limited settings.

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [806] [Exploring Image Quality Assessment from a New Perspective: Pupil Size](https://arxiv.org/pdf/2505.13841)
*Yixuan Gao, Xiongkuo Min, Guangtao Zhai*

Main category: eess.IV

TL;DR: The paper investigates how image quality assessment (IQA) tasks influence cognitive processes via pupil size, revealing a link between pupil size changes and image quality.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive impact of IQA tasks and explore the relationship between pupil size and image quality perception.

Method: Conducted subjective experiments with free observation and IQA tasks, analyzing pupil size differences between them.

Result: Found activation of visual attention during IQA tasks and a correlation between pupil size changes and image quality.

Conclusion: The study offers insights for objective IQA methods and proposes a new subjective IQA approach for authentic quality assessment.

Abstract: This paper explores how the image quality assessment (IQA) task affects the
cognitive processes of people from the perspective of pupil size and studies
the relationship between pupil size and image quality. Specifically, we first
invited subjects to participate in a subjective experiment, which includes two
tasks: free observation and IQA. In the free observation task, subjects did not
need to perform any action, and they only needed to observe images as they
usually do with an album. In the IQA task, subjects were required to score
images according to their overall impression of image quality. Then, by
analyzing the difference in pupil size between the two tasks, we find that
people may activate the visual attention mechanism when evaluating image
quality. Meanwhile, we also find that the change in pupil size is closely
related to image quality in the IQA task. For future research on IQA, this
research can not only provide a theoretical basis for the objective IQA method
and promote the development of more effective objective IQA methods, but also
provide a new subjective IQA method for collecting the authentic subjective
impression of image quality.

</details>


### [807] [Automated Quality Evaluation of Cervical Cytopathology Whole Slide Images Based on Content Analysis](https://arxiv.org/pdf/2505.13875)
*Lanlan Kang, Jian Wang, Jian QIn, Yiqin Liang, Yongjun He*

Main category: eess.IV

TL;DR: The paper proposes an AI-based automated quality assessment method for cervical cytopathology WSIs, outperforming manual methods in speed and consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional manual evaluation of cervical cancer screening samples is subjective, costly, and unreliable, necessitating an automated solution.

Method: The method uses AI algorithms (object detection, classification, segmentation) and XGBoost to analyze WSIs based on TBS standards, quantifying metrics like staining quality and cell counts.

Result: Tests on 100 WSIs show the method is faster and more consistent than manual evaluation.

Conclusion: The automated system offers a reliable, efficient alternative to traditional cervical cancer screening quality assessment.

Abstract: The ThinPrep Cytologic Test (TCT) is the most widely used method for cervical
cancer screening, and the sample quality directly impacts the accuracy of the
diagnosis. Traditional manual evaluation methods rely on the observation of
pathologist under microscopes. These methods exhibit high subjectivity, high
cost, long duration, and low reliability. With the development of
computer-aided diagnosis (CAD), an automated quality assessment system that
performs at the level of a professional pathologist is necessary. To address
this need, we propose a fully automated quality assessment method for Cervical
Cytopathology Whole Slide Images (WSIs) based on The Bethesda System (TBS)
diagnostic standards, artificial intelligence algorithms, and the
characteristics of clinical data. The method analysis the context of WSIs to
quantify quality evaluation metrics which are focused by TBS such as staining
quality, cell counts and cell mass proportion through multiple models including
object detection, classification and segmentation. Subsequently, the XGBoost
model is used to mine the attention paid by pathologists to different quality
evaluation metrics when evaluating samples, thereby obtaining a comprehensive
WSI sample score calculation model. Experimental results on 100 WSIs
demonstrate that the proposed evaluation method has significant advantages in
terms of speed and consistency.

</details>


### [808] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/pdf/2505.13906)
*Soyabul Islam Lincoln, Mirza Mohd Shahriar Maswood*

Main category: eess.IV

TL;DR: A novel deep learning architecture for Alzheimer's disease diagnosis using MRI and deep neural networks achieves high accuracy across multiple datasets and classification tasks.


<details>
  <summary>Details</summary>
Motivation: The need for precise diagnosis and efficient treatment of Alzheimer's disease, given rising healthcare costs and AI's growing role in medical diagnostics.

Method: Proposes a deep learning model with multiresidual blocks, spatial attention, grouped query attention, and multi-head attention, tested on four datasets for binary and multiclass classification.

Result: Achieves accuracies up to 100% in binary classification and 99.66% in 4-class classification, outperforming state-of-the-art methods.

Conclusion: The model effectively classifies AD stages and retrieves critical information from MRI images, demonstrating superior performance over existing approaches.

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [809] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/pdf/2505.13911)
*Ruijie Zhao, Zuopeng Tan, Xiao Xue, Longfei Zhao, Bing Li, Zicheng Liao, Ying Ming, Jiaru Wang, Ran Xiao, Sirong Piao, Rui Zhao, Qiqi Xu, Wei Song*

Main category: eess.IV

TL;DR: A weakly supervised learning method (AHSL) uses anatomical hierarchy and bronchovascular tree information for pulmonary segment segmentation, reducing reliance on pixel-wise annotations.


<details>
  <summary>Details</summary>
Motivation: Pixel-wise annotation of pulmonary segments is laborious due to indistinguishable boundaries in medical images.

Method: AHSL employs segment-level and lobe-level supervision, a two-stage segmentation strategy, and a consistency loss for smooth boundaries.

Result: Experiments on a private dataset show effectiveness in segmenting pulmonary segments accurately.

Conclusion: AHSL provides a viable solution for pulmonary segment segmentation with reduced annotation effort.

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [810] [End-to-end Cortical Surface Reconstruction from Clinical Magnetic Resonance Images](https://arxiv.org/pdf/2505.14017)
*Jesper Duemose Nielsen, Karthik Gopinath, Andrew Hoopes, Adrian Dalca, Colin Magdamo, Steven Arnold, Sudeshna Das, Axel Thielscher, Juan Eugenio Iglesias, Oula Puonti*

Main category: eess.IV

TL;DR: A neural network method for cortical surface estimation from diverse clinical MR scans, outperforming existing tools in accuracy and enabling broader studies.


<details>
  <summary>Details</summary>
Motivation: Current tools for cortical surface estimation are limited to specific MR contrasts and resolutions, excluding most clinical scans. This work aims to overcome these limitations.

Method: Uses synthetic domain-randomized data to train a neural network for explicit cortical surface estimation, deforming a template mesh for topological correctness.

Result: Achieves a 50% reduction in cortical thickness error compared to existing methods and better captures aging-related thinning patterns.

Conclusion: The method enables fast, accurate surface reconstruction for clinical scans, facilitating large-scale and diverse population studies.

Abstract: Surface-based cortical analysis is valuable for a variety of neuroimaging
tasks, such as spatial normalization, parcellation, and gray matter (GM)
thickness estimation. However, most tools for estimating cortical surfaces work
exclusively on scans with at least 1 mm isotropic resolution and are tuned to a
specific magnetic resonance (MR) contrast, often T1-weighted (T1w). This
precludes application using most clinical MR scans, which are very
heterogeneous in terms of contrast and resolution. Here, we use synthetic
domain-randomized data to train the first neural network for explicit
estimation of cortical surfaces from scans of any contrast and resolution,
without retraining. Our method deforms a template mesh to the white matter (WM)
surface, which guarantees topological correctness. This mesh is further
deformed to estimate the GM surface. We compare our method to
recon-all-clinical (RAC), an implicit surface reconstruction method which is
currently the only other tool capable of processing heterogeneous clinical MR
scans, on ADNI and a large clinical dataset (n=1,332). We show a approximately
50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect
to RAC and better recovery of the aging-related cortical thinning patterns
detected by FreeSurfer on high-resolution T1w scans. Our method enables fast
and accurate surface reconstruction of clinical scans, allowing studies (1)
with sample sizes far beyond what is feasible in a research setting, and (2) of
clinical populations that are difficult to enroll in research studies. The code
is publicly available at https://github.com/simnibs/brainnet.

</details>


### [811] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/pdf/2505.14064)
*Cosmin I. Bercea, Jun Li, Philipp Raffler, Evamaria O. Riedel, Lena Schmitzer, Angela Kurz, Felix Bitzer, Paula Roßmüller, Julian Canisius, Mirjam L. Beyrle, Che Liu, Wenjia Bai, Bernhard Kainz, Julia A. Schnabel, Benedikt Wiestler*

Main category: eess.IV

TL;DR: NOVA is a challenging benchmark for evaluating out-of-distribution generalization in models, focusing on rare brain MRI pathologies.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to test models on rare or novel conditions, masking real-world performance gaps.

Method: NOVA includes ~900 brain MRI scans with 281 rare pathologies, clinical narratives, and expert annotations for joint evaluation.

Result: Leading vision-language models show significant performance drops on NOVA, highlighting its rigor.

Conclusion: NOVA serves as an extreme stress-test for models, pushing advancements in detecting and reasoning about unknown anomalies.

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [812] [Neural Video Compression with Context Modulation](https://arxiv.org/pdf/2505.14541)
*Chuanbo Tang, Zhuoyuan Li, Yifan Bian, Li Li, Dong Liu*

Main category: eess.IV

TL;DR: The paper proposes a two-step method to enhance temporal context modulation in neural video codecs, achieving significant bitrate reductions compared to H.266/VVC and DCVC-FM.


<details>
  <summary>Details</summary>
Motivation: Existing neural video codecs (NVC) underutilize reference frame information due to limitations in temporal context propagation.

Method: The approach involves flow orientation to mine inter-correlation and context compensation to modulate temporal context, aided by a synergy mechanism and decoupling loss.

Result: The codec achieves 22.7% bitrate reduction over H.266/VVC and 10.1% over DCVC-FM.

Conclusion: The proposed method effectively improves temporal context utilization, enhancing compression performance in NVC.

Abstract: Efficient video coding is highly dependent on exploiting the temporal
redundancy, which is usually achieved by extracting and leveraging the temporal
context in the emerging conditional coding-based neural video codec (NVC).
Although the latest NVC has achieved remarkable progress in improving the
compression performance, the inherent temporal context propagation mechanism
lacks the ability to sufficiently leverage the reference information, limiting
further improvement. In this paper, we address the limitation by modulating the
temporal context with the reference frame in two steps. Specifically, we first
propose the flow orientation to mine the inter-correlation between the
reference frame and prediction frame for generating the additional oriented
temporal context. Moreover, we introduce the context compensation to leverage
the oriented context to modulate the propagated temporal context generated from
the propagated reference feature. Through the synergy mechanism and decoupling
loss supervision, the irrelevant propagated information can be effectively
eliminated to ensure better context modeling. Experimental results demonstrate
that our codec achieves on average 22.7% bitrate reduction over the advanced
traditional video codec H.266/VVC, and offers an average 10.1% bitrate saving
over the previous state-of-the-art NVC DCVC-FM. The code is available at
https://github.com/Austin4USTC/DCMVC.

</details>


### [813] [Neural Inverse Scattering with Score-based Regularization](https://arxiv.org/pdf/2505.14560)
*Yuan Gao, Wenhan Guo, Yu Sun*

Main category: eess.IV

TL;DR: The paper proposes a regularized neural field (NF) approach for inverse scattering, integrating denoising score functions for better imaging quality.


<details>
  <summary>Details</summary>
Motivation: Inverse scattering is a key challenge in imaging, requiring joint estimation of images and scattering fields, which demands effective regularization.

Method: The method combines neural fields with denoising score functions from score-based generative models to regularize joint estimation.

Result: Tests on high-contrast simulated objects show superior imaging quality compared to state-of-the-art NF methods using total variation regularization.

Conclusion: The proposed approach outperforms existing NF methods by leveraging denoising score functions for better structural priors.

Abstract: Inverse scattering is a fundamental challenge in many imaging applications,
ranging from microscopy to remote sensing. Solving this problem often requires
jointly estimating two unknowns -- the image and the scattering field inside
the object -- necessitating effective image prior to regularize the inference.
In this paper, we propose a regularized neural field (NF) approach which
integrates the denoising score function used in score-based generative models.
The neural field formulation offers convenient flexibility to performing joint
estimation, while the denoising score function imposes the rich structural
prior of images. Our results on three high-contrast simulated objects show that
the proposed approach yields a better imaging quality compared to the
state-of-the-art NF approach, where regularization is based on total variation.

</details>


### [814] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/pdf/2505.14572)
*Jayroop Ramesh, Valentin Bacher, Mark C. Eid, Hoda Kalabizadeh, Christian Rupprecht, Ana IL Namburete, Pak-Hei Yeung, Madeleine K. Wyburd, Nicola K. Dinsdale*

Main category: eess.IV

TL;DR: An automated pipeline for fetal biometry measurement using ultrasound to improve reliability and reduce variability in predicting instrumental vaginal delivery outcomes.


<details>
  <summary>Details</summary>
Motivation: To address intra- and inter-observer variability in ultrasound-derived parameters (AoP and HSD) for monitoring labor progression.

Method: A three-task pipeline: (i) standard plane classification, (ii) segmentation of fetal head and pubic symphysis, (iii) computation of AoP and HSD using ensemble-based deep learning and ellipse fitting.

Result: High accuracy (ACC: 0.9452) and robustness in measurements, with low errors (ΔAoP: 8.90, ΔHSD: 14.35) on unseen data.

Conclusion: The pipeline enhances reliability in labor monitoring, aiding clinical risk stratification and prenatal care.

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


### [815] [StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining](https://arxiv.org/pdf/2403.11340)
*Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: STAINDIFFUSER is a multitask diffusion model for virtual IHC staining, outperforming baselines with limited data.


<details>
  <summary>Details</summary>
Motivation: IHC stains improve diagnostic accuracy but are resource-intensive; deep learning offers a cost-effective alternative.

Method: STAINDIFFUSER trains two diffusion processes: IHC stain generation from H&E images and H&E-based cell segmentation.

Result: STAINDIFFUSER generates high-quality virtual stains for two markers, surpassing 20 I2I baselines.

Conclusion: STAINDIFFUSER is a promising solution for virtual staining with limited datasets.

Abstract: Hematoxylin and Eosin (H&E) staining is widely regarded as the standard in
pathology for diagnosing diseases and tracking tumor recurrence. While H&E
staining shows tissue structures, it lacks the ability to reveal specific
proteins that are associated with disease severity and treatment response.
Immunohistochemical (IHC) stains use antibodies to highlight the expression of
these proteins on their respective cell types, improving diagnostic accuracy,
and assisting with drug selection for treatment. Despite their value, IHC
stains require additional time and resources, limiting their utilization in
some clinical settings. Recent advances in deep learning have positioned
Image-to-Image (I2I) translation as a computational, cost-effective alternative
for IHC. I2I generates high fidelity stain transformations digitally,
potentially replacing manual staining in IHC. Diffusion models, the current
state of the art in image generation and conditional tasks, are particularly
well suited for virtual IHC due to their ability to produce high quality images
and resilience to mode collapse. However, these models require extensive and
diverse datasets (often millions of samples) to achieve a robust performance, a
challenge in virtual staining applications where only thousands of samples are
typically available. Inspired by the success of multitask deep learning models
in scenarios with limited data, we introduce STAINDIFFUSER, a novel multitask
diffusion architecture tailored to virtual staining that achieves convergence
with smaller datasets. STAINDIFFUSER simultaneously trains two diffusion
processes: (a) generating cell specific IHC stains from H&E images and (b)
performing H&E based cell segmentation, utilizing coarse segmentation labels
exclusively during training. STAINDIFFUSER generates high-quality virtual
stains for two markers, outperforming over twenty I2I baselines.

</details>


### [816] [Explaining Uncertainty in Multiple Sclerosis Lesion Segmentation Beyond Prediction Errors](https://arxiv.org/pdf/2504.04814)
*Nataliia Molchanova, Pedro M. Gordaliza, Alessandro Cagol, Mario Ocampo--Pineda, Po--Jui Lu, Matthias Weigel, Xinjie Chen, Erin S. Beck, Haris Tsagkas, Daniel Reich, Anna Stölting, Pietro Maggi, Delphine Ribes, Adrien Depeursinge, Cristina Granziera, Henning Müller, Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: The paper introduces a framework to explain predictive uncertainty in AI for medical image segmentation, focusing on cortical lesion segmentation in multiple sclerosis. It links uncertainty to lesion characteristics and validates findings with expert feedback.


<details>
  <summary>Details</summary>
Motivation: To enhance AI reliability in healthcare by improving understanding of uncertainty sources in medical image segmentation, particularly for high-stakes tasks.

Method: Proposes a novel framework using deep ensembles to analyze predictive uncertainty, focusing on lesion size, shape, and cortical involvement. Validates with expert feedback and evaluations on two datasets.

Result: Instance-wise uncertainty correlates with lesion characteristics. Expert feedback confirms similar factors affect annotator confidence. Framework performs well under in-domain and distribution-shift conditions.

Conclusion: The framework effectively explains uncertainty sources in medical imaging, aiding clinical interpretability and reliability of AI in healthcare.

Abstract: Trustworthy artificial intelligence (AI) is essential in healthcare,
particularly for high-stakes tasks like medical image segmentation. Explainable
AI and uncertainty quantification significantly enhance AI reliability by
addressing key attributes such as robustness, usability, and explainability.
Despite extensive technical advances in uncertainty quantification for medical
imaging, understanding the clinical informativeness and interpretability of
uncertainty remains limited. This study introduces a novel framework to explain
the potential sources of predictive uncertainty, specifically in cortical
lesion segmentation in multiple sclerosis using deep ensembles. The proposed
analysis shifts the focus from the uncertainty-error relationship towards
relevant medical and engineering factors. Our findings reveal that
instance-wise uncertainty is strongly related to lesion size, shape, and
cortical involvement. Expert rater feedback confirms that similar factors
impede annotator confidence. Evaluations conducted on two datasets (206
patients, almost 2000 lesions) under both in-domain and distribution-shift
conditions highlight the utility of the framework in different scenarios.

</details>


### [817] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/pdf/2505.08616)
*Yifan Li, Peter Ho, Jo Woon Chong*

Main category: eess.IV

TL;DR: A smartphone-based method for diagnosing keratoconus (KC) uses Placido disc imaging and a two-stage diagnostic process, achieving 96.94% accuracy in pinpointing KC locations.


<details>
  <summary>Details</summary>
Motivation: Traditional KC diagnostic tools are bulky, costly, and require professional operation, prompting the need for a portable and accessible solution.

Method: The method involves capturing corneal reflections of a smartphone-generated Placido disc, using k-means clustering for KC identification, and a distance matrix with logistic regression for precise localization.

Result: The logistic regression model achieved 96.94% accuracy in classifying KC-affected areas, enabling effective detection and localization.

Conclusion: This smartphone-based approach offers a portable, accurate, and efficient solution for KC diagnosis and treatment planning.

Abstract: Keratoconus (KC) is a corneal disorder that results in blurry and distorted
vision. Traditional diagnostic tools, while effective, are often bulky, costly,
and require professional operation. In this paper, we present a portable and
innovative methodology for diagnosing. Our proposed approach first captures the
image reflected on the eye's cornea when a smartphone screen-generated Placido
disc sheds its light on an eye, then utilizes a two-stage diagnosis for
identifying the KC cornea and pinpointing the location of the KC on the cornea.
The first stage estimates the height and width of the Placido disc extracted
from the captured image to identify whether it has KC. In this KC
identification, k-means clustering is implemented to discern statistical
characteristics, such as height and width values of extracted Placido discs,
from non-KC (control) and KC-affected groups. The second stage involves the
creation of a distance matrix, providing a precise localization of KC on the
cornea, which is critical for efficient treatment planning. The analysis of
these distance matrices, paired with a logistic regression model and robust
statistical analysis, reveals a clear distinction between control and KC
groups. The logistic regression model, which classifies small areas on the
cornea as either control or KC-affected based on the corresponding inter-disc
distances in the distance matrix, reported a classification accuracy of 96.94%,
which indicates that we can effectively pinpoint the protrusion caused by KC.
This comprehensive, smartphone-based method is expected to detect KC and
streamline timely treatment.

</details>


### [818] [Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems](https://arxiv.org/pdf/2505.10311)
*Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian*

Main category: eess.IV

TL;DR: Whitened Score (WS) diffusion models avoid covariance inversion in anisotropic Gaussian diffusion, outperforming conventional models on tasks like imaging with structured noise.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion models struggle with anisotropic Gaussian diffusion due to covariance matrix inversion, limiting their applicability.

Method: Proposes WS diffusion models, learning the Whitened Score function via stochastic differential equations to bypass covariance inversion.

Result: WS models outperform conventional ones on computational imaging tasks (CIFAR, CelebA) with anisotropic noise.

Conclusion: WS diffusion models extend score-based DMs, enabling stable training and better performance for structured noise problems.

Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic
Gaussian diffusion processes due to the required inversion of covariance
matrices in the denoising score matching training objective
\cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion
models, a novel framework based on stochastic differential equations that
learns the Whitened Score function instead of the standard score. This approach
circumvents covariance inversion, extending score-based DMs by enabling stable
training of DMs on arbitrary Gaussian forward noising processes. WS DMs
establish equivalence with flow matching for arbitrary Gaussian noise, allow
for tailored spectral inductive biases, and provide strong Bayesian priors for
imaging inverse problems with structured noise. We experiment with a variety of
computational imaging tasks using the CIFAR and CelebA ($64\times64$) datasets
and demonstrate that WS diffusion priors trained on anisotropic Gaussian
noising processes consistently outperform conventional diffusion priors based
on isotropic Gaussian noise. Our code is open-sourced at
\href{https://github.com/jeffreyalido/wsdiffusion}{\texttt{github.com/jeffreyalido/wsdiffusion}}.

</details>


### [819] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/pdf/2505.10464)
*Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong*

Main category: eess.IV

TL;DR: The paper introduces the GCM 2025 dataset for gastric cancer MRI scans and proposes HWA-UNETR, a 3D segmentation framework, improving accuracy by 1.68% in Dice score.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multimodal medical image segmentation for gastric cancer due to dataset scarcity and misaligned modalities.

Method: Introduces HWA-UNETR with HWA blocks and tri-orientated fusion mamba for dynamic feature alignment and long-range dependencies.

Result: Outperforms existing methods by 1.68% in Dice score, validated on GCM 2025 and BraTS 2021 datasets.

Conclusion: The GCM 2025 dataset and HWA-UNETR framework advance gastric cancer segmentation with improved accuracy and robustness.

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [820] [Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis](https://arxiv.org/pdf/2505.12061)
*Samuel T. M. Ball*

Main category: eess.IV

TL;DR: Bayesian CNNs improve retinal OCT segmentation by providing uncertainty maps, enhancing clinical reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Manual retinal layer delineation in OCT scans is time-consuming and biased. Existing deep learning methods lack uncertainty estimation, leading to unreliable models.

Method: Bayesian convolutional neural networks (BCNNs) were applied to segment OCT scans, providing uncertainty maps for segmentation and layer thickness measurements.

Result: Achieved a Dice score of 95.65%, outperforming previous methods, and identified uncertain samples with recording artefacts.

Conclusion: BCNNs enhance clinical applicability, robustness, and performance in retinal OCT segmentation.

Abstract: Optical Coherence Tomography (OCT) provides valuable insights in
ophthalmology, cardiology, and neurology due to high-resolution,
cross-sectional images of the retina. One critical task for ophthalmologists
using OCT is delineation of retinal layers within scans. This process is
time-consuming and prone to human bias, affecting the accuracy and reliability
of diagnoses. Previous efforts to automate delineation using deep learning face
challenges in uptake from clinicians and statisticians due to the absence of
uncertainty estimation, leading to "confidently wrong" models via
hallucinations. In this study, we address these challenges by applying Bayesian
convolutional neural networks (BCNNs) to segment an openly available OCT
imaging dataset containing 35 human retina OCTs split between healthy controls
and patients with multiple sclerosis. Our findings demonstrate that Bayesian
models can be used to provide uncertainty maps of the segmentation, which can
further be used to identify highly uncertain samples that exhibit recording
artefacts such as noise or miscalibration at inference time. Our method also
allows for uncertainty-estimation for important secondary measurements such as
layer thicknesses, that are medically relevant for patients. We show that these
features come in addition to greater performance compared to similar work over
all delineations; with an overall Dice score of 95.65%. Our work brings greater
clinical applicability, statistical robustness, and performance to retinal OCT
segmentation.

</details>
